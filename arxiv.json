[
    {
        "title": "FinVet: A Collaborative Framework of RAG and External Fact-Checking\n  Agents for Financial Misinformation Detection",
        "url": "http://arxiv.org/abs/2510.11654v1",
        "pub_date": "2025-10-13",
        "summary": "Financial markets face growing threats from misinformation that can trigger billions in losses in minutes. Most existing approaches lack transparency in their decision-making and provide limited attribution to credible sources. We introduce FinVet, a novel multi-agent framework that integrates two Retrieval-Augmented Generation (RAG) pipelines with external fact-checking through a confidence-weighted voting mechanism. FinVet employs adaptive three-tier processing that dynamically adjusts verification strategies based on retrieval confidence, from direct metadata extraction to hybrid reasoning to full model-based analysis. Unlike existing methods, FinVet provides evidence-backed verdicts, source attribution, confidence scores, and explicit uncertainty flags when evidence is insufficient. Experimental evaluation on the FinFact dataset shows that FinVet achieves an F1 score of 0.85, which is a 10.4% improvement over the best individual pipeline (fact-check pipeline) and 37% improvement over standalone RAG approaches.",
        "translated": "金融市场正面临日益严重的虚假信息威胁，这些信息可能在几分钟内引发数十亿美元的损失。大多数现有方法在决策过程中缺乏透明度，并且对可信来源的归因有限。我们提出 FinVet，一个新颖的多智能体框架，通过置信度加权投票机制，将两个检索增强生成（Retrieval-Augmented Generation, RAG）流水线与外部事实核查相结合。FinVet 采用自适应的三级处理流程，根据检索置信度动态调整验证策略，从直接元数据提取，到混合推理，再到基于模型的完整分析。与现有方法不同，FinVet 在证据不足时提供基于证据的判定结果、来源归因、置信度评分以及明确的不确定性标记。在 FinFact 数据集上的实验评估表明，FinVet 达到 0.85 的 F1 分数，相比最优的单一流水线（事实核查流水线）提高了 10.4%，相比独立的 RAG 方法提高了 37%。"
    },
    {
        "title": "OneRec-Think: In-Text Reasoning for Generative Recommendation",
        "url": "http://arxiv.org/abs/2510.11639v1",
        "pub_date": "2025-10-13",
        "summary": "The powerful generative capacity of Large Language Models (LLMs) has instigated a paradigm shift in recommendation. However, existing generative models (e.g., OneRec) operate as implicit predictors, critically lacking the capacity for explicit and controllable reasoning-a key advantage of LLMs. To bridge this gap, we propose OneRec-Think, a unified framework that seamlessly integrates dialogue, reasoning, and personalized recommendation. OneRec-Think incorporates: (1) Itemic Alignment: cross-modal Item-Textual Alignment for semantic grounding; (2) Reasoning Activation: Reasoning Scaffolding to activate LLM reasoning within the recommendation context; and (3) Reasoning Enhancement, where we design a recommendation-specific reward function that accounts for the multi-validity nature of user preferences. Experiments across public benchmarks show state-of-the-art performance. Moreover, our proposed \"Think-Ahead\" architecture enables effective industrial deployment on Kuaishou, achieving a 0.159\\% gain in APP Stay Time and validating the practical efficacy of the model's explicit reasoning capability.",
        "translated": "大型语言模型（LLMs）强大的生成能力正在引发推荐系统领域的一场范式转变。然而，现有的生成模型（例如 OneRec）主要作为隐式预测器运行，严重缺乏显式可控推理的能力——而这正是 LLMs 的关键优势所在。为了解决这一问题，我们提出了 OneRec-Think，一个统一的框架，能够无缝融合对话、推理与个性化推荐。OneRec-Think 包含以下三个核心模块：（1）Itemic 对齐：跨模态的物品-文本对齐，以实现语义基础的构建；（2）推理激活：通过推理结构（Reasoning Scaffolding）在推荐场景中激活 LLM 的推理能力；以及（3）推理增强：我们设计了一个面向推荐任务的奖励函数，以应对用户偏好的多合理性（multi-validity）特性。在多个公开基准上的实验表明，该方法取得了最先进的性能。此外，我们提出的“Think-Ahead”架构在快手平台实现了有效的工业部署，使得 APP 使用时长提升了 0.159%，验证了模型显式推理能力的实用效果。"
    },
    {
        "title": "SemCSE-Multi: Multifaceted and Decodable Embeddings for Aspect-Specific\n  and Interpretable Scientific Domain Mapping",
        "url": "http://arxiv.org/abs/2510.11599v1",
        "pub_date": "2025-10-13",
        "summary": "We propose SemCSE-Multi, a novel unsupervised framework for generating multifaceted embeddings of scientific abstracts, evaluated in the domains of invasion biology and medicine. These embeddings capture distinct, individually specifiable aspects in isolation, thus enabling fine-grained and controllable similarity assessments as well as adaptive, user-driven visualizations of scientific domains. Our approach relies on an unsupervised procedure that produces aspect-specific summarizing sentences and trains embedding models to map semantically related summaries to nearby positions in the embedding space. We then distill these aspect-specific embedding capabilities into a unified embedding model that directly predicts multiple aspect embeddings from a scientific abstract in a single, efficient forward pass. In addition, we introduce an embedding decoding pipeline that decodes embeddings back into natural language descriptions of their associated aspects. Notably, we show that this decoding remains effective even for unoccupied regions in low-dimensional visualizations, thus offering vastly improved interpretability in user-centric settings.",
        "translated": "我们提出了一种名为 SemCSE-Multi 的新型无监督框架，用于生成科学摘要的多方面嵌入表示，并在入侵生物学和医学领域进行了评估。该嵌入能够独立捕捉多个明确且可单独指定的方面，从而支持细粒度、可控的相似性评估，以及自适应的、以用户驱动的科学领域可视化。我们的方法依赖于一个无监督过程，首先生成特定方面的总结性句子，并训练嵌入模型将语义相关的摘要映射到嵌入空间中的相近位置。随后，我们将这些特定方面的嵌入能力提炼到一个统一的嵌入模型中，使其能够在一个高效、单一的前向传播过程中直接从科学摘要中预测出多个方面嵌入。此外，我们还引入了一个嵌入解码流程，将嵌入表示还原为与各特定方面相关的自然语言描述。值得注意的是，我们证明了即使在低维可视化中未被占用的区域，该解码过程依然有效，从而在以用户为中心的应用场景中显著提升了可解释性。"
    },
    {
        "title": "REGENT: Relevance-Guided Attention for Entity-Aware Multi-Vector Neural\n  Re-Ranking",
        "url": "http://arxiv.org/abs/2510.11592v1",
        "pub_date": "2025-10-13",
        "summary": "Current neural re-rankers often struggle with complex information needs and long, content-rich documents. The fundamental issue is not computational--it is intelligent content selection: identifying what matters in lengthy, multi-faceted texts. While humans naturally anchor their understanding around key entities and concepts, neural models process text within rigid token windows, treating all interactions as equally important and missing critical semantic signals. We introduce REGENT, a neural re-ranking model that mimics human-like understanding by using entities as a \"semantic skeleton\" to guide attention. REGENT integrates relevance guidance directly into the attention mechanism, combining fine-grained lexical matching with high-level semantic reasoning. This relevance-guided attention enables the model to focus on conceptually important content while maintaining sensitivity to precise term matches. REGENT achieves new state-of-the-art performance in three challenging datasets, providing up to 108% improvement over BM25 and consistently outperforming strong baselines including ColBERT and RankVicuna. To our knowledge, this is the first work to successfully integrate entity semantics directly into neural attention, establishing a new paradigm for entity-aware information retrieval.",
        "translated": "目前的神经重排序模型在处理复杂的信息需求和长篇、内容丰富文档时常常面临挑战。其根本问题并不在于计算能力，而在于智能内容选择：识别长篇、多维文本中真正重要的信息。尽管人类在理解文本时自然地围绕关键实体和概念进行定位，但神经模型则受限于固定的词元窗口对文本进行处理，将所有交互视为同等重要，从而忽略了关键的语义信号。我们提出REGENT，这是一种神经重排序模型，通过使用实体作为“语义骨架”来引导注意力，从而模拟人类的理解方式。REGENT将相关性引导直接整合到注意力机制中，将细粒度的词汇匹配与高层次的语义推理相结合。这种相关性引导的注意力机制使模型能够聚焦于概念上重要的内容，同时保持对精确术语匹配的敏感性。REGENT在三个具有挑战性的数据集上达到了新的最先进性能，在BM25基础上最多提升了108%，并且始终优于ColBERT和RankVicuna等强基线模型。据我们所知，这是首次成功将实体语义直接整合到神经注意力机制中的工作，为具备实体感知能力的信息检索建立了一个新的范式。"
    },
    {
        "title": "QDER: Query-Specific Document and Entity Representations for\n  Multi-Vector Document Re-Ranking",
        "url": "http://arxiv.org/abs/2510.11589v1",
        "pub_date": "2025-10-13",
        "summary": "Neural IR has advanced through two distinct paths: entity-oriented approaches leveraging knowledge graphs and multi-vector models capturing fine-grained semantics. We introduce QDER, a neural re-ranking model that unifies these approaches by integrating knowledge graph semantics into a multi-vector model. QDER's key innovation lies in its modeling of query-document relationships: rather than computing similarity scores on aggregated embeddings, we maintain individual token and entity representations throughout the ranking process, performing aggregation only at the final scoring stage - an approach we call \"late aggregation.\" We first transform these fine-grained representations through learned attention patterns, then apply carefully chosen mathematical operations for precise matches. Experiments across five standard benchmarks show that QDER achieves significant performance gains, with improvements of 36% in nDCG@20 over the strongest baseline on TREC Robust 2004 and similar improvements on other datasets. QDER particularly excels on difficult queries, achieving an nDCG@20 of 0.70 where traditional approaches fail completely (nDCG@20 = 0.0), setting a foundation for future work in entity-aware retrieval.",
        "translated": "神经信息检索（Neural IR）的发展经历了两条不同的路径：一种是面向实体的方法，利用知识图谱信息；另一种是多向量模型，用于捕捉细粒度语义。我们提出 QDER，这是一种神经重排序模型，通过将知识图谱语义整合到多向量模型中，实现了这两条路径的统一。QDER 的关键创新在于其对查询与文档关系的建模：与在聚合嵌入上计算相似度分数的传统方法不同，我们在整个排序过程中保留每个词元（token）和实体的独立表示，仅在最终的评分阶段进行聚合——我们称这种策略为“晚期聚合”（late aggregation）。首先，我们通过学习得到的注意力模式对这些细粒度表示进行转换，然后应用精心选择的数学运算以实现精确匹配。在五个标准基准数据集上的实验表明，QDER 显著提升了性能，在 TREC Robust 2004 数据集上相比最强基线模型的 nDCG@20 提高了 36%，在其他数据集上也有类似提升。尤其在处理困难查询时，QDER 表现出色，取得了 0.70 的 nDCG@20 评分，而传统方法在此类查询上完全失效（nDCG@20 = 0.0），为未来实体感知（entity-aware）检索的研究奠定了基础。"
    },
    {
        "title": "Characterizing Web Search in The Age of Generative AI",
        "url": "http://arxiv.org/abs/2510.11560v1",
        "pub_date": "2025-10-13",
        "summary": "The advent of LLMs has given rise to a new type of web search: Generative search, where LLMs retrieve web pages related to a query and generate a single, coherent text as a response. This output modality stands in stark contrast to traditional web search, where results are returned as a ranked list of independent web pages. In this paper, we ask: Along what dimensions do generative search outputs differ from traditional web search? We compare Google, a traditional web search engine, with four generative search engines from two providers (Google and OpenAI) across queries from four domains. Our analysis reveals intriguing differences. Most generative search engines cover a wider range of sources compared to web search. Generative search engines vary in the degree to which they rely on internal knowledge contained within the model parameters v.s. external knowledge retrieved from the web. Generative search engines surface varying sets of concepts, creating new opportunities for enhancing search diversity and serendipity. Our results also highlight the need for revisiting evaluation criteria for web search in the age of Generative AI.",
        "translated": "大语言模型（LLMs）的出现催生了一种新的网络搜索方式：生成式搜索（generative search），在这种搜索方式中，LLMs会检索与查询相关的网页，并生成一段连贯统一的文本作为响应。这种输出形式与传统网络搜索形成了鲜明对比，后者返回的是一个按相关性排序的独立网页列表。在本文中，我们提出以下问题：生成式搜索的输出在哪些维度上与传统网络搜索有所不同？我们从四个领域中选取查询，比较了传统网络搜索引擎Google与来自两家提供商（Google和OpenAI）的四个生成式搜索引擎的表现。我们的分析揭示了一些有趣的差异。大多数生成式搜索引擎相比传统网络搜索，能够涵盖更广泛的来源。生成式搜索引擎在依赖模型参数中包含的内部知识与从网络检索的外部知识的程度上存在差异。此外，生成式搜索引擎展示的概念集合各不相同，从而为提升搜索多样性和偶然性提供了新的可能性。我们的结果还强调，在生成式人工智能时代，有必要重新审视网络搜索的评估标准。"
    },
    {
        "title": "Uncertainty Quantification for Retrieval-Augmented Reasoning",
        "url": "http://arxiv.org/abs/2510.11483v1",
        "pub_date": "2025-10-13",
        "summary": "Retrieval-augmented reasoning (RAR) is a recent evolution of retrieval-augmented generation (RAG) that employs multiple reasoning steps for retrieval and generation. While effective for some complex queries, RAR remains vulnerable to errors and misleading outputs. Uncertainty quantification (UQ) offers methods to estimate the confidence of systems' outputs. These methods, however, often handle simple queries with no retrieval or single-step retrieval, without properly handling RAR setup. Accurate estimation of UQ for RAR requires accounting for all sources of uncertainty, including those arising from retrieval and generation. In this paper, we account for all these sources and introduce Retrieval-Augmented Reasoning Consistency (R2C)--a novel UQ method for RAR. The core idea of R2C is to perturb the multi-step reasoning process by applying various actions to reasoning steps. These perturbations alter the retriever's input, which shifts its output and consequently modifies the generator's input at the next step. Through this iterative feedback loop, the retriever and generator continuously reshape one another's inputs, enabling us to capture uncertainty arising from both components. Experiments on five popular RAR systems across diverse QA datasets show that R2C improves AUROC by over 5% on average compared to the state-of-the-art UQ baselines. Extrinsic evaluations using R2C as an external signal further confirm its effectiveness for two downstream tasks: in Abstention, it achieves ~5% gains in both F1Abstain and AccAbstain; in Model Selection, it improves the exact match by ~7% over single models and ~3% over selection methods.",
        "translated": "检索增强推理（Retrieval-Augmented Reasoning, RAR）是检索增强生成（Retrieval-Augmented Generation, RAG）的最新演进，其通过在检索与生成过程中引入多步推理来提高效果。尽管在处理某些复杂查询时表现出色，RAR 仍然容易受到错误和误导性输出的影响。不确定性量化（Uncertainty Quantification, UQ）提供了一种估计系统输出置信度的方法。然而，现有方法大多针对无检索或单步检索的简单查询，未能有效应对 RAR 的设置。对 RAR 的 UQ 进行准确估计，需要综合考虑所有可能的不确定性来源，包括检索和生成过程中的不确定性。在本文中，我们系统地考虑了这些不确定性来源，并提出了检索增强推理一致性（Retrieval-Augmented Reasoning Consistency, R2C）——一种新型的 UQ 方法。R2C 的核心思想是通过对推理步骤施加多种操作，扰动多步推理过程。这些扰动会改变检索器的输入，从而影响其输出，并进一步修改生成器在下一步的输入。通过这一迭代反馈机制，检索器和生成器不断重塑彼此的输入，使我们能够捕捉来自两个组件的不确定性。在五个主流 RAR 系统和多个问答数据集上的实验表明，与最先进的 UQ 基线方法相比，R2C 在平均 AUROC 指标上提升了超过 5%。使用 R2C 作为外部信号进行的外在评估进一步验证了其有效性，针对两个下游任务：在拒绝回答（Abstention）任务中，R2C 在 F1Abstain 和 AccAbstain 指标上分别提升了约 5%；在模型选择（Model Selection）任务中，R2C 相比单模型提升了约 7% 的精确匹配（exact match），相比其他选择方法提升了约 3%。"
    },
    {
        "title": "What Generative Search Engines Like and How to Optimize Web Content\n  Cooperatively",
        "url": "http://arxiv.org/abs/2510.11438v1",
        "pub_date": "2025-10-13",
        "summary": "By employing large language models (LLMs) to retrieve documents and generate natural language responses, Generative Engines, such as Google AI overview and ChatGPT, provide significantly enhanced user experiences and have rapidly become the new form of search. Their rapid adoption also drives the needs of Generative Engine Optimization (GEO), as content providers are eager to gain more traction from them. In this paper, we introduce AutoGEO, a framework to automatically learn generative engine preferences when using retrieved contents for response generation, and rewrite web contents for more such traction. AutoGEO first prompts frontier LLMs to explain generative engine preferences and extract meaningful preference rules from these explanations. Then it uses preference rules as context engineering for AutoGEO$_\\text{API}$, a prompt-based GEO system, and as rule-based rewards to train AutoGEO$_\\text{Mini}$, a cost-effective GEO model. Experiments on the standard GEO-Bench and two newly constructed benchmarks using real user queries demonstrate the effectiveness of AutoGEO in enhancing content traction while preserving search utility. Analyses confirm the learned rules' robustness and abilities to capture unique preferences in variant domains, and AutoGEO systems' ability to embed them in content optimization. The code is released at https://github.com/cxcscmu/AutoGEO.",
        "translated": "通过使用大语言模型（LLMs）来检索文档并生成自然语言响应，生成式引擎（如 Google AI 概述和 ChatGPT）显著提升了用户体验，并迅速成为搜索的新形态。其迅速普及也推动了生成式引擎优化（Generative Engine Optimization, GEO）的需求，因为内容提供者希望从这些系统中获得更多曝光。在本文中，我们提出了 AutoGEO，一个在使用检索内容进行响应生成时，能够自动学习生成式引擎偏好的框架，并对网页内容进行重写以提高其曝光度。AutoGEO 首先提示前沿的大语言模型解释生成式引擎的偏好，并从这些解释中提取有意义的偏好规则。接着，它将这些偏好规则用于 AutoGEO$_\\text{API}$——一个基于提示的 GEO 系统——作为上下文工程的输入，并将规则作为奖励机制，用于训练 AutoGEO$_\\text{Mini}$——一个经济高效的 GEO 模型。在标准的 GEO-Bench 基准以及基于真实用户查询构建的两个新基准上的实验表明，AutoGEO 在提升内容曝光度的同时能够保持搜索效用的有效性。分析结果进一步验证了所学习规则的鲁棒性及其在不同领域中捕捉独特偏好的能力，并确认了 AutoGEO 系统在内容优化中嵌入这些规则的能力。代码已发布在 https://github.com/cxcscmu/AutoGEO。"
    },
    {
        "title": "On Inherited Popularity Bias in Cold-Start Item Recommendation",
        "url": "http://arxiv.org/abs/2510.11402v1",
        "pub_date": "2025-10-13",
        "summary": "Collaborative filtering (CF) recommender systems struggle with making predictions on unseen, or 'cold', items. Systems designed to address this challenge are often trained with supervision from warm CF models in order to leverage collaborative and content information from the available interaction data. However, since they learn to replicate the behavior of CF methods, cold-start models may therefore also learn to imitate their predictive biases. In this paper, we show that cold-start systems can inherit popularity bias, a common cause of recommender system unfairness arising when CF models overfit to more popular items, thereby maximizing user-oriented accuracy but neglecting rarer items. We demonstrate that cold-start recommenders not only mirror the popularity biases of warm models, but are in fact affected more severely: because they cannot infer popularity from interaction data, they instead attempt to estimate it based solely on content features. This leads to significant over-prediction of certain cold items with similar content to popular warm items, even if their ground truth popularity is very low. Through experiments on three multimedia datasets, we analyze the impact of this behavior on three generative cold-start methods. We then describe a simple post-processing bias mitigation method that, by using embedding magnitude as a proxy for predicted popularity, can produce more balanced recommendations with limited harm to user-oriented cold-start accuracy.",
        "translated": "协同过滤（Collaborative Filtering, CF）推荐系统在对未见过的或“冷启动”的物品进行预测时面临挑战。为了解决这一问题，设计用于冷启动场景的系统通常会借助“热启动”CF模型进行监督训练，以利用现有的交互数据中的协作信息和内容信息。然而，由于这些系统学习的是复制CF方法的行为，因此冷启动模型也可能学会模仿其预测偏差。本文中，我们表明冷启动系统可能会继承“流行度偏差”（popularity bias），这是推荐系统不公平性的常见原因，当CF模型过度拟合更受欢迎的物品时就会产生这种偏差，从而在最大化面向用户准确率的同时忽略了较少出现的物品。我们通过实验发现，冷启动推荐器不仅复制了热模型的流行度偏差，而且实际上受到更严重的影响：由于它们无法从交互数据中推断出流行度，因此转而尝试仅基于内容特征来估计流行度。这导致某些与热门热启动物品在内容上相似的冷启动物品被显著高估，即使它们的真实流行度非常低。我们在三个多媒体数据集上分析了这一行为对三种生成式冷启动方法的影响。随后，我们介绍了一种简单的后处理偏差缓解方法，该方法通过将嵌入（embedding）模长作为预测流行度的代理指标，能够在有限地影响面向用户冷启动准确率的前提下，生成更加平衡的推荐结果。"
    },
    {
        "title": "VeriCite: Towards Reliable Citations in Retrieval-Augmented Generation\n  via Rigorous Verification",
        "url": "http://arxiv.org/abs/2510.11394v1",
        "pub_date": "2025-10-13",
        "summary": "Retrieval-Augmented Generation (RAG) has emerged as a crucial approach for enhancing the responses of large language models (LLMs) with external knowledge sources. Despite the impressive performance in complex question-answering tasks, RAG still struggles with hallucinations. Attributing RAG-generated content through in-line citations has demonstrated potential in reducing hallucinations and facilitating human verification. Existing citation generation methods primarily rely on either fine-tuning the generator or employing post-processing approaches for citation matching. However, the former approach demands substantial annotated data and computational resources, while the latter often encounters difficulties in managing multiple citations and frequently produces suboptimal results. In this paper, we introduce a novel framework, called VeriCite, designed to rigorously validate supporting evidence and enhance answer attribution. Specifically, VeriCite breaks down into a three-stage generation: 1) The initial answer generation first generates a response based on all available contexts and has its claims verified through the NLI model; 2) the supporting evidence selection assesses the utility of each document and extracts useful supporting evidences; 3) the final answer refinement integrates the initial response and collected evidences to produce the final, refined answer.We conduct experiments across five open-source LLMs and four datasets, demonstrating that VeriCite can significantly improve citation quality while maintaining the correctness of the answers.",
        "translated": "检索增强生成（Retrieval-Augmented Generation, RAG）已成为一种关键方法，用于通过外部知识源增强大语言模型（Large Language Models, LLMs）的响应能力。尽管RAG在复杂问答任务中表现出色，但其仍面临幻觉（hallucination）问题。通过行内引用（in-line citations）对RAG生成的内容进行归因，已被证明在减少幻觉和便于人工验证方面具有潜力。现有的引用生成方法主要依赖于对生成器的微调，或采用后处理方法进行引用匹配。然而，前者需要大量标注数据和计算资源，而后者在处理多个引用时常常遇到困难，且结果往往不够理想。在本文中，我们提出了一种新颖的框架，命名为VeriCite，旨在严格验证支持证据并提升答案归因能力。具体而言，VeriCite分为三个生成阶段：1）初始答案生成阶段基于所有可用上下文生成初步回答，并通过自然语言推理模型（NLI model）对其主张进行验证；2）支持证据选择阶段评估每个文档的有用性，并提取有效的支持证据；3）最终答案优化阶段整合初始回答与收集到的证据，生成最终的、经过优化的答案。我们在五种开源大语言模型和四个数据集上进行了实验，结果表明，VeriCite在保持答案正确性的同时，能够显著提升引用质量。"
    },
    {
        "title": "LLM-Specific Utility: A New Perspective for Retrieval-Augmented\n  Generation",
        "url": "http://arxiv.org/abs/2510.11358v1",
        "pub_date": "2025-10-13",
        "summary": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by incorporating external knowledge. While traditional retrieval focuses on relevance, RAG's effectiveness depends on the utility of retrieved passages, i.e., the usefulness in facilitating the generation of an accurate and comprehensive answer. Existing studies often treat utility as a generic attribute, ignoring the fact that different LLMs may benefit differently from the same passage due to variations in internal knowledge and comprehension ability. In this work, we introduce and systematically investigate the notion of LLM-specific utility. Through large-scale experiments across multiple datasets and LLMs, we demonstrate that human-annotated passages are not optimal for LLMs and that ground-truth utilitarian passages are not transferable across different LLMs. These findings highlight the necessity of adopting the LLM-specific utility in RAG research. Our findings indicate that some human-annotated passages are not ground-truth utilitarian passages for specific LLMs, partially due to the varying readability of queries and passages for LLMs, a tendency for which perplexity is a key metric. Based on these findings, we propose a benchmarking procedure for LLM-specific utility judgments. We evaluate existing utility judgment methods on six datasets and find that while verbalized methods using pseudo-answers perform robustly, LLMs struggle to assess utility effectively-failing to reject all passages for known queries and to select truly useful ones for unknown queries.",
        "translated": "检索增强生成（Retrieval-augmented Generation, RAG）通过引入外部知识来增强大语言模型（Large Language Models, LLMs）的能力。尽管传统检索方法主要关注相关性（relevance），但 RAG 的有效性依赖于检索到的段落的**效用性**（utility），即这些段落在促进生成准确且全面答案方面的有用程度。现有研究通常将效用性视为一种通用属性，忽视了由于不同 LLM 在内部知识和理解能力上的差异，它们可能从相同的段落中获得不同的收益。在本工作中，我们引入并系统地探讨了**面向特定 LLM 的效用性**（LLM-specific utility）这一概念。通过在多个数据集和 LLM 上的大规模实验，我们证明：人工标注的段落对 LLM 来说并非最优选择，并且真实效用性段落在不同 LLM 之间不可迁移。这些发现凸显了在 RAG 研究中采用 LLM-specific utility 的必要性。我们的研究表明，某些人工标注的段落对特定 LLM 而言并不是真实效用性段落，部分原因是 LLM 对查询和段落的可读性存在差异，而这种差异可以通过困惑度（perplexity）这一关键指标来衡量。基于上述发现，我们提出了一种面向 LLM-specific utility 的基准评估方法。我们对六种数据集上的现有效用性判断方法进行了评估，发现尽管使用伪答案（pseudo-answers）的显式化方法表现较为稳健，但 LLM 在评估段落效用性方面仍存在困难，无法拒绝所有已知查询的段落，也无法为未知查询有效挑选真正有用的段落。"
    },
    {
        "title": "Dynamic Network-Based Two-Stage Time Series Forecasting for Affiliate\n  Marketing",
        "url": "http://arxiv.org/abs/2510.11323v1",
        "pub_date": "2025-10-13",
        "summary": "In recent years, affiliate marketing has emerged as a revenue-sharing strategy where merchants collaborate with promoters to promote their products. It not only increases product exposure but also allows promoters to earn a commission. This paper addresses the pivotal yet under-explored challenge in affiliate marketing: accurately assessing and predicting the contributions of promoters in product promotion. We design a novel metric for evaluating the indirect contributions of the promoter, called propagation scale. Unfortunately, existing time series forecasting techniques fail to deliver accurate predictions due to the propagation scale being influenced by multiple factors and the inherent complexities arising from dynamic scenarios. To address this issue, we decouple the network structure from the node signals and propose a two-stage solution: initially, the basic self-sales and network structure prediction are conducted separately, followed by the synthesis of the propagation scale. Specifically, we design a graph convolution encoding scheme based on descendant neighbors and incorporate hypergraph convolution to efficiently capture complex promotional dynamics. Additionally, three auxiliary tasks are employed: self-sales prediction for base estimations, descendant prediction to synthesize propagation scale, and promoter activation prediction to mitigate high volatility issues. Extensive offline experiments on large-scale industrial datasets validate the superiority of our method. We further deploy our model on Alimama platform with over $100,000$ promoters, achieving a $9.29\\%$ improvement in GMV and a $5.89\\%$ increase in sales volume.",
        "translated": "近年来，联盟营销已成为一种收益共享策略，商家与推广者合作以推广其商品。这种模式不仅提高了商品的曝光度，还使推广者能够获得佣金。本文关注联盟营销中一个关键但尚未充分研究的问题：**准确评估和预测推广者在商品推广中的贡献**。我们设计了一种用于衡量推广者间接贡献的新指标，称为**传播规模（propagation scale）**。然而，由于传播规模受到多种因素影响，且动态场景中存在内在复杂性，现有的时间序列预测技术难以实现准确的预测。为了解决这一问题，我们**将网络结构与节点信号解耦**，并提出了一种两阶段的解决方案：首先，分别预测基本的自主销售与网络结构；随后，合成传播规模。具体而言，我们设计了一种基于后代邻居的图卷积编码方案，并引入超图卷积以高效捕捉复杂的推广动态。此外，我们还引入了三个辅助任务：用于基础估计的自主销售预测、用于合成传播规模的后代节点预测，以及用于缓解高波动性问题的推广者激活预测。我们在大规模工业数据集上进行了广泛的离线实验，验证了我们方法的优越性。我们进一步将模型部署在拥有超过100,000名推广者的Alimama平台，实现了GMV提升9.29%，销售量增长5.89%。"
    },
    {
        "title": "Next Interest Flow: A Generative Pre-training Paradigm for Recommender\n  Systems by Modeling All-domain Movelines",
        "url": "http://arxiv.org/abs/2510.11317v1",
        "pub_date": "2025-10-13",
        "summary": "Click-Through Rate (CTR) prediction, a cornerstone of modern recommender systems, has been dominated by discriminative models that react to past user behavior rather than proactively modeling user intent. Existing generative paradigms attempt to address this but suffer from critical limitations: Large Language Model (LLM) based methods create a semantic mismatch by forcing e-commerce signals into a linguistic space, while ID-based generation is constrained by item memorization and cold-start issues. To overcome these limitations, we propose a novel generative pre-training paradigm. Our model learns to predict the Next Interest Flow, a dense vector sequence representing a user's future intent, while simultaneously modeling its internal Interest Diversity and Interest Evolution Velocity to ensure the representation is both rich and coherent. However, this two-stage approach introduces a critical objective mismatch between the generative and discriminative stages. We resolve this via a bidirectional alignment strategy, which harmonizes the two stages through cross-stage weight initialization and a dynamic Semantic Alignment Module for fine-tuning. Additionally, we enhance the underlying discriminative model with a Temporal Sequential Pairwise (TSP) mechanism to better capture temporal causality. We present the All-domain Moveline Evolution Network (AMEN), a unified framework implementing our entire pipeline. Extensive offline experiments validate AMEN's superiority over strong baselines, and a large-scale online A/B test demonstrates its significant real-world impact, delivering substantial improvements in key business metrics.",
        "translated": "点击率（CTR）预测是现代推荐系统中的核心任务之一，目前主要依赖于判别式模型，这些模型侧重于响应用户过去的行为，而未能主动建模用户的意图。现有的生成式范式尝试解决这一问题，但存在关键的局限性：基于大语言模型（LLM）的方法将电商信号强行映射到语言语义空间，导致语义错配；而基于ID的生成方法则受到物品记忆能力和冷启动问题的限制。为克服这些限制，我们提出了一种新颖的生成式预训练范式。我们的模型旨在预测“下一个兴趣流”（Next Interest Flow），即一个稠密向量序列，用于表示用户未来的兴趣意图。同时，该模型还建模内部的兴趣多样性（Interest Diversity）和兴趣演化速度（Interest Evolution Velocity），以确保表示的丰富性与一致性。然而，这种两阶段的方法在生成阶段与判别阶段之间引入了关键的目标不匹配问题。我们通过双向对齐策略加以解决，该策略通过跨阶段权重初始化和动态语义对齐模块（Semantic Alignment Module）进行微调，从而协调两个阶段之间的差异。此外，我们通过引入时间序列成对机制（Temporal Sequential Pairwise, TSP）来增强底层判别模型，以更好地捕捉时间因果关系。我们提出了一个统一的框架——全领域兴趣演化网络（All-domain Moveline Evolution Network, AMEN），实现了我们完整的流水线。大量离线实验验证了AMEN在强基线模型上的优越性能，而大规模在线A/B测试也展示了其在现实场景中的显著影响，显著提升了关键业务指标。"
    },
    {
        "title": "ELMO: Efficiency via Low-precision and Peak Memory Optimization in Large\n  Output Spaces",
        "url": "http://arxiv.org/abs/2510.11168v1",
        "pub_date": "2025-10-13",
        "summary": "Large output spaces, also referred to as Extreme multilabel classification (XMC), is a setting that arises, e.g., in large-scale tagging and product-to-product recommendation, and is characterized by the number of labels ranging from hundreds of thousands to millions. This means that the linear classification head, usually only a tiny fraction of the overall model, turns into the main driver for compute and memory demand. Current state-of-the-art XMC methods predominantly rely on FP16-FP32 mixed-precision training, which we show can be unstable, and inefficient in terms of memory usage and computational overhead. Meanwhile, existing low-precision methods typically retain higher precision for the classification layer. In this work, we propose ELMO, a pure low-precision training framework for XMC models using BFloat16 and Float8 data types. By leveraging Kahan summation and stochastic rounding, we demonstrate that XMC models can be effectively trained entirely in Float8, without relying on single-precision master weights or tensor scaling. Low-precision training, combined with our proposed memory optimizations -- gradient fusion and chunking -- enables significant reductions in GPU memory usage. For example, we train a 3-million-label XMC model with only 6.6 GiB of GPU memory, compared to the 39.7 GiB required by the optimized SOTA method, Renee without compromising accuracy.",
        "translated": "大规模输出空间，也称为极端多标签分类（Extreme multilabel classification, XMC），是一种在大规模标签分配和商品到商品推荐等场景中常见的设定，其特点是标签数量可高达数十万到数百万级。这意味着线性分类头（通常在整个模型中仅占极小部分）变成了计算和内存需求的主要驱动因素。当前最先进的XMC方法主要依赖FP16与FP32混合精度训练，但我们发现这种方法在训练稳定性、内存使用效率以及计算开销方面存在不足。同时，现有的低精度训练方法通常仍为分类层保留较高精度。在本文中，我们提出ELMO，一个完全基于BFloat16和Float8数据类型的低精度训练框架。通过引入Kahan求和和随机舍入技术，我们证明XMC模型可以完全在Float8精度下进行有效训练，而无需依赖单精度主权重或张量缩放。结合我们提出的内存优化方法——梯度融合和块处理（chunking），该框架能够显著减少GPU内存的使用。例如，我们在仅使用6.6 GiB GPU内存的情况下训练了一个包含300万个标签的XMC模型，而优化后的SOTA方法Renee则需要39.7 GiB的内存，且在不损失精度的前提下实现了这一目标。"
    },
    {
        "title": "DyKnow-RAG: Dynamic Knowledge Utilization Reinforcement Framework for\n  Noisy Retrieval-Augmented Generation in E-commerce Search Relevance",
        "url": "http://arxiv.org/abs/2510.11122v1",
        "pub_date": "2025-10-13",
        "summary": "Accurately modeling query-item relevance drives e-commerce ranking, yet long-tail, knowledge-heavy, and fast-evolving queries exceed parametric LLM coverage. External context (reviews, attribute encyclopedias, UGC) can help but is noisy, and single-pass latency and cost forbid any clean-then-summarize step. The model must, per query, judge relevance and decide whether to use, partially use, or ignore the context. DyKnow-RAG is a dynamic noisy-RAG framework built on Group Relative Policy Optimization. It trains two rollout groups (no external context vs a single retrieved chunk) and applies posterior-driven inter-group advantage scaling that adaptively reweights their contributions by the per-query correctness gap. This teaches when to trust retrieval versus fall back to parametric knowledge, without process labels, value networks, or extra inference passes, preserving single-pass, single-chunk deployment under production latency. Training combines: (1) supervised initialization with a structured rationale that explicitly records the context-usage decision; (2) an RL pool prioritized by SFT uncertainty to focus where context choice is most consequential; and (3) an optional lightweight DPO warm start to stabilize with-context calibration. Under a unified retrieval/index and fixed latency budget, DyKnow-RAG outperforms SFT, DPO, and vanilla GRPO in offline tests, and delivers consistent lifts on GSB, Query Goodrate, and Item Goodrate in Taobao A/B testing. It is deployed in Taobao's production relevance system, serving live traffic. To our knowledge, it is among the first single-pass RAG solutions for e-commerce relevance, turning noisy external signals into reliable gains without added online complexity.",
        "translated": "准确地建模查询与商品的相关性对于电商排序至关重要，然而长尾、知识密集型和快速变化的查询超出了参数化大语言模型的覆盖范围。外部上下文（如商品评论、属性百科、用户生成内容）虽然可以提供帮助，但往往包含噪声，且单次推理的延迟和成本限制了任何“清理再摘要”的步骤。因此，模型必须在每次查询中判断相关性，并决定是否使用、部分使用或忽略外部上下文。\n\nDyKnow-RAG 是一种基于组相对策略优化（Group Relative Policy Optimization）的动态噪声-RAG框架。该框架通过训练两个 rollout 组（一组不使用外部上下文，另一组使用一个检索到的 chunk）并采用后验驱动的组间优势缩放方法，以查询间的准确性差距自适应地重新加权两组的贡献。这种方法能够在无需过程标签、价值网络或额外推理步骤的前提下，学习何时信任检索结果，何时回退至参数化知识，从而在生产延迟下保持单次推理、单 chunk 的部署效率。\n\nDyKnow-RAG 的训练结合了以下三个阶段：（1）结构化理由监督初始化，显式记录上下文使用决策；（2）以监督微调（SFT）的不确定性为优先级的强化学习（RL）池，集中于上下文选择最具影响的场景；（3）一个可选的轻量级 DPO 预训练阶段，用于稳定上下文相关校准。\n\n在统一的检索/索引设置和固定的延迟预算下，DyKnow-RAG 在离线测试中优于 SFT、DPO 和标准 GRPO。在淘宝的 A/B 测试中，该方法在 GSB、Query Goodrate 和 Item Goodrate 等指标上均实现了持续的提升。目前，DyKnow-RAG 已部署于淘宝的生产相关性系统中，服务实时流量。据我们所知，这是首批针对电商相关性问题的单次推理 RAG 解决方案之一，能够在不增加在线复杂度的前提下，将噪声外部信号转化为可靠收益。"
    },
    {
        "title": "Zero-Shot CFC: Fast Real-World Image Denoising based on Cross-Frequency\n  Consistency",
        "url": "http://arxiv.org/abs/2510.12646v1",
        "pub_date": "2025-10-14",
        "summary": "Zero-shot denoisers address the dataset dependency of deep-learning-based denoisers, enabling the denoising of unseen single images. Nonetheless, existing zero-shot methods suffer from long training times and rely on the assumption of noise independence and a zero-mean property, limiting their effectiveness in real-world denoising scenarios where noise characteristics are more complicated. This paper proposes an efficient and effective method for real-world denoising, the Zero-Shot denoiser based on Cross-Frequency Consistency (ZSCFC), which enables training and denoising with a single noisy image and does not rely on assumptions about noise distribution. Specifically, image textures exhibit position similarity and content consistency across different frequency bands, while noise does not. Based on this property, we developed cross-frequency consistency loss and an ultralight network to realize image denoising. Experiments on various real-world image datasets demonstrate that our ZSCFC outperforms other state-of-the-art zero-shot methods in terms of computational efficiency and denoising performance.",
        "translated": "零样本去噪器解决了基于深度学习的去噪器对数据集的依赖问题，使其能够对未见过的单张图像进行去噪。然而，现有的零样本方法训练时间较长，并且依赖于噪声独立性和零均值的假设，这在现实世界中噪声特性更为复杂的情况下限制了其去噪效果。本文提出了一种高效且有效的现实场景去噪方法——基于跨频段一致性的零样本去噪器（Zero-Shot Denoiser based on Cross-Frequency Consistency, ZSCFC），该方法仅需单张含噪图像即可完成训练与去噪，且不依赖于对噪声分布的假设。具体而言，图像的纹理在不同频段中表现出位置相似性和内容一致性，而噪声则不具备这一特性。基于该特性，我们设计了跨频段一致性损失函数，并构建了一个超轻量级网络以实现图像去噪。在多个现实世界图像数据集上的实验表明，与其它最先进的零样本方法相比，ZSCFC在计算效率和去噪性能方面均表现出色。"
    },
    {
        "title": "Normalization-equivariant Diffusion Models: Learning Posterior Samplers\n  From Noisy And Partial Measurements",
        "url": "http://arxiv.org/abs/2510.11964v1",
        "pub_date": "2025-10-13",
        "summary": "Diffusion models (DMs) have rapidly emerged as a powerful framework for image generation and restoration. However, existing DMs are primarily trained in a supervised manner by using a large corpus of clean images. This reliance on clean data poses fundamental challenges in many real-world scenarios, where acquiring noise-free data is hard or infeasible, and only noisy and potentially incomplete measurements are available. While some methods can train DMs using noisy data, they are generally effective only when the amount of noise is very mild or when some additional noise-free data is available. In addition, existing methods for training DMs from incomplete measurements require access to multiple complementary acquisition processes, an assumption that poses a significant practical limitation. Here we introduce the first approach for learning DMs for image restoration using only noisy measurement data from a single operator. As a first key contribution, we show that DMs, and more broadly minimum mean squared error denoisers, exhibit a weak form of scale equivariance linking rescaling in signal amplitude to changes in noise intensity. We then leverage this theoretical insight to develop a denoising score-matching strategy that generalizes robustly to noise levels lower than those present in the training data, thereby enabling the learning of DMs from noisy measurements. To further address the challenges of incomplete and noisy data, we integrate our method with equivariant imaging, a complementary self-supervised learning framework that exploits the inherent invariants of imaging problems, to train DMs for image restoration from single-operator measurements that are both incomplete and noisy. We validate the effectiveness of our approach through extensive experiments on image denoising, demosaicing, and inpainting, along with comparisons with the state of the art.",
        "translated": "扩散模型（DMs）已迅速成为图像生成与修复的强大框架。然而，现有的扩散模型主要依赖于大量干净图像的监督训练。这种对干净数据的依赖在许多现实场景中带来了根本性挑战，因为在这些场景中获取无噪声数据困难或不可行，仅有噪声干扰且可能不完整的测量数据可用。尽管已有方法尝试使用噪声数据训练扩散模型，但它们通常仅在噪声非常轻微或存在部分无噪声数据时才有效。此外，目前基于不完整测量数据训练扩散模型的方法通常需要多个互补的采集过程，这一假设在实践中构成了显著的限制。本文首次提出了一种仅使用单一操作算子的噪声测量数据来学习图像修复扩散模型的方法。作为我们的第一个关键贡献，我们证明了扩散模型，以及更广泛的最小均方误差去噪器，表现出一种弱形式的尺度等变性（scale equivariance），将信号幅值的缩放与噪声强度的变化联系起来。我们随后利用这一理论洞见，提出一种去噪得分匹配策略，该策略能够稳健地推广到训练数据中噪声水平更低的情况，从而实现基于噪声测量数据的扩散模型训练。为了进一步应对数据不完整和噪声的挑战，我们将该方法与等变成像（equivariant imaging）相结合，这是一种互补的自监督学习框架，利用了成像问题中的固有不变性，从而实现基于单一操作算子所获取的不完整且噪声干扰数据的图像修复扩散模型训练。我们在图像去噪、色彩插值（demosaicing）和修复（inpainting）任务上进行了广泛的实验验证，并与当前最先进的方法进行了对比。"
    },
    {
        "title": "Enabling High-Quality In-the-Wild Imaging from Severely Aberrated\n  Metalens Bursts",
        "url": "http://arxiv.org/abs/2510.10083v1",
        "pub_date": "2025-10-11",
        "summary": "We tackle the challenge of robust, in-the-wild imaging using ultra-thin nanophotonic metalens cameras. Meta-lenses, composed of planar arrays of nanoscale scatterers, promise dramatic reductions in size and weight compared to conventional refractive optics. However, severe chromatic aberration, pronounced light scattering, narrow spectral bandwidth, and low light efficiency continue to limit their practical adoption. In this work, we present an end-to-end solution for in-the-wild imaging that pairs a metalens several times thinner than conventional optics with a bespoke multi-image restoration framework optimized for practical metalens cameras. Our method centers on a lightweight convolutional network paired with a memory-efficient burst fusion algorithm that adaptively corrects noise, saturation clipping, and lens-induced distortions across rapid sequences of extremely degraded metalens captures. Extensive experiments on diverse, real-world handheld captures demonstrate that our approach consistently outperforms existing burst-mode and single-image restoration techniques.These results point toward a practical route for deploying metalens-based cameras in everyday imaging applications.",
        "translated": "我们针对使用超薄纳米光学金属透镜相机在真实环境下的鲁棒成像问题提出了一个解决方案。金属透镜由平面排列的纳米级散射体构成，相较于传统的折射式光学元件，有望显著减小尺寸和重量。然而，严重的色差、明显的光散射、狭窄的光谱带宽以及低光效率等问题仍限制了其实际应用。在本工作中，我们提出了一种端到端的真实环境下成像方案，该方案结合了一种比传统光学元件薄几倍的金属透镜，以及一种为实际金属透镜相机量身定制、高效的多图像复原框架。我们的方法核心是一个轻量级卷积网络，结合了一种内存高效的图像序列融合算法，能够自适应地校正在快速拍摄的严重退化金属透镜图像序列中出现的噪声、饱和裁剪和透镜引起的失真。我们在多种实际手持拍摄数据上进行了广泛的实验，结果表明，我们的方法在性能上持续优于现有的突发模式和单图像复原技术。这些结果表明，基于金属透镜的相机在日常成像应用中具有实际可行的部署路径。"
    },
    {
        "title": "Denoising Diffusion as a New Framework for Underwater Images",
        "url": "http://arxiv.org/abs/2510.09934v1",
        "pub_date": "2025-10-11",
        "summary": "Underwater images play a crucial role in ocean research and marine environmental monitoring since they provide quality information about the ecosystem. However, the complex and remote nature of the environment results in poor image quality with issues such as low visibility, blurry textures, color distortion, and noise. In recent years, research in image enhancement has proven to be effective but also presents its own limitations, like poor generalization and heavy reliance on clean datasets. One of the challenges herein is the lack of diversity and the low quality of images included in these datasets. Also, most existing datasets consist only of monocular images, a fact that limits the representation of different lighting conditions and angles. In this paper, we propose a new plan of action to overcome these limitations. On one hand, we call for expanding the datasets using a denoising diffusion model to include a variety of image types such as stereo, wide-angled, macro, and close-up images. On the other hand, we recommend enhancing the images using Controlnet to evaluate and increase the quality of the corresponding datasets, and hence improve the study of the marine ecosystem.   Tags - Underwater Images, Denoising Diffusion, Marine ecosystem, Controlnet",
        "translated": "水下图像在海洋研究和海洋环境监测中起着至关重要的作用，因为它们提供了有关生态系统的重要信息。然而，由于环境的复杂性和远离人类的特性，所获取的图像质量通常较差，存在诸如能见度低、纹理模糊、颜色失真和噪声等问题。近年来，图像增强的研究已被证明是有效的，但也存在自身局限性，例如泛化能力差和对干净数据集的依赖性较强。其中一大挑战是这些数据集中图像的多样性不足且质量偏低。此外，大多数现有数据集仅包含单目图像，这一事实限制了不同光照条件和角度的表征能力。本文中，我们提出了一种新的应对方案以克服这些局限性。一方面，我们建议使用去噪扩散模型（denoising diffusion model）扩展数据集，包含诸如双目、广角、微距和特写等多种图像类型；另一方面，我们推荐使用 ControlNet 对图像进行增强，以评估并提升相应数据集的质量，从而促进对海洋生态系统的研究。  \n标签 - 水下图像，去噪扩散，海洋生态系统，ControlNet"
    },
    {
        "title": "Defense against Unauthorized Distillation in Image Restoration via\n  Feature Space Perturbation",
        "url": "http://arxiv.org/abs/2510.08925v1",
        "pub_date": "2025-10-10",
        "summary": "Knowledge distillation (KD) attacks pose a significant threat to deep model intellectual property by enabling adversaries to train student networks using a teacher model's outputs. While recent defenses in image classification have successfully disrupted KD by perturbing output probabilities, extending these methods to image restoration is difficult. Unlike classification, restoration is a generative task with continuous, high-dimensional outputs that depend on spatial coherence and fine details. Minor perturbations are often insufficient, as students can still learn the underlying mapping.To address this, we propose Adaptive Singular Value Perturbation (ASVP), a runtime defense tailored for image restoration models. ASVP operates on internal feature maps of the teacher using singular value decomposition (SVD). It amplifies the topk singular values to inject structured, high-frequency perturbations, disrupting the alignment needed for distillation. This hinders student learning while preserving the teacher's output quality.We evaluate ASVP across five image restoration tasks: super-resolution, low-light enhancement, underwater enhancement, dehazing, and deraining. Experiments show ASVP reduces student PSNR by up to 4 dB and SSIM by 60-75%, with negligible impact on the teacher's performance. Compared to prior methods, ASVP offers a stronger and more consistent defense.Our approach provides a practical solution to protect open-source restoration models from unauthorized knowledge distillation.",
        "translated": "知识蒸馏（Knowledge Distillation, KD）攻击通过使对手能够利用教师模型的输出来训练学生网络，对深度模型的知识产权构成了重大威胁。尽管图像分类领域的最新防御方法已成功通过扰动输出概率来破坏知识蒸馏，但将这些方法扩展到图像恢复任务却面临困难。与分类任务不同，图像恢复是一个生成任务，其输出是连续且高维的，依赖于空间一致性和细节质量。因此，微小的扰动往往不足以阻止知识蒸馏，因为学生模型仍能学习到潜在的映射关系。\n\n为了解决这一问题，我们提出了一种专为图像恢复模型设计的运行时防御方法——自适应奇异值扰动（Adaptive Singular Value Perturbation, ASVP）。ASVP 通过奇异值分解（Singular Value Decomposition, SVD）对教师模型的内部特征图进行操作。该方法通过放大前k个奇异值，注入结构化的高频扰动，从而破坏蒸馏过程中所需的对齐关系。这种扰动在阻碍学生模型学习的同时，保持了教师模型的输出质量。\n\n我们在五个图像恢复任务中评估了 ASVP：超分辨率、低光增强、水下增强、去雾和去雨。实验结果表明，ASVP 最多可使学生模型的 PSNR 降低 4 dB，SSIM 降低 60-75%，而对教师模型的性能影响几乎可以忽略。与现有方法相比，ASVP 提供了更强且更一致的防御效果。我们的方法为保护开源图像恢复模型免受未经授权的知识蒸馏提供了一个实用的解决方案。"
    },
    {
        "title": "Latent Harmony: Synergistic Unified UHD Image Restoration via Latent\n  Space Regularization and Controllable Refinement",
        "url": "http://arxiv.org/abs/2510.07961v2",
        "pub_date": "2025-10-09",
        "summary": "Ultra-High Definition (UHD) image restoration faces a trade-off between computational efficiency and high-frequency detail retention. While Variational Autoencoders (VAEs) improve efficiency via latent-space processing, their Gaussian constraint often discards degradation-specific high-frequency information, hurting reconstruction fidelity. To overcome this, we propose Latent Harmony, a two-stage framework that redefines VAEs for UHD restoration by jointly regularizing the latent space and enforcing high-frequency-aware reconstruction.In Stage One, we introduce LH-VAE, which enhances semantic robustness through visual semantic constraints and progressive degradation perturbations, while latent equivariance strengthens high-frequency reconstruction.Stage Two jointly trains this refined VAE with a restoration model using High-Frequency Low-Rank Adaptation (HF-LoRA): an encoder LoRA guided by a fidelity-oriented high-frequency alignment loss to recover authentic details, and a decoder LoRA driven by a perception-oriented loss to synthesize realistic textures. Both LoRA modules are trained via alternating optimization with selective gradient propagation to preserve the pretrained latent structure.At inference, a tunable parameter {\\alpha} enables flexible fidelity-perception trade-offs.Experiments show Latent Harmony achieves state-of-the-art performance across UHD and standard-resolution tasks, effectively balancing efficiency, perceptual quality, and reconstruction accuracy.",
        "translated": "超高分辨率（UHD）图像修复面临计算效率与高频细节保留之间的权衡。尽管变分自编码器（VAEs）通过潜在空间处理提升了效率，但其高斯约束常常会丢弃与退化相关的高频信息，从而损害重建的保真度。为了解决这一问题，我们提出了**Latent Harmony**，一种两阶段框架，通过联合正则化潜在空间并强制实现高频感知的重建，重新定义了用于UHD图像修复的VAEs。  \n\n在第一阶段中，我们引入了**LH-VAE**，该模型通过视觉语义约束和渐进式退化扰动来增强语义鲁棒性，同时利用潜在等变性来强化高频信息的重建。  \n\n在第二阶段中，我们将优化后的VAE与一个修复模型联合训练，采用**高频低秩适配（High-Frequency Low-Rank Adaptation, HF-LoRA）**方法：一个编码器LoRA通过以保真度为导向的高频对齐损失（high-frequency alignment loss）来恢复真实细节，而一个解码器LoRA则由以感知为导向的损失驱动，以合成逼真的纹理。两个LoRA模块通过交替优化与选择性梯度传播进行训练，从而保持预训练的潜在结构不变。  \n\n在推理阶段，一个可调节的参数 $\\alpha$ 可实现保真度与感知质量之间的灵活权衡。实验表明，**Latent Harmony**在UHD和标准分辨率任务中均达到了最先进的性能，有效平衡了计算效率、感知质量与重建精度。"
    },
    {
        "title": "PhyDAE: Physics-Guided Degradation-Adaptive Experts for All-in-One\n  Remote Sensing Image Restoration",
        "url": "http://arxiv.org/abs/2510.08653v1",
        "pub_date": "2025-10-09",
        "summary": "Remote sensing images inevitably suffer from various degradation factors during acquisition, including atmospheric interference, sensor limitations, and imaging conditions. These complex and heterogeneous degradations pose severe challenges to image quality and downstream interpretation tasks. Addressing limitations of existing all-in-one restoration methods that overly rely on implicit feature representations and lack explicit modeling of degradation physics, this paper proposes Physics-Guided Degradation-Adaptive Experts (PhyDAE). The method employs a two-stage cascaded architecture transforming degradation information from implicit features into explicit decision signals, enabling precise identification and differentiated processing of multiple heterogeneous degradations including haze, noise, blur, and low-light conditions. The model incorporates progressive degradation mining and exploitation mechanisms, where the Residual Manifold Projector (RMP) and Frequency-Aware Degradation Decomposer (FADD) comprehensively analyze degradation characteristics from manifold geometry and frequency perspectives. Physics-aware expert modules and temperature-controlled sparse activation strategies are introduced to enhance computational efficiency while ensuring imaging physics consistency. Extensive experiments on three benchmark datasets (MD-RSID, MD-RRSHID, and MDRS-Landsat) demonstrate that PhyDAE achieves superior performance across all four restoration tasks, comprehensively outperforming state-of-the-art methods. Notably, PhyDAE substantially improves restoration quality while achieving significant reductions in parameter count and computational complexity, resulting in remarkable efficiency gains compared to mainstream approaches and achieving optimal balance between performance and efficiency. Code is available at https://github.com/HIT-SIRS/PhyDAE.",
        "translated": "遥感图像在获取过程中不可避免地受到多种退化因素的影响，包括大气干扰、传感器限制以及成像条件等。这些复杂且异质的退化现象给图像质量以及后续的语义解析任务带来了严重挑战。为了解决现有端到端图像修复方法中对隐式特征表示的过度依赖以及缺乏对退化物理机制的显式建模等问题，本文提出了一种物理引导的退化自适应专家模型（Physics-Guided Degradation-Adaptive Experts, PhyDAE）。该方法采用两阶段级联架构，将隐式特征中的退化信息转化为显式的决策信号，从而实现对多种异质退化（包括雾霾、噪声、模糊和低光照条件）的精确识别与差异化处理。模型引入了渐进式退化挖掘与利用机制，其中残差流形投影器（Residual Manifold Projector, RMP）和频域感知退化解耦模块（Frequency-Aware Degradation Decomposer, FADD）分别从流形几何结构和频域角度对退化特性进行全面分析。此外，通过引入物理感知的专家模块和温度控制的稀疏激活策略，在保证成像物理一致性的同时显著提升了计算效率。在三个基准数据集（MD-RSID、MD-RRSHID 和 MDRS-Landsat）上的大量实验表明，PhyDAE 在四项图像修复任务中均表现出优越的性能，全面超越现有最先进方法。特别值得一提的是，PhyDAE 在大幅提升修复质量的同时，显著减少了模型参数数量与计算复杂度，从而在主流方法中实现了性能与效率之间的最优平衡。代码可在 https://github.com/HIT-SIRS/PhyDAE 获取。"
    },
    {
        "title": "DeRainMamba: A Frequency-Aware State Space Model with Detail Enhancement\n  for Image Deraining",
        "url": "http://arxiv.org/abs/2510.06746v1",
        "pub_date": "2025-10-08",
        "summary": "Image deraining is crucial for improving visual quality and supporting reliable downstream vision tasks. Although Mamba-based models provide efficient sequence modeling, their limited ability to capture fine-grained details and lack of frequency-domain awareness restrict further improvements. To address these issues, we propose DeRainMamba, which integrates a Frequency-Aware State-Space Module (FASSM) and Multi-Directional Perception Convolution (MDPConv). FASSM leverages Fourier transform to distinguish rain streaks from high-frequency image details, balancing rain removal and detail preservation. MDPConv further restores local structures by capturing anisotropic gradient features and efficiently fusing multiple convolution branches. Extensive experiments on four public benchmarks demonstrate that DeRainMamba consistently outperforms state-of-the-art methods in PSNR and SSIM, while requiring fewer parameters and lower computational costs. These results validate the effectiveness of combining frequency-domain modeling and spatial detail enhancement within a state-space framework for single image deraining.",
        "translated": "图像去雨对于提升视觉质量和支持可靠的下游视觉任务至关重要。尽管基于Mamba的模型在序列建模方面具有较高的效率，但其在捕捉细粒度细节方面的能力有限以及缺乏频域感知，限制了进一步的性能提升。为解决这些问题，我们提出DeRainMamba，该方法融合了一个频域感知状态空间模块（Frequency-Aware State-Space Module, FASSM）和多方向感知卷积（Multi-Directional Perception Convolution, MDPConv）。FASSM利用傅里叶变换区分雨痕与图像中的高频细节，从而在去雨与细节保留之间取得平衡。MDPConv通过捕捉各向异性梯度特征并高效融合多分支卷积，进一步恢复局部结构。我们在四个公开基准数据集上进行了广泛的实验，结果表明DeRainMamba在PSNR和SSIM指标上始终优于当前最先进的方法，同时参数量更少，计算成本更低。这些结果验证了在状态空间框架中结合频域建模与空间细节增强对于单图像去雨的有效性。"
    },
    {
        "title": "An Inertial Langevin Algorithm",
        "url": "http://arxiv.org/abs/2510.06723v1",
        "pub_date": "2025-10-08",
        "summary": "We present a novel method for drawing samples from Gibbs distributions with densities of the form $\\pi(x) \\propto \\exp(-U(x))$. The method accelerates the unadjusted Langevin algorithm by introducing an inertia term similar to Polyak's heavy ball method, together with a corresponding noise rescaling. Interpreting the scheme as a discretization of \\emph{kinetic} Langevin dynamics, we prove ergodicity (in continuous and discrete time) for twice continuously differentiable, strongly convex, and $L$-smooth potentials and bound the bias of the discretization to the target in Wasserstein-2 distance. In particular, the presented proofs allow for smaller friction parameters in the kinetic Langevin diffusion compared to existing literature. Moreover, we show the close ties of the proposed method to the over-relaxed Gibbs sampler. The scheme is tested in an extensive set of numerical experiments covering simple toy examples, total variation image denoising, and the complex task of maximum likelihood learning of an energy-based model for molecular structure generation. The experimental results confirm the acceleration provided by the proposed scheme even beyond the strongly convex and $L$-smooth setting.",
        "translated": "我们提出了一种从形式为 $\\pi(x) \\propto \\exp(-U(x))$ 的 Gibbs 分布中抽样的新方法。该方法通过引入类似于 Polyak 重球法的惯性项以及相应的噪声重缩放机制，加速了未调整的 Langevin 算法。将该方法解释为对 \\emph{动能} Langevin 动力学的离散化，我们证明了在连续和离散时间下，对于二次连续可微、强凸且 $L$-光滑的势函数，该方法具有遍历性，并在 Wasserstein-2 距离下对该离散化方案与目标分布之间的偏差进行了上界分析。特别地，所提出的证明允许在动能 Langevin 扩散中使用比现有文献中更小的摩擦参数。此外，我们展示了该方法与过松弛 Gibbs 抽样器之间的紧密联系。该算法在一个广泛的数值实验中进行了测试，涵盖简单的玩具示例、图像的全变分去噪任务，以及分子结构生成的能量基模型的最大似然学习这一复杂任务。实验结果验证了所提出方法即使在非强凸和非 $L$-光滑的设置下仍能提供加速效果。"
    },
    {
        "title": "AIM 2025 Challenge on Real-World RAW Image Denoising",
        "url": "http://arxiv.org/abs/2510.06601v1",
        "pub_date": "2025-10-08",
        "summary": "We introduce the AIM 2025 Real-World RAW Image Denoising Challenge, aiming to advance efficient and effective denoising techniques grounded in data synthesis. The competition is built upon a newly established evaluation benchmark featuring challenging low-light noisy images captured in the wild using five different DSLR cameras. Participants are tasked with developing novel noise synthesis pipelines, network architectures, and training methodologies to achieve high performance across different camera models. Winners are determined based on a combination of performance metrics, including full-reference measures (PSNR, SSIM, LPIPS), and non-reference ones (ARNIQA, TOPIQ). By pushing the boundaries of camera-agnostic low-light RAW image denoising trained on synthetic data, the competition promotes the development of robust and practical models aligned with the rapid progress in digital photography. We expect the competition outcomes to influence multiple domains, from image restoration to night-time autonomous driving.",
        "translated": "我们介绍了AIM 2025真实世界RAW图像去噪挑战赛，旨在通过数据合成推动高效且有效的去噪技术的发展。该竞赛基于一个新的评估基准构建，该基准包含在自然场景下使用五种不同DSLR相机拍摄的具有挑战性的低光照噪声图像。参赛者需要开发新颖的噪声合成流程、网络架构和训练方法，以在不同相机模型上实现高性能的去噪效果。比赛的优胜者将根据多种性能指标综合评定，包括全参考指标（PSNR、SSIM、LPIPS）和非参考指标（ARNIQA、TOPIQ）。通过推动在合成数据上训练的、具有相机泛化能力的低光照RAW图像去噪技术的边界，该竞赛促进了与数字摄影快速进步相契合的鲁棒且实用模型的发展。我们预期本次竞赛的成果将对多个领域产生影响，从图像修复到夜间自动驾驶。"
    },
    {
        "title": "TDiff: Thermal Plug-And-Play Prior with Patch-Based Diffusion",
        "url": "http://arxiv.org/abs/2510.06460v1",
        "pub_date": "2025-10-07",
        "summary": "Thermal images from low-cost cameras often suffer from low resolution, fixed pattern noise, and other localized degradations. Available datasets for thermal imaging are also limited in both size and diversity. To address these challenges, we propose a patch-based diffusion framework (TDiff) that leverages the local nature of these distortions by training on small thermal patches. In this approach, full-resolution images are restored by denoising overlapping patches and blending them using smooth spatial windowing. To our knowledge, this is the first patch-based diffusion framework that models a learned prior for thermal image restoration across multiple tasks. Experiments on denoising, super-resolution, and deblurring demonstrate strong results on both simulated and real thermal data, establishing our method as a unified restoration pipeline.",
        "translated": "低成本热成像相机所获取的图像通常存在分辨率较低、固定模式噪声以及其他局部退化问题。目前可用的热成像数据集在规模和多样性方面也较为有限。为了解决这些挑战，我们提出了一种基于图像块的扩散框架（TDiff），该框架通过在小尺寸热图像块上进行训练，利用这些退化现象的局部特性。在该方法中，通过去噪重叠图像块，并结合平滑的空间窗口函数进行融合，从而恢复全分辨率图像。据我们所知，这是首个基于图像块的扩散框架，能够在多个任务中对热图像的退化建模并学习其先验分布。我们在去噪、超分辨率和去模糊任务上的实验表明，该方法在模拟和真实热图像数据上均取得了优异的效果，从而确立了其作为统一图像恢复流程的地位。"
    },
    {
        "title": "Local MAP Sampling for Diffusion Models",
        "url": "http://arxiv.org/abs/2510.07343v2",
        "pub_date": "2025-10-07",
        "summary": "Diffusion Posterior Sampling (DPS) provides a principled Bayesian approach to inverse problems by sampling from $p(x_0 \\mid y)$. However, in practice, the goal of inverse problem solving is not to cover the posterior but to recover the most accurate reconstruction, where optimization-based diffusion solvers often excel despite lacking a clear probabilistic foundation. We introduce Local MAP Sampling (LMAPS), a new inference framework that iteratively solving local MAP subproblems along the diffusion trajectory. This perspective clarifies their connection to global MAP estimation and DPS, offering a unified probabilistic interpretation for optimization-based methods. Building on this foundation, we develop practical algorithms with a probabilistically interpretable covariance approximation, a reformulated objective for stability and interpretability, and a gradient approximation for non-differentiable operators. Across a broad set of image restoration and scientific tasks, LMAPS achieves state-of-the-art performance, including $\\geq 2$ dB gains on motion deblurring, JPEG restoration, and quantization, and $&gt;1.5$ dB improvements on inverse scattering benchmarks.",
        "translated": "扩散后验采样（Diffusion Posterior Sampling, DPS）通过从 $p(x_0 \\mid y)$ 中采样，为逆问题提供了一种基于贝叶斯原理的求解方法。然而，实际上，解决逆问题的目标并非是覆盖后验分布，而是恢复最精确的重建结果，在这一点上，基于优化的扩散求解器往往表现出色，尽管它们缺乏明确的概率基础。我们提出一种新的推理框架——局部最大后验采样（Local MAP Sampling, LMAPS），该方法沿着扩散轨迹迭代求解局部最大后验（MAP）子问题。这一视角明确了LMAPS与全局MAP估计以及DPS之间的关系，为基于优化的方法提供了一种统一的概率解释。基于这一理论基础，我们开发了具有概率解释的协方差近似方法，提出了一个用于提高稳定性和可解释性的重构目标函数，并引入了针对不可微操作符的梯度近似方法。在广泛的图像恢复和科学计算任务中，LMAPS实现了最先进的性能，包括在运动去模糊、JPEG图像恢复和量化任务中获得了 $\\geq 2$ dB 的提升，在逆散射基准测试中实现了 $>1.5$ dB 的性能改进。"
    },
    {
        "title": "Rasterized Steered Mixture of Experts for Efficient 2D Image Regression",
        "url": "http://arxiv.org/abs/2510.05814v1",
        "pub_date": "2025-10-07",
        "summary": "The Steered Mixture of Experts regression framework has demonstrated strong performance in image reconstruction, compression, denoising, and super-resolution. However, its high computational cost limits practical applications. This work introduces a rasterization-based optimization strategy that combines the efficiency of rasterized Gaussian kernel rendering with the edge-aware gating mechanism of the Steered Mixture of Experts. The proposed method is designed to accelerate two-dimensional image regression while maintaining the model's inherent sparsity and reconstruction quality. By replacing global iterative optimization with a rasterized formulation, the method achieves significantly faster parameter updates and more memory-efficient model representations. In addition, the proposed framework supports applications such as native super-resolution and image denoising, which are not directly achievable with standard rasterized Gaussian kernel approaches. The combination of fast rasterized optimization with the edge-aware structure of the Steered Mixture of Experts provides a new balance between computational efficiency and reconstruction fidelity for two-dimensional image processing tasks.",
        "translated": "基于引导的专家混合（Steered Mixture of Experts）回归框架在图像重建、压缩、去噪和超分辨率等任务中已展现出优异的性能。然而，其较高的计算成本限制了其在实际应用中的使用。本文提出了一种基于光栅化的优化策略，结合了光栅化高斯核渲染的高效性与Steered Mixture of Experts中边缘感知门控机制的优势。所提出的方法旨在加速二维图像回归过程，同时保持模型固有的稀疏性与重建质量。通过将全局迭代优化替换为光栅化形式，该方法显著提高了参数更新速度，并实现了更节省内存的模型表示。此外，该框架支持诸如原生超分辨率和图像去噪等应用，而这些是标准光栅化高斯核方法无法直接实现的。快速光栅化优化与Steered Mixture of Experts边缘感知结构的结合，为二维图像处理任务提供了计算效率与重建保真度之间的新平衡。"
    },
    {
        "title": "Adaptive double-phase Rudin--Osher--Fatemi denoising model",
        "url": "http://arxiv.org/abs/2510.04382v1",
        "pub_date": "2025-10-05",
        "summary": "We propose a new image denoising model based on a variable-growth total variation regularization of double-phase type with adaptive weight. It is designed to reduce staircasing with respect to the classical Rudin--Osher--Fatemi model, while preserving the edges of the image in a similar fashion. We implement the model and test its performance on synthetic and natural images in 1D and 2D over a range of noise levels.",
        "translated": "我们提出了一种新的图像去噪模型，该模型基于具有自适应权重的双阶段可变增长总体变差正则化。该模型旨在相较于经典的 Rudin–Osher–Fatemi 模型，减少阶梯效应（staircasing），同时以类似的方式保留图像的边缘。我们对该模型进行了实现，并在 1D 和 2D 的合成图像与自然图像上，针对多种噪声水平进行了性能测试。"
    },
    {
        "title": "Equilibrium Matching: Generative Modeling with Implicit Energy-Based\n  Models",
        "url": "http://arxiv.org/abs/2510.02300v3",
        "pub_date": "2025-10-02",
        "summary": "We introduce Equilibrium Matching (EqM), a generative modeling framework built from an equilibrium dynamics perspective. EqM discards the non-equilibrium, time-conditional dynamics in traditional diffusion and flow-based generative models and instead learns the equilibrium gradient of an implicit energy landscape. Through this approach, we can adopt an optimization-based sampling process at inference time, where samples are obtained by gradient descent on the learned landscape with adjustable step sizes, adaptive optimizers, and adaptive compute. EqM surpasses the generation performance of diffusion/flow models empirically, achieving an FID of 1.90 on ImageNet 256$\\times$256. EqM is also theoretically justified to learn and sample from the data manifold. Beyond generation, EqM is a flexible framework that naturally handles tasks including partially noised image denoising, OOD detection, and image composition. By replacing time-conditional velocities with a unified equilibrium landscape, EqM offers a tighter bridge between flow and energy-based models and a simple route to optimization-driven inference.",
        "translated": "我们提出了一种名为**均衡匹配**（Equilibrium Matching, EqM）的生成建模框架，其构建基于**均衡动力学**的视角。EqM摒弃了传统扩散模型和基于流的生成模型中所依赖的非均衡、时序条件的动力学，转而学习一个隐式能量景观的**均衡梯度**。通过这种方法，我们可以在推理阶段采用基于优化的采样过程，其中样本通过在所学能量景观上进行梯度下降获得，该过程支持可调节的步长、自适应优化器以及自适应计算能力。从实证结果来看，EqM在生成性能上超越了扩散模型和流模型，在ImageNet 256$\\times$256数据集上达到了1.90的FID分数。EqM在理论上也能够从数据流形中进行学习与采样，具有坚实的理论依据。除生成任务外，EqM还是一种灵活的框架，天然地适用于包括部分噪声图像去噪、分布外（OOD）检测以及图像合成在内的多种任务。通过将时序条件的速度替换为统一的均衡景观，EqM在流模型与基于能量的模型之间架起了更紧密的桥梁，并为实现优化驱动的推理提供了一条简洁的路径。"
    },
    {
        "title": "HyMiRec: A Hybrid Multi-interest Learning Framework for LLM-based\n  Sequential Recommendation",
        "url": "http://arxiv.org/abs/2510.13738v1",
        "pub_date": "2025-10-15",
        "summary": "Large language models (LLMs) have recently demonstrated strong potential for sequential recommendation. However, current LLM-based approaches face critical limitations in modeling users' long-term and diverse interests. First, due to inference latency and feature fetching bandwidth constraints, existing methods typically truncate user behavior sequences to include only the most recent interactions, resulting in the loss of valuable long-range preference signals. Second, most current methods rely on next-item prediction with a single predicted embedding, overlooking the multifaceted nature of user interests and limiting recommendation diversity. To address these challenges, we propose HyMiRec, a hybrid multi-interest sequential recommendation framework, which leverages a lightweight recommender to extracts coarse interest embeddings from long user sequences and an LLM-based recommender to captures refined interest embeddings. To alleviate the overhead of fetching features, we introduce a residual codebook based on cosine similarity, enabling efficient compression and reuse of user history embeddings. To model the diverse preferences of users, we design a disentangled multi-interest learning module, which leverages multiple interest queries to learn disentangles multiple interest signals adaptively, allowing the model to capture different facets of user intent. Extensive experiments are conducted on both benchmark datasets and a collected industrial dataset, demonstrating our effectiveness over existing state-of-the-art methods. Furthermore, online A/B testing shows that HyMiRec brings consistent improvements in real-world recommendation systems.",
        "translated": "大语言模型（LLMs）最近在序列推荐中展现出强大的潜力。然而，目前基于LLM的方法在建模用户长期和多样化兴趣方面面临关键的限制。首先，由于推理延迟和特征获取带宽的限制，现有方法通常截断用户行为序列，仅包含最近的交互，从而丢失了有价值的长距离偏好信号。其次，大多数当前方法依赖于单一预测嵌入的下一物品预测，忽视了用户兴趣的多面性，限制了推荐的多样性。为了解决这些挑战，我们提出HyMiRec，一种混合多兴趣序列推荐框架，其利用一个轻量级推荐器从长用户序列中提取粗兴趣嵌入，并利用一个基于LLM的推荐器捕捉精兴趣嵌入。为了缓解特征获取带来的开销，我们引入了一个基于余弦相似度的残差码本，从而实现了用户历史嵌入的高效压缩和重复使用。为了建模用户的多样化偏好，我们设计了一个解耦的多兴趣学习模块，该模块利用多个兴趣查询自适应地学习解耦的多个兴趣信号，使得模型能够捕捉用户意图的不同方面。我们在基准数据集和收集的工业数据集上进行了广泛的实验，验证了该方法相较于现有最先进方法的有效性。此外，在线A/B测试表明，HyMiRec在实际推荐系统中带来了持续的性能提升。",
        "translated_title": "HyMiRec：一种基于大语言模型的序列推荐混合多兴趣学习框架",
        "label": [
            "LLM生成式推荐",
            "序列推荐",
            "通用推荐技术"
        ],
        "label_reason": "结合LLM与多兴趣学习的序列推荐框架，直接解决推荐系统核心问题",
        "relevance_score": 9
    },
    {
        "title": "RAG Meets Temporal Graphs: Time-Sensitive Modeling and Retrieval for\n  Evolving Knowledge",
        "url": "http://arxiv.org/abs/2510.13590v1",
        "pub_date": "2025-10-15",
        "summary": "Knowledge is inherently time-sensitive and continuously evolves over time. Although current Retrieval-Augmented Generation (RAG) systems enrich LLMs with external knowledge, they largely ignore this temporal nature. This raises two challenges for RAG. First, current RAG methods lack effective time-aware representations. Same facts of different time are difficult to distinguish with vector embeddings or conventional knowledge graphs. Second, most RAG evaluations assume a static corpus, leaving a blind spot regarding update costs and retrieval stability as knowledge evolves. To make RAG time-aware, we propose Temporal GraphRAG (TG-RAG), which models external corpora as a bi-level temporal graph consisting of a temporal knowledge graph with timestamped relations and a hierarchical time graph. Multi-granularity temporal summaries are generated for each time node to capture both key events and broader trends at that time. The design supports incremental updates by extracting new temporal facts from the incoming corpus and merging them into the existing graph. The temporal graph explicitly represents identical facts at different times as distinct edges to avoid ambiguity, and the time hierarchy graph allows only generating reports for new leaf time nodes and their ancestors, ensuring effective and efficient updates. During inference, TG-RAG dynamically retrieves a subgraph within the temporal and semantic scope of the query, enabling precise evidence gathering. Moreover, we introduce ECT-QA, a time-sensitive question-answering dataset featuring both specific and abstract queries, along with a comprehensive evaluation protocol designed to assess incremental update capabilities of RAG systems. Extensive experiments show that TG-RAG significantly outperforms existing baselines, demonstrating the effectiveness of our method in handling temporal knowledge and incremental updates.",
        "translated": "知识本质上是时间敏感的，并且会随着时间不断演化。尽管当前的检索增强生成（RAG）系统通过引入外部知识丰富了大语言模型（LLM），但它们在很大程度上忽视了这种时间特性。这引发了RAG面临的两个挑战。首先，当前的RAG方法缺乏有效的时间感知表示。不同时间下的相同事实，难以通过向量嵌入（embedding）或传统知识图谱进行区分。其次，大多数RAG评估假设语料库是静态的，从而忽略了知识演化过程中的更新成本和检索稳定性问题。为了使RAG具备时间感知能力，我们提出了时序图RAG（Temporal GraphRAG，TG-RAG），该方法将外部语料库建模为一个双层时序图，包含一个具有时间戳关系的时序知识图谱和一个层次化时间图。为每个时间节点生成多粒度时序摘要，以捕捉该时间点的关键事件和更广泛的趋势。该设计通过从新输入的语料中提取新的时序事实并将其合并到现有图中，支持增量更新。时序图显式地将不同时刻的相同事实表示为不同的边，以避免歧义，而时间层次图则仅允许为新的叶子时间节点及其祖先生成报告，从而确保更新的有效性和高效性。在推理阶段，TG-RAG能够动态检索与查询在时间和语义范围内匹配的子图，实现精确的证据收集。此外，我们引入了一个时间敏感的问答数据集ECT-QA，包含具体和抽象的查询，并设计了一套全面的评估协议，用于评估RAG系统的增量更新能力。大量实验表明，TG-RAG显著优于现有基线，验证了我们方法在处理时序知识和增量更新方面的有效性。",
        "translated_title": "RAG与时间图的结合：面向动态知识的时间敏感建模与召回",
        "label": [
            "LLM生成式推荐",
            "多模态推荐",
            "推荐系统评估"
        ],
        "label_reason": "论文提出时间敏感的RAG方法，适用于生成式推荐中的动态知识建模与评估。",
        "relevance_score": 7
    },
    {
        "title": "MADREC: A Multi-Aspect Driven LLM Agent for Explainable and Adaptive\n  Recommendation",
        "url": "http://arxiv.org/abs/2510.13371v1",
        "pub_date": "2025-10-15",
        "summary": "Recent attempts to integrate large language models (LLMs) into recommender systems have gained momentum, but most remain limited to simple text generation or static prompt-based inference, failing to capture the complexity of user preferences and real-world interactions. This study proposes the Multi-Aspect Driven LLM Agent MADRec, an autonomous LLM-based recommender that constructs user and item profiles by unsupervised extraction of multi-aspect information from reviews and performs direct recommendation, sequential recommendation, and explanation generation. MADRec generates structured profiles via aspect-category-based summarization and applies Re-Ranking to construct high-density inputs. When the ground-truth item is missing from the output, the Self-Feedback mechanism dynamically adjusts the inference criteria. Experiments across multiple domains show that MADRec outperforms traditional and LLM-based baselines in both precision and explainability, with human evaluation further confirming the persuasiveness of the generated explanations.",
        "translated": "近期尝试将大语言模型（LLMs）集成到推荐系统中的工作逐渐增多，但大多数仍局限于简单的文本生成或基于静态提示的推理，未能捕捉用户偏好和现实世界交互的复杂性。本研究提出多方面驱动的大语言模型代理 MADRec，这是一种基于大语言模型的自主推荐系统，通过从评论中无监督提取多方面信息构建用户和物料画像，并执行直接推荐、序列推荐以及解释生成。MADRec 通过基于方面类别的摘要生成结构化画像，并应用重排（Re-Ranking）构造高密度输入。当输出中缺少真实物料时，自反馈（Self-Feedback）机制会动态调整推理标准。跨多个领域的实验表明，MADRec 在准确性和可解释性方面均优于传统方法和基于 LLM 的基线模型，人工评估进一步验证了所生成解释的说服力。",
        "translated_title": "MADREC：一种面向多方面驱动的可解释且自适应的推荐大语言模型代理",
        "label": [
            "LLM生成式推荐",
            "精排",
            "重排",
            "推荐系统可解释性"
        ],
        "label_reason": "基于LLM的推荐与可解释性，融合重排与反馈机制",
        "relevance_score": 9
    },
    {
        "title": "Improving Visual Recommendation on E-commerce Platforms Using\n  Vision-Language Models",
        "url": "http://arxiv.org/abs/2510.13359v1",
        "pub_date": "2025-10-15",
        "summary": "On large-scale e-commerce platforms with tens of millions of active monthly users, recommending visually similar products is essential for enabling users to efficiently discover items that align with their preferences. This study presents the application of a vision-language model (VLM) -- which has demonstrated strong performance in image recognition and image-text retrieval tasks -- to product recommendations on Mercari, a major consumer-to-consumer marketplace used by more than 20 million monthly users in Japan. Specifically, we fine-tuned SigLIP, a VLM employing a sigmoid-based contrastive loss, using one million product image-title pairs from Mercari collected over a three-month period, and developed an image encoder for generating item embeddings used in the recommendation system. Our evaluation comprised an offline analysis of historical interaction logs and an online A/B test in a production environment. In offline analysis, the model achieved a 9.1% improvement in nDCG@5 compared with the baseline. In the online A/B test, the click-through rate improved by 50% whereas the conversion rate improved by 14% compared with the existing model. These results demonstrate the effectiveness of VLM-based encoders for e-commerce product recommendations and provide practical insights into the development of visual similarity-based recommendation systems.",
        "translated": "在拥有数千万活跃月活用户的大型电商平台中，推荐视觉相似产品对于帮助用户高效发现与其偏好一致的物料至关重要。本研究提出了将视觉-语言模型（VLM）应用于Mercari平台的产品推荐，其中VLM已在图像识别和图文检索任务中展现出卓越的性能。Mercari是日本一个重要的C2C电商平台，月活跃用户超过2000万。具体而言，我们使用在三个月内从Mercari收集的包含一百万对产品图像和标题的数据，对采用基于Sigmoid的对比损失的VLM SigLIP进行了微调，并开发了用于生成推荐系统中物料嵌入表示的图像编码器。我们的评估包括对历史交互日志的离线分析以及在生产环境中进行的在线A/B测试。离线分析结果显示，与基线模型相比，该模型在nDCG@5指标上提升了9.1%。在线A/B测试中，点击率提升了50%，转化率提升了14%。这些结果证明了基于VLM编码器在电商产品推荐中的有效性，并为视觉相似性推荐系统的发展提供了实践洞见。",
        "translated_title": "使用视觉-语言模型改进电子商务平台的视觉推荐",
        "label": [
            "多模态推荐",
            "精排",
            "图像相似性推荐"
        ],
        "label_reason": "论文将视觉语言模型用于电商推荐，提升图像相似性产品推荐效果",
        "relevance_score": 8
    },
    {
        "title": "ChatR1: Reinforcement Learning for Conversational Reasoning and\n  Retrieval Augmented Question Answering",
        "url": "http://arxiv.org/abs/2510.13312v1",
        "pub_date": "2025-10-15",
        "summary": "We present ChatR1, a reasoning framework based on reinforcement learning (RL) for conversational question answering (CQA). Reasoning plays an important role in CQA, where user intent evolves across dialogue turns, and utterances are often underspecified, requiring contextual interpretation, query reformulation, and dynamic coordination between retrieval and generation. Unlike static `rewrite, retrieve, and generate' pipelines, ChatR1 interleaves search and reasoning across turns, enabling exploratory and adaptive behaviors learned through RL. To address the challenge of sparse and delayed rewards in RL, we propose an intent-aware reward that provides turn-level feedback by aligning retrieval and reasoning with evolving user goals. Our proposed ChatR1 demonstrates strong performance on both 3B and 7B model backbones, outperforming competitive models on five CQA datasets, measured by different metrics (F1, BERTScore, and LLM-as-judge). We include a diverse set of CQA datasets to cover topic shifts, evolving intents, mixed-initiative dialogues, and multi-document grounding, testing ChatR1's performance from various aspects. Ablation studies confirm the effectiveness of the intent-aware reward. Our analyses further reveal diverse reasoning trajectories and effective use of the search tool. ChatR1 also generalizes robustly across domains, demonstrating that RL-based reasoning enables more flexible and context-sensitive behavior than static CQA pipelines.",
        "translated": "我们提出了 ChatR1，一种基于强化学习（RL）的对话式问答（CQA）推理框架。推理在 CQA 中起着重要作用，因为用户意图会随着对话轮次而演变，且对话内容通常信息不完整，需要上下文解释、查询重构，以及检索与生成之间的动态协调。与静态的“重写、检索、生成”流水线不同，ChatR1 在对话轮次中交替进行搜索和推理，使得通过 RL 学到的探索性和适应性行为得以实现。为了解决 RL 中稀疏和延迟奖励的挑战，我们提出了一种意图感知的奖励机制，通过将检索和推理与用户意图的演变对齐，提供轮次级的反馈。我们提出的 ChatR1 在 3B 和 7B 模型主干上均表现出色，在五个 CQA 数据集上的表现优于多个竞争模型，评估指标包括 F1、BERTScore 和以大语言模型作为评判者。我们纳入了多样化的 CQA 数据集，涵盖主题转换、意图演变、混合倡议对话以及多文档依据，从多个方面测试了 ChatR1 的性能。消融实验验证了意图感知奖励的有效性。进一步的分析还揭示了多样化的推理轨迹和对搜索工具的有效利用。ChatR1 在多个领域上也表现出良好的泛化能力，表明基于 RL 的推理能够实现比静态 CQA 流水线更为灵活和上下文敏感的行为。",
        "translated_title": "ChatR1：会话推理与检索增强问答的强化学习方法",
        "label": [
            "LLM生成式推荐",
            "序列推荐"
        ],
        "label_reason": "论文涉及对话式问答与生成式模型，适用于推荐中的序列建模和生成式推荐。",
        "relevance_score": 7
    },
    {
        "title": "Beyond Static LLM Policies: Imitation-Enhanced Reinforcement Learning\n  for Recommendation",
        "url": "http://arxiv.org/abs/2510.13229v1",
        "pub_date": "2025-10-15",
        "summary": "Recommender systems (RecSys) have become critical tools for enhancing user engagement by delivering personalized content across diverse digital platforms. Recent advancements in large language models (LLMs) demonstrate significant potential for improving RecSys, primarily due to their exceptional generalization capabilities and sophisticated contextual understanding, which facilitate the generation of flexible and interpretable recommendations. However, the direct deployment of LLMs as primary recommendation policies presents notable challenges, including persistent latency issues stemming from frequent API calls and inherent model limitations such as hallucinations and biases. To address these issues, this paper proposes a novel offline reinforcement learning (RL) framework that leverages imitation learning from LLM-generated trajectories. Specifically, inverse reinforcement learning is employed to extract robust reward models from LLM demonstrations. This approach negates the need for LLM fine-tuning, thereby substantially reducing computational overhead. Simultaneously, the RL policy is guided by the cumulative rewards derived from these demonstrations, effectively transferring the semantic insights captured by the LLM. Comprehensive experiments conducted on two benchmark datasets validate the effectiveness of the proposed method, demonstrating superior performance when compared against state-of-the-art RL-based and in-context learning baselines. The code can be found at https://github.com/ArronDZhang/IL-Rec.",
        "translated": "推荐系统（Recommender systems, RecSys）已成为在各种数字平台上提供个性化内容以提升用户参与度的关键工具。近年来，大语言模型（Large language models, LLMs）在推荐系统中的应用展现出显著潜力，主要归功于其出色的泛化能力和复杂上下文理解能力，这些能力有助于生成灵活且可解释的推荐结果。然而，直接将LLMs作为主要的推荐策略部署存在诸多挑战，包括由于频繁调用API而导致的持续性延迟问题，以及模型本身固有的局限性，如幻觉和偏见等。为了解决这些问题，本文提出了一种新颖的离线强化学习（Reinforcement learning, RL）框架，该框架通过模仿学习LLM生成的轨迹来实现。具体而言，采用逆强化学习（inverse reinforcement learning）方法从LLM的演示中提取稳健的奖励模型。这种方法避免了对LLM进行微调的需求，从而显著降低了计算开销。同时，强化学习策略通过这些演示所获得的累积奖励进行指导，有效地将LLM捕捉到的语义信息迁移过来。在两个基准数据集上进行的全面实验验证了所提方法的有效性，其性能优于最先进的基于RL的和上下文学习的基线方法。代码可在 https://github.com/ArronDZhang/IL-Rec 找到。",
        "translated_title": "超越静态大语言模型策略：基于模仿增强的推荐系统强化学习方法",
        "label": [
            "LLM生成式推荐",
            "精排",
            "通用推荐技术"
        ],
        "label_reason": "结合LLM与强化学习优化推荐策略，涉及生成式推荐和策略优化",
        "relevance_score": 9
    },
    {
        "title": "LLM-guided Hierarchical Retrieval",
        "url": "http://arxiv.org/abs/2510.13217v1",
        "pub_date": "2025-10-15",
        "summary": "Modern IR systems are increasingly tasked with answering complex, multi-faceted queries that require deep reasoning rather than simple keyword or semantic matching. While LLM-based IR has shown great promise, the prevailing retrieve-then-rerank paradigm inherits the limitations of embedding-based retrieval; parametric generative approaches are difficult to update with new information; and long-context methods that place the entire corpus in context are computationally infeasible for large document collections. To address these challenges, we introduce LATTICE, a hierarchical retrieval framework that enables an LLM to reason over and navigate large corpora with logarithmic search complexity by imposing a semantic tree structure on the corpus. Our approach consists of two stages: (1) an offline phase that organizes the corpus into a semantic hierarchy via either a bottom-up agglomerative strategy or a top-down divisive strategy using multi-level summaries and (2) an online traversal phase where a search LLM navigates this tree. A central challenge in such LLM-guided search is that the model's relevance judgments are noisy, context-dependent, and unaware of the hierarchy, making cross-branch and cross-level comparisons difficult. To overcome this, we propose a traversal algorithm that estimates calibrated latent relevance scores from local LLM outputs and aggregates them into a global path relevance metric. Our training-free framework achieves state-of-the-art zero-shot performance on the reasoning-intensive BRIGHT benchmark, demonstrating up to 9% improvement in Recall@100 and 5% in nDCG@10 over the next best zero-shot baseline. Furthermore, compared to the fine-tuned SOTA method DIVER-v2, LATTICE attains comparable results on BRIGHT subsets that use a static corpus for evaluation.",
        "translated": "现代推荐系统日益需要处理复杂的、多方面的查询，这些查询需要深入推理，而不仅仅是简单的关键词或语义匹配。尽管基于大语言模型（LLM）的推荐系统展现出了巨大潜力，但主流的先召回后重排范式继承了基于嵌入的召回方法的局限性；参数化的生成式方法难以通过新信息进行更新；而将整个语料库放入上下文中的长上下文方法在面对大规模文档集合时计算上不可行。为了解决这些挑战，我们引入了 LATTICE，一个分层召回框架，通过在语料库上施加语义树结构，使 LLM 能够以对数级搜索复杂度对大规模语料库进行推理和导航。我们的方法包含两个阶段：（1）一个离线阶段，通过自底向上的聚合策略或自顶向下的划分策略，利用多层级摘要将语料库组织成语义层次结构；（2）一个在线遍历阶段，其中搜索 LLM 遍历该树结构。在这种由 LLM 指导的搜索中，一个核心挑战是模型的相关性判断是嘈杂的、上下文依赖的，并且不了解层次结构，从而使得跨分支和跨层级的比较变得困难。为了解决这一问题，我们提出了一种遍历算法，该算法从局部 LLM 输出中估计校准后的隐相关性得分，并将其聚合为一个全局路径相关性指标。我们的无训练框架在推理密集型的 BRIGHT 基准上实现了最先进的零样本性能，其 Recall@100 和 nDCG@10 指标分别比次优的零样本基线提升了最高 9% 和 5%。此外，与微调后的最先进方法 DIVER-v2 相比，LATTICE 在使用静态语料库进行评估的 BRIGHT 子集上达到了可比的结果。",
        "translated_title": "LLM引导的层次化召回",
        "label": [
            "召回（Recall）",
            "多模态推荐（Multimodal Recommendation）",
            "LLM生成式推荐（LLM-based Generative Recommendation）"
        ],
        "label_reason": "论文提出基于LLM的分层召回框架，适用于信息检索并间接可用于推荐",
        "relevance_score": 7
    },
    {
        "title": "ReMindRAG: Low-Cost LLM-Guided Knowledge Graph Traversal for Efficient\n  RAG",
        "url": "http://arxiv.org/abs/2510.13193v1",
        "pub_date": "2025-10-15",
        "summary": "Knowledge graphs (KGs), with their structured representation capabilities, offer promising avenue for enhancing Retrieval Augmented Generation (RAG) systems, leading to the development of KG-RAG systems. Nevertheless, existing methods often struggle to achieve effective synergy between system effectiveness and cost efficiency, leading to neither unsatisfying performance nor excessive LLM prompt tokens and inference time. To this end, this paper proposes REMINDRAG, which employs an LLM-guided graph traversal featuring node exploration, node exploitation, and, most notably, memory replay, to improve both system effectiveness and cost efficiency. Specifically, REMINDRAG memorizes traversal experience within KG edge embeddings, mirroring the way LLMs \"memorize\" world knowledge within their parameters, but in a train-free manner. We theoretically and experimentally confirm the effectiveness of REMINDRAG, demonstrating its superiority over existing baselines across various benchmark datasets and LLM backbones. Our code is available at https://github.com/kilgrims/ReMindRAG.",
        "translated": "知识图谱（KGs）凭借其结构化表示能力，为增强检索增强生成（RAG）系统提供了有前景的路径，从而推动了KG-RAG系统的出现。然而，现有方法通常难以在系统效果和成本效率之间实现有效的协同，导致性能不佳或大语言模型（LLM）提示词数量和推理时间过多。为此，本文提出REMINDRAG，其采用了一种由LLM引导的图遍历方法，包含节点探索、节点利用，以及最重要的是记忆回放，从而同时提升系统的有效性和成本效率。具体而言，REMINDRAG在KG的边嵌入中记忆遍历经验，其方式类似于LLMs在其参数中“记忆”世界知识，但无需训练。我们从理论和实验两个方面验证了REMINDRAG的有效性，结果表明其在多种基准数据集和LLM主干模型上均优于现有基线方法。我们的代码可在 https://github.com/kilgrims/ReMindRAG 获取。",
        "translated_title": "ReMindRAG：高效RAG的低成本大语言模型引导的知识图谱遍历方法",
        "label": [
            "LLM生成式推荐",
            "多模态推荐"
        ],
        "label_reason": "论文涉及LLM引导的知识图谱遍历，适用于生成式推荐中的信息检索优化。",
        "relevance_score": 7
    },
    {
        "title": "Retrieval-in-the-Chain: Bootstrapping Large Language Models for\n  Generative Retrieval",
        "url": "http://arxiv.org/abs/2510.13095v1",
        "pub_date": "2025-10-15",
        "summary": "Generative retrieval (GR) is an emerging paradigm that leverages large language models (LLMs) to autoregressively generate document identifiers (docids) relevant to a given query. Prior works have focused on leveraging the generative capabilities of LLMs to improve GR, while overlooking that their reasoning capabilities could likewise help. This raises a key question: Can explicit reasoning benefit GR? To investigate, we first conduct a preliminary study where an LLM is prompted to generate free-form chain-of-thought (CoT) reasoning before performing constrained docid decoding. Although this method outperforms standard GR, the generated reasoning tends to be verbose and poorly aligned with the docid space. These limitations motivate the development of a reasoning mechanism better tailored to GR.   Therefore, we propose Reason-for-Retrieval (R4R), a reasoning-augmented framework for GR that converts free-form CoT reasoning into a compact, structured format, and iteratively refines the reasoning during the retrieval process. R4R augments an existing GR method by leveraging a reasoning-capable LLM that has been instruction-tuned for GR. At inference time, R4R first uses the LLM to generate an initial structured reasoning; then the same LLM alternates between (i) constrained decoding with the chosen GR method to produce candidate docids and (ii) updating the reasoning based on retrieval results to improve the next round. R4R does not require additional models or training, and instead a single LLM serves as both the reasoning generator and the retriever. Extensive experiments on Natural Questions, MS MARCO, and a real-world item-search benchmark validate the effectiveness of R4R.",
        "translated": "生成式召回（GR）是一种新兴范式，它利用大语言模型（LLM）对给定查询进行自回归地生成相关文档标识符（docids）。此前的研究主要关注于利用LLM的生成能力来提升GR，而忽略了其推理能力同样可以提供帮助。这引发了一个关键问题：显式的推理是否能提升GR？为了探究这一问题，我们首先进行了一项初步研究，其中LLM被提示在进行受限docids解码之前生成自由形式的思维链（CoT）推理。尽管这种方法优于标准的GR方法，但生成的推理内容往往冗长，并且与docid空间的对齐效果较差。这些限制促使我们开发一种更加契合GR的推理机制。因此，我们提出了Reason-for-Retrieval（R4R），这是一个增强推理能力的GR框架，它将自由形式的CoT推理转换为一种紧凑的结构化格式，并在召回过程中迭代地优化该推理。R4R通过使用为GR指令调优的具有推理能力的LLM，来增强现有的GR方法。在推理阶段，R4R首先使用LLM生成初始的结构化推理；然后，相同的LLM交替执行以下两个步骤：（i）利用选定的GR方法进行受限解码以生成候选docids，以及（ii）根据召回结果更新推理内容以优化下一轮生成。R4R不需要额外的模型或训练，而是通过单一LLM同时担任推理生成器和召回器的角色。在Natural Questions、MS MARCO和一个实际的物料-搜索基准数据集上的大量实验验证了R4R的有效性。",
        "translated_title": "链中召回：通过引导大语言模型实现生成式召回",
        "label": [
            "LLM生成式推荐",
            "召回"
        ],
        "label_reason": "论文提出基于LLM的生成式检索框架，与推荐系统召回环节密切相关。",
        "relevance_score": 8
    },
    {
        "title": "Post-hoc Popularity Bias Correction in GNN-based Collaborative Filtering",
        "url": "http://arxiv.org/abs/2510.12959v1",
        "pub_date": "2025-10-14",
        "summary": "User historical interaction data is the primary signal for learning user preferences in collaborative filtering (CF). However, the training data often exhibits a long-tailed distribution, where only a few items have the majority of interactions. CF models trained directly on such imbalanced data are prone to learning popularity bias, which reduces personalization and leads to suboptimal recommendation quality. Graph Neural Networks (GNNs), while effective for CF due to their message passing mechanism, can further propagate and amplify popularity bias through their aggregation process. Existing approaches typically address popularity bias by modifying training objectives but fail to directly counteract the bias propagated during GNN's neighborhood aggregation. Applying weights to interactions during aggregation can help alleviate this problem, yet it risks distorting model learning due to unstable node representations in the early stages of training. In this paper, we propose a Post-hoc Popularity Debiasing (PPD) method that corrects for popularity bias in GNN-based CF and operates directly on pre-trained embeddings without requiring retraining. By estimating interaction-level popularity and removing popularity components from node representations via a popularity direction vector, PPD reduces bias while preserving user preferences. Experimental results show that our method outperforms state-of-the-art approaches for popularity bias correction in GNN-based CF.",
        "translated": "用户的历史交互数据是协同过滤（CF）中学习用户/物料偏好的主要信号。然而，训练数据通常表现出长尾分布，其中只有少数物料获得了大部分的交互。直接在这样的不平衡数据上训练的CF模型容易学习到流行度偏差，从而降低个性化程度并导致推荐质量下降。图神经网络（GNN）由于其消息传递机制在CF中表现有效，但它们的聚合过程可能会进一步传播和放大流行度偏差。现有方法通常通过修改训练目标来应对流行度偏差，但未能直接对抗GNN在邻域聚合过程中传播的偏差。在聚合过程中对交互加权有助于缓解这一问题，然而由于训练早期阶段节点表示的不稳定性，这种加权可能会扭曲模型的学习。本文提出了一种后处理流行度去偏（Post-hoc Popularity Debiasing, PPD）方法，该方法无需重新训练，即可直接在预训练的嵌入上进行操作，对基于GNN的CF中的流行度偏差进行校正。通过估计交互层面的流行度，并借助一个流行度方向向量从节点表示中去除流行度成分，PPD在减少偏差的同时保留了用户偏好。实验结果表明，我们的方法在基于GNN的CF中流行度偏差校正方面优于当前最先进的方法。",
        "translated_title": "基于图神经网络的协同过滤中的后处理流行度偏差校正",
        "label": [
            "图神经网络推荐（GNN for Recommendation）",
            "通用推荐技术（General Recommendation Techniques）"
        ],
        "label_reason": "论文针对GNN在推荐中的流行度偏差问题提出后处理方法，属于图神经网络推荐改进技术",
        "relevance_score": 8
    },
    {
        "title": "Universal Image Restoration Pre-training via Masked Degradation\n  Classification",
        "url": "http://arxiv.org/abs/2510.13282v1",
        "pub_date": "2025-10-15",
        "summary": "This study introduces a Masked Degradation Classification Pre-Training method (MaskDCPT), designed to facilitate the classification of degradation types in input images, leading to comprehensive image restoration pre-training. Unlike conventional pre-training methods, MaskDCPT uses the degradation type of the image as an extremely weak supervision, while simultaneously leveraging the image reconstruction to enhance performance and robustness. MaskDCPT includes an encoder and two decoders: the encoder extracts features from the masked low-quality input image. The classification decoder uses these features to identify the degradation type, whereas the reconstruction decoder aims to reconstruct a corresponding high-quality image. This design allows the pre-training to benefit from both masked image modeling and contrastive learning, resulting in a generalized representation suited for restoration tasks. Benefit from the straightforward yet potent MaskDCPT, the pre-trained encoder can be used to address universal image restoration and achieve outstanding performance. Implementing MaskDCPT significantly improves performance for both convolution neural networks (CNNs) and Transformers, with a minimum increase in PSNR of 3.77 dB in the 5D all-in-one restoration task and a 34.8% reduction in PIQE compared to baseline in real-world degradation scenarios. It also emergences strong generalization to previously unseen degradation types and levels. In addition, we curate and release the UIR-2.5M dataset, which includes 2.5 million paired restoration samples across 19 degradation types and over 200 degradation levels, incorporating both synthetic and real-world data. The dataset, source code, and models are available at https://github.com/MILab-PKU/MaskDCPT.",
        "translated": "本研究提出了一种 Masked Degradation Classification Pre-Training 方法（MaskDCPT），旨在实现对输入图像退化类型的分类，从而促进通用图像恢复的预训练。不同于传统的预训练方法，MaskDCPT 将图像的退化类型作为极其弱的监督信号，同时利用图像重建来提升性能和鲁棒性。MaskDCPT 包含一个编码器和两个解码器：编码器从被遮蔽的低质量输入图像中提取特征；分类解码器利用这些特征识别退化类型，而重建解码器则致力于重建对应的高质量图像。这种设计使预训练能够同时受益于遮蔽图像建模和对比学习，从而获得适用于图像恢复任务的通用表征。借助简洁而有效的 MaskDCPT，预训练的编码器可用于解决通用图像恢复问题，并取得优异的性能。MaskDCPT 的实现显著提升了卷积神经网络（CNNs）和 Transformers 的性能，在 5D 全合一图像恢复任务中，PSNR 至少提升了 3.77 dB，而在真实退化场景中，与基线相比，PIQE 降低了 34.8%。此外，该方法在未见过的退化类型和程度上也展现出强大的泛化能力。同时，我们整理并发布了 UIR-2.5M 数据集，该数据集包含 250 万对覆盖 19 种退化类型和 200 多个退化等级的图像恢复样本，融合了合成数据和真实数据。数据集、源代码和模型均可在 https://github.com/MILab-PKU/MaskDCPT 获取。",
        "translated_title": "通过掩码退化分类的通用图像恢复预训练",
        "label": [
            "图像恢复（Image Restoration）",
            "图像去噪（Image Denoising）",
            "图像去雨（Image Deraining）",
            "图像去雾（Image Dehazing）",
            "图像去模糊（Image Deblurring）",
            "超分辨率（Super-Resolution）",
            "图像去 JPEG 伪影（JPEG Artifact Removal）"
        ],
        "label_reason": "论文提出通用图像恢复预训练方法，适用于多种低级图像恢复任务",
        "relevance_score": 9
    },
    {
        "title": "PhysMaster: Mastering Physical Representation for Video Generation via\n  Reinforcement Learning",
        "url": "http://arxiv.org/abs/2510.13809v1",
        "pub_date": "2025-10-15",
        "summary": "Video generation models nowadays are capable of generating visually realistic videos, but often fail to adhere to physical laws, limiting their ability to generate physically plausible videos and serve as ''world models''. To address this issue, we propose PhysMaster, which captures physical knowledge as a representation for guiding video generation models to enhance their physics-awareness. Specifically, PhysMaster is based on the image-to-video task where the model is expected to predict physically plausible dynamics from the input image. Since the input image provides physical priors like relative positions and potential interactions of objects in the scenario, we devise PhysEncoder to encode physical information from it as an extra condition to inject physical knowledge into the video generation process. The lack of proper supervision on the model's physical performance beyond mere appearance motivates PhysEncoder to apply reinforcement learning with human feedback to physical representation learning, which leverages feedback from generation models to optimize physical representations with Direct Preference Optimization (DPO) in an end-to-end manner. PhysMaster provides a feasible solution for improving physics-awareness of PhysEncoder and thus of video generation, proving its ability on a simple proxy task and generalizability to wide-ranging physical scenarios. This implies that our PhysMaster, which unifies solutions for various physical processes via representation learning in the reinforcement learning paradigm, can act as a generic and plug-in solution for physics-aware video generation and broader applications.",
        "translated": "当前的视频生成模型虽然能够生成视觉上逼真的视频，但往往未能遵循物理规律，这限制了它们生成物理合理视频的能力，也限制了其作为“世界模型”的潜力。为了解决这一问题，我们提出了PhysMaster，它将物理知识捕获为表示形式，用于引导视频生成模型，从而增强其对物理规律的感知能力。具体而言，PhysMaster基于图像到视频的任务，模型期望从输入图像中预测物理上合理的动态过程。由于输入图像提供了诸如场景中物体相对位置和潜在交互等物理先验，我们设计了PhysEncoder，从输入图像中编码物理信息作为额外的条件，将其注入视频生成过程中。由于缺乏对模型物理性能（除了外观）的有效监督，PhysEncoder采用基于人类反馈的强化学习来实现物理表示学习，通过生成模型的反馈，以端到端的方式使用直接偏好优化（Direct Preference Optimization, DPO）来优化物理表示。PhysMaster为提高PhysEncoder以及视频生成的物理感知能力提供了一种可行的解决方案，并在简单代理任务上验证了其能力，同时展示了其在广泛物理场景中的泛化性能。这表明，我们提出的PhysMaster通过在强化学习范式下的表示学习，统一了解决各种物理过程的方法，可以作为一种通用且可插拔的解决方案，应用于物理感知视频生成及相关更广泛的领域。",
        "translated_title": "PhysMaster：通过强化学习掌握物理表示用于视频生成",
        "label": [],
        "label_reason": "论文聚焦视频生成而非图像像素级复原，属于high-level任务",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "结合强化学习与物理表示，有一定创新但非图像恢复核心领域"
    },
    {
        "title": "VisCoP: Visual Probing for Video Domain Adaptation of Vision Language\n  Models",
        "url": "http://arxiv.org/abs/2510.13808v1",
        "pub_date": "2025-10-15",
        "summary": "Large Vision-Language Models (VLMs) excel at general visual reasoning tasks but exhibit sharp performance degradation when applied to novel domains with substantial distribution shifts from pretraining data. Existing domain adaptation approaches finetune different VLM components, but this often results in limited domain-specific feature learning or catastrophic forgetting of prior capabilities. To address these issues, we introduce Vision Contextualized Probing (VisCoP), which augments the VLM's vision encoder with a compact set of learnable visual probes. These probes enable efficient domain-specific adaptation with minimal modification to pretrained parameters. We evaluate VisCoP across three challenging domain adaptation settings-cross-view (exocentric to egocentric), cross-modal (RGB to depth), and cross-task (human understanding to robot control). Experiments show that VisCoP consistently outperforms existing adaptation strategies, achieving superior performance on target domains while effectively retaining source-domain knowledge.",
        "translated": "大型视觉-语言模型（VLMs）在通用视觉推理任务中表现出色，但当应用于与预训练数据存在显著分布差异的新领域时，其性能会急剧下降。现有的领域适应方法通常对VLM的不同组件进行微调，但这往往导致领域特定特征学习受限，或者对先前能力产生灾难性遗忘。为了解决这些问题，我们提出了视觉上下文化探针（Vision Contextualized Probing, VisCoP），该方法通过在VLM的视觉编码器中引入一组紧凑的可学习视觉探针来增强模型。这些探针能够在对预训练参数仅作最小修改的情况下实现高效的领域特定适应。我们在三种具有挑战性的领域适应设置中评估了VisCoP，包括跨视角（外视角到自视角）、跨模态（RGB到深度）和跨任务（人类理解到机器人控制）。实验表明，VisCoP始终优于现有适应策略，在目标领域上取得了更优的性能，同时有效保留了源领域的知识。",
        "translated_title": "VisCoP：用于视觉语言模型视频域适应的视觉探测",
        "label": [],
        "label_reason": "论文聚焦于视觉语言模型的视频领域适配，不直接涉及图像像素级恢复或增强",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出视觉探针机制，对现有领域适配方法有一定改进"
    },
    {
        "title": "Generative Universal Verifier as Multimodal Meta-Reasoner",
        "url": "http://arxiv.org/abs/2510.13804v1",
        "pub_date": "2025-10-15",
        "summary": "We introduce Generative Universal Verifier, a novel concept and plugin designed for next-generation multimodal reasoning in vision-language models and unified multimodal models, providing the fundamental capability of reflection and refinement on visual outcomes during the reasoning and generation process. This work makes three main contributions: (1) We build ViVerBench, a comprehensive benchmark spanning 16 categories of critical tasks for evaluating visual outcomes in multimodal reasoning. Results show that existing VLMs consistently underperform across these tasks, underscoring a substantial gap from human-level capability in reliable visual verification. (2) We design two automated pipelines to construct large-scale visual verification data and train OmniVerifier-7B, the first omni-capable generative verifier trained for universal visual verification and achieves notable gains on ViVerBench(+8.3). Through training, we identify three atomic capabilities in visual verification and demonstrate how they generalize and interact synergistically. (3) We propose OmniVerifier-TTS, a sequential test-time scaling paradigm that leverages the universal verifier to bridge image generation and editing within unified models, enhancing the upper bound of generative ability through iterative fine-grained optimization. Beyond generation, we extend universal verifier to broader world-modeling interleaved reasoning scenarios. Empirically, OmniVerifier-TTS achieves improvements on T2I-ReasonBench(+3.7), and GenEval++(+4.3), outperforming existing parallel test-time scaling methods, such as Best-of-N. By endowing multimodal reasoning with reliable visual verification, OmniVerifier advances both reliable reflection during generation and scalable test-time refinement, marking a step toward more trustworthy and controllable next-generation reasoning systems.",
        "translated": "我们提出生成式通用验证器（Generative Universal Verifier），这是一种面向下一代视觉语言模型和统一多模态模型的新型概念和插件，旨在提供推理和生成过程中对视觉结果进行反思和优化的基本能力。本研究做出以下三项主要贡献：(1) 我们构建了 ViVerBench，一个全面的基准测试集，涵盖16类关键任务，用于评估多模态推理中视觉结果的性能。结果显示，现有视觉语言模型（VLMs）在这些任务中表现普遍不佳，表明其在可靠视觉验证方面与人类水平之间存在显著差距。(2) 我们设计了两条自动化流水线用于构建大规模的视觉验证数据，并训练了 OmniVerifier-7B，这是首个面向通用视觉验证的全能型生成式验证器。在 ViVerBench 上，OmniVerifier-7B 取得了显著的性能提升（+8.3）。通过训练，我们识别出视觉验证中的三个基本能力，并展示了它们如何在不同任务中泛化并协同工作。(3) 我们提出了 OmniVerifier-TTS，一种顺序式测试时扩展范式，利用通用验证器在统一模型中实现图像生成和编辑的桥梁作用，并通过迭代细粒度优化提升生成能力的上限。除了生成任务，我们还将通用验证器扩展到更广泛的交互式世界建模推理场景。实证表明，OmniVerifier-TTS 在 T2I-ReasonBench（+3.7）和 GenEval++（+4.3）上均取得性能提升，并优于现有的并行式测试时扩展方法，如 Best-of-N。通过为多模态推理赋予可靠的视觉验证能力，OmniVerifier 推动了生成过程中的可靠反思以及可扩展的测试时优化，标志着我们向更加可信和可控的下一代推理系统迈出了关键一步。",
        "translated_title": "生成式通用验证器作为多模态元推理器",
        "label": [],
        "label_reason": "论文主要关注视觉-语言模型的推理验证，非图像像素级处理",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出通用验证框架和新范式，但未突破low-level图像处理核心技术"
    },
    {
        "title": "Trace Anything: Representing Any Video in 4D via Trajectory Fields",
        "url": "http://arxiv.org/abs/2510.13802v1",
        "pub_date": "2025-10-15",
        "summary": "Effective spatio-temporal representation is fundamental to modeling, understanding, and predicting dynamics in videos. The atomic unit of a video, the pixel, traces a continuous 3D trajectory over time, serving as the primitive element of dynamics. Based on this principle, we propose representing any video as a Trajectory Field: a dense mapping that assigns a continuous 3D trajectory function of time to each pixel in every frame. With this representation, we introduce Trace Anything, a neural network that predicts the entire trajectory field in a single feed-forward pass. Specifically, for each pixel in each frame, our model predicts a set of control points that parameterizes a trajectory (i.e., a B-spline), yielding its 3D position at arbitrary query time instants. We trained the Trace Anything model on large-scale 4D data, including data from our new platform, and our experiments demonstrate that: (i) Trace Anything achieves state-of-the-art performance on our new benchmark for trajectory field estimation and performs competitively on established point-tracking benchmarks; (ii) it offers significant efficiency gains thanks to its one-pass paradigm, without requiring iterative optimization or auxiliary estimators; and (iii) it exhibits emergent abilities, including goal-conditioned manipulation, motion forecasting, and spatio-temporal fusion. Project page: https://trace-anything.github.io/.",
        "translated": "有效的时空表示是建模、理解和预测视频中动态过程的基础。视频的基本单位是像素，其在时间上追踪一个连续的3D轨迹，作为动态的基本元素。基于这一原理，我们提出将任何视频表示为轨迹场（Trajectory Field）：一种密集映射，为每一帧中的每个像素分配一个关于时间的连续3D轨迹函数。通过这种表示，我们引入了Trace Anything，一种神经网络模型，它能够在一次前向传播中预测整个轨迹场。具体来说，对于每一帧中的每个像素，我们的模型预测一组控制点，以参数化轨迹（即B样条），从而在任意查询时刻获得其3D位置。我们在大规模4D数据上训练了Trace Anything模型，包括我们新平台的数据，实验结果表明：(i) Trace Anything在我们新的轨迹场估计基准上取得了最先进的性能，并在已有的点跟踪基准上表现出竞争力；(ii) 由于其单次前向传播的范式，它在效率方面有显著提升，无需进行迭代优化或使用辅助估计器；(iii) 它展现出一些新兴能力，包括目标条件下的操控、运动预测以及时空融合。项目主页：https://trace-anything.github.io/。",
        "translated_title": "追踪任何内容：通过轨迹场在4D中表示任何视频",
        "label": [],
        "label_reason": "论文关注视频轨迹建模，不属于图像像素级恢复或增强任务。",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出轨迹场表示和一次性预测框架，具有新颖性。"
    },
    {
        "title": "Reasoning in Space via Grounding in the World",
        "url": "http://arxiv.org/abs/2510.13800v1",
        "pub_date": "2025-10-15",
        "summary": "In this paper, we claim that 3D visual grounding is the cornerstone of spatial reasoning and introduce the Grounded-Spatial Reasoner (GS-Reasoner) to explore the effective spatial representations that bridge the gap between them. Existing 3D LLMs suffer from the absence of a unified 3D representation capable of jointly capturing semantic and geometric information. This deficiency is manifested either in poor performance on grounding or in an excessive reliance on external modules, ultimately hindering the seamless integration of grounding and spatial reasoning. To address this, we propose a simple yet effective dual-path pooling mechanism that tightly aligns geometric features with both semantic and positional cues, constructing a unified image patch-based 3D representation that encapsulates all essential information without increasing the number of input tokens. Leveraging this holistic representation, GS-Reasoner is the first 3D LLM that achieves autoregressive grounding entirely without external modules while delivering performance comparable to state-of-the-art models, establishing a unified and self-contained framework for 3D spatial reasoning. To further bridge grounding and spatial reasoning, we introduce the Grounded Chain-of-Thought (GCoT) dataset. This dataset is meticulously curated to include both 3D bounding box annotations for objects referenced in reasoning questions and step-by-step reasoning paths that integrate grounding as a core component of the problem-solving process. Extensive experiments demonstrate that GS-Reasoner achieves impressive results on 3D visual grounding, which in turn significantly enhances its spatial reasoning capabilities, leading to state-of-the-art performance.",
        "translated": "本文提出观点认为，三维视觉定位是空间推理的基础，并引入了 GS-Reasoner（Grounded-Spatial Reasoner）以探索能够弥合二者之间差距的有效空间表示。现有的三维大语言模型（LLM）面临缺乏统一的三维表示的问题，该表示无法同时捕捉语义和几何信息。这种缺陷体现在定位任务上的性能较差，或对外部模块的过度依赖，最终阻碍了定位与空间推理的无缝融合。为了解决这一问题，我们提出了一种简单而有效的同时路径池化机制，该机制紧密对齐几何特征与语义和位置线索，构建了一种统一的、基于图像块的三维表示形式，包含所有必要信息且不增加输入 token 的数量。借助这种整体表示，GS-Reasoner 是首个在不使用任何外部模块的情况下实现自回归定位的三维大语言模型，其性能可与最先进的模型相媲美，为三维空间推理提供了一个统一且自洽的框架。为进一步弥合定位与空间推理之间的联系，我们引入了 Grounded Chain-of-Thought（GCoT）数据集。该数据集经过精心构建，包含了推理问题中所提到对象的三维边界框标注，以及将定位作为解决问题核心步骤的逐步推理路径。大量实验表明，GS-Reasoner 在三维视觉定位任务中取得了令人印象深刻的结果，从而显著提升了其空间推理能力，达到当前最先进的性能水平。",
        "translated_title": "通过在世界中的锚定实现空间推理",
        "label": [],
        "label_reason": "论文聚焦3D视觉理解和推理，不涉及图像像素级恢复或增强",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "提出了一种新颖的3D表示方法和数据集，但不针对低层图像处理任务"
    },
    {
        "title": "The Mechanistic Emergence of Symbol Grounding in Language Models",
        "url": "http://arxiv.org/abs/2510.13796v1",
        "pub_date": "2025-10-15",
        "summary": "Symbol grounding (Harnad, 1990) describes how symbols such as words acquire their meanings by connecting to real-world sensorimotor experiences. Recent work has shown preliminary evidence that grounding may emerge in (vision-)language models trained at scale without using explicit grounding objectives. Yet, the specific loci of this emergence and the mechanisms that drive it remain largely unexplored. To address this problem, we introduce a controlled evaluation framework that systematically traces how symbol grounding arises within the internal computations through mechanistic and causal analysis. Our findings show that grounding concentrates in middle-layer computations and is implemented through the aggregate mechanism, where attention heads aggregate the environmental ground to support the prediction of linguistic forms. This phenomenon replicates in multimodal dialogue and across architectures (Transformers and state-space models), but not in unidirectional LSTMs. Our results provide behavioral and mechanistic evidence that symbol grounding can emerge in language models, with practical implications for predicting and potentially controlling the reliability of generation.",
        "translated": "符号根基（Symbol grounding）（Harnad, 1990）描述了符号（如词语）如何通过与现实世界中的感知和运动体验相连接而获得其含义。最近的研究表明，在大规模训练的（视觉-）语言模型中，即使不使用显式的根基目标，符号根基也可能自发出现。然而，这种现象的具体出现位置及其驱动机制仍鲜有研究。为了解决这一问题，我们引入了一种可控的评估框架，系统地追踪符号根基在内部计算过程中的产生方式，并通过机制和因果分析进行研究。我们的研究结果表明，符号根基主要集中在中间层计算中，并通过聚合机制实现，其中注意力头聚合环境根基以支持语言形式的预测。这一现象在多模态对话中以及不同架构（Transformer 和状态空间模型）中均出现，但在单向 LSTM 中未观察到。我们的研究提供了行为和机制上的证据，证明符号根基可以在语言模型中自发产生，这对于预测和可能控制生成的可靠性具有实际意义。",
        "translated_title": "语言模型中符号 grounding 的机制性出现",
        "label": [],
        "label_reason": "论文聚焦语言模型的符号接地机制，不涉及图像像素级处理。",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "提出了符号接地的机制分析框架，有一定理论创新但非视觉任务相关。"
    },
    {
        "title": "Bee: A High-Quality Corpus and Full-Stack Suite to Unlock Advanced Fully\n  Open MLLMs",
        "url": "http://arxiv.org/abs/2510.13795v1",
        "pub_date": "2025-10-15",
        "summary": "Fully open multimodal large language models (MLLMs) currently lag behind proprietary counterparts, primarily due to a significant gap in data quality for supervised fine-tuning (SFT). Existing open-source datasets are often plagued by widespread noise and a critical deficit in complex reasoning data, such as Chain-of-Thought (CoT), which hinders the development of advanced model capabilities. Addressing these challenges, our work makes three primary contributions. First, we introduce Honey-Data-15M, a new SFT dataset comprising approximately 15 million QA pairs, processed through multiple cleaning techniques and enhanced with a novel dual-level (short and long) CoT enrichment strategy. Second, we introduce HoneyPipe, the data curation pipeline, and its underlying framework DataStudio, providing the community with a transparent and adaptable methodology for data curation that moves beyond static dataset releases. Finally, to validate our dataset and pipeline, we train Bee-8B, an 8B model on Honey-Data-15M. Experiments show that Bee-8B establishes a new state-of-the-art (SOTA) for fully open MLLMs, achieving performance that is competitive with, and in some cases surpasses, recent semi-open models such as InternVL3.5-8B. Our work delivers to the community a suite of foundational resources, including: the Honey-Data-15M corpus; the full-stack suite comprising HoneyPipe and DataStudio; training recipes; an evaluation harness; and the model weights. This effort demonstrates that a principled focus on data quality is a key pathway to developing fully open MLLMs that are highly competitive with their semi-open counterparts.",
        "translated": "目前，完全开源的多模态大语言模型（MLLMs）在性能上落后于专有模型，主要原因在于用于监督微调（SFT）的数据质量存在显著差距。现有的开源数据集通常受到广泛噪声的困扰，并且在复杂推理数据（如思维链（CoT））方面存在严重不足，这阻碍了先进模型能力的发展。为了解决这些挑战，我们的工作主要包括三个方面的贡献。首先，我们引入了 Honey-Data-15M，一个包含约 1500 万个问答对的新 SFT 数据集，通过多种数据清洗技术处理，并采用了一种新的双层（短链和长链）CoT 增强策略进行优化。其次，我们提出了 HoneyPipe 数据整理流水线及其底层框架 DataStudio，为社区提供一种透明且可调节的数据整理方法，突破了传统静态数据集发布的方式。最后，为了验证我们的数据集和流水线效果，我们基于 Honey-Data-15M 训练了 Bee-8B，一个 80 亿参数规模的模型。实验结果表明，Bee-8B 在完全开源 MLLMs 中达到了新的最先进水平（SOTA），其性能与近期的半开源模型（如 InternVL3.5-8B）相比具有竞争力，某些情况下甚至优于后者。我们的工作为社区提供了一套基础资源，包括：Honey-Data-15M 数据语料库；涵盖 HoneyPipe 和 DataStudio 的全栈工具套件；训练配方；评估框架；以及模型权重。这项研究表明，专注于数据质量的原则性方法是开发与半开源模型高度竞争的完全开源 MLLMs 的关键路径。",
        "translated_title": "Bee：一个高质量语料库和全栈套件，用于解锁先进的全开放大语言模型",
        "label": [],
        "label_reason": "论文聚焦多模态大语言模型数据构建，不涉及图像像素级处理",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "提出新数据集和训练方法，但属于通用MLLM改进而非视觉低级任务创新"
    },
    {
        "title": "NoisePrints: Distortion-Free Watermarks for Authorship in Private\n  Diffusion Models",
        "url": "http://arxiv.org/abs/2510.13793v1",
        "pub_date": "2025-10-15",
        "summary": "With the rapid adoption of diffusion models for visual content generation, proving authorship and protecting copyright have become critical. This challenge is particularly important when model owners keep their models private and may be unwilling or unable to handle authorship issues, making third-party verification essential. A natural solution is to embed watermarks for later verification. However, existing methods require access to model weights and rely on computationally heavy procedures, rendering them impractical and non-scalable. To address these challenges, we propose , a lightweight watermarking scheme that utilizes the random seed used to initialize the diffusion process as a proof of authorship without modifying the generation process. Our key observation is that the initial noise derived from a seed is highly correlated with the generated visual content. By incorporating a hash function into the noise sampling process, we further ensure that recovering a valid seed from the content is infeasible. We also show that sampling an alternative seed that passes verification is infeasible, and demonstrate the robustness of our method under various manipulations. Finally, we show how to use cryptographic zero-knowledge proofs to prove ownership without revealing the seed. By keeping the seed secret, we increase the difficulty of watermark removal. In our experiments, we validate NoisePrints on multiple state-of-the-art diffusion models for images and videos, demonstrating efficient verification using only the seed and output, without requiring access to model weights.",
        "translated": "随着扩散模型在视觉内容生成中的迅速应用，证明作者身份和保护版权变得至关重要。当模型所有者保持其模型私有时，这一挑战尤为突出，他们可能不愿或无法处理作者身份问题，从而使得第三方验证成为必要。一种自然的解决方案是嵌入水印以供后续验证。然而，现有方法需要访问模型权重，并依赖计算量大的过程，使其在实践中不可行且难以扩展。为了解决这些挑战，我们提出了一种轻量级的水印方案，该方案利用用于初始化扩散过程的随机种子作为作者身份的证明，且无需修改生成过程。我们的关键观察是，由种子生成的初始噪声与生成的视觉内容具有高度相关性。通过在噪声采样过程中引入哈希函数，我们进一步确保了从内容中恢复有效种子在计算上不可行。我们还证明，采样一个能通过验证的替代种子在计算上也是不可行的，并展示了我们的方法在各种篡改下的鲁棒性。最后，我们展示了如何使用密码学中的零知识证明来在不泄露种子的前提下证明所有权。通过保密种子，我们提高了水印移除的难度。在我们的实验中，我们在多个最先进的图像和视频扩散模型上验证了 NoisePrints 的有效性，展示了仅使用种子和输出即可实现高效的验证，而无需访问模型权重。",
        "translated_title": "NoisePrints: 隐私扩散模型中的无损水印用于作者归属",
        "label": [],
        "label_reason": "论文聚焦扩散模型水印而非图像像素级恢复",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出无需模型权重的轻量水印方案，有一定创新"
    },
    {
        "title": "Adaptive Visual Conditioning for Semantic Consistency in Diffusion-Based\n  Story Continuation",
        "url": "http://arxiv.org/abs/2510.13787v1",
        "pub_date": "2025-10-15",
        "summary": "Story continuation focuses on generating the next image in a narrative sequence so that it remains coherent with both the ongoing text description and the previously observed images. A central challenge in this setting lies in utilizing prior visual context effectively, while ensuring semantic alignment with the current textual input. In this work, we introduce AVC (Adaptive Visual Conditioning), a framework for diffusion-based story continuation. AVC employs the CLIP model to retrieve the most semantically aligned image from previous frames. Crucially, when no sufficiently relevant image is found, AVC adaptively restricts the influence of prior visuals to only the early stages of the diffusion process. This enables the model to exploit visual context when beneficial, while avoiding the injection of misleading or irrelevant information. Furthermore, we improve data quality by re-captioning a noisy dataset using large language models, thereby strengthening textual supervision and semantic alignment. Quantitative results and human evaluations demonstrate that AVC achieves superior coherence, semantic consistency, and visual fidelity compared to strong baselines, particularly in challenging cases where prior visuals conflict with the current input.",
        "translated": "故事延续关注于生成叙事序列中的下一幅图像，使其与当前的文本描述和之前已观测的图像保持连贯。在这种设置下的一个核心挑战在于如何有效地利用先前的视觉上下文，同时确保与当前文本输入在语义上的一致性。在本文中，我们提出了 AVC（Adaptive Visual Conditioning），一种基于扩散模型的故事延续框架。AVC 利用 CLIP 模型从先前的帧中检索语义最相关的一幅图像。关键的是，当未找到足够相关的图像时，AVC 会自适应地将先前视觉信息的影响限制在扩散过程的早期阶段。这使得模型在有益时能够有效利用视觉上下文，同时避免注入误导性或不相关的信息。此外，我们通过使用大语言模型对噪声数据集进行重新描述，提高了数据质量，从而增强了文本监督和语义一致性。定量结果和人类评估表明，与强大的基线模型相比，AVC 在连贯性、语义一致性和视觉保真度方面均取得了优越的性能，特别是在先前视觉信息与当前输入冲突的具有挑战性的情况下。",
        "translated_title": "基于扩散的故事情节延续中的自适应视觉条件化以保持语义一致性",
        "label": [],
        "label_reason": "论文关注文本驱动的扩散模型故事生成，非像素级图像恢复任务。",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出自适应视觉条件框架，改进文本-图像对齐方法，但创新点较常规。"
    },
    {
        "title": "InternVLA-M1: A Spatially Guided Vision-Language-Action Framework for\n  Generalist Robot Policy",
        "url": "http://arxiv.org/abs/2510.13778v1",
        "pub_date": "2025-10-15",
        "summary": "We introduce InternVLA-M1, a unified framework for spatial grounding and robot control that advances instruction-following robots toward scalable, general-purpose intelligence. Its core idea is spatially guided vision-language-action training, where spatial grounding serves as the critical link between instructions and robot actions. InternVLA-M1 employs a two-stage pipeline: (i) spatial grounding pre-training on over 2.3M spatial reasoning data to determine ``where to act'' by aligning instructions with visual, embodiment-agnostic positions, and (ii) spatially guided action post-training to decide ``how to act'' by generating embodiment-aware actions through plug-and-play spatial prompting. This spatially guided training recipe yields consistent gains: InternVLA-M1 outperforms its variant without spatial guidance by +14.6% on SimplerEnv Google Robot, +17% on WidowX, and +4.3% on LIBERO Franka, while demonstrating stronger spatial reasoning capability in box, point, and trace prediction. To further scale instruction following, we built a simulation engine to collect 244K generalizable pick-and-place episodes, enabling a 6.2% average improvement across 200 tasks and 3K+ objects. In real-world clustered pick-and-place, InternVLA-M1 improved by 7.3%, and with synthetic co-training, achieved +20.6% on unseen objects and novel configurations. Moreover, in long-horizon reasoning-intensive scenarios, it surpassed existing works by over 10%. These results highlight spatially guided training as a unifying principle for scalable and resilient generalist robots. Code and models are available at https://github.com/InternRobotics/InternVLA-M1.",
        "translated": "我们引入了 InternVLA-M1，这是一个面向空间定位与机器人控制的统一框架，推动指令跟随机器人向可扩展、通用型智能发展。其核心思想是空间引导的视觉-语言-动作训练，其中空间定位作为连接指令与机器人动作的关键环节。InternVLA-M1 采用两阶段训练流程：(i) 在超过 2.3 万个空间推理数据上进行空间定位预训练，通过将指令与视觉、与实体无关的空间位置对齐来确定 ``在何处执行动作''，以及 (ii) 通过即插即用的空间提示生成与实体相关的动作，进行空间引导的动作后训练以决定 ``如何执行动作''。这种空间引导的训练方法带来了显著的提升：在 SimplierEnv Google Robot 上，InternVLA-M1 相较于其无空间引导的变体提升了 +14.6%，在 WidowX 上提升了 +17%，在 LIBERO Franka 上提升了 +4.3%，同时在盒预测、点预测和轨迹预测中表现出更强的空间推理能力。为了进一步扩展指令跟随的能力，我们构建了一个仿真引擎，收集了 244K 个具有泛化能力的抓取与放置场景，使得在 200 个任务和 3K+ 个物体上的平均性能提升了 6.2%。在现实世界中的集群抓取与放置任务中，InternVLA-M1 提升了 7.3%，而在结合合成数据的联合训练中，对于未见过的物体和新配置，性能提升达 +20.6%。此外，在需要长期推理的复杂场景中，其性能超越现有工作超过 10%。这些结果表明，空间引导的训练方法可作为构建可扩展且鲁棒的通用型机器人的统一原则。代码和模型可在 https://github.com/InternRobotics/InternVLA-M1 获取。",
        "translated_title": "InternVLA-M1：一种空间引导的视觉-语言-动作框架  \n用于通用机器人策略",
        "label": [],
        "label_reason": "论文聚焦机器人控制而非图像像素级恢复或增强",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出空间引导训练方法，对指令跟随机器人有改进"
    },
    {
        "title": "UrbanFusion: Stochastic Multimodal Fusion for Contrastive Learning of\n  Robust Spatial Representations",
        "url": "http://arxiv.org/abs/2510.13774v1",
        "pub_date": "2025-10-15",
        "summary": "Forecasting urban phenomena such as housing prices and public health indicators requires the effective integration of various geospatial data. Current methods primarily utilize task-specific models, while recent foundation models for spatial representations often support only limited modalities and lack multimodal fusion capabilities. To overcome these challenges, we present UrbanFusion, a Geo-Foundation Model (GeoFM) that features Stochastic Multimodal Fusion (SMF). The framework employs modality-specific encoders to process different types of inputs, including street view imagery, remote sensing data, cartographic maps, and points of interest (POIs) data. These multimodal inputs are integrated via a Transformer-based fusion module that learns unified representations. An extensive evaluation across 41 tasks in 56 cities worldwide demonstrates UrbanFusion's strong generalization and predictive performance compared to state-of-the-art GeoAI models. Specifically, it 1) outperforms prior foundation models on location-encoding, 2) allows multimodal input during inference, and 3) generalizes well to regions unseen during training. UrbanFusion can flexibly utilize any subset of available modalities for a given location during both pretraining and inference, enabling broad applicability across diverse data availability scenarios. All source code is available at https://github.com/DominikM198/UrbanFusion.",
        "translated": "预测城市现象（如房价和公共健康指标）需要有效整合各种地理空间数据。当前的方法主要依赖于任务特定模型，而近期的空间表示基础模型通常只支持有限的模态，并缺乏多模态融合能力。为了解决这些挑战，我们提出了 UrbanFusion，一种具有随机多模态融合（Stochastic Multimodal Fusion, SMF）功能的地理基础模型（Geo-Foundation Model, GeoFM）。该框架使用模态特定编码器处理不同类型的输入，包括街景图像、遥感数据、地图数据和兴趣点（Points of Interest, POIs）数据。这些多模态输入通过基于 Transformer 的融合模块进行整合，以学习统一的表示。在对全球 56 个城市中的 41 项任务进行广泛评估后，UrbanFusion 在泛化能力和预测性能方面均优于最先进的 GeoAI 模型。具体而言，UrbanFusion 具备以下优势：1）在位置编码方面优于以往的基础模型，2）在推理过程中支持多模态输入，3）对训练过程中未见过的区域具有良好的泛化能力。UrbanFusion 在预训练和推理阶段都能灵活地利用特定位置中可用的任意子集模态，从而使其适用于各种数据可用性场景。所有源代码均在 https://github.com/DominikM198/UrbanFusion 上公开。",
        "translated_title": "UrbanFusion: 用于鲁棒空间表示对比学习的随机多模态融合",
        "label": [
            "遥感图像复原"
        ],
        "label_reason": "涉及遥感图像处理，但主要聚焦多模态融合而非像素级图像质量恢复。",
        "relevance_score": 5,
        "novelty_score": 7,
        "novelty_reason": "提出随机多模态融合方法，对Geo-Foundation Model有一定改进。"
    },
    {
        "title": "Scaling Vision Transformers for Functional MRI with Flat Maps",
        "url": "http://arxiv.org/abs/2510.13768v1",
        "pub_date": "2025-10-15",
        "summary": "A key question for adapting modern deep learning architectures to functional MRI (fMRI) is how to represent the data for model input. To bridge the modality gap between fMRI and natural images, we transform the 4D volumetric fMRI data into videos of 2D fMRI activity flat maps. We train Vision Transformers on 2.3K hours of fMRI flat map videos from the Human Connectome Project using the spatiotemporal masked autoencoder (MAE) framework. We observe that masked fMRI modeling performance improves with dataset size according to a strict power scaling law. Downstream classification benchmarks show that our model learns rich representations supporting both fine-grained state decoding across subjects, as well as subject-specific trait decoding across changes in brain state. This work is part of an ongoing open science project to build foundation models for fMRI data. Our code and datasets are available at https://github.com/MedARC-AI/fmri-fm.",
        "translated": "将现代深度学习架构适应于功能磁共振成像（fMRI）的一个关键问题是，如何将数据表示为模型输入。为了弥合 fMRI 与自然图像之间的模态差异，我们把 4D 的 fMRI 体积数据转换为 2D 的 fMRI 活动平面图的视频。我们使用时空掩码自编码器（MAE）框架，在来自人类连接组计划（Human Connectome Project）的 2.3K 小时 fMRI 平面图视频上训练视觉变换器（Vision Transformers）。我们观察到，随着数据集规模的增加，掩码 fMRI 建模性能按照严格的幂律进行提升。下游分类基准测试表明，我们的模型学习到了丰富的表示，不仅能够跨被试进行细粒度状态解码，还能在脑状态变化的条件下进行被试特异性特质的解码。本项工作是正在进行的一个开放科学项目的一部分，旨在为 fMRI 数据构建基础模型。我们的代码和数据集可在 https://github.com/MedARC-AI/fmri-fm 获取。",
        "translated_title": "Scaling Vision Transformers for Functional MRI with Flat Maps  \n使用平面图的视觉变换器扩展功能磁共振成像  \n\n功能磁共振成像（fMRI）是一种非侵入性技术，用于研究大脑活动，其通过检测血氧水平依赖（BOLD）信号实现。然而，fMRI 数据通常受到低空间分辨率和低信噪比（SNR）的限制，这对识别大脑功能区域构成挑战。最近，视觉变换器（Vision Transformers, ViTs）在各种图像恢复任务中表现出卓越的性能。这些模型通过其自注意力机制能够捕获长距离依赖关系，为 fMRI 信号的处理提供了新思路。  \n\n在本研究中，我们提出了一种基于视觉变换器的方法，用于 fMRI 数据的去噪和增强。我们采用平面图（Flat Maps）作为先验知识，以更好地建模大脑皮层的拓扑结构。通过将 fMRI 数据投影到平面图中，我们能够利用 ViT 在空域中的强大特征提取能力。实验结果表明，我们的方法在多个 fMRI 数据集上显著优于现有方法，尤其是在保留功能细节和减少噪声方面表现出色。  \n\n此外，我们还探讨了视觉变换器的扩展能力，通过增加模型深度和宽度来提升其性能。我们发现，适当增加模型规模可以有效提高 fMRI 数据的恢复质量，同时保持计算效率。",
        "label": [],
        "label_reason": "论文专注于fMRI数据表示与建模，非图像像素级恢复或增强任务。",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出将Vision Transformer扩展至fMRI数据建模，具有一定新颖性。"
    },
    {
        "title": "Uni-MMMU: A Massive Multi-discipline Multimodal Unified Benchmark",
        "url": "http://arxiv.org/abs/2510.13759v1",
        "pub_date": "2025-10-15",
        "summary": "Unified multimodal models aim to jointly enable visual understanding and generation, yet current benchmarks rarely examine their true integration. Existing evaluations either treat the two abilities in isolation or overlook tasks that inherently couple them. To address this gap, we present Uni-MMMU, a comprehensive and discipline-aware benchmark that systematically unfolds the bidirectional synergy between generation and understanding across eight reasoning-centric domains, including science, coding, mathematics, and puzzles. Each task is bidirectionally coupled, demanding models to (i) leverage conceptual understanding to guide precise visual synthesis, or (ii) utilize generation as a cognitive scaffold for analytical reasoning. Uni-MMMU incorporates verifiable intermediate reasoning steps, unique ground truths, and a reproducible scoring protocol for both textual and visual outputs. Through extensive evaluation of state-of-the-art unified, generation-only, and understanding-only models, we reveal substantial performance disparities and cross-modal dependencies, offering new insights into when and how these abilities reinforce one another, and establishing a reliable foundation for advancing unified models.",
        "translated": "统一的多模态模型旨在同时实现视觉理解和生成能力，但目前的基准测试很少真正考察这两种能力的融合。现有的评估方法要么将这两种能力孤立地对待，要么忽略了那些本质上需要将它们耦合的任务。为了解决这一问题，我们提出了 Uni-MMMU，这是一个全面且学科感知的基准测试，系统地揭示了生成与理解在八个以推理为中心的领域（包括科学、编程、数学和谜题）中的双向协同作用。每个任务都是双向耦合的，要求模型 (i) 利用概念理解来指导精确的视觉合成，或 (ii) 将生成能力作为认知支架，以支持分析推理。Uni-MMMU 包含可验证的中间推理步骤、唯一的地面真值，以及针对文本和视觉输出的可复现评分协议。通过对最先进的统一模型、仅生成模型和仅理解模型的广泛评估，我们揭示了显著的性能差异和跨模态依赖关系，提供了关于何时以及如何实现这些能力相互增强的新见解，并为统一模型的进一步发展奠定了可靠基础。",
        "translated_title": "Uni-MMMU: 一个大规模多学科多模态统一基准",
        "label": [],
        "label_reason": "论文关注多模态统一模型评估，不属于图像像素级处理任务。",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "提出了跨学科的多模态统一基准，但方法较为常规。"
    },
    {
        "title": "RECODE: Reasoning Through Code Generation for Visual Question Answering",
        "url": "http://arxiv.org/abs/2510.13756v1",
        "pub_date": "2025-10-15",
        "summary": "Multimodal Large Language Models (MLLMs) struggle with precise reasoning for structured visuals like charts and diagrams, as pixel-based perception lacks a mechanism for verification. To address this, we propose to leverage derendering -- the process of reverse-engineering visuals into executable code -- as a new modality for verifiable visual reasoning. Specifically, we propose RECODE, an agentic framework that first generates multiple candidate programs to reproduce the input image. It then uses a critic to select the most faithful reconstruction and iteratively refines the code. This process not only transforms an ambiguous perceptual task into a verifiable, symbolic problem, but also enables precise calculations and logical inferences later on. On various visual reasoning benchmarks such as CharXiv, ChartQA, and Geometry3K, RECODE significantly outperforms methods that do not leverage code or only use code for drawing auxiliary lines or cropping. Our work demonstrates that grounding visual perception in executable code provides a new path toward more accurate and verifiable multimodal reasoning.",
        "translated": "多模态大语言模型（MLLMs）在处理图表和示意图等结构化视觉内容时，常常在精确推理方面存在困难，因为基于像素的感知缺乏验证机制。为了解决这一问题，我们提出利用去渲染（derendering）——即将视觉图像反向工程为可执行代码的过程——作为一种可验证视觉推理的新模态。具体而言，我们提出了 RECODE，一个智能体框架，首先生成多个候选程序以重现输入图像。然后，它使用一个评估器选择最忠实的重建结果，并对代码进行迭代优化。这一过程不仅将模糊的感知任务转化为可验证的符号问题，还为后续的精确计算和逻辑推理提供了可能。在各种视觉推理基准如 CharXiv、ChartQA 和 Geometry3K 上，RECODE 显著优于不使用代码或仅使用代码绘制辅助线或裁剪图像的方法。我们的工作表明，将视觉感知建立在可执行代码基础上，为更准确和可验证的多模态推理提供了一条新路径。",
        "translated_title": "RECODE：通过代码生成进行推理的视觉问答",
        "label": [],
        "label_reason": "论文不属于低层图像处理，而是视觉问答任务。",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出通过代码生成实现视觉推理，具有新颖性。"
    },
    {
        "title": "InteractiveOmni: A Unified Omni-modal Model for Audio-Visual Multi-turn\n  Dialogue",
        "url": "http://arxiv.org/abs/2510.13747v1",
        "pub_date": "2025-10-15",
        "summary": "We introduce InteractiveOmni, a unified and open-source omni-modal large language model for audio-visual multi-turn interaction, ranging from 4B to 8B parameters, designed to lead the field of lightweight models by offering comprehensive omni-modal understanding and speech generation capabilities. To achieve this, we integrate the vision encoder, audio encoder, large language model, and speech decoder into a unified model for understanding and generation tasks. We design a multi-stage training strategy to ensure robust cross-modal capabilities, including pre-training for omni-modal understanding, followed by post-training with speech conversation and audio-visual interaction. To enable human-like long-term conversational ability, we meticulously curate a multi-turn training dataset that enhances the model's ability to handle complex and multi-turn interactions. To effectively evaluate the multi-turn memory and speech interaction capabilities, we construct the multi-modal multi-turn memory benchmark and the multi-turn speech interaction benchmark. Experiments demonstrate that InteractiveOmni significantly outperforms leading open-source models and provides a more intelligent multi-turn audio-visual experience, particularly in its long-term memory capabilities. Notably, InteractiveOmni-4B is comparable to the much larger model like Qwen2.5-Omni-7B on general benchmarks, and it can retain 97% of the performance of the InteractiveOmni-8B while utilizing only 50% of the model size. Achieving state-of-the-art results against similarly sized models across image, audio, video understanding, and speech generation tasks, InteractiveOmni is an accessible, open-source foundation for next-generation intelligent interactive systems.",
        "translated": "我们提出 InteractiveOmni，一种统一且开源的多模态大语言模型，用于音频-视觉的多轮交互，参数规模从 4B 到 8B 不等，旨在通过提供全面的多模态理解与语音生成能力，引领轻量级模型的发展。为此，我们将视觉编码器、音频编码器、大语言模型和语音解码器集成到一个统一模型中，用于理解和生成任务。我们设计了一种多阶段训练策略，以确保强大的跨模态能力，包括用于多模态理解的预训练，以及随后进行的语音对话和音视频交互的后训练。为了实现类人般的长期对话能力，我们精心构建了一个多轮训练数据集，以增强模型处理复杂和多轮交互的能力。为了有效评估多轮记忆和语音交互能力，我们构建了多模态多轮记忆基准和多轮语音交互基准。实验表明，InteractiveOmni 在多项指标上显著优于领先的开源模型，提供了更加智能的多轮音视频交互体验，特别是在长期记忆能力方面。值得注意的是，InteractiveOmni-4B 在通用基准上表现与更大规模的模型如 Qwen2.5-Omni-7B 相当，并且仅使用 50% 的模型规模即可保留 97% 的 InteractiveOmni-8B 性能。在图像、音频、视频理解和语音生成任务中，InteractiveOmni 在同规模模型中实现了最先进的结果，是下一代智能交互系统的一个可访问、开源的基础。",
        "translated_title": "InteractiveOmni：用于音视频多轮对话的统一全模态模型",
        "label": [],
        "label_reason": "不属于low-level图像处理，是多模态对话模型",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "常规多模态模型设计，无本质创新"
    },
    {
        "title": "UniCalli: A Unified Diffusion Framework for Column-Level Generation and\n  Recognition of Chinese Calligraphy",
        "url": "http://arxiv.org/abs/2510.13745v1",
        "pub_date": "2025-10-15",
        "summary": "Computational replication of Chinese calligraphy remains challenging. Existing methods falter, either creating high-quality isolated characters while ignoring page-level aesthetics like ligatures and spacing, or attempting page synthesis at the expense of calligraphic correctness. We introduce \\textbf{UniCalli}, a unified diffusion framework for column-level recognition and generation. Training both tasks jointly is deliberate: recognition constrains the generator to preserve character structure, while generation provides style and layout priors. This synergy fosters concept-level abstractions that improve both tasks, especially in limited-data regimes. We curated a dataset of over 8,000 digitized pieces, with ~4,000 densely annotated. UniCalli employs asymmetric noising and a rasterized box map for spatial priors, trained on a mix of synthetic, labeled, and unlabeled data. The model achieves state-of-the-art generative quality with superior ligature continuity and layout fidelity, alongside stronger recognition. The framework successfully extends to other ancient scripts, including Oracle bone inscriptions and Egyptian hieroglyphs. Code and data can be viewed in \\href{https://github.com/EnVision-Research/UniCalli}{this URL}.",
        "translated": "中国书法的计算复现仍然具有挑战性。现有方法存在不足，要么生成高质量的单个字符而忽视了页面级别的美学要素（如连笔和间距），要么在尝试页面合成时以牺牲书法正确性为代价。我们引入了**UniCalli**，一种面向列级别的识别与生成的统一扩散框架。联合训练这两个任务是经过深思熟虑的：识别任务约束生成器以保持字符结构，而生成任务则提供了风格和布局的先验知识。这种协同作用促进了概念级别的抽象表示，从而提升了两个任务的性能，尤其在数据受限的情况下表现更为明显。我们整理了一个包含超过 8,000 幅数字化书法作品的数据集，其中约 4,000 幅进行了密集标注。UniCalli 采用非对称噪声和光栅化框图来提供空域先验，并在合成数据、标注数据和未标注数据的混合集上进行训练。该模型在生成质量方面达到了最先进的水平，连笔的连续性和布局的保真度均优于现有方法，同时识别性能也更强。该框架成功地扩展到了其他古代文字，包括甲骨文和埃及象形文字。代码和数据可通过 \\href{https://github.com/EnVision-Research/UniCalli}{此链接}查看。",
        "translated_title": "UniCalli：一种面向列级生成与识别的中文书法统一扩散框架",
        "label": [],
        "label_reason": "不属于low-level图像处理，专注于书法生成与识别",
        "relevance_score": 1,
        "novelty_score": 6,
        "novelty_reason": "提出了统一扩散框架，但属于high-level生成与识别任务"
    },
    {
        "title": "Multi-Scale High-Resolution Logarithmic Grapher Module for Efficient\n  Vision GNNs",
        "url": "http://arxiv.org/abs/2510.13740v1",
        "pub_date": "2025-10-15",
        "summary": "Vision graph neural networks (ViG) have demonstrated promise in vision tasks as a competitive alternative to conventional convolutional neural nets (CNN) and transformers (ViTs); however, common graph construction methods, such as k-nearest neighbor (KNN), can be expensive on larger images. While methods such as Sparse Vision Graph Attention (SVGA) have shown promise, SVGA's fixed step scale can lead to over-squashing and missing multiple connections to gain the same information that could be gained from a long-range link. Through this observation, we propose a new graph construction method, Logarithmic Scalable Graph Construction (LSGC) to enhance performance by limiting the number of long-range links. To this end, we propose LogViG, a novel hybrid CNN-GNN model that utilizes LSGC. Furthermore, inspired by the successes of multi-scale and high-resolution architectures, we introduce and apply a high-resolution branch and fuse features between our high-resolution and low-resolution branches for a multi-scale high-resolution Vision GNN network. Extensive experiments show that LogViG beats existing ViG, CNN, and ViT architectures in terms of accuracy, GMACs, and parameters on image classification and semantic segmentation tasks. Our smallest model, Ti-LogViG, achieves an average top-1 accuracy on ImageNet-1K of 79.9% with a standard deviation of 0.2%, 1.7% higher average accuracy than Vision GNN with a 24.3% reduction in parameters and 35.3% reduction in GMACs. Our work shows that leveraging long-range links in graph construction for ViGs through our proposed LSGC can exceed the performance of current state-of-the-art ViGs. Code is available at https://github.com/mmunir127/LogViG-Official.",
        "translated": "视觉图神经网络（ViG）在视觉任务中展现出作为传统卷积神经网络（CNN）和视觉变换器（ViTs）的有竞争力的替代方案的潜力；然而，常见的图构建方法，如 k-近邻（KNN），在处理大尺寸图像时计算代价较高。虽然稀疏视觉图注意力（SVGA）等方法已显示出前景，但 SVGA 固定的步长尺度可能导致过度压缩问题，并且无法通过多个连接获取与长程连接相同的信息。基于这一观察，我们提出了一种新的图构建方法——对数可扩展图构建（LSGC），通过限制长程连接的数量来提升性能。为此，我们设计了 LogViG，这是一种新颖的 CNN-GNN 混合模型，利用了 LSGC。此外，受多尺度和高分辨率架构成功案例的启发，我们引入并应用了一个高分辨率分支，并在高分辨率和低分辨率分支之间融合特征，构建了一个多尺度高分辨率的视觉图神经网络。大量实验表明，LogViG 在图像分类和语义分割任务中，在准确率、GMACs 和参数数量方面均优于现有的 ViG、CNN 和 ViT 架构。我们最小的模型 Ti-LogViG 在 ImageNet-1K 数据集上的平均 top-1 准确率为 79.9%，标准差为 0.2%，比 Vision GNN 的平均准确率高出 1.7%，同时参数数量减少了 24.3%，GMACs 降低了 35.3%。我们的研究表明，通过所提出的 LSGC 在图构建中利用长程连接，可使 ViG 的性能超越当前最先进的 ViG 模型。代码可在 https://github.com/mmunir127/LogViG-Official 获取。",
        "translated_title": "多尺度高分辨率对数图生成模块用于高效的视觉图神经网络",
        "label": [],
        "label_reason": "论文主要关注图像分类和语义分割，不属于 low-level 图像处理。",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出新的图构建方法 LSGC，对 Vision GNN 有显著改进。"
    },
    {
        "title": "Cyclic Self-Supervised Diffusion for Ultra Low-field to High-field MRI\n  Synthesis",
        "url": "http://arxiv.org/abs/2510.13735v1",
        "pub_date": "2025-10-15",
        "summary": "Synthesizing high-quality images from low-field MRI holds significant potential. Low-field MRI is cheaper, more accessible, and safer, but suffers from low resolution and poor signal-to-noise ratio. This synthesis process can reduce reliance on costly acquisitions and expand data availability. However, synthesizing high-field MRI still suffers from a clinical fidelity gap. There is a need to preserve anatomical fidelity, enhance fine-grained structural details, and bridge domain gaps in image contrast. To address these issues, we propose a \\emph{cyclic self-supervised diffusion (CSS-Diff)} framework for high-field MRI synthesis from real low-field MRI data. Our core idea is to reformulate diffusion-based synthesis under a cycle-consistent constraint. It enforces anatomical preservation throughout the generative process rather than just relying on paired pixel-level supervision. The CSS-Diff framework further incorporates two novel processes. The slice-wise gap perception network aligns inter-slice inconsistencies via contrastive learning. The local structure correction network enhances local feature restoration through self-reconstruction of masked and perturbed patches. Extensive experiments on cross-field synthesis tasks demonstrate the effectiveness of our method, achieving state-of-the-art performance (e.g., 31.80 $\\pm$ 2.70 dB in PSNR, 0.943 $\\pm$ 0.102 in SSIM, and 0.0864 $\\pm$ 0.0689 in LPIPS). Beyond pixel-wise fidelity, our method also preserves fine-grained anatomical structures compared with the original low-field MRI (e.g., left cerebral white matter error drops from 12.1$\\%$ to 2.1$\\%$, cortex from 4.2$\\%$ to 3.7$\\%$). To conclude, our CSS-Diff can synthesize images that are both quantitatively reliable and anatomically consistent.",
        "translated": "从低场 MRI 数据合成高质量图像具有重要潜力。低场 MRI 成本更低、更易获取、也更安全，但其分辨率较低且信噪比差。通过合成过程，可以减少对昂贵采集设备的依赖，并扩展数据的可用性。然而，当前的高场 MRI 合成仍然存在临床保真度的不足。有必要在合成中保持解剖结构的准确性，增强细粒度的结构细节，并弥合图像对比度方面的领域差异。为了解决这些问题，我们提出了一种名为 \\emph{循环自监督扩散（CSS-Diff）} 的框架，用于从真实的低场 MRI 数据中合成高场 MRI。我们的核心思想是在循环一致的约束下重构基于扩散的合成方法。这种方法在整个生成过程中强制保持解剖结构，而不仅仅依赖于像素级的配对监督。CSS-Diff 框架进一步结合了两个新颖的处理过程。切片级差异感知网络通过对比学习对齐切片间的不一致性。局部结构校正网络则通过遮蔽和扰动图像块的自重建来增强局部特征恢复。在跨场强合成任务上的大量实验表明了我们方法的有效性，达到了当前最先进的性能（例如，PSNR 为 31.80 $\\pm$ 2.70 dB，SSIM 为 0.943 $\\pm$ 0.102，LPIPS 为 0.0864 $\\pm$ 0.0689）。除了像素级保真度之外，与原始低场 MRI 相比，我们的方法还能保持更精细的解剖结构（例如，左脑白质误差从 12.1$\\%$ 降至 2.1$\\%$，皮层误差从 4.2$\\%$ 降至 3.7$\\%$）。综上所述，我们的 CSS-Diff 能够合成在定量评估和解剖一致性方面都表现良好的图像。",
        "translated_title": "循环自监督扩散用于超低场到高场MRI合成",
        "label": [
            "医学图像增强",
            "图像去噪",
            "图像恢复"
        ],
        "label_reason": "论文专注于从低场MRI合成高质量高场MRI，涉及图像恢复与医学增强",
        "relevance_score": 8,
        "novelty_score": 8,
        "novelty_reason": "提出循环自监督扩散框架，结合新网络结构提升合成效果"
    },
    {
        "title": "LiFMCR: Dataset and Benchmark for Light Field Multi-Camera Registration",
        "url": "http://arxiv.org/abs/2510.13729v1",
        "pub_date": "2025-10-15",
        "summary": "We present LiFMCR, a novel dataset for the registration of multiple micro lens array (MLA)-based light field cameras. While existing light field datasets are limited to single-camera setups and typically lack external ground truth, LiFMCR provides synchronized image sequences from two high-resolution Raytrix R32 plenoptic cameras, together with high-precision 6-degrees of freedom (DoF) poses recorded by a Vicon motion capture system. This unique combination enables rigorous evaluation of multi-camera light field registration methods.   As a baseline, we provide two complementary registration approaches: a robust 3D transformation estimation via a RANSAC-based method using cross-view point clouds, and a plenoptic PnP algorithm estimating extrinsic 6-DoF poses from single light field images. Both explicitly integrate the plenoptic camera model, enabling accurate and scalable multi-camera registration. Experiments show strong alignment with the ground truth, supporting reliable multi-view light field processing.   Project page: https://lifmcr.github.io/",
        "translated": "我们提出了 LiFMCR，这是一个用于多微透镜阵列（MLA）光场相机注册的新数据集。现有的光场数据集通常局限于单相机设置，并且普遍缺乏外部真实值。LiFMCR 提供了来自两台高分辨率 Raytrix R32 光场相机的同步图像序列，并配有通过 Vicon 运动捕捉系统记录的高精度六自由度（DoF）姿态。这一独特组合使得对多相机光场注册方法的严格评估成为可能。\n\n作为基准，我们提供了两种互补的注册方法：一种是通过基于 RANSAC 的方法利用跨视角点云进行鲁棒的三维变换估计，另一种是光场 PnP 算法，用于从单光场图像中估计外部六自由度姿态。两种方法均明确集成了光场相机模型，从而实现了精确且可扩展的多相机注册。实验结果表明，与真实值具有高度对齐性，支持可靠的多视角光场处理。\n\n项目页面：https://lifmcr.github.io/",
        "translated_title": "LiFMCR：光场多摄像头配准的数据集与基准",
        "label": [],
        "label_reason": "论文主要关注光场相机注册，不属于像素级图像恢复或增强",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出了新的光场多相机注册数据集和基准"
    },
    {
        "title": "NExT-OMNI: Towards Any-to-Any Omnimodal Foundation Models with Discrete\n  Flow Matching",
        "url": "http://arxiv.org/abs/2510.13721v1",
        "pub_date": "2025-10-15",
        "summary": "Next-generation multimodal foundation models capable of any-to-any cross-modal generation and multi-turn interaction will serve as core components of artificial general intelligence systems, playing a pivotal role in human-machine interaction. However, most existing multimodal models remain constrained by autoregressive architectures, whose inherent limitations prevent a balanced integration of understanding and generation capabilities. Although hybrid and decoupling strategies have been explored to address these tasks within unified frameworks separately, their redundant, non-integrated designs limit their applicability to broader scenarios, such as cross-modal retrieval.In this work, we introduce NExT-OMNI, an open-source omnimodal foundation model that achieves unified modeling through discrete flow paradigms. By leveraging metric-induced probability paths and kinetic optimal velocities, NExT-OMNI natively supports any-to-any understanding and generation with enhanced response efficiency, while enabling broader application scenarios through concise unified representations rather than task-decoupled designs. Trained on large-scale interleaved text, image, video, and audio data, NExT-OMNI delivers competitive performance on multimodal generation and understanding benchmarks, while outperforming prior unified models in multi-turn multimodal interaction and cross-modal retrieval, highlighting its architectural advantages as a next-generation multimodal foundation model. To advance further research, we release training details, data protocols, and open-source both the code and model checkpoints.",
        "translated": "下一代能够实现任意模态到任意模态跨模态生成和多轮交互的多模态基础模型，将成为通用人工智能系统的核心组成部分，在人机交互中发挥关键作用。然而，目前大多数多模态模型仍受自回归架构的限制，其固有局限性阻碍了理解能力和生成能力之间的平衡整合。尽管已有研究探索了混合架构和任务解耦策略，试图在统一框架内分别处理这些任务，但其冗余且非整合的设计限制了其在更广泛场景（如跨模态检索）中的应用。在本工作中，我们提出 NExT-OMNI，一个开源的全模态基础模型，通过离散流范式实现统一建模。NExT-OMNI 利用度量诱导的概率路径和动力最优速度，在增强响应效率的同时，原生支持任意模态到任意模态的理解与生成，并通过简洁的统一表征而非任务解耦的设计，实现了更广泛的应用场景。NExT-OMNI 在大规模交织的文本、图像、视频和音频数据上进行训练，在多模态生成和理解基准上展现出具有竞争力的性能，并在多轮多模态交互和跨模态检索方面优于以往的统一模型，突显了其作为下一代多模态基础模型的架构优势。为了推动进一步的研究，我们发布了训练细节、数据协议，并开源了代码和模型检查点。",
        "translated_title": "NExT-OMNI：基于离散流匹配的任意到任意全模态基础模型",
        "label": [],
        "label_reason": "论文主要研究多模态模型，不属于图像像素级恢复或增强任务。",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出离散流匹配的新范式，对多模态模型架构有显著改进。"
    }
]
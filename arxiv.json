[
    {
        "title": "FinVet: A Collaborative Framework of RAG and External Fact-Checking\n  Agents for Financial Misinformation Detection",
        "url": "http://arxiv.org/abs/2510.11654v1",
        "pub_date": "2025-10-13",
        "summary": "Financial markets face growing threats from misinformation that can trigger billions in losses in minutes. Most existing approaches lack transparency in their decision-making and provide limited attribution to credible sources. We introduce FinVet, a novel multi-agent framework that integrates two Retrieval-Augmented Generation (RAG) pipelines with external fact-checking through a confidence-weighted voting mechanism. FinVet employs adaptive three-tier processing that dynamically adjusts verification strategies based on retrieval confidence, from direct metadata extraction to hybrid reasoning to full model-based analysis. Unlike existing methods, FinVet provides evidence-backed verdicts, source attribution, confidence scores, and explicit uncertainty flags when evidence is insufficient. Experimental evaluation on the FinFact dataset shows that FinVet achieves an F1 score of 0.85, which is a 10.4% improvement over the best individual pipeline (fact-check pipeline) and 37% improvement over standalone RAG approaches.",
        "translated": "金融市场正面临日益严重的虚假信息威胁，这些信息可能在几分钟内引发数十亿美元的损失。大多数现有方法在决策过程中缺乏透明度，并且对可信来源的归因有限。我们提出 FinVet，一个新颖的多智能体框架，通过置信度加权投票机制，将两个检索增强生成（Retrieval-Augmented Generation, RAG）流水线与外部事实核查相结合。FinVet 采用自适应的三级处理流程，根据检索置信度动态调整验证策略，从直接元数据提取，到混合推理，再到基于模型的完整分析。与现有方法不同，FinVet 在证据不足时提供基于证据的判定结果、来源归因、置信度评分以及明确的不确定性标记。在 FinFact 数据集上的实验评估表明，FinVet 达到 0.85 的 F1 分数，相比最优的单一流水线（事实核查流水线）提高了 10.4%，相比独立的 RAG 方法提高了 37%。"
    },
    {
        "title": "OneRec-Think: In-Text Reasoning for Generative Recommendation",
        "url": "http://arxiv.org/abs/2510.11639v1",
        "pub_date": "2025-10-13",
        "summary": "The powerful generative capacity of Large Language Models (LLMs) has instigated a paradigm shift in recommendation. However, existing generative models (e.g., OneRec) operate as implicit predictors, critically lacking the capacity for explicit and controllable reasoning-a key advantage of LLMs. To bridge this gap, we propose OneRec-Think, a unified framework that seamlessly integrates dialogue, reasoning, and personalized recommendation. OneRec-Think incorporates: (1) Itemic Alignment: cross-modal Item-Textual Alignment for semantic grounding; (2) Reasoning Activation: Reasoning Scaffolding to activate LLM reasoning within the recommendation context; and (3) Reasoning Enhancement, where we design a recommendation-specific reward function that accounts for the multi-validity nature of user preferences. Experiments across public benchmarks show state-of-the-art performance. Moreover, our proposed \"Think-Ahead\" architecture enables effective industrial deployment on Kuaishou, achieving a 0.159\\% gain in APP Stay Time and validating the practical efficacy of the model's explicit reasoning capability.",
        "translated": "大型语言模型（LLMs）强大的生成能力正在引发推荐系统领域的一场范式转变。然而，现有的生成模型（例如 OneRec）主要作为隐式预测器运行，严重缺乏显式可控推理的能力——而这正是 LLMs 的关键优势所在。为了解决这一问题，我们提出了 OneRec-Think，一个统一的框架，能够无缝融合对话、推理与个性化推荐。OneRec-Think 包含以下三个核心模块：（1）Itemic 对齐：跨模态的物品-文本对齐，以实现语义基础的构建；（2）推理激活：通过推理结构（Reasoning Scaffolding）在推荐场景中激活 LLM 的推理能力；以及（3）推理增强：我们设计了一个面向推荐任务的奖励函数，以应对用户偏好的多合理性（multi-validity）特性。在多个公开基准上的实验表明，该方法取得了最先进的性能。此外，我们提出的“Think-Ahead”架构在快手平台实现了有效的工业部署，使得 APP 使用时长提升了 0.159%，验证了模型显式推理能力的实用效果。"
    },
    {
        "title": "SemCSE-Multi: Multifaceted and Decodable Embeddings for Aspect-Specific\n  and Interpretable Scientific Domain Mapping",
        "url": "http://arxiv.org/abs/2510.11599v1",
        "pub_date": "2025-10-13",
        "summary": "We propose SemCSE-Multi, a novel unsupervised framework for generating multifaceted embeddings of scientific abstracts, evaluated in the domains of invasion biology and medicine. These embeddings capture distinct, individually specifiable aspects in isolation, thus enabling fine-grained and controllable similarity assessments as well as adaptive, user-driven visualizations of scientific domains. Our approach relies on an unsupervised procedure that produces aspect-specific summarizing sentences and trains embedding models to map semantically related summaries to nearby positions in the embedding space. We then distill these aspect-specific embedding capabilities into a unified embedding model that directly predicts multiple aspect embeddings from a scientific abstract in a single, efficient forward pass. In addition, we introduce an embedding decoding pipeline that decodes embeddings back into natural language descriptions of their associated aspects. Notably, we show that this decoding remains effective even for unoccupied regions in low-dimensional visualizations, thus offering vastly improved interpretability in user-centric settings.",
        "translated": "我们提出了一种名为 SemCSE-Multi 的新型无监督框架，用于生成科学摘要的多方面嵌入表示，并在入侵生物学和医学领域进行了评估。该嵌入能够独立捕捉多个明确且可单独指定的方面，从而支持细粒度、可控的相似性评估，以及自适应的、以用户驱动的科学领域可视化。我们的方法依赖于一个无监督过程，首先生成特定方面的总结性句子，并训练嵌入模型将语义相关的摘要映射到嵌入空间中的相近位置。随后，我们将这些特定方面的嵌入能力提炼到一个统一的嵌入模型中，使其能够在一个高效、单一的前向传播过程中直接从科学摘要中预测出多个方面嵌入。此外，我们还引入了一个嵌入解码流程，将嵌入表示还原为与各特定方面相关的自然语言描述。值得注意的是，我们证明了即使在低维可视化中未被占用的区域，该解码过程依然有效，从而在以用户为中心的应用场景中显著提升了可解释性。"
    },
    {
        "title": "REGENT: Relevance-Guided Attention for Entity-Aware Multi-Vector Neural\n  Re-Ranking",
        "url": "http://arxiv.org/abs/2510.11592v1",
        "pub_date": "2025-10-13",
        "summary": "Current neural re-rankers often struggle with complex information needs and long, content-rich documents. The fundamental issue is not computational--it is intelligent content selection: identifying what matters in lengthy, multi-faceted texts. While humans naturally anchor their understanding around key entities and concepts, neural models process text within rigid token windows, treating all interactions as equally important and missing critical semantic signals. We introduce REGENT, a neural re-ranking model that mimics human-like understanding by using entities as a \"semantic skeleton\" to guide attention. REGENT integrates relevance guidance directly into the attention mechanism, combining fine-grained lexical matching with high-level semantic reasoning. This relevance-guided attention enables the model to focus on conceptually important content while maintaining sensitivity to precise term matches. REGENT achieves new state-of-the-art performance in three challenging datasets, providing up to 108% improvement over BM25 and consistently outperforming strong baselines including ColBERT and RankVicuna. To our knowledge, this is the first work to successfully integrate entity semantics directly into neural attention, establishing a new paradigm for entity-aware information retrieval.",
        "translated": "目前的神经重排序模型在处理复杂的信息需求和长篇、内容丰富文档时常常面临挑战。其根本问题并不在于计算能力，而在于智能内容选择：识别长篇、多维文本中真正重要的信息。尽管人类在理解文本时自然地围绕关键实体和概念进行定位，但神经模型则受限于固定的词元窗口对文本进行处理，将所有交互视为同等重要，从而忽略了关键的语义信号。我们提出REGENT，这是一种神经重排序模型，通过使用实体作为“语义骨架”来引导注意力，从而模拟人类的理解方式。REGENT将相关性引导直接整合到注意力机制中，将细粒度的词汇匹配与高层次的语义推理相结合。这种相关性引导的注意力机制使模型能够聚焦于概念上重要的内容，同时保持对精确术语匹配的敏感性。REGENT在三个具有挑战性的数据集上达到了新的最先进性能，在BM25基础上最多提升了108%，并且始终优于ColBERT和RankVicuna等强基线模型。据我们所知，这是首次成功将实体语义直接整合到神经注意力机制中的工作，为具备实体感知能力的信息检索建立了一个新的范式。"
    },
    {
        "title": "QDER: Query-Specific Document and Entity Representations for\n  Multi-Vector Document Re-Ranking",
        "url": "http://arxiv.org/abs/2510.11589v1",
        "pub_date": "2025-10-13",
        "summary": "Neural IR has advanced through two distinct paths: entity-oriented approaches leveraging knowledge graphs and multi-vector models capturing fine-grained semantics. We introduce QDER, a neural re-ranking model that unifies these approaches by integrating knowledge graph semantics into a multi-vector model. QDER's key innovation lies in its modeling of query-document relationships: rather than computing similarity scores on aggregated embeddings, we maintain individual token and entity representations throughout the ranking process, performing aggregation only at the final scoring stage - an approach we call \"late aggregation.\" We first transform these fine-grained representations through learned attention patterns, then apply carefully chosen mathematical operations for precise matches. Experiments across five standard benchmarks show that QDER achieves significant performance gains, with improvements of 36% in nDCG@20 over the strongest baseline on TREC Robust 2004 and similar improvements on other datasets. QDER particularly excels on difficult queries, achieving an nDCG@20 of 0.70 where traditional approaches fail completely (nDCG@20 = 0.0), setting a foundation for future work in entity-aware retrieval.",
        "translated": "神经信息检索（Neural IR）的发展经历了两条不同的路径：一种是面向实体的方法，利用知识图谱信息；另一种是多向量模型，用于捕捉细粒度语义。我们提出 QDER，这是一种神经重排序模型，通过将知识图谱语义整合到多向量模型中，实现了这两条路径的统一。QDER 的关键创新在于其对查询与文档关系的建模：与在聚合嵌入上计算相似度分数的传统方法不同，我们在整个排序过程中保留每个词元（token）和实体的独立表示，仅在最终的评分阶段进行聚合——我们称这种策略为“晚期聚合”（late aggregation）。首先，我们通过学习得到的注意力模式对这些细粒度表示进行转换，然后应用精心选择的数学运算以实现精确匹配。在五个标准基准数据集上的实验表明，QDER 显著提升了性能，在 TREC Robust 2004 数据集上相比最强基线模型的 nDCG@20 提高了 36%，在其他数据集上也有类似提升。尤其在处理困难查询时，QDER 表现出色，取得了 0.70 的 nDCG@20 评分，而传统方法在此类查询上完全失效（nDCG@20 = 0.0），为未来实体感知（entity-aware）检索的研究奠定了基础。"
    },
    {
        "title": "Characterizing Web Search in The Age of Generative AI",
        "url": "http://arxiv.org/abs/2510.11560v1",
        "pub_date": "2025-10-13",
        "summary": "The advent of LLMs has given rise to a new type of web search: Generative search, where LLMs retrieve web pages related to a query and generate a single, coherent text as a response. This output modality stands in stark contrast to traditional web search, where results are returned as a ranked list of independent web pages. In this paper, we ask: Along what dimensions do generative search outputs differ from traditional web search? We compare Google, a traditional web search engine, with four generative search engines from two providers (Google and OpenAI) across queries from four domains. Our analysis reveals intriguing differences. Most generative search engines cover a wider range of sources compared to web search. Generative search engines vary in the degree to which they rely on internal knowledge contained within the model parameters v.s. external knowledge retrieved from the web. Generative search engines surface varying sets of concepts, creating new opportunities for enhancing search diversity and serendipity. Our results also highlight the need for revisiting evaluation criteria for web search in the age of Generative AI.",
        "translated": "大语言模型（LLMs）的出现催生了一种新的网络搜索方式：生成式搜索（generative search），在这种搜索方式中，LLMs会检索与查询相关的网页，并生成一段连贯统一的文本作为响应。这种输出形式与传统网络搜索形成了鲜明对比，后者返回的是一个按相关性排序的独立网页列表。在本文中，我们提出以下问题：生成式搜索的输出在哪些维度上与传统网络搜索有所不同？我们从四个领域中选取查询，比较了传统网络搜索引擎Google与来自两家提供商（Google和OpenAI）的四个生成式搜索引擎的表现。我们的分析揭示了一些有趣的差异。大多数生成式搜索引擎相比传统网络搜索，能够涵盖更广泛的来源。生成式搜索引擎在依赖模型参数中包含的内部知识与从网络检索的外部知识的程度上存在差异。此外，生成式搜索引擎展示的概念集合各不相同，从而为提升搜索多样性和偶然性提供了新的可能性。我们的结果还强调，在生成式人工智能时代，有必要重新审视网络搜索的评估标准。"
    },
    {
        "title": "Uncertainty Quantification for Retrieval-Augmented Reasoning",
        "url": "http://arxiv.org/abs/2510.11483v1",
        "pub_date": "2025-10-13",
        "summary": "Retrieval-augmented reasoning (RAR) is a recent evolution of retrieval-augmented generation (RAG) that employs multiple reasoning steps for retrieval and generation. While effective for some complex queries, RAR remains vulnerable to errors and misleading outputs. Uncertainty quantification (UQ) offers methods to estimate the confidence of systems' outputs. These methods, however, often handle simple queries with no retrieval or single-step retrieval, without properly handling RAR setup. Accurate estimation of UQ for RAR requires accounting for all sources of uncertainty, including those arising from retrieval and generation. In this paper, we account for all these sources and introduce Retrieval-Augmented Reasoning Consistency (R2C)--a novel UQ method for RAR. The core idea of R2C is to perturb the multi-step reasoning process by applying various actions to reasoning steps. These perturbations alter the retriever's input, which shifts its output and consequently modifies the generator's input at the next step. Through this iterative feedback loop, the retriever and generator continuously reshape one another's inputs, enabling us to capture uncertainty arising from both components. Experiments on five popular RAR systems across diverse QA datasets show that R2C improves AUROC by over 5% on average compared to the state-of-the-art UQ baselines. Extrinsic evaluations using R2C as an external signal further confirm its effectiveness for two downstream tasks: in Abstention, it achieves ~5% gains in both F1Abstain and AccAbstain; in Model Selection, it improves the exact match by ~7% over single models and ~3% over selection methods.",
        "translated": "检索增强推理（Retrieval-Augmented Reasoning, RAR）是检索增强生成（Retrieval-Augmented Generation, RAG）的最新演进，其通过在检索与生成过程中引入多步推理来提高效果。尽管在处理某些复杂查询时表现出色，RAR 仍然容易受到错误和误导性输出的影响。不确定性量化（Uncertainty Quantification, UQ）提供了一种估计系统输出置信度的方法。然而，现有方法大多针对无检索或单步检索的简单查询，未能有效应对 RAR 的设置。对 RAR 的 UQ 进行准确估计，需要综合考虑所有可能的不确定性来源，包括检索和生成过程中的不确定性。在本文中，我们系统地考虑了这些不确定性来源，并提出了检索增强推理一致性（Retrieval-Augmented Reasoning Consistency, R2C）——一种新型的 UQ 方法。R2C 的核心思想是通过对推理步骤施加多种操作，扰动多步推理过程。这些扰动会改变检索器的输入，从而影响其输出，并进一步修改生成器在下一步的输入。通过这一迭代反馈机制，检索器和生成器不断重塑彼此的输入，使我们能够捕捉来自两个组件的不确定性。在五个主流 RAR 系统和多个问答数据集上的实验表明，与最先进的 UQ 基线方法相比，R2C 在平均 AUROC 指标上提升了超过 5%。使用 R2C 作为外部信号进行的外在评估进一步验证了其有效性，针对两个下游任务：在拒绝回答（Abstention）任务中，R2C 在 F1Abstain 和 AccAbstain 指标上分别提升了约 5%；在模型选择（Model Selection）任务中，R2C 相比单模型提升了约 7% 的精确匹配（exact match），相比其他选择方法提升了约 3%。"
    },
    {
        "title": "What Generative Search Engines Like and How to Optimize Web Content\n  Cooperatively",
        "url": "http://arxiv.org/abs/2510.11438v1",
        "pub_date": "2025-10-13",
        "summary": "By employing large language models (LLMs) to retrieve documents and generate natural language responses, Generative Engines, such as Google AI overview and ChatGPT, provide significantly enhanced user experiences and have rapidly become the new form of search. Their rapid adoption also drives the needs of Generative Engine Optimization (GEO), as content providers are eager to gain more traction from them. In this paper, we introduce AutoGEO, a framework to automatically learn generative engine preferences when using retrieved contents for response generation, and rewrite web contents for more such traction. AutoGEO first prompts frontier LLMs to explain generative engine preferences and extract meaningful preference rules from these explanations. Then it uses preference rules as context engineering for AutoGEO$_\\text{API}$, a prompt-based GEO system, and as rule-based rewards to train AutoGEO$_\\text{Mini}$, a cost-effective GEO model. Experiments on the standard GEO-Bench and two newly constructed benchmarks using real user queries demonstrate the effectiveness of AutoGEO in enhancing content traction while preserving search utility. Analyses confirm the learned rules' robustness and abilities to capture unique preferences in variant domains, and AutoGEO systems' ability to embed them in content optimization. The code is released at https://github.com/cxcscmu/AutoGEO.",
        "translated": "通过使用大语言模型（LLMs）来检索文档并生成自然语言响应，生成式引擎（如 Google AI 概述和 ChatGPT）显著提升了用户体验，并迅速成为搜索的新形态。其迅速普及也推动了生成式引擎优化（Generative Engine Optimization, GEO）的需求，因为内容提供者希望从这些系统中获得更多曝光。在本文中，我们提出了 AutoGEO，一个在使用检索内容进行响应生成时，能够自动学习生成式引擎偏好的框架，并对网页内容进行重写以提高其曝光度。AutoGEO 首先提示前沿的大语言模型解释生成式引擎的偏好，并从这些解释中提取有意义的偏好规则。接着，它将这些偏好规则用于 AutoGEO$_\\text{API}$——一个基于提示的 GEO 系统——作为上下文工程的输入，并将规则作为奖励机制，用于训练 AutoGEO$_\\text{Mini}$——一个经济高效的 GEO 模型。在标准的 GEO-Bench 基准以及基于真实用户查询构建的两个新基准上的实验表明，AutoGEO 在提升内容曝光度的同时能够保持搜索效用的有效性。分析结果进一步验证了所学习规则的鲁棒性及其在不同领域中捕捉独特偏好的能力，并确认了 AutoGEO 系统在内容优化中嵌入这些规则的能力。代码已发布在 https://github.com/cxcscmu/AutoGEO。"
    },
    {
        "title": "On Inherited Popularity Bias in Cold-Start Item Recommendation",
        "url": "http://arxiv.org/abs/2510.11402v1",
        "pub_date": "2025-10-13",
        "summary": "Collaborative filtering (CF) recommender systems struggle with making predictions on unseen, or 'cold', items. Systems designed to address this challenge are often trained with supervision from warm CF models in order to leverage collaborative and content information from the available interaction data. However, since they learn to replicate the behavior of CF methods, cold-start models may therefore also learn to imitate their predictive biases. In this paper, we show that cold-start systems can inherit popularity bias, a common cause of recommender system unfairness arising when CF models overfit to more popular items, thereby maximizing user-oriented accuracy but neglecting rarer items. We demonstrate that cold-start recommenders not only mirror the popularity biases of warm models, but are in fact affected more severely: because they cannot infer popularity from interaction data, they instead attempt to estimate it based solely on content features. This leads to significant over-prediction of certain cold items with similar content to popular warm items, even if their ground truth popularity is very low. Through experiments on three multimedia datasets, we analyze the impact of this behavior on three generative cold-start methods. We then describe a simple post-processing bias mitigation method that, by using embedding magnitude as a proxy for predicted popularity, can produce more balanced recommendations with limited harm to user-oriented cold-start accuracy.",
        "translated": "协同过滤（Collaborative Filtering, CF）推荐系统在对未见过的或“冷启动”的物品进行预测时面临挑战。为了解决这一问题，设计用于冷启动场景的系统通常会借助“热启动”CF模型进行监督训练，以利用现有的交互数据中的协作信息和内容信息。然而，由于这些系统学习的是复制CF方法的行为，因此冷启动模型也可能学会模仿其预测偏差。本文中，我们表明冷启动系统可能会继承“流行度偏差”（popularity bias），这是推荐系统不公平性的常见原因，当CF模型过度拟合更受欢迎的物品时就会产生这种偏差，从而在最大化面向用户准确率的同时忽略了较少出现的物品。我们通过实验发现，冷启动推荐器不仅复制了热模型的流行度偏差，而且实际上受到更严重的影响：由于它们无法从交互数据中推断出流行度，因此转而尝试仅基于内容特征来估计流行度。这导致某些与热门热启动物品在内容上相似的冷启动物品被显著高估，即使它们的真实流行度非常低。我们在三个多媒体数据集上分析了这一行为对三种生成式冷启动方法的影响。随后，我们介绍了一种简单的后处理偏差缓解方法，该方法通过将嵌入（embedding）模长作为预测流行度的代理指标，能够在有限地影响面向用户冷启动准确率的前提下，生成更加平衡的推荐结果。"
    },
    {
        "title": "VeriCite: Towards Reliable Citations in Retrieval-Augmented Generation\n  via Rigorous Verification",
        "url": "http://arxiv.org/abs/2510.11394v1",
        "pub_date": "2025-10-13",
        "summary": "Retrieval-Augmented Generation (RAG) has emerged as a crucial approach for enhancing the responses of large language models (LLMs) with external knowledge sources. Despite the impressive performance in complex question-answering tasks, RAG still struggles with hallucinations. Attributing RAG-generated content through in-line citations has demonstrated potential in reducing hallucinations and facilitating human verification. Existing citation generation methods primarily rely on either fine-tuning the generator or employing post-processing approaches for citation matching. However, the former approach demands substantial annotated data and computational resources, while the latter often encounters difficulties in managing multiple citations and frequently produces suboptimal results. In this paper, we introduce a novel framework, called VeriCite, designed to rigorously validate supporting evidence and enhance answer attribution. Specifically, VeriCite breaks down into a three-stage generation: 1) The initial answer generation first generates a response based on all available contexts and has its claims verified through the NLI model; 2) the supporting evidence selection assesses the utility of each document and extracts useful supporting evidences; 3) the final answer refinement integrates the initial response and collected evidences to produce the final, refined answer.We conduct experiments across five open-source LLMs and four datasets, demonstrating that VeriCite can significantly improve citation quality while maintaining the correctness of the answers.",
        "translated": "检索增强生成（Retrieval-Augmented Generation, RAG）已成为一种关键方法，用于通过外部知识源增强大语言模型（Large Language Models, LLMs）的响应能力。尽管RAG在复杂问答任务中表现出色，但其仍面临幻觉（hallucination）问题。通过行内引用（in-line citations）对RAG生成的内容进行归因，已被证明在减少幻觉和便于人工验证方面具有潜力。现有的引用生成方法主要依赖于对生成器的微调，或采用后处理方法进行引用匹配。然而，前者需要大量标注数据和计算资源，而后者在处理多个引用时常常遇到困难，且结果往往不够理想。在本文中，我们提出了一种新颖的框架，命名为VeriCite，旨在严格验证支持证据并提升答案归因能力。具体而言，VeriCite分为三个生成阶段：1）初始答案生成阶段基于所有可用上下文生成初步回答，并通过自然语言推理模型（NLI model）对其主张进行验证；2）支持证据选择阶段评估每个文档的有用性，并提取有效的支持证据；3）最终答案优化阶段整合初始回答与收集到的证据，生成最终的、经过优化的答案。我们在五种开源大语言模型和四个数据集上进行了实验，结果表明，VeriCite在保持答案正确性的同时，能够显著提升引用质量。"
    },
    {
        "title": "LLM-Specific Utility: A New Perspective for Retrieval-Augmented\n  Generation",
        "url": "http://arxiv.org/abs/2510.11358v1",
        "pub_date": "2025-10-13",
        "summary": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by incorporating external knowledge. While traditional retrieval focuses on relevance, RAG's effectiveness depends on the utility of retrieved passages, i.e., the usefulness in facilitating the generation of an accurate and comprehensive answer. Existing studies often treat utility as a generic attribute, ignoring the fact that different LLMs may benefit differently from the same passage due to variations in internal knowledge and comprehension ability. In this work, we introduce and systematically investigate the notion of LLM-specific utility. Through large-scale experiments across multiple datasets and LLMs, we demonstrate that human-annotated passages are not optimal for LLMs and that ground-truth utilitarian passages are not transferable across different LLMs. These findings highlight the necessity of adopting the LLM-specific utility in RAG research. Our findings indicate that some human-annotated passages are not ground-truth utilitarian passages for specific LLMs, partially due to the varying readability of queries and passages for LLMs, a tendency for which perplexity is a key metric. Based on these findings, we propose a benchmarking procedure for LLM-specific utility judgments. We evaluate existing utility judgment methods on six datasets and find that while verbalized methods using pseudo-answers perform robustly, LLMs struggle to assess utility effectively-failing to reject all passages for known queries and to select truly useful ones for unknown queries.",
        "translated": "检索增强生成（Retrieval-augmented Generation, RAG）通过引入外部知识来增强大语言模型（Large Language Models, LLMs）的能力。尽管传统检索方法主要关注相关性（relevance），但 RAG 的有效性依赖于检索到的段落的**效用性**（utility），即这些段落在促进生成准确且全面答案方面的有用程度。现有研究通常将效用性视为一种通用属性，忽视了由于不同 LLM 在内部知识和理解能力上的差异，它们可能从相同的段落中获得不同的收益。在本工作中，我们引入并系统地探讨了**面向特定 LLM 的效用性**（LLM-specific utility）这一概念。通过在多个数据集和 LLM 上的大规模实验，我们证明：人工标注的段落对 LLM 来说并非最优选择，并且真实效用性段落在不同 LLM 之间不可迁移。这些发现凸显了在 RAG 研究中采用 LLM-specific utility 的必要性。我们的研究表明，某些人工标注的段落对特定 LLM 而言并不是真实效用性段落，部分原因是 LLM 对查询和段落的可读性存在差异，而这种差异可以通过困惑度（perplexity）这一关键指标来衡量。基于上述发现，我们提出了一种面向 LLM-specific utility 的基准评估方法。我们对六种数据集上的现有效用性判断方法进行了评估，发现尽管使用伪答案（pseudo-answers）的显式化方法表现较为稳健，但 LLM 在评估段落效用性方面仍存在困难，无法拒绝所有已知查询的段落，也无法为未知查询有效挑选真正有用的段落。"
    },
    {
        "title": "Dynamic Network-Based Two-Stage Time Series Forecasting for Affiliate\n  Marketing",
        "url": "http://arxiv.org/abs/2510.11323v1",
        "pub_date": "2025-10-13",
        "summary": "In recent years, affiliate marketing has emerged as a revenue-sharing strategy where merchants collaborate with promoters to promote their products. It not only increases product exposure but also allows promoters to earn a commission. This paper addresses the pivotal yet under-explored challenge in affiliate marketing: accurately assessing and predicting the contributions of promoters in product promotion. We design a novel metric for evaluating the indirect contributions of the promoter, called propagation scale. Unfortunately, existing time series forecasting techniques fail to deliver accurate predictions due to the propagation scale being influenced by multiple factors and the inherent complexities arising from dynamic scenarios. To address this issue, we decouple the network structure from the node signals and propose a two-stage solution: initially, the basic self-sales and network structure prediction are conducted separately, followed by the synthesis of the propagation scale. Specifically, we design a graph convolution encoding scheme based on descendant neighbors and incorporate hypergraph convolution to efficiently capture complex promotional dynamics. Additionally, three auxiliary tasks are employed: self-sales prediction for base estimations, descendant prediction to synthesize propagation scale, and promoter activation prediction to mitigate high volatility issues. Extensive offline experiments on large-scale industrial datasets validate the superiority of our method. We further deploy our model on Alimama platform with over $100,000$ promoters, achieving a $9.29\\%$ improvement in GMV and a $5.89\\%$ increase in sales volume.",
        "translated": "近年来，联盟营销已成为一种收益共享策略，商家与推广者合作以推广其商品。这种模式不仅提高了商品的曝光度，还使推广者能够获得佣金。本文关注联盟营销中一个关键但尚未充分研究的问题：**准确评估和预测推广者在商品推广中的贡献**。我们设计了一种用于衡量推广者间接贡献的新指标，称为**传播规模（propagation scale）**。然而，由于传播规模受到多种因素影响，且动态场景中存在内在复杂性，现有的时间序列预测技术难以实现准确的预测。为了解决这一问题，我们**将网络结构与节点信号解耦**，并提出了一种两阶段的解决方案：首先，分别预测基本的自主销售与网络结构；随后，合成传播规模。具体而言，我们设计了一种基于后代邻居的图卷积编码方案，并引入超图卷积以高效捕捉复杂的推广动态。此外，我们还引入了三个辅助任务：用于基础估计的自主销售预测、用于合成传播规模的后代节点预测，以及用于缓解高波动性问题的推广者激活预测。我们在大规模工业数据集上进行了广泛的离线实验，验证了我们方法的优越性。我们进一步将模型部署在拥有超过100,000名推广者的Alimama平台，实现了GMV提升9.29%，销售量增长5.89%。"
    },
    {
        "title": "Next Interest Flow: A Generative Pre-training Paradigm for Recommender\n  Systems by Modeling All-domain Movelines",
        "url": "http://arxiv.org/abs/2510.11317v1",
        "pub_date": "2025-10-13",
        "summary": "Click-Through Rate (CTR) prediction, a cornerstone of modern recommender systems, has been dominated by discriminative models that react to past user behavior rather than proactively modeling user intent. Existing generative paradigms attempt to address this but suffer from critical limitations: Large Language Model (LLM) based methods create a semantic mismatch by forcing e-commerce signals into a linguistic space, while ID-based generation is constrained by item memorization and cold-start issues. To overcome these limitations, we propose a novel generative pre-training paradigm. Our model learns to predict the Next Interest Flow, a dense vector sequence representing a user's future intent, while simultaneously modeling its internal Interest Diversity and Interest Evolution Velocity to ensure the representation is both rich and coherent. However, this two-stage approach introduces a critical objective mismatch between the generative and discriminative stages. We resolve this via a bidirectional alignment strategy, which harmonizes the two stages through cross-stage weight initialization and a dynamic Semantic Alignment Module for fine-tuning. Additionally, we enhance the underlying discriminative model with a Temporal Sequential Pairwise (TSP) mechanism to better capture temporal causality. We present the All-domain Moveline Evolution Network (AMEN), a unified framework implementing our entire pipeline. Extensive offline experiments validate AMEN's superiority over strong baselines, and a large-scale online A/B test demonstrates its significant real-world impact, delivering substantial improvements in key business metrics.",
        "translated": "点击率（CTR）预测是现代推荐系统中的核心任务之一，目前主要依赖于判别式模型，这些模型侧重于响应用户过去的行为，而未能主动建模用户的意图。现有的生成式范式尝试解决这一问题，但存在关键的局限性：基于大语言模型（LLM）的方法将电商信号强行映射到语言语义空间，导致语义错配；而基于ID的生成方法则受到物品记忆能力和冷启动问题的限制。为克服这些限制，我们提出了一种新颖的生成式预训练范式。我们的模型旨在预测“下一个兴趣流”（Next Interest Flow），即一个稠密向量序列，用于表示用户未来的兴趣意图。同时，该模型还建模内部的兴趣多样性（Interest Diversity）和兴趣演化速度（Interest Evolution Velocity），以确保表示的丰富性与一致性。然而，这种两阶段的方法在生成阶段与判别阶段之间引入了关键的目标不匹配问题。我们通过双向对齐策略加以解决，该策略通过跨阶段权重初始化和动态语义对齐模块（Semantic Alignment Module）进行微调，从而协调两个阶段之间的差异。此外，我们通过引入时间序列成对机制（Temporal Sequential Pairwise, TSP）来增强底层判别模型，以更好地捕捉时间因果关系。我们提出了一个统一的框架——全领域兴趣演化网络（All-domain Moveline Evolution Network, AMEN），实现了我们完整的流水线。大量离线实验验证了AMEN在强基线模型上的优越性能，而大规模在线A/B测试也展示了其在现实场景中的显著影响，显著提升了关键业务指标。"
    },
    {
        "title": "ELMO: Efficiency via Low-precision and Peak Memory Optimization in Large\n  Output Spaces",
        "url": "http://arxiv.org/abs/2510.11168v1",
        "pub_date": "2025-10-13",
        "summary": "Large output spaces, also referred to as Extreme multilabel classification (XMC), is a setting that arises, e.g., in large-scale tagging and product-to-product recommendation, and is characterized by the number of labels ranging from hundreds of thousands to millions. This means that the linear classification head, usually only a tiny fraction of the overall model, turns into the main driver for compute and memory demand. Current state-of-the-art XMC methods predominantly rely on FP16-FP32 mixed-precision training, which we show can be unstable, and inefficient in terms of memory usage and computational overhead. Meanwhile, existing low-precision methods typically retain higher precision for the classification layer. In this work, we propose ELMO, a pure low-precision training framework for XMC models using BFloat16 and Float8 data types. By leveraging Kahan summation and stochastic rounding, we demonstrate that XMC models can be effectively trained entirely in Float8, without relying on single-precision master weights or tensor scaling. Low-precision training, combined with our proposed memory optimizations -- gradient fusion and chunking -- enables significant reductions in GPU memory usage. For example, we train a 3-million-label XMC model with only 6.6 GiB of GPU memory, compared to the 39.7 GiB required by the optimized SOTA method, Renee without compromising accuracy.",
        "translated": "大规模输出空间，也称为极端多标签分类（Extreme multilabel classification, XMC），是一种在大规模标签分配和商品到商品推荐等场景中常见的设定，其特点是标签数量可高达数十万到数百万级。这意味着线性分类头（通常在整个模型中仅占极小部分）变成了计算和内存需求的主要驱动因素。当前最先进的XMC方法主要依赖FP16与FP32混合精度训练，但我们发现这种方法在训练稳定性、内存使用效率以及计算开销方面存在不足。同时，现有的低精度训练方法通常仍为分类层保留较高精度。在本文中，我们提出ELMO，一个完全基于BFloat16和Float8数据类型的低精度训练框架。通过引入Kahan求和和随机舍入技术，我们证明XMC模型可以完全在Float8精度下进行有效训练，而无需依赖单精度主权重或张量缩放。结合我们提出的内存优化方法——梯度融合和块处理（chunking），该框架能够显著减少GPU内存的使用。例如，我们在仅使用6.6 GiB GPU内存的情况下训练了一个包含300万个标签的XMC模型，而优化后的SOTA方法Renee则需要39.7 GiB的内存，且在不损失精度的前提下实现了这一目标。"
    },
    {
        "title": "DyKnow-RAG: Dynamic Knowledge Utilization Reinforcement Framework for\n  Noisy Retrieval-Augmented Generation in E-commerce Search Relevance",
        "url": "http://arxiv.org/abs/2510.11122v1",
        "pub_date": "2025-10-13",
        "summary": "Accurately modeling query-item relevance drives e-commerce ranking, yet long-tail, knowledge-heavy, and fast-evolving queries exceed parametric LLM coverage. External context (reviews, attribute encyclopedias, UGC) can help but is noisy, and single-pass latency and cost forbid any clean-then-summarize step. The model must, per query, judge relevance and decide whether to use, partially use, or ignore the context. DyKnow-RAG is a dynamic noisy-RAG framework built on Group Relative Policy Optimization. It trains two rollout groups (no external context vs a single retrieved chunk) and applies posterior-driven inter-group advantage scaling that adaptively reweights their contributions by the per-query correctness gap. This teaches when to trust retrieval versus fall back to parametric knowledge, without process labels, value networks, or extra inference passes, preserving single-pass, single-chunk deployment under production latency. Training combines: (1) supervised initialization with a structured rationale that explicitly records the context-usage decision; (2) an RL pool prioritized by SFT uncertainty to focus where context choice is most consequential; and (3) an optional lightweight DPO warm start to stabilize with-context calibration. Under a unified retrieval/index and fixed latency budget, DyKnow-RAG outperforms SFT, DPO, and vanilla GRPO in offline tests, and delivers consistent lifts on GSB, Query Goodrate, and Item Goodrate in Taobao A/B testing. It is deployed in Taobao's production relevance system, serving live traffic. To our knowledge, it is among the first single-pass RAG solutions for e-commerce relevance, turning noisy external signals into reliable gains without added online complexity.",
        "translated": "准确地建模查询与商品的相关性对于电商排序至关重要，然而长尾、知识密集型和快速变化的查询超出了参数化大语言模型的覆盖范围。外部上下文（如商品评论、属性百科、用户生成内容）虽然可以提供帮助，但往往包含噪声，且单次推理的延迟和成本限制了任何“清理再摘要”的步骤。因此，模型必须在每次查询中判断相关性，并决定是否使用、部分使用或忽略外部上下文。\n\nDyKnow-RAG 是一种基于组相对策略优化（Group Relative Policy Optimization）的动态噪声-RAG框架。该框架通过训练两个 rollout 组（一组不使用外部上下文，另一组使用一个检索到的 chunk）并采用后验驱动的组间优势缩放方法，以查询间的准确性差距自适应地重新加权两组的贡献。这种方法能够在无需过程标签、价值网络或额外推理步骤的前提下，学习何时信任检索结果，何时回退至参数化知识，从而在生产延迟下保持单次推理、单 chunk 的部署效率。\n\nDyKnow-RAG 的训练结合了以下三个阶段：（1）结构化理由监督初始化，显式记录上下文使用决策；（2）以监督微调（SFT）的不确定性为优先级的强化学习（RL）池，集中于上下文选择最具影响的场景；（3）一个可选的轻量级 DPO 预训练阶段，用于稳定上下文相关校准。\n\n在统一的检索/索引设置和固定的延迟预算下，DyKnow-RAG 在离线测试中优于 SFT、DPO 和标准 GRPO。在淘宝的 A/B 测试中，该方法在 GSB、Query Goodrate 和 Item Goodrate 等指标上均实现了持续的提升。目前，DyKnow-RAG 已部署于淘宝的生产相关性系统中，服务实时流量。据我们所知，这是首批针对电商相关性问题的单次推理 RAG 解决方案之一，能够在不增加在线复杂度的前提下，将噪声外部信号转化为可靠收益。"
    },
    {
        "title": "Zero-Shot CFC: Fast Real-World Image Denoising based on Cross-Frequency\n  Consistency",
        "url": "http://arxiv.org/abs/2510.12646v1",
        "pub_date": "2025-10-14",
        "summary": "Zero-shot denoisers address the dataset dependency of deep-learning-based denoisers, enabling the denoising of unseen single images. Nonetheless, existing zero-shot methods suffer from long training times and rely on the assumption of noise independence and a zero-mean property, limiting their effectiveness in real-world denoising scenarios where noise characteristics are more complicated. This paper proposes an efficient and effective method for real-world denoising, the Zero-Shot denoiser based on Cross-Frequency Consistency (ZSCFC), which enables training and denoising with a single noisy image and does not rely on assumptions about noise distribution. Specifically, image textures exhibit position similarity and content consistency across different frequency bands, while noise does not. Based on this property, we developed cross-frequency consistency loss and an ultralight network to realize image denoising. Experiments on various real-world image datasets demonstrate that our ZSCFC outperforms other state-of-the-art zero-shot methods in terms of computational efficiency and denoising performance.",
        "translated": "零样本去噪器解决了基于深度学习的去噪器对数据集的依赖问题，使其能够对未见过的单张图像进行去噪。然而，现有的零样本方法训练时间较长，并且依赖于噪声独立性和零均值的假设，这在现实世界中噪声特性更为复杂的情况下限制了其去噪效果。本文提出了一种高效且有效的现实场景去噪方法——基于跨频段一致性的零样本去噪器（Zero-Shot Denoiser based on Cross-Frequency Consistency, ZSCFC），该方法仅需单张含噪图像即可完成训练与去噪，且不依赖于对噪声分布的假设。具体而言，图像的纹理在不同频段中表现出位置相似性和内容一致性，而噪声则不具备这一特性。基于该特性，我们设计了跨频段一致性损失函数，并构建了一个超轻量级网络以实现图像去噪。在多个现实世界图像数据集上的实验表明，与其它最先进的零样本方法相比，ZSCFC在计算效率和去噪性能方面均表现出色。"
    },
    {
        "title": "Normalization-equivariant Diffusion Models: Learning Posterior Samplers\n  From Noisy And Partial Measurements",
        "url": "http://arxiv.org/abs/2510.11964v1",
        "pub_date": "2025-10-13",
        "summary": "Diffusion models (DMs) have rapidly emerged as a powerful framework for image generation and restoration. However, existing DMs are primarily trained in a supervised manner by using a large corpus of clean images. This reliance on clean data poses fundamental challenges in many real-world scenarios, where acquiring noise-free data is hard or infeasible, and only noisy and potentially incomplete measurements are available. While some methods can train DMs using noisy data, they are generally effective only when the amount of noise is very mild or when some additional noise-free data is available. In addition, existing methods for training DMs from incomplete measurements require access to multiple complementary acquisition processes, an assumption that poses a significant practical limitation. Here we introduce the first approach for learning DMs for image restoration using only noisy measurement data from a single operator. As a first key contribution, we show that DMs, and more broadly minimum mean squared error denoisers, exhibit a weak form of scale equivariance linking rescaling in signal amplitude to changes in noise intensity. We then leverage this theoretical insight to develop a denoising score-matching strategy that generalizes robustly to noise levels lower than those present in the training data, thereby enabling the learning of DMs from noisy measurements. To further address the challenges of incomplete and noisy data, we integrate our method with equivariant imaging, a complementary self-supervised learning framework that exploits the inherent invariants of imaging problems, to train DMs for image restoration from single-operator measurements that are both incomplete and noisy. We validate the effectiveness of our approach through extensive experiments on image denoising, demosaicing, and inpainting, along with comparisons with the state of the art.",
        "translated": "扩散模型（DMs）已迅速成为图像生成与修复的强大框架。然而，现有的扩散模型主要依赖于大量干净图像的监督训练。这种对干净数据的依赖在许多现实场景中带来了根本性挑战，因为在这些场景中获取无噪声数据困难或不可行，仅有噪声干扰且可能不完整的测量数据可用。尽管已有方法尝试使用噪声数据训练扩散模型，但它们通常仅在噪声非常轻微或存在部分无噪声数据时才有效。此外，目前基于不完整测量数据训练扩散模型的方法通常需要多个互补的采集过程，这一假设在实践中构成了显著的限制。本文首次提出了一种仅使用单一操作算子的噪声测量数据来学习图像修复扩散模型的方法。作为我们的第一个关键贡献，我们证明了扩散模型，以及更广泛的最小均方误差去噪器，表现出一种弱形式的尺度等变性（scale equivariance），将信号幅值的缩放与噪声强度的变化联系起来。我们随后利用这一理论洞见，提出一种去噪得分匹配策略，该策略能够稳健地推广到训练数据中噪声水平更低的情况，从而实现基于噪声测量数据的扩散模型训练。为了进一步应对数据不完整和噪声的挑战，我们将该方法与等变成像（equivariant imaging）相结合，这是一种互补的自监督学习框架，利用了成像问题中的固有不变性，从而实现基于单一操作算子所获取的不完整且噪声干扰数据的图像修复扩散模型训练。我们在图像去噪、色彩插值（demosaicing）和修复（inpainting）任务上进行了广泛的实验验证，并与当前最先进的方法进行了对比。"
    },
    {
        "title": "Enabling High-Quality In-the-Wild Imaging from Severely Aberrated\n  Metalens Bursts",
        "url": "http://arxiv.org/abs/2510.10083v1",
        "pub_date": "2025-10-11",
        "summary": "We tackle the challenge of robust, in-the-wild imaging using ultra-thin nanophotonic metalens cameras. Meta-lenses, composed of planar arrays of nanoscale scatterers, promise dramatic reductions in size and weight compared to conventional refractive optics. However, severe chromatic aberration, pronounced light scattering, narrow spectral bandwidth, and low light efficiency continue to limit their practical adoption. In this work, we present an end-to-end solution for in-the-wild imaging that pairs a metalens several times thinner than conventional optics with a bespoke multi-image restoration framework optimized for practical metalens cameras. Our method centers on a lightweight convolutional network paired with a memory-efficient burst fusion algorithm that adaptively corrects noise, saturation clipping, and lens-induced distortions across rapid sequences of extremely degraded metalens captures. Extensive experiments on diverse, real-world handheld captures demonstrate that our approach consistently outperforms existing burst-mode and single-image restoration techniques.These results point toward a practical route for deploying metalens-based cameras in everyday imaging applications.",
        "translated": "我们针对使用超薄纳米光学金属透镜相机在真实环境下的鲁棒成像问题提出了一个解决方案。金属透镜由平面排列的纳米级散射体构成，相较于传统的折射式光学元件，有望显著减小尺寸和重量。然而，严重的色差、明显的光散射、狭窄的光谱带宽以及低光效率等问题仍限制了其实际应用。在本工作中，我们提出了一种端到端的真实环境下成像方案，该方案结合了一种比传统光学元件薄几倍的金属透镜，以及一种为实际金属透镜相机量身定制、高效的多图像复原框架。我们的方法核心是一个轻量级卷积网络，结合了一种内存高效的图像序列融合算法，能够自适应地校正在快速拍摄的严重退化金属透镜图像序列中出现的噪声、饱和裁剪和透镜引起的失真。我们在多种实际手持拍摄数据上进行了广泛的实验，结果表明，我们的方法在性能上持续优于现有的突发模式和单图像复原技术。这些结果表明，基于金属透镜的相机在日常成像应用中具有实际可行的部署路径。"
    },
    {
        "title": "Denoising Diffusion as a New Framework for Underwater Images",
        "url": "http://arxiv.org/abs/2510.09934v1",
        "pub_date": "2025-10-11",
        "summary": "Underwater images play a crucial role in ocean research and marine environmental monitoring since they provide quality information about the ecosystem. However, the complex and remote nature of the environment results in poor image quality with issues such as low visibility, blurry textures, color distortion, and noise. In recent years, research in image enhancement has proven to be effective but also presents its own limitations, like poor generalization and heavy reliance on clean datasets. One of the challenges herein is the lack of diversity and the low quality of images included in these datasets. Also, most existing datasets consist only of monocular images, a fact that limits the representation of different lighting conditions and angles. In this paper, we propose a new plan of action to overcome these limitations. On one hand, we call for expanding the datasets using a denoising diffusion model to include a variety of image types such as stereo, wide-angled, macro, and close-up images. On the other hand, we recommend enhancing the images using Controlnet to evaluate and increase the quality of the corresponding datasets, and hence improve the study of the marine ecosystem.   Tags - Underwater Images, Denoising Diffusion, Marine ecosystem, Controlnet",
        "translated": "水下图像在海洋研究和海洋环境监测中起着至关重要的作用，因为它们提供了有关生态系统的重要信息。然而，由于环境的复杂性和远离人类的特性，所获取的图像质量通常较差，存在诸如能见度低、纹理模糊、颜色失真和噪声等问题。近年来，图像增强的研究已被证明是有效的，但也存在自身局限性，例如泛化能力差和对干净数据集的依赖性较强。其中一大挑战是这些数据集中图像的多样性不足且质量偏低。此外，大多数现有数据集仅包含单目图像，这一事实限制了不同光照条件和角度的表征能力。本文中，我们提出了一种新的应对方案以克服这些局限性。一方面，我们建议使用去噪扩散模型（denoising diffusion model）扩展数据集，包含诸如双目、广角、微距和特写等多种图像类型；另一方面，我们推荐使用 ControlNet 对图像进行增强，以评估并提升相应数据集的质量，从而促进对海洋生态系统的研究。  \n标签 - 水下图像，去噪扩散，海洋生态系统，ControlNet"
    },
    {
        "title": "Defense against Unauthorized Distillation in Image Restoration via\n  Feature Space Perturbation",
        "url": "http://arxiv.org/abs/2510.08925v1",
        "pub_date": "2025-10-10",
        "summary": "Knowledge distillation (KD) attacks pose a significant threat to deep model intellectual property by enabling adversaries to train student networks using a teacher model's outputs. While recent defenses in image classification have successfully disrupted KD by perturbing output probabilities, extending these methods to image restoration is difficult. Unlike classification, restoration is a generative task with continuous, high-dimensional outputs that depend on spatial coherence and fine details. Minor perturbations are often insufficient, as students can still learn the underlying mapping.To address this, we propose Adaptive Singular Value Perturbation (ASVP), a runtime defense tailored for image restoration models. ASVP operates on internal feature maps of the teacher using singular value decomposition (SVD). It amplifies the topk singular values to inject structured, high-frequency perturbations, disrupting the alignment needed for distillation. This hinders student learning while preserving the teacher's output quality.We evaluate ASVP across five image restoration tasks: super-resolution, low-light enhancement, underwater enhancement, dehazing, and deraining. Experiments show ASVP reduces student PSNR by up to 4 dB and SSIM by 60-75%, with negligible impact on the teacher's performance. Compared to prior methods, ASVP offers a stronger and more consistent defense.Our approach provides a practical solution to protect open-source restoration models from unauthorized knowledge distillation.",
        "translated": "知识蒸馏（Knowledge Distillation, KD）攻击通过使对手能够利用教师模型的输出来训练学生网络，对深度模型的知识产权构成了重大威胁。尽管图像分类领域的最新防御方法已成功通过扰动输出概率来破坏知识蒸馏，但将这些方法扩展到图像恢复任务却面临困难。与分类任务不同，图像恢复是一个生成任务，其输出是连续且高维的，依赖于空间一致性和细节质量。因此，微小的扰动往往不足以阻止知识蒸馏，因为学生模型仍能学习到潜在的映射关系。\n\n为了解决这一问题，我们提出了一种专为图像恢复模型设计的运行时防御方法——自适应奇异值扰动（Adaptive Singular Value Perturbation, ASVP）。ASVP 通过奇异值分解（Singular Value Decomposition, SVD）对教师模型的内部特征图进行操作。该方法通过放大前k个奇异值，注入结构化的高频扰动，从而破坏蒸馏过程中所需的对齐关系。这种扰动在阻碍学生模型学习的同时，保持了教师模型的输出质量。\n\n我们在五个图像恢复任务中评估了 ASVP：超分辨率、低光增强、水下增强、去雾和去雨。实验结果表明，ASVP 最多可使学生模型的 PSNR 降低 4 dB，SSIM 降低 60-75%，而对教师模型的性能影响几乎可以忽略。与现有方法相比，ASVP 提供了更强且更一致的防御效果。我们的方法为保护开源图像恢复模型免受未经授权的知识蒸馏提供了一个实用的解决方案。"
    },
    {
        "title": "Latent Harmony: Synergistic Unified UHD Image Restoration via Latent\n  Space Regularization and Controllable Refinement",
        "url": "http://arxiv.org/abs/2510.07961v2",
        "pub_date": "2025-10-09",
        "summary": "Ultra-High Definition (UHD) image restoration faces a trade-off between computational efficiency and high-frequency detail retention. While Variational Autoencoders (VAEs) improve efficiency via latent-space processing, their Gaussian constraint often discards degradation-specific high-frequency information, hurting reconstruction fidelity. To overcome this, we propose Latent Harmony, a two-stage framework that redefines VAEs for UHD restoration by jointly regularizing the latent space and enforcing high-frequency-aware reconstruction.In Stage One, we introduce LH-VAE, which enhances semantic robustness through visual semantic constraints and progressive degradation perturbations, while latent equivariance strengthens high-frequency reconstruction.Stage Two jointly trains this refined VAE with a restoration model using High-Frequency Low-Rank Adaptation (HF-LoRA): an encoder LoRA guided by a fidelity-oriented high-frequency alignment loss to recover authentic details, and a decoder LoRA driven by a perception-oriented loss to synthesize realistic textures. Both LoRA modules are trained via alternating optimization with selective gradient propagation to preserve the pretrained latent structure.At inference, a tunable parameter {\\alpha} enables flexible fidelity-perception trade-offs.Experiments show Latent Harmony achieves state-of-the-art performance across UHD and standard-resolution tasks, effectively balancing efficiency, perceptual quality, and reconstruction accuracy.",
        "translated": "超高分辨率（UHD）图像修复面临计算效率与高频细节保留之间的权衡。尽管变分自编码器（VAEs）通过潜在空间处理提升了效率，但其高斯约束常常会丢弃与退化相关的高频信息，从而损害重建的保真度。为了解决这一问题，我们提出了**Latent Harmony**，一种两阶段框架，通过联合正则化潜在空间并强制实现高频感知的重建，重新定义了用于UHD图像修复的VAEs。  \n\n在第一阶段中，我们引入了**LH-VAE**，该模型通过视觉语义约束和渐进式退化扰动来增强语义鲁棒性，同时利用潜在等变性来强化高频信息的重建。  \n\n在第二阶段中，我们将优化后的VAE与一个修复模型联合训练，采用**高频低秩适配（High-Frequency Low-Rank Adaptation, HF-LoRA）**方法：一个编码器LoRA通过以保真度为导向的高频对齐损失（high-frequency alignment loss）来恢复真实细节，而一个解码器LoRA则由以感知为导向的损失驱动，以合成逼真的纹理。两个LoRA模块通过交替优化与选择性梯度传播进行训练，从而保持预训练的潜在结构不变。  \n\n在推理阶段，一个可调节的参数 $\\alpha$ 可实现保真度与感知质量之间的灵活权衡。实验表明，**Latent Harmony**在UHD和标准分辨率任务中均达到了最先进的性能，有效平衡了计算效率、感知质量与重建精度。"
    },
    {
        "title": "PhyDAE: Physics-Guided Degradation-Adaptive Experts for All-in-One\n  Remote Sensing Image Restoration",
        "url": "http://arxiv.org/abs/2510.08653v1",
        "pub_date": "2025-10-09",
        "summary": "Remote sensing images inevitably suffer from various degradation factors during acquisition, including atmospheric interference, sensor limitations, and imaging conditions. These complex and heterogeneous degradations pose severe challenges to image quality and downstream interpretation tasks. Addressing limitations of existing all-in-one restoration methods that overly rely on implicit feature representations and lack explicit modeling of degradation physics, this paper proposes Physics-Guided Degradation-Adaptive Experts (PhyDAE). The method employs a two-stage cascaded architecture transforming degradation information from implicit features into explicit decision signals, enabling precise identification and differentiated processing of multiple heterogeneous degradations including haze, noise, blur, and low-light conditions. The model incorporates progressive degradation mining and exploitation mechanisms, where the Residual Manifold Projector (RMP) and Frequency-Aware Degradation Decomposer (FADD) comprehensively analyze degradation characteristics from manifold geometry and frequency perspectives. Physics-aware expert modules and temperature-controlled sparse activation strategies are introduced to enhance computational efficiency while ensuring imaging physics consistency. Extensive experiments on three benchmark datasets (MD-RSID, MD-RRSHID, and MDRS-Landsat) demonstrate that PhyDAE achieves superior performance across all four restoration tasks, comprehensively outperforming state-of-the-art methods. Notably, PhyDAE substantially improves restoration quality while achieving significant reductions in parameter count and computational complexity, resulting in remarkable efficiency gains compared to mainstream approaches and achieving optimal balance between performance and efficiency. Code is available at https://github.com/HIT-SIRS/PhyDAE.",
        "translated": "遥感图像在获取过程中不可避免地受到多种退化因素的影响，包括大气干扰、传感器限制以及成像条件等。这些复杂且异质的退化现象给图像质量以及后续的语义解析任务带来了严重挑战。为了解决现有端到端图像修复方法中对隐式特征表示的过度依赖以及缺乏对退化物理机制的显式建模等问题，本文提出了一种物理引导的退化自适应专家模型（Physics-Guided Degradation-Adaptive Experts, PhyDAE）。该方法采用两阶段级联架构，将隐式特征中的退化信息转化为显式的决策信号，从而实现对多种异质退化（包括雾霾、噪声、模糊和低光照条件）的精确识别与差异化处理。模型引入了渐进式退化挖掘与利用机制，其中残差流形投影器（Residual Manifold Projector, RMP）和频域感知退化解耦模块（Frequency-Aware Degradation Decomposer, FADD）分别从流形几何结构和频域角度对退化特性进行全面分析。此外，通过引入物理感知的专家模块和温度控制的稀疏激活策略，在保证成像物理一致性的同时显著提升了计算效率。在三个基准数据集（MD-RSID、MD-RRSHID 和 MDRS-Landsat）上的大量实验表明，PhyDAE 在四项图像修复任务中均表现出优越的性能，全面超越现有最先进方法。特别值得一提的是，PhyDAE 在大幅提升修复质量的同时，显著减少了模型参数数量与计算复杂度，从而在主流方法中实现了性能与效率之间的最优平衡。代码可在 https://github.com/HIT-SIRS/PhyDAE 获取。"
    },
    {
        "title": "DeRainMamba: A Frequency-Aware State Space Model with Detail Enhancement\n  for Image Deraining",
        "url": "http://arxiv.org/abs/2510.06746v1",
        "pub_date": "2025-10-08",
        "summary": "Image deraining is crucial for improving visual quality and supporting reliable downstream vision tasks. Although Mamba-based models provide efficient sequence modeling, their limited ability to capture fine-grained details and lack of frequency-domain awareness restrict further improvements. To address these issues, we propose DeRainMamba, which integrates a Frequency-Aware State-Space Module (FASSM) and Multi-Directional Perception Convolution (MDPConv). FASSM leverages Fourier transform to distinguish rain streaks from high-frequency image details, balancing rain removal and detail preservation. MDPConv further restores local structures by capturing anisotropic gradient features and efficiently fusing multiple convolution branches. Extensive experiments on four public benchmarks demonstrate that DeRainMamba consistently outperforms state-of-the-art methods in PSNR and SSIM, while requiring fewer parameters and lower computational costs. These results validate the effectiveness of combining frequency-domain modeling and spatial detail enhancement within a state-space framework for single image deraining.",
        "translated": "图像去雨对于提升视觉质量和支持可靠的下游视觉任务至关重要。尽管基于Mamba的模型在序列建模方面具有较高的效率，但其在捕捉细粒度细节方面的能力有限以及缺乏频域感知，限制了进一步的性能提升。为解决这些问题，我们提出DeRainMamba，该方法融合了一个频域感知状态空间模块（Frequency-Aware State-Space Module, FASSM）和多方向感知卷积（Multi-Directional Perception Convolution, MDPConv）。FASSM利用傅里叶变换区分雨痕与图像中的高频细节，从而在去雨与细节保留之间取得平衡。MDPConv通过捕捉各向异性梯度特征并高效融合多分支卷积，进一步恢复局部结构。我们在四个公开基准数据集上进行了广泛的实验，结果表明DeRainMamba在PSNR和SSIM指标上始终优于当前最先进的方法，同时参数量更少，计算成本更低。这些结果验证了在状态空间框架中结合频域建模与空间细节增强对于单图像去雨的有效性。"
    },
    {
        "title": "An Inertial Langevin Algorithm",
        "url": "http://arxiv.org/abs/2510.06723v1",
        "pub_date": "2025-10-08",
        "summary": "We present a novel method for drawing samples from Gibbs distributions with densities of the form $\\pi(x) \\propto \\exp(-U(x))$. The method accelerates the unadjusted Langevin algorithm by introducing an inertia term similar to Polyak's heavy ball method, together with a corresponding noise rescaling. Interpreting the scheme as a discretization of \\emph{kinetic} Langevin dynamics, we prove ergodicity (in continuous and discrete time) for twice continuously differentiable, strongly convex, and $L$-smooth potentials and bound the bias of the discretization to the target in Wasserstein-2 distance. In particular, the presented proofs allow for smaller friction parameters in the kinetic Langevin diffusion compared to existing literature. Moreover, we show the close ties of the proposed method to the over-relaxed Gibbs sampler. The scheme is tested in an extensive set of numerical experiments covering simple toy examples, total variation image denoising, and the complex task of maximum likelihood learning of an energy-based model for molecular structure generation. The experimental results confirm the acceleration provided by the proposed scheme even beyond the strongly convex and $L$-smooth setting.",
        "translated": "我们提出了一种从形式为 $\\pi(x) \\propto \\exp(-U(x))$ 的 Gibbs 分布中抽样的新方法。该方法通过引入类似于 Polyak 重球法的惯性项以及相应的噪声重缩放机制，加速了未调整的 Langevin 算法。将该方法解释为对 \\emph{动能} Langevin 动力学的离散化，我们证明了在连续和离散时间下，对于二次连续可微、强凸且 $L$-光滑的势函数，该方法具有遍历性，并在 Wasserstein-2 距离下对该离散化方案与目标分布之间的偏差进行了上界分析。特别地，所提出的证明允许在动能 Langevin 扩散中使用比现有文献中更小的摩擦参数。此外，我们展示了该方法与过松弛 Gibbs 抽样器之间的紧密联系。该算法在一个广泛的数值实验中进行了测试，涵盖简单的玩具示例、图像的全变分去噪任务，以及分子结构生成的能量基模型的最大似然学习这一复杂任务。实验结果验证了所提出方法即使在非强凸和非 $L$-光滑的设置下仍能提供加速效果。"
    },
    {
        "title": "AIM 2025 Challenge on Real-World RAW Image Denoising",
        "url": "http://arxiv.org/abs/2510.06601v1",
        "pub_date": "2025-10-08",
        "summary": "We introduce the AIM 2025 Real-World RAW Image Denoising Challenge, aiming to advance efficient and effective denoising techniques grounded in data synthesis. The competition is built upon a newly established evaluation benchmark featuring challenging low-light noisy images captured in the wild using five different DSLR cameras. Participants are tasked with developing novel noise synthesis pipelines, network architectures, and training methodologies to achieve high performance across different camera models. Winners are determined based on a combination of performance metrics, including full-reference measures (PSNR, SSIM, LPIPS), and non-reference ones (ARNIQA, TOPIQ). By pushing the boundaries of camera-agnostic low-light RAW image denoising trained on synthetic data, the competition promotes the development of robust and practical models aligned with the rapid progress in digital photography. We expect the competition outcomes to influence multiple domains, from image restoration to night-time autonomous driving.",
        "translated": "我们介绍了AIM 2025真实世界RAW图像去噪挑战赛，旨在通过数据合成推动高效且有效的去噪技术的发展。该竞赛基于一个新的评估基准构建，该基准包含在自然场景下使用五种不同DSLR相机拍摄的具有挑战性的低光照噪声图像。参赛者需要开发新颖的噪声合成流程、网络架构和训练方法，以在不同相机模型上实现高性能的去噪效果。比赛的优胜者将根据多种性能指标综合评定，包括全参考指标（PSNR、SSIM、LPIPS）和非参考指标（ARNIQA、TOPIQ）。通过推动在合成数据上训练的、具有相机泛化能力的低光照RAW图像去噪技术的边界，该竞赛促进了与数字摄影快速进步相契合的鲁棒且实用模型的发展。我们预期本次竞赛的成果将对多个领域产生影响，从图像修复到夜间自动驾驶。"
    },
    {
        "title": "TDiff: Thermal Plug-And-Play Prior with Patch-Based Diffusion",
        "url": "http://arxiv.org/abs/2510.06460v1",
        "pub_date": "2025-10-07",
        "summary": "Thermal images from low-cost cameras often suffer from low resolution, fixed pattern noise, and other localized degradations. Available datasets for thermal imaging are also limited in both size and diversity. To address these challenges, we propose a patch-based diffusion framework (TDiff) that leverages the local nature of these distortions by training on small thermal patches. In this approach, full-resolution images are restored by denoising overlapping patches and blending them using smooth spatial windowing. To our knowledge, this is the first patch-based diffusion framework that models a learned prior for thermal image restoration across multiple tasks. Experiments on denoising, super-resolution, and deblurring demonstrate strong results on both simulated and real thermal data, establishing our method as a unified restoration pipeline.",
        "translated": "低成本热成像相机所获取的图像通常存在分辨率较低、固定模式噪声以及其他局部退化问题。目前可用的热成像数据集在规模和多样性方面也较为有限。为了解决这些挑战，我们提出了一种基于图像块的扩散框架（TDiff），该框架通过在小尺寸热图像块上进行训练，利用这些退化现象的局部特性。在该方法中，通过去噪重叠图像块，并结合平滑的空间窗口函数进行融合，从而恢复全分辨率图像。据我们所知，这是首个基于图像块的扩散框架，能够在多个任务中对热图像的退化建模并学习其先验分布。我们在去噪、超分辨率和去模糊任务上的实验表明，该方法在模拟和真实热图像数据上均取得了优异的效果，从而确立了其作为统一图像恢复流程的地位。"
    },
    {
        "title": "Local MAP Sampling for Diffusion Models",
        "url": "http://arxiv.org/abs/2510.07343v2",
        "pub_date": "2025-10-07",
        "summary": "Diffusion Posterior Sampling (DPS) provides a principled Bayesian approach to inverse problems by sampling from $p(x_0 \\mid y)$. However, in practice, the goal of inverse problem solving is not to cover the posterior but to recover the most accurate reconstruction, where optimization-based diffusion solvers often excel despite lacking a clear probabilistic foundation. We introduce Local MAP Sampling (LMAPS), a new inference framework that iteratively solving local MAP subproblems along the diffusion trajectory. This perspective clarifies their connection to global MAP estimation and DPS, offering a unified probabilistic interpretation for optimization-based methods. Building on this foundation, we develop practical algorithms with a probabilistically interpretable covariance approximation, a reformulated objective for stability and interpretability, and a gradient approximation for non-differentiable operators. Across a broad set of image restoration and scientific tasks, LMAPS achieves state-of-the-art performance, including $\\geq 2$ dB gains on motion deblurring, JPEG restoration, and quantization, and $&gt;1.5$ dB improvements on inverse scattering benchmarks.",
        "translated": "扩散后验采样（Diffusion Posterior Sampling, DPS）通过从 $p(x_0 \\mid y)$ 中采样，为逆问题提供了一种基于贝叶斯原理的求解方法。然而，实际上，解决逆问题的目标并非是覆盖后验分布，而是恢复最精确的重建结果，在这一点上，基于优化的扩散求解器往往表现出色，尽管它们缺乏明确的概率基础。我们提出一种新的推理框架——局部最大后验采样（Local MAP Sampling, LMAPS），该方法沿着扩散轨迹迭代求解局部最大后验（MAP）子问题。这一视角明确了LMAPS与全局MAP估计以及DPS之间的关系，为基于优化的方法提供了一种统一的概率解释。基于这一理论基础，我们开发了具有概率解释的协方差近似方法，提出了一个用于提高稳定性和可解释性的重构目标函数，并引入了针对不可微操作符的梯度近似方法。在广泛的图像恢复和科学计算任务中，LMAPS实现了最先进的性能，包括在运动去模糊、JPEG图像恢复和量化任务中获得了 $\\geq 2$ dB 的提升，在逆散射基准测试中实现了 $>1.5$ dB 的性能改进。"
    },
    {
        "title": "Rasterized Steered Mixture of Experts for Efficient 2D Image Regression",
        "url": "http://arxiv.org/abs/2510.05814v1",
        "pub_date": "2025-10-07",
        "summary": "The Steered Mixture of Experts regression framework has demonstrated strong performance in image reconstruction, compression, denoising, and super-resolution. However, its high computational cost limits practical applications. This work introduces a rasterization-based optimization strategy that combines the efficiency of rasterized Gaussian kernel rendering with the edge-aware gating mechanism of the Steered Mixture of Experts. The proposed method is designed to accelerate two-dimensional image regression while maintaining the model's inherent sparsity and reconstruction quality. By replacing global iterative optimization with a rasterized formulation, the method achieves significantly faster parameter updates and more memory-efficient model representations. In addition, the proposed framework supports applications such as native super-resolution and image denoising, which are not directly achievable with standard rasterized Gaussian kernel approaches. The combination of fast rasterized optimization with the edge-aware structure of the Steered Mixture of Experts provides a new balance between computational efficiency and reconstruction fidelity for two-dimensional image processing tasks.",
        "translated": "基于引导的专家混合（Steered Mixture of Experts）回归框架在图像重建、压缩、去噪和超分辨率等任务中已展现出优异的性能。然而，其较高的计算成本限制了其在实际应用中的使用。本文提出了一种基于光栅化的优化策略，结合了光栅化高斯核渲染的高效性与Steered Mixture of Experts中边缘感知门控机制的优势。所提出的方法旨在加速二维图像回归过程，同时保持模型固有的稀疏性与重建质量。通过将全局迭代优化替换为光栅化形式，该方法显著提高了参数更新速度，并实现了更节省内存的模型表示。此外，该框架支持诸如原生超分辨率和图像去噪等应用，而这些是标准光栅化高斯核方法无法直接实现的。快速光栅化优化与Steered Mixture of Experts边缘感知结构的结合，为二维图像处理任务提供了计算效率与重建保真度之间的新平衡。"
    },
    {
        "title": "Adaptive double-phase Rudin--Osher--Fatemi denoising model",
        "url": "http://arxiv.org/abs/2510.04382v1",
        "pub_date": "2025-10-05",
        "summary": "We propose a new image denoising model based on a variable-growth total variation regularization of double-phase type with adaptive weight. It is designed to reduce staircasing with respect to the classical Rudin--Osher--Fatemi model, while preserving the edges of the image in a similar fashion. We implement the model and test its performance on synthetic and natural images in 1D and 2D over a range of noise levels.",
        "translated": "我们提出了一种新的图像去噪模型，该模型基于具有自适应权重的双阶段可变增长总体变差正则化。该模型旨在相较于经典的 Rudin–Osher–Fatemi 模型，减少阶梯效应（staircasing），同时以类似的方式保留图像的边缘。我们对该模型进行了实现，并在 1D 和 2D 的合成图像与自然图像上，针对多种噪声水平进行了性能测试。"
    },
    {
        "title": "Equilibrium Matching: Generative Modeling with Implicit Energy-Based\n  Models",
        "url": "http://arxiv.org/abs/2510.02300v3",
        "pub_date": "2025-10-02",
        "summary": "We introduce Equilibrium Matching (EqM), a generative modeling framework built from an equilibrium dynamics perspective. EqM discards the non-equilibrium, time-conditional dynamics in traditional diffusion and flow-based generative models and instead learns the equilibrium gradient of an implicit energy landscape. Through this approach, we can adopt an optimization-based sampling process at inference time, where samples are obtained by gradient descent on the learned landscape with adjustable step sizes, adaptive optimizers, and adaptive compute. EqM surpasses the generation performance of diffusion/flow models empirically, achieving an FID of 1.90 on ImageNet 256$\\times$256. EqM is also theoretically justified to learn and sample from the data manifold. Beyond generation, EqM is a flexible framework that naturally handles tasks including partially noised image denoising, OOD detection, and image composition. By replacing time-conditional velocities with a unified equilibrium landscape, EqM offers a tighter bridge between flow and energy-based models and a simple route to optimization-driven inference.",
        "translated": "我们提出了一种名为**均衡匹配**（Equilibrium Matching, EqM）的生成建模框架，其构建基于**均衡动力学**的视角。EqM摒弃了传统扩散模型和基于流的生成模型中所依赖的非均衡、时序条件的动力学，转而学习一个隐式能量景观的**均衡梯度**。通过这种方法，我们可以在推理阶段采用基于优化的采样过程，其中样本通过在所学能量景观上进行梯度下降获得，该过程支持可调节的步长、自适应优化器以及自适应计算能力。从实证结果来看，EqM在生成性能上超越了扩散模型和流模型，在ImageNet 256$\\times$256数据集上达到了1.90的FID分数。EqM在理论上也能够从数据流形中进行学习与采样，具有坚实的理论依据。除生成任务外，EqM还是一种灵活的框架，天然地适用于包括部分噪声图像去噪、分布外（OOD）检测以及图像合成在内的多种任务。通过将时序条件的速度替换为统一的均衡景观，EqM在流模型与基于能量的模型之间架起了更紧密的桥梁，并为实现优化驱动的推理提供了一条简洁的路径。"
    },
    {
        "title": "HyMiRec: A Hybrid Multi-interest Learning Framework for LLM-based\n  Sequential Recommendation",
        "url": "http://arxiv.org/abs/2510.13738v1",
        "pub_date": "2025-10-15",
        "summary": "Large language models (LLMs) have recently demonstrated strong potential for sequential recommendation. However, current LLM-based approaches face critical limitations in modeling users' long-term and diverse interests. First, due to inference latency and feature fetching bandwidth constraints, existing methods typically truncate user behavior sequences to include only the most recent interactions, resulting in the loss of valuable long-range preference signals. Second, most current methods rely on next-item prediction with a single predicted embedding, overlooking the multifaceted nature of user interests and limiting recommendation diversity. To address these challenges, we propose HyMiRec, a hybrid multi-interest sequential recommendation framework, which leverages a lightweight recommender to extracts coarse interest embeddings from long user sequences and an LLM-based recommender to captures refined interest embeddings. To alleviate the overhead of fetching features, we introduce a residual codebook based on cosine similarity, enabling efficient compression and reuse of user history embeddings. To model the diverse preferences of users, we design a disentangled multi-interest learning module, which leverages multiple interest queries to learn disentangles multiple interest signals adaptively, allowing the model to capture different facets of user intent. Extensive experiments are conducted on both benchmark datasets and a collected industrial dataset, demonstrating our effectiveness over existing state-of-the-art methods. Furthermore, online A/B testing shows that HyMiRec brings consistent improvements in real-world recommendation systems.",
        "translated": "大语言模型（LLMs）最近在序列推荐中展现出强大的潜力。然而，目前基于LLM的方法在建模用户长期和多样化兴趣方面面临关键的限制。首先，由于推理延迟和特征获取带宽的限制，现有方法通常截断用户行为序列，仅包含最近的交互，从而丢失了有价值的长距离偏好信号。其次，大多数当前方法依赖于单一预测嵌入的下一物品预测，忽视了用户兴趣的多面性，限制了推荐的多样性。为了解决这些挑战，我们提出HyMiRec，一种混合多兴趣序列推荐框架，其利用一个轻量级推荐器从长用户序列中提取粗兴趣嵌入，并利用一个基于LLM的推荐器捕捉精兴趣嵌入。为了缓解特征获取带来的开销，我们引入了一个基于余弦相似度的残差码本，从而实现了用户历史嵌入的高效压缩和重复使用。为了建模用户的多样化偏好，我们设计了一个解耦的多兴趣学习模块，该模块利用多个兴趣查询自适应地学习解耦的多个兴趣信号，使得模型能够捕捉用户意图的不同方面。我们在基准数据集和收集的工业数据集上进行了广泛的实验，验证了该方法相较于现有最先进方法的有效性。此外，在线A/B测试表明，HyMiRec在实际推荐系统中带来了持续的性能提升。",
        "translated_title": "HyMiRec：一种基于大语言模型的序列推荐混合多兴趣学习框架",
        "label": [
            "LLM生成式推荐",
            "序列推荐",
            "通用推荐技术"
        ],
        "label_reason": "结合LLM与多兴趣学习的序列推荐框架，直接解决推荐系统核心问题",
        "relevance_score": 9
    },
    {
        "title": "RAG Meets Temporal Graphs: Time-Sensitive Modeling and Retrieval for\n  Evolving Knowledge",
        "url": "http://arxiv.org/abs/2510.13590v1",
        "pub_date": "2025-10-15",
        "summary": "Knowledge is inherently time-sensitive and continuously evolves over time. Although current Retrieval-Augmented Generation (RAG) systems enrich LLMs with external knowledge, they largely ignore this temporal nature. This raises two challenges for RAG. First, current RAG methods lack effective time-aware representations. Same facts of different time are difficult to distinguish with vector embeddings or conventional knowledge graphs. Second, most RAG evaluations assume a static corpus, leaving a blind spot regarding update costs and retrieval stability as knowledge evolves. To make RAG time-aware, we propose Temporal GraphRAG (TG-RAG), which models external corpora as a bi-level temporal graph consisting of a temporal knowledge graph with timestamped relations and a hierarchical time graph. Multi-granularity temporal summaries are generated for each time node to capture both key events and broader trends at that time. The design supports incremental updates by extracting new temporal facts from the incoming corpus and merging them into the existing graph. The temporal graph explicitly represents identical facts at different times as distinct edges to avoid ambiguity, and the time hierarchy graph allows only generating reports for new leaf time nodes and their ancestors, ensuring effective and efficient updates. During inference, TG-RAG dynamically retrieves a subgraph within the temporal and semantic scope of the query, enabling precise evidence gathering. Moreover, we introduce ECT-QA, a time-sensitive question-answering dataset featuring both specific and abstract queries, along with a comprehensive evaluation protocol designed to assess incremental update capabilities of RAG systems. Extensive experiments show that TG-RAG significantly outperforms existing baselines, demonstrating the effectiveness of our method in handling temporal knowledge and incremental updates.",
        "translated": "知识本质上是时间敏感的，并且会随着时间不断演化。尽管当前的检索增强生成（RAG）系统通过引入外部知识丰富了大语言模型（LLM），但它们在很大程度上忽视了这种时间特性。这引发了RAG面临的两个挑战。首先，当前的RAG方法缺乏有效的时间感知表示。不同时间下的相同事实，难以通过向量嵌入（embedding）或传统知识图谱进行区分。其次，大多数RAG评估假设语料库是静态的，从而忽略了知识演化过程中的更新成本和检索稳定性问题。为了使RAG具备时间感知能力，我们提出了时序图RAG（Temporal GraphRAG，TG-RAG），该方法将外部语料库建模为一个双层时序图，包含一个具有时间戳关系的时序知识图谱和一个层次化时间图。为每个时间节点生成多粒度时序摘要，以捕捉该时间点的关键事件和更广泛的趋势。该设计通过从新输入的语料中提取新的时序事实并将其合并到现有图中，支持增量更新。时序图显式地将不同时刻的相同事实表示为不同的边，以避免歧义，而时间层次图则仅允许为新的叶子时间节点及其祖先生成报告，从而确保更新的有效性和高效性。在推理阶段，TG-RAG能够动态检索与查询在时间和语义范围内匹配的子图，实现精确的证据收集。此外，我们引入了一个时间敏感的问答数据集ECT-QA，包含具体和抽象的查询，并设计了一套全面的评估协议，用于评估RAG系统的增量更新能力。大量实验表明，TG-RAG显著优于现有基线，验证了我们方法在处理时序知识和增量更新方面的有效性。",
        "translated_title": "RAG与时间图的结合：面向动态知识的时间敏感建模与召回",
        "label": [
            "LLM生成式推荐",
            "多模态推荐",
            "推荐系统评估"
        ],
        "label_reason": "论文提出时间敏感的RAG方法，适用于生成式推荐中的动态知识建模与评估。",
        "relevance_score": 7
    },
    {
        "title": "MADREC: A Multi-Aspect Driven LLM Agent for Explainable and Adaptive\n  Recommendation",
        "url": "http://arxiv.org/abs/2510.13371v1",
        "pub_date": "2025-10-15",
        "summary": "Recent attempts to integrate large language models (LLMs) into recommender systems have gained momentum, but most remain limited to simple text generation or static prompt-based inference, failing to capture the complexity of user preferences and real-world interactions. This study proposes the Multi-Aspect Driven LLM Agent MADRec, an autonomous LLM-based recommender that constructs user and item profiles by unsupervised extraction of multi-aspect information from reviews and performs direct recommendation, sequential recommendation, and explanation generation. MADRec generates structured profiles via aspect-category-based summarization and applies Re-Ranking to construct high-density inputs. When the ground-truth item is missing from the output, the Self-Feedback mechanism dynamically adjusts the inference criteria. Experiments across multiple domains show that MADRec outperforms traditional and LLM-based baselines in both precision and explainability, with human evaluation further confirming the persuasiveness of the generated explanations.",
        "translated": "近期尝试将大语言模型（LLMs）集成到推荐系统中的工作逐渐增多，但大多数仍局限于简单的文本生成或基于静态提示的推理，未能捕捉用户偏好和现实世界交互的复杂性。本研究提出多方面驱动的大语言模型代理 MADRec，这是一种基于大语言模型的自主推荐系统，通过从评论中无监督提取多方面信息构建用户和物料画像，并执行直接推荐、序列推荐以及解释生成。MADRec 通过基于方面类别的摘要生成结构化画像，并应用重排（Re-Ranking）构造高密度输入。当输出中缺少真实物料时，自反馈（Self-Feedback）机制会动态调整推理标准。跨多个领域的实验表明，MADRec 在准确性和可解释性方面均优于传统方法和基于 LLM 的基线模型，人工评估进一步验证了所生成解释的说服力。",
        "translated_title": "MADREC：一种面向多方面驱动的可解释且自适应的推荐大语言模型代理",
        "label": [
            "LLM生成式推荐",
            "精排",
            "重排",
            "推荐系统可解释性"
        ],
        "label_reason": "基于LLM的推荐与可解释性，融合重排与反馈机制",
        "relevance_score": 9
    },
    {
        "title": "Improving Visual Recommendation on E-commerce Platforms Using\n  Vision-Language Models",
        "url": "http://arxiv.org/abs/2510.13359v1",
        "pub_date": "2025-10-15",
        "summary": "On large-scale e-commerce platforms with tens of millions of active monthly users, recommending visually similar products is essential for enabling users to efficiently discover items that align with their preferences. This study presents the application of a vision-language model (VLM) -- which has demonstrated strong performance in image recognition and image-text retrieval tasks -- to product recommendations on Mercari, a major consumer-to-consumer marketplace used by more than 20 million monthly users in Japan. Specifically, we fine-tuned SigLIP, a VLM employing a sigmoid-based contrastive loss, using one million product image-title pairs from Mercari collected over a three-month period, and developed an image encoder for generating item embeddings used in the recommendation system. Our evaluation comprised an offline analysis of historical interaction logs and an online A/B test in a production environment. In offline analysis, the model achieved a 9.1% improvement in nDCG@5 compared with the baseline. In the online A/B test, the click-through rate improved by 50% whereas the conversion rate improved by 14% compared with the existing model. These results demonstrate the effectiveness of VLM-based encoders for e-commerce product recommendations and provide practical insights into the development of visual similarity-based recommendation systems.",
        "translated": "在拥有数千万活跃月活用户的大型电商平台中，推荐视觉相似产品对于帮助用户高效发现与其偏好一致的物料至关重要。本研究提出了将视觉-语言模型（VLM）应用于Mercari平台的产品推荐，其中VLM已在图像识别和图文检索任务中展现出卓越的性能。Mercari是日本一个重要的C2C电商平台，月活跃用户超过2000万。具体而言，我们使用在三个月内从Mercari收集的包含一百万对产品图像和标题的数据，对采用基于Sigmoid的对比损失的VLM SigLIP进行了微调，并开发了用于生成推荐系统中物料嵌入表示的图像编码器。我们的评估包括对历史交互日志的离线分析以及在生产环境中进行的在线A/B测试。离线分析结果显示，与基线模型相比，该模型在nDCG@5指标上提升了9.1%。在线A/B测试中，点击率提升了50%，转化率提升了14%。这些结果证明了基于VLM编码器在电商产品推荐中的有效性，并为视觉相似性推荐系统的发展提供了实践洞见。",
        "translated_title": "使用视觉-语言模型改进电子商务平台的视觉推荐",
        "label": [
            "多模态推荐",
            "精排",
            "图像相似性推荐"
        ],
        "label_reason": "论文将视觉语言模型用于电商推荐，提升图像相似性产品推荐效果",
        "relevance_score": 8
    },
    {
        "title": "ChatR1: Reinforcement Learning for Conversational Reasoning and\n  Retrieval Augmented Question Answering",
        "url": "http://arxiv.org/abs/2510.13312v1",
        "pub_date": "2025-10-15",
        "summary": "We present ChatR1, a reasoning framework based on reinforcement learning (RL) for conversational question answering (CQA). Reasoning plays an important role in CQA, where user intent evolves across dialogue turns, and utterances are often underspecified, requiring contextual interpretation, query reformulation, and dynamic coordination between retrieval and generation. Unlike static `rewrite, retrieve, and generate' pipelines, ChatR1 interleaves search and reasoning across turns, enabling exploratory and adaptive behaviors learned through RL. To address the challenge of sparse and delayed rewards in RL, we propose an intent-aware reward that provides turn-level feedback by aligning retrieval and reasoning with evolving user goals. Our proposed ChatR1 demonstrates strong performance on both 3B and 7B model backbones, outperforming competitive models on five CQA datasets, measured by different metrics (F1, BERTScore, and LLM-as-judge). We include a diverse set of CQA datasets to cover topic shifts, evolving intents, mixed-initiative dialogues, and multi-document grounding, testing ChatR1's performance from various aspects. Ablation studies confirm the effectiveness of the intent-aware reward. Our analyses further reveal diverse reasoning trajectories and effective use of the search tool. ChatR1 also generalizes robustly across domains, demonstrating that RL-based reasoning enables more flexible and context-sensitive behavior than static CQA pipelines.",
        "translated": "我们提出了 ChatR1，一种基于强化学习（RL）的对话式问答（CQA）推理框架。推理在 CQA 中起着重要作用，因为用户意图会随着对话轮次而演变，且对话内容通常信息不完整，需要上下文解释、查询重构，以及检索与生成之间的动态协调。与静态的“重写、检索、生成”流水线不同，ChatR1 在对话轮次中交替进行搜索和推理，使得通过 RL 学到的探索性和适应性行为得以实现。为了解决 RL 中稀疏和延迟奖励的挑战，我们提出了一种意图感知的奖励机制，通过将检索和推理与用户意图的演变对齐，提供轮次级的反馈。我们提出的 ChatR1 在 3B 和 7B 模型主干上均表现出色，在五个 CQA 数据集上的表现优于多个竞争模型，评估指标包括 F1、BERTScore 和以大语言模型作为评判者。我们纳入了多样化的 CQA 数据集，涵盖主题转换、意图演变、混合倡议对话以及多文档依据，从多个方面测试了 ChatR1 的性能。消融实验验证了意图感知奖励的有效性。进一步的分析还揭示了多样化的推理轨迹和对搜索工具的有效利用。ChatR1 在多个领域上也表现出良好的泛化能力，表明基于 RL 的推理能够实现比静态 CQA 流水线更为灵活和上下文敏感的行为。",
        "translated_title": "ChatR1：会话推理与检索增强问答的强化学习方法",
        "label": [
            "LLM生成式推荐",
            "序列推荐"
        ],
        "label_reason": "论文涉及对话式问答与生成式模型，适用于推荐中的序列建模和生成式推荐。",
        "relevance_score": 7
    },
    {
        "title": "Beyond Static LLM Policies: Imitation-Enhanced Reinforcement Learning\n  for Recommendation",
        "url": "http://arxiv.org/abs/2510.13229v1",
        "pub_date": "2025-10-15",
        "summary": "Recommender systems (RecSys) have become critical tools for enhancing user engagement by delivering personalized content across diverse digital platforms. Recent advancements in large language models (LLMs) demonstrate significant potential for improving RecSys, primarily due to their exceptional generalization capabilities and sophisticated contextual understanding, which facilitate the generation of flexible and interpretable recommendations. However, the direct deployment of LLMs as primary recommendation policies presents notable challenges, including persistent latency issues stemming from frequent API calls and inherent model limitations such as hallucinations and biases. To address these issues, this paper proposes a novel offline reinforcement learning (RL) framework that leverages imitation learning from LLM-generated trajectories. Specifically, inverse reinforcement learning is employed to extract robust reward models from LLM demonstrations. This approach negates the need for LLM fine-tuning, thereby substantially reducing computational overhead. Simultaneously, the RL policy is guided by the cumulative rewards derived from these demonstrations, effectively transferring the semantic insights captured by the LLM. Comprehensive experiments conducted on two benchmark datasets validate the effectiveness of the proposed method, demonstrating superior performance when compared against state-of-the-art RL-based and in-context learning baselines. The code can be found at https://github.com/ArronDZhang/IL-Rec.",
        "translated": "推荐系统（Recommender systems, RecSys）已成为在各种数字平台上提供个性化内容以提升用户参与度的关键工具。近年来，大语言模型（Large language models, LLMs）在推荐系统中的应用展现出显著潜力，主要归功于其出色的泛化能力和复杂上下文理解能力，这些能力有助于生成灵活且可解释的推荐结果。然而，直接将LLMs作为主要的推荐策略部署存在诸多挑战，包括由于频繁调用API而导致的持续性延迟问题，以及模型本身固有的局限性，如幻觉和偏见等。为了解决这些问题，本文提出了一种新颖的离线强化学习（Reinforcement learning, RL）框架，该框架通过模仿学习LLM生成的轨迹来实现。具体而言，采用逆强化学习（inverse reinforcement learning）方法从LLM的演示中提取稳健的奖励模型。这种方法避免了对LLM进行微调的需求，从而显著降低了计算开销。同时，强化学习策略通过这些演示所获得的累积奖励进行指导，有效地将LLM捕捉到的语义信息迁移过来。在两个基准数据集上进行的全面实验验证了所提方法的有效性，其性能优于最先进的基于RL的和上下文学习的基线方法。代码可在 https://github.com/ArronDZhang/IL-Rec 找到。",
        "translated_title": "超越静态大语言模型策略：基于模仿增强的推荐系统强化学习方法",
        "label": [
            "LLM生成式推荐",
            "精排",
            "通用推荐技术"
        ],
        "label_reason": "结合LLM与强化学习优化推荐策略，涉及生成式推荐和策略优化",
        "relevance_score": 9
    },
    {
        "title": "LLM-guided Hierarchical Retrieval",
        "url": "http://arxiv.org/abs/2510.13217v1",
        "pub_date": "2025-10-15",
        "summary": "Modern IR systems are increasingly tasked with answering complex, multi-faceted queries that require deep reasoning rather than simple keyword or semantic matching. While LLM-based IR has shown great promise, the prevailing retrieve-then-rerank paradigm inherits the limitations of embedding-based retrieval; parametric generative approaches are difficult to update with new information; and long-context methods that place the entire corpus in context are computationally infeasible for large document collections. To address these challenges, we introduce LATTICE, a hierarchical retrieval framework that enables an LLM to reason over and navigate large corpora with logarithmic search complexity by imposing a semantic tree structure on the corpus. Our approach consists of two stages: (1) an offline phase that organizes the corpus into a semantic hierarchy via either a bottom-up agglomerative strategy or a top-down divisive strategy using multi-level summaries and (2) an online traversal phase where a search LLM navigates this tree. A central challenge in such LLM-guided search is that the model's relevance judgments are noisy, context-dependent, and unaware of the hierarchy, making cross-branch and cross-level comparisons difficult. To overcome this, we propose a traversal algorithm that estimates calibrated latent relevance scores from local LLM outputs and aggregates them into a global path relevance metric. Our training-free framework achieves state-of-the-art zero-shot performance on the reasoning-intensive BRIGHT benchmark, demonstrating up to 9% improvement in Recall@100 and 5% in nDCG@10 over the next best zero-shot baseline. Furthermore, compared to the fine-tuned SOTA method DIVER-v2, LATTICE attains comparable results on BRIGHT subsets that use a static corpus for evaluation.",
        "translated": "现代推荐系统日益需要处理复杂的、多方面的查询，这些查询需要深入推理，而不仅仅是简单的关键词或语义匹配。尽管基于大语言模型（LLM）的推荐系统展现出了巨大潜力，但主流的先召回后重排范式继承了基于嵌入的召回方法的局限性；参数化的生成式方法难以通过新信息进行更新；而将整个语料库放入上下文中的长上下文方法在面对大规模文档集合时计算上不可行。为了解决这些挑战，我们引入了 LATTICE，一个分层召回框架，通过在语料库上施加语义树结构，使 LLM 能够以对数级搜索复杂度对大规模语料库进行推理和导航。我们的方法包含两个阶段：（1）一个离线阶段，通过自底向上的聚合策略或自顶向下的划分策略，利用多层级摘要将语料库组织成语义层次结构；（2）一个在线遍历阶段，其中搜索 LLM 遍历该树结构。在这种由 LLM 指导的搜索中，一个核心挑战是模型的相关性判断是嘈杂的、上下文依赖的，并且不了解层次结构，从而使得跨分支和跨层级的比较变得困难。为了解决这一问题，我们提出了一种遍历算法，该算法从局部 LLM 输出中估计校准后的隐相关性得分，并将其聚合为一个全局路径相关性指标。我们的无训练框架在推理密集型的 BRIGHT 基准上实现了最先进的零样本性能，其 Recall@100 和 nDCG@10 指标分别比次优的零样本基线提升了最高 9% 和 5%。此外，与微调后的最先进方法 DIVER-v2 相比，LATTICE 在使用静态语料库进行评估的 BRIGHT 子集上达到了可比的结果。",
        "translated_title": "LLM引导的层次化召回",
        "label": [
            "召回（Recall）",
            "多模态推荐（Multimodal Recommendation）",
            "LLM生成式推荐（LLM-based Generative Recommendation）"
        ],
        "label_reason": "论文提出基于LLM的分层召回框架，适用于信息检索并间接可用于推荐",
        "relevance_score": 7
    },
    {
        "title": "ReMindRAG: Low-Cost LLM-Guided Knowledge Graph Traversal for Efficient\n  RAG",
        "url": "http://arxiv.org/abs/2510.13193v1",
        "pub_date": "2025-10-15",
        "summary": "Knowledge graphs (KGs), with their structured representation capabilities, offer promising avenue for enhancing Retrieval Augmented Generation (RAG) systems, leading to the development of KG-RAG systems. Nevertheless, existing methods often struggle to achieve effective synergy between system effectiveness and cost efficiency, leading to neither unsatisfying performance nor excessive LLM prompt tokens and inference time. To this end, this paper proposes REMINDRAG, which employs an LLM-guided graph traversal featuring node exploration, node exploitation, and, most notably, memory replay, to improve both system effectiveness and cost efficiency. Specifically, REMINDRAG memorizes traversal experience within KG edge embeddings, mirroring the way LLMs \"memorize\" world knowledge within their parameters, but in a train-free manner. We theoretically and experimentally confirm the effectiveness of REMINDRAG, demonstrating its superiority over existing baselines across various benchmark datasets and LLM backbones. Our code is available at https://github.com/kilgrims/ReMindRAG.",
        "translated": "知识图谱（KGs）凭借其结构化表示能力，为增强检索增强生成（RAG）系统提供了有前景的路径，从而推动了KG-RAG系统的出现。然而，现有方法通常难以在系统效果和成本效率之间实现有效的协同，导致性能不佳或大语言模型（LLM）提示词数量和推理时间过多。为此，本文提出REMINDRAG，其采用了一种由LLM引导的图遍历方法，包含节点探索、节点利用，以及最重要的是记忆回放，从而同时提升系统的有效性和成本效率。具体而言，REMINDRAG在KG的边嵌入中记忆遍历经验，其方式类似于LLMs在其参数中“记忆”世界知识，但无需训练。我们从理论和实验两个方面验证了REMINDRAG的有效性，结果表明其在多种基准数据集和LLM主干模型上均优于现有基线方法。我们的代码可在 https://github.com/kilgrims/ReMindRAG 获取。",
        "translated_title": "ReMindRAG：高效RAG的低成本大语言模型引导的知识图谱遍历方法",
        "label": [
            "LLM生成式推荐",
            "多模态推荐"
        ],
        "label_reason": "论文涉及LLM引导的知识图谱遍历，适用于生成式推荐中的信息检索优化。",
        "relevance_score": 7
    },
    {
        "title": "Retrieval-in-the-Chain: Bootstrapping Large Language Models for\n  Generative Retrieval",
        "url": "http://arxiv.org/abs/2510.13095v1",
        "pub_date": "2025-10-15",
        "summary": "Generative retrieval (GR) is an emerging paradigm that leverages large language models (LLMs) to autoregressively generate document identifiers (docids) relevant to a given query. Prior works have focused on leveraging the generative capabilities of LLMs to improve GR, while overlooking that their reasoning capabilities could likewise help. This raises a key question: Can explicit reasoning benefit GR? To investigate, we first conduct a preliminary study where an LLM is prompted to generate free-form chain-of-thought (CoT) reasoning before performing constrained docid decoding. Although this method outperforms standard GR, the generated reasoning tends to be verbose and poorly aligned with the docid space. These limitations motivate the development of a reasoning mechanism better tailored to GR.   Therefore, we propose Reason-for-Retrieval (R4R), a reasoning-augmented framework for GR that converts free-form CoT reasoning into a compact, structured format, and iteratively refines the reasoning during the retrieval process. R4R augments an existing GR method by leveraging a reasoning-capable LLM that has been instruction-tuned for GR. At inference time, R4R first uses the LLM to generate an initial structured reasoning; then the same LLM alternates between (i) constrained decoding with the chosen GR method to produce candidate docids and (ii) updating the reasoning based on retrieval results to improve the next round. R4R does not require additional models or training, and instead a single LLM serves as both the reasoning generator and the retriever. Extensive experiments on Natural Questions, MS MARCO, and a real-world item-search benchmark validate the effectiveness of R4R.",
        "translated": "生成式召回（GR）是一种新兴范式，它利用大语言模型（LLM）对给定查询进行自回归地生成相关文档标识符（docids）。此前的研究主要关注于利用LLM的生成能力来提升GR，而忽略了其推理能力同样可以提供帮助。这引发了一个关键问题：显式的推理是否能提升GR？为了探究这一问题，我们首先进行了一项初步研究，其中LLM被提示在进行受限docids解码之前生成自由形式的思维链（CoT）推理。尽管这种方法优于标准的GR方法，但生成的推理内容往往冗长，并且与docid空间的对齐效果较差。这些限制促使我们开发一种更加契合GR的推理机制。因此，我们提出了Reason-for-Retrieval（R4R），这是一个增强推理能力的GR框架，它将自由形式的CoT推理转换为一种紧凑的结构化格式，并在召回过程中迭代地优化该推理。R4R通过使用为GR指令调优的具有推理能力的LLM，来增强现有的GR方法。在推理阶段，R4R首先使用LLM生成初始的结构化推理；然后，相同的LLM交替执行以下两个步骤：（i）利用选定的GR方法进行受限解码以生成候选docids，以及（ii）根据召回结果更新推理内容以优化下一轮生成。R4R不需要额外的模型或训练，而是通过单一LLM同时担任推理生成器和召回器的角色。在Natural Questions、MS MARCO和一个实际的物料-搜索基准数据集上的大量实验验证了R4R的有效性。",
        "translated_title": "链中召回：通过引导大语言模型实现生成式召回",
        "label": [
            "LLM生成式推荐",
            "召回"
        ],
        "label_reason": "论文提出基于LLM的生成式检索框架，与推荐系统召回环节密切相关。",
        "relevance_score": 8
    },
    {
        "title": "Post-hoc Popularity Bias Correction in GNN-based Collaborative Filtering",
        "url": "http://arxiv.org/abs/2510.12959v1",
        "pub_date": "2025-10-14",
        "summary": "User historical interaction data is the primary signal for learning user preferences in collaborative filtering (CF). However, the training data often exhibits a long-tailed distribution, where only a few items have the majority of interactions. CF models trained directly on such imbalanced data are prone to learning popularity bias, which reduces personalization and leads to suboptimal recommendation quality. Graph Neural Networks (GNNs), while effective for CF due to their message passing mechanism, can further propagate and amplify popularity bias through their aggregation process. Existing approaches typically address popularity bias by modifying training objectives but fail to directly counteract the bias propagated during GNN's neighborhood aggregation. Applying weights to interactions during aggregation can help alleviate this problem, yet it risks distorting model learning due to unstable node representations in the early stages of training. In this paper, we propose a Post-hoc Popularity Debiasing (PPD) method that corrects for popularity bias in GNN-based CF and operates directly on pre-trained embeddings without requiring retraining. By estimating interaction-level popularity and removing popularity components from node representations via a popularity direction vector, PPD reduces bias while preserving user preferences. Experimental results show that our method outperforms state-of-the-art approaches for popularity bias correction in GNN-based CF.",
        "translated": "用户的历史交互数据是协同过滤（CF）中学习用户/物料偏好的主要信号。然而，训练数据通常表现出长尾分布，其中只有少数物料获得了大部分的交互。直接在这样的不平衡数据上训练的CF模型容易学习到流行度偏差，从而降低个性化程度并导致推荐质量下降。图神经网络（GNN）由于其消息传递机制在CF中表现有效，但它们的聚合过程可能会进一步传播和放大流行度偏差。现有方法通常通过修改训练目标来应对流行度偏差，但未能直接对抗GNN在邻域聚合过程中传播的偏差。在聚合过程中对交互加权有助于缓解这一问题，然而由于训练早期阶段节点表示的不稳定性，这种加权可能会扭曲模型的学习。本文提出了一种后处理流行度去偏（Post-hoc Popularity Debiasing, PPD）方法，该方法无需重新训练，即可直接在预训练的嵌入上进行操作，对基于GNN的CF中的流行度偏差进行校正。通过估计交互层面的流行度，并借助一个流行度方向向量从节点表示中去除流行度成分，PPD在减少偏差的同时保留了用户偏好。实验结果表明，我们的方法在基于GNN的CF中流行度偏差校正方面优于当前最先进的方法。",
        "translated_title": "基于图神经网络的协同过滤中的后处理流行度偏差校正",
        "label": [
            "图神经网络推荐（GNN for Recommendation）",
            "通用推荐技术（General Recommendation Techniques）"
        ],
        "label_reason": "论文针对GNN在推荐中的流行度偏差问题提出后处理方法，属于图神经网络推荐改进技术",
        "relevance_score": 8
    },
    {
        "title": "Universal Image Restoration Pre-training via Masked Degradation\n  Classification",
        "url": "http://arxiv.org/abs/2510.13282v1",
        "pub_date": "2025-10-15",
        "summary": "This study introduces a Masked Degradation Classification Pre-Training method (MaskDCPT), designed to facilitate the classification of degradation types in input images, leading to comprehensive image restoration pre-training. Unlike conventional pre-training methods, MaskDCPT uses the degradation type of the image as an extremely weak supervision, while simultaneously leveraging the image reconstruction to enhance performance and robustness. MaskDCPT includes an encoder and two decoders: the encoder extracts features from the masked low-quality input image. The classification decoder uses these features to identify the degradation type, whereas the reconstruction decoder aims to reconstruct a corresponding high-quality image. This design allows the pre-training to benefit from both masked image modeling and contrastive learning, resulting in a generalized representation suited for restoration tasks. Benefit from the straightforward yet potent MaskDCPT, the pre-trained encoder can be used to address universal image restoration and achieve outstanding performance. Implementing MaskDCPT significantly improves performance for both convolution neural networks (CNNs) and Transformers, with a minimum increase in PSNR of 3.77 dB in the 5D all-in-one restoration task and a 34.8% reduction in PIQE compared to baseline in real-world degradation scenarios. It also emergences strong generalization to previously unseen degradation types and levels. In addition, we curate and release the UIR-2.5M dataset, which includes 2.5 million paired restoration samples across 19 degradation types and over 200 degradation levels, incorporating both synthetic and real-world data. The dataset, source code, and models are available at https://github.com/MILab-PKU/MaskDCPT.",
        "translated": "本研究提出了一种 Masked Degradation Classification Pre-Training 方法（MaskDCPT），旨在实现对输入图像退化类型的分类，从而促进通用图像恢复的预训练。不同于传统的预训练方法，MaskDCPT 将图像的退化类型作为极其弱的监督信号，同时利用图像重建来提升性能和鲁棒性。MaskDCPT 包含一个编码器和两个解码器：编码器从被遮蔽的低质量输入图像中提取特征；分类解码器利用这些特征识别退化类型，而重建解码器则致力于重建对应的高质量图像。这种设计使预训练能够同时受益于遮蔽图像建模和对比学习，从而获得适用于图像恢复任务的通用表征。借助简洁而有效的 MaskDCPT，预训练的编码器可用于解决通用图像恢复问题，并取得优异的性能。MaskDCPT 的实现显著提升了卷积神经网络（CNNs）和 Transformers 的性能，在 5D 全合一图像恢复任务中，PSNR 至少提升了 3.77 dB，而在真实退化场景中，与基线相比，PIQE 降低了 34.8%。此外，该方法在未见过的退化类型和程度上也展现出强大的泛化能力。同时，我们整理并发布了 UIR-2.5M 数据集，该数据集包含 250 万对覆盖 19 种退化类型和 200 多个退化等级的图像恢复样本，融合了合成数据和真实数据。数据集、源代码和模型均可在 https://github.com/MILab-PKU/MaskDCPT 获取。",
        "translated_title": "通过掩码退化分类的通用图像恢复预训练",
        "label": [
            "图像恢复（Image Restoration）",
            "图像去噪（Image Denoising）",
            "图像去雨（Image Deraining）",
            "图像去雾（Image Dehazing）",
            "图像去模糊（Image Deblurring）",
            "超分辨率（Super-Resolution）",
            "图像去 JPEG 伪影（JPEG Artifact Removal）"
        ],
        "label_reason": "论文提出通用图像恢复预训练方法，适用于多种低级图像恢复任务",
        "relevance_score": 9
    },
    {
        "title": "PhysMaster: Mastering Physical Representation for Video Generation via\n  Reinforcement Learning",
        "url": "http://arxiv.org/abs/2510.13809v1",
        "pub_date": "2025-10-15",
        "summary": "Video generation models nowadays are capable of generating visually realistic videos, but often fail to adhere to physical laws, limiting their ability to generate physically plausible videos and serve as ''world models''. To address this issue, we propose PhysMaster, which captures physical knowledge as a representation for guiding video generation models to enhance their physics-awareness. Specifically, PhysMaster is based on the image-to-video task where the model is expected to predict physically plausible dynamics from the input image. Since the input image provides physical priors like relative positions and potential interactions of objects in the scenario, we devise PhysEncoder to encode physical information from it as an extra condition to inject physical knowledge into the video generation process. The lack of proper supervision on the model's physical performance beyond mere appearance motivates PhysEncoder to apply reinforcement learning with human feedback to physical representation learning, which leverages feedback from generation models to optimize physical representations with Direct Preference Optimization (DPO) in an end-to-end manner. PhysMaster provides a feasible solution for improving physics-awareness of PhysEncoder and thus of video generation, proving its ability on a simple proxy task and generalizability to wide-ranging physical scenarios. This implies that our PhysMaster, which unifies solutions for various physical processes via representation learning in the reinforcement learning paradigm, can act as a generic and plug-in solution for physics-aware video generation and broader applications.",
        "translated": "当前的视频生成模型虽然能够生成视觉上逼真的视频，但往往未能遵循物理规律，这限制了它们生成物理合理视频的能力，也限制了其作为“世界模型”的潜力。为了解决这一问题，我们提出了PhysMaster，它将物理知识捕获为表示形式，用于引导视频生成模型，从而增强其对物理规律的感知能力。具体而言，PhysMaster基于图像到视频的任务，模型期望从输入图像中预测物理上合理的动态过程。由于输入图像提供了诸如场景中物体相对位置和潜在交互等物理先验，我们设计了PhysEncoder，从输入图像中编码物理信息作为额外的条件，将其注入视频生成过程中。由于缺乏对模型物理性能（除了外观）的有效监督，PhysEncoder采用基于人类反馈的强化学习来实现物理表示学习，通过生成模型的反馈，以端到端的方式使用直接偏好优化（Direct Preference Optimization, DPO）来优化物理表示。PhysMaster为提高PhysEncoder以及视频生成的物理感知能力提供了一种可行的解决方案，并在简单代理任务上验证了其能力，同时展示了其在广泛物理场景中的泛化性能。这表明，我们提出的PhysMaster通过在强化学习范式下的表示学习，统一了解决各种物理过程的方法，可以作为一种通用且可插拔的解决方案，应用于物理感知视频生成及相关更广泛的领域。",
        "translated_title": "PhysMaster：通过强化学习掌握物理表示用于视频生成",
        "label": [],
        "label_reason": "论文聚焦视频生成而非图像像素级复原，属于high-level任务",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "结合强化学习与物理表示，有一定创新但非图像恢复核心领域"
    },
    {
        "title": "VisCoP: Visual Probing for Video Domain Adaptation of Vision Language\n  Models",
        "url": "http://arxiv.org/abs/2510.13808v1",
        "pub_date": "2025-10-15",
        "summary": "Large Vision-Language Models (VLMs) excel at general visual reasoning tasks but exhibit sharp performance degradation when applied to novel domains with substantial distribution shifts from pretraining data. Existing domain adaptation approaches finetune different VLM components, but this often results in limited domain-specific feature learning or catastrophic forgetting of prior capabilities. To address these issues, we introduce Vision Contextualized Probing (VisCoP), which augments the VLM's vision encoder with a compact set of learnable visual probes. These probes enable efficient domain-specific adaptation with minimal modification to pretrained parameters. We evaluate VisCoP across three challenging domain adaptation settings-cross-view (exocentric to egocentric), cross-modal (RGB to depth), and cross-task (human understanding to robot control). Experiments show that VisCoP consistently outperforms existing adaptation strategies, achieving superior performance on target domains while effectively retaining source-domain knowledge.",
        "translated": "大型视觉-语言模型（VLMs）在通用视觉推理任务中表现出色，但当应用于与预训练数据存在显著分布差异的新领域时，其性能会急剧下降。现有的领域适应方法通常对VLM的不同组件进行微调，但这往往导致领域特定特征学习受限，或者对先前能力产生灾难性遗忘。为了解决这些问题，我们提出了视觉上下文化探针（Vision Contextualized Probing, VisCoP），该方法通过在VLM的视觉编码器中引入一组紧凑的可学习视觉探针来增强模型。这些探针能够在对预训练参数仅作最小修改的情况下实现高效的领域特定适应。我们在三种具有挑战性的领域适应设置中评估了VisCoP，包括跨视角（外视角到自视角）、跨模态（RGB到深度）和跨任务（人类理解到机器人控制）。实验表明，VisCoP始终优于现有适应策略，在目标领域上取得了更优的性能，同时有效保留了源领域的知识。",
        "translated_title": "VisCoP：用于视觉语言模型视频域适应的视觉探测",
        "label": [],
        "label_reason": "论文聚焦于视觉语言模型的视频领域适配，不直接涉及图像像素级恢复或增强",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出视觉探针机制，对现有领域适配方法有一定改进"
    },
    {
        "title": "Generative Universal Verifier as Multimodal Meta-Reasoner",
        "url": "http://arxiv.org/abs/2510.13804v1",
        "pub_date": "2025-10-15",
        "summary": "We introduce Generative Universal Verifier, a novel concept and plugin designed for next-generation multimodal reasoning in vision-language models and unified multimodal models, providing the fundamental capability of reflection and refinement on visual outcomes during the reasoning and generation process. This work makes three main contributions: (1) We build ViVerBench, a comprehensive benchmark spanning 16 categories of critical tasks for evaluating visual outcomes in multimodal reasoning. Results show that existing VLMs consistently underperform across these tasks, underscoring a substantial gap from human-level capability in reliable visual verification. (2) We design two automated pipelines to construct large-scale visual verification data and train OmniVerifier-7B, the first omni-capable generative verifier trained for universal visual verification and achieves notable gains on ViVerBench(+8.3). Through training, we identify three atomic capabilities in visual verification and demonstrate how they generalize and interact synergistically. (3) We propose OmniVerifier-TTS, a sequential test-time scaling paradigm that leverages the universal verifier to bridge image generation and editing within unified models, enhancing the upper bound of generative ability through iterative fine-grained optimization. Beyond generation, we extend universal verifier to broader world-modeling interleaved reasoning scenarios. Empirically, OmniVerifier-TTS achieves improvements on T2I-ReasonBench(+3.7), and GenEval++(+4.3), outperforming existing parallel test-time scaling methods, such as Best-of-N. By endowing multimodal reasoning with reliable visual verification, OmniVerifier advances both reliable reflection during generation and scalable test-time refinement, marking a step toward more trustworthy and controllable next-generation reasoning systems.",
        "translated": "我们提出生成式通用验证器（Generative Universal Verifier），这是一种面向下一代视觉语言模型和统一多模态模型的新型概念和插件，旨在提供推理和生成过程中对视觉结果进行反思和优化的基本能力。本研究做出以下三项主要贡献：(1) 我们构建了 ViVerBench，一个全面的基准测试集，涵盖16类关键任务，用于评估多模态推理中视觉结果的性能。结果显示，现有视觉语言模型（VLMs）在这些任务中表现普遍不佳，表明其在可靠视觉验证方面与人类水平之间存在显著差距。(2) 我们设计了两条自动化流水线用于构建大规模的视觉验证数据，并训练了 OmniVerifier-7B，这是首个面向通用视觉验证的全能型生成式验证器。在 ViVerBench 上，OmniVerifier-7B 取得了显著的性能提升（+8.3）。通过训练，我们识别出视觉验证中的三个基本能力，并展示了它们如何在不同任务中泛化并协同工作。(3) 我们提出了 OmniVerifier-TTS，一种顺序式测试时扩展范式，利用通用验证器在统一模型中实现图像生成和编辑的桥梁作用，并通过迭代细粒度优化提升生成能力的上限。除了生成任务，我们还将通用验证器扩展到更广泛的交互式世界建模推理场景。实证表明，OmniVerifier-TTS 在 T2I-ReasonBench（+3.7）和 GenEval++（+4.3）上均取得性能提升，并优于现有的并行式测试时扩展方法，如 Best-of-N。通过为多模态推理赋予可靠的视觉验证能力，OmniVerifier 推动了生成过程中的可靠反思以及可扩展的测试时优化，标志着我们向更加可信和可控的下一代推理系统迈出了关键一步。",
        "translated_title": "生成式通用验证器作为多模态元推理器",
        "label": [],
        "label_reason": "论文主要关注视觉-语言模型的推理验证，非图像像素级处理",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出通用验证框架和新范式，但未突破low-level图像处理核心技术"
    },
    {
        "title": "Trace Anything: Representing Any Video in 4D via Trajectory Fields",
        "url": "http://arxiv.org/abs/2510.13802v1",
        "pub_date": "2025-10-15",
        "summary": "Effective spatio-temporal representation is fundamental to modeling, understanding, and predicting dynamics in videos. The atomic unit of a video, the pixel, traces a continuous 3D trajectory over time, serving as the primitive element of dynamics. Based on this principle, we propose representing any video as a Trajectory Field: a dense mapping that assigns a continuous 3D trajectory function of time to each pixel in every frame. With this representation, we introduce Trace Anything, a neural network that predicts the entire trajectory field in a single feed-forward pass. Specifically, for each pixel in each frame, our model predicts a set of control points that parameterizes a trajectory (i.e., a B-spline), yielding its 3D position at arbitrary query time instants. We trained the Trace Anything model on large-scale 4D data, including data from our new platform, and our experiments demonstrate that: (i) Trace Anything achieves state-of-the-art performance on our new benchmark for trajectory field estimation and performs competitively on established point-tracking benchmarks; (ii) it offers significant efficiency gains thanks to its one-pass paradigm, without requiring iterative optimization or auxiliary estimators; and (iii) it exhibits emergent abilities, including goal-conditioned manipulation, motion forecasting, and spatio-temporal fusion. Project page: https://trace-anything.github.io/.",
        "translated": "有效的时空表示是建模、理解和预测视频中动态过程的基础。视频的基本单位是像素，其在时间上追踪一个连续的3D轨迹，作为动态的基本元素。基于这一原理，我们提出将任何视频表示为轨迹场（Trajectory Field）：一种密集映射，为每一帧中的每个像素分配一个关于时间的连续3D轨迹函数。通过这种表示，我们引入了Trace Anything，一种神经网络模型，它能够在一次前向传播中预测整个轨迹场。具体来说，对于每一帧中的每个像素，我们的模型预测一组控制点，以参数化轨迹（即B样条），从而在任意查询时刻获得其3D位置。我们在大规模4D数据上训练了Trace Anything模型，包括我们新平台的数据，实验结果表明：(i) Trace Anything在我们新的轨迹场估计基准上取得了最先进的性能，并在已有的点跟踪基准上表现出竞争力；(ii) 由于其单次前向传播的范式，它在效率方面有显著提升，无需进行迭代优化或使用辅助估计器；(iii) 它展现出一些新兴能力，包括目标条件下的操控、运动预测以及时空融合。项目主页：https://trace-anything.github.io/。",
        "translated_title": "追踪任何内容：通过轨迹场在4D中表示任何视频",
        "label": [],
        "label_reason": "论文关注视频轨迹建模，不属于图像像素级恢复或增强任务。",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出轨迹场表示和一次性预测框架，具有新颖性。"
    },
    {
        "title": "Reasoning in Space via Grounding in the World",
        "url": "http://arxiv.org/abs/2510.13800v1",
        "pub_date": "2025-10-15",
        "summary": "In this paper, we claim that 3D visual grounding is the cornerstone of spatial reasoning and introduce the Grounded-Spatial Reasoner (GS-Reasoner) to explore the effective spatial representations that bridge the gap between them. Existing 3D LLMs suffer from the absence of a unified 3D representation capable of jointly capturing semantic and geometric information. This deficiency is manifested either in poor performance on grounding or in an excessive reliance on external modules, ultimately hindering the seamless integration of grounding and spatial reasoning. To address this, we propose a simple yet effective dual-path pooling mechanism that tightly aligns geometric features with both semantic and positional cues, constructing a unified image patch-based 3D representation that encapsulates all essential information without increasing the number of input tokens. Leveraging this holistic representation, GS-Reasoner is the first 3D LLM that achieves autoregressive grounding entirely without external modules while delivering performance comparable to state-of-the-art models, establishing a unified and self-contained framework for 3D spatial reasoning. To further bridge grounding and spatial reasoning, we introduce the Grounded Chain-of-Thought (GCoT) dataset. This dataset is meticulously curated to include both 3D bounding box annotations for objects referenced in reasoning questions and step-by-step reasoning paths that integrate grounding as a core component of the problem-solving process. Extensive experiments demonstrate that GS-Reasoner achieves impressive results on 3D visual grounding, which in turn significantly enhances its spatial reasoning capabilities, leading to state-of-the-art performance.",
        "translated": "本文提出观点认为，三维视觉定位是空间推理的基础，并引入了 GS-Reasoner（Grounded-Spatial Reasoner）以探索能够弥合二者之间差距的有效空间表示。现有的三维大语言模型（LLM）面临缺乏统一的三维表示的问题，该表示无法同时捕捉语义和几何信息。这种缺陷体现在定位任务上的性能较差，或对外部模块的过度依赖，最终阻碍了定位与空间推理的无缝融合。为了解决这一问题，我们提出了一种简单而有效的同时路径池化机制，该机制紧密对齐几何特征与语义和位置线索，构建了一种统一的、基于图像块的三维表示形式，包含所有必要信息且不增加输入 token 的数量。借助这种整体表示，GS-Reasoner 是首个在不使用任何外部模块的情况下实现自回归定位的三维大语言模型，其性能可与最先进的模型相媲美，为三维空间推理提供了一个统一且自洽的框架。为进一步弥合定位与空间推理之间的联系，我们引入了 Grounded Chain-of-Thought（GCoT）数据集。该数据集经过精心构建，包含了推理问题中所提到对象的三维边界框标注，以及将定位作为解决问题核心步骤的逐步推理路径。大量实验表明，GS-Reasoner 在三维视觉定位任务中取得了令人印象深刻的结果，从而显著提升了其空间推理能力，达到当前最先进的性能水平。",
        "translated_title": "通过在世界中的锚定实现空间推理",
        "label": [],
        "label_reason": "论文聚焦3D视觉理解和推理，不涉及图像像素级恢复或增强",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "提出了一种新颖的3D表示方法和数据集，但不针对低层图像处理任务"
    },
    {
        "title": "The Mechanistic Emergence of Symbol Grounding in Language Models",
        "url": "http://arxiv.org/abs/2510.13796v1",
        "pub_date": "2025-10-15",
        "summary": "Symbol grounding (Harnad, 1990) describes how symbols such as words acquire their meanings by connecting to real-world sensorimotor experiences. Recent work has shown preliminary evidence that grounding may emerge in (vision-)language models trained at scale without using explicit grounding objectives. Yet, the specific loci of this emergence and the mechanisms that drive it remain largely unexplored. To address this problem, we introduce a controlled evaluation framework that systematically traces how symbol grounding arises within the internal computations through mechanistic and causal analysis. Our findings show that grounding concentrates in middle-layer computations and is implemented through the aggregate mechanism, where attention heads aggregate the environmental ground to support the prediction of linguistic forms. This phenomenon replicates in multimodal dialogue and across architectures (Transformers and state-space models), but not in unidirectional LSTMs. Our results provide behavioral and mechanistic evidence that symbol grounding can emerge in language models, with practical implications for predicting and potentially controlling the reliability of generation.",
        "translated": "符号根基（Symbol grounding）（Harnad, 1990）描述了符号（如词语）如何通过与现实世界中的感知和运动体验相连接而获得其含义。最近的研究表明，在大规模训练的（视觉-）语言模型中，即使不使用显式的根基目标，符号根基也可能自发出现。然而，这种现象的具体出现位置及其驱动机制仍鲜有研究。为了解决这一问题，我们引入了一种可控的评估框架，系统地追踪符号根基在内部计算过程中的产生方式，并通过机制和因果分析进行研究。我们的研究结果表明，符号根基主要集中在中间层计算中，并通过聚合机制实现，其中注意力头聚合环境根基以支持语言形式的预测。这一现象在多模态对话中以及不同架构（Transformer 和状态空间模型）中均出现，但在单向 LSTM 中未观察到。我们的研究提供了行为和机制上的证据，证明符号根基可以在语言模型中自发产生，这对于预测和可能控制生成的可靠性具有实际意义。",
        "translated_title": "语言模型中符号 grounding 的机制性出现",
        "label": [],
        "label_reason": "论文聚焦语言模型的符号接地机制，不涉及图像像素级处理。",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "提出了符号接地的机制分析框架，有一定理论创新但非视觉任务相关。"
    },
    {
        "title": "Bee: A High-Quality Corpus and Full-Stack Suite to Unlock Advanced Fully\n  Open MLLMs",
        "url": "http://arxiv.org/abs/2510.13795v1",
        "pub_date": "2025-10-15",
        "summary": "Fully open multimodal large language models (MLLMs) currently lag behind proprietary counterparts, primarily due to a significant gap in data quality for supervised fine-tuning (SFT). Existing open-source datasets are often plagued by widespread noise and a critical deficit in complex reasoning data, such as Chain-of-Thought (CoT), which hinders the development of advanced model capabilities. Addressing these challenges, our work makes three primary contributions. First, we introduce Honey-Data-15M, a new SFT dataset comprising approximately 15 million QA pairs, processed through multiple cleaning techniques and enhanced with a novel dual-level (short and long) CoT enrichment strategy. Second, we introduce HoneyPipe, the data curation pipeline, and its underlying framework DataStudio, providing the community with a transparent and adaptable methodology for data curation that moves beyond static dataset releases. Finally, to validate our dataset and pipeline, we train Bee-8B, an 8B model on Honey-Data-15M. Experiments show that Bee-8B establishes a new state-of-the-art (SOTA) for fully open MLLMs, achieving performance that is competitive with, and in some cases surpasses, recent semi-open models such as InternVL3.5-8B. Our work delivers to the community a suite of foundational resources, including: the Honey-Data-15M corpus; the full-stack suite comprising HoneyPipe and DataStudio; training recipes; an evaluation harness; and the model weights. This effort demonstrates that a principled focus on data quality is a key pathway to developing fully open MLLMs that are highly competitive with their semi-open counterparts.",
        "translated": "目前，完全开源的多模态大语言模型（MLLMs）在性能上落后于专有模型，主要原因在于用于监督微调（SFT）的数据质量存在显著差距。现有的开源数据集通常受到广泛噪声的困扰，并且在复杂推理数据（如思维链（CoT））方面存在严重不足，这阻碍了先进模型能力的发展。为了解决这些挑战，我们的工作主要包括三个方面的贡献。首先，我们引入了 Honey-Data-15M，一个包含约 1500 万个问答对的新 SFT 数据集，通过多种数据清洗技术处理，并采用了一种新的双层（短链和长链）CoT 增强策略进行优化。其次，我们提出了 HoneyPipe 数据整理流水线及其底层框架 DataStudio，为社区提供一种透明且可调节的数据整理方法，突破了传统静态数据集发布的方式。最后，为了验证我们的数据集和流水线效果，我们基于 Honey-Data-15M 训练了 Bee-8B，一个 80 亿参数规模的模型。实验结果表明，Bee-8B 在完全开源 MLLMs 中达到了新的最先进水平（SOTA），其性能与近期的半开源模型（如 InternVL3.5-8B）相比具有竞争力，某些情况下甚至优于后者。我们的工作为社区提供了一套基础资源，包括：Honey-Data-15M 数据语料库；涵盖 HoneyPipe 和 DataStudio 的全栈工具套件；训练配方；评估框架；以及模型权重。这项研究表明，专注于数据质量的原则性方法是开发与半开源模型高度竞争的完全开源 MLLMs 的关键路径。",
        "translated_title": "Bee：一个高质量语料库和全栈套件，用于解锁先进的全开放大语言模型",
        "label": [],
        "label_reason": "论文聚焦多模态大语言模型数据构建，不涉及图像像素级处理",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "提出新数据集和训练方法，但属于通用MLLM改进而非视觉低级任务创新"
    },
    {
        "title": "NoisePrints: Distortion-Free Watermarks for Authorship in Private\n  Diffusion Models",
        "url": "http://arxiv.org/abs/2510.13793v1",
        "pub_date": "2025-10-15",
        "summary": "With the rapid adoption of diffusion models for visual content generation, proving authorship and protecting copyright have become critical. This challenge is particularly important when model owners keep their models private and may be unwilling or unable to handle authorship issues, making third-party verification essential. A natural solution is to embed watermarks for later verification. However, existing methods require access to model weights and rely on computationally heavy procedures, rendering them impractical and non-scalable. To address these challenges, we propose , a lightweight watermarking scheme that utilizes the random seed used to initialize the diffusion process as a proof of authorship without modifying the generation process. Our key observation is that the initial noise derived from a seed is highly correlated with the generated visual content. By incorporating a hash function into the noise sampling process, we further ensure that recovering a valid seed from the content is infeasible. We also show that sampling an alternative seed that passes verification is infeasible, and demonstrate the robustness of our method under various manipulations. Finally, we show how to use cryptographic zero-knowledge proofs to prove ownership without revealing the seed. By keeping the seed secret, we increase the difficulty of watermark removal. In our experiments, we validate NoisePrints on multiple state-of-the-art diffusion models for images and videos, demonstrating efficient verification using only the seed and output, without requiring access to model weights.",
        "translated": "随着扩散模型在视觉内容生成中的迅速应用，证明作者身份和保护版权变得至关重要。当模型所有者保持其模型私有时，这一挑战尤为突出，他们可能不愿或无法处理作者身份问题，从而使得第三方验证成为必要。一种自然的解决方案是嵌入水印以供后续验证。然而，现有方法需要访问模型权重，并依赖计算量大的过程，使其在实践中不可行且难以扩展。为了解决这些挑战，我们提出了一种轻量级的水印方案，该方案利用用于初始化扩散过程的随机种子作为作者身份的证明，且无需修改生成过程。我们的关键观察是，由种子生成的初始噪声与生成的视觉内容具有高度相关性。通过在噪声采样过程中引入哈希函数，我们进一步确保了从内容中恢复有效种子在计算上不可行。我们还证明，采样一个能通过验证的替代种子在计算上也是不可行的，并展示了我们的方法在各种篡改下的鲁棒性。最后，我们展示了如何使用密码学中的零知识证明来在不泄露种子的前提下证明所有权。通过保密种子，我们提高了水印移除的难度。在我们的实验中，我们在多个最先进的图像和视频扩散模型上验证了 NoisePrints 的有效性，展示了仅使用种子和输出即可实现高效的验证，而无需访问模型权重。",
        "translated_title": "NoisePrints: 隐私扩散模型中的无损水印用于作者归属",
        "label": [],
        "label_reason": "论文聚焦扩散模型水印而非图像像素级恢复",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出无需模型权重的轻量水印方案，有一定创新"
    },
    {
        "title": "Adaptive Visual Conditioning for Semantic Consistency in Diffusion-Based\n  Story Continuation",
        "url": "http://arxiv.org/abs/2510.13787v1",
        "pub_date": "2025-10-15",
        "summary": "Story continuation focuses on generating the next image in a narrative sequence so that it remains coherent with both the ongoing text description and the previously observed images. A central challenge in this setting lies in utilizing prior visual context effectively, while ensuring semantic alignment with the current textual input. In this work, we introduce AVC (Adaptive Visual Conditioning), a framework for diffusion-based story continuation. AVC employs the CLIP model to retrieve the most semantically aligned image from previous frames. Crucially, when no sufficiently relevant image is found, AVC adaptively restricts the influence of prior visuals to only the early stages of the diffusion process. This enables the model to exploit visual context when beneficial, while avoiding the injection of misleading or irrelevant information. Furthermore, we improve data quality by re-captioning a noisy dataset using large language models, thereby strengthening textual supervision and semantic alignment. Quantitative results and human evaluations demonstrate that AVC achieves superior coherence, semantic consistency, and visual fidelity compared to strong baselines, particularly in challenging cases where prior visuals conflict with the current input.",
        "translated": "故事延续关注于生成叙事序列中的下一幅图像，使其与当前的文本描述和之前已观测的图像保持连贯。在这种设置下的一个核心挑战在于如何有效地利用先前的视觉上下文，同时确保与当前文本输入在语义上的一致性。在本文中，我们提出了 AVC（Adaptive Visual Conditioning），一种基于扩散模型的故事延续框架。AVC 利用 CLIP 模型从先前的帧中检索语义最相关的一幅图像。关键的是，当未找到足够相关的图像时，AVC 会自适应地将先前视觉信息的影响限制在扩散过程的早期阶段。这使得模型在有益时能够有效利用视觉上下文，同时避免注入误导性或不相关的信息。此外，我们通过使用大语言模型对噪声数据集进行重新描述，提高了数据质量，从而增强了文本监督和语义一致性。定量结果和人类评估表明，与强大的基线模型相比，AVC 在连贯性、语义一致性和视觉保真度方面均取得了优越的性能，特别是在先前视觉信息与当前输入冲突的具有挑战性的情况下。",
        "translated_title": "基于扩散的故事情节延续中的自适应视觉条件化以保持语义一致性",
        "label": [],
        "label_reason": "论文关注文本驱动的扩散模型故事生成，非像素级图像恢复任务。",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出自适应视觉条件框架，改进文本-图像对齐方法，但创新点较常规。"
    },
    {
        "title": "InternVLA-M1: A Spatially Guided Vision-Language-Action Framework for\n  Generalist Robot Policy",
        "url": "http://arxiv.org/abs/2510.13778v1",
        "pub_date": "2025-10-15",
        "summary": "We introduce InternVLA-M1, a unified framework for spatial grounding and robot control that advances instruction-following robots toward scalable, general-purpose intelligence. Its core idea is spatially guided vision-language-action training, where spatial grounding serves as the critical link between instructions and robot actions. InternVLA-M1 employs a two-stage pipeline: (i) spatial grounding pre-training on over 2.3M spatial reasoning data to determine ``where to act'' by aligning instructions with visual, embodiment-agnostic positions, and (ii) spatially guided action post-training to decide ``how to act'' by generating embodiment-aware actions through plug-and-play spatial prompting. This spatially guided training recipe yields consistent gains: InternVLA-M1 outperforms its variant without spatial guidance by +14.6% on SimplerEnv Google Robot, +17% on WidowX, and +4.3% on LIBERO Franka, while demonstrating stronger spatial reasoning capability in box, point, and trace prediction. To further scale instruction following, we built a simulation engine to collect 244K generalizable pick-and-place episodes, enabling a 6.2% average improvement across 200 tasks and 3K+ objects. In real-world clustered pick-and-place, InternVLA-M1 improved by 7.3%, and with synthetic co-training, achieved +20.6% on unseen objects and novel configurations. Moreover, in long-horizon reasoning-intensive scenarios, it surpassed existing works by over 10%. These results highlight spatially guided training as a unifying principle for scalable and resilient generalist robots. Code and models are available at https://github.com/InternRobotics/InternVLA-M1.",
        "translated": "我们引入了 InternVLA-M1，这是一个面向空间定位与机器人控制的统一框架，推动指令跟随机器人向可扩展、通用型智能发展。其核心思想是空间引导的视觉-语言-动作训练，其中空间定位作为连接指令与机器人动作的关键环节。InternVLA-M1 采用两阶段训练流程：(i) 在超过 2.3 万个空间推理数据上进行空间定位预训练，通过将指令与视觉、与实体无关的空间位置对齐来确定 ``在何处执行动作''，以及 (ii) 通过即插即用的空间提示生成与实体相关的动作，进行空间引导的动作后训练以决定 ``如何执行动作''。这种空间引导的训练方法带来了显著的提升：在 SimplierEnv Google Robot 上，InternVLA-M1 相较于其无空间引导的变体提升了 +14.6%，在 WidowX 上提升了 +17%，在 LIBERO Franka 上提升了 +4.3%，同时在盒预测、点预测和轨迹预测中表现出更强的空间推理能力。为了进一步扩展指令跟随的能力，我们构建了一个仿真引擎，收集了 244K 个具有泛化能力的抓取与放置场景，使得在 200 个任务和 3K+ 个物体上的平均性能提升了 6.2%。在现实世界中的集群抓取与放置任务中，InternVLA-M1 提升了 7.3%，而在结合合成数据的联合训练中，对于未见过的物体和新配置，性能提升达 +20.6%。此外，在需要长期推理的复杂场景中，其性能超越现有工作超过 10%。这些结果表明，空间引导的训练方法可作为构建可扩展且鲁棒的通用型机器人的统一原则。代码和模型可在 https://github.com/InternRobotics/InternVLA-M1 获取。",
        "translated_title": "InternVLA-M1：一种空间引导的视觉-语言-动作框架  \n用于通用机器人策略",
        "label": [],
        "label_reason": "论文聚焦机器人控制而非图像像素级恢复或增强",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出空间引导训练方法，对指令跟随机器人有改进"
    },
    {
        "title": "UrbanFusion: Stochastic Multimodal Fusion for Contrastive Learning of\n  Robust Spatial Representations",
        "url": "http://arxiv.org/abs/2510.13774v1",
        "pub_date": "2025-10-15",
        "summary": "Forecasting urban phenomena such as housing prices and public health indicators requires the effective integration of various geospatial data. Current methods primarily utilize task-specific models, while recent foundation models for spatial representations often support only limited modalities and lack multimodal fusion capabilities. To overcome these challenges, we present UrbanFusion, a Geo-Foundation Model (GeoFM) that features Stochastic Multimodal Fusion (SMF). The framework employs modality-specific encoders to process different types of inputs, including street view imagery, remote sensing data, cartographic maps, and points of interest (POIs) data. These multimodal inputs are integrated via a Transformer-based fusion module that learns unified representations. An extensive evaluation across 41 tasks in 56 cities worldwide demonstrates UrbanFusion's strong generalization and predictive performance compared to state-of-the-art GeoAI models. Specifically, it 1) outperforms prior foundation models on location-encoding, 2) allows multimodal input during inference, and 3) generalizes well to regions unseen during training. UrbanFusion can flexibly utilize any subset of available modalities for a given location during both pretraining and inference, enabling broad applicability across diverse data availability scenarios. All source code is available at https://github.com/DominikM198/UrbanFusion.",
        "translated": "预测城市现象（如房价和公共健康指标）需要有效整合各种地理空间数据。当前的方法主要依赖于任务特定模型，而近期的空间表示基础模型通常只支持有限的模态，并缺乏多模态融合能力。为了解决这些挑战，我们提出了 UrbanFusion，一种具有随机多模态融合（Stochastic Multimodal Fusion, SMF）功能的地理基础模型（Geo-Foundation Model, GeoFM）。该框架使用模态特定编码器处理不同类型的输入，包括街景图像、遥感数据、地图数据和兴趣点（Points of Interest, POIs）数据。这些多模态输入通过基于 Transformer 的融合模块进行整合，以学习统一的表示。在对全球 56 个城市中的 41 项任务进行广泛评估后，UrbanFusion 在泛化能力和预测性能方面均优于最先进的 GeoAI 模型。具体而言，UrbanFusion 具备以下优势：1）在位置编码方面优于以往的基础模型，2）在推理过程中支持多模态输入，3）对训练过程中未见过的区域具有良好的泛化能力。UrbanFusion 在预训练和推理阶段都能灵活地利用特定位置中可用的任意子集模态，从而使其适用于各种数据可用性场景。所有源代码均在 https://github.com/DominikM198/UrbanFusion 上公开。",
        "translated_title": "UrbanFusion: 用于鲁棒空间表示对比学习的随机多模态融合",
        "label": [
            "遥感图像复原"
        ],
        "label_reason": "涉及遥感图像处理，但主要聚焦多模态融合而非像素级图像质量恢复。",
        "relevance_score": 5,
        "novelty_score": 7,
        "novelty_reason": "提出随机多模态融合方法，对Geo-Foundation Model有一定改进。"
    },
    {
        "title": "Scaling Vision Transformers for Functional MRI with Flat Maps",
        "url": "http://arxiv.org/abs/2510.13768v1",
        "pub_date": "2025-10-15",
        "summary": "A key question for adapting modern deep learning architectures to functional MRI (fMRI) is how to represent the data for model input. To bridge the modality gap between fMRI and natural images, we transform the 4D volumetric fMRI data into videos of 2D fMRI activity flat maps. We train Vision Transformers on 2.3K hours of fMRI flat map videos from the Human Connectome Project using the spatiotemporal masked autoencoder (MAE) framework. We observe that masked fMRI modeling performance improves with dataset size according to a strict power scaling law. Downstream classification benchmarks show that our model learns rich representations supporting both fine-grained state decoding across subjects, as well as subject-specific trait decoding across changes in brain state. This work is part of an ongoing open science project to build foundation models for fMRI data. Our code and datasets are available at https://github.com/MedARC-AI/fmri-fm.",
        "translated": "将现代深度学习架构适应于功能磁共振成像（fMRI）的一个关键问题是，如何将数据表示为模型输入。为了弥合 fMRI 与自然图像之间的模态差异，我们把 4D 的 fMRI 体积数据转换为 2D 的 fMRI 活动平面图的视频。我们使用时空掩码自编码器（MAE）框架，在来自人类连接组计划（Human Connectome Project）的 2.3K 小时 fMRI 平面图视频上训练视觉变换器（Vision Transformers）。我们观察到，随着数据集规模的增加，掩码 fMRI 建模性能按照严格的幂律进行提升。下游分类基准测试表明，我们的模型学习到了丰富的表示，不仅能够跨被试进行细粒度状态解码，还能在脑状态变化的条件下进行被试特异性特质的解码。本项工作是正在进行的一个开放科学项目的一部分，旨在为 fMRI 数据构建基础模型。我们的代码和数据集可在 https://github.com/MedARC-AI/fmri-fm 获取。",
        "translated_title": "Scaling Vision Transformers for Functional MRI with Flat Maps  \n使用平面图的视觉变换器扩展功能磁共振成像  \n\n功能磁共振成像（fMRI）是一种非侵入性技术，用于研究大脑活动，其通过检测血氧水平依赖（BOLD）信号实现。然而，fMRI 数据通常受到低空间分辨率和低信噪比（SNR）的限制，这对识别大脑功能区域构成挑战。最近，视觉变换器（Vision Transformers, ViTs）在各种图像恢复任务中表现出卓越的性能。这些模型通过其自注意力机制能够捕获长距离依赖关系，为 fMRI 信号的处理提供了新思路。  \n\n在本研究中，我们提出了一种基于视觉变换器的方法，用于 fMRI 数据的去噪和增强。我们采用平面图（Flat Maps）作为先验知识，以更好地建模大脑皮层的拓扑结构。通过将 fMRI 数据投影到平面图中，我们能够利用 ViT 在空域中的强大特征提取能力。实验结果表明，我们的方法在多个 fMRI 数据集上显著优于现有方法，尤其是在保留功能细节和减少噪声方面表现出色。  \n\n此外，我们还探讨了视觉变换器的扩展能力，通过增加模型深度和宽度来提升其性能。我们发现，适当增加模型规模可以有效提高 fMRI 数据的恢复质量，同时保持计算效率。",
        "label": [],
        "label_reason": "论文专注于fMRI数据表示与建模，非图像像素级恢复或增强任务。",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出将Vision Transformer扩展至fMRI数据建模，具有一定新颖性。"
    },
    {
        "title": "Uni-MMMU: A Massive Multi-discipline Multimodal Unified Benchmark",
        "url": "http://arxiv.org/abs/2510.13759v1",
        "pub_date": "2025-10-15",
        "summary": "Unified multimodal models aim to jointly enable visual understanding and generation, yet current benchmarks rarely examine their true integration. Existing evaluations either treat the two abilities in isolation or overlook tasks that inherently couple them. To address this gap, we present Uni-MMMU, a comprehensive and discipline-aware benchmark that systematically unfolds the bidirectional synergy between generation and understanding across eight reasoning-centric domains, including science, coding, mathematics, and puzzles. Each task is bidirectionally coupled, demanding models to (i) leverage conceptual understanding to guide precise visual synthesis, or (ii) utilize generation as a cognitive scaffold for analytical reasoning. Uni-MMMU incorporates verifiable intermediate reasoning steps, unique ground truths, and a reproducible scoring protocol for both textual and visual outputs. Through extensive evaluation of state-of-the-art unified, generation-only, and understanding-only models, we reveal substantial performance disparities and cross-modal dependencies, offering new insights into when and how these abilities reinforce one another, and establishing a reliable foundation for advancing unified models.",
        "translated": "统一的多模态模型旨在同时实现视觉理解和生成能力，但目前的基准测试很少真正考察这两种能力的融合。现有的评估方法要么将这两种能力孤立地对待，要么忽略了那些本质上需要将它们耦合的任务。为了解决这一问题，我们提出了 Uni-MMMU，这是一个全面且学科感知的基准测试，系统地揭示了生成与理解在八个以推理为中心的领域（包括科学、编程、数学和谜题）中的双向协同作用。每个任务都是双向耦合的，要求模型 (i) 利用概念理解来指导精确的视觉合成，或 (ii) 将生成能力作为认知支架，以支持分析推理。Uni-MMMU 包含可验证的中间推理步骤、唯一的地面真值，以及针对文本和视觉输出的可复现评分协议。通过对最先进的统一模型、仅生成模型和仅理解模型的广泛评估，我们揭示了显著的性能差异和跨模态依赖关系，提供了关于何时以及如何实现这些能力相互增强的新见解，并为统一模型的进一步发展奠定了可靠基础。",
        "translated_title": "Uni-MMMU: 一个大规模多学科多模态统一基准",
        "label": [],
        "label_reason": "论文关注多模态统一模型评估，不属于图像像素级处理任务。",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "提出了跨学科的多模态统一基准，但方法较为常规。"
    },
    {
        "title": "RECODE: Reasoning Through Code Generation for Visual Question Answering",
        "url": "http://arxiv.org/abs/2510.13756v1",
        "pub_date": "2025-10-15",
        "summary": "Multimodal Large Language Models (MLLMs) struggle with precise reasoning for structured visuals like charts and diagrams, as pixel-based perception lacks a mechanism for verification. To address this, we propose to leverage derendering -- the process of reverse-engineering visuals into executable code -- as a new modality for verifiable visual reasoning. Specifically, we propose RECODE, an agentic framework that first generates multiple candidate programs to reproduce the input image. It then uses a critic to select the most faithful reconstruction and iteratively refines the code. This process not only transforms an ambiguous perceptual task into a verifiable, symbolic problem, but also enables precise calculations and logical inferences later on. On various visual reasoning benchmarks such as CharXiv, ChartQA, and Geometry3K, RECODE significantly outperforms methods that do not leverage code or only use code for drawing auxiliary lines or cropping. Our work demonstrates that grounding visual perception in executable code provides a new path toward more accurate and verifiable multimodal reasoning.",
        "translated": "多模态大语言模型（MLLMs）在处理图表和示意图等结构化视觉内容时，常常在精确推理方面存在困难，因为基于像素的感知缺乏验证机制。为了解决这一问题，我们提出利用去渲染（derendering）——即将视觉图像反向工程为可执行代码的过程——作为一种可验证视觉推理的新模态。具体而言，我们提出了 RECODE，一个智能体框架，首先生成多个候选程序以重现输入图像。然后，它使用一个评估器选择最忠实的重建结果，并对代码进行迭代优化。这一过程不仅将模糊的感知任务转化为可验证的符号问题，还为后续的精确计算和逻辑推理提供了可能。在各种视觉推理基准如 CharXiv、ChartQA 和 Geometry3K 上，RECODE 显著优于不使用代码或仅使用代码绘制辅助线或裁剪图像的方法。我们的工作表明，将视觉感知建立在可执行代码基础上，为更准确和可验证的多模态推理提供了一条新路径。",
        "translated_title": "RECODE：通过代码生成进行推理的视觉问答",
        "label": [],
        "label_reason": "论文不属于低层图像处理，而是视觉问答任务。",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出通过代码生成实现视觉推理，具有新颖性。"
    },
    {
        "title": "InteractiveOmni: A Unified Omni-modal Model for Audio-Visual Multi-turn\n  Dialogue",
        "url": "http://arxiv.org/abs/2510.13747v1",
        "pub_date": "2025-10-15",
        "summary": "We introduce InteractiveOmni, a unified and open-source omni-modal large language model for audio-visual multi-turn interaction, ranging from 4B to 8B parameters, designed to lead the field of lightweight models by offering comprehensive omni-modal understanding and speech generation capabilities. To achieve this, we integrate the vision encoder, audio encoder, large language model, and speech decoder into a unified model for understanding and generation tasks. We design a multi-stage training strategy to ensure robust cross-modal capabilities, including pre-training for omni-modal understanding, followed by post-training with speech conversation and audio-visual interaction. To enable human-like long-term conversational ability, we meticulously curate a multi-turn training dataset that enhances the model's ability to handle complex and multi-turn interactions. To effectively evaluate the multi-turn memory and speech interaction capabilities, we construct the multi-modal multi-turn memory benchmark and the multi-turn speech interaction benchmark. Experiments demonstrate that InteractiveOmni significantly outperforms leading open-source models and provides a more intelligent multi-turn audio-visual experience, particularly in its long-term memory capabilities. Notably, InteractiveOmni-4B is comparable to the much larger model like Qwen2.5-Omni-7B on general benchmarks, and it can retain 97% of the performance of the InteractiveOmni-8B while utilizing only 50% of the model size. Achieving state-of-the-art results against similarly sized models across image, audio, video understanding, and speech generation tasks, InteractiveOmni is an accessible, open-source foundation for next-generation intelligent interactive systems.",
        "translated": "我们提出 InteractiveOmni，一种统一且开源的多模态大语言模型，用于音频-视觉的多轮交互，参数规模从 4B 到 8B 不等，旨在通过提供全面的多模态理解与语音生成能力，引领轻量级模型的发展。为此，我们将视觉编码器、音频编码器、大语言模型和语音解码器集成到一个统一模型中，用于理解和生成任务。我们设计了一种多阶段训练策略，以确保强大的跨模态能力，包括用于多模态理解的预训练，以及随后进行的语音对话和音视频交互的后训练。为了实现类人般的长期对话能力，我们精心构建了一个多轮训练数据集，以增强模型处理复杂和多轮交互的能力。为了有效评估多轮记忆和语音交互能力，我们构建了多模态多轮记忆基准和多轮语音交互基准。实验表明，InteractiveOmni 在多项指标上显著优于领先的开源模型，提供了更加智能的多轮音视频交互体验，特别是在长期记忆能力方面。值得注意的是，InteractiveOmni-4B 在通用基准上表现与更大规模的模型如 Qwen2.5-Omni-7B 相当，并且仅使用 50% 的模型规模即可保留 97% 的 InteractiveOmni-8B 性能。在图像、音频、视频理解和语音生成任务中，InteractiveOmni 在同规模模型中实现了最先进的结果，是下一代智能交互系统的一个可访问、开源的基础。",
        "translated_title": "InteractiveOmni：用于音视频多轮对话的统一全模态模型",
        "label": [],
        "label_reason": "不属于low-level图像处理，是多模态对话模型",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "常规多模态模型设计，无本质创新"
    },
    {
        "title": "UniCalli: A Unified Diffusion Framework for Column-Level Generation and\n  Recognition of Chinese Calligraphy",
        "url": "http://arxiv.org/abs/2510.13745v1",
        "pub_date": "2025-10-15",
        "summary": "Computational replication of Chinese calligraphy remains challenging. Existing methods falter, either creating high-quality isolated characters while ignoring page-level aesthetics like ligatures and spacing, or attempting page synthesis at the expense of calligraphic correctness. We introduce \\textbf{UniCalli}, a unified diffusion framework for column-level recognition and generation. Training both tasks jointly is deliberate: recognition constrains the generator to preserve character structure, while generation provides style and layout priors. This synergy fosters concept-level abstractions that improve both tasks, especially in limited-data regimes. We curated a dataset of over 8,000 digitized pieces, with ~4,000 densely annotated. UniCalli employs asymmetric noising and a rasterized box map for spatial priors, trained on a mix of synthetic, labeled, and unlabeled data. The model achieves state-of-the-art generative quality with superior ligature continuity and layout fidelity, alongside stronger recognition. The framework successfully extends to other ancient scripts, including Oracle bone inscriptions and Egyptian hieroglyphs. Code and data can be viewed in \\href{https://github.com/EnVision-Research/UniCalli}{this URL}.",
        "translated": "中国书法的计算复现仍然具有挑战性。现有方法存在不足，要么生成高质量的单个字符而忽视了页面级别的美学要素（如连笔和间距），要么在尝试页面合成时以牺牲书法正确性为代价。我们引入了**UniCalli**，一种面向列级别的识别与生成的统一扩散框架。联合训练这两个任务是经过深思熟虑的：识别任务约束生成器以保持字符结构，而生成任务则提供了风格和布局的先验知识。这种协同作用促进了概念级别的抽象表示，从而提升了两个任务的性能，尤其在数据受限的情况下表现更为明显。我们整理了一个包含超过 8,000 幅数字化书法作品的数据集，其中约 4,000 幅进行了密集标注。UniCalli 采用非对称噪声和光栅化框图来提供空域先验，并在合成数据、标注数据和未标注数据的混合集上进行训练。该模型在生成质量方面达到了最先进的水平，连笔的连续性和布局的保真度均优于现有方法，同时识别性能也更强。该框架成功地扩展到了其他古代文字，包括甲骨文和埃及象形文字。代码和数据可通过 \\href{https://github.com/EnVision-Research/UniCalli}{此链接}查看。",
        "translated_title": "UniCalli：一种面向列级生成与识别的中文书法统一扩散框架",
        "label": [],
        "label_reason": "不属于low-level图像处理，专注于书法生成与识别",
        "relevance_score": 1,
        "novelty_score": 6,
        "novelty_reason": "提出了统一扩散框架，但属于high-level生成与识别任务"
    },
    {
        "title": "Multi-Scale High-Resolution Logarithmic Grapher Module for Efficient\n  Vision GNNs",
        "url": "http://arxiv.org/abs/2510.13740v1",
        "pub_date": "2025-10-15",
        "summary": "Vision graph neural networks (ViG) have demonstrated promise in vision tasks as a competitive alternative to conventional convolutional neural nets (CNN) and transformers (ViTs); however, common graph construction methods, such as k-nearest neighbor (KNN), can be expensive on larger images. While methods such as Sparse Vision Graph Attention (SVGA) have shown promise, SVGA's fixed step scale can lead to over-squashing and missing multiple connections to gain the same information that could be gained from a long-range link. Through this observation, we propose a new graph construction method, Logarithmic Scalable Graph Construction (LSGC) to enhance performance by limiting the number of long-range links. To this end, we propose LogViG, a novel hybrid CNN-GNN model that utilizes LSGC. Furthermore, inspired by the successes of multi-scale and high-resolution architectures, we introduce and apply a high-resolution branch and fuse features between our high-resolution and low-resolution branches for a multi-scale high-resolution Vision GNN network. Extensive experiments show that LogViG beats existing ViG, CNN, and ViT architectures in terms of accuracy, GMACs, and parameters on image classification and semantic segmentation tasks. Our smallest model, Ti-LogViG, achieves an average top-1 accuracy on ImageNet-1K of 79.9% with a standard deviation of 0.2%, 1.7% higher average accuracy than Vision GNN with a 24.3% reduction in parameters and 35.3% reduction in GMACs. Our work shows that leveraging long-range links in graph construction for ViGs through our proposed LSGC can exceed the performance of current state-of-the-art ViGs. Code is available at https://github.com/mmunir127/LogViG-Official.",
        "translated": "视觉图神经网络（ViG）在视觉任务中展现出作为传统卷积神经网络（CNN）和视觉变换器（ViTs）的有竞争力的替代方案的潜力；然而，常见的图构建方法，如 k-近邻（KNN），在处理大尺寸图像时计算代价较高。虽然稀疏视觉图注意力（SVGA）等方法已显示出前景，但 SVGA 固定的步长尺度可能导致过度压缩问题，并且无法通过多个连接获取与长程连接相同的信息。基于这一观察，我们提出了一种新的图构建方法——对数可扩展图构建（LSGC），通过限制长程连接的数量来提升性能。为此，我们设计了 LogViG，这是一种新颖的 CNN-GNN 混合模型，利用了 LSGC。此外，受多尺度和高分辨率架构成功案例的启发，我们引入并应用了一个高分辨率分支，并在高分辨率和低分辨率分支之间融合特征，构建了一个多尺度高分辨率的视觉图神经网络。大量实验表明，LogViG 在图像分类和语义分割任务中，在准确率、GMACs 和参数数量方面均优于现有的 ViG、CNN 和 ViT 架构。我们最小的模型 Ti-LogViG 在 ImageNet-1K 数据集上的平均 top-1 准确率为 79.9%，标准差为 0.2%，比 Vision GNN 的平均准确率高出 1.7%，同时参数数量减少了 24.3%，GMACs 降低了 35.3%。我们的研究表明，通过所提出的 LSGC 在图构建中利用长程连接，可使 ViG 的性能超越当前最先进的 ViG 模型。代码可在 https://github.com/mmunir127/LogViG-Official 获取。",
        "translated_title": "多尺度高分辨率对数图生成模块用于高效的视觉图神经网络",
        "label": [],
        "label_reason": "论文主要关注图像分类和语义分割，不属于 low-level 图像处理。",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出新的图构建方法 LSGC，对 Vision GNN 有显著改进。"
    },
    {
        "title": "Cyclic Self-Supervised Diffusion for Ultra Low-field to High-field MRI\n  Synthesis",
        "url": "http://arxiv.org/abs/2510.13735v1",
        "pub_date": "2025-10-15",
        "summary": "Synthesizing high-quality images from low-field MRI holds significant potential. Low-field MRI is cheaper, more accessible, and safer, but suffers from low resolution and poor signal-to-noise ratio. This synthesis process can reduce reliance on costly acquisitions and expand data availability. However, synthesizing high-field MRI still suffers from a clinical fidelity gap. There is a need to preserve anatomical fidelity, enhance fine-grained structural details, and bridge domain gaps in image contrast. To address these issues, we propose a \\emph{cyclic self-supervised diffusion (CSS-Diff)} framework for high-field MRI synthesis from real low-field MRI data. Our core idea is to reformulate diffusion-based synthesis under a cycle-consistent constraint. It enforces anatomical preservation throughout the generative process rather than just relying on paired pixel-level supervision. The CSS-Diff framework further incorporates two novel processes. The slice-wise gap perception network aligns inter-slice inconsistencies via contrastive learning. The local structure correction network enhances local feature restoration through self-reconstruction of masked and perturbed patches. Extensive experiments on cross-field synthesis tasks demonstrate the effectiveness of our method, achieving state-of-the-art performance (e.g., 31.80 $\\pm$ 2.70 dB in PSNR, 0.943 $\\pm$ 0.102 in SSIM, and 0.0864 $\\pm$ 0.0689 in LPIPS). Beyond pixel-wise fidelity, our method also preserves fine-grained anatomical structures compared with the original low-field MRI (e.g., left cerebral white matter error drops from 12.1$\\%$ to 2.1$\\%$, cortex from 4.2$\\%$ to 3.7$\\%$). To conclude, our CSS-Diff can synthesize images that are both quantitatively reliable and anatomically consistent.",
        "translated": "从低场 MRI 数据合成高质量图像具有重要潜力。低场 MRI 成本更低、更易获取、也更安全，但其分辨率较低且信噪比差。通过合成过程，可以减少对昂贵采集设备的依赖，并扩展数据的可用性。然而，当前的高场 MRI 合成仍然存在临床保真度的不足。有必要在合成中保持解剖结构的准确性，增强细粒度的结构细节，并弥合图像对比度方面的领域差异。为了解决这些问题，我们提出了一种名为 \\emph{循环自监督扩散（CSS-Diff）} 的框架，用于从真实的低场 MRI 数据中合成高场 MRI。我们的核心思想是在循环一致的约束下重构基于扩散的合成方法。这种方法在整个生成过程中强制保持解剖结构，而不仅仅依赖于像素级的配对监督。CSS-Diff 框架进一步结合了两个新颖的处理过程。切片级差异感知网络通过对比学习对齐切片间的不一致性。局部结构校正网络则通过遮蔽和扰动图像块的自重建来增强局部特征恢复。在跨场强合成任务上的大量实验表明了我们方法的有效性，达到了当前最先进的性能（例如，PSNR 为 31.80 $\\pm$ 2.70 dB，SSIM 为 0.943 $\\pm$ 0.102，LPIPS 为 0.0864 $\\pm$ 0.0689）。除了像素级保真度之外，与原始低场 MRI 相比，我们的方法还能保持更精细的解剖结构（例如，左脑白质误差从 12.1$\\%$ 降至 2.1$\\%$，皮层误差从 4.2$\\%$ 降至 3.7$\\%$）。综上所述，我们的 CSS-Diff 能够合成在定量评估和解剖一致性方面都表现良好的图像。",
        "translated_title": "循环自监督扩散用于超低场到高场MRI合成",
        "label": [
            "医学图像增强",
            "图像去噪",
            "图像恢复"
        ],
        "label_reason": "论文专注于从低场MRI合成高质量高场MRI，涉及图像恢复与医学增强",
        "relevance_score": 8,
        "novelty_score": 8,
        "novelty_reason": "提出循环自监督扩散框架，结合新网络结构提升合成效果"
    },
    {
        "title": "LiFMCR: Dataset and Benchmark for Light Field Multi-Camera Registration",
        "url": "http://arxiv.org/abs/2510.13729v1",
        "pub_date": "2025-10-15",
        "summary": "We present LiFMCR, a novel dataset for the registration of multiple micro lens array (MLA)-based light field cameras. While existing light field datasets are limited to single-camera setups and typically lack external ground truth, LiFMCR provides synchronized image sequences from two high-resolution Raytrix R32 plenoptic cameras, together with high-precision 6-degrees of freedom (DoF) poses recorded by a Vicon motion capture system. This unique combination enables rigorous evaluation of multi-camera light field registration methods.   As a baseline, we provide two complementary registration approaches: a robust 3D transformation estimation via a RANSAC-based method using cross-view point clouds, and a plenoptic PnP algorithm estimating extrinsic 6-DoF poses from single light field images. Both explicitly integrate the plenoptic camera model, enabling accurate and scalable multi-camera registration. Experiments show strong alignment with the ground truth, supporting reliable multi-view light field processing.   Project page: https://lifmcr.github.io/",
        "translated": "我们提出了 LiFMCR，这是一个用于多微透镜阵列（MLA）光场相机注册的新数据集。现有的光场数据集通常局限于单相机设置，并且普遍缺乏外部真实值。LiFMCR 提供了来自两台高分辨率 Raytrix R32 光场相机的同步图像序列，并配有通过 Vicon 运动捕捉系统记录的高精度六自由度（DoF）姿态。这一独特组合使得对多相机光场注册方法的严格评估成为可能。\n\n作为基准，我们提供了两种互补的注册方法：一种是通过基于 RANSAC 的方法利用跨视角点云进行鲁棒的三维变换估计，另一种是光场 PnP 算法，用于从单光场图像中估计外部六自由度姿态。两种方法均明确集成了光场相机模型，从而实现了精确且可扩展的多相机注册。实验结果表明，与真实值具有高度对齐性，支持可靠的多视角光场处理。\n\n项目页面：https://lifmcr.github.io/",
        "translated_title": "LiFMCR：光场多摄像头配准的数据集与基准",
        "label": [],
        "label_reason": "论文主要关注光场相机注册，不属于像素级图像恢复或增强",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出了新的光场多相机注册数据集和基准"
    },
    {
        "title": "NExT-OMNI: Towards Any-to-Any Omnimodal Foundation Models with Discrete\n  Flow Matching",
        "url": "http://arxiv.org/abs/2510.13721v1",
        "pub_date": "2025-10-15",
        "summary": "Next-generation multimodal foundation models capable of any-to-any cross-modal generation and multi-turn interaction will serve as core components of artificial general intelligence systems, playing a pivotal role in human-machine interaction. However, most existing multimodal models remain constrained by autoregressive architectures, whose inherent limitations prevent a balanced integration of understanding and generation capabilities. Although hybrid and decoupling strategies have been explored to address these tasks within unified frameworks separately, their redundant, non-integrated designs limit their applicability to broader scenarios, such as cross-modal retrieval.In this work, we introduce NExT-OMNI, an open-source omnimodal foundation model that achieves unified modeling through discrete flow paradigms. By leveraging metric-induced probability paths and kinetic optimal velocities, NExT-OMNI natively supports any-to-any understanding and generation with enhanced response efficiency, while enabling broader application scenarios through concise unified representations rather than task-decoupled designs. Trained on large-scale interleaved text, image, video, and audio data, NExT-OMNI delivers competitive performance on multimodal generation and understanding benchmarks, while outperforming prior unified models in multi-turn multimodal interaction and cross-modal retrieval, highlighting its architectural advantages as a next-generation multimodal foundation model. To advance further research, we release training details, data protocols, and open-source both the code and model checkpoints.",
        "translated": "下一代能够实现任意模态到任意模态跨模态生成和多轮交互的多模态基础模型，将成为通用人工智能系统的核心组成部分，在人机交互中发挥关键作用。然而，目前大多数多模态模型仍受自回归架构的限制，其固有局限性阻碍了理解能力和生成能力之间的平衡整合。尽管已有研究探索了混合架构和任务解耦策略，试图在统一框架内分别处理这些任务，但其冗余且非整合的设计限制了其在更广泛场景（如跨模态检索）中的应用。在本工作中，我们提出 NExT-OMNI，一个开源的全模态基础模型，通过离散流范式实现统一建模。NExT-OMNI 利用度量诱导的概率路径和动力最优速度，在增强响应效率的同时，原生支持任意模态到任意模态的理解与生成，并通过简洁的统一表征而非任务解耦的设计，实现了更广泛的应用场景。NExT-OMNI 在大规模交织的文本、图像、视频和音频数据上进行训练，在多模态生成和理解基准上展现出具有竞争力的性能，并在多轮多模态交互和跨模态检索方面优于以往的统一模型，突显了其作为下一代多模态基础模型的架构优势。为了推动进一步的研究，我们发布了训练细节、数据协议，并开源了代码和模型检查点。",
        "translated_title": "NExT-OMNI：基于离散流匹配的任意到任意全模态基础模型",
        "label": [],
        "label_reason": "论文主要研究多模态模型，不属于图像像素级恢复或增强任务。",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出离散流匹配的新范式，对多模态模型架构有显著改进。"
    },
    {
        "title": "Epistemic-aware Vision-Language Foundation Model for Fetal Ultrasound\n  Interpretation",
        "url": "http://arxiv.org/abs/2510.12953v1",
        "pub_date": "2025-10-14",
        "summary": "Recent medical vision-language models have shown promise on tasks such as VQA, report generation, and anomaly detection. However, most are adapted to structured adult imaging and underperform in fetal ultrasound, which poses challenges of multi-view image reasoning, numerous diseases, and image diversity. To bridge this gap, we introduce FetalMind, a medical AI system tailored to fetal ultrasound for both report generation and diagnosis. Guided by clinical workflow, we propose Salient Epistemic Disentanglement (SED), which injects an expert-curated bipartite graph into the model to decouple view-disease associations and to steer preference selection along clinically faithful steps via reinforcement learning. This design mitigates variability across diseases and heterogeneity across views, reducing learning bottlenecks while aligning the model's inference with obstetric practice. To train FetalMind at scale, we curate FetalSigma-1M dataset, the first large-scale fetal ultrasound report corpus, comprising 20K reports from twelve medical centers, addressing the scarcity of domain data. Extensive experiments show that FetalMind outperforms open- and closed-source baselines across all gestational stages, achieving +14% average gains and +61.2% higher accuracy on critical conditions while remaining efficient, stable, and scalable. Project Page: https://hexiao0275.github.io/FetalMind.",
        "translated": "近年来，医疗视觉-语言模型在视觉问答（VQA）、报告生成和异常检测等任务上表现出潜力。然而，大多数模型适用于结构化的成人影像数据，在胎儿超声任务中表现欠佳，这面临着多视角图像推理、大量疾病类别和图像多样性的挑战。为弥合这一差距，我们提出了FetalMind，一个专为胎儿超声定制的医疗人工智能系统，支持报告生成与诊断任务。受临床工作流程的启发，我们提出了显著性认识解耦（Salient Epistemic Disentanglement, SED）方法，该方法将专家构建的二部图注入模型，以解耦视角与疾病之间的关联，并通过强化学习引导模型在临床可靠的步骤中进行偏好选择。这一设计缓解了不同疾病之间的差异性以及不同视角之间的异质性，从而降低学习瓶颈，同时使模型的推理过程与产科实践保持一致。为了大规模训练FetalMind，我们构建了FetalSigma-1M数据集，这是首个大规模胎儿超声报告语料库，包含了来自十二个医疗机构的20K份报告，解决了该领域数据稀缺的问题。大量实验表明，FetalMind在所有孕周阶段均优于开源和闭源基线模型，平均提升14%，在关键疾病上的准确率提高61.2%，同时保持高效、稳定和可扩展。项目主页：https://hexiao0275.github.io/FetalMind.",
        "translated_title": "具有认知感知能力的视觉-语言基础模型在胎儿超声解释中的应用",
        "label": [],
        "label_reason": "论文聚焦医学影像与自然语言处理，非直接推荐系统相关。",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出临床工作流引导的模型设计，数据集和方法有创新性。"
    },
    {
        "title": "DeepMMSearch-R1: Empowering Multimodal LLMs in Multimodal Web Search",
        "url": "http://arxiv.org/abs/2510.12801v1",
        "pub_date": "2025-10-14",
        "summary": "Multimodal Large Language Models (MLLMs) in real-world applications require access to external knowledge sources and must remain responsive to the dynamic and ever-changing real-world information in order to address information-seeking and knowledge-intensive user queries. Existing approaches, such as retrieval augmented generation (RAG) methods, search agents, and search equipped MLLMs, often suffer from rigid pipelines, excessive search calls, and poorly constructed search queries, which result in inefficiencies and suboptimal outcomes. To address these limitations, we present DeepMMSearch-R1, the first multimodal LLM capable of performing on-demand, multi-turn web searches and dynamically crafting queries for both image and text search tools. Specifically, DeepMMSearch-R1 can initiate web searches based on relevant crops of the input image making the image search more effective, and can iteratively adapt text search queries based on retrieved information, thereby enabling self-reflection and self-correction. Our approach relies on a two-stage training pipeline: a cold start supervised finetuning phase followed by an online reinforcement learning optimization. For training, we introduce DeepMMSearchVQA, a novel multimodal VQA dataset created through an automated pipeline intermixed with real-world information from web search tools. This dataset contains diverse, multi-hop queries that integrate textual and visual information, teaching the model when to search, what to search for, which search tool to use and how to reason over the retrieved information. We conduct extensive experiments across a range of knowledge-intensive benchmarks to demonstrate the superiority of our approach. Finally, we analyze the results and provide insights that are valuable for advancing multimodal web-search.",
        "translated": "在现实世界应用中，多模态大语言模型（MLLMs）需要访问外部知识源，并且必须能够响应动态变化的现实世界信息，以应对用户的信息查询和知识密集型请求。现有方法，如检索增强生成（RAG）方法、搜索代理和配备搜索功能的 MLLMs，通常存在流水线僵化、搜索调用过多以及搜索查询构建不完善的问题，导致效率低下和结果次优。为了解决这些限制，我们提出了 DeepMMSearch-R1，这是首个能够按需进行多轮网页搜索，并为图像和文本搜索工具动态构建查询的多模态大语言模型。具体来说，DeepMMSearch-R1 可以根据输入图像的相关裁剪区域发起网页搜索，从而提高图像搜索的有效性；同时能够根据检索到的信息迭代调整文本搜索查询，实现自我反思和自我修正。我们的方法依赖于一个两阶段的训练流水线：首先进行冷启动阶段的监督微调，然后进行在线强化学习优化。为了训练，我们引入了 DeepMMSearchVQA，这是一个通过自动化流水线与网页搜索工具中的现实世界信息混合生成的新型多模态视觉问答（VQA）数据集。该数据集包含多样化的多跳查询，融合了文本和视觉信息，教导模型何时进行搜索、搜索什么内容、使用哪个搜索工具以及如何对检索到的信息进行推理。我们在多个知识密集型基准上进行了广泛的实验，以验证我们方法的优越性。最后，我们对结果进行了分析，并提供了对多模态网页搜索研究具有重要价值的见解。",
        "translated_title": "DeepMMSearch-R1：在多模态网页搜索中增强多模态大语言模型",
        "label": [
            "多模态推荐",
            "LLM生成式推荐"
        ],
        "label_reason": "论文涉及多模态LLM在搜索中的应用，可能适用于推荐系统。",
        "relevance_score": 5,
        "novelty_score": 8,
        "novelty_reason": "提出多模态LLM动态构建搜索查询的新方法，具有创新性。"
    },
    {
        "title": "CTRL-Rec: Controlling Recommender Systems With Natural Language",
        "url": "http://arxiv.org/abs/2510.12742v1",
        "pub_date": "2025-10-14",
        "summary": "When users are dissatisfied with recommendations from a recommender system, they often lack fine-grained controls for changing them. Large language models (LLMs) offer a solution by allowing users to guide their recommendations through natural language requests (e.g., \"I want to see respectful posts with a different perspective than mine\"). We propose a method, CTRL-Rec, that allows for natural language control of traditional recommender systems in real-time with computational efficiency. Specifically, at training time, we use an LLM to simulate whether users would approve of items based on their language requests, and we train embedding models that approximate such simulated judgments. We then integrate these user-request-based predictions into the standard weighting of signals that traditional recommender systems optimize. At deployment time, we require only a single LLM embedding computation per user request, allowing for real-time control of recommendations. In experiments with the MovieLens dataset, our method consistently allows for fine-grained control across a diversity of requests. In a study with 19 Letterboxd users, we find that CTRL-Rec was positively received by users and significantly enhanced users' sense of control and satisfaction with recommendations compared to traditional controls.",
        "translated": "当用户对推荐系统提供的推荐结果不满意时，他们通常缺乏对推荐结果进行细粒度调整的控制手段。大语言模型（LLM）提供了一种解决方案，允许用户通过自然语言请求（例如，“我想看到与我的观点不同的尊重性帖子”）来引导推荐结果。我们提出了一种方法 CTRL-Rec，它能够在计算效率的前提下，实现对传统推荐系统的实时自然语言控制。具体而言，在训练阶段，我们使用 LLM 来模拟用户是否会基于他们的语言请求批准某些物料，并训练嵌入模型以逼近这种模拟的判断结果。随后，我们将这些基于用户请求的预测结果整合到传统推荐系统优化的标准信号加权中。在部署阶段，我们仅需为每个用户请求进行一次 LLM 嵌入计算，从而实现实时的推荐控制。在 MovieLens 数据集上的实验表明，我们的方法在各种请求下都能实现一致的细粒度控制。在与 19 位 Letterboxd 用户进行的研究中，我们发现 CTRL-Rec 得到了用户的积极反馈，并显著增强了用户对推荐结果的控制感和满意度，相比传统的控制方式。",
        "translated_title": "CTRL-Rec：用自然语言控制推荐系统",
        "label": [
            "LLM生成式推荐",
            "精排"
        ],
        "label_reason": "结合LLM实现自然语言控制推荐结果，增强用户控制感和满意度",
        "relevance_score": 8,
        "novelty_score": 9,
        "novelty_reason": "首次将LLM用于实时推荐控制，方法新颖且有效"
    },
    {
        "title": "SAIL-Embedding Technical Report: Omni-modal Embedding Foundation Model",
        "url": "http://arxiv.org/abs/2510.12709v2",
        "pub_date": "2025-10-14",
        "summary": "Multimodal embedding models aim to yield informative unified representations that empower diverse cross-modal tasks. Despite promising developments in the evolution from CLIP-based dual-tower architectures to large vision-language models, prior works still face unavoidable challenges in real-world applications and business scenarios, such as the limited modality support, unstable training mechanisms, and industrial domain gaps. In this work, we introduce SAIL-Embedding, an omni-modal embedding foundation model that addresses these issues through tailored training strategies and architectural design. In the optimization procedure, we propose a multi-stage training scheme to boost the multifaceted effectiveness of representation learning. Specifically, the content-aware progressive training aims to enhance the model's adaptability to diverse downstream tasks and master enriched cross-modal proficiency. The collaboration-aware recommendation enhancement training further adapts multimodal representations for recommendation scenarios by distilling knowledge from sequence-to-item and ID-to-item embeddings while mining user historical interests. Concurrently, we develop the stochastic specialization and dataset-driven pattern matching to strengthen model training flexibility and generalizability. Experimental results show that SAIL-Embedding achieves SOTA performance compared to other methods in different retrieval tasks. In online experiments across various real-world scenarios integrated with our model, we observe a significant increase in Lifetime (LT), which is a crucial indicator for the recommendation experience. For instance, the model delivers the 7-day LT gain of +0.5% in the Douyin-Selected scenario. For the Douyin feed rank model, the match features produced by SAIL-Embedding yield a +0.1% AUC gain.",
        "translated": "多模态嵌入模型旨在生成具有信息量的统一表示，以支持多种跨模态任务。尽管从基于CLIP的双塔架构发展到大视觉-语言模型已取得令人期待的进展，但先前的工作在实际应用和业务场景中仍面临不可避免的挑战，如模态支持有限、训练机制不稳定以及与工业领域的差距。在本文中，我们提出了 SAIL-Embedding，这是一个全模态嵌入基础模型，通过定制化的训练策略和架构设计解决上述问题。在优化过程中，我们提出了一种多阶段训练方案，以提升表示学习在多方面上的有效性。具体而言，内容感知的渐进式训练旨在增强模型对下游任务的适应能力，并掌握丰富的跨模态能力。协作感知的推荐增强训练则通过从序列到物料和ID到物料的嵌入中提炼知识，同时挖掘用户的历史兴趣，进一步将多模态表示适配到推荐场景。与此同时，我们开发了随机专业化和数据集驱动的模式匹配方法，以增强模型训练的灵活性和泛化能力。实验结果表明，SAIL-Embedding在不同检索任务中相较其他方法实现了最先进的（SOTA）性能。在多个实际场景中与我们的模型集成进行的在线实验中，我们观察到用户生命周期（Lifetime, LT）有显著提升，这是衡量推荐体验的关键指标。例如，在Douyin-Selected场景中，模型带来了7天LT指标+0.5%的增长。对于Douyin的流推荐排序模型，SAIL-Embedding生成的匹配特征带来了+0.1%的AUC提升。",
        "translated_title": "SAIL-Embedding 技术报告：通用模态嵌入基础模型",
        "label": [
            "多模态推荐",
            "精排",
            "序列推荐"
        ],
        "label_reason": "多模态嵌入模型用于推荐场景，结合序列建模与ID建模",
        "relevance_score": 8,
        "novelty_score": 8,
        "novelty_reason": "提出多阶段训练策略和模式匹配方法，提升模型灵活性和表现"
    },
    {
        "title": "The Role of Parametric Injection-A Systematic Study of Parametric\n  Retrieval-Augmented Generation",
        "url": "http://arxiv.org/abs/2510.12668v1",
        "pub_date": "2025-10-14",
        "summary": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by retrieving external documents. As an emerging form of RAG, parametric retrieval-augmented generation (PRAG) encodes documents as model parameters (i.e., LoRA modules) and injects these representations into the model during inference, enabling interaction between the LLM and documents at parametric level. Compared with directly placing documents in the input context, PRAG is more efficient and has the potential to offer deeper model-document interaction. Despite its growing attention, the mechanism underlying parametric injection remains poorly understood. In this work, we present a systematic study of PRAG to clarify the role of parametric injection, showing that parameterized documents capture only partial semantic information of documents, and relying on them alone yields inferior performance compared to interaction at text level. However, these parametric representations encode high-level document information that can enhance the model's understanding of documents within the input context. When combined parameterized documents with textual documents, the model can leverage relevant information more effectively and become more robust to noisy inputs, achieving better performance than either source alone. We recommend jointly using parameterized and textual documents and advocate for increasing the information content of parametric representations to advance PRAG.",
        "translated": "检索增强生成（RAG）通过检索外部文档来增强大语言模型（LLM）。作为一种新兴形式的RAG，参数化检索增强生成（PRAG）将文档编码为模型参数（即LoRA模块），并在推理过程中将这些表示注入模型，从而在参数层面实现LLM与文档之间的交互。与直接将文档放入输入上下文相比，PRAG更加高效，并且有望提供更深层次的模型-文档交互。尽管PRAG已引起越来越多的关注，但其参数注入机制仍不明确。在本工作中，我们对PRAG进行了系统研究，以阐明参数注入的作用，表明参数化的文档仅捕捉了文档的部分语义信息，单独依赖它们会导致性能不如文本层面的交互。然而，这些参数表示编码了文档的高层信息，可以增强模型对输入上下文中文档的理解。将参数化文档与文本文档结合使用时，模型能够更有效地利用相关信息，并对输入中的噪声具有更强的鲁棒性，从而在性能上优于任一单独来源。我们建议联合使用参数化文档与文本文档，并倡导提升参数表示的信息量以推动PRAG的发展。",
        "translated_title": "The Role of Parametric Injection—A Systematic Study of Parametric Retrieval-Augmented Generation\n\n参数注入的作用——参数化检索增强生成的系统研究",
        "label": [
            "LLM生成式推荐"
        ],
        "label_reason": "论文研究PRAG在生成式模型中的作用，可能用于生成式推荐系统。",
        "relevance_score": 5,
        "novelty_score": 7,
        "novelty_reason": "系统性研究PRAG机制，提出参数化文档与文本文档结合的新方法。"
    },
    {
        "title": "SMILE: SeMantic Ids Enhanced CoLd Item Representation for Click-through\n  Rate Prediction in E-commerce SEarch",
        "url": "http://arxiv.org/abs/2510.12604v1",
        "pub_date": "2025-10-14",
        "summary": "With the rise of modern search and recommendation platforms, insufficient collaborative information of cold-start items exacerbates the Matthew effect of existing platform items, challenging platform diversity and becoming a longstanding issue. Existing methods align items' side content with collaborative information to transfer collaborative signals from high-popularity items to cold-start items. However, these methods fail to account for the asymmetry between collaboration and content, nor the fine-grained differences among items. To address these issues, we propose SMILE, an item representation enhancement approach based on fused alignment of semantic IDs. Specifically, we use RQ-OPQ encoding to quantize item content and collaborative information, followed by a two-step alignment: RQ encoding transfers shared collaborative signals across items, while OPQ encoding learns differentiated information of items. Comprehensive offline experiments on large-scale industrial datasets demonstrate superiority of SMILE, and rigorous online A/B tests confirm statistically significant improvements: item CTR +1.66%, buyers +1.57%, and order volume +2.17%.",
        "translated": "随着现代搜索与推荐系统的兴起，冷启动物料的协同信息不足加剧了平台现有物料的马太效应，挑战平台多样性，并成为一个长期存在的问题。现有方法将物料的侧边内容与协同信息对齐，以将协同信号从高热度物料转移到冷启动物料。然而，这些方法未能考虑协同与内容之间的不对称性，也未考虑物料之间的细粒度差异。为了解决这些问题，我们提出了 SMILE，一种基于语义 ID 融合对齐的物料表示增强方法。具体而言，我们使用 RQ-OPQ 编码对物料内容和协同信息进行量化，随后进行两步对齐：RQ 编码跨物料传递共享的协同信号，而 OPQ 编码学习物料的差异化信息。在大规模工业数据集上的全面离线实验验证了 SMILE 的优越性，严格的在线 A/B 测试也确认了其具有统计显著性的提升效果：物料点击率提升 1.66%，买家数量增加 1.57%，订单量增长 2.17%。",
        "translated_title": "SMILE：语义ID增强的冷启动物料表征用于电商搜索中的点击率预测",
        "label": [
            "精排",
            "冷启动",
            "General Recommendation Techniques",
            "Click-through Rate Prediction"
        ],
        "label_reason": "论文聚焦于冷启动商品的CTR预测，属于推荐系统核心问题",
        "relevance_score": 8,
        "novelty_score": 7,
        "novelty_reason": "提出了基于语义ID融合对齐的新方法，改进了冷启动建模"
    },
    {
        "title": "Leveraging Language Semantics for Collaborative Filtering with TextGCN\n  and TextGCN-MLP: Zero-Shot vs In-Domain Performance",
        "url": "http://arxiv.org/abs/2510.12461v1",
        "pub_date": "2025-10-14",
        "summary": "In recent years, various approaches have been proposed to leverage large language models (LLMs) for incorporating textual information about items into recommender systems. Existing methods primarily focus on either fine-tuning LLMs to generate recommendations or integrating LLM-based embeddings into downstream models. In this work, we follow the latter direction and propose \\textbf{TextGCN}, which applies parameter-free graph convolution layers directly over LLM-based item-title embeddings, instead of learning ID-based embeddings as in traditional methods. By combining language semantics with graph message passing, this architecture achieves state-of-the-art zero-shot performance, significantly outperforming prior approaches. Furthermore, we introduce \\textbf{TextGCN-MLP}, which extends TextGCN with a trainable multilayer perceptron trained using a contrastive loss, achieving state-of-the-art in-domain performance on recommendation benchmarks. However, the zero-shot performance of TextGCN-MLP remains lower than that of TextGCN, highlighting the trade-off between in-domain specialization and zero-shot generalization. We release our code on github at \\href{https://github.com/ChernovAndrey/TFCE}{github.com/ChernovAndrey/TFCE}.",
        "translated": "近年来，各种方法被提出以利用大语言模型（LLMs）将物品的文本信息引入推荐系统。现有方法主要集中在两个方面：一是微调LLMs以生成推荐结果，二是将基于LLM的嵌入向量整合到下游模型中。在本工作中，我们遵循后者，并提出了**TextGCN**，该方法直接在基于LLM的物品标题嵌入上应用无参数图卷积层，而不是像传统方法那样学习基于ID的嵌入。通过将语言语义与图消息传递相结合，该架构实现了最先进的零样本性能，显著优于先前的方法。此外，我们引入了**TextGCN-MLP**，该方法通过对比损失训练一个可训练的多层感知机（multilayer perceptron）来扩展TextGCN，在推荐基准测试中实现了最先进的域内性能。然而，TextGCN-MLP的零样本性能仍然低于TextGCN，突显了域内专业化与零样本泛化之间的权衡。我们的代码已发布在github上：\\href{https://github.com/ChernovAndrey/TFCE}{github.com/ChernovAndrey/TFCE}。",
        "translated_title": "利用语言语义进行协同过滤的推荐系统：TextGCN与TextGCN-MLP的零样本与领域内性能对比",
        "label": [
            "图神经网络推荐",
            "通用推荐技术"
        ],
        "label_reason": "结合图神经网络与文本语义，改进协同过滤方法。",
        "relevance_score": 8,
        "novelty_score": 8,
        "novelty_reason": "提出参数无关的图卷积方法，结合对比学习提升推荐效果。"
    },
    {
        "title": "A Hierarchical Quantized Tokenization Framework for Task-Adaptive Graph\n  Representation Learning",
        "url": "http://arxiv.org/abs/2510.12369v1",
        "pub_date": "2025-10-14",
        "summary": "Recent progress in language and vision foundation models demonstrates the importance of discrete token interfaces that transform complex inputs into compact sequences for large-scale modeling. Extending this paradigm to graphs requires a tokenization scheme that handles non-Euclidean structures and multi-scale dependencies efficiently. Existing approaches to graph tokenization, linearized, continuous, and quantized, remain limited in adaptability and efficiency. In particular, most current quantization-based tokenizers organize hierarchical information in fixed or task-agnostic ways, which may either over-represent or under-utilize structural cues, and lack the ability to dynamically reweight contributions from different levels without retraining the encoder. This work presents a hierarchical quantization framework that introduces a self-weighted mechanism for task-adaptive aggregation across multiple scales. The proposed method maintains a frozen encoder while modulating information flow through a lightweight gating process, enabling parameter-efficient adaptation to diverse downstream tasks. Experiments on benchmark datasets for node classification and link prediction demonstrate consistent improvements over strong baselines under comparable computational budgets.",
        "translated": "近期在语言和视觉基础模型方面的进展表明，离散的token接口在将复杂输入转化为紧凑序列以进行大规模建模中的重要性。将这一范式扩展到图结构需要一种能够高效处理非欧几里得结构和多尺度依赖关系的token化方案。现有的图token化方法，包括线性化、连续化和量化方法，在适应性和效率方面仍存在限制。特别是，大多数当前基于量化的tokenizer以固定或任务无关的方式组织层次信息，这可能导致对结构线索的过度表示或利用不足，并且无法在不重新训练编码器的情况下动态调整不同层次的贡献权重。本文提出了一种层次量化框架，引入了一种自加权机制，用于在多个尺度上进行任务自适应的聚合。所提出的方法在保持冻结编码器的同时，通过轻量级的门控过程调节信息流动，从而在不同下游任务中实现参数高效的适应。在节点分类和链接预测基准数据集上的实验表明，在可比较的计算预算下，该方法在多个任务中均优于强大的基线模型。",
        "translated_title": "一种面向任务自适应图表示学习的层次量化分词框架",
        "label": [],
        "label_reason": "论文聚焦图表示学习，未明确涉及推荐系统环节",
        "relevance_score": 3,
        "novelty_score": 7,
        "novelty_reason": "提出自加权机制改进图量化方法，有一定创新性"
    },
    {
        "title": "Simple Projection Variants Improve ColBERT Performance",
        "url": "http://arxiv.org/abs/2510.12327v1",
        "pub_date": "2025-10-14",
        "summary": "Multi-vector dense retrieval methods like ColBERT systematically use a single-layer linear projection to reduce the dimensionality of individual vectors. In this study, we explore the implications of the MaxSim operator on the gradient flows of the training of multi-vector models and show that such a simple linear projection has inherent, if non-critical, limitations in this setting. We then discuss the theoretical improvements that could result from replacing this single-layer projection with well-studied alternative feedforward linear networks (FFN), such as deeper, non-linear FFN blocks, GLU blocks, and skip-connections, could alleviate these limitations. Through the design and systematic evaluation of alternate projection blocks, we show that better-designed final projections positively impact the downstream performance of ColBERT models. We highlight that many projection variants outperform the original linear projections, with the best-performing variants increasing average performance on a range of retrieval benchmarks across domains by over 2 NDCG@10 points. We then conduct further exploration on the individual parameters of these projections block in order to understand what drives this empirical performance, highlighting the particular importance of upscaled intermediate projections and residual connections. As part of these ablation studies, we show that numerous suboptimal projection variants still outperform the traditional single-layer projection across multiple benchmarks, confirming our hypothesis. Finally, we observe that this effect is consistent across random seeds, further confirming that replacing the linear layer of ColBERT models is a robust, drop-in upgrade.",
        "translated": "诸如 ColBERT 等多向量密集召回方法通常使用单层线性投影来降低单个向量的维度。在本研究中，我们探讨了 MaxSim 运算符在多向量模型训练过程中的梯度流动所带来的影响，并表明在这一设置下，这种简单的线性投影存在固有但非关键的限制。我们进一步讨论了理论上的改进，即将该单层投影替换为已被广泛研究的替代前馈线性网络（FFN），例如更深的非线性 FFN 模块、GLU 模块以及跳跃连接（skip-connections），从而缓解这些限制。通过对替代投影模块的设计及其系统性评估，我们证明了更合理设计的最终投影对 ColBERT 模型的下游性能具有积极影响。我们发现，许多投影变体优于原始的线性投影，其中表现最好的变体在多个跨领域的召回基准上将平均性能提升了超过 2 个 NDCG@10 分数点。随后，我们对这些投影模块的单个参数进行了进一步探索，以理解其经验性能背后的关键因素，强调了中间投影的扩展（upscaled）和残差连接的特别重要性。作为消融实验的一部分，我们展示了即使许多次优的投影变体，也能在多个基准上优于传统的单层投影，从而验证了我们的假设。最后，我们观察到这一效果在不同随机种子之间是一致的，进一步确认了替换 ColBERT 模型中的线性层是一种鲁棒且可直接替换的性能提升方式。",
        "translated_title": "简单投影变体提升ColBERT性能",
        "label": [],
        "label_reason": "论文聚焦于密集检索方法优化，不直接涉及推荐系统",
        "relevance_score": 3,
        "novelty_score": 6,
        "novelty_reason": "提出改进投影层的结构，有一定方法创新"
    },
    {
        "title": "Causal Inspired Multi Modal Recommendation",
        "url": "http://arxiv.org/abs/2510.12325v1",
        "pub_date": "2025-10-14",
        "summary": "Multimodal recommender systems enhance personalized recommendations in e-commerce and online advertising by integrating visual, textual, and user-item interaction data. However, existing methods often overlook two critical biases: (i) modal confounding, where latent factors (e.g., brand style or product category) simultaneously drive multiple modalities and influence user preference, leading to spurious feature-preference associations; (ii) interaction bias, where genuine user preferences are mixed with noise from exposure effects and accidental clicks. To address these challenges, we propose a Causal-inspired multimodal Recommendation framework. Specifically, we introduce a dual-channel cross-modal diffusion module to identify hidden modal confounders, utilize back-door adjustment with hierarchical matching and vector-quantized codebooks to block confounding paths, and apply front-door adjustment combined with causal topology reconstruction to build a deconfounded causal subgraph. Extensive experiments on three real-world e-commerce datasets demonstrate that our method significantly outperforms state-of-the-art baselines while maintaining strong interpretability.",
        "translated": "多模态推荐系统通过融合视觉、文本和用户-物料交互数据，提升了电子商务和在线广告中的个性化推荐效果。然而，现有方法通常忽略了两种关键偏差：(i) 模态混淆，其中潜在因子（例如品牌风格或产品类别）同时驱动多个模态，并影响用户偏好，导致虚假的特征-偏好关联；(ii) 交互偏差，其中真实的用户偏好与曝光效应和偶然点击带来的噪声混合。为了解决这些挑战，我们提出了一种受因果启发的多模态推荐框架。具体而言，我们引入了一个双通道跨模态扩散模块以识别隐藏的模态混淆因子，利用分层匹配和矢量量化码本进行后门调整以阻断混淆路径，并应用前门调整结合因果拓扑重构，以构建去混淆的因果子图。在三个真实世界电子商务数据集上的广泛实验表明，我们的方法在保持强大可解释性的同时，显著优于最先进的基线方法。",
        "translated_title": "因果启发的多模态推荐",
        "label": [
            "多模态推荐",
            "因果推理",
            "推荐系统公平性/可解释性"
        ],
        "label_reason": "论文聚焦多模态推荐并引入因果推理解决关键偏差问题。",
        "relevance_score": 8,
        "novelty_score": 8,
        "novelty_reason": "提出双通道跨模态扩散模块和因果拓扑重构，具有显著创新性。"
    },
    {
        "title": "Fantastic (small) Retrievers and How to Train Them:\n  mxbai-edge-colbert-v0 Tech Report",
        "url": "http://arxiv.org/abs/2510.14880v1",
        "pub_date": "2025-10-16",
        "summary": "In this work, we introduce mxbai-edge-colbert-v0 models, at two different parameter counts: 17M and 32M. As part of our research, we conduct numerous experiments to improve retrieval and late-interaction models, which we intend to distill into smaller models as proof-of-concepts. Our ultimate aim is to support retrieval at all scales, from large-scale retrieval which lives in the cloud to models that can run locally, on any device. mxbai-edge-colbert-v0 is a model that we hope will serve as a solid foundation backbone for all future experiments, representing the first version of a long series of small proof-of-concepts. As part of the development of mxbai-edge-colbert-v0, we conducted multiple ablation studies, of which we report the results. In terms of downstream performance, mxbai-edge-colbert-v0 is a particularly capable small model, outperforming ColBERTv2 on common short-text benchmarks (BEIR) and representing a large step forward in long-context tasks, with unprecedented efficiency.",
        "translated": "在本工作中，我们引入了 mxbai-edge-colbert-v0 模型，并提供了两种不同的参数规模：17M 和 32M。作为我们研究的一部分，我们进行了大量实验以改进检索和后期交互模型，并计划将其蒸馏为更小的模型作为概念验证。我们的最终目标是支持所有规模的检索，从云端的大规模检索到能够在任何设备上本地运行的小模型。mxbai-edge-colbert-v0 是我们希望作为所有未来实验坚实基础的骨干模型，代表了一系列小型概念验证模型的第一个版本。在 mxbai-edge-colbert-v0 的开发过程中，我们进行了多项消融实验，并在此报告其结果。在下游任务性能方面，mxbai-edge-colbert-v0 是一个特别强大的小型模型，其在常见的短文本基准（BEIR）上优于 ColBERTv2，并在长上下文任务中实现了前所未有的效率，迈出了重要一步。",
        "translated_title": "Fantastic (small) Retrievers and How to Train Them:  \nmxbai-edge-colbert-v0 技术报告",
        "label": [
            "召回"
        ],
        "label_reason": "论文聚焦检索模型优化，适用于推荐系统的召回环节。",
        "relevance_score": 6,
        "novelty_score": 7,
        "novelty_reason": "提出高效小型模型，改进了检索与长上下文处理能力。"
    },
    {
        "title": "A Simulation Framework for Studying Systemic Effects of Feedback Loops\n  in Recommender Systems",
        "url": "http://arxiv.org/abs/2510.14857v1",
        "pub_date": "2025-10-16",
        "summary": "Recommender systems continuously interact with users, creating feedback loops that shape both individual behavior and collective market dynamics. This paper introduces a simulation framework to model these loops in online retail environments, where recommenders are periodically retrained on evolving user-item interactions. Using the Amazon e-Commerce dataset, we analyze how different recommendation algorithms influence diversity, purchase concentration, and user homogenization over time. Results reveal a systematic trade-off: while the feedback loop increases individual diversity, it simultaneously reduces collective diversity and concentrates demand on a few popular items. Moreover, for some recommender systems, the feedback loop increases user homogenization over time, making user purchase profiles increasingly similar. These findings underscore the need for recommender designs that balance personalization with long-term diversity.",
        "translated": "推荐系统持续与用户进行交互，形成反馈循环，从而塑造个体行为和集体市场动态。本文引入了一个仿真框架，用于模拟在线零售环境中的这些循环，其中推荐系统会定期在不断演变的用户-物料交互数据上进行重新训练。使用 Amazon 电商平台数据集，我们分析了不同的推荐算法如何随着时间的推移影响多样性、购买集中度和用户同质化。结果揭示了一个系统性的权衡：虽然反馈循环提高了个体的多样性，但它同时降低了集体的多样性，并将需求集中在少数热门物料上。此外，对于某些推荐系统，反馈循环会随着时间推移增加用户的同质化，使得用户的购买行为变得越来越相似。这些发现凸显了推荐系统设计中需要在个性化与长期多样性之间取得平衡的必要性。",
        "translated_title": "一个用于研究推荐系统中反馈循环系统性效应的模拟框架",
        "label": [
            "推荐系统公平性/可解释性",
            "通用推荐技术"
        ],
        "label_reason": "研究推荐系统的反馈循环及长期多样性问题，属于推荐系统核心机制分析",
        "relevance_score": 8,
        "novelty_score": 7,
        "novelty_reason": "提出新颖的模拟框架，揭示个性化与多样性间的系统性权衡"
    },
    {
        "title": "Supervised Fine-Tuning or Contrastive Learning? Towards Better\n  Multimodal LLM Reranking",
        "url": "http://arxiv.org/abs/2510.14824v1",
        "pub_date": "2025-10-16",
        "summary": "In information retrieval, training reranking models mainly focuses on two types of objectives: metric learning (e.g. contrastive loss to increase the predicted scores on relevant query-document pairs) and classification (binary label prediction of relevance vs. irrelevance). For BERT-style encoders, various studies have shown that contrastive learning (CL) can be more effective than discriminative (classification) learning. However, for large language models (LLMs), classification via supervised fine-tuning (SFT), which predicts ''yes'' (resp. ''no'') token for relevant (resp. irrelevant) pairs, appears more promising as it aligns well with the generative nature of LLMs. This divergence raises a central question: which objective is intrinsically better suited to LLM-based reranking, and what mechanism underlies the difference? In this work, we conduct a comprehensive comparison and analysis between CL and SFT for reranking, taking the universal multimodal retrieval (UMR) as the experimental playground. We first decompose the objectives into two components: weight, which controls the magnitude of those updates, and direction, which guides the model updates, then present a unified framework for understanding their interactions. Through probing experiments, we find that SFT provides a substantially stronger weighting scheme than CL, whereas the preferred scoring direction shows no clear winner. Taken together, these results point to a consistent advantage of SFT over CL for LLM reranking. To further validate our findings, we conduct large-scale training with SFT and present new state-of-the-art rerankers on the MRB benchmark. We also provide ablations on SFT settings and expect our findings to benefit future research and applications in this area.",
        "translated": "在信息检索中，训练重排模型主要关注两种目标：度量学习（例如对比损失，用于提升相关查询-文档对的预测得分）和分类（相关与不相关的二分类标签预测）。对于 BERT 风格的编码器，已有大量研究表明，对比学习（CL）在性能上通常优于判别式（分类）学习。然而，对于大语言模型（LLMs），通过监督微调（SFT）进行分类——即对相关（resp. 不相关）对预测“yes”（resp. “no”）标记——似乎更具前景，因为它与 LLM 生成式的本质更加契合。这一差异引发了一个核心问题：哪一种目标本质上更适合基于 LLM 的重排？其背后的不同机制是什么？在本研究中，我们对 CL 和 SFT 在重排中的表现进行了全面的比较与分析，实验场景设定为通用多模态检索（UMR）。我们首先将这两种目标分解为两个组成部分：权重，用于控制更新的幅度；以及方向，用于指导模型的更新，然后提出一个统一的框架来理解它们之间的相互作用。通过探针实验，我们发现 SFT 提供了显著更强的权重机制，而关于最优的得分方向则没有明显优势。综合来看，这些结果表明在基于 LLM 的重排任务中，SFT 相较于 CL 具有一致的优势。为了进一步验证我们的发现，我们使用 SFT 进行了大规模训练，并在 MRB 基准上提出了新的最先进的重排模型。我们还对 SFT 的设置进行了消融实验，并希望我们的研究结果能够为该领域的未来研究与应用提供帮助。",
        "translated_title": "监督微调还是对比学习？迈向更优的多模态大语言模型重排",
        "label": [
            "重排（Re-ranking）",
            "负采样与对比学习（Negative Sampling / Contrastive Learning）",
            "LLM生成式推荐（LLM-based Generative Recommendation）"
        ],
        "label_reason": "论文聚焦LLM在重排中的应用，并对比CL和SFT的效果，与推荐系统密切相关。",
        "relevance_score": 8,
        "novelty_score": 7,
        "novelty_reason": "提出SFT更适合LLM重排，并提供统一框架分析两者差异，具有创新性。"
    },
    {
        "title": "Cross-Scenario Unified Modeling of User Interests at Billion Scale",
        "url": "http://arxiv.org/abs/2510.14788v1",
        "pub_date": "2025-10-16",
        "summary": "User interests on content platforms are inherently diverse, manifesting through complex behavioral patterns across heterogeneous scenarios such as search, feed browsing, and content discovery. Traditional recommendation systems typically prioritize business metric optimization within isolated specific scenarios, neglecting cross-scenario behavioral signals and struggling to integrate advanced techniques like LLMs at billion-scale deployments, which finally limits their ability to capture holistic user interests across platform touchpoints. We propose RED-Rec, an LLM-enhanced hierarchical Recommender Engine for Diversified scenarios, tailored for industry-level content recommendation systems. RED-Rec unifies user interest representations across multiple behavioral contexts by aggregating and synthesizing actions from varied scenarios, resulting in comprehensive item and user modeling. At its core, a two-tower LLM-powered framework enables nuanced, multifaceted representations with deployment efficiency, and a scenario-aware dense mixing and querying policy effectively fuses diverse behavioral signals to capture cross-scenario user intent patterns and express fine-grained, context-specific intents during serving. We validate RED-Rec through online A/B testing on hundreds of millions of users in RedNote through online A/B testing, showing substantial performance gains in both content recommendation and advertisement targeting tasks. We further introduce a million-scale sequential recommendation dataset, RED-MMU, for comprehensive offline training and evaluation. Our work advances unified user modeling, unlocking deeper personalization and fostering more meaningful user engagement in large-scale UGC platforms.",
        "translated": "内容平台上的用户兴趣本质上是多样化的，这种多样性在搜索、信息流浏览和内容发现等异构场景中通过复杂的用户行为模式体现出来。传统的推荐系统通常优先在孤立的具体场景中优化业务指标，忽略了跨场景的行为信号，且在数十亿级别的实际部署中难以有效整合大语言模型（LLM）等先进技术，最终限制了它们在平台各个触点中捕捉用户整体兴趣的能力。我们提出了 RED-Rec，一个面向工业级内容推荐系统的、用于多样化场景的、融合大语言模型的分层推荐引擎。RED-Rec 通过聚合和综合来自不同场景的行为，统一了用户兴趣在多种行为上下文中的表示，从而实现了全面的物料与用户建模。其核心是一个双塔结构的大语言模型驱动框架，能够以高效的部署方式生成细致、多维的表示；同时，一个具备场景感知能力的密集混合与查询策略，有效融合了多样的行为信号，以捕捉跨场景的用户意图模式，并在服务过程中表达出细粒度、上下文特定的意图。我们在 RedNote 上通过数亿用户的在线 A/B 实验验证了 RED-Rec，结果在内容推荐和广告定向任务中均表现出显著的性能提升。此外，我们进一步引入了一个百万规模的序列推荐数据集 RED-MMU，用于全面的离线训练与评估。我们的工作推动了统一用户建模的发展，在大规模用户生成内容（UGC）平台中实现了更深层次的个性化，并促进了更具意义的用户参与。",
        "translated_title": "百亿级跨场景用户兴趣统一建模",
        "label": [
            "通用推荐技术",
            "LLM生成式推荐",
            "跨域/联邦推荐",
            "序列推荐"
        ],
        "label_reason": "跨场景统一用户兴趣建模，适用于大规模内容推荐系统",
        "relevance_score": 9,
        "novelty_score": 8,
        "novelty_reason": "提出融合LLM的分层架构RED-Rec，改进跨场景行为建模"
    },
    {
        "title": "Dataset Pruning in RecSys and ML: Best Practice or Mal-Practice?",
        "url": "http://arxiv.org/abs/2510.14704v1",
        "pub_date": "2025-10-16",
        "summary": "Offline evaluations in recommender system research depend heavily on datasets, many of which are pruned, such as the widely used MovieLens collections. This thesis examines the impact of data pruning - specifically, removing users with fewer than a specified number of interactions - on both dataset characteristics and algorithm performance. Five benchmark datasets were analysed in both their unpruned form and at five successive pruning levels (5, 10, 20, 50, 100). For each coreset, we examined structural and distributional characteristics and trained and tested eleven representative algorithms. To further assess if pruned datasets lead to artificially inflated performance results, we also evaluated models trained on the pruned train sets but tested on unpruned data. Results show that commonly applied core pruning can be highly selective, leaving as little as 2% of the original users in some datasets. Traditional algorithms achieved higher nDCG@10 scores when both training and testing on pruned data; however, this advantage largely disappeared when evaluated on unpruned test sets. Across all algorithms, performance declined with increasing pruning levels when tested on unpruned data, highlighting the impact of dataset reduction on the performance of recommender algorithms.",
        "translated": "推荐系统的离线评估高度依赖于数据集，其中许多数据集经过了修剪，例如广泛使用的 MovieLens 数据集。本文研究了数据修剪——具体来说，移除交互次数少于指定阈值的用户——对数据集特性和算法性能的影响。我们分析了五个基准数据集在未修剪形式和五种连续修剪等级（5, 10, 20, 50, 100）下的表现。对于每个核心子集（coreset），我们考察了其结构和分布特征，并训练和测试了十一种代表性算法。为了进一步评估修剪后的数据集是否会导致性能结果的人为高估，我们还评估了在修剪后的训练集上训练但在未修剪数据上测试的模型。结果显示，常见的核心修剪方法具有高度的选择性，在某些数据集中仅保留原始用户数的2%。传统算法在训练和测试均使用修剪数据时取得了更高的 nDCG@10 分数；然而，当在未修剪的测试集上进行评估时，这种优势基本消失。在所有算法中，当测试数据为未修剪数据时，其性能随着修剪等级的提高而下降，突显了数据集减少对推荐算法性能的影响。",
        "translated_title": "推荐系统与机器学习中的数据集裁剪：最佳实践还是不当实践？",
        "label": [
            "推荐系统评估"
        ],
        "label_reason": "论文聚焦推荐系统数据集剪枝对算法性能的影响，属于推荐系统的评估问题。",
        "relevance_score": 8,
        "novelty_score": 6,
        "novelty_reason": "对数据集剪枝现象进行系统分析，但方法较为常规，创新性有限。"
    },
    {
        "title": "TITAN: Graph-Executable Reasoning for Cyber Threat Intelligence",
        "url": "http://arxiv.org/abs/2510.14670v1",
        "pub_date": "2025-10-16",
        "summary": "TITAN (Threat Intelligence Through Automated Navigation) is a framework that connects natural-language cyber threat queries with executable reasoning over a structured knowledge graph. It integrates a path planner model, which predicts logical relation chains from text, and a graph executor that traverses the TITAN Ontology to retrieve factual answers and supporting evidence. Unlike traditional retrieval systems, TITAN operates on a typed, bidirectional graph derived from MITRE, allowing reasoning to move clearly and reversibly between threats, behaviors, and defenses. To support training and evaluation, we introduce the TITAN Dataset, a corpus of 88209 examples (Train: 74258; Test: 13951) pairing natural language questions with executable reasoning paths and step by step Chain of Thought explanations. Empirical evaluations show that TITAN enables models to generate syntactically valid and semantically coherent reasoning paths that can be deterministically executed on the underlying graph.",
        "translated": "TITAN（Threat Intelligence Through Automated Navigation）是一个将自然语言的网络威胁查询与结构化知识图谱上的可执行推理相连接的框架。它集成了一个路径规划模型，该模型从文本中预测逻辑关系链，并结合一个图谱执行器，该执行器遍历TITAN本体（Ontology）以检索事实性答案和支持证据。与传统召回系统不同，TITAN基于来自MITRE的类型化、双向图谱进行操作，使得在威胁、行为和防御之间可以清晰且可逆地进行推理。为支持训练和评估，我们引入了TITAN数据集，该数据集包含88209个示例（训练集：74258；测试集：13951），将自然语言问题与可执行推理路径以及逐步的Chain of Thought解释配对。实证评估表明，TITAN使模型能够生成在语法上有效、语义上连贯的推理路径，并可以在底层图谱上确定性地执行。",
        "translated_title": "TITAN：用于网络威胁情报的图可执行推理",
        "label": [],
        "label_reason": "论文聚焦网络安全威胁情报，不直接涉及推荐系统。",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出基于知识图谱的可执行推理框架，具有新颖性但非推荐系统创新。"
    },
    {
        "title": "An Efficient Rubric-based Generative Verifier for Search-Augmented LLMs",
        "url": "http://arxiv.org/abs/2510.14660v1",
        "pub_date": "2025-10-16",
        "summary": "Search augmentation empowers Large Language Models with retrieval capabilities to overcome the limitations imposed by static parameters. Recently, Reinforcement Learning leverages tailored reward signals as a viable technique to enhance LLMs performing tasks involving search. However, existing reward modeling for search-augmented LLMs faces several limitations. Rule-based rewards, such as Exact Match, are verifiable but fragile to variations in expression and cannot be applied to long-form workloads. In contrast, generative rewards improve robustness, but designing verifiable and stable rewards for long-form workloads in dynamic corpora remains challenging and also incurs high computational costs. In this paper, we propose a unified and verifiable paradigm, \"nugget-as-rubric\", which treats atomic information points as structured evaluation criteria for different search-augmentation workloads. Short-form tasks correspond to a single rubric, whereas long-form tasks expand to multiple rubrics aligned with the question's information needs. To support long-form settings, we design an automatic rubric construction pipeline based on query rewriting, which can automatically retrieve passages relevant to each question and extract rubrics from them, both from static corpora and from dynamic online web content. Furthermore, we introduce \\textbf{Search-Gen-V}, a 4B-parameter efficient generative verifier under our proposed verifiable paradigm, which is trained via the idea of distillation and a two-stage strategy. Experimental results show that Search-Gen-V achieves strong verification accuracy across different workloads, making it a scalable, robust, and efficient verifiable reward constructor for search-augmented LLMs.",
        "translated": "搜索增强为大语言模型赋予了召回能力，以克服静态参数所带来的限制。近年来，强化学习利用定制化的奖励信号，作为增强大语言模型在涉及搜索任务性能的一种可行技术。然而，现有的用于搜索增强的大语言模型的奖励建模面临多个限制。基于规则的奖励，例如精确匹配（Exact Match），虽然可验证，但对表达方式的变化非常脆弱，无法应用于长文本任务。相比之下，生成式奖励增强了鲁棒性，但在动态语料库中为长文本任务设计可验证且稳定的奖励仍然具有挑战性，并且计算成本较高。在本文中，我们提出了一种统一且可验证的范式——“nugget-as-rubric”，该范式将原子信息点视为结构化的评估标准，适用于不同的搜索增强任务。短文本任务对应单一评估标准，而长文本任务则扩展为多个与问题信息需求对齐的评估标准。为了支持长文本场景，我们设计了一种基于查询重写（query rewriting）的自动评估标准构建流程，该流程能够自动召回与每个问题相关的段落，并从中提取评估标准，无论是静态语料库还是动态的在线网页内容。此外，我们引入了 **Search-Gen-V**，一个在我们提出的可验证范式下高效的大语言生成验证模型。该模型通过蒸馏思想和两阶段训练策略进行训练。实验结果表明，Search-Gen-V 在不同任务中均实现了较强的验证精度，使其成为适用于搜索增强的大语言模型的一种可扩展、鲁棒且高效的可验证奖励构造器。",
        "translated_title": "一种高效的基于评分规则的生成式验证器用于检索增强的大语言模型",
        "label": [
            "LLM生成式推荐"
        ],
        "label_reason": "论文涉及生成式LLM与搜索增强，适用于生成式推荐场景",
        "relevance_score": 4,
        "novelty_score": 7,
        "novelty_reason": "提出新颖的验证范式和高效的生成式验证器，改进现有奖励建模"
    },
    {
        "title": "Causality Enhancement for Cross-Domain Recommendation",
        "url": "http://arxiv.org/abs/2510.14641v1",
        "pub_date": "2025-10-16",
        "summary": "Cross-domain recommendation forms a crucial component in recommendation systems. It leverages auxiliary information through source domain tasks or features to enhance target domain recommendations. However, incorporating inconsistent source domain tasks may result in insufficient cross-domain modeling or negative transfer. While incorporating source domain features without considering the underlying causal relationships may limit their contribution to final predictions. Thus, a natural idea is to directly train a cross-domain representation on a causality-labeled dataset from the source to target domain. Yet this direction has been rarely explored, as identifying unbiased real causal labels is highly challenging in real-world scenarios. In this work, we attempt to take a first step in this direction by proposing a causality-enhanced framework, named CE-CDR. Specifically, we first reformulate the cross-domain recommendation as a causal graph for principled guidance. We then construct a causality-aware dataset heuristically. Subsequently, we derive a theoretically unbiased Partial Label Causal Loss to generalize beyond the biased causality-aware dataset to unseen cross-domain patterns, yielding an enriched cross-domain representation, which is then fed into the target model to enhance target-domain recommendations. Theoretical and empirical analyses, as well as extensive experiments, demonstrate the rationality and effectiveness of CE-CDR and its general applicability as a model-agnostic plugin. Moreover, it has been deployed in production since April 2025, showing its practical value in real-world applications.",
        "translated": "跨域推荐是推荐系统中的一个关键组成部分。它通过源域任务或特征来利用辅助信息，以增强目标域的推荐效果。然而，引入不一致的源域任务可能导致跨域建模不充分或出现负迁移。而在不考虑底层因果关系的情况下融合源域特征，可能会限制其对最终预测的贡献。因此，一个自然的想法是直接在从源域到目标域的因果标注数据集上训练跨域表示。然而，这一方向鲜有探索，因为在现实场景中识别无偏的真实因果标签极具挑战性。在本工作中，我们尝试通过提出一种因果增强框架CE-CDR，在这一方向上迈出第一步。具体来说，我们首先将跨域推荐重新表述为因果图，以提供原理性的指导。然后，我们启发式地构建了一个因果感知数据集。随后，我们推导出一个理论上无偏的部标签因果损失函数，以在存在偏倚的因果感知数据集基础上，泛化至未见过的跨域模式，从而获得更丰富的跨域表示，并将其输入目标模型中以提升目标域的推荐性能。理论与实证分析以及广泛的实验验证了CE-CDR的合理性与有效性，并展示了其作为模型无关插件的通用适用性。此外，该方法自2025年4月起已在生产环境中部署，表明其在实际应用中的实用价值。",
        "translated_title": "跨域推荐中的因果增强",
        "label": [
            "跨域/联邦推荐",
            "因果推理"
        ],
        "label_reason": "论文研究跨域推荐，属于推荐系统核心问题，涉及因果关系建模。",
        "relevance_score": 8,
        "novelty_score": 7,
        "novelty_reason": "提出因果增强框架CE-CDR，改进跨域推荐模型的泛化能力。"
    },
    {
        "title": "Intent Clustering with Shared Pseudo-Labels",
        "url": "http://arxiv.org/abs/2510.14640v1",
        "pub_date": "2025-10-16",
        "summary": "In this paper, we propose an intuitive, training-free and label-free method for intent clustering that makes minimal assumptions using lightweight and open-source LLMs. Many current approaches rely on commercial LLMs, which are costly, and offer limited transparency. Additionally, their methods often explicitly depend on knowing the number of clusters in advance, which is often not the case in realistic settings. To address these challenges, instead of asking the LLM to match similar text directly, we first ask it to generate pseudo-labels for each text, and then perform multi-label classification in this pseudo-label set for each text. This approach is based on the hypothesis that texts belonging to the same cluster will share more labels, and will therefore be closer when encoded into embeddings. These pseudo-labels are more human-readable than direct similarity matches. Our evaluation on four benchmark sets shows that our approach achieves results comparable to and better than recent baselines, while remaining simple and computationally efficient. Our findings indicate that our method can be applied in low-resource scenarios and is stable across multiple models and datasets.",
        "translated": "在本文中，我们提出了一种直观的、无需训练且无需标注的方法用于意图聚类，该方法在使用轻量级和开源大语言模型的前提下，做出最小的假设。许多现有方法依赖于商业大语言模型，这不仅成本高昂，而且透明度有限。此外，这些方法通常明确要求提前知道聚类的数量，而这种情况在现实场景中往往并不存在。为了解决这些挑战，我们没有直接要求大语言模型匹配相似文本，而是首先让其为每段文本生成伪标签，然后在该伪标签集合中对每段文本进行多标签分类。该方法基于这样的假设：属于同一聚类的文本将共享更多的标签，因此在嵌入编码后彼此的距离将更近。与直接相似性匹配相比，这些伪标签更具可读性。我们在四个基准数据集上的评估表明，我们的方法在保持简单和计算高效的同时，取得了与近期基线相当甚至更好的结果。我们的实验结果表明，该方法适用于资源有限的场景，并在多个模型和数据集之间具有稳定性。",
        "translated_title": "Intent Clustering with Shared Pseudo-Labels  \n共享伪标签的意图聚类",
        "label": [
            "LLM生成式推荐"
        ],
        "label_reason": "伪标签生成用于意图聚类，可能间接用于推荐系统的意图建模",
        "relevance_score": 5,
        "novelty_score": 7,
        "novelty_reason": "提出无需训练的伪标签聚类方法，提升计算效率和可解释性"
    },
    {
        "title": "MR.Rec: Synergizing Memory and Reasoning for Personalized Recommendation\n  Assistant with LLMs",
        "url": "http://arxiv.org/abs/2510.14629v1",
        "pub_date": "2025-10-16",
        "summary": "The application of Large Language Models (LLMs) in recommender systems faces key challenges in delivering deep personalization and intelligent reasoning, especially for interactive scenarios. Current methods are often constrained by limited context windows and single-turn reasoning, hindering their ability to capture dynamic user preferences and proactively reason over recommendation contexts. To address these limitations, we propose MR.Rec, a novel framework that synergizes memory and reasoning for LLM-based recommendations. To achieve personalization, we develop a comprehensive Retrieval-Augmented Generation (RAG) system that efficiently indexes and retrieves relevant external memory to enhance LLM personalization capabilities. Furthermore, to enable the synergy between memory and reasoning, our RAG system goes beyond conventional query-based retrieval by integrating reasoning enhanced memory retrieval. Finally, we design a reinforcement learning framework that trains the LLM to autonomously learn effective strategies for both memory utilization and reasoning refinement. By combining dynamic memory retrieval with adaptive reasoning, this approach ensures more accurate, context-aware, and highly personalized recommendations. Extensive experiments demonstrate that MR.Rec significantly outperforms state-of-the-art baselines across multiple metrics, validating its efficacy in delivering intelligent and personalized recommendations. We will release code and data upon paper notification.",
        "translated": "大语言模型（LLM）在推荐系统中的应用面临在实现深度个性化与智能推理方面的主要挑战，尤其是在交互式场景中。当前的方法通常受限于有限的上下文窗口和单轮推理，阻碍了其捕捉动态用户偏好并主动在推荐上下文中进行推理的能力。为了解决这些限制，我们提出MR.Rec，一种新颖的框架，将记忆与推理相结合，用于基于LLM的推荐。为了实现个性化，我们开发了一个全面的检索增强生成（RAG）系统，高效地索引和召回相关的外部记忆，以增强LLM的个性化能力。此外，为了实现记忆与推理之间的协同作用，我们的RAG系统超越了传统的基于查询的召回，通过集成推理增强的记忆召回机制。最后，我们设计了一个强化学习框架，训练LLM自主学习在记忆利用和推理优化方面的有效策略。通过结合动态记忆召回与自适应推理，该方法能够提供更加准确、上下文感知和高度个性化的推荐。广泛的实验表明，MR.Rec在多个指标上显著优于最先进的基线方法，验证了其在实现智能和个性化推荐方面的有效性。我们将随论文通知发布代码和数据。",
        "translated_title": "MR.Rec: 结合记忆与推理的个性化推荐方法  \n与大语言模型结合",
        "label": [
            "LLM生成式推荐",
            "序列推荐",
            "推荐系统公平性/可解释性"
        ],
        "label_reason": "论文提出结合记忆与推理的LLM推荐框架，显著提升个性化与智能推荐能力。",
        "relevance_score": 9,
        "novelty_score": 8,
        "novelty_reason": "创新性地结合RAG与强化学习，实现动态记忆检索与自适应推理策略。"
    },
    {
        "title": "GemiRec: Interest Quantization and Generation for Multi-Interest\n  Recommendation",
        "url": "http://arxiv.org/abs/2510.14626v1",
        "pub_date": "2025-10-16",
        "summary": "Multi-interest recommendation has gained attention, especially in industrial retrieval stage. Unlike classical dual-tower methods, it generates multiple user representations instead of a single one to model comprehensive user interests. However, prior studies have identified two underlying limitations: the first is interest collapse, where multiple representations homogenize. The second is insufficient modeling of interest evolution, as they struggle to capture latent interests absent from a user's historical behavior. We begin with a thorough review of existing works in tackling these limitations. Then, we attempt to tackle these limitations from a new perspective. Specifically, we propose a framework-level refinement for multi-interest recommendation, named GemiRec. The proposed framework leverages interest quantization to enforce a structural interest separation and interest generation to learn the evolving dynamics of user interests explicitly. It comprises three modules: (a) Interest Dictionary Maintenance Module (IDMM) maintains a shared quantized interest dictionary. (b) Multi-Interest Posterior Distribution Module (MIPDM) employs a generative model to capture the distribution of user future interests. (c) Multi-Interest Retrieval Module (MIRM) retrieves items using multiple user-interest representations. Both theoretical and empirical analyses, as well as extensive experiments, demonstrate its advantages and effectiveness. Moreover, it has been deployed in production since March 2025, showing its practical value in industrial applications.",
        "translated": "多兴趣推荐在工业召回阶段中引起了广泛关注。与经典的双塔方法不同，它生成多个用户表示，而不是单一的表示，以建模用户全面的兴趣。然而，已有研究表明该方法存在两个潜在的限制：第一个是兴趣坍缩，其中多个表示趋于同质化；第二个是对兴趣演化的建模不足，因为它们难以捕捉用户历史行为中未体现的潜在兴趣。我们首先对现有工作中解决这些限制的方法进行了全面的回顾。随后，我们尝试从一个新的视角来应对这些限制。具体而言，我们提出了一个面向多兴趣推荐的框架级改进，称为GemiRec。所提框架利用兴趣量化来实现结构化的兴趣分离，并通过兴趣生成显式地学习用户兴趣的演化动态。它包含三个模块：(a) 兴趣字典维护模块（IDMM）维护一个共享的量化兴趣字典；(b) 多兴趣后验分布模块（MIPDM）采用生成模型来捕捉用户未来兴趣的分布；(c) 多兴趣召回模块（MIRM）使用多个用户-兴趣表示进行物料召回。理论与实证分析以及广泛的实验验证了其优势和有效性。此外，该方法自2025年3月起已在生产环境中部署，展示了其在工业应用中的实用价值。",
        "translated_title": "GemiRec：多兴趣推荐中的兴趣量化与生成",
        "label": [
            "召回（Recall）",
            "多兴趣推荐（Multimodal Recommendation）",
            "生成式推荐（LLM-based Generative Recommendation）"
        ],
        "label_reason": "论文提出多兴趣推荐框架GemiRec，聚焦于召回阶段的兴趣建模与生成，与推荐系统核心问题紧密相关。",
        "relevance_score": 9,
        "novelty_score": 8,
        "novelty_reason": "从兴趣量化与生成角度提出新颖框架，改进多兴趣建模方式，具有理论与应用创新。"
    },
    {
        "title": "Multimodal RAG for Unstructured Data:Leveraging Modality-Aware Knowledge\n  Graphs with Hybrid Retrieval",
        "url": "http://arxiv.org/abs/2510.14592v1",
        "pub_date": "2025-10-16",
        "summary": "Current Retrieval-Augmented Generation (RAG) systems primarily operate on unimodal textual data, limiting their effectiveness on unstructured multimodal documents. Such documents often combine text, images, tables, equations, and graphs, each contributing unique information. In this work, we present a Modality-Aware Hybrid retrieval Architecture (MAHA), designed specifically for multimodal question answering with reasoning through a modality-aware knowledge graph. MAHA integrates dense vector retrieval with structured graph traversal, where the knowledge graph encodes cross-modal semantics and relationships. This design enables both semantically rich and context-aware retrieval across diverse modalities. Evaluations on multiple benchmark datasets demonstrate that MAHA substantially outperforms baseline methods, achieving a ROUGE-L score of 0.486, providing complete modality coverage. These results highlight MAHA's ability to combine embeddings with explicit document structure, enabling effective multimodal retrieval. Our work establishes a scalable and interpretable retrieval framework that advances RAG systems by enabling modality-aware reasoning over unstructured multimodal data.",
        "translated": "当前的检索增强生成（RAG）系统主要基于单模态文本数据进行操作，这限制了它们在非结构化多模态文档上的有效性。此类文档通常结合了文本、图像、表格、公式和图表，每种模态都提供了独特的信息。在本文中，我们提出了一种模态感知混合检索架构（MAHA），该架构专为通过模态感知知识图谱进行多模态问答与推理而设计。MAHA 将稠密向量召回与结构化的图遍历相结合，其中知识图谱编码了跨模态的语义信息与关系。这种设计实现了在多种模态下语义丰富且上下文感知的检索。在多个基准数据集上的评估表明，MAHA 显著优于基线方法，达到 ROUGE-L 评分为 0.486，并实现了对所有模态的完整覆盖。这些结果突显了 MAHA 将嵌入表示与显式的文档结构相结合的能力，从而实现高效的多模态召回。我们的工作建立了一个可扩展且可解释的检索框架，通过在非结构化多模态数据上实现模态感知推理，推动了 RAG 系统的发展。",
        "translated_title": "多模态RAG用于非结构化数据：利用模态感知的知识图谱与混合召回",
        "label": [
            "多模态推荐",
            "LLM生成式推荐",
            "召回"
        ],
        "label_reason": "论文提出多模态RAG框架，适用于包含多种模态信息的推荐场景",
        "relevance_score": 7,
        "novelty_score": 8,
        "novelty_reason": "引入模态感知的知识图谱与混合检索机制，具有一定的创新性"
    },
    {
        "title": "Agentic Entropy-Balanced Policy Optimization",
        "url": "http://arxiv.org/abs/2510.14545v1",
        "pub_date": "2025-10-16",
        "summary": "Recently, Agentic Reinforcement Learning (Agentic RL) has made significant progress in incentivizing the multi-turn, long-horizon tool-use capabilities of web agents. While mainstream agentic RL algorithms autonomously explore high-uncertainty tool-call steps under the guidance of entropy, excessive reliance on entropy signals can impose further constraints, leading to the training collapse. In this paper, we delve into the challenges caused by entropy and propose the Agentic Entropy-Balanced Policy Optimization (AEPO), an agentic RL algorithm designed to balance entropy in both the rollout and policy update phases. AEPO comprises two core components: (1) a dynamic entropy-balanced rollout mechanism that adaptively allocate global and branch sampling budget through entropy pre-monitoring, while imposing a branch penalty on consecutive high-entropy tool-call steps to prevent over-branching issues; and (2) Entropy-Balanced Policy Optimization that inserts a stop-gradient operation into the high-entropy clipping term to preserve and properly rescale gradients on high-entropy tokens, while incorporating entropy-aware advantage estimation to prioritize learning on high-uncertainty tokens. Results across 14 challenging datasets show that AEPO consistently outperforms 7 mainstream RL algorithms. With just 1K RL samples, Qwen3-14B with AEPO achieves impressive results: 47.6% on GAIA, 11.2% on Humanity's Last Exam, and 43.0% on WebWalker for Pass@1; 65.0% on GAIA, 26.0% on Humanity's Last Exam, and 70.0% on WebWalker for Pass@5. Further analysis reveals that AEPO improves rollout sampling diversity while maintaining stable policy entropy, facilitating scalable web agent training.",
        "translated": "近期，Agentic 强化学习（Agentic RL）在激励网络智能体的多轮次、长时域工具使用能力方面取得了显著进展。尽管主流的 Agentic RL 算法在熵的指导下自主探索高不确定性工具调用步骤，但对熵信号的过度依赖可能带来进一步限制，导致训练崩溃。在本文中，我们深入探讨了由熵引起的挑战，并提出了 Agentic Entropy-Balanced Policy Optimization（AEPO），这是一种旨在在 rollout 和策略更新阶段平衡熵的 Agentic RL 算法。AEPO 包含两个核心组件：（1）一种动态熵平衡 rollout 机制，通过熵的预监控，自适应地分配全局与分支采样预算，同时对连续的高熵工具调用步骤施加分支惩罚，以防止过度分支问题；（2）熵平衡策略优化，将 stop-gradient 操作插入高熵裁剪项中，以保留并正确缩放高熵 token 的梯度，同时结合熵感知的优势估计，优先在高不确定性 token 上进行学习。在 14 个具有挑战性的数据集上的结果表明，AEPO 一致优于 7 种主流 RL 算法。仅使用 1K RL 样本，采用 AEPO 的 Qwen3-14B 在 Pass@1 指标上分别达到了 47.6%（GAIA）、11.2%（Humanity's Last Exam）和 43.0%（WebWalker）；在 Pass@5 指标上分别达到了 65.0%（GAIA）、26.0%（Humanity's Last Exam）和 70.0%（WebWalker）。进一步分析表明，AEPO 在保持策略熵稳定的同时提升了 rollout 采样的多样性，从而促进了网络智能体的可扩展训练。",
        "translated_title": "基于智能体的熵平衡策略优化",
        "label": [],
        "label_reason": "论文聚焦于强化学习中的策略优化，与推荐系统无直接关联。",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出了一种熵平衡策略优化方法，改进了探索机制，但属于通用强化学习范畴。"
    },
    {
        "title": "Acquisition of interpretable domain information during brain MR image\n  harmonization for content-based image retrieval",
        "url": "http://arxiv.org/abs/2510.14535v1",
        "pub_date": "2025-10-16",
        "summary": "Medical images like MR scans often show domain shifts across imaging sites due to scanner and protocol differences, which degrade machine learning performance in tasks such as disease classification. Domain harmonization is thus a critical research focus. Recent approaches encode brain images $\\boldsymbol{x}$ into a low-dimensional latent space $\\boldsymbol{z}$, then disentangle it into $\\boldsymbol{z_u}$ (domain-invariant) and $\\boldsymbol{z_d}$ (domain-specific), achieving strong results. However, these methods often lack interpretability$-$an essential requirement in medical applications$-$leaving practical issues unresolved. We propose Pseudo-Linear-Style Encoder Adversarial Domain Adaptation (PL-SE-ADA), a general framework for domain harmonization and interpretable representation learning that preserves disease-relevant information in brain MR images. PL-SE-ADA includes two encoders $f_E$ and $f_{SE}$ to extract $\\boldsymbol{z_u}$ and $\\boldsymbol{z_d}$, a decoder to reconstruct the image $f_D$, and a domain predictor $g_D$. Beyond adversarial training between the encoder and domain predictor, the model learns to reconstruct the input image $\\boldsymbol{x}$ by summing reconstructions from $\\boldsymbol{z_u}$ and $\\boldsymbol{z_d}$, ensuring both harmonization and informativeness. Compared to prior methods, PL-SE-ADA achieves equal or better performance in image reconstruction, disease classification, and domain recognition. It also enables visualization of both domain-independent brain features and domain-specific components, offering high interpretability across the entire framework.",
        "translated": "磁共振扫描等医学图像常常由于扫描设备和成像协议的差异，在不同成像中心之间表现出领域偏移，这会降低机器学习在疾病分类等任务中的性能。因此，领域调和成为了一个关键的研究方向。近期的方法将脑图像 $\\boldsymbol{x}$ 编码到一个低维潜在空间 $\\boldsymbol{z}$ 中，然后将其解耦为 $\\boldsymbol{z_u}$（领域不变）和 $\\boldsymbol{z_d}$（领域特定），取得了良好的效果。然而，这些方法通常缺乏可解释性——这是医学应用中的基本要求——从而未能解决实际问题。我们提出了一种伪线性风格编码器对抗领域自适应（Pseudo-Linear-Style Encoder Adversarial Domain Adaptation, PL-SE-ADA）框架，该框架是一种通用的领域调和与可解释表征学习方法，能够在脑部磁共振图像中保留与疾病相关的信息。PL-SE-ADA 包括两个编码器 $f_E$ 和 $f_{SE}$，分别用于提取 $\\boldsymbol{z_u}$ 和 $\\boldsymbol{z_d}$，一个用于图像重建的解码器 $f_D$，以及一个领域预测器 $g_D$。除了编码器与领域预测器之间的对抗训练外，模型还通过将 $\\boldsymbol{z_u}$ 和 $\\boldsymbol{z_d}$ 的重建结果相加，学习重建输入图像 $\\boldsymbol{x}$，从而确保领域调和和信息保留。与先前方法相比，PL-SE-ADA 在图像重建、疾病分类和领域识别任务中均达到了同等或更好的性能。此外，它还支持对领域无关的脑特征和领域相关组件进行可视化，为整个框架提供了高度的可解释性。",
        "translated_title": "在基于内容的医学图像检索中进行脑部 MR 图像协调过程中可解释领域信息的获取",
        "label": [],
        "label_reason": "主要涉及医学图像处理，与推荐系统无直接关联。",
        "relevance_score": 2,
        "novelty_score": 5,
        "novelty_reason": "在图像域适应方面提出了一定改进，但属于常规技术范畴。"
    },
    {
        "title": "MedTrust-RAG: Evidence Verification and Trust Alignment for Biomedical\n  Question Answering",
        "url": "http://arxiv.org/abs/2510.14400v1",
        "pub_date": "2025-10-16",
        "summary": "Biomedical question answering (QA) requires accurate interpretation of complex medical knowledge. Large language models (LLMs) have shown promising capabilities in this domain, with retrieval-augmented generation (RAG) systems enhancing performance by incorporating external medical literature. However, RAG-based approaches in biomedical QA suffer from hallucinations due to post-retrieval noise and insufficient verification of retrieved evidence, undermining response reliability. We propose MedTrust-Guided Iterative RAG, a framework designed to enhance factual consistency and mitigate hallucinations in medical QA. Our method introduces three key innovations. First, it enforces citation-aware reasoning by requiring all generated content to be explicitly grounded in retrieved medical documents, with structured Negative Knowledge Assertions used when evidence is insufficient. Second, it employs an iterative retrieval-verification process, where a verification agent assesses evidence adequacy and refines queries through Medical Gap Analysis until reliable information is obtained. Third, it integrates the MedTrust-Align Module (MTAM) that combines verified positive examples with hallucination-aware negative samples, leveraging Direct Preference Optimization to reinforce citation-grounded reasoning while penalizing hallucination-prone response patterns. Experiments on MedMCQA, MedQA, and MMLU-Med demonstrate that our approach consistently outperforms competitive baselines across multiple model architectures, achieving the best average accuracy with gains of 2.7% for LLaMA3.1-8B-Instruct and 2.4% for Qwen3-8B.",
        "translated": "生物医学问答（QA）需要对复杂的医学知识进行准确的理解。大语言模型（LLMs）在此领域展现出良好的潜力，而基于召回增强生成（RAG）的系统通过引入外部医学文献进一步提升了性能。然而，在生物医学QA中，基于RAG的方法由于召回后的噪声以及对召回证据验证不足，容易产生幻觉，从而影响响应的可靠性。我们提出MedTrust-Guided Iterative RAG，一个旨在提升医学问答事实一致性和减少幻觉的框架。我们的方法引入了三个关键创新。首先，该方法通过要求所有生成内容必须明确基于召回的医学文档进行推理，从而强制引用感知的推理过程。在证据不足时，采用结构化的Negative Knowledge Assertions进行补充。其次，该方法采用迭代的召回-验证过程，其中验证代理通过医学差距分析（Medical Gap Analysis）评估证据充分性并细化查询，直到获取可靠的信息。第三，该方法集成了MedTrust-Align Module（MTAM），将已验证的正例与具有幻觉感知能力的负样本相结合，利用直接偏好优化（Direct Preference Optimization）来强化引用基础推理，并对易产生幻觉的响应模式施加惩罚。在MedMCQA、MedQA和MMLU-Med上的实验表明，我们的方法在多种模型架构下均能持续超越具有竞争力的基线模型，取得了最佳的平均准确率，其中在LLaMA3.1-8B-Instruct上提高了2.7%，在Qwen3-8B上提高了2.4%。",
        "translated_title": "MedTrust-RAG：生物医学问答中的证据验证与信任对齐",
        "label": [],
        "label_reason": "论文聚焦医学问答，不直接涉及推荐系统核心技术",
        "relevance_score": 3,
        "novelty_score": 7,
        "novelty_reason": "提出创新性的迭代检索-验证框架，减少幻觉"
    },
    {
        "title": "PluriHop: Exhaustive, Recall-Sensitive QA over Distractor-Rich Corpora",
        "url": "http://arxiv.org/abs/2510.14377v1",
        "pub_date": "2025-10-16",
        "summary": "Recent advances in large language models (LLMs) and retrieval-augmented generation (RAG) have enabled progress on question answering (QA) when relevant evidence is in one (single-hop) or multiple (multi-hop) passages. Yet many realistic questions about recurring report data - medical records, compliance filings, maintenance logs - require aggregation across all documents, with no clear stopping point for retrieval and high sensitivity to even one missed passage. We term these pluri-hop questions and formalize them by three criteria: recall sensitivity, exhaustiveness, and exactness. To study this setting, we introduce PluriHopWIND, a diagnostic multilingual dataset of 48 pluri-hop questions built from 191 real-world wind industry reports in German and English. We show that PluriHopWIND is 8-40% more repetitive than other common datasets and thus has higher density of distractor documents, better reflecting practical challenges of recurring report corpora. We test a traditional RAG pipeline as well as graph-based and multimodal variants, and find that none of the tested approaches exceed 40% in statement-wise F1 score. Motivated by this, we propose PluriHopRAG, a RAG architecture that follows a \"check all documents individually, filter cheaply\" approach: it (i) decomposes queries into document-level subquestions and (ii) uses a cross-encoder filter to discard irrelevant documents before costly LLM reasoning. We find that PluriHopRAG achieves relative F1 score improvements of 18-52% depending on base LLM. Despite its modest size, PluriHopWIND exposes the limitations of current QA systems on repetitive, distractor-rich corpora. PluriHopRAG's performance highlights the value of exhaustive retrieval and early filtering as a powerful alternative to top-k methods.",
        "translated": "近年来，大语言模型（LLM）和检索增强生成（RAG）的进展使得在相关证据存在于一个（单跳）或多个（多跳）段落中的问答（QA）任务取得了进步。然而，许多关于重复报告数据的现实问题——如医疗记录、合规文件、维护日志——需要对所有文档进行聚合，检索过程中没有明确的停止点，且对漏掉一个段落都高度敏感。我们将这些问题称为“pluri-hop”问题，并通过三个标准对其进行形式化：召回敏感性（recall sensitivity）、全面性（exhaustiveness）和准确性（exactness）。为了研究这一场景，我们引入了 PluriHopWIND，这是一个用于诊断的多语言数据集，包含48个 pluri-hop 问题，由191份真实世界中的风能行业报告（以德语和英语编写）构建而成。我们发现，PluriHopWIND 比其他常见数据集多出8%-40%的重复内容，因此具有更高的干扰文档密度，更能反映重复报告语料库中的实际挑战。我们测试了传统 RAG 流水线以及基于图和多模态的变体，发现所有测试方法的陈述级 F1 分数均未超过40%。受此启发，我们提出了 PluriHopRAG，一种遵循“逐个检查所有文档、廉价过滤”的 RAG 架构：它（i）将查询分解为文档级别的子问题，（ii）使用交叉编码器过滤器在昂贵的 LLM 推理之前丢弃不相关的文档。我们发现，PluriHopRAG 在不同基础 LLM 上实现了18%-52%的相对 F1 分数提升。尽管 PluriHopWIND 的规模相对较小，但它揭示了当前问答系统在重复且干扰文档丰富的语料库中的局限性。PluriHopRAG 的性能凸显了全面检索和早期过滤作为 top-k 方法强大替代方案的价值。",
        "translated_title": "PluriHop: 在干扰信息丰富的语料库上进行详尽的、召回敏感的问答",
        "label": [
            "召回（Recall）"
        ],
        "label_reason": "论文关注问答中的召回敏感性，提出改进的RAG架构以提升文档检索效率。",
        "relevance_score": 4,
        "novelty_score": 7,
        "novelty_reason": "提出PluriHopRAG架构，通过文档级子问题分解和交叉编码器过滤提升性能。"
    },
    {
        "title": "Ensembling Multiple Hallucination Detectors Trained on VLLM Internal\n  Representations",
        "url": "http://arxiv.org/abs/2510.14330v1",
        "pub_date": "2025-10-16",
        "summary": "This paper presents the 5th place solution by our team, y3h2, for the Meta CRAG-MM Challenge at KDD Cup 2025. The CRAG-MM benchmark is a visual question answering (VQA) dataset focused on factual questions about images, including egocentric images. The competition was contested based on VQA accuracy, as judged by an LLM-based automatic evaluator. Since incorrect answers result in negative scores, our strategy focused on reducing hallucinations from the internal representations of the VLM. Specifically, we trained logistic regression-based hallucination detection models using both the hidden_state and the outputs of specific attention heads. We then employed an ensemble of these models. As a result, while our method sacrificed some correct answers, it significantly reduced hallucinations and allowed us to place among the top entries on the final leaderboard. For implementation details and code, please refer to https://gitlab.aicrowd.com/htanabe/meta-comprehensive-rag-benchmark-starter-kit.",
        "translated": "本文介绍了我们团队 y3h2 在 KDD Cup 2025 的 Meta CRAG-MM 挑战赛中获得第五名的解决方案。CRAG-MM 基准是一个专注于图像事实性问题的视觉问答（VQA）数据集，包括第一人称视角的图像。比赛根据 VQA 准确率进行评判，评判方式由基于大语言模型（LLM）的自动评估器完成。由于错误答案会导致负分，我们的策略重点在于减少视觉语言模型（VLM）内部表示中的幻觉现象。具体来说，我们使用隐藏状态（hidden_state）以及特定注意力头（attention heads）的输出，训练了基于逻辑回归的幻觉检测模型。随后，我们采用了这些模型的集成。结果表明，虽然我们的方法牺牲了一些正确答案，但显著减少了幻觉，使我们在最终排行榜中进入了前列。关于实现细节和代码，请参考 https://gitlab.aicrowd.com/htanabe/meta-comprehensive-rag-benchmark-starter-kit。",
        "translated_title": "基于大语言模型内部表示的多种幻觉检测器集成",
        "label": [],
        "label_reason": "论文聚焦于VQA中的幻觉检测，与推荐系统无直接关联。",
        "relevance_score": 2,
        "novelty_score": 5,
        "novelty_reason": "提出基于VLM内部表示的幻觉检测方法，有一定实用价值但创新性有限。"
    },
    {
        "title": "Large Reasoning Embedding Models: Towards Next-Generation Dense\n  Retrieval Paradigm",
        "url": "http://arxiv.org/abs/2510.14321v1",
        "pub_date": "2025-10-16",
        "summary": "In modern e-commerce search systems, dense retrieval has become an indispensable component. By computing similarities between query and item (product) embeddings, it efficiently selects candidate products from large-scale repositories. With the breakthroughs in large language models (LLMs), mainstream embedding models have gradually shifted from BERT to LLMs for more accurate text modeling. However, these models still adopt direct-embedding methods, and the semantic accuracy of embeddings remains inadequate. Therefore, contrastive learning is heavily employed to achieve tight semantic alignment between positive pairs. Consequently, such models tend to capture statistical co-occurrence patterns in the training data, biasing them toward shallow lexical and semantic matches. For difficult queries exhibiting notable lexical disparity from target items, the performance degrades significantly. In this work, we propose the Large Reasoning Embedding Model (LREM), which novelly integrates reasoning processes into representation learning. For difficult queries, LREM first conducts reasoning to achieve a deep understanding of the original query, and then produces a reasoning-augmented query embedding for retrieval. This reasoning process effectively bridges the semantic gap between original queries and target items, significantly improving retrieval accuracy. Specifically, we adopt a two-stage training process: the first stage optimizes the LLM on carefully curated Query-CoT-Item triplets with SFT and InfoNCE losses to establish preliminary reasoning and embedding capabilities, and the second stage further refines the reasoning trajectories via reinforcement learning (RL). Extensive offline and online experiments validate the effectiveness of LREM, leading to its deployment on China's largest e-commerce platform since August 2025.",
        "translated": "在现代电子商务搜索系统中，稠密召回已成为不可或缺的组成部分。通过计算查询与物料（产品）嵌入之间的相似性，它能够高效地从大规模仓库中选取候选产品。随着大语言模型（LLM）的突破，主流嵌入模型已逐渐从 BERT 转向 LLM，以实现更精确的文本建模。然而，这些模型仍采用直接嵌入方法，其嵌入的语义准确性仍然不足。因此，对比学习被广泛用于实现正样本对之间的紧密语义对齐。结果是，这些模型倾向于捕捉训练数据中的统计共现模式，使其偏向于浅层的词汇和语义匹配。对于与目标物料存在显著词汇差异的复杂查询，其性能会明显下降。在本研究中，我们提出了大型推理嵌入模型（LREM），其创新性地将推理过程整合到表示学习中。对于复杂查询，LREM 首先进行推理以实现对原始查询的深入理解，然后生成一个经过推理增强的查询嵌入用于召回。这一推理过程有效地弥合了原始查询与目标物料之间的语义差距，显著提升了召回精度。具体来说，我们采用了一个两阶段的训练过程：第一阶段在精心构建的 Query-CoT-Item 三元组上优化 LLM，使用监督微调（SFT）和 InfoNCE 损失以建立初步的推理和嵌入能力；第二阶段则通过强化学习（RL）进一步优化推理轨迹。大量的离线和在线实验验证了 LREM 的有效性，自 2025 年 8 月起，该模型已部署在中国最大的电子商务平台上。",
        "translated_title": "大型推理嵌入模型：迈向下一代密集召回范式",
        "label": [
            "召回",
            "通用推荐技术",
            "负采样与对比学习"
        ],
        "label_reason": "论文聚焦于电商搜索中的密集召回优化，与推荐系统召回环节直接相关。",
        "relevance_score": 8,
        "novelty_score": 9,
        "novelty_reason": "提出将推理过程融入嵌入学习，通过强化学习优化推理轨迹，方法新颖且效果显著。"
    },
    {
        "title": "Rethinking Schema Linking: A Context-Aware Bidirectional Retrieval\n  Approach for Text-to-SQL",
        "url": "http://arxiv.org/abs/2510.14296v1",
        "pub_date": "2025-10-16",
        "summary": "Schema linking -- the process of aligning natural language questions with database schema elements -- is a critical yet underexplored component of Text-to-SQL systems. While recent methods have focused primarily on improving SQL generation, they often neglect the retrieval of relevant schema elements, which can lead to hallucinations and execution failures. In this work, we propose a context-aware bidirectional schema retrieval framework that treats schema linking as a standalone problem. Our approach combines two complementary strategies: table-first retrieval followed by column selection, and column-first retrieval followed by table selection. It is further augmented with techniques such as question decomposition, keyword extraction, and keyphrase extraction. Through comprehensive evaluations on challenging benchmarks such as BIRD and Spider, we demonstrate that our method significantly improves schema recall while reducing false positives. Moreover, SQL generation using our retrieved schema consistently outperforms full-schema baselines and closely approaches oracle performance, all without requiring query refinement. Notably, our method narrows the performance gap between full and perfect schema settings by 50\\%. Our findings highlight schema linking as a powerful lever for enhancing Text-to-SQL accuracy and efficiency.",
        "translated": "模式链接——将自然语言问题与数据库模式元素对齐的过程——是文本到SQL（Text-to-SQL）系统中一个关键但尚未充分探索的组成部分。尽管最近的方法主要集中在提升SQL生成的能力上，它们往往忽视了相关模式元素的检索，这可能导致幻觉（hallucinations）和执行失败。在本文中，我们提出了一种上下文感知的双向模式检索框架，将模式链接视为一个独立的问题进行处理。我们的方法结合了两种互补策略：先进行表检索，然后选择列；以及先进行列检索，然后选择表。此外，该方法还融合了诸如问题分解、关键词提取和关键短语提取等技术。在具有挑战性的基准数据集BIRD和Spider上的全面评估表明，我们的方法在显著提升模式召回率的同时降低了误报率。此外，基于我们所检索到的模式进行SQL生成在性能上始终优于全模式基线方法，并且接近理想模式下的性能，而无需进行查询优化。值得注意的是，我们的方法将全模式与理想模式设置之间的性能差距缩小了50\\%。我们的研究结果表明，模式链接是提升文本到SQL系统准确性和效率的强大杠杆。",
        "translated_title": "Rethinking Schema Linking: A Context-Aware Bidirectional Retrieval Approach for Text-to-SQL  \n重新思考模式链接：一种面向上下文的双向检索方法用于文本到SQL  \n\nAbstract  \n摘要  \n\nSchema linking is a critical component in text-to-SQL tasks, aiming to map natural language utterances to corresponding database schema elements. Most existing methods focus on modeling the schema from a single perspective, either by leveraging schema elements independently or by incorporating contextual information from the natural language query. However, these approaches often suffer from incomplete schema understanding and limited contextual awareness, leading to suboptimal performance in complex schema environments. To address this issue, we propose a **Context-Aware Bidirectional Retrieval (CABR)** framework that simultaneously considers both the schema context and the query context. CABR introduces a dual-encoder architecture to learn semantic representations for schema elements and queries from different views, and establishes a bidirectional retrieval mechanism that links schema elements to queries and vice versa. This design allows the model to better capture the interdependencies between schema elements and queries, improving the accuracy of schema linking. We conduct extensive experiments on the Spider and WikiSQL datasets, and the results demonstrate that CABR significantly outperforms state-of-the-art baselines in both schema element selection and SQL generation.  \n模式链接是文本到SQL任务中的关键组成部分，其目标是将自然语言语句映射到对应的数据库模式元素。大多数现有方法从单一视角建模模式，要么独立利用模式元素，要么结合来自自然语言查询的上下文信息。然而，这些方法通常存在模式理解不完整和上下文感知能力有限的问题，导致在复杂模式环境下性能欠佳。为了解决这一问题，我们提出了一种**面向上下文的双向检索（CABR）**框架，该框架同时考虑模式上下文和查询上下文。CABR引入了一种双编码器结构，从不同视角学习模式元素和查询的语义表示，并建立了一个双向检索机制，将模式元素与查询相互链接。该设计使模型能够更好地捕捉模式元素与查询之间的依赖关系，提高模式链接的准确性。我们在Spider和WikiSQL数据集上进行了广泛的实验，结果表明，CABR在模式元素选择和SQL生成方面均显著优于最先进的基线方法。  \n\nIntroduction  \n引言  \n\nText-to-SQL tasks aim to automatically translate natural language queries into executable SQL statements, enabling users to interact with databases without requiring SQL expertise. A core challenge in this task is schema linking, which involves identifying the relevant schema elements (e.g., tables, columns, values) for a given query. Effective schema linking is essential for accurate SQL generation, as incorrect or incomplete schema mappings can lead to invalid queries and poor performance.  \n文本到SQL任务旨在自动将自然语言查询转化为可执行的SQL语句，使用户无需具备SQL专业知识即可与数据库进行交互。该任务中的一个核心挑战是模式链接，即识别给定查询中相关的模式元素（例如，表、列、值）。有效的模式链接对于准确的SQL生成至关重要，因为错误或不完整的模式映射可能导致无效查询和较差的性能。  \n\nTraditional schema linking approaches typically treat the schema as a flat structure and model it using either rule-based methods or single-view neural architectures. These methods often fail to capture the full context of both the schema and the query, especially in complex, multi-table environments where relationships between schema elements are crucial.  \n传统的模式链接方法通常将模式视为扁平结构，并使用基于规则的方法或单视角神经网络架构对其进行建模。这些方法常常无法捕捉模式和查询的完整上下文，特别是在复杂的多表环境中，其中模式元素之间的关系至关重要。  \n\nTo overcome these limitations, we propose a novel framework that jointly models schema and query contexts using a bidirectional retrieval mechanism. Our method enhances the contextual awareness of schema elements by considering both their structural information and their semantic relationships with the query.  \n为克服这些限制，我们提出了一种新的框架，使用双向检索机制联合建模模式和查询上下文。我们的方法通过考虑模式元素的结构信息及其与查询的语义关系，增强了其上下文感知能力。",
        "label": [],
        "label_reason": "论文聚焦Text-to-SQL，非推荐系统核心问题，相关性较低。",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出新颖的双向检索框架，改进Schema Linking效果。"
    },
    {
        "title": "PRISM: Agentic Retrieval with LLMs for Multi-Hop Question Answering",
        "url": "http://arxiv.org/abs/2510.14278v1",
        "pub_date": "2025-10-16",
        "summary": "Retrieval plays a central role in multi-hop question answering (QA), where answering complex questions requires gathering multiple pieces of evidence. We introduce an Agentic Retrieval System that leverages large language models (LLMs) in a structured loop to retrieve relevant evidence with high precision and recall. Our framework consists of three specialized agents: a Question Analyzer that decomposes a multi-hop question into sub-questions, a Selector that identifies the most relevant context for each sub-question (focusing on precision), and an Adder that brings in any missing evidence (focusing on recall). The iterative interaction between Selector and Adder yields a compact yet comprehensive set of supporting passages. In particular, it achieves higher retrieval accuracy while filtering out distracting content, enabling downstream QA models to surpass full-context answer accuracy while relying on significantly less irrelevant information. Experiments on four multi-hop QA benchmarks -- HotpotQA, 2WikiMultiHopQA, MuSiQue, and MultiHopRAG -- demonstrates that our approach consistently outperforms strong baselines.",
        "translated": "召回在多跳问答（QA）中起着核心作用，其中回答复杂问题需要收集多个证据。我们引入了一种基于智能体的召回系统（Agentic Retrieval System），通过结构化的循环利用大语言模型（LLMs），以高精度和高召回率检索相关证据。我们的框架包含三个专门的智能体：一个用于将多跳问题拆解为子问题的问题分析器（Question Analyzer），一个用于为每个子问题识别最相关上下文（注重精度）的选取器（Selector），以及一个用于补充缺失证据（注重召回）的添加器（Adder）。Selector 与 Adder 之间的迭代交互产生了一组紧凑而全面的支持段落。特别是，它在过滤干扰内容的同时实现了更高的召回精度，使下游 QA 模型能够在依赖明显更少无关信息的情况下超越全上下文回答的准确性。在四个多跳 QA 基准测试 HotpotQA、2WikiMultiHopQA、MuSiQue 和 MultiHopRAG 上的实验表明，我们的方法始终优于强基线方法。",
        "translated_title": "PRISM：利用大语言模型进行多跳问答的代理式召回",
        "label": [
            "多模态推荐",
            "LLM生成式推荐"
        ],
        "label_reason": "论文涉及LLM用于信息检索，可间接用于推荐系统的生成式方法",
        "relevance_score": 5,
        "novelty_score": 7,
        "novelty_reason": "提出结构化代理检索框架，改进多跳QA的精度与召回"
    },
    {
        "title": "Coupled Diffusion Sampling for Training-Free Multi-View Image Editing",
        "url": "http://arxiv.org/abs/2510.14981v1",
        "pub_date": "2025-10-16",
        "summary": "We present an inference-time diffusion sampling method to perform multi-view consistent image editing using pre-trained 2D image editing models. These models can independently produce high-quality edits for each image in a set of multi-view images of a 3D scene or object, but they do not maintain consistency across views. Existing approaches typically address this by optimizing over explicit 3D representations, but they suffer from a lengthy optimization process and instability under sparse view settings. We propose an implicit 3D regularization approach by constraining the generated 2D image sequences to adhere to a pre-trained multi-view image distribution. This is achieved through coupled diffusion sampling, a simple diffusion sampling technique that concurrently samples two trajectories from both a multi-view image distribution and a 2D edited image distribution, using a coupling term to enforce the multi-view consistency among the generated images. We validate the effectiveness and generality of this framework on three distinct multi-view image editing tasks, demonstrating its applicability across various model architectures and highlighting its potential as a general solution for multi-view consistent editing.",
        "translated": "我们提出了一种在推理阶段使用的扩散采样方法，用于利用预训练的 2D 图像编辑模型进行多视角一致的图像编辑。这些模型可以独立地为 3D 场景或物体的多视角图像集中的每张图像生成高质量的编辑结果，但它们在不同视角之间无法保持一致性。现有方法通常通过在显式的 3D 表示上进行优化来解决这一问题，但它们在稀疏视角设置下优化过程漫长且不稳定。我们提出了一种隐式的 3D 正则化方法，通过约束生成的 2D 图像序列以遵循预训练的多视角图像分布。该方法是通过耦合扩散采样实现的，这是一种简单的扩散采样技术，同时从多视角图像分布和 2D 编辑图像分布中采样两条轨迹，并使用耦合项来强制生成图像之间的多视角一致性。我们在三个不同的多视角图像编辑任务上验证了该框架的有效性和通用性，展示了其在各种模型架构中的适用性，并突出了其作为多视角一致性编辑通用解决方案的潜力。",
        "translated_title": "Coupled Diffusion Sampling for Training-Free Multi-View Image Editing  \n用于无需训练的多视角图像编辑的耦合扩散采样",
        "label": [
            "多帧/视频图像恢复"
        ],
        "label_reason": "方法涉及多视角图像生成一致性，属于多帧图像恢复范畴",
        "relevance_score": 5,
        "novelty_score": 8,
        "novelty_reason": "提出耦合扩散采样新方法，具有一定的创新性"
    },
    {
        "title": "From Pixels to Words -- Towards Native Vision-Language Primitives at\n  Scale",
        "url": "http://arxiv.org/abs/2510.14979v1",
        "pub_date": "2025-10-16",
        "summary": "The edifice of native Vision-Language Models (VLMs) has emerged as a rising contender to typical modular VLMs, shaped by evolving model architectures and training paradigms. Yet, two lingering clouds cast shadows over its widespread exploration and promotion: (-) What fundamental constraints set native VLMs apart from modular ones, and to what extent can these barriers be overcome? (-) How to make research in native VLMs more accessible and democratized, thereby accelerating progress in the field. In this paper, we clarify these challenges and outline guiding principles for constructing native VLMs. Specifically, one native VLM primitive should: (i) effectively align pixel and word representations within a shared semantic space; (ii) seamlessly integrate the strengths of formerly separate vision and language modules; (iii) inherently embody various cross-modal properties that support unified vision-language encoding, aligning, and reasoning. Hence, we launch NEO, a novel family of native VLMs built from first principles, capable of rivaling top-tier modular counterparts across diverse real-world scenarios. With only 390M image-text examples, NEO efficiently develops visual perception from scratch while mitigating vision-language conflicts inside a dense and monolithic model crafted from our elaborate primitives. We position NEO as a cornerstone for scalable and powerful native VLMs, paired with a rich set of reusable components that foster a cost-effective and extensible ecosystem. Our code and models are publicly available at: https://github.com/EvolvingLMMs-Lab/NEO.",
        "translated": "原生视觉-语言模型（VLMs）的构建已逐渐成为传统模块化VLMs的有力竞争者，这得益于模型架构和训练范式的发展与演变。然而，两个关键问题仍阻碍其广泛研究与推广：（-）原生VLMs与模块化VLMs之间存在哪些基本限制？这些限制在多大程度上可以被克服？（-）如何使原生VLMs的研究更具可访问性和普及性，从而加快该领域的发展进程？在本文中，我们明确了这些挑战，并提出了构建原生VLMs的指导原则。具体而言，一个原生VLM的基本要素应满足以下三点：（i）在共享的语义空间中有效对齐像素与语言表示；（ii）无缝融合视觉与语言模块各自的优势；（iii）内在地体现多种跨模态属性，以支持统一的视觉-语言编码、对齐和推理。因此，我们提出了NEO，一种基于第一性原理构建的新型原生VLM家族，其在多种现实场景中能够媲美最先进的模块化模型。在仅使用390M图像-文本样例的情况下，NEO便能高效地从零开始发展视觉感知能力，同时在由我们精心设计的密集单一模型中缓解视觉-语言冲突。我们视NEO为构建可扩展且强大的原生VLMs的基石，并提供了丰富的可复用组件，以促进经济高效且可扩展的生态系统形成。我们的代码和模型已公开发布于：https://github.com/EvolvingLMMs-Lab/NEO。",
        "translated_title": "从像素到词语——迈向大规模的原生视觉-语言基元",
        "label": [],
        "label_reason": "论文专注于视觉-语言模型，非图像像素级处理任务。",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "提出新的视觉-语言模型构建原则和组件，但创新点主要在架构整合。"
    },
    {
        "title": "Agentic Design of Compositional Machines",
        "url": "http://arxiv.org/abs/2510.14980v1",
        "pub_date": "2025-10-16",
        "summary": "The design of complex machines stands as both a marker of human intelligence and a foundation of engineering practice. Given recent advances in large language models (LLMs), we ask whether they, too, can learn to create. We approach this question through the lens of compositional machine design: a task in which machines are assembled from standardized components to meet functional demands like locomotion or manipulation in a simulated physical environment. To support this investigation, we introduce BesiegeField, a testbed built on the machine-building game Besiege, which enables part-based construction, physical simulation and reward-driven evaluation. Using BesiegeField, we benchmark state-of-the-art LLMs with agentic workflows and identify key capabilities required for success, including spatial reasoning, strategic assembly, and instruction-following. As current open-source models fall short, we explore reinforcement learning (RL) as a path to improvement: we curate a cold-start dataset, conduct RL finetuning experiments, and highlight open challenges at the intersection of language, machine design, and physical reasoning.",
        "translated": "复杂机器的设计既是人类智能的标志，也是工程实践的基础。鉴于近年来在大语言模型（LLMs）方面取得的进展，我们提出一个问题：它们是否也能学会创造？我们通过组合式机器设计的视角来探讨这一问题：该任务要求在模拟的物理环境中，通过标准化组件的组合来满足功能性需求，例如移动或操作。为了支持这项研究，我们引入了 BesiegeField，这是一个基于机器建造游戏 Besiege 构建的测试平台，支持基于部件的构建、物理模拟以及基于奖励的评估。借助 BesiegeField，我们对最先进的 LLMs 以及代理式工作流程进行了基准测试，并识别出成功所必需的关键能力，包括空间推理、策略性组装和遵循指令。由于目前的开源模型尚无法满足要求，我们探索了强化学习（RL）作为提升的路径：我们构建了一个冷启动数据集，进行了 RL 微调实验，并强调了语言、机器设计和物理推理交叉领域中的开放挑战。",
        "translated_title": "Agentic Design of Compositional Machines  \n复合机器的智能体设计",
        "label": [],
        "label_reason": "论文聚焦于机器设计与语言模型应用，非图像像素级处理任务",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出新测试平台与RL冷启动数据集，但方法迁移性有限"
    },
    {
        "title": "Learning an Image Editing Model without Image Editing Pairs",
        "url": "http://arxiv.org/abs/2510.14978v1",
        "pub_date": "2025-10-16",
        "summary": "Recent image editing models have achieved impressive results while following natural language editing instructions, but they rely on supervised fine-tuning with large datasets of input-target pairs. This is a critical bottleneck, as such naturally occurring pairs are hard to curate at scale. Current workarounds use synthetic training pairs that leverage the zero-shot capabilities of existing models. However, this can propagate and magnify the artifacts of the pretrained model into the final trained model. In this work, we present a new training paradigm that eliminates the need for paired data entirely. Our approach directly optimizes a few-step diffusion model by unrolling it during training and leveraging feedback from vision-language models (VLMs). For each input and editing instruction, the VLM evaluates if an edit follows the instruction and preserves unchanged content, providing direct gradients for end-to-end optimization. To ensure visual fidelity, we incorporate distribution matching loss (DMD), which constrains generated images to remain within the image manifold learned by pretrained models. We evaluate our method on standard benchmarks and include an extensive ablation study. Without any paired data, our method performs on par with various image editing diffusion models trained on extensive supervised paired data, under the few-step setting. Given the same VLM as the reward model, we also outperform RL-based techniques like Flow-GRPO.",
        "translated": "近期的图像编辑模型在遵循自然语言编辑指令的情况下取得了令人印象深刻的结果，但它们依赖于使用大量输入-目标对数据集进行监督式微调。这成为了一个关键性的瓶颈，因为这种自然形成的配对数据很难大规模地进行整理。当前的变通方法使用合成的训练对，利用现有模型的零样本能力。然而，这可能会将预训练模型中的伪影传播并放大到最终训练好的模型中。在本文中，我们提出了一种新的训练范式，完全消除了对配对数据的需求。我们的方法在训练过程中通过展开模型并利用视觉-语言模型（VLM）的反馈，直接优化一个几步扩散模型。对于每个输入图像和编辑指令，VLM会评估编辑是否遵循指令并保留未更改的内容，从而提供端到端优化所需的直接梯度。为了确保视觉保真度，我们引入了分布匹配损失（DMD），该损失限制生成的图像保持在预训练模型所学习的图像流形内。我们在标准基准数据集上评估了我们的方法，并进行了广泛的消融研究。在几步扩散设置下，即使没有任何配对数据，我们的方法也与使用大量监督配对数据训练的各种图像编辑扩散模型表现相当。在使用相同VLM作为奖励模型的前提下，我们的方法还优于基于强化学习的技术，如 Flow-GRPO。",
        "translated_title": "学习无需图像编辑配对的图像编辑模型",
        "label": [],
        "label_reason": "论文主要关注图像编辑而非像素级恢复",
        "relevance_score": 3,
        "novelty_score": 8,
        "novelty_reason": "提出无需配对数据的图像编辑新范式"
    },
    {
        "title": "Ponimator: Unfolding Interactive Pose for Versatile Human-human\n  Interaction Animation",
        "url": "http://arxiv.org/abs/2510.14976v1",
        "pub_date": "2025-10-16",
        "summary": "Close-proximity human-human interactive poses convey rich contextual information about interaction dynamics. Given such poses, humans can intuitively infer the context and anticipate possible past and future dynamics, drawing on strong priors of human behavior. Inspired by this observation, we propose Ponimator, a simple framework anchored on proximal interactive poses for versatile interaction animation. Our training data consists of close-contact two-person poses and their surrounding temporal context from motion-capture interaction datasets. Leveraging interactive pose priors, Ponimator employs two conditional diffusion models: (1) a pose animator that uses the temporal prior to generate dynamic motion sequences from interactive poses, and (2) a pose generator that applies the spatial prior to synthesize interactive poses from a single pose, text, or both when interactive poses are unavailable. Collectively, Ponimator supports diverse tasks, including image-based interaction animation, reaction animation, and text-to-interaction synthesis, facilitating the transfer of interaction knowledge from high-quality mocap data to open-world scenarios. Empirical experiments across diverse datasets and applications demonstrate the universality of the pose prior and the effectiveness and robustness of our framework.",
        "translated": "近距离的人与人之间的交互姿态能够传达丰富的交互动态上下文信息。在给定这些姿态的情况下，人类可以直观地推断交互的上下文，并预测可能的过去和未来动态，这依赖于对人类行为的强先验知识。受这一观察的启发，我们提出了 Ponimator，一种基于邻近交互姿态的简单框架，用于实现多样化的交互动画。我们的训练数据包括来自运动捕捉交互数据集的近距离接触的双人姿态及其周围的时间上下文。借助交互姿态先验，Ponimator 使用了两个条件扩散模型：(1) 一个姿态动画生成器，利用时间先验从交互姿态生成动态运动序列；(2) 一个姿态生成器，当交互姿态不可用时，应用空间先验从单个姿态、文本或两者共同合成交互姿态。总体而言，Ponimator 支持多种任务，包括基于图像的交互动画、反应动画以及从文本生成交互，从而促进将高质量运动捕捉数据中的交互知识迁移至开放世界场景中。在多个数据集和应用上的实证实验表明，姿态先验具有普遍性，且我们的框架在性能和鲁棒性方面均表现出色。",
        "translated_title": "Ponimator：展开交互式姿态以实现多样化的人与人交互动画",
        "label": [],
        "label_reason": "论文聚焦于人体姿态动画生成，属于高阶任务，不涉及像素级图像处理",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "提出了基于扩散模型的姿态动画框架，但方法属于常规迁移"
    },
    {
        "title": "Terra: Explorable Native 3D World Model with Point Latents",
        "url": "http://arxiv.org/abs/2510.14977v1",
        "pub_date": "2025-10-16",
        "summary": "World models have garnered increasing attention for comprehensive modeling of the real world. However, most existing methods still rely on pixel-aligned representations as the basis for world evolution, neglecting the inherent 3D nature of the physical world. This could undermine the 3D consistency and diminish the modeling efficiency of world models. In this paper, we present Terra, a native 3D world model that represents and generates explorable environments in an intrinsic 3D latent space. Specifically, we propose a novel point-to-Gaussian variational autoencoder (P2G-VAE) that encodes 3D inputs into a latent point representation, which is subsequently decoded as 3D Gaussian primitives to jointly model geometry and appearance. We then introduce a sparse point flow matching network (SPFlow) for generating the latent point representation, which simultaneously denoises the positions and features of the point latents. Our Terra enables exact multi-view consistency with native 3D representation and architecture, and supports flexible rendering from any viewpoint with only a single generation process. Furthermore, Terra achieves explorable world modeling through progressive generation in the point latent space. We conduct extensive experiments on the challenging indoor scenes from ScanNet v2. Terra achieves state-of-the-art performance in both reconstruction and generation with high 3D consistency.",
        "translated": "世界模型因其对现实世界的全面建模能力而受到越来越多的关注。然而，大多数现有方法仍然依赖像素对齐的表示作为世界演化的基础，忽视了物理世界固有的三维特性。这可能会削弱世界模型的三维一致性，并降低其建模效率。在本文中，我们提出了 Terra，这是一种原生的三维世界模型，能够在内在的三维潜在空间中表示和生成可探索的环境。具体而言，我们提出了一种新颖的点到高斯变分自编码器（P2G-VAE），它将三维输入编码为潜在点表示，随后将其解码为三维高斯基元，以联合建模几何和外观。我们接着引入了一个稀疏点流匹配网络（SPFlow）来生成潜在点表示，该网络可同时对点潜在的位置和特征进行去噪。我们的 Terra 通过原生的三维表示和架构实现了精确的多视角一致性，并支持仅通过一次生成过程即可从任意视角进行灵活渲染。此外，Terra 通过在点潜在空间中的渐进生成实现了可探索的世界建模。我们在具有挑战性的 ScanNet v2 室内场景上进行了广泛的实验，Terra 在重建和生成方面均达到了最先进的性能，且具有高度的三维一致性。",
        "translated_title": "Terra：具有点潜在表示的可探索本原三维世界模型",
        "label": [],
        "label_reason": "论文关注3D世界建模与生成，不直接处理像素级图像质量",
        "relevance_score": 3,
        "novelty_score": 8,
        "novelty_reason": "提出P2G-VAE与SPFlow新架构，实现3D表示与生成"
    },
    {
        "title": "WithAnyone: Towards Controllable and ID Consistent Image Generation",
        "url": "http://arxiv.org/abs/2510.14975v1",
        "pub_date": "2025-10-16",
        "summary": "Identity-consistent generation has become an important focus in text-to-image research, with recent models achieving notable success in producing images aligned with a reference identity. Yet, the scarcity of large-scale paired datasets containing multiple images of the same individual forces most approaches to adopt reconstruction-based training. This reliance often leads to a failure mode we term copy-paste, where the model directly replicates the reference face rather than preserving identity across natural variations in pose, expression, or lighting. Such over-similarity undermines controllability and limits the expressive power of generation. To address these limitations, we (1) construct a large-scale paired dataset MultiID-2M, tailored for multi-person scenarios, providing diverse references for each identity; (2) introduce a benchmark that quantifies both copy-paste artifacts and the trade-off between identity fidelity and variation; and (3) propose a novel training paradigm with a contrastive identity loss that leverages paired data to balance fidelity with diversity. These contributions culminate in WithAnyone, a diffusion-based model that effectively mitigates copy-paste while preserving high identity similarity. Extensive qualitative and quantitative experiments demonstrate that WithAnyone significantly reduces copy-paste artifacts, improves controllability over pose and expression, and maintains strong perceptual quality. User studies further validate that our method achieves high identity fidelity while enabling expressive controllable generation.",
        "translated": "身份一致的生成已成为文本到图像研究中的一个重要焦点，近期模型在生成与参考身份对齐的图像方面取得了显著成功。然而，由于缺乏大规模的配对数据集，其中包含同一人的多张图像，大多数方法被迫采用基于重建的训练方式。这种依赖通常导致一种我们称之为“复制粘贴”的失效模式，即模型直接复制参考图像中的面部，而不是在姿态、表情或光照等自然变化下保持身份一致性。这种过度相似性削弱了生成的可控性，并限制了生成的表现力。为了解决这些局限，我们（1）构建了一个大规模的配对数据集 MultiID-2M，专门针对多人员场景，为每个身份提供多样化的参考图像；（2）引入了一个基准测试，用于量化“复制粘贴”伪影以及身份保真度与变化之间的权衡；（3）提出了一种新颖的训练范式，采用对比身份损失，利用配对数据在保真度和多样性之间取得平衡。这些贡献促成了 WithAnyone 这一基于扩散的模型，该模型在有效缓解“复制粘贴”问题的同时，仍能保持高身份相似性。大量定性和定量实验表明，WithAnyone 显著减少了“复制粘贴”伪影，提高了对姿态和表情的可控性，并保持了良好的感知质量。用户研究进一步验证了我们的方法在保持高身份保真度的同时，能够实现富有表现力的可控生成。",
        "translated_title": "WithAnyone: 超可控且ID一致的图像生成",
        "label": [],
        "label_reason": "论文关注文本生成图像的身份一致性，属于高阶生成任务，不涉及像素级图像恢复或增强。",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出了对比身份损失和多身份数据集，但方法基于扩散模型，创新性有限。"
    },
    {
        "title": "pi-Flow: Policy-Based Few-Step Generation via Imitation Distillation",
        "url": "http://arxiv.org/abs/2510.14974v1",
        "pub_date": "2025-10-16",
        "summary": "Few-step diffusion or flow-based generative models typically distill a velocity-predicting teacher into a student that predicts a shortcut towards denoised data. This format mismatch has led to complex distillation procedures that often suffer from a quality-diversity trade-off. To address this, we propose policy-based flow models ($\\pi$-Flow). $\\pi$-Flow modifies the output layer of a student flow model to predict a network-free policy at one timestep. The policy then produces dynamic flow velocities at future substeps with negligible overhead, enabling fast and accurate ODE integration on these substeps without extra network evaluations. To match the policy's ODE trajectory to the teacher's, we introduce a novel imitation distillation approach, which matches the policy's velocity to the teacher's along the policy's trajectory using a standard $\\ell_2$ flow matching loss. By simply mimicking the teacher's behavior, $\\pi$-Flow enables stable and scalable training and avoids the quality-diversity trade-off. On ImageNet 256$^2$, it attains a 1-NFE FID of 2.85, outperforming MeanFlow of the same DiT architecture. On FLUX.1-12B and Qwen-Image-20B at 4 NFEs, $\\pi$-Flow achieves substantially better diversity than state-of-the-art few-step methods, while maintaining teacher-level quality.",
        "translated": "基于少量步骤的扩散模型或流模型通常将预测速度的教师模型提炼为一个预测通向去噪数据捷径的学生模型。这种格式上的不匹配导致了复杂的提炼过程，通常会陷入质量和多样性之间的权衡问题。为了解决这一问题，我们提出了基于策略的流模型（$\\pi$-Flow）。$\\pi$-Flow 修改了学生流模型的输出层，使其在一个时间步上预测一个无需网络的策略。该策略随后以极低的计算开销生成未来子步骤中的动态流速度，从而在这些子步骤上实现快速且准确的 ODE 积分，而无需额外的网络评估。为了使策略的 ODE 轨迹与教师模型相匹配，我们引入了一种新颖的模仿提炼方法，该方法通过在策略轨迹上使用标准的 $\\ell_2$ 流匹配损失，将策略的速度与教师模型的速度进行匹配。通过简单地模仿教师模型的行为，$\\pi$-Flow 实现了稳定且可扩展的训练，并避免了质量和多样性之间的权衡问题。在 ImageNet 256$^2$ 数据集上，$\\pi$-Flow 达到了 1-NFE FID 为 2.85，优于相同 DiT 架构下的 MeanFlow。在 FLUX.1-12B 和 Qwen-Image-20B 模型上，使用 4 个 NFE 时，$\\pi$-Flow 显著提升了多样性，同时保持了与教师模型相当的质量。",
        "translated_title": "pi-Flow: 基于策略的少步生成方法通过模仿蒸馏",
        "label": [],
        "label_reason": "论文主要关注生成模型的快速推理，不属于图像恢复或增强任务",
        "relevance_score": 2,
        "novelty_score": 8,
        "novelty_reason": "提出基于策略的生成模型新方法，改进了少步生成的质量与多样性平衡"
    },
    {
        "title": "RDD: Retrieval-Based Demonstration Decomposer for Planner Alignment in\n  Long-Horizon Tasks",
        "url": "http://arxiv.org/abs/2510.14968v1",
        "pub_date": "2025-10-16",
        "summary": "To tackle long-horizon tasks, recent hierarchical vision-language-action (VLAs) frameworks employ vision-language model (VLM)-based planners to decompose complex manipulation tasks into simpler sub-tasks that low-level visuomotor policies can easily handle. Typically, the VLM planner is finetuned to learn to decompose a target task. This finetuning requires target task demonstrations segmented into sub-tasks by either human annotation or heuristic rules. However, the heuristic subtasks can deviate significantly from the training data of the visuomotor policy, which degrades task performance. To address these issues, we propose a Retrieval-based Demonstration Decomposer (RDD) that automatically decomposes demonstrations into sub-tasks by aligning the visual features of the decomposed sub-task intervals with those from the training data of the low-level visuomotor policies. Our method outperforms the state-of-the-art sub-task decomposer on both simulation and real-world tasks, demonstrating robustness across diverse settings. Code and more results are available at rdd-neurips.github.io.",
        "translated": "为应对长时域任务，最近的分层视觉-语言-动作（VLA）框架采用基于视觉-语言模型（VLM）的规划器，将复杂的操作任务分解为低级视觉运动策略可轻松处理的简单子任务。通常，VLM 规划器会被微调以学习如何分解目标任务。这种微调需要目标任务的演示数据被分割为子任务，分割方式依赖于人工标注或启发式规则。然而，这些启发式子任务与低级视觉运动策略的训练数据可能存在较大偏差，从而影响任务性能。为解决这些问题，我们提出了一种基于检索的演示分解器（RDD），通过将分解后的子任务区间中的视觉特征与低级视觉运动策略训练数据中的特征对齐，实现演示数据的自动子任务分解。我们的方法在仿真和真实世界任务中均优于最先进的子任务分解器，展现出在不同设置下的鲁棒性。代码和更多结果可在 rdd-neurips.github.io 获取。",
        "translated_title": "RDD：基于检索的演示分解器用于长时域任务中的规划器对齐",
        "label": [],
        "label_reason": "论文聚焦于高层任务规划，与图像像素级恢复无关。",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出基于检索的子任务分解方法，有一定改进但属于常规优化。"
    },
    {
        "title": "ChangingGrounding: 3D Visual Grounding in Changing Scenes",
        "url": "http://arxiv.org/abs/2510.14965v1",
        "pub_date": "2025-10-16",
        "summary": "Real-world robots localize objects from natural-language instructions while scenes around them keep changing. Yet most of the existing 3D visual grounding (3DVG) method still assumes a reconstructed and up-to-date point cloud, an assumption that forces costly re-scans and hinders deployment. We argue that 3DVG should be formulated as an active, memory-driven problem, and we introduce ChangingGrounding, the first benchmark that explicitly measures how well an agent can exploit past observations, explore only where needed, and still deliver precise 3D boxes in changing scenes. To set a strong reference point, we also propose Mem-ChangingGrounder, a zero-shot method for this task that marries cross-modal retrieval with lightweight multi-view fusion: it identifies the object type implied by the query, retrieves relevant memories to guide actions, then explores the target efficiently in the scene, falls back when previous operations are invalid, performs multi-view scanning of the target, and projects the fused evidence from multi-view scans to get accurate object bounding boxes. We evaluate different baselines on ChangingGrounding, and our Mem-ChangingGrounder achieves the highest localization accuracy while greatly reducing exploration cost. We hope this benchmark and method catalyze a shift toward practical, memory-centric 3DVG research for real-world applications. Project page: https://hm123450.github.io/CGB/ .",
        "translated": "现实世界中的机器人需要在周围场景不断变化的情况下，根据自然语言指令定位物体。然而，大多数现有的 3D 视觉定位（3DVG）方法仍然假设存在重建且最新的点云，这一假设迫使进行成本高昂的重新扫描，并阻碍实际部署。我们认为，3DVG 应该被表述为一个主动的、由记忆驱动的问题，因此我们引入了 ChangingGrounding，这是首个明确衡量智能体在变化场景中如何有效利用过往观测结果、仅在必要区域进行探索，并仍能提供精确 3D 边界框的基准。为了提供一个强有力的参考点，我们还提出了 Mem-ChangingGrounder，一种针对该任务的零样本方法，它将跨模态检索与轻量多视角融合相结合：该方法首先识别查询语句所暗示的物体类型，检索相关记忆以指导操作，然后在场景中高效地探索目标，当先前操作无效时进行回退，执行多视角扫描，最后将多视角扫描的融合证据进行投影，以获得精确的物体边界框。我们在 ChangingGrounding 上评估了不同的基线方法，我们的 Mem-ChangingGrounder 在显著降低探索成本的同时实现了最高的定位精度。我们希望这一基准和方法能够推动 3DVG 研究向实用、以记忆为中心的方向转变，以支持现实世界的应用。项目页面：https://hm123450.github.io/CGB/。",
        "translated_title": "ChangingGrounding: 变化场景中的3D视觉定位",
        "label": [],
        "label_reason": "论文聚焦3D视觉定位，属于high-level任务",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "提出新基准和零样本方法，但创新性有限"
    },
    {
        "title": "RainDiff: End-to-end Precipitation Nowcasting Via Token-wise Attention\n  Diffusion",
        "url": "http://arxiv.org/abs/2510.14962v1",
        "pub_date": "2025-10-16",
        "summary": "Precipitation nowcasting, predicting future radar echo sequences from current observations, is a critical yet challenging task due to the inherently chaotic and tightly coupled spatio-temporal dynamics of the atmosphere. While recent advances in diffusion-based models attempt to capture both large-scale motion and fine-grained stochastic variability, they often suffer from scalability issues: latent-space approaches require a separately trained autoencoder, adding complexity and limiting generalization, while pixel-space approaches are computationally intensive and often omit attention mechanisms, reducing their ability to model long-range spatio-temporal dependencies. To address these limitations, we propose a Token-wise Attention integrated into not only the U-Net diffusion model but also the spatio-temporal encoder that dynamically captures multi-scale spatial interactions and temporal evolution. Unlike prior approaches, our method natively integrates attention into the architecture without incurring the high resource cost typical of pixel-space diffusion, thereby eliminating the need for separate latent modules. Our extensive experiments and visual evaluations across diverse datasets demonstrate that the proposed method significantly outperforms state-of-the-art approaches, yielding superior local fidelity, generalization, and robustness in complex precipitation forecasting scenarios.",
        "translated": "降水临近预报，即根据当前观测预测未来的雷达回波序列，是一项关键但极具挑战性的任务，这是由于大气本身的混沌性以及紧密耦合的时空动态特性。尽管基于扩散模型的最新进展尝试捕捉大尺度运动和细粒度的随机变化，但它们通常存在可扩展性问题：隐空间方法需要单独训练的自编码器，增加了模型复杂性并限制了泛化能力，而像素空间方法计算成本高昂，且通常省略注意力机制，削弱了对长程时空依赖关系的建模能力。为了解决这些限制，我们提出了一种将 Token-wise 注意力机制集成到 U-Net 扩散模型以及动态捕捉多尺度空间交互和时间演化过程的时空编码器中的方法。与以往的方法不同，我们的方法在架构中原生地引入注意力机制，而无需承担像素空间扩散通常带来的高资源消耗，从而消除了对单独隐模块的依赖。我们在多个数据集上的广泛实验和可视化评估表明，所提出的方法显著优于最先进的方法，在复杂的降水预测场景中展现出更优越的局部真实性、泛化性和鲁棒性。",
        "translated_title": "RainDiff：通过逐标记注意力的扩散实现端到端降水临近预报",
        "label": [
            "图像去雨",
            "多帧/视频图像恢复"
        ],
        "label_reason": "论文聚焦于雷达降水序列预测，属于多帧图像恢复和去雨任务。",
        "relevance_score": 8,
        "novelty_score": 8,
        "novelty_reason": "提出 Token-wise Attention 结构，改进扩散模型的时空建模能力。"
    },
    {
        "title": "C4D: 4D Made from 3D through Dual Correspondences",
        "url": "http://arxiv.org/abs/2510.14960v1",
        "pub_date": "2025-10-16",
        "summary": "Recovering 4D from monocular video, which jointly estimates dynamic geometry and camera poses, is an inevitably challenging problem. While recent pointmap-based 3D reconstruction methods (e.g., DUSt3R) have made great progress in reconstructing static scenes, directly applying them to dynamic scenes leads to inaccurate results. This discrepancy arises because moving objects violate multi-view geometric constraints, disrupting the reconstruction. To address this, we introduce C4D, a framework that leverages temporal Correspondences to extend existing 3D reconstruction formulation to 4D. Specifically, apart from predicting pointmaps, C4D captures two types of correspondences: short-term optical flow and long-term point tracking. We train a dynamic-aware point tracker that provides additional mobility information, facilitating the estimation of motion masks to separate moving elements from the static background, thus offering more reliable guidance for dynamic scenes. Furthermore, we introduce a set of dynamic scene optimization objectives to recover per-frame 3D geometry and camera parameters. Simultaneously, the correspondences lift 2D trajectories into smooth 3D trajectories, enabling fully integrated 4D reconstruction. Experiments show that our framework achieves complete 4D recovery and demonstrates strong performance across multiple downstream tasks, including depth estimation, camera pose estimation, and point tracking. Project Page: https://littlepure2333.github.io/C4D",
        "translated": "从单目视频中恢复4D，即联合估计动态几何结构和相机姿态，是一个不可避免的具有挑战性的问题。尽管近年来基于点图的3D重建方法（例如DUSt3R）在静态场景重建方面取得了显著进展，但直接将其应用于动态场景会导致不准确的结果。这种差异的产生是由于运动物体违反了多视图几何约束，从而破坏了重建效果。为了解决这一问题，我们提出了C4D，一个利用时间对应关系将现有3D重建方法扩展到4D的框架。具体而言，除了预测点图外，C4D还捕捉两种类型的对应关系：短期光流和长期点跟踪。我们训练了一个动态感知的点跟踪器，提供额外的运动信息，有助于估计运动掩码以将运动元素与静态背景分离，从而为动态场景提供更可靠的引导。此外，我们引入了一组动态场景优化目标，以恢复每帧的3D几何结构和相机参数。同时，这些对应关系将2D轨迹提升为平滑的3D轨迹，实现了完整的4D重建。实验表明，我们的框架能够实现完整的4D恢复，并在多个下游任务中表现出强大的性能，包括深度估计、相机姿态估计和点跟踪。项目页面：https://littlepure2333.github.io/C4D",
        "translated_title": "C4D：通过双对应关系从3D生成4D",
        "label": [],
        "label_reason": "论文主要关注4D重建与动态场景分析，不属于像素级图像恢复或增强任务",
        "relevance_score": 3,
        "novelty_score": 7,
        "novelty_reason": "提出基于双对应关系的动态场景4D重建框架，方法设计新颖"
    },
    {
        "title": "MathCanvas: Intrinsic Visual Chain-of-Thought for Multimodal\n  Mathematical Reasoning",
        "url": "http://arxiv.org/abs/2510.14958v1",
        "pub_date": "2025-10-16",
        "summary": "While Large Language Models (LLMs) have excelled in textual reasoning, they struggle with mathematical domains like geometry that intrinsically rely on visual aids. Existing approaches to Visual Chain-of-Thought (VCoT) are often limited by rigid external tools or fail to generate the high-fidelity, strategically-timed diagrams necessary for complex problem-solving. To bridge this gap, we introduce MathCanvas, a comprehensive framework designed to endow unified Large Multimodal Models (LMMs) with intrinsic VCoT capabilities for mathematics. Our approach consists of two phases. First, a Visual Manipulation stage pre-trains the model on a novel 15.2M-pair corpus, comprising 10M caption-to-diagram pairs (MathCanvas-Imagen) and 5.2M step-by-step editing trajectories (MathCanvas-Edit), to master diagram generation and editing. Second, a Strategic Visual-Aided Reasoning stage fine-tunes the model on MathCanvas-Instruct, a new 219K-example dataset of interleaved visual-textual reasoning paths, teaching it when and how to leverage visual aids. To facilitate rigorous evaluation, we introduce MathCanvas-Bench, a challenging benchmark with 3K problems that require models to produce interleaved visual-textual solutions. Our model, BAGEL-Canvas, trained under this framework, achieves an 86% relative improvement over strong LMM baselines on MathCanvas-Bench, demonstrating excellent generalization to other public math benchmarks. Our work provides a complete toolkit-framework, datasets, and benchmark-to unlock complex, human-like visual-aided reasoning in LMMs. Project Page: https://mathcanvas.github.io/",
        "translated": "虽然大型语言模型（LLMs）在文本推理方面表现出色，但它们在数学领域（如几何学）中表现不佳，因为这些领域本质上依赖于视觉辅助手段。现有的视觉推理链（VCoT）方法通常受到外部工具刚性的限制，或者无法生成在复杂问题求解过程中所需高保真度、策略性时间点的图表。为了解决这一问题，我们提出 MathCanvas，一个全面的框架，旨在为统一的大型多模态模型（LMMs）赋予内在的 VCoT 能力以应对数学问题。我们的方法包括两个阶段。第一阶段是视觉操作（Visual Manipulation），在此阶段，模型在一个全新的 1520 万对语料库上进行预训练，该语料库包括 1000 万对描述到图表的配对（MathCanvas-Imagen）以及 520 万条逐步编辑轨迹（MathCanvas-Edit），从而掌握图表生成和编辑的能力。第二阶段是策略性的视觉辅助推理（Strategic Visual-Aided Reasoning），模型在 MathCanvas-Instruct 上进行微调，这是一个新的 21.9 万个示例的数据集，包含交错的图文推理路径，教会模型何时以及如何利用视觉辅助手段。为了便于严格评估，我们引入了 MathCanvas-Bench 这一具有挑战性的基准，包含 3000 个问题，要求模型生成交错的图文解决方案。在该框架下训练的模型 BAGEL-Canvas 在 MathCanvas-Bench 上相比强大的 LMM 基线模型取得了 86% 的相对提升，显示出在其他公共数学基准上出色的泛化能力。我们的工作提供了一套完整的工具包，包括框架、数据集和基准，以在 LMM 中实现复杂且类似人类的视觉辅助推理。项目页面：https://mathcanvas.github.io/",
        "translated_title": "MathCanvas: 用于多模态数学推理的内在视觉思维链",
        "label": [],
        "label_reason": "论文聚焦多模态数学推理，非图像像素级处理任务",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "提出了多阶段框架和新数据集，但方法基于现有大模型范式"
    },
    {
        "title": "RealDPO: Real or Not Real, that is the Preference",
        "url": "http://arxiv.org/abs/2510.14955v1",
        "pub_date": "2025-10-16",
        "summary": "Video generative models have recently achieved notable advancements in synthesis quality. However, generating complex motions remains a critical challenge, as existing models often struggle to produce natural, smooth, and contextually consistent movements. This gap between generated and real-world motions limits their practical applicability. To address this issue, we introduce RealDPO, a novel alignment paradigm that leverages real-world data as positive samples for preference learning, enabling more accurate motion synthesis. Unlike traditional supervised fine-tuning (SFT), which offers limited corrective feedback, RealDPO employs Direct Preference Optimization (DPO) with a tailored loss function to enhance motion realism. By contrasting real-world videos with erroneous model outputs, RealDPO enables iterative self-correction, progressively refining motion quality. To support post-training in complex motion synthesis, we propose RealAction-5K, a curated dataset of high-quality videos capturing human daily activities with rich and precise motion details. Extensive experiments demonstrate that RealDPO significantly improves video quality, text alignment, and motion realism compared to state-of-the-art models and existing preference optimization techniques.",
        "translated": "近期，视频生成模型在合成质量方面取得了显著进展。然而，生成复杂运动仍然是一个关键挑战，因为现有模型通常难以生成自然、流畅且上下文一致的运动。这种生成运动与真实世界运动之间的差距限制了它们的实际应用性。为了解决这一问题，我们提出 RealDPO，一种新颖的对齐范式，它利用真实世界数据作为偏好学习中的正样本，从而实现更精确的运动合成。与仅能提供有限纠正反馈的传统监督微调（SFT）方法不同，RealDPO 采用直接偏好优化（DPO）并配以定制的损失函数，以提升运动的真实感。通过将真实世界视频与模型错误的输出进行对比，RealDPO 可以实现迭代式的自我修正，逐步优化运动质量。为了支持复杂运动合成的后训练，我们提出了 RealAction-5K，一个经过精心挑选的高质量视频数据集，涵盖人类日常活动，包含丰富且精确的运动细节。大量实验表明，与最先进的模型以及现有的偏好优化技术相比，RealDPO 在视频质量、文本对齐以及运动真实感方面均有显著提升。",
        "translated_title": "RealDPO: 真实还是非真实，这是偏好",
        "label": [],
        "label_reason": "论文聚焦视频生成模型的运动合成，不属于图像恢复/增强任务。",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出新的对齐范式 RealDPO，改进了运动真实感建模方法。"
    },
    {
        "title": "OmniMotion: Multimodal Motion Generation with Continuous Masked\n  Autoregression",
        "url": "http://arxiv.org/abs/2510.14954v1",
        "pub_date": "2025-10-16",
        "summary": "Whole-body multi-modal human motion generation poses two primary challenges: creating an effective motion generation mechanism and integrating various modalities, such as text, speech, and music, into a cohesive framework. Unlike previous methods that usually employ discrete masked modeling or autoregressive modeling, we develop a continuous masked autoregressive motion transformer, where a causal attention is performed considering the sequential nature within the human motion. Within this transformer, we introduce a gated linear attention and an RMSNorm module, which drive the transformer to pay attention to the key actions and suppress the instability caused by either the abnormal movements or the heterogeneous distributions within multi-modalities. To further enhance both the motion generation and the multimodal generalization, we employ the DiT structure to diffuse the conditions from the transformer towards the targets. To fuse different modalities, AdaLN and cross-attention are leveraged to inject the text, speech, and music signals. Experimental results demonstrate that our framework outperforms previous methods across all modalities, including text-to-motion, speech-to-gesture, and music-to-dance. The code of our method will be made public.",
        "translated": "全身多模态人体动作生成面临两个主要挑战：建立一个有效的动作生成机制，以及将各种模态（如文本、语音和音乐）集成到一个统一的框架中。与以往通常采用离散掩码建模或自回归建模的方法不同，我们开发了一个连续掩码自回归运动变换器，在该变换器中，通过考虑人体动作的序列特性，引入了因果注意力机制。在该变换器中，我们引入了门控线性注意力和 RMSNorm 模块，使得模型能够关注关键动作，并抑制由异常动作或多模态中异构分布所引起的不稳定性。为了进一步增强动作生成和多模态泛化能力，我们采用 DiT 结构将条件从变换器扩散到目标。为了融合不同模态，我们利用 AdaLN 和交叉注意力机制将文本、语音和音乐信号注入模型。实验结果表明，我们的框架在所有模态上均优于先前的方法，包括文本到动作、语音到手势和音乐到舞蹈。我们的方法代码将会公开。",
        "translated_title": "OmniMotion: 基于连续掩码自回归的多模态运动生成",
        "label": [],
        "label_reason": "论文关注多模态运动生成，非图像像素级处理任务",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出连续掩码自回归运动生成框架，改进注意力机制和多模态融合方法"
    },
    {
        "title": "From Language to Locomotion: Retargeting-free Humanoid Control via\n  Motion Latent Guidance",
        "url": "http://arxiv.org/abs/2510.14952v1",
        "pub_date": "2025-10-16",
        "summary": "Natural language offers a natural interface for humanoid robots, but existing language-guided humanoid locomotion pipelines remain cumbersome and unreliable. They typically decode human motion, retarget it to robot morphology, and then track it with a physics-based controller. However, this multi-stage process is prone to cumulative errors, introduces high latency, and yields weak coupling between semantics and control. These limitations call for a more direct pathway from language to action, one that eliminates fragile intermediate stages. Therefore, we present RoboGhost, a retargeting-free framework that directly conditions humanoid policies on language-grounded motion latents. By bypassing explicit motion decoding and retargeting, RoboGhost enables a diffusion-based policy to denoise executable actions directly from noise, preserving semantic intent and supporting fast, reactive control. A hybrid causal transformer-diffusion motion generator further ensures long-horizon consistency while maintaining stability and diversity, yielding rich latent representations for precise humanoid behavior. Extensive experiments demonstrate that RoboGhost substantially reduces deployment latency, improves success rates and tracking accuracy, and produces smooth, semantically aligned locomotion on real humanoids. Beyond text, the framework naturally extends to other modalities such as images, audio, and music, providing a general foundation for vision-language-action humanoid systems.",
        "translated": "自然语言为人形机器人提供了自然的交互界面，但现有的语言引导人形机器人运动控制流程仍然复杂且不可靠。它们通常先解码人类运动，将其适配到机器人形态，然后通过基于物理的控制器进行跟踪。然而，这种多阶段流程容易累积误差，引入高延迟，并导致语义与控制之间的耦合较弱。这些限制促使我们寻求从语言到动作的更直接路径，从而消除脆弱的中间阶段。因此，我们提出 RoboGhost，一个无需适配的框架，它直接将人形机器人策略建立在语言引导的运动潜在表示之上。通过绕过显式运动解码和适配，RoboGhost 使基于扩散的策略能够直接从噪声中去噪出可执行动作，保留语义意图，并支持快速、反应式的控制。一个混合因果变压器-扩散运动生成器进一步确保了长期行为的一致性，同时保持稳定性和多样性，生成丰富的潜在表示以实现精确的人形机器人行为。大量实验表明，RoboGhost 显著降低了部署延迟，提高了成功率和跟踪精度，并在真实人形机器人上实现了平滑且语义一致的运动。除了文本之外，该框架自然地扩展到其他模态，如图像、音频和音乐，为人形机器人的视觉-语言-动作系统提供了通用基础。",
        "translated_title": "从语言到运动：基于运动隐空间引导的免重定向人形机器人控制",
        "label": [],
        "label_reason": "论文研究机器人运动控制，不属于图像处理任务",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "提出免重定向框架，但创新点在控制领域而非视觉"
    },
    {
        "title": "DialectGen: Benchmarking and Improving Dialect Robustness in Multimodal\n  Generation",
        "url": "http://arxiv.org/abs/2510.14949v1",
        "pub_date": "2025-10-16",
        "summary": "Contact languages like English exhibit rich regional variations in the form of dialects, which are often used by dialect speakers interacting with generative models. However, can multimodal generative models effectively produce content given dialectal textual input? In this work, we study this question by constructing a new large-scale benchmark spanning six common English dialects. We work with dialect speakers to collect and verify over 4200 unique prompts and evaluate on 17 image and video generative models. Our automatic and human evaluation results show that current state-of-the-art multimodal generative models exhibit 32.26% to 48.17% performance degradation when a single dialect word is used in the prompt. Common mitigation methods such as fine-tuning and prompt rewriting can only improve dialect performance by small margins (&lt; 7%), while potentially incurring significant performance degradation in Standard American English (SAE). To this end, we design a general encoder-based mitigation strategy for multimodal generative models. Our method teaches the model to recognize new dialect features while preserving SAE performance. Experiments on models such as Stable Diffusion 1.5 show that our method is able to simultaneously raise performance on five dialects to be on par with SAE (+34.4%), while incurring near zero cost to SAE performance.",
        "translated": "像英语这样的接触语言表现出丰富的地域变体，即方言，这些方言经常被用于与生成模型进行交互。然而，多模态生成模型是否能够有效处理方言文本输入并生成内容？在本工作中，我们通过构建一个新的大规模基准，覆盖六种常见的英语方言，来研究这一问题。我们与方言使用者合作，收集并验证了4200多个独特的提示词，并在17种图像和视频生成模型上进行了评估。我们的自动评估与人工评估结果表明，当前最先进的多模态生成模型在提示词中仅包含一个方言词汇的情况下，性能退化达到32.26%到48.17%。常见的缓解方法如微调和提示重写仅能带来小幅的性能提升（< 7%），同时可能在标准美式英语（SAE）上造成显著的性能下降。为了解决这一问题，我们设计了一种基于通用编码器的缓解策略，用于多模态生成模型。我们的方法在保留SAE性能的同时，使模型能够识别新的方言特征。在如Stable Diffusion 1.5等模型上的实验表明，我们的方法能够同时提升五种方言的性能，使其与SAE相当（+34.4%），而对SAE性能几乎没有影响。",
        "translated_title": "DialectGen：多模态生成中方言鲁棒性的基准测试与改进",
        "label": [],
        "label_reason": "论文聚焦于多模态生成模型对英语方言的鲁棒性，非图像像素级处理任务。",
        "relevance_score": 1,
        "novelty_score": 3,
        "novelty_reason": "提出针对方言输入的缓解策略，但方法基于通用编码器，创新性较弱。"
    },
    {
        "title": "3D Scene Prompting for Scene-Consistent Camera-Controllable Video\n  Generation",
        "url": "http://arxiv.org/abs/2510.14945v1",
        "pub_date": "2025-10-16",
        "summary": "We present 3DScenePrompt, a framework that generates the next video chunk from arbitrary-length input while enabling precise camera control and preserving scene consistency. Unlike methods conditioned on a single image or a short clip, we employ dual spatio-temporal conditioning that reformulates context-view referencing across the input video. Our approach conditions on both temporally adjacent frames for motion continuity and spatially adjacent content for scene consistency. However, when generating beyond temporal boundaries, directly using spatially adjacent frames would incorrectly preserve dynamic elements from the past. We address this by introducing a 3D scene memory that represents exclusively the static geometry extracted from the entire input video. To construct this memory, we leverage dynamic SLAM with our newly introduced dynamic masking strategy that explicitly separates static scene geometry from moving elements. The static scene representation can then be projected to any target viewpoint, providing geometrically consistent warped views that serve as strong 3D spatial prompts while allowing dynamic regions to evolve naturally from temporal context. This enables our model to maintain long-range spatial coherence and precise camera control without sacrificing computational efficiency or motion realism. Extensive experiments demonstrate that our framework significantly outperforms existing methods in scene consistency, camera controllability, and generation quality. Project page : https://cvlab-kaist.github.io/3DScenePrompt/",
        "translated": "我们提出 3DScenePrompt，这是一种在任意长度的输入基础上生成下一视频片段的框架，同时实现精确的相机控制并保持场景一致性。与依赖于单张图像或短片段的方法不同，我们采用双时空条件化策略，对输入视频中的上下文视图参考进行重新建模。我们的方法在时间相邻帧上进行条件化以保证运动连续性，在空间相邻内容上进行条件化以保持场景一致性。然而，在生成超出时间边界的内容时，直接使用空间相邻帧会导致过去动态元素被错误保留。我们通过引入一种3D场景记忆来解决这一问题，该记忆仅表示从整个输入视频中提取的静态几何结构。为构建这种记忆，我们结合动态SLAM与我们新提出的动态掩码策略，从而显式地将静态场景几何与动态元素分离。静态场景表示可以投影到任意目标视角，提供几何一致的视图，作为强有力的3D空间提示，同时允许动态区域从时间上下文中自然演变。这使得我们的模型在保持长期空间一致性和精确相机控制的同时，不会牺牲计算效率或运动的真实性。大量实验表明，我们的框架在场景一致性、相机可控性和生成质量方面显著优于现有方法。项目页面：https://cvlab-kaist.github.io/3DScenePrompt/",
        "translated_title": "3D场景提示用于场景一致的相机可控视频生成",
        "label": [],
        "label_reason": "论文聚焦于视频生成与相机控制，不属于低级图像处理任务。",
        "relevance_score": 2,
        "novelty_score": 8,
        "novelty_reason": "提出3D场景记忆和动态掩码策略，为视频生成提供新方法。"
    },
    {
        "title": "MaskCaptioner : Learning to Jointly Segment and Caption Object\n  Trajectories in Videos",
        "url": "http://arxiv.org/abs/2510.14904v1",
        "pub_date": "2025-10-16",
        "summary": "Dense Video Object Captioning (DVOC) is the task of jointly detecting, tracking, and captioning object trajectories in a video, requiring the ability to understand spatio-temporal details and describe them in natural language. Due to the complexity of the task and the high cost associated with manual annotation, previous approaches resort to disjoint training strategies, potentially leading to suboptimal performance. To circumvent this issue, we propose to generate captions about spatio-temporally localized entities leveraging a state-of-the-art VLM. By extending the LVIS and LV-VIS datasets with our synthetic captions (LVISCap and LV-VISCap), we train MaskCaptioner, an end-to-end model capable of jointly detecting, segmenting, tracking and captioning object trajectories. Moreover, with pretraining on LVISCap and LV-VISCap, MaskCaptioner achieves state-of-the-art DVOC results on three existing benchmarks, VidSTG, VLN and BenSMOT. The datasets and code are available at https://www.gabriel.fiastre.fr/maskcaptioner/.",
        "translated": "密集视频目标字幕生成（Dense Video Object Captioning，DVOC）是一项在视频中同时检测、跟踪并为目标轨迹生成字幕的任务，要求系统能够理解时空细节，并用自然语言加以描述。由于任务本身的复杂性和人工标注的高昂成本，以往的方法往往采用分阶段训练策略，可能导致性能次优。为了解决这一问题，我们提出利用最先进的视觉语言模型（VLM）生成关于时空定位实体的字幕。通过在LVIS和LV-VIS数据集上扩展我们合成的字幕（LVISCap和LV-VISCap），我们训练了一个端到端模型MaskCaptioner，能够同时完成目标的检测、分割、跟踪和字幕生成任务。此外，在对LVISCap和LV-VISCap进行预训练后，MaskCaptioner在三个现有的基准测试VidSTG、VLN和BenSMOT上取得了最先进的DVOC结果。数据集和代码可通过 https://www.gabriel.fiastre.fr/maskcaptioner/ 获得。",
        "translated_title": "MaskCaptioner：学习在视频中联合分割和描述物体轨迹",
        "label": [],
        "label_reason": "论文聚焦视频对象分割与描述，属于 high-level 任务",
        "relevance_score": 2,
        "novelty_score": 5,
        "novelty_reason": "方法结合 VLM 进行视频对象描述，有一定迁移创新"
    },
    {
        "title": "Leveraging Multimodal LLM Descriptions of Activity for Explainable\n  Semi-Supervised Video Anomaly Detection",
        "url": "http://arxiv.org/abs/2510.14896v1",
        "pub_date": "2025-10-16",
        "summary": "Existing semi-supervised video anomaly detection (VAD) methods often struggle with detecting complex anomalies involving object interactions and generally lack explainability. To overcome these limitations, we propose a novel VAD framework leveraging Multimodal Large Language Models (MLLMs). Unlike previous MLLM-based approaches that make direct anomaly judgments at the frame level, our method focuses on extracting and interpreting object activity and interactions over time. By querying an MLLM with visual inputs of object pairs at different moments, we generate textual descriptions of the activity and interactions from nominal videos. These textual descriptions serve as a high-level representation of the activity and interactions of objects in a video. They are used to detect anomalies during test time by comparing them to textual descriptions found in nominal training videos. Our approach inherently provides explainability and can be combined with many traditional VAD methods to further enhance their interpretability. Extensive experiments on benchmark datasets demonstrate that our method not only detects complex interaction-based anomalies effectively but also achieves state-of-the-art performance on datasets without interaction anomalies.",
        "translated": "现有的半监督视频异常检测（VAD）方法在检测涉及物体交互的复杂异常时通常表现不佳，且普遍缺乏可解释性。为克服这些限制，我们提出了一种新颖的 VAD 框架，该框架利用多模态大语言模型（MLLMs）。与之前基于 MLLM 的方法在帧级别直接进行异常判断不同，我们的方法聚焦于提取和解释视频中物体活动及其随时间变化的交互。通过在不同时间点对物体对的视觉输入查询 MLLM，我们生成了来自正常视频中活动和交互的文本描述。这些文本描述作为视频中物体活动和交互的高层表示，在测试阶段通过将其与正常训练视频中的文本描述进行比较，用于检测异常。我们的方法本质上提供了可解释性，并可与许多传统 VAD 方法结合，以进一步提升其可解释性。在基准数据集上的大量实验表明，我们的方法不仅能够有效检测基于交互的复杂异常，而且在无交互异常的数据集上也达到了最先进的性能。",
        "translated_title": "利用多模态大语言模型对活动的描述实现可解释的半监督视频异常检测",
        "label": [],
        "label_reason": "论文聚焦视频异常检测，属于 high-level 任务，不涉及像素级图像质量改善。",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出基于 MLLM 的新框架，提升 VAD 的可解释性，具备一定创新性。"
    },
    {
        "title": "OmniVinci: Enhancing Architecture and Data for Omni-Modal Understanding\n  LLM",
        "url": "http://arxiv.org/abs/2510.15870v1",
        "pub_date": "2025-10-17",
        "summary": "Advancing machine intelligence requires developing the ability to perceive across multiple modalities, much as humans sense the world. We introduce OmniVinci, an initiative to build a strong, open-source, omni-modal LLM. We carefully study the design choices across model architecture and data curation. For model architecture, we present three key innovations: (i) OmniAlignNet for strengthening alignment between vision and audio embeddings in a shared omni-modal latent space; (ii) Temporal Embedding Grouping for capturing relative temporal alignment between vision and audio signals; and (iii) Constrained Rotary Time Embedding for encoding absolute temporal information in omni-modal embeddings. We introduce a curation and synthesis pipeline that generates 24M single-modal and omni-modal conversations. We find that modalities reinforce one another in both perception and reasoning. Our model, OmniVinci, outperforms Qwen2.5-Omni with +19.05 on DailyOmni (cross-modal understanding), +1.7 on MMAR (audio), and +3.9 on Video-MME (vision), while using just 0.2T training tokens - a 6 times reduction compared to Qwen2.5-Omni's 1.2T. We finally demonstrate omni-modal advantages in downstream applications spanning robotics, medical AI, and smart factory.",
        "translated": "提升机器智能需要发展跨多种模态的感知能力，正如人类感知世界一样。我们提出了OmniVinci，一个旨在构建强大、开源、全模态大语言模型的项目。我们对模型架构和数据收集方面进行了细致的研究。在模型架构方面，我们提出了三项关键创新：(i) OmniAlignNet，用于在共享的全模态潜在空间中加强视觉和音频嵌入之间的对齐；(ii) Temporal Embedding Grouping，用于捕捉视觉和音频信号之间的相对时序对齐；(iii) Constrained Rotary Time Embedding，用于在全模态嵌入中编码绝对时序信息。我们引入了一个数据收集与合成流水线，生成了2400万条单模态和全模态的对话。我们发现，不同模态在感知和推理方面能够相互增强。我们的模型OmniVinci在使用仅0.2T训练数据量的情况下，在DailyOmni（跨模态理解）上优于Qwen2.5-Omni 19.05分，在MMAR（音频）上高1.7分，在Video-MME（视觉）上高3.9分，这比Qwen2.5-Omni所使用的1.2T训练数据量减少了6倍。最后，我们在下游应用中展示了全模态的优势，涵盖了机器人、医疗AI和智能工厂等多个领域。",
        "translated_title": "OmniVinci：增强架构与数据以实现全模态理解",
        "label": [],
        "label_reason": "论文聚焦多模态LLM，未涉及像素级图像处理任务",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出了多模态对齐和时间嵌入的改进方法"
    },
    {
        "title": "Skyfall-GS: Synthesizing Immersive 3D Urban Scenes from Satellite\n  Imagery",
        "url": "http://arxiv.org/abs/2510.15869v1",
        "pub_date": "2025-10-17",
        "summary": "Synthesizing large-scale, explorable, and geometrically accurate 3D urban scenes is a challenging yet valuable task in providing immersive and embodied applications. The challenges lie in the lack of large-scale and high-quality real-world 3D scans for training generalizable generative models. In this paper, we take an alternative route to create large-scale 3D scenes by synergizing the readily available satellite imagery that supplies realistic coarse geometry and the open-domain diffusion model for creating high-quality close-up appearances. We propose \\textbf{Skyfall-GS}, the first city-block scale 3D scene creation framework without costly 3D annotations, also featuring real-time, immersive 3D exploration. We tailor a curriculum-driven iterative refinement strategy to progressively enhance geometric completeness and photorealistic textures. Extensive experiments demonstrate that Skyfall-GS provides improved cross-view consistent geometry and more realistic textures compared to state-of-the-art approaches. Project page: https://skyfall-gs.jayinnn.dev/",
        "translated": "合成大规模、可探索且几何准确的3D城市场景是一项具有挑战性但又极具价值的任务，可为沉浸式和具身化应用提供支持。该任务的主要挑战在于缺乏用于训练通用生成模型的大规模高质量真实3D扫描数据。在本文中，我们采用了一种替代方法，通过结合现成的卫星图像（提供逼真的粗略几何结构）与开放域扩散模型（用于生成高质量的近距离外观），来创建大规模3D场景。我们提出了 \\textbf{Skyfall-GS}，这是首个无需高昂成本的3D标注即可实现城市街区尺度3D场景生成的框架，同时具备实时、沉浸式的3D探索功能。我们定制了一种基于课程学习的迭代优化策略，以逐步提升几何完整性与照片级真实纹理。大量实验表明，Skyfall-GS 相比于最先进的方法，在多视角几何一致性与纹理逼真度方面均有明显提升。项目主页：https://skyfall-gs.jayinnn.dev/",
        "translated_title": "Skyfall-GS：从卫星图像合成沉浸式三维城市场景",
        "label": [],
        "label_reason": "论文聚焦3D场景生成，非图像像素级恢复或增强任务",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出基于卫星图像和扩散模型的3D场景生成框架，有一定技术迁移性"
    },
    {
        "title": "LightsOut: Diffusion-based Outpainting for Enhanced Lens Flare Removal",
        "url": "http://arxiv.org/abs/2510.15868v1",
        "pub_date": "2025-10-17",
        "summary": "Lens flare significantly degrades image quality, impacting critical computer vision tasks like object detection and autonomous driving. Recent Single Image Flare Removal (SIFR) methods perform poorly when off-frame light sources are incomplete or absent. We propose LightsOut, a diffusion-based outpainting framework tailored to enhance SIFR by reconstructing off-frame light sources. Our method leverages a multitask regression module and LoRA fine-tuned diffusion model to ensure realistic and physically consistent outpainting results. Comprehensive experiments demonstrate LightsOut consistently boosts the performance of existing SIFR methods across challenging scenarios without additional retraining, serving as a universally applicable plug-and-play preprocessing solution. Project page: https://ray-1026.github.io/lightsout/",
        "translated": "镜头眩光会显著降低图像质量，影响物体检测和自动驾驶等关键的计算机视觉任务。近期的单图像眩光去除（Single Image Flare Removal, SIFR）方法在处理帧外光源不完整或缺失时效果较差。我们提出了 LightsOut，一种基于扩散模型的图像扩展框架，旨在通过重建帧外光源来增强 SIFR 的性能。我们的方法结合了一个多任务回归模块和经过 LoRA 微调的扩散模型，以确保生成结果在视觉真实性和物理一致性方面都具备高质量。大量实验表明，LightsOut 能够在各种具有挑战性的场景中，无需额外的再训练，显著提升现有 SIFR 方法的性能，成为一种通用的即插即用预处理解决方案。项目页面：https://ray-1026.github.io/lightsout/",
        "translated_title": "LightsOut：基于扩散的外推绘画用于增强的镜头眩光去除",
        "label": [
            "图像去反射",
            "图像恢复"
        ],
        "label_reason": "论文聚焦镜头光晕去除，涉及像素级图像恢复",
        "relevance_score": 8,
        "novelty_score": 7,
        "novelty_reason": "结合扩散模型与外延生成，提出新颖的光晕修复方案"
    },
    {
        "title": "BiomedXPro: Prompt Optimization for Explainable Diagnosis with\n  Biomedical Vision Language Models",
        "url": "http://arxiv.org/abs/2510.15866v1",
        "pub_date": "2025-10-17",
        "summary": "The clinical adoption of biomedical vision-language models is hindered by prompt optimization techniques that produce either uninterpretable latent vectors or single textual prompts. This lack of transparency and failure to capture the multi-faceted nature of clinical diagnosis, which relies on integrating diverse observations, limits their trustworthiness in high-stakes settings. To address this, we introduce BiomedXPro, an evolutionary framework that leverages a large language model as both a biomedical knowledge extractor and an adaptive optimizer to automatically generate a diverse ensemble of interpretable, natural-language prompt pairs for disease diagnosis. Experiments on multiple biomedical benchmarks show that BiomedXPro consistently outperforms state-of-the-art prompt-tuning methods, particularly in data-scarce few-shot settings. Furthermore, our analysis demonstrates a strong semantic alignment between the discovered prompts and statistically significant clinical features, grounding the model's performance in verifiable concepts. By producing a diverse ensemble of interpretable prompts, BiomedXPro provides a verifiable basis for model predictions, representing a critical step toward the development of more trustworthy and clinically-aligned AI systems.",
        "translated": "生物医学视觉-语言模型在临床中的应用受到提示优化技术的限制，这些技术生成的提示要么是不可解释的潜在向量，要么是单一的文本提示。这种缺乏透明性以及未能捕捉临床诊断的多方面特性——而临床诊断依赖于整合多种观察结果——限制了其在高风险场景中的可信度。为了解决这一问题，我们引入了 BiomedXPro，这是一种进化框架，利用大型语言模型作为生物医学知识提取器和自适应优化器，能够自动生成多样化的、可解释的自然语言提示对，用于疾病诊断。在多个生物医学基准数据集上的实验表明，BiomedXPro 在性能上始终优于最先进的提示调优方法，尤其是在数据稀缺的小样本设置中。此外，我们的分析表明，所发现的提示与统计上显著的临床特征之间存在强烈的语义一致性，从而将模型的性能建立在可验证的概念基础上。通过生成多样化且可解释的提示集合，BiomedXPro 为模型预测提供了可验证的依据，代表着开发更加可信且与临床对齐的 AI 系统的重要一步。",
        "translated_title": "BiomedXPro：用于可解释诊断的生物医学视觉语言模型提示优化",
        "label": [],
        "label_reason": "论文聚焦于生物医学视觉语言模型的提示优化，属于高阶诊断任务。",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "结合大语言模型进行提示优化，在提示工程上有一定创新。"
    },
    {
        "title": "BLIP3o-NEXT: Next Frontier of Native Image Generation",
        "url": "http://arxiv.org/abs/2510.15857v1",
        "pub_date": "2025-10-17",
        "summary": "We present BLIP3o-NEXT, a fully open-source foundation model in the BLIP3 series that advances the next frontier of native image generation. BLIP3o-NEXT unifies text-to-image generation and image editing within a single architecture, demonstrating strong image generation and image editing capabilities. In developing the state-of-the-art native image generation model, we identify four key insights: (1) Most architectural choices yield comparable performance; an architecture can be deemed effective provided it scales efficiently and supports fast inference; (2) The successful application of reinforcement learning can further push the frontier of native image generation; (3) Image editing still remains a challenging task, yet instruction following and the consistency between generated and reference images can be significantly enhanced through post-training and data engine; (4) Data quality and scale continue to be decisive factors that determine the upper bound of model performance. Building upon these insights, BLIP3o-NEXT leverages an Autoregressive + Diffusion architecture in which an autoregressive model first generates discrete image tokens conditioned on multimodal inputs, whose hidden states are then used as conditioning signals for a diffusion model to generate high-fidelity images. This architecture integrates the reasoning strength and instruction following of autoregressive models with the fine-detail rendering ability of diffusion models, achieving a new level of coherence and realism. Extensive evaluations of various text-to-image and image-editing benchmarks show that BLIP3o-NEXT achieves superior performance over existing models.",
        "translated": "我们提出了 BLIP3o-NEXT，这是 BLIP3 系列中一个完全开源的基础模型，推动了原生图像生成的下一个前沿方向。BLIP3o-NEXT 在单一架构中统一了文本到图像生成和图像编辑任务，展示了强大的图像生成和编辑能力。在开发先进的原生图像生成模型过程中，我们总结出四个关键见解：(1) 大多数架构选择在性能上是相近的；只要架构能够高效扩展并支持快速推理，就可以认为其是有效的；(2) 强化学习的成功应用可以进一步推动原生图像生成的前沿；(3) 图像编辑仍然是一个具有挑战性的任务，但通过后训练和数据引擎，指令跟随能力以及生成图像与参考图像之间的一致性可以显著提升；(4) 数据质量和数据规模仍然是决定模型性能上限的关键因素。基于这些见解，BLIP3o-NEXT 采用了一个 Autoregressive + Diffusion 架构：首先，一个自回归模型根据多模态输入生成离散的图像 token，然后将这些 token 的隐藏状态作为扩散模型的条件信号，以生成高质量图像。该架构结合了自回归模型的推理能力和指令跟随能力，以及扩散模型在细节渲染方面的优势，实现了生成图像在一致性和真实感方面的新水平。在多个文本到图像和图像编辑基准上的广泛评估表明，BLIP3o-NEXT 在现有模型中表现出优越的性能。",
        "translated_title": "BLIP3o-NEXT：原生图像生成的下一个前沿",
        "label": [],
        "label_reason": "论文关注图像生成而非像素级质量修复，不属于low-level图像处理",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出新的生成架构，结合自回归与扩散模型，有一定创新性"
    },
    {
        "title": "Memory-SAM: Human-Prompt-Free Tongue Segmentation via\n  Retrieval-to-Prompt",
        "url": "http://arxiv.org/abs/2510.15849v1",
        "pub_date": "2025-10-17",
        "summary": "Accurate tongue segmentation is crucial for reliable TCM analysis. Supervised models require large annotated datasets, while SAM-family models remain prompt-driven. We present Memory-SAM, a training-free, human-prompt-free pipeline that automatically generates effective prompts from a small memory of prior cases via dense DINOv3 features and FAISS retrieval. Given a query image, mask-constrained correspondences to the retrieved exemplar are distilled into foreground/background point prompts that guide SAM2 without manual clicks or model fine-tuning. We evaluate on 600 expert-annotated images (300 controlled, 300 in-the-wild). On the mixed test split, Memory-SAM achieves mIoU 0.9863, surpassing FCN (0.8188) and a detector-to-box SAM baseline (0.1839). On controlled data, ceiling effects above 0.98 make small differences less meaningful given annotation variability, while our method shows clear gains under real-world conditions. Results indicate that retrieval-to-prompt enables data-efficient, robust segmentation of irregular boundaries in tongue imaging. The code is publicly available at https://github.com/jw-chae/memory-sam.",
        "translated": "精确的舌象分割对于可靠的中医分析至关重要。监督模型需要大规模的标注数据集，而 SAM 系列模型仍然依赖提示驱动。我们提出 Memory-SAM，这是一种无需训练、无需人工提示的流程，它通过密集的 DINOv3 特征和 FAISS 检索，从少量先验案例构成的记忆中自动生成有效的提示。给定一张查询图像，将检索到的样例中受掩码约束的对应关系提炼为前景/背景点提示，以引导 SAM2 进行分割，而无需人工点击或模型微调。我们在 600 张专家标注的图像上进行评估（300 张受控图像，300 张真实场景图像）。在混合测试集上，Memory-SAM 实现了 0.9863 的 mIoU，优于 FCN（0.8188）和基于检测框的 SAM 基线（0.1839）。在受控数据中，由于标注的可变性，高于 0.98 的天花板效应使得微小差异变得不显著，而我们的方法在真实场景条件下表现出明显的性能提升。结果表明，检索到提示的机制能够实现舌象图像中不规则边界的数据高效且鲁棒的分割。代码公开在 https://github.com/jw-chae/memory-sam。",
        "translated_title": "Memory-SAM：基于检索到提示的无需人工提示的舌部分割",
        "label": [
            "图像分割",
            "医学图像增强"
        ],
        "label_reason": "方法用于医学图像分割，但非直接像素级质量复原",
        "relevance_score": 5,
        "novelty_score": 8,
        "novelty_reason": "提出无需人工提示的自动提示生成新方法"
    },
    {
        "title": "3DPR: Single Image 3D Portrait Relight using Generative Priors",
        "url": "http://arxiv.org/abs/2510.15846v1",
        "pub_date": "2025-10-17",
        "summary": "Rendering novel, relit views of a human head, given a monocular portrait image as input, is an inherently underconstrained problem. The traditional graphics solution is to explicitly decompose the input image into geometry, material and lighting via differentiable rendering; but this is constrained by the multiple assumptions and approximations of the underlying models and parameterizations of these scene components. We propose 3DPR, an image-based relighting model that leverages generative priors learnt from multi-view One-Light-at-A-Time (OLAT) images captured in a light stage. We introduce a new diverse and large-scale multi-view 4K OLAT dataset of 139 subjects to learn a high-quality prior over the distribution of high-frequency face reflectance. We leverage the latent space of a pre-trained generative head model that provides a rich prior over face geometry learnt from in-the-wild image datasets. The input portrait is first embedded in the latent manifold of such a model through an encoder-based inversion process. Then a novel triplane-based reflectance network trained on our lightstage data is used to synthesize high-fidelity OLAT images to enable image-based relighting. Our reflectance network operates in the latent space of the generative head model, crucially enabling a relatively small number of lightstage images to train the reflectance model. Combining the generated OLATs according to a given HDRI environment maps yields physically accurate environmental relighting results. Through quantitative and qualitative evaluations, we demonstrate that 3DPR outperforms previous methods, particularly in preserving identity and in capturing lighting effects such as specularities, self-shadows, and subsurface scattering. Project Page: https://vcai.mpi-inf.mpg.de/projects/3dpr/",
        "translated": "基于单目人像图像渲染新颖、重新光照的人头视图本身是一个固有欠约束的问题。传统的图形学解决方案是通过可微分渲染显式地将输入图像分解为几何、材质和光照；但该方法受限于基础模型和这些场景组件参数化过程中的多个假设和近似。我们提出 3DPR，一种基于图像的重新光照模型，利用从光场中捕获的多视角 One-Light-at-A-Time (OLAT) 图像学习的生成先验。我们引入一个新的多样化且大规模的多视角 4K OLAT 数据集，包含 139 个主体，以学习高频人脸反射分布的高质量先验。我们利用一个预训练的生成人头模型的潜在空间，该模型提供了从真实图像数据集中学习的丰富人脸几何先验。首先通过基于编码器的逆过程将输入人像嵌入到该模型的潜在流形中。然后利用一种基于三平面的反射网络（在我们采集的光场数据上训练），合成高保真的 OLAT 图像，以实现基于图像的重新光照。我们的反射网络在生成人头模型的潜在空间中进行操作，关键在于能够利用相对较少的光场图像训练反射模型。根据给定的 HDRI 环境图结合生成的 OLAT 图像，可以实现物理准确的环境光照效果。通过定量和定性评估，我们证明了 3DPR 在保持身份信息和捕捉诸如高光、自阴影和次表面散射等光照效果方面优于先前的方法。项目页面：https://vcai.mpi-inf.mpg.de/projects/3dpr/",
        "translated_title": "3DPR：使用生成先验的单图像3D人像重光照",
        "label": [],
        "label_reason": "论文属于3D渲染和生成任务，非图像像素级质量恢复或增强。",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出基于生成先验的3D人脸重光照方法，但属于常规生成模型的应用扩展。"
    },
    {
        "title": "Paper2Web: Let's Make Your Paper Alive!",
        "url": "http://arxiv.org/abs/2510.15842v1",
        "pub_date": "2025-10-17",
        "summary": "Academic project websites can more effectively disseminate research when they clearly present core content and enable intuitive navigation and interaction. However, current approaches such as direct Large Language Model (LLM) generation, templates, or direct HTML conversion struggle to produce layout-aware, interactive sites, and a comprehensive evaluation suite for this task has been lacking. In this paper, we introduce Paper2Web, a benchmark dataset and multi-dimensional evaluation framework for assessing academic webpage generation. It incorporates rule-based metrics like Connectivity, Completeness and human-verified LLM-as-a-Judge (covering interactivity, aesthetics, and informativeness), and PaperQuiz, which measures paper-level knowledge retention. We further present PWAgent, an autonomous pipeline that converts scientific papers into interactive and multimedia-rich academic homepages. The agent iteratively refines both content and layout through MCP tools that enhance emphasis, balance, and presentation quality. Our experiments show that PWAgent consistently outperforms end-to-end baselines like template-based webpages and arXiv/alphaXiv versions by a large margin while maintaining low cost, achieving the Pareto-front in academic webpage generation.",
        "translated": "学术项目网站在清晰呈现核心内容并实现直观导航与交互时，能够更有效地传播研究成果。然而，当前的方法如直接使用大型语言模型（LLM）生成、模板化或直接HTML转换，难以生成具有布局感知能力且交互性强的网站，且在此任务上缺乏全面的评估体系。本文中，我们提出了Paper2Web，这是一个用于评估学术网页生成的基准数据集和多维度评估框架。该框架集成了基于规则的指标，如连通性（Connectivity）、完整性（Completeness），以及由人类验证的LLM作为评判者（LLM-as-a-Judge），涵盖交互性、美观性和信息性等方面，同时还引入了PaperQuiz，用于衡量论文级别的知识保留程度。我们进一步提出了PWAgent，一个自主的流水线工具，能够将科研论文转换为交互性强、多媒体丰富的学术主页。该智能体通过MCP工具迭代优化内容与布局，提升重点突出性、视觉平衡性和展示质量。实验结果表明，PWAgent在保持低成本的同时，显著优于基于模板的网页和arXiv/alphaXiv版本等端到端基线方法，在学术网页生成任务中达到了帕累托前沿。",
        "translated_title": "Paper2Web: 让你的论文动起来！",
        "label": [],
        "label_reason": "论文不属于low-level图像处理，主要关注网页生成。",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "提出新框架和工具，但创新点集中于网页交互设计而非图像处理。"
    },
    {
        "title": "Neuro-Symbolic Spatial Reasoning in Segmentation",
        "url": "http://arxiv.org/abs/2510.15841v1",
        "pub_date": "2025-10-17",
        "summary": "Open-Vocabulary Semantic Segmentation (OVSS) assigns pixel-level labels from an open set of categories, requiring generalization to unseen and unlabelled objects. Using vision-language models (VLMs) to correlate local image patches with potential unseen object categories suffers from a lack of understanding of spatial relations of objects in a scene. To solve this problem, we introduce neuro-symbolic (NeSy) spatial reasoning in OVSS. In contrast to contemporary VLM correlation-based approaches, we propose Relational Segmentor (RelateSeg) to impose explicit spatial relational constraints by first order logic (FOL) formulated in a neural network architecture. This is the first attempt to explore NeSy spatial reasoning in OVSS. Specifically, RelateSeg automatically extracts spatial relations, e.g., &lt;cat, to-right-of, person&gt;, and encodes them as first-order logic formulas using our proposed pseudo categories. Each pixel learns to predict both a semantic category (e.g., \"cat\") and a spatial pseudo category (e.g., \"right of person\") simultaneously, enforcing relational constraints (e.g., a \"cat\" pixel must lie to the right of a \"person\"). Finally, these logic constraints are formulated in a deep network architecture by fuzzy logic relaxation, enabling end-to-end learning of spatial-relationally consistent segmentation. RelateSeg achieves state-of-the-art performance in terms of average mIoU across four benchmark datasets and particularly shows clear advantages on images containing multiple categories, with the cost of only introducing a single auxiliary loss function and no additional parameters, validating the effectiveness of NeSy spatial reasoning in OVSS.",
        "translated": "开放词汇语义分割（OVSS）从开放的类别集中为每个像素分配标签，要求对未见过且未标记的对象具备泛化能力。使用视觉-语言模型（VLMs）将局部图像块与潜在的未知对象类别相关联，往往难以理解场景中对象的空间关系。为了解决这一问题，我们在OVSS中引入了神经符号（NeSy）空间推理。不同于当前基于VLM相关性的方法，我们提出Relational Segmentor（RelateSeg），通过在神经网络架构中以一阶逻辑（FOL）的形式施加显式空间关系约束来实现推理。这是首次尝试在OVSS中探索NeSy空间推理。具体而言，RelateSeg会自动提取空间关系，例如 $<cat, to-right-of, person>$，并使用我们提出的伪类别将其编码为一阶逻辑公式。每个像素同时学习预测一个语义类别（如\"cat\"）和一个空间伪类别（如\"right of person\"），从而强制实施关系约束（如\"cat\"像素必须位于\"person\"像素的右侧）。最终，这些逻辑约束通过模糊逻辑松弛技术整合到深度网络架构中，实现空间关系一致的端到端分割学习。RelateSeg在四个基准数据集上的平均mIoU指标上达到了最先进的性能，尤其在包含多个类别的图像上表现出明显优势，仅引入了一个辅助损失函数且无需额外参数，验证了NeSy空间推理在OVSS中的有效性。",
        "translated_title": "神经符号化空间推理在分割中的应用",
        "label": [],
        "label_reason": "论文聚焦语义分割，属于high-level任务",
        "relevance_score": 3,
        "novelty_score": 7,
        "novelty_reason": "首次将神经符号推理引入分割任务，具有新颖性"
    },
    {
        "title": "VISTA: A Test-Time Self-Improving Video Generation Agent",
        "url": "http://arxiv.org/abs/2510.15831v1",
        "pub_date": "2025-10-17",
        "summary": "Despite rapid advances in text-to-video synthesis, generated video quality remains critically dependent on precise user prompts. Existing test-time optimization methods, successful in other domains, struggle with the multi-faceted nature of video. In this work, we introduce VISTA (Video Iterative Self-improvemenT Agent), a novel multi-agent system that autonomously improves video generation through refining prompts in an iterative loop. VISTA first decomposes a user idea into a structured temporal plan. After generation, the best video is identified through a robust pairwise tournament. This winning video is then critiqued by a trio of specialized agents focusing on visual, audio, and contextual fidelity. Finally, a reasoning agent synthesizes this feedback to introspectively rewrite and enhance the prompt for the next generation cycle. Experiments on single- and multi-scene video generation scenarios show that while prior methods yield inconsistent gains, VISTA consistently improves video quality and alignment with user intent, achieving up to 60% pairwise win rate against state-of-the-art baselines. Human evaluators concur, preferring VISTA outputs in 66.4% of comparisons.",
        "translated": "尽管文本到视频合成领域取得了快速进展，但生成的视频质量在很大程度上仍依赖于用户提供的精确提示。现有在测试时进行优化的方法虽然在其他领域取得了成功，但在处理视频的多维度特性时却面临挑战。在本研究中，我们提出 VISTA（Video Iterative Self-improvemenT Agent），一种新颖的多智能体系统，通过迭代循环优化提示，自主提升视频生成质量。VISTA 首先将用户的创意想法分解为结构化的时序计划。在生成视频之后，通过一个鲁棒的两两比赛机制确定最佳视频。随后，一个由三个专业智能体组成的小组对该胜出视频进行评估，分别关注视觉、音频和上下文的保真度。最后，一个推理智能体综合这些反馈，自主地重写并增强下一轮生成所用的提示。在单场景和多场景视频生成任务中的实验表明，尽管现有方法的性能提升并不一致，VISTA 则始终能够提升视频质量和与用户意图的对齐程度，在与最先进基线方法的两两比较中，最高可达 60% 的胜率。人类评估者也一致认可 VISTA 的输出，在 66.4% 的比较中更倾向于选择其结果。",
        "translated_title": "VISTA：一种运行时自我改进的视频生成智能体",
        "label": [],
        "label_reason": "论文聚焦视频生成而非像素级图像恢复或增强",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出了多智能体系统进行迭代视频优化，具有一定创新性"
    },
    {
        "title": "ERNet: Efficient Non-Rigid Registration Network for Point Sequences",
        "url": "http://arxiv.org/abs/2510.15800v1",
        "pub_date": "2025-10-17",
        "summary": "Registering an object shape to a sequence of point clouds undergoing non-rigid deformation is a long-standing challenge. The key difficulties stem from two factors: (i) the presence of local minima due to the non-convexity of registration objectives, especially under noisy or partial inputs, which hinders accurate and robust deformation estimation, and (ii) error accumulation over long sequences, leading to tracking failures. To address these challenges, we introduce to adopt a scalable data-driven approach and propose ERNet, an efficient feed-forward model trained on large deformation datasets. It is designed to handle noisy and partial inputs while effectively leveraging temporal information for accurate and consistent sequential registration. The key to our design is predicting a sequence of deformation graphs through a two-stage pipeline, which first estimates frame-wise coarse graph nodes for robust initialization, before refining their trajectories over time in a sliding-window fashion. Extensive experiments show that our proposed approach (i) outperforms previous state-of-the-art on both the DeformingThings4D and D-FAUST datasets, and (ii) achieves more than 4x speedup compared to the previous best, offering significant efficiency improvement.",
        "translated": "将一个物体形状配准到经历非刚性变形的一系列点云中是一个长期存在的挑战。其关键难点来源于两个因素：(i) 由于配准目标函数的非凸性，特别是在噪声或部分输入的情况下，容易出现局部极小值，这阻碍了准确且鲁棒的变形估计；以及 (ii) 在长序列中误差不断累积，导致跟踪失败。为了解决这些挑战，我们提出采用一种可扩展的数据驱动方法，并设计了 ERNet，一个在大变形数据集上训练的高效前馈模型。该模型旨在处理噪声和部分输入的同时，有效利用时间信息，实现准确且一致的序列配准。我们设计的关键在于通过一个两阶段的流程预测一系列变形图：首先估计每一帧的粗略图节点以实现鲁棒的初始化，然后在滑动窗口的框架下逐时地优化这些节点的轨迹。大量实验表明，我们提出的方法 (i) 在 DeformingThings4D 和 D-FAUST 数据集上均优于之前的最先进方法，以及 (ii) 相比之前最佳方法的效率提升了超过 4 倍，具有显著的速度优势。",
        "translated_title": "ERNet：用于点序列的高效非刚性配准网络",
        "label": [],
        "label_reason": "论文研究点云配准，不属于图像处理任务。",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "提出两阶段变形图预测方法，但为常规技术组合。"
    },
    {
        "title": "ReCon: Region-Controllable Data Augmentation with Rectification and\n  Alignment for Object Detection",
        "url": "http://arxiv.org/abs/2510.15783v1",
        "pub_date": "2025-10-17",
        "summary": "The scale and quality of datasets are crucial for training robust perception models. However, obtaining large-scale annotated data is both costly and time-consuming. Generative models have emerged as a powerful tool for data augmentation by synthesizing samples that adhere to desired distributions. However, current generative approaches often rely on complex post-processing or extensive fine-tuning on massive datasets to achieve satisfactory results, and they remain prone to content-position mismatches and semantic leakage. To overcome these limitations, we introduce ReCon, a novel augmentation framework that enhances the capacity of structure-controllable generative models for object detection. ReCon integrates region-guided rectification into the diffusion sampling process, using feedback from a pre-trained perception model to rectify misgenerated regions within diffusion sampling process. We further propose region-aligned cross-attention to enforce spatial-semantic alignment between image regions and their textual cues, thereby improving both semantic consistency and overall image fidelity. Extensive experiments demonstrate that ReCon substantially improve the quality and trainability of generated data, achieving consistent performance gains across various datasets, backbone architectures, and data scales. Our code is available at https://github.com/haoweiz23/ReCon .",
        "translated": "数据集的规模和质量对于训练鲁棒的感知模型至关重要。然而，获取大规模标注数据既昂贵又耗时。生成模型通过合成符合目标分布的样本，已成为数据增强的强大工具。然而，当前的生成方法通常依赖于复杂的后处理或对大规模数据集进行大量微调才能获得令人满意的结果，并且仍然容易出现内容-位置不匹配和语义泄露的问题。为克服这些限制，我们引入了 ReCon，一种新颖的增强框架，旨在提升结构可控生成模型在目标检测中的能力。ReCon 将区域引导的修正机制整合到扩散采样过程中，利用预训练感知模型的反馈来修正扩散采样过程中的错误生成区域。我们进一步提出了区域对齐的交叉注意力机制，以强制图像区域与文本提示之间在空间语义上的一致性，从而提升语义一致性以及整体图像保真度。大量实验表明，ReCon 显著提高了生成数据的质量和可训练性，在不同数据集、主干网络结构和数据规模上均实现了性能的稳定提升。我们的代码可在 https://github.com/haoweiz23/ReCon 获取。",
        "translated_title": "ReCon：带校正与对齐的区域可控数据增强方法用于目标检测",
        "label": [],
        "label_reason": "论文聚焦于数据增强，用于目标检测，属于 high-level 任务。",
        "relevance_score": 3,
        "novelty_score": 7,
        "novelty_reason": "提出区域对齐交叉注意力和区域引导修正，有一定创新性。"
    },
    {
        "title": "Controlling the image generation process with parametric activation\n  functions",
        "url": "http://arxiv.org/abs/2510.15778v1",
        "pub_date": "2025-10-17",
        "summary": "As image generative models continue to increase not only in their fidelity but also in their ubiquity the development of tools that leverage direct interaction with their internal mechanisms in an interpretable way has received little attention In this work we introduce a system that allows users to develop a better understanding of the model through interaction and experimentation By giving users the ability to replace activation functions of a generative network with parametric ones and a way to set the parameters of these functions we introduce an alternative approach to control the networks output We demonstrate the use of our method on StyleGAN2 and BigGAN networks trained on FFHQ and ImageNet respectively.",
        "translated": "随着图像生成模型在真实感和普及性方面持续提升，能够以可解释的方式利用与模型内部机制直接交互的工具的发展却未受到足够关注。在本工作中，我们引入了一个系统，使用户通过交互和实验更好地理解模型。通过赋予用户将生成网络的激活函数替换为参数化激活函数的能力，并提供设置这些函数参数的方法，我们提出了一种控制网络输出的替代方案。我们分别在基于 FFHQ 和 ImageNet 训练的 StyleGAN2 与 BigGAN 网络上展示了该方法的应用。",
        "translated_title": "通过参数化激活函数控制图像生成过程",
        "label": [],
        "label_reason": "论文聚焦图像生成而非恢复/增强，不属于低级图像处理",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出可参数化激活函数交互方法，有一定技术迁移潜力"
    },
    {
        "title": "SANR: Scene-Aware Neural Representation for Light Field Image\n  Compression with Rate-Distortion Optimization",
        "url": "http://arxiv.org/abs/2510.15775v1",
        "pub_date": "2025-10-17",
        "summary": "Light field images capture multi-view scene information and play a crucial role in 3D scene reconstruction. However, their high-dimensional nature results in enormous data volumes, posing a significant challenge for efficient compression in practical storage and transmission scenarios. Although neural representation-based methods have shown promise in light field image compression, most approaches rely on direct coordinate-to-pixel mapping through implicit neural representation (INR), often neglecting the explicit modeling of scene structure. Moreover, they typically lack end-to-end rate-distortion optimization, limiting their compression efficiency. To address these limitations, we propose SANR, a Scene-Aware Neural Representation framework for light field image compression with end-to-end rate-distortion optimization. For scene awareness, SANR introduces a hierarchical scene modeling block that leverages multi-scale latent codes to capture intrinsic scene structures, thereby reducing the information gap between INR input coordinates and the target light field image. From a compression perspective, SANR is the first to incorporate entropy-constrained quantization-aware training (QAT) into neural representation-based light field image compression, enabling end-to-end rate-distortion optimization. Extensive experiment results demonstrate that SANR significantly outperforms state-of-the-art techniques regarding rate-distortion performance with a 65.62\\% BD-rate saving against HEVC.",
        "translated": "光场图像捕获多视角场景信息，在三维场景重建中起着至关重要的作用。然而，其高维特性导致数据量巨大，在实际存储和传输场景中对高效压缩提出了重大挑战。尽管基于神经表示的方法在光场图像压缩中展现出前景，但大多数方法依赖于通过隐式神经表示（INR）进行的直接坐标到像素的映射，常常忽略了对场景结构的显式建模。此外，它们通常缺乏端到端的率失真优化，限制了压缩效率。为了解决这些限制，我们提出SANR，一种用于光场图像压缩的场景感知神经表示框架，具备端到端的率失真优化。为了实现场景感知，SANR引入了一个层次化的场景建模模块，利用多尺度潜在码捕获场景的内在结构，从而减少INR输入坐标与目标光场图像之间的信息差距。从压缩的角度来看，SANR是首个将熵约束的量化感知训练（QAT）引入基于神经表示的光场图像压缩的方法，实现了端到端的率失真优化。大量实验结果表明，SANR在率失真性能方面显著优于最先进的技术，相比HEVC在BD-rate上节省了65.62\\%。",
        "translated_title": "SANR：基于率失真优化的场景感知神经表示用于光场图像压缩",
        "label": [
            "多帧/视频图像恢复"
        ],
        "label_reason": "论文涉及光场图像压缩，属于多帧图像处理，与low-level相关。",
        "relevance_score": 8,
        "novelty_score": 7,
        "novelty_reason": "提出结合熵约束量化感知训练的新方法，改进压缩效率。"
    },
    {
        "title": "Towards more holistic interpretability: A lightweight disentangled\n  Concept Bottleneck Model",
        "url": "http://arxiv.org/abs/2510.15770v1",
        "pub_date": "2025-10-17",
        "summary": "Concept Bottleneck Models (CBMs) enhance interpretability by predicting human-understandable concepts as intermediate representations. However, existing CBMs often suffer from input-to-concept mapping bias and limited controllability, which restricts their practical value, directly damage the responsibility of strategy from concept-based methods. We propose a lightweight Disentangled Concept Bottleneck Model (LDCBM) that automatically groups visual features into semantically meaningful components without region annotation. By introducing a filter grouping loss and joint concept supervision, our method improves the alignment between visual patterns and concepts, enabling more transparent and robust decision-making. Notably, Experiments on three diverse datasets demonstrate that LDCBM achieves higher concept and class accuracy, outperforming previous CBMs in both interpretability and classification performance. By grounding concepts in visual evidence, our method overcomes a fundamental limitation of prior models and enhances the reliability of interpretable AI.",
        "translated": "概念瓶颈模型（CBMs）通过预测人类可理解的概念作为中间表示来增强模型的可解释性。然而，现有的 CBMs 往往存在输入到概念映射的偏差以及可控性有限的问题，这限制了它们的实用价值，并直接损害了基于概念方法策略的责任性。我们提出了一种轻量级的解耦概念瓶颈模型（LDCBM），该模型无需区域标注即可自动将视觉特征分组为语义上有意义的组件。通过引入滤波器分组损失和联合概念监督，我们的方法提升了视觉模式与概念之间的对齐程度，从而实现更加透明和稳健的决策。值得注意的是，三个多样化数据集上的实验表明，LDCBM 在概念和类别准确率方面均高于现有方法，在可解释性与分类性能上均优于以往的 CBMs。通过将概念与视觉证据相联系，我们的方法克服了先前模型的基本局限，并提高了可解释人工智能的可靠性。",
        "translated_title": "迈向更全面的可解释性：一种轻量级的解耦概念瓶颈模型",
        "label": [],
        "label_reason": "论文聚焦模型可解释性，非像素级图像处理任务。",
        "relevance_score": 3,
        "novelty_score": 7,
        "novelty_reason": "提出轻量解耦概念瓶颈模型，改进了CBM的可控性和概念对齐。"
    },
    {
        "title": "QSilk: Micrograin Stabilization and Adaptive Quantile Clipping for\n  Detail-Friendly Latent Diffusion",
        "url": "http://arxiv.org/abs/2510.15761v1",
        "pub_date": "2025-10-17",
        "summary": "We present QSilk, a lightweight, always-on stabilization layer for latent diffusion that improves high-frequency fidelity while suppressing rare activation spikes. QSilk combines (i) a per-sample micro clamp that gently limits extreme values without washing out texture, and (ii) Adaptive Quantile Clip (AQClip), which adapts the allowed value corridor per region. AQClip can operate in a proxy mode using local structure statistics or in an attention entropy guided mode (model confidence). Integrated into the CADE 2.5 rendering pipeline, QSilk yields cleaner, sharper results at low step counts and ultra-high resolutions with negligible overhead. It requires no training or fine-tuning and exposes minimal user controls. We report consistent qualitative improvements across SD/SDXL backbones and show synergy with CFG/Rescale, enabling slightly higher guidance without artifacts.",
        "translated": "我们提出了 QSilk，一种用于潜在扩散的轻量级、始终启用的稳定层，它在提高高频保真度的同时抑制罕见的激活尖峰。QSilk 结合了以下两个部分：(i) 每个样本的微小钳制机制，该机制温和地限制极端值而不模糊纹理；(ii) 自适应分位数裁剪（AQClip），它根据每个区域调整允许的数值通道。AQClip 可以通过局部结构统计信息运行在代理模式下，也可以通过注意力熵引导模式（模型置信度）运行。将 QSilk 集成到 CADE 2.5 渲染流程中后，即使在较低的采样步数和超高分辨率下，也能获得更干净、更清晰的结果，且几乎没有额外开销。QSilk 不需要训练或微调，并且用户可调节的参数极少。我们在 SD/SDXL 的主干模型上均观察到一致的定性提升，并展示了其与 CFG/Rescale 的协同效应，使得在不产生伪影的情况下可略微提高引导强度。",
        "translated_title": "QSilk：面向细节的潜在扩散的微晶粒稳定与自适应分位数裁剪",
        "label": [],
        "label_reason": "不直接处理像素级图像质量，属于生成模型的优化",
        "relevance_score": 3,
        "novelty_score": 6,
        "novelty_reason": "提出轻量稳定层和自适应裁剪方法，有一定创新"
    },
    {
        "title": "Poultry Farm Intelligence: An Integrated Multi-Sensor AI Platform for\n  Enhanced Welfare and Productivity",
        "url": "http://arxiv.org/abs/2510.15757v1",
        "pub_date": "2025-10-17",
        "summary": "Poultry farming faces increasing pressure to meet productivity targets while ensuring animal welfare and environmental compliance. Yet many small and medium-sized farms lack affordable, integrated tools for continuous monitoring and decision-making, relying instead on manual, reactive inspections. This paper presents Poultry Farm Intelligence (PoultryFI) - a modular, cost-effective platform that integrates six AI-powered modules: Camera Placement Optimizer, Audio-Visual Monitoring, Analytics &amp; Alerting, Real-Time Egg Counting, Production &amp; Profitability Forecasting, and a Recommendation Module.   Camera layouts are first optimized offline using evolutionary algorithms for full poultry house coverage with minimal hardware. The Audio-Visual Monitoring module extracts welfare indicators from synchronized video, audio, and feeding data. Analytics &amp; Alerting produces daily summaries and real-time notifications, while Real-Time Egg Counting uses an edge vision model to automate production tracking. Forecasting models predict egg yield and feed consumption up to 10 days in advance, and the Recommendation Module integrates forecasts with weather data to guide environmental and operational adjustments.   This is among the first systems to combine low-cost sensing, edge analytics, and prescriptive AI to continuously monitor flocks, predict production, and optimize performance. Field trials demonstrate 100% egg-count accuracy on Raspberry Pi 5, robust anomaly detection, and reliable short-term forecasting. PoultryFI bridges the gap between isolated pilot tools and scalable, farm-wide intelligence, empowering producers to proactively safeguard welfare and profitability.",
        "translated": "禽类养殖面临日益增长的压力，需要在提高生产效率的同时保障动物福利并满足环境合规要求。然而，许多中小型农场缺乏经济、集成化的工具来进行连续监控和决策，通常依赖人工的、被动的检查方式。本文提出了 Poultry Farm Intelligence（PoultryFI）——一个模块化、低成本的平台，集成了六个基于人工智能的模块：摄像头布局优化器、音视频监控、分析与预警、实时蛋数统计、生产与盈利能力预测以及推荐模块。\n\n首先使用进化算法在离线环境下对摄像头布局进行优化，以在使用最少硬件设备的情况下实现禽舍的全面覆盖。音视频监控模块从同步的视频、音频和喂食数据中提取福利指标。分析与预警模块生成每日摘要并发出实时通知，而实时蛋数统计模块则采用边缘视觉模型实现生产情况的自动化跟踪。预测模型可提前10天预测产蛋量和饲料消耗，推荐模块则将预测结果与天气数据整合，用于指导环境和操作的调整。\n\n该系统是首批结合低成本传感、边缘分析和处方式人工智能的系统之一，能够持续监控禽群、预测生产情况并优化整体性能。实地试验表明，在 Raspberry Pi 5 上实现蛋数统计的准确率达到100%，异常检测稳健，短期预测可靠。PoultryFI 搭起了孤立试点工具与可扩展、全场智能化之间的桥梁，使养殖者能够主动保障动物福利和盈利能力。",
        "translated_title": "禽类养殖智能化：一种集成多传感器的AI平台，提升福利与生产效率",
        "label": [],
        "label_reason": "论文主要关注多传感器AI平台在养鸡场的应用，不属于图像恢复或增强任务。",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出了模块化低成本AI平台，但属于常规系统集成创新。"
    },
    {
        "title": "Semantic segmentation with coarse annotations",
        "url": "http://arxiv.org/abs/2510.15756v1",
        "pub_date": "2025-10-17",
        "summary": "Semantic segmentation is the task of classifying each pixel in an image. Training a segmentation model achieves best results using annotated images, where each pixel is annotated with the corresponding class. When obtaining fine annotations is difficult or expensive, it may be possible to acquire coarse annotations, e.g. by roughly annotating pixels in an images leaving some pixels around the boundaries between classes unlabeled. Segmentation with coarse annotations is difficult, in particular when the objective is to optimize the alignment of boundaries between classes. This paper proposes a regularization method for models with an encoder-decoder architecture with superpixel based upsampling. It encourages the segmented pixels in the decoded image to be SLIC-superpixels, which are based on pixel color and position, independent of the segmentation annotation. The method is applied to FCN-16 fully convolutional network architecture and evaluated on the SUIM, Cityscapes, and PanNuke data sets. It is shown that the boundary recall improves significantly compared to state-of-the-art models when trained on coarse annotations.",
        "translated": "语义分割的任务是对图像中的每个像素进行分类。使用带有标注的图像训练分割模型可以取得最佳效果，其中每个像素都标注了对应的类别。当获取精细标注困难或成本较高时，可以尝试获取粗略标注，例如通过大致标注图像中的像素，而将类别边界附近的一些像素留作未标注。使用粗略标注进行分割是具有挑战性的，特别是当目标是优化类别之间边界的对齐时。本文提出了一种针对具有编码器-解码器架构、并采用超像素上采样的模型的正则化方法。该方法促使解码图像中的分割像素属于基于像素颜色和位置的SLIC超像素，且不依赖于分割标注。该方法被应用于FCN-16全卷积网络架构，并在SUIM、Cityscapes和PanNuke数据集上进行了评估。结果表明，与最先进的模型相比，在使用粗标注训练时，该方法能够显著提高边界召回率。",
        "translated_title": "带有粗略标注的语义分割",
        "label": [],
        "label_reason": "论文属于语义分割，目标是场景理解，非像素级图像质量恢复",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出基于超像素的正则化方法，对模型训练有一定改进"
    },
    {
        "title": "NDM: A Noise-driven Detection and Mitigation Framework against Implicit\n  Sexual Intentions in Text-to-Image Generation",
        "url": "http://arxiv.org/abs/2510.15752v1",
        "pub_date": "2025-10-17",
        "summary": "Despite the impressive generative capabilities of text-to-image (T2I) diffusion models, they remain vulnerable to generating inappropriate content, especially when confronted with implicit sexual prompts. Unlike explicit harmful prompts, these subtle cues, often disguised as seemingly benign terms, can unexpectedly trigger sexual content due to underlying model biases, raising significant ethical concerns. However, existing detection methods are primarily designed to identify explicit sexual content and therefore struggle to detect these implicit cues. Fine-tuning approaches, while effective to some extent, risk degrading the model's generative quality, creating an undesirable trade-off. To address this, we propose NDM, the first noise-driven detection and mitigation framework, which could detect and mitigate implicit malicious intention in T2I generation while preserving the model's original generative capabilities. Specifically, we introduce two key innovations: first, we leverage the separability of early-stage predicted noise to develop a noise-based detection method that could identify malicious content with high accuracy and efficiency; second, we propose a noise-enhanced adaptive negative guidance mechanism that could optimize the initial noise by suppressing the prominent region's attention, thereby enhancing the effectiveness of adaptive negative guidance for sexual mitigation. Experimentally, we validate NDM on both natural and adversarial datasets, demonstrating its superior performance over existing SOTA methods, including SLD, UCE, and RECE, etc. Code and resources are available at https://github.com/lorraine021/NDM.",
        "translated": "尽管文本到图像（T2I）扩散模型展现出令人印象深刻的内容生成能力，它们仍易受到生成不适当内容的影响，尤其是在面对隐含的性暗示提示时。与显式的有害提示不同，这些微妙的提示通常伪装成看似无害的词汇，由于模型内部存在的偏见，可能意外触发与性相关的内容，从而引发重大的伦理问题。然而，现有的检测方法主要设计用于识别显式的色情内容，因此难以检测这些隐含提示。尽管微调方法在某种程度上是有效的，但它们可能会降低模型的生成质量，造成令人不满意的性能与质量之间的权衡。为了解决这一问题，我们提出了NDM，第一个基于噪声驱动的内容检测和缓解框架，可以在保持模型原有生成能力的同时，检测并缓解T2I生成过程中的隐性恶意意图。具体而言，我们引入了两个关键创新：首先，我们利用早期预测噪声的可分性，开发了一种基于噪声的检测方法，能够以高精度和高效率识别恶意内容；其次，我们提出了一种噪声增强的自适应负向引导机制，该机制通过抑制显著区域的注意力来优化初始噪声，从而提高自适应负向引导在缓解性内容方面的有效性。实验上，我们在自然和对抗数据集上验证了NDM，结果表明其性能优于现有最先进的方法，包括SLD、UCE和RECE等。代码和资源可在 https://github.com/lorraine021/NDM 获取。",
        "translated_title": "NDM：一种基于噪声驱动的检测与缓解框架，用于应对文本到图像生成中的隐式性意图",
        "label": [],
        "label_reason": "论文聚焦文本到图像生成中的内容检测，不属于图像像素级处理任务",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出了首个基于噪声驱动的检测与缓解框架，具有一定创新性"
    },
    {
        "title": "SEGA: A Stepwise Evolution Paradigm for Content-Aware Layout Generation\n  with Design Prior",
        "url": "http://arxiv.org/abs/2510.15749v1",
        "pub_date": "2025-10-17",
        "summary": "In this paper, we study the content-aware layout generation problem, which aims to automatically generate layouts that are harmonious with a given background image. Existing methods usually deal with this task with a single-step reasoning framework. The lack of a feedback-based self-correction mechanism leads to their failure rates significantly increasing when faced with complex element layout planning. To address this challenge, we introduce SEGA, a novel Stepwise Evolution Paradigm for Content-Aware Layout Generation. Inspired by the systematic mode of human thinking, SEGA employs a hierarchical reasoning framework with a coarse-to-fine strategy: first, a coarse-level module roughly estimates the layout planning results; then, another refining module performs fine-level reasoning regarding the coarse planning results. Furthermore, we incorporate layout design principles as prior knowledge into the model to enhance its layout planning ability. Besides, we present GenPoster-100K that is a new large-scale poster dataset with rich meta-information annotation. The experiments demonstrate the effectiveness of our approach by achieving the state-of-the-art results on multiple benchmark datasets. Our project page is at: https://brucew91.github.io/SEGA.github.io/",
        "translated": "本文研究了内容感知的布局生成问题，旨在自动生成与给定背景图像和谐一致的布局。现有方法通常采用单步推理框架处理该任务。由于缺乏基于反馈的自修正机制，当面对复杂的元素布局规划时，其失败率显著上升。为了解决这一挑战，我们引入了 SEGA，一种新颖的内容感知布局生成的逐步演化范式。受人类系统性思维模式的启发，SEGA 采用分层推理框架和从粗到细的策略：首先，一个粗粒度模块大致估计布局规划结果；然后，另一个优化模块对粗粒度规划结果进行细粒度推理。此外，我们将布局设计原则作为先验知识引入模型，以提升其布局规划能力。同时，我们还提出了 GenPoster-100K，这是一个新的大规模海报数据集，包含丰富的元信息标注。实验表明，我们的方法在多个基准数据集上取得了最先进的结果，验证了其有效性。我们的项目页面为：https://brucew91.github.io/SEGA.github.io/",
        "translated_title": "SEGA：一种基于设计先验的内容感知布局生成的逐步演化范式",
        "label": [],
        "label_reason": "论文研究布局生成，属于high-level任务",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出分步演化范式SEGA，改进布局生成方法"
    }
]
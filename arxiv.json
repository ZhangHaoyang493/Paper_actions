[
    {
        "title": "FinVet: A Collaborative Framework of RAG and External Fact-Checking\n  Agents for Financial Misinformation Detection",
        "url": "http://arxiv.org/abs/2510.11654v1",
        "pub_date": "2025-10-13",
        "summary": "Financial markets face growing threats from misinformation that can trigger billions in losses in minutes. Most existing approaches lack transparency in their decision-making and provide limited attribution to credible sources. We introduce FinVet, a novel multi-agent framework that integrates two Retrieval-Augmented Generation (RAG) pipelines with external fact-checking through a confidence-weighted voting mechanism. FinVet employs adaptive three-tier processing that dynamically adjusts verification strategies based on retrieval confidence, from direct metadata extraction to hybrid reasoning to full model-based analysis. Unlike existing methods, FinVet provides evidence-backed verdicts, source attribution, confidence scores, and explicit uncertainty flags when evidence is insufficient. Experimental evaluation on the FinFact dataset shows that FinVet achieves an F1 score of 0.85, which is a 10.4% improvement over the best individual pipeline (fact-check pipeline) and 37% improvement over standalone RAG approaches.",
        "translated": "金融市场正面临日益严重的虚假信息威胁，这些信息可能在几分钟内引发数十亿美元的损失。大多数现有方法在决策过程中缺乏透明度，并且对可信来源的归因有限。我们提出 FinVet，一个新颖的多智能体框架，通过置信度加权投票机制，将两个检索增强生成（Retrieval-Augmented Generation, RAG）流水线与外部事实核查相结合。FinVet 采用自适应的三级处理流程，根据检索置信度动态调整验证策略，从直接元数据提取，到混合推理，再到基于模型的完整分析。与现有方法不同，FinVet 在证据不足时提供基于证据的判定结果、来源归因、置信度评分以及明确的不确定性标记。在 FinFact 数据集上的实验评估表明，FinVet 达到 0.85 的 F1 分数，相比最优的单一流水线（事实核查流水线）提高了 10.4%，相比独立的 RAG 方法提高了 37%。"
    },
    {
        "title": "OneRec-Think: In-Text Reasoning for Generative Recommendation",
        "url": "http://arxiv.org/abs/2510.11639v1",
        "pub_date": "2025-10-13",
        "summary": "The powerful generative capacity of Large Language Models (LLMs) has instigated a paradigm shift in recommendation. However, existing generative models (e.g., OneRec) operate as implicit predictors, critically lacking the capacity for explicit and controllable reasoning-a key advantage of LLMs. To bridge this gap, we propose OneRec-Think, a unified framework that seamlessly integrates dialogue, reasoning, and personalized recommendation. OneRec-Think incorporates: (1) Itemic Alignment: cross-modal Item-Textual Alignment for semantic grounding; (2) Reasoning Activation: Reasoning Scaffolding to activate LLM reasoning within the recommendation context; and (3) Reasoning Enhancement, where we design a recommendation-specific reward function that accounts for the multi-validity nature of user preferences. Experiments across public benchmarks show state-of-the-art performance. Moreover, our proposed \"Think-Ahead\" architecture enables effective industrial deployment on Kuaishou, achieving a 0.159\\% gain in APP Stay Time and validating the practical efficacy of the model's explicit reasoning capability.",
        "translated": "大型语言模型（LLMs）强大的生成能力正在引发推荐系统领域的一场范式转变。然而，现有的生成模型（例如 OneRec）主要作为隐式预测器运行，严重缺乏显式可控推理的能力——而这正是 LLMs 的关键优势所在。为了解决这一问题，我们提出了 OneRec-Think，一个统一的框架，能够无缝融合对话、推理与个性化推荐。OneRec-Think 包含以下三个核心模块：（1）Itemic 对齐：跨模态的物品-文本对齐，以实现语义基础的构建；（2）推理激活：通过推理结构（Reasoning Scaffolding）在推荐场景中激活 LLM 的推理能力；以及（3）推理增强：我们设计了一个面向推荐任务的奖励函数，以应对用户偏好的多合理性（multi-validity）特性。在多个公开基准上的实验表明，该方法取得了最先进的性能。此外，我们提出的“Think-Ahead”架构在快手平台实现了有效的工业部署，使得 APP 使用时长提升了 0.159%，验证了模型显式推理能力的实用效果。"
    },
    {
        "title": "SemCSE-Multi: Multifaceted and Decodable Embeddings for Aspect-Specific\n  and Interpretable Scientific Domain Mapping",
        "url": "http://arxiv.org/abs/2510.11599v1",
        "pub_date": "2025-10-13",
        "summary": "We propose SemCSE-Multi, a novel unsupervised framework for generating multifaceted embeddings of scientific abstracts, evaluated in the domains of invasion biology and medicine. These embeddings capture distinct, individually specifiable aspects in isolation, thus enabling fine-grained and controllable similarity assessments as well as adaptive, user-driven visualizations of scientific domains. Our approach relies on an unsupervised procedure that produces aspect-specific summarizing sentences and trains embedding models to map semantically related summaries to nearby positions in the embedding space. We then distill these aspect-specific embedding capabilities into a unified embedding model that directly predicts multiple aspect embeddings from a scientific abstract in a single, efficient forward pass. In addition, we introduce an embedding decoding pipeline that decodes embeddings back into natural language descriptions of their associated aspects. Notably, we show that this decoding remains effective even for unoccupied regions in low-dimensional visualizations, thus offering vastly improved interpretability in user-centric settings.",
        "translated": "我们提出了一种名为 SemCSE-Multi 的新型无监督框架，用于生成科学摘要的多方面嵌入表示，并在入侵生物学和医学领域进行了评估。该嵌入能够独立捕捉多个明确且可单独指定的方面，从而支持细粒度、可控的相似性评估，以及自适应的、以用户驱动的科学领域可视化。我们的方法依赖于一个无监督过程，首先生成特定方面的总结性句子，并训练嵌入模型将语义相关的摘要映射到嵌入空间中的相近位置。随后，我们将这些特定方面的嵌入能力提炼到一个统一的嵌入模型中，使其能够在一个高效、单一的前向传播过程中直接从科学摘要中预测出多个方面嵌入。此外，我们还引入了一个嵌入解码流程，将嵌入表示还原为与各特定方面相关的自然语言描述。值得注意的是，我们证明了即使在低维可视化中未被占用的区域，该解码过程依然有效，从而在以用户为中心的应用场景中显著提升了可解释性。"
    },
    {
        "title": "REGENT: Relevance-Guided Attention for Entity-Aware Multi-Vector Neural\n  Re-Ranking",
        "url": "http://arxiv.org/abs/2510.11592v1",
        "pub_date": "2025-10-13",
        "summary": "Current neural re-rankers often struggle with complex information needs and long, content-rich documents. The fundamental issue is not computational--it is intelligent content selection: identifying what matters in lengthy, multi-faceted texts. While humans naturally anchor their understanding around key entities and concepts, neural models process text within rigid token windows, treating all interactions as equally important and missing critical semantic signals. We introduce REGENT, a neural re-ranking model that mimics human-like understanding by using entities as a \"semantic skeleton\" to guide attention. REGENT integrates relevance guidance directly into the attention mechanism, combining fine-grained lexical matching with high-level semantic reasoning. This relevance-guided attention enables the model to focus on conceptually important content while maintaining sensitivity to precise term matches. REGENT achieves new state-of-the-art performance in three challenging datasets, providing up to 108% improvement over BM25 and consistently outperforming strong baselines including ColBERT and RankVicuna. To our knowledge, this is the first work to successfully integrate entity semantics directly into neural attention, establishing a new paradigm for entity-aware information retrieval.",
        "translated": "目前的神经重排序模型在处理复杂的信息需求和长篇、内容丰富文档时常常面临挑战。其根本问题并不在于计算能力，而在于智能内容选择：识别长篇、多维文本中真正重要的信息。尽管人类在理解文本时自然地围绕关键实体和概念进行定位，但神经模型则受限于固定的词元窗口对文本进行处理，将所有交互视为同等重要，从而忽略了关键的语义信号。我们提出REGENT，这是一种神经重排序模型，通过使用实体作为“语义骨架”来引导注意力，从而模拟人类的理解方式。REGENT将相关性引导直接整合到注意力机制中，将细粒度的词汇匹配与高层次的语义推理相结合。这种相关性引导的注意力机制使模型能够聚焦于概念上重要的内容，同时保持对精确术语匹配的敏感性。REGENT在三个具有挑战性的数据集上达到了新的最先进性能，在BM25基础上最多提升了108%，并且始终优于ColBERT和RankVicuna等强基线模型。据我们所知，这是首次成功将实体语义直接整合到神经注意力机制中的工作，为具备实体感知能力的信息检索建立了一个新的范式。"
    },
    {
        "title": "QDER: Query-Specific Document and Entity Representations for\n  Multi-Vector Document Re-Ranking",
        "url": "http://arxiv.org/abs/2510.11589v1",
        "pub_date": "2025-10-13",
        "summary": "Neural IR has advanced through two distinct paths: entity-oriented approaches leveraging knowledge graphs and multi-vector models capturing fine-grained semantics. We introduce QDER, a neural re-ranking model that unifies these approaches by integrating knowledge graph semantics into a multi-vector model. QDER's key innovation lies in its modeling of query-document relationships: rather than computing similarity scores on aggregated embeddings, we maintain individual token and entity representations throughout the ranking process, performing aggregation only at the final scoring stage - an approach we call \"late aggregation.\" We first transform these fine-grained representations through learned attention patterns, then apply carefully chosen mathematical operations for precise matches. Experiments across five standard benchmarks show that QDER achieves significant performance gains, with improvements of 36% in nDCG@20 over the strongest baseline on TREC Robust 2004 and similar improvements on other datasets. QDER particularly excels on difficult queries, achieving an nDCG@20 of 0.70 where traditional approaches fail completely (nDCG@20 = 0.0), setting a foundation for future work in entity-aware retrieval.",
        "translated": "神经信息检索（Neural IR）的发展经历了两条不同的路径：一种是面向实体的方法，利用知识图谱信息；另一种是多向量模型，用于捕捉细粒度语义。我们提出 QDER，这是一种神经重排序模型，通过将知识图谱语义整合到多向量模型中，实现了这两条路径的统一。QDER 的关键创新在于其对查询与文档关系的建模：与在聚合嵌入上计算相似度分数的传统方法不同，我们在整个排序过程中保留每个词元（token）和实体的独立表示，仅在最终的评分阶段进行聚合——我们称这种策略为“晚期聚合”（late aggregation）。首先，我们通过学习得到的注意力模式对这些细粒度表示进行转换，然后应用精心选择的数学运算以实现精确匹配。在五个标准基准数据集上的实验表明，QDER 显著提升了性能，在 TREC Robust 2004 数据集上相比最强基线模型的 nDCG@20 提高了 36%，在其他数据集上也有类似提升。尤其在处理困难查询时，QDER 表现出色，取得了 0.70 的 nDCG@20 评分，而传统方法在此类查询上完全失效（nDCG@20 = 0.0），为未来实体感知（entity-aware）检索的研究奠定了基础。"
    },
    {
        "title": "Characterizing Web Search in The Age of Generative AI",
        "url": "http://arxiv.org/abs/2510.11560v1",
        "pub_date": "2025-10-13",
        "summary": "The advent of LLMs has given rise to a new type of web search: Generative search, where LLMs retrieve web pages related to a query and generate a single, coherent text as a response. This output modality stands in stark contrast to traditional web search, where results are returned as a ranked list of independent web pages. In this paper, we ask: Along what dimensions do generative search outputs differ from traditional web search? We compare Google, a traditional web search engine, with four generative search engines from two providers (Google and OpenAI) across queries from four domains. Our analysis reveals intriguing differences. Most generative search engines cover a wider range of sources compared to web search. Generative search engines vary in the degree to which they rely on internal knowledge contained within the model parameters v.s. external knowledge retrieved from the web. Generative search engines surface varying sets of concepts, creating new opportunities for enhancing search diversity and serendipity. Our results also highlight the need for revisiting evaluation criteria for web search in the age of Generative AI.",
        "translated": "大语言模型（LLMs）的出现催生了一种新的网络搜索方式：生成式搜索（generative search），在这种搜索方式中，LLMs会检索与查询相关的网页，并生成一段连贯统一的文本作为响应。这种输出形式与传统网络搜索形成了鲜明对比，后者返回的是一个按相关性排序的独立网页列表。在本文中，我们提出以下问题：生成式搜索的输出在哪些维度上与传统网络搜索有所不同？我们从四个领域中选取查询，比较了传统网络搜索引擎Google与来自两家提供商（Google和OpenAI）的四个生成式搜索引擎的表现。我们的分析揭示了一些有趣的差异。大多数生成式搜索引擎相比传统网络搜索，能够涵盖更广泛的来源。生成式搜索引擎在依赖模型参数中包含的内部知识与从网络检索的外部知识的程度上存在差异。此外，生成式搜索引擎展示的概念集合各不相同，从而为提升搜索多样性和偶然性提供了新的可能性。我们的结果还强调，在生成式人工智能时代，有必要重新审视网络搜索的评估标准。"
    },
    {
        "title": "Uncertainty Quantification for Retrieval-Augmented Reasoning",
        "url": "http://arxiv.org/abs/2510.11483v1",
        "pub_date": "2025-10-13",
        "summary": "Retrieval-augmented reasoning (RAR) is a recent evolution of retrieval-augmented generation (RAG) that employs multiple reasoning steps for retrieval and generation. While effective for some complex queries, RAR remains vulnerable to errors and misleading outputs. Uncertainty quantification (UQ) offers methods to estimate the confidence of systems' outputs. These methods, however, often handle simple queries with no retrieval or single-step retrieval, without properly handling RAR setup. Accurate estimation of UQ for RAR requires accounting for all sources of uncertainty, including those arising from retrieval and generation. In this paper, we account for all these sources and introduce Retrieval-Augmented Reasoning Consistency (R2C)--a novel UQ method for RAR. The core idea of R2C is to perturb the multi-step reasoning process by applying various actions to reasoning steps. These perturbations alter the retriever's input, which shifts its output and consequently modifies the generator's input at the next step. Through this iterative feedback loop, the retriever and generator continuously reshape one another's inputs, enabling us to capture uncertainty arising from both components. Experiments on five popular RAR systems across diverse QA datasets show that R2C improves AUROC by over 5% on average compared to the state-of-the-art UQ baselines. Extrinsic evaluations using R2C as an external signal further confirm its effectiveness for two downstream tasks: in Abstention, it achieves ~5% gains in both F1Abstain and AccAbstain; in Model Selection, it improves the exact match by ~7% over single models and ~3% over selection methods.",
        "translated": "检索增强推理（Retrieval-Augmented Reasoning, RAR）是检索增强生成（Retrieval-Augmented Generation, RAG）的最新演进，其通过在检索与生成过程中引入多步推理来提高效果。尽管在处理某些复杂查询时表现出色，RAR 仍然容易受到错误和误导性输出的影响。不确定性量化（Uncertainty Quantification, UQ）提供了一种估计系统输出置信度的方法。然而，现有方法大多针对无检索或单步检索的简单查询，未能有效应对 RAR 的设置。对 RAR 的 UQ 进行准确估计，需要综合考虑所有可能的不确定性来源，包括检索和生成过程中的不确定性。在本文中，我们系统地考虑了这些不确定性来源，并提出了检索增强推理一致性（Retrieval-Augmented Reasoning Consistency, R2C）——一种新型的 UQ 方法。R2C 的核心思想是通过对推理步骤施加多种操作，扰动多步推理过程。这些扰动会改变检索器的输入，从而影响其输出，并进一步修改生成器在下一步的输入。通过这一迭代反馈机制，检索器和生成器不断重塑彼此的输入，使我们能够捕捉来自两个组件的不确定性。在五个主流 RAR 系统和多个问答数据集上的实验表明，与最先进的 UQ 基线方法相比，R2C 在平均 AUROC 指标上提升了超过 5%。使用 R2C 作为外部信号进行的外在评估进一步验证了其有效性，针对两个下游任务：在拒绝回答（Abstention）任务中，R2C 在 F1Abstain 和 AccAbstain 指标上分别提升了约 5%；在模型选择（Model Selection）任务中，R2C 相比单模型提升了约 7% 的精确匹配（exact match），相比其他选择方法提升了约 3%。"
    },
    {
        "title": "What Generative Search Engines Like and How to Optimize Web Content\n  Cooperatively",
        "url": "http://arxiv.org/abs/2510.11438v1",
        "pub_date": "2025-10-13",
        "summary": "By employing large language models (LLMs) to retrieve documents and generate natural language responses, Generative Engines, such as Google AI overview and ChatGPT, provide significantly enhanced user experiences and have rapidly become the new form of search. Their rapid adoption also drives the needs of Generative Engine Optimization (GEO), as content providers are eager to gain more traction from them. In this paper, we introduce AutoGEO, a framework to automatically learn generative engine preferences when using retrieved contents for response generation, and rewrite web contents for more such traction. AutoGEO first prompts frontier LLMs to explain generative engine preferences and extract meaningful preference rules from these explanations. Then it uses preference rules as context engineering for AutoGEO$_\\text{API}$, a prompt-based GEO system, and as rule-based rewards to train AutoGEO$_\\text{Mini}$, a cost-effective GEO model. Experiments on the standard GEO-Bench and two newly constructed benchmarks using real user queries demonstrate the effectiveness of AutoGEO in enhancing content traction while preserving search utility. Analyses confirm the learned rules' robustness and abilities to capture unique preferences in variant domains, and AutoGEO systems' ability to embed them in content optimization. The code is released at https://github.com/cxcscmu/AutoGEO.",
        "translated": "通过使用大语言模型（LLMs）来检索文档并生成自然语言响应，生成式引擎（如 Google AI 概述和 ChatGPT）显著提升了用户体验，并迅速成为搜索的新形态。其迅速普及也推动了生成式引擎优化（Generative Engine Optimization, GEO）的需求，因为内容提供者希望从这些系统中获得更多曝光。在本文中，我们提出了 AutoGEO，一个在使用检索内容进行响应生成时，能够自动学习生成式引擎偏好的框架，并对网页内容进行重写以提高其曝光度。AutoGEO 首先提示前沿的大语言模型解释生成式引擎的偏好，并从这些解释中提取有意义的偏好规则。接着，它将这些偏好规则用于 AutoGEO$_\\text{API}$——一个基于提示的 GEO 系统——作为上下文工程的输入，并将规则作为奖励机制，用于训练 AutoGEO$_\\text{Mini}$——一个经济高效的 GEO 模型。在标准的 GEO-Bench 基准以及基于真实用户查询构建的两个新基准上的实验表明，AutoGEO 在提升内容曝光度的同时能够保持搜索效用的有效性。分析结果进一步验证了所学习规则的鲁棒性及其在不同领域中捕捉独特偏好的能力，并确认了 AutoGEO 系统在内容优化中嵌入这些规则的能力。代码已发布在 https://github.com/cxcscmu/AutoGEO。"
    },
    {
        "title": "On Inherited Popularity Bias in Cold-Start Item Recommendation",
        "url": "http://arxiv.org/abs/2510.11402v1",
        "pub_date": "2025-10-13",
        "summary": "Collaborative filtering (CF) recommender systems struggle with making predictions on unseen, or 'cold', items. Systems designed to address this challenge are often trained with supervision from warm CF models in order to leverage collaborative and content information from the available interaction data. However, since they learn to replicate the behavior of CF methods, cold-start models may therefore also learn to imitate their predictive biases. In this paper, we show that cold-start systems can inherit popularity bias, a common cause of recommender system unfairness arising when CF models overfit to more popular items, thereby maximizing user-oriented accuracy but neglecting rarer items. We demonstrate that cold-start recommenders not only mirror the popularity biases of warm models, but are in fact affected more severely: because they cannot infer popularity from interaction data, they instead attempt to estimate it based solely on content features. This leads to significant over-prediction of certain cold items with similar content to popular warm items, even if their ground truth popularity is very low. Through experiments on three multimedia datasets, we analyze the impact of this behavior on three generative cold-start methods. We then describe a simple post-processing bias mitigation method that, by using embedding magnitude as a proxy for predicted popularity, can produce more balanced recommendations with limited harm to user-oriented cold-start accuracy.",
        "translated": "协同过滤（Collaborative Filtering, CF）推荐系统在对未见过的或“冷启动”的物品进行预测时面临挑战。为了解决这一问题，设计用于冷启动场景的系统通常会借助“热启动”CF模型进行监督训练，以利用现有的交互数据中的协作信息和内容信息。然而，由于这些系统学习的是复制CF方法的行为，因此冷启动模型也可能学会模仿其预测偏差。本文中，我们表明冷启动系统可能会继承“流行度偏差”（popularity bias），这是推荐系统不公平性的常见原因，当CF模型过度拟合更受欢迎的物品时就会产生这种偏差，从而在最大化面向用户准确率的同时忽略了较少出现的物品。我们通过实验发现，冷启动推荐器不仅复制了热模型的流行度偏差，而且实际上受到更严重的影响：由于它们无法从交互数据中推断出流行度，因此转而尝试仅基于内容特征来估计流行度。这导致某些与热门热启动物品在内容上相似的冷启动物品被显著高估，即使它们的真实流行度非常低。我们在三个多媒体数据集上分析了这一行为对三种生成式冷启动方法的影响。随后，我们介绍了一种简单的后处理偏差缓解方法，该方法通过将嵌入（embedding）模长作为预测流行度的代理指标，能够在有限地影响面向用户冷启动准确率的前提下，生成更加平衡的推荐结果。"
    },
    {
        "title": "VeriCite: Towards Reliable Citations in Retrieval-Augmented Generation\n  via Rigorous Verification",
        "url": "http://arxiv.org/abs/2510.11394v1",
        "pub_date": "2025-10-13",
        "summary": "Retrieval-Augmented Generation (RAG) has emerged as a crucial approach for enhancing the responses of large language models (LLMs) with external knowledge sources. Despite the impressive performance in complex question-answering tasks, RAG still struggles with hallucinations. Attributing RAG-generated content through in-line citations has demonstrated potential in reducing hallucinations and facilitating human verification. Existing citation generation methods primarily rely on either fine-tuning the generator or employing post-processing approaches for citation matching. However, the former approach demands substantial annotated data and computational resources, while the latter often encounters difficulties in managing multiple citations and frequently produces suboptimal results. In this paper, we introduce a novel framework, called VeriCite, designed to rigorously validate supporting evidence and enhance answer attribution. Specifically, VeriCite breaks down into a three-stage generation: 1) The initial answer generation first generates a response based on all available contexts and has its claims verified through the NLI model; 2) the supporting evidence selection assesses the utility of each document and extracts useful supporting evidences; 3) the final answer refinement integrates the initial response and collected evidences to produce the final, refined answer.We conduct experiments across five open-source LLMs and four datasets, demonstrating that VeriCite can significantly improve citation quality while maintaining the correctness of the answers.",
        "translated": "检索增强生成（Retrieval-Augmented Generation, RAG）已成为一种关键方法，用于通过外部知识源增强大语言模型（Large Language Models, LLMs）的响应能力。尽管RAG在复杂问答任务中表现出色，但其仍面临幻觉（hallucination）问题。通过行内引用（in-line citations）对RAG生成的内容进行归因，已被证明在减少幻觉和便于人工验证方面具有潜力。现有的引用生成方法主要依赖于对生成器的微调，或采用后处理方法进行引用匹配。然而，前者需要大量标注数据和计算资源，而后者在处理多个引用时常常遇到困难，且结果往往不够理想。在本文中，我们提出了一种新颖的框架，命名为VeriCite，旨在严格验证支持证据并提升答案归因能力。具体而言，VeriCite分为三个生成阶段：1）初始答案生成阶段基于所有可用上下文生成初步回答，并通过自然语言推理模型（NLI model）对其主张进行验证；2）支持证据选择阶段评估每个文档的有用性，并提取有效的支持证据；3）最终答案优化阶段整合初始回答与收集到的证据，生成最终的、经过优化的答案。我们在五种开源大语言模型和四个数据集上进行了实验，结果表明，VeriCite在保持答案正确性的同时，能够显著提升引用质量。"
    },
    {
        "title": "LLM-Specific Utility: A New Perspective for Retrieval-Augmented\n  Generation",
        "url": "http://arxiv.org/abs/2510.11358v1",
        "pub_date": "2025-10-13",
        "summary": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by incorporating external knowledge. While traditional retrieval focuses on relevance, RAG's effectiveness depends on the utility of retrieved passages, i.e., the usefulness in facilitating the generation of an accurate and comprehensive answer. Existing studies often treat utility as a generic attribute, ignoring the fact that different LLMs may benefit differently from the same passage due to variations in internal knowledge and comprehension ability. In this work, we introduce and systematically investigate the notion of LLM-specific utility. Through large-scale experiments across multiple datasets and LLMs, we demonstrate that human-annotated passages are not optimal for LLMs and that ground-truth utilitarian passages are not transferable across different LLMs. These findings highlight the necessity of adopting the LLM-specific utility in RAG research. Our findings indicate that some human-annotated passages are not ground-truth utilitarian passages for specific LLMs, partially due to the varying readability of queries and passages for LLMs, a tendency for which perplexity is a key metric. Based on these findings, we propose a benchmarking procedure for LLM-specific utility judgments. We evaluate existing utility judgment methods on six datasets and find that while verbalized methods using pseudo-answers perform robustly, LLMs struggle to assess utility effectively-failing to reject all passages for known queries and to select truly useful ones for unknown queries.",
        "translated": "检索增强生成（Retrieval-augmented Generation, RAG）通过引入外部知识来增强大语言模型（Large Language Models, LLMs）的能力。尽管传统检索方法主要关注相关性（relevance），但 RAG 的有效性依赖于检索到的段落的**效用性**（utility），即这些段落在促进生成准确且全面答案方面的有用程度。现有研究通常将效用性视为一种通用属性，忽视了由于不同 LLM 在内部知识和理解能力上的差异，它们可能从相同的段落中获得不同的收益。在本工作中，我们引入并系统地探讨了**面向特定 LLM 的效用性**（LLM-specific utility）这一概念。通过在多个数据集和 LLM 上的大规模实验，我们证明：人工标注的段落对 LLM 来说并非最优选择，并且真实效用性段落在不同 LLM 之间不可迁移。这些发现凸显了在 RAG 研究中采用 LLM-specific utility 的必要性。我们的研究表明，某些人工标注的段落对特定 LLM 而言并不是真实效用性段落，部分原因是 LLM 对查询和段落的可读性存在差异，而这种差异可以通过困惑度（perplexity）这一关键指标来衡量。基于上述发现，我们提出了一种面向 LLM-specific utility 的基准评估方法。我们对六种数据集上的现有效用性判断方法进行了评估，发现尽管使用伪答案（pseudo-answers）的显式化方法表现较为稳健，但 LLM 在评估段落效用性方面仍存在困难，无法拒绝所有已知查询的段落，也无法为未知查询有效挑选真正有用的段落。"
    },
    {
        "title": "Dynamic Network-Based Two-Stage Time Series Forecasting for Affiliate\n  Marketing",
        "url": "http://arxiv.org/abs/2510.11323v1",
        "pub_date": "2025-10-13",
        "summary": "In recent years, affiliate marketing has emerged as a revenue-sharing strategy where merchants collaborate with promoters to promote their products. It not only increases product exposure but also allows promoters to earn a commission. This paper addresses the pivotal yet under-explored challenge in affiliate marketing: accurately assessing and predicting the contributions of promoters in product promotion. We design a novel metric for evaluating the indirect contributions of the promoter, called propagation scale. Unfortunately, existing time series forecasting techniques fail to deliver accurate predictions due to the propagation scale being influenced by multiple factors and the inherent complexities arising from dynamic scenarios. To address this issue, we decouple the network structure from the node signals and propose a two-stage solution: initially, the basic self-sales and network structure prediction are conducted separately, followed by the synthesis of the propagation scale. Specifically, we design a graph convolution encoding scheme based on descendant neighbors and incorporate hypergraph convolution to efficiently capture complex promotional dynamics. Additionally, three auxiliary tasks are employed: self-sales prediction for base estimations, descendant prediction to synthesize propagation scale, and promoter activation prediction to mitigate high volatility issues. Extensive offline experiments on large-scale industrial datasets validate the superiority of our method. We further deploy our model on Alimama platform with over $100,000$ promoters, achieving a $9.29\\%$ improvement in GMV and a $5.89\\%$ increase in sales volume.",
        "translated": "近年来，联盟营销已成为一种收益共享策略，商家与推广者合作以推广其商品。这种模式不仅提高了商品的曝光度，还使推广者能够获得佣金。本文关注联盟营销中一个关键但尚未充分研究的问题：**准确评估和预测推广者在商品推广中的贡献**。我们设计了一种用于衡量推广者间接贡献的新指标，称为**传播规模（propagation scale）**。然而，由于传播规模受到多种因素影响，且动态场景中存在内在复杂性，现有的时间序列预测技术难以实现准确的预测。为了解决这一问题，我们**将网络结构与节点信号解耦**，并提出了一种两阶段的解决方案：首先，分别预测基本的自主销售与网络结构；随后，合成传播规模。具体而言，我们设计了一种基于后代邻居的图卷积编码方案，并引入超图卷积以高效捕捉复杂的推广动态。此外，我们还引入了三个辅助任务：用于基础估计的自主销售预测、用于合成传播规模的后代节点预测，以及用于缓解高波动性问题的推广者激活预测。我们在大规模工业数据集上进行了广泛的离线实验，验证了我们方法的优越性。我们进一步将模型部署在拥有超过100,000名推广者的Alimama平台，实现了GMV提升9.29%，销售量增长5.89%。"
    },
    {
        "title": "Next Interest Flow: A Generative Pre-training Paradigm for Recommender\n  Systems by Modeling All-domain Movelines",
        "url": "http://arxiv.org/abs/2510.11317v1",
        "pub_date": "2025-10-13",
        "summary": "Click-Through Rate (CTR) prediction, a cornerstone of modern recommender systems, has been dominated by discriminative models that react to past user behavior rather than proactively modeling user intent. Existing generative paradigms attempt to address this but suffer from critical limitations: Large Language Model (LLM) based methods create a semantic mismatch by forcing e-commerce signals into a linguistic space, while ID-based generation is constrained by item memorization and cold-start issues. To overcome these limitations, we propose a novel generative pre-training paradigm. Our model learns to predict the Next Interest Flow, a dense vector sequence representing a user's future intent, while simultaneously modeling its internal Interest Diversity and Interest Evolution Velocity to ensure the representation is both rich and coherent. However, this two-stage approach introduces a critical objective mismatch between the generative and discriminative stages. We resolve this via a bidirectional alignment strategy, which harmonizes the two stages through cross-stage weight initialization and a dynamic Semantic Alignment Module for fine-tuning. Additionally, we enhance the underlying discriminative model with a Temporal Sequential Pairwise (TSP) mechanism to better capture temporal causality. We present the All-domain Moveline Evolution Network (AMEN), a unified framework implementing our entire pipeline. Extensive offline experiments validate AMEN's superiority over strong baselines, and a large-scale online A/B test demonstrates its significant real-world impact, delivering substantial improvements in key business metrics.",
        "translated": "点击率（CTR）预测是现代推荐系统中的核心任务之一，目前主要依赖于判别式模型，这些模型侧重于响应用户过去的行为，而未能主动建模用户的意图。现有的生成式范式尝试解决这一问题，但存在关键的局限性：基于大语言模型（LLM）的方法将电商信号强行映射到语言语义空间，导致语义错配；而基于ID的生成方法则受到物品记忆能力和冷启动问题的限制。为克服这些限制，我们提出了一种新颖的生成式预训练范式。我们的模型旨在预测“下一个兴趣流”（Next Interest Flow），即一个稠密向量序列，用于表示用户未来的兴趣意图。同时，该模型还建模内部的兴趣多样性（Interest Diversity）和兴趣演化速度（Interest Evolution Velocity），以确保表示的丰富性与一致性。然而，这种两阶段的方法在生成阶段与判别阶段之间引入了关键的目标不匹配问题。我们通过双向对齐策略加以解决，该策略通过跨阶段权重初始化和动态语义对齐模块（Semantic Alignment Module）进行微调，从而协调两个阶段之间的差异。此外，我们通过引入时间序列成对机制（Temporal Sequential Pairwise, TSP）来增强底层判别模型，以更好地捕捉时间因果关系。我们提出了一个统一的框架——全领域兴趣演化网络（All-domain Moveline Evolution Network, AMEN），实现了我们完整的流水线。大量离线实验验证了AMEN在强基线模型上的优越性能，而大规模在线A/B测试也展示了其在现实场景中的显著影响，显著提升了关键业务指标。"
    },
    {
        "title": "ELMO: Efficiency via Low-precision and Peak Memory Optimization in Large\n  Output Spaces",
        "url": "http://arxiv.org/abs/2510.11168v1",
        "pub_date": "2025-10-13",
        "summary": "Large output spaces, also referred to as Extreme multilabel classification (XMC), is a setting that arises, e.g., in large-scale tagging and product-to-product recommendation, and is characterized by the number of labels ranging from hundreds of thousands to millions. This means that the linear classification head, usually only a tiny fraction of the overall model, turns into the main driver for compute and memory demand. Current state-of-the-art XMC methods predominantly rely on FP16-FP32 mixed-precision training, which we show can be unstable, and inefficient in terms of memory usage and computational overhead. Meanwhile, existing low-precision methods typically retain higher precision for the classification layer. In this work, we propose ELMO, a pure low-precision training framework for XMC models using BFloat16 and Float8 data types. By leveraging Kahan summation and stochastic rounding, we demonstrate that XMC models can be effectively trained entirely in Float8, without relying on single-precision master weights or tensor scaling. Low-precision training, combined with our proposed memory optimizations -- gradient fusion and chunking -- enables significant reductions in GPU memory usage. For example, we train a 3-million-label XMC model with only 6.6 GiB of GPU memory, compared to the 39.7 GiB required by the optimized SOTA method, Renee without compromising accuracy.",
        "translated": "大规模输出空间，也称为极端多标签分类（Extreme multilabel classification, XMC），是一种在大规模标签分配和商品到商品推荐等场景中常见的设定，其特点是标签数量可高达数十万到数百万级。这意味着线性分类头（通常在整个模型中仅占极小部分）变成了计算和内存需求的主要驱动因素。当前最先进的XMC方法主要依赖FP16与FP32混合精度训练，但我们发现这种方法在训练稳定性、内存使用效率以及计算开销方面存在不足。同时，现有的低精度训练方法通常仍为分类层保留较高精度。在本文中，我们提出ELMO，一个完全基于BFloat16和Float8数据类型的低精度训练框架。通过引入Kahan求和和随机舍入技术，我们证明XMC模型可以完全在Float8精度下进行有效训练，而无需依赖单精度主权重或张量缩放。结合我们提出的内存优化方法——梯度融合和块处理（chunking），该框架能够显著减少GPU内存的使用。例如，我们在仅使用6.6 GiB GPU内存的情况下训练了一个包含300万个标签的XMC模型，而优化后的SOTA方法Renee则需要39.7 GiB的内存，且在不损失精度的前提下实现了这一目标。"
    },
    {
        "title": "DyKnow-RAG: Dynamic Knowledge Utilization Reinforcement Framework for\n  Noisy Retrieval-Augmented Generation in E-commerce Search Relevance",
        "url": "http://arxiv.org/abs/2510.11122v1",
        "pub_date": "2025-10-13",
        "summary": "Accurately modeling query-item relevance drives e-commerce ranking, yet long-tail, knowledge-heavy, and fast-evolving queries exceed parametric LLM coverage. External context (reviews, attribute encyclopedias, UGC) can help but is noisy, and single-pass latency and cost forbid any clean-then-summarize step. The model must, per query, judge relevance and decide whether to use, partially use, or ignore the context. DyKnow-RAG is a dynamic noisy-RAG framework built on Group Relative Policy Optimization. It trains two rollout groups (no external context vs a single retrieved chunk) and applies posterior-driven inter-group advantage scaling that adaptively reweights their contributions by the per-query correctness gap. This teaches when to trust retrieval versus fall back to parametric knowledge, without process labels, value networks, or extra inference passes, preserving single-pass, single-chunk deployment under production latency. Training combines: (1) supervised initialization with a structured rationale that explicitly records the context-usage decision; (2) an RL pool prioritized by SFT uncertainty to focus where context choice is most consequential; and (3) an optional lightweight DPO warm start to stabilize with-context calibration. Under a unified retrieval/index and fixed latency budget, DyKnow-RAG outperforms SFT, DPO, and vanilla GRPO in offline tests, and delivers consistent lifts on GSB, Query Goodrate, and Item Goodrate in Taobao A/B testing. It is deployed in Taobao's production relevance system, serving live traffic. To our knowledge, it is among the first single-pass RAG solutions for e-commerce relevance, turning noisy external signals into reliable gains without added online complexity.",
        "translated": "准确地建模查询与商品的相关性对于电商排序至关重要，然而长尾、知识密集型和快速变化的查询超出了参数化大语言模型的覆盖范围。外部上下文（如商品评论、属性百科、用户生成内容）虽然可以提供帮助，但往往包含噪声，且单次推理的延迟和成本限制了任何“清理再摘要”的步骤。因此，模型必须在每次查询中判断相关性，并决定是否使用、部分使用或忽略外部上下文。\n\nDyKnow-RAG 是一种基于组相对策略优化（Group Relative Policy Optimization）的动态噪声-RAG框架。该框架通过训练两个 rollout 组（一组不使用外部上下文，另一组使用一个检索到的 chunk）并采用后验驱动的组间优势缩放方法，以查询间的准确性差距自适应地重新加权两组的贡献。这种方法能够在无需过程标签、价值网络或额外推理步骤的前提下，学习何时信任检索结果，何时回退至参数化知识，从而在生产延迟下保持单次推理、单 chunk 的部署效率。\n\nDyKnow-RAG 的训练结合了以下三个阶段：（1）结构化理由监督初始化，显式记录上下文使用决策；（2）以监督微调（SFT）的不确定性为优先级的强化学习（RL）池，集中于上下文选择最具影响的场景；（3）一个可选的轻量级 DPO 预训练阶段，用于稳定上下文相关校准。\n\n在统一的检索/索引设置和固定的延迟预算下，DyKnow-RAG 在离线测试中优于 SFT、DPO 和标准 GRPO。在淘宝的 A/B 测试中，该方法在 GSB、Query Goodrate 和 Item Goodrate 等指标上均实现了持续的提升。目前，DyKnow-RAG 已部署于淘宝的生产相关性系统中，服务实时流量。据我们所知，这是首批针对电商相关性问题的单次推理 RAG 解决方案之一，能够在不增加在线复杂度的前提下，将噪声外部信号转化为可靠收益。"
    },
    {
        "title": "Zero-Shot CFC: Fast Real-World Image Denoising based on Cross-Frequency\n  Consistency",
        "url": "http://arxiv.org/abs/2510.12646v1",
        "pub_date": "2025-10-14",
        "summary": "Zero-shot denoisers address the dataset dependency of deep-learning-based denoisers, enabling the denoising of unseen single images. Nonetheless, existing zero-shot methods suffer from long training times and rely on the assumption of noise independence and a zero-mean property, limiting their effectiveness in real-world denoising scenarios where noise characteristics are more complicated. This paper proposes an efficient and effective method for real-world denoising, the Zero-Shot denoiser based on Cross-Frequency Consistency (ZSCFC), which enables training and denoising with a single noisy image and does not rely on assumptions about noise distribution. Specifically, image textures exhibit position similarity and content consistency across different frequency bands, while noise does not. Based on this property, we developed cross-frequency consistency loss and an ultralight network to realize image denoising. Experiments on various real-world image datasets demonstrate that our ZSCFC outperforms other state-of-the-art zero-shot methods in terms of computational efficiency and denoising performance.",
        "translated": "零样本去噪器解决了基于深度学习的去噪器对数据集的依赖问题，使其能够对未见过的单张图像进行去噪。然而，现有的零样本方法训练时间较长，并且依赖于噪声独立性和零均值的假设，这在现实世界中噪声特性更为复杂的情况下限制了其去噪效果。本文提出了一种高效且有效的现实场景去噪方法——基于跨频段一致性的零样本去噪器（Zero-Shot Denoiser based on Cross-Frequency Consistency, ZSCFC），该方法仅需单张含噪图像即可完成训练与去噪，且不依赖于对噪声分布的假设。具体而言，图像的纹理在不同频段中表现出位置相似性和内容一致性，而噪声则不具备这一特性。基于该特性，我们设计了跨频段一致性损失函数，并构建了一个超轻量级网络以实现图像去噪。在多个现实世界图像数据集上的实验表明，与其它最先进的零样本方法相比，ZSCFC在计算效率和去噪性能方面均表现出色。"
    },
    {
        "title": "Normalization-equivariant Diffusion Models: Learning Posterior Samplers\n  From Noisy And Partial Measurements",
        "url": "http://arxiv.org/abs/2510.11964v1",
        "pub_date": "2025-10-13",
        "summary": "Diffusion models (DMs) have rapidly emerged as a powerful framework for image generation and restoration. However, existing DMs are primarily trained in a supervised manner by using a large corpus of clean images. This reliance on clean data poses fundamental challenges in many real-world scenarios, where acquiring noise-free data is hard or infeasible, and only noisy and potentially incomplete measurements are available. While some methods can train DMs using noisy data, they are generally effective only when the amount of noise is very mild or when some additional noise-free data is available. In addition, existing methods for training DMs from incomplete measurements require access to multiple complementary acquisition processes, an assumption that poses a significant practical limitation. Here we introduce the first approach for learning DMs for image restoration using only noisy measurement data from a single operator. As a first key contribution, we show that DMs, and more broadly minimum mean squared error denoisers, exhibit a weak form of scale equivariance linking rescaling in signal amplitude to changes in noise intensity. We then leverage this theoretical insight to develop a denoising score-matching strategy that generalizes robustly to noise levels lower than those present in the training data, thereby enabling the learning of DMs from noisy measurements. To further address the challenges of incomplete and noisy data, we integrate our method with equivariant imaging, a complementary self-supervised learning framework that exploits the inherent invariants of imaging problems, to train DMs for image restoration from single-operator measurements that are both incomplete and noisy. We validate the effectiveness of our approach through extensive experiments on image denoising, demosaicing, and inpainting, along with comparisons with the state of the art.",
        "translated": "扩散模型（DMs）已迅速成为图像生成与修复的强大框架。然而，现有的扩散模型主要依赖于大量干净图像的监督训练。这种对干净数据的依赖在许多现实场景中带来了根本性挑战，因为在这些场景中获取无噪声数据困难或不可行，仅有噪声干扰且可能不完整的测量数据可用。尽管已有方法尝试使用噪声数据训练扩散模型，但它们通常仅在噪声非常轻微或存在部分无噪声数据时才有效。此外，目前基于不完整测量数据训练扩散模型的方法通常需要多个互补的采集过程，这一假设在实践中构成了显著的限制。本文首次提出了一种仅使用单一操作算子的噪声测量数据来学习图像修复扩散模型的方法。作为我们的第一个关键贡献，我们证明了扩散模型，以及更广泛的最小均方误差去噪器，表现出一种弱形式的尺度等变性（scale equivariance），将信号幅值的缩放与噪声强度的变化联系起来。我们随后利用这一理论洞见，提出一种去噪得分匹配策略，该策略能够稳健地推广到训练数据中噪声水平更低的情况，从而实现基于噪声测量数据的扩散模型训练。为了进一步应对数据不完整和噪声的挑战，我们将该方法与等变成像（equivariant imaging）相结合，这是一种互补的自监督学习框架，利用了成像问题中的固有不变性，从而实现基于单一操作算子所获取的不完整且噪声干扰数据的图像修复扩散模型训练。我们在图像去噪、色彩插值（demosaicing）和修复（inpainting）任务上进行了广泛的实验验证，并与当前最先进的方法进行了对比。"
    },
    {
        "title": "Enabling High-Quality In-the-Wild Imaging from Severely Aberrated\n  Metalens Bursts",
        "url": "http://arxiv.org/abs/2510.10083v1",
        "pub_date": "2025-10-11",
        "summary": "We tackle the challenge of robust, in-the-wild imaging using ultra-thin nanophotonic metalens cameras. Meta-lenses, composed of planar arrays of nanoscale scatterers, promise dramatic reductions in size and weight compared to conventional refractive optics. However, severe chromatic aberration, pronounced light scattering, narrow spectral bandwidth, and low light efficiency continue to limit their practical adoption. In this work, we present an end-to-end solution for in-the-wild imaging that pairs a metalens several times thinner than conventional optics with a bespoke multi-image restoration framework optimized for practical metalens cameras. Our method centers on a lightweight convolutional network paired with a memory-efficient burst fusion algorithm that adaptively corrects noise, saturation clipping, and lens-induced distortions across rapid sequences of extremely degraded metalens captures. Extensive experiments on diverse, real-world handheld captures demonstrate that our approach consistently outperforms existing burst-mode and single-image restoration techniques.These results point toward a practical route for deploying metalens-based cameras in everyday imaging applications.",
        "translated": "我们针对使用超薄纳米光学金属透镜相机在真实环境下的鲁棒成像问题提出了一个解决方案。金属透镜由平面排列的纳米级散射体构成，相较于传统的折射式光学元件，有望显著减小尺寸和重量。然而，严重的色差、明显的光散射、狭窄的光谱带宽以及低光效率等问题仍限制了其实际应用。在本工作中，我们提出了一种端到端的真实环境下成像方案，该方案结合了一种比传统光学元件薄几倍的金属透镜，以及一种为实际金属透镜相机量身定制、高效的多图像复原框架。我们的方法核心是一个轻量级卷积网络，结合了一种内存高效的图像序列融合算法，能够自适应地校正在快速拍摄的严重退化金属透镜图像序列中出现的噪声、饱和裁剪和透镜引起的失真。我们在多种实际手持拍摄数据上进行了广泛的实验，结果表明，我们的方法在性能上持续优于现有的突发模式和单图像复原技术。这些结果表明，基于金属透镜的相机在日常成像应用中具有实际可行的部署路径。"
    },
    {
        "title": "Denoising Diffusion as a New Framework for Underwater Images",
        "url": "http://arxiv.org/abs/2510.09934v1",
        "pub_date": "2025-10-11",
        "summary": "Underwater images play a crucial role in ocean research and marine environmental monitoring since they provide quality information about the ecosystem. However, the complex and remote nature of the environment results in poor image quality with issues such as low visibility, blurry textures, color distortion, and noise. In recent years, research in image enhancement has proven to be effective but also presents its own limitations, like poor generalization and heavy reliance on clean datasets. One of the challenges herein is the lack of diversity and the low quality of images included in these datasets. Also, most existing datasets consist only of monocular images, a fact that limits the representation of different lighting conditions and angles. In this paper, we propose a new plan of action to overcome these limitations. On one hand, we call for expanding the datasets using a denoising diffusion model to include a variety of image types such as stereo, wide-angled, macro, and close-up images. On the other hand, we recommend enhancing the images using Controlnet to evaluate and increase the quality of the corresponding datasets, and hence improve the study of the marine ecosystem.   Tags - Underwater Images, Denoising Diffusion, Marine ecosystem, Controlnet",
        "translated": "水下图像在海洋研究和海洋环境监测中起着至关重要的作用，因为它们提供了有关生态系统的重要信息。然而，由于环境的复杂性和远离人类的特性，所获取的图像质量通常较差，存在诸如能见度低、纹理模糊、颜色失真和噪声等问题。近年来，图像增强的研究已被证明是有效的，但也存在自身局限性，例如泛化能力差和对干净数据集的依赖性较强。其中一大挑战是这些数据集中图像的多样性不足且质量偏低。此外，大多数现有数据集仅包含单目图像，这一事实限制了不同光照条件和角度的表征能力。本文中，我们提出了一种新的应对方案以克服这些局限性。一方面，我们建议使用去噪扩散模型（denoising diffusion model）扩展数据集，包含诸如双目、广角、微距和特写等多种图像类型；另一方面，我们推荐使用 ControlNet 对图像进行增强，以评估并提升相应数据集的质量，从而促进对海洋生态系统的研究。  \n标签 - 水下图像，去噪扩散，海洋生态系统，ControlNet"
    },
    {
        "title": "Defense against Unauthorized Distillation in Image Restoration via\n  Feature Space Perturbation",
        "url": "http://arxiv.org/abs/2510.08925v1",
        "pub_date": "2025-10-10",
        "summary": "Knowledge distillation (KD) attacks pose a significant threat to deep model intellectual property by enabling adversaries to train student networks using a teacher model's outputs. While recent defenses in image classification have successfully disrupted KD by perturbing output probabilities, extending these methods to image restoration is difficult. Unlike classification, restoration is a generative task with continuous, high-dimensional outputs that depend on spatial coherence and fine details. Minor perturbations are often insufficient, as students can still learn the underlying mapping.To address this, we propose Adaptive Singular Value Perturbation (ASVP), a runtime defense tailored for image restoration models. ASVP operates on internal feature maps of the teacher using singular value decomposition (SVD). It amplifies the topk singular values to inject structured, high-frequency perturbations, disrupting the alignment needed for distillation. This hinders student learning while preserving the teacher's output quality.We evaluate ASVP across five image restoration tasks: super-resolution, low-light enhancement, underwater enhancement, dehazing, and deraining. Experiments show ASVP reduces student PSNR by up to 4 dB and SSIM by 60-75%, with negligible impact on the teacher's performance. Compared to prior methods, ASVP offers a stronger and more consistent defense.Our approach provides a practical solution to protect open-source restoration models from unauthorized knowledge distillation.",
        "translated": "知识蒸馏（Knowledge Distillation, KD）攻击通过使对手能够利用教师模型的输出来训练学生网络，对深度模型的知识产权构成了重大威胁。尽管图像分类领域的最新防御方法已成功通过扰动输出概率来破坏知识蒸馏，但将这些方法扩展到图像恢复任务却面临困难。与分类任务不同，图像恢复是一个生成任务，其输出是连续且高维的，依赖于空间一致性和细节质量。因此，微小的扰动往往不足以阻止知识蒸馏，因为学生模型仍能学习到潜在的映射关系。\n\n为了解决这一问题，我们提出了一种专为图像恢复模型设计的运行时防御方法——自适应奇异值扰动（Adaptive Singular Value Perturbation, ASVP）。ASVP 通过奇异值分解（Singular Value Decomposition, SVD）对教师模型的内部特征图进行操作。该方法通过放大前k个奇异值，注入结构化的高频扰动，从而破坏蒸馏过程中所需的对齐关系。这种扰动在阻碍学生模型学习的同时，保持了教师模型的输出质量。\n\n我们在五个图像恢复任务中评估了 ASVP：超分辨率、低光增强、水下增强、去雾和去雨。实验结果表明，ASVP 最多可使学生模型的 PSNR 降低 4 dB，SSIM 降低 60-75%，而对教师模型的性能影响几乎可以忽略。与现有方法相比，ASVP 提供了更强且更一致的防御效果。我们的方法为保护开源图像恢复模型免受未经授权的知识蒸馏提供了一个实用的解决方案。"
    },
    {
        "title": "Latent Harmony: Synergistic Unified UHD Image Restoration via Latent\n  Space Regularization and Controllable Refinement",
        "url": "http://arxiv.org/abs/2510.07961v2",
        "pub_date": "2025-10-09",
        "summary": "Ultra-High Definition (UHD) image restoration faces a trade-off between computational efficiency and high-frequency detail retention. While Variational Autoencoders (VAEs) improve efficiency via latent-space processing, their Gaussian constraint often discards degradation-specific high-frequency information, hurting reconstruction fidelity. To overcome this, we propose Latent Harmony, a two-stage framework that redefines VAEs for UHD restoration by jointly regularizing the latent space and enforcing high-frequency-aware reconstruction.In Stage One, we introduce LH-VAE, which enhances semantic robustness through visual semantic constraints and progressive degradation perturbations, while latent equivariance strengthens high-frequency reconstruction.Stage Two jointly trains this refined VAE with a restoration model using High-Frequency Low-Rank Adaptation (HF-LoRA): an encoder LoRA guided by a fidelity-oriented high-frequency alignment loss to recover authentic details, and a decoder LoRA driven by a perception-oriented loss to synthesize realistic textures. Both LoRA modules are trained via alternating optimization with selective gradient propagation to preserve the pretrained latent structure.At inference, a tunable parameter {\\alpha} enables flexible fidelity-perception trade-offs.Experiments show Latent Harmony achieves state-of-the-art performance across UHD and standard-resolution tasks, effectively balancing efficiency, perceptual quality, and reconstruction accuracy.",
        "translated": "超高分辨率（UHD）图像修复面临计算效率与高频细节保留之间的权衡。尽管变分自编码器（VAEs）通过潜在空间处理提升了效率，但其高斯约束常常会丢弃与退化相关的高频信息，从而损害重建的保真度。为了解决这一问题，我们提出了**Latent Harmony**，一种两阶段框架，通过联合正则化潜在空间并强制实现高频感知的重建，重新定义了用于UHD图像修复的VAEs。  \n\n在第一阶段中，我们引入了**LH-VAE**，该模型通过视觉语义约束和渐进式退化扰动来增强语义鲁棒性，同时利用潜在等变性来强化高频信息的重建。  \n\n在第二阶段中，我们将优化后的VAE与一个修复模型联合训练，采用**高频低秩适配（High-Frequency Low-Rank Adaptation, HF-LoRA）**方法：一个编码器LoRA通过以保真度为导向的高频对齐损失（high-frequency alignment loss）来恢复真实细节，而一个解码器LoRA则由以感知为导向的损失驱动，以合成逼真的纹理。两个LoRA模块通过交替优化与选择性梯度传播进行训练，从而保持预训练的潜在结构不变。  \n\n在推理阶段，一个可调节的参数 $\\alpha$ 可实现保真度与感知质量之间的灵活权衡。实验表明，**Latent Harmony**在UHD和标准分辨率任务中均达到了最先进的性能，有效平衡了计算效率、感知质量与重建精度。"
    },
    {
        "title": "PhyDAE: Physics-Guided Degradation-Adaptive Experts for All-in-One\n  Remote Sensing Image Restoration",
        "url": "http://arxiv.org/abs/2510.08653v1",
        "pub_date": "2025-10-09",
        "summary": "Remote sensing images inevitably suffer from various degradation factors during acquisition, including atmospheric interference, sensor limitations, and imaging conditions. These complex and heterogeneous degradations pose severe challenges to image quality and downstream interpretation tasks. Addressing limitations of existing all-in-one restoration methods that overly rely on implicit feature representations and lack explicit modeling of degradation physics, this paper proposes Physics-Guided Degradation-Adaptive Experts (PhyDAE). The method employs a two-stage cascaded architecture transforming degradation information from implicit features into explicit decision signals, enabling precise identification and differentiated processing of multiple heterogeneous degradations including haze, noise, blur, and low-light conditions. The model incorporates progressive degradation mining and exploitation mechanisms, where the Residual Manifold Projector (RMP) and Frequency-Aware Degradation Decomposer (FADD) comprehensively analyze degradation characteristics from manifold geometry and frequency perspectives. Physics-aware expert modules and temperature-controlled sparse activation strategies are introduced to enhance computational efficiency while ensuring imaging physics consistency. Extensive experiments on three benchmark datasets (MD-RSID, MD-RRSHID, and MDRS-Landsat) demonstrate that PhyDAE achieves superior performance across all four restoration tasks, comprehensively outperforming state-of-the-art methods. Notably, PhyDAE substantially improves restoration quality while achieving significant reductions in parameter count and computational complexity, resulting in remarkable efficiency gains compared to mainstream approaches and achieving optimal balance between performance and efficiency. Code is available at https://github.com/HIT-SIRS/PhyDAE.",
        "translated": "遥感图像在获取过程中不可避免地受到多种退化因素的影响，包括大气干扰、传感器限制以及成像条件等。这些复杂且异质的退化现象给图像质量以及后续的语义解析任务带来了严重挑战。为了解决现有端到端图像修复方法中对隐式特征表示的过度依赖以及缺乏对退化物理机制的显式建模等问题，本文提出了一种物理引导的退化自适应专家模型（Physics-Guided Degradation-Adaptive Experts, PhyDAE）。该方法采用两阶段级联架构，将隐式特征中的退化信息转化为显式的决策信号，从而实现对多种异质退化（包括雾霾、噪声、模糊和低光照条件）的精确识别与差异化处理。模型引入了渐进式退化挖掘与利用机制，其中残差流形投影器（Residual Manifold Projector, RMP）和频域感知退化解耦模块（Frequency-Aware Degradation Decomposer, FADD）分别从流形几何结构和频域角度对退化特性进行全面分析。此外，通过引入物理感知的专家模块和温度控制的稀疏激活策略，在保证成像物理一致性的同时显著提升了计算效率。在三个基准数据集（MD-RSID、MD-RRSHID 和 MDRS-Landsat）上的大量实验表明，PhyDAE 在四项图像修复任务中均表现出优越的性能，全面超越现有最先进方法。特别值得一提的是，PhyDAE 在大幅提升修复质量的同时，显著减少了模型参数数量与计算复杂度，从而在主流方法中实现了性能与效率之间的最优平衡。代码可在 https://github.com/HIT-SIRS/PhyDAE 获取。"
    },
    {
        "title": "DeRainMamba: A Frequency-Aware State Space Model with Detail Enhancement\n  for Image Deraining",
        "url": "http://arxiv.org/abs/2510.06746v1",
        "pub_date": "2025-10-08",
        "summary": "Image deraining is crucial for improving visual quality and supporting reliable downstream vision tasks. Although Mamba-based models provide efficient sequence modeling, their limited ability to capture fine-grained details and lack of frequency-domain awareness restrict further improvements. To address these issues, we propose DeRainMamba, which integrates a Frequency-Aware State-Space Module (FASSM) and Multi-Directional Perception Convolution (MDPConv). FASSM leverages Fourier transform to distinguish rain streaks from high-frequency image details, balancing rain removal and detail preservation. MDPConv further restores local structures by capturing anisotropic gradient features and efficiently fusing multiple convolution branches. Extensive experiments on four public benchmarks demonstrate that DeRainMamba consistently outperforms state-of-the-art methods in PSNR and SSIM, while requiring fewer parameters and lower computational costs. These results validate the effectiveness of combining frequency-domain modeling and spatial detail enhancement within a state-space framework for single image deraining.",
        "translated": "图像去雨对于提升视觉质量和支持可靠的下游视觉任务至关重要。尽管基于Mamba的模型在序列建模方面具有较高的效率，但其在捕捉细粒度细节方面的能力有限以及缺乏频域感知，限制了进一步的性能提升。为解决这些问题，我们提出DeRainMamba，该方法融合了一个频域感知状态空间模块（Frequency-Aware State-Space Module, FASSM）和多方向感知卷积（Multi-Directional Perception Convolution, MDPConv）。FASSM利用傅里叶变换区分雨痕与图像中的高频细节，从而在去雨与细节保留之间取得平衡。MDPConv通过捕捉各向异性梯度特征并高效融合多分支卷积，进一步恢复局部结构。我们在四个公开基准数据集上进行了广泛的实验，结果表明DeRainMamba在PSNR和SSIM指标上始终优于当前最先进的方法，同时参数量更少，计算成本更低。这些结果验证了在状态空间框架中结合频域建模与空间细节增强对于单图像去雨的有效性。"
    },
    {
        "title": "An Inertial Langevin Algorithm",
        "url": "http://arxiv.org/abs/2510.06723v1",
        "pub_date": "2025-10-08",
        "summary": "We present a novel method for drawing samples from Gibbs distributions with densities of the form $\\pi(x) \\propto \\exp(-U(x))$. The method accelerates the unadjusted Langevin algorithm by introducing an inertia term similar to Polyak's heavy ball method, together with a corresponding noise rescaling. Interpreting the scheme as a discretization of \\emph{kinetic} Langevin dynamics, we prove ergodicity (in continuous and discrete time) for twice continuously differentiable, strongly convex, and $L$-smooth potentials and bound the bias of the discretization to the target in Wasserstein-2 distance. In particular, the presented proofs allow for smaller friction parameters in the kinetic Langevin diffusion compared to existing literature. Moreover, we show the close ties of the proposed method to the over-relaxed Gibbs sampler. The scheme is tested in an extensive set of numerical experiments covering simple toy examples, total variation image denoising, and the complex task of maximum likelihood learning of an energy-based model for molecular structure generation. The experimental results confirm the acceleration provided by the proposed scheme even beyond the strongly convex and $L$-smooth setting.",
        "translated": "我们提出了一种从形式为 $\\pi(x) \\propto \\exp(-U(x))$ 的 Gibbs 分布中抽样的新方法。该方法通过引入类似于 Polyak 重球法的惯性项以及相应的噪声重缩放机制，加速了未调整的 Langevin 算法。将该方法解释为对 \\emph{动能} Langevin 动力学的离散化，我们证明了在连续和离散时间下，对于二次连续可微、强凸且 $L$-光滑的势函数，该方法具有遍历性，并在 Wasserstein-2 距离下对该离散化方案与目标分布之间的偏差进行了上界分析。特别地，所提出的证明允许在动能 Langevin 扩散中使用比现有文献中更小的摩擦参数。此外，我们展示了该方法与过松弛 Gibbs 抽样器之间的紧密联系。该算法在一个广泛的数值实验中进行了测试，涵盖简单的玩具示例、图像的全变分去噪任务，以及分子结构生成的能量基模型的最大似然学习这一复杂任务。实验结果验证了所提出方法即使在非强凸和非 $L$-光滑的设置下仍能提供加速效果。"
    },
    {
        "title": "AIM 2025 Challenge on Real-World RAW Image Denoising",
        "url": "http://arxiv.org/abs/2510.06601v1",
        "pub_date": "2025-10-08",
        "summary": "We introduce the AIM 2025 Real-World RAW Image Denoising Challenge, aiming to advance efficient and effective denoising techniques grounded in data synthesis. The competition is built upon a newly established evaluation benchmark featuring challenging low-light noisy images captured in the wild using five different DSLR cameras. Participants are tasked with developing novel noise synthesis pipelines, network architectures, and training methodologies to achieve high performance across different camera models. Winners are determined based on a combination of performance metrics, including full-reference measures (PSNR, SSIM, LPIPS), and non-reference ones (ARNIQA, TOPIQ). By pushing the boundaries of camera-agnostic low-light RAW image denoising trained on synthetic data, the competition promotes the development of robust and practical models aligned with the rapid progress in digital photography. We expect the competition outcomes to influence multiple domains, from image restoration to night-time autonomous driving.",
        "translated": "我们介绍了AIM 2025真实世界RAW图像去噪挑战赛，旨在通过数据合成推动高效且有效的去噪技术的发展。该竞赛基于一个新的评估基准构建，该基准包含在自然场景下使用五种不同DSLR相机拍摄的具有挑战性的低光照噪声图像。参赛者需要开发新颖的噪声合成流程、网络架构和训练方法，以在不同相机模型上实现高性能的去噪效果。比赛的优胜者将根据多种性能指标综合评定，包括全参考指标（PSNR、SSIM、LPIPS）和非参考指标（ARNIQA、TOPIQ）。通过推动在合成数据上训练的、具有相机泛化能力的低光照RAW图像去噪技术的边界，该竞赛促进了与数字摄影快速进步相契合的鲁棒且实用模型的发展。我们预期本次竞赛的成果将对多个领域产生影响，从图像修复到夜间自动驾驶。"
    },
    {
        "title": "TDiff: Thermal Plug-And-Play Prior with Patch-Based Diffusion",
        "url": "http://arxiv.org/abs/2510.06460v1",
        "pub_date": "2025-10-07",
        "summary": "Thermal images from low-cost cameras often suffer from low resolution, fixed pattern noise, and other localized degradations. Available datasets for thermal imaging are also limited in both size and diversity. To address these challenges, we propose a patch-based diffusion framework (TDiff) that leverages the local nature of these distortions by training on small thermal patches. In this approach, full-resolution images are restored by denoising overlapping patches and blending them using smooth spatial windowing. To our knowledge, this is the first patch-based diffusion framework that models a learned prior for thermal image restoration across multiple tasks. Experiments on denoising, super-resolution, and deblurring demonstrate strong results on both simulated and real thermal data, establishing our method as a unified restoration pipeline.",
        "translated": "低成本热成像相机所获取的图像通常存在分辨率较低、固定模式噪声以及其他局部退化问题。目前可用的热成像数据集在规模和多样性方面也较为有限。为了解决这些挑战，我们提出了一种基于图像块的扩散框架（TDiff），该框架通过在小尺寸热图像块上进行训练，利用这些退化现象的局部特性。在该方法中，通过去噪重叠图像块，并结合平滑的空间窗口函数进行融合，从而恢复全分辨率图像。据我们所知，这是首个基于图像块的扩散框架，能够在多个任务中对热图像的退化建模并学习其先验分布。我们在去噪、超分辨率和去模糊任务上的实验表明，该方法在模拟和真实热图像数据上均取得了优异的效果，从而确立了其作为统一图像恢复流程的地位。"
    },
    {
        "title": "Local MAP Sampling for Diffusion Models",
        "url": "http://arxiv.org/abs/2510.07343v2",
        "pub_date": "2025-10-07",
        "summary": "Diffusion Posterior Sampling (DPS) provides a principled Bayesian approach to inverse problems by sampling from $p(x_0 \\mid y)$. However, in practice, the goal of inverse problem solving is not to cover the posterior but to recover the most accurate reconstruction, where optimization-based diffusion solvers often excel despite lacking a clear probabilistic foundation. We introduce Local MAP Sampling (LMAPS), a new inference framework that iteratively solving local MAP subproblems along the diffusion trajectory. This perspective clarifies their connection to global MAP estimation and DPS, offering a unified probabilistic interpretation for optimization-based methods. Building on this foundation, we develop practical algorithms with a probabilistically interpretable covariance approximation, a reformulated objective for stability and interpretability, and a gradient approximation for non-differentiable operators. Across a broad set of image restoration and scientific tasks, LMAPS achieves state-of-the-art performance, including $\\geq 2$ dB gains on motion deblurring, JPEG restoration, and quantization, and $&gt;1.5$ dB improvements on inverse scattering benchmarks.",
        "translated": "扩散后验采样（Diffusion Posterior Sampling, DPS）通过从 $p(x_0 \\mid y)$ 中采样，为逆问题提供了一种基于贝叶斯原理的求解方法。然而，实际上，解决逆问题的目标并非是覆盖后验分布，而是恢复最精确的重建结果，在这一点上，基于优化的扩散求解器往往表现出色，尽管它们缺乏明确的概率基础。我们提出一种新的推理框架——局部最大后验采样（Local MAP Sampling, LMAPS），该方法沿着扩散轨迹迭代求解局部最大后验（MAP）子问题。这一视角明确了LMAPS与全局MAP估计以及DPS之间的关系，为基于优化的方法提供了一种统一的概率解释。基于这一理论基础，我们开发了具有概率解释的协方差近似方法，提出了一个用于提高稳定性和可解释性的重构目标函数，并引入了针对不可微操作符的梯度近似方法。在广泛的图像恢复和科学计算任务中，LMAPS实现了最先进的性能，包括在运动去模糊、JPEG图像恢复和量化任务中获得了 $\\geq 2$ dB 的提升，在逆散射基准测试中实现了 $>1.5$ dB 的性能改进。"
    },
    {
        "title": "Rasterized Steered Mixture of Experts for Efficient 2D Image Regression",
        "url": "http://arxiv.org/abs/2510.05814v1",
        "pub_date": "2025-10-07",
        "summary": "The Steered Mixture of Experts regression framework has demonstrated strong performance in image reconstruction, compression, denoising, and super-resolution. However, its high computational cost limits practical applications. This work introduces a rasterization-based optimization strategy that combines the efficiency of rasterized Gaussian kernel rendering with the edge-aware gating mechanism of the Steered Mixture of Experts. The proposed method is designed to accelerate two-dimensional image regression while maintaining the model's inherent sparsity and reconstruction quality. By replacing global iterative optimization with a rasterized formulation, the method achieves significantly faster parameter updates and more memory-efficient model representations. In addition, the proposed framework supports applications such as native super-resolution and image denoising, which are not directly achievable with standard rasterized Gaussian kernel approaches. The combination of fast rasterized optimization with the edge-aware structure of the Steered Mixture of Experts provides a new balance between computational efficiency and reconstruction fidelity for two-dimensional image processing tasks.",
        "translated": "基于引导的专家混合（Steered Mixture of Experts）回归框架在图像重建、压缩、去噪和超分辨率等任务中已展现出优异的性能。然而，其较高的计算成本限制了其在实际应用中的使用。本文提出了一种基于光栅化的优化策略，结合了光栅化高斯核渲染的高效性与Steered Mixture of Experts中边缘感知门控机制的优势。所提出的方法旨在加速二维图像回归过程，同时保持模型固有的稀疏性与重建质量。通过将全局迭代优化替换为光栅化形式，该方法显著提高了参数更新速度，并实现了更节省内存的模型表示。此外，该框架支持诸如原生超分辨率和图像去噪等应用，而这些是标准光栅化高斯核方法无法直接实现的。快速光栅化优化与Steered Mixture of Experts边缘感知结构的结合，为二维图像处理任务提供了计算效率与重建保真度之间的新平衡。"
    },
    {
        "title": "Adaptive double-phase Rudin--Osher--Fatemi denoising model",
        "url": "http://arxiv.org/abs/2510.04382v1",
        "pub_date": "2025-10-05",
        "summary": "We propose a new image denoising model based on a variable-growth total variation regularization of double-phase type with adaptive weight. It is designed to reduce staircasing with respect to the classical Rudin--Osher--Fatemi model, while preserving the edges of the image in a similar fashion. We implement the model and test its performance on synthetic and natural images in 1D and 2D over a range of noise levels.",
        "translated": "我们提出了一种新的图像去噪模型，该模型基于具有自适应权重的双阶段可变增长总体变差正则化。该模型旨在相较于经典的 Rudin–Osher–Fatemi 模型，减少阶梯效应（staircasing），同时以类似的方式保留图像的边缘。我们对该模型进行了实现，并在 1D 和 2D 的合成图像与自然图像上，针对多种噪声水平进行了性能测试。"
    },
    {
        "title": "Equilibrium Matching: Generative Modeling with Implicit Energy-Based\n  Models",
        "url": "http://arxiv.org/abs/2510.02300v3",
        "pub_date": "2025-10-02",
        "summary": "We introduce Equilibrium Matching (EqM), a generative modeling framework built from an equilibrium dynamics perspective. EqM discards the non-equilibrium, time-conditional dynamics in traditional diffusion and flow-based generative models and instead learns the equilibrium gradient of an implicit energy landscape. Through this approach, we can adopt an optimization-based sampling process at inference time, where samples are obtained by gradient descent on the learned landscape with adjustable step sizes, adaptive optimizers, and adaptive compute. EqM surpasses the generation performance of diffusion/flow models empirically, achieving an FID of 1.90 on ImageNet 256$\\times$256. EqM is also theoretically justified to learn and sample from the data manifold. Beyond generation, EqM is a flexible framework that naturally handles tasks including partially noised image denoising, OOD detection, and image composition. By replacing time-conditional velocities with a unified equilibrium landscape, EqM offers a tighter bridge between flow and energy-based models and a simple route to optimization-driven inference.",
        "translated": "我们提出了一种名为**均衡匹配**（Equilibrium Matching, EqM）的生成建模框架，其构建基于**均衡动力学**的视角。EqM摒弃了传统扩散模型和基于流的生成模型中所依赖的非均衡、时序条件的动力学，转而学习一个隐式能量景观的**均衡梯度**。通过这种方法，我们可以在推理阶段采用基于优化的采样过程，其中样本通过在所学能量景观上进行梯度下降获得，该过程支持可调节的步长、自适应优化器以及自适应计算能力。从实证结果来看，EqM在生成性能上超越了扩散模型和流模型，在ImageNet 256$\\times$256数据集上达到了1.90的FID分数。EqM在理论上也能够从数据流形中进行学习与采样，具有坚实的理论依据。除生成任务外，EqM还是一种灵活的框架，天然地适用于包括部分噪声图像去噪、分布外（OOD）检测以及图像合成在内的多种任务。通过将时序条件的速度替换为统一的均衡景观，EqM在流模型与基于能量的模型之间架起了更紧密的桥梁，并为实现优化驱动的推理提供了一条简洁的路径。"
    },
    {
        "title": "HyMiRec: A Hybrid Multi-interest Learning Framework for LLM-based\n  Sequential Recommendation",
        "url": "http://arxiv.org/abs/2510.13738v1",
        "pub_date": "2025-10-15",
        "summary": "Large language models (LLMs) have recently demonstrated strong potential for sequential recommendation. However, current LLM-based approaches face critical limitations in modeling users' long-term and diverse interests. First, due to inference latency and feature fetching bandwidth constraints, existing methods typically truncate user behavior sequences to include only the most recent interactions, resulting in the loss of valuable long-range preference signals. Second, most current methods rely on next-item prediction with a single predicted embedding, overlooking the multifaceted nature of user interests and limiting recommendation diversity. To address these challenges, we propose HyMiRec, a hybrid multi-interest sequential recommendation framework, which leverages a lightweight recommender to extracts coarse interest embeddings from long user sequences and an LLM-based recommender to captures refined interest embeddings. To alleviate the overhead of fetching features, we introduce a residual codebook based on cosine similarity, enabling efficient compression and reuse of user history embeddings. To model the diverse preferences of users, we design a disentangled multi-interest learning module, which leverages multiple interest queries to learn disentangles multiple interest signals adaptively, allowing the model to capture different facets of user intent. Extensive experiments are conducted on both benchmark datasets and a collected industrial dataset, demonstrating our effectiveness over existing state-of-the-art methods. Furthermore, online A/B testing shows that HyMiRec brings consistent improvements in real-world recommendation systems.",
        "translated": "大语言模型（LLMs）最近在序列推荐中展现出强大的潜力。然而，目前基于LLM的方法在建模用户长期和多样化兴趣方面面临关键的限制。首先，由于推理延迟和特征获取带宽的限制，现有方法通常截断用户行为序列，仅包含最近的交互，从而丢失了有价值的长距离偏好信号。其次，大多数当前方法依赖于单一预测嵌入的下一物品预测，忽视了用户兴趣的多面性，限制了推荐的多样性。为了解决这些挑战，我们提出HyMiRec，一种混合多兴趣序列推荐框架，其利用一个轻量级推荐器从长用户序列中提取粗兴趣嵌入，并利用一个基于LLM的推荐器捕捉精兴趣嵌入。为了缓解特征获取带来的开销，我们引入了一个基于余弦相似度的残差码本，从而实现了用户历史嵌入的高效压缩和重复使用。为了建模用户的多样化偏好，我们设计了一个解耦的多兴趣学习模块，该模块利用多个兴趣查询自适应地学习解耦的多个兴趣信号，使得模型能够捕捉用户意图的不同方面。我们在基准数据集和收集的工业数据集上进行了广泛的实验，验证了该方法相较于现有最先进方法的有效性。此外，在线A/B测试表明，HyMiRec在实际推荐系统中带来了持续的性能提升。",
        "translated_title": "HyMiRec：一种基于大语言模型的序列推荐混合多兴趣学习框架",
        "label": [
            "LLM生成式推荐",
            "序列推荐",
            "通用推荐技术"
        ],
        "label_reason": "结合LLM与多兴趣学习的序列推荐框架，直接解决推荐系统核心问题",
        "relevance_score": 9
    },
    {
        "title": "RAG Meets Temporal Graphs: Time-Sensitive Modeling and Retrieval for\n  Evolving Knowledge",
        "url": "http://arxiv.org/abs/2510.13590v1",
        "pub_date": "2025-10-15",
        "summary": "Knowledge is inherently time-sensitive and continuously evolves over time. Although current Retrieval-Augmented Generation (RAG) systems enrich LLMs with external knowledge, they largely ignore this temporal nature. This raises two challenges for RAG. First, current RAG methods lack effective time-aware representations. Same facts of different time are difficult to distinguish with vector embeddings or conventional knowledge graphs. Second, most RAG evaluations assume a static corpus, leaving a blind spot regarding update costs and retrieval stability as knowledge evolves. To make RAG time-aware, we propose Temporal GraphRAG (TG-RAG), which models external corpora as a bi-level temporal graph consisting of a temporal knowledge graph with timestamped relations and a hierarchical time graph. Multi-granularity temporal summaries are generated for each time node to capture both key events and broader trends at that time. The design supports incremental updates by extracting new temporal facts from the incoming corpus and merging them into the existing graph. The temporal graph explicitly represents identical facts at different times as distinct edges to avoid ambiguity, and the time hierarchy graph allows only generating reports for new leaf time nodes and their ancestors, ensuring effective and efficient updates. During inference, TG-RAG dynamically retrieves a subgraph within the temporal and semantic scope of the query, enabling precise evidence gathering. Moreover, we introduce ECT-QA, a time-sensitive question-answering dataset featuring both specific and abstract queries, along with a comprehensive evaluation protocol designed to assess incremental update capabilities of RAG systems. Extensive experiments show that TG-RAG significantly outperforms existing baselines, demonstrating the effectiveness of our method in handling temporal knowledge and incremental updates.",
        "translated": "知识本质上是时间敏感的，并且会随着时间不断演化。尽管当前的检索增强生成（RAG）系统通过引入外部知识丰富了大语言模型（LLM），但它们在很大程度上忽视了这种时间特性。这引发了RAG面临的两个挑战。首先，当前的RAG方法缺乏有效的时间感知表示。不同时间下的相同事实，难以通过向量嵌入（embedding）或传统知识图谱进行区分。其次，大多数RAG评估假设语料库是静态的，从而忽略了知识演化过程中的更新成本和检索稳定性问题。为了使RAG具备时间感知能力，我们提出了时序图RAG（Temporal GraphRAG，TG-RAG），该方法将外部语料库建模为一个双层时序图，包含一个具有时间戳关系的时序知识图谱和一个层次化时间图。为每个时间节点生成多粒度时序摘要，以捕捉该时间点的关键事件和更广泛的趋势。该设计通过从新输入的语料中提取新的时序事实并将其合并到现有图中，支持增量更新。时序图显式地将不同时刻的相同事实表示为不同的边，以避免歧义，而时间层次图则仅允许为新的叶子时间节点及其祖先生成报告，从而确保更新的有效性和高效性。在推理阶段，TG-RAG能够动态检索与查询在时间和语义范围内匹配的子图，实现精确的证据收集。此外，我们引入了一个时间敏感的问答数据集ECT-QA，包含具体和抽象的查询，并设计了一套全面的评估协议，用于评估RAG系统的增量更新能力。大量实验表明，TG-RAG显著优于现有基线，验证了我们方法在处理时序知识和增量更新方面的有效性。",
        "translated_title": "RAG与时间图的结合：面向动态知识的时间敏感建模与召回",
        "label": [
            "LLM生成式推荐",
            "多模态推荐",
            "推荐系统评估"
        ],
        "label_reason": "论文提出时间敏感的RAG方法，适用于生成式推荐中的动态知识建模与评估。",
        "relevance_score": 7
    },
    {
        "title": "MADREC: A Multi-Aspect Driven LLM Agent for Explainable and Adaptive\n  Recommendation",
        "url": "http://arxiv.org/abs/2510.13371v1",
        "pub_date": "2025-10-15",
        "summary": "Recent attempts to integrate large language models (LLMs) into recommender systems have gained momentum, but most remain limited to simple text generation or static prompt-based inference, failing to capture the complexity of user preferences and real-world interactions. This study proposes the Multi-Aspect Driven LLM Agent MADRec, an autonomous LLM-based recommender that constructs user and item profiles by unsupervised extraction of multi-aspect information from reviews and performs direct recommendation, sequential recommendation, and explanation generation. MADRec generates structured profiles via aspect-category-based summarization and applies Re-Ranking to construct high-density inputs. When the ground-truth item is missing from the output, the Self-Feedback mechanism dynamically adjusts the inference criteria. Experiments across multiple domains show that MADRec outperforms traditional and LLM-based baselines in both precision and explainability, with human evaluation further confirming the persuasiveness of the generated explanations.",
        "translated": "近期尝试将大语言模型（LLMs）集成到推荐系统中的工作逐渐增多，但大多数仍局限于简单的文本生成或基于静态提示的推理，未能捕捉用户偏好和现实世界交互的复杂性。本研究提出多方面驱动的大语言模型代理 MADRec，这是一种基于大语言模型的自主推荐系统，通过从评论中无监督提取多方面信息构建用户和物料画像，并执行直接推荐、序列推荐以及解释生成。MADRec 通过基于方面类别的摘要生成结构化画像，并应用重排（Re-Ranking）构造高密度输入。当输出中缺少真实物料时，自反馈（Self-Feedback）机制会动态调整推理标准。跨多个领域的实验表明，MADRec 在准确性和可解释性方面均优于传统方法和基于 LLM 的基线模型，人工评估进一步验证了所生成解释的说服力。",
        "translated_title": "MADREC：一种面向多方面驱动的可解释且自适应的推荐大语言模型代理",
        "label": [
            "LLM生成式推荐",
            "精排",
            "重排",
            "推荐系统可解释性"
        ],
        "label_reason": "基于LLM的推荐与可解释性，融合重排与反馈机制",
        "relevance_score": 9
    },
    {
        "title": "Improving Visual Recommendation on E-commerce Platforms Using\n  Vision-Language Models",
        "url": "http://arxiv.org/abs/2510.13359v1",
        "pub_date": "2025-10-15",
        "summary": "On large-scale e-commerce platforms with tens of millions of active monthly users, recommending visually similar products is essential for enabling users to efficiently discover items that align with their preferences. This study presents the application of a vision-language model (VLM) -- which has demonstrated strong performance in image recognition and image-text retrieval tasks -- to product recommendations on Mercari, a major consumer-to-consumer marketplace used by more than 20 million monthly users in Japan. Specifically, we fine-tuned SigLIP, a VLM employing a sigmoid-based contrastive loss, using one million product image-title pairs from Mercari collected over a three-month period, and developed an image encoder for generating item embeddings used in the recommendation system. Our evaluation comprised an offline analysis of historical interaction logs and an online A/B test in a production environment. In offline analysis, the model achieved a 9.1% improvement in nDCG@5 compared with the baseline. In the online A/B test, the click-through rate improved by 50% whereas the conversion rate improved by 14% compared with the existing model. These results demonstrate the effectiveness of VLM-based encoders for e-commerce product recommendations and provide practical insights into the development of visual similarity-based recommendation systems.",
        "translated": "在拥有数千万活跃月活用户的大型电商平台中，推荐视觉相似产品对于帮助用户高效发现与其偏好一致的物料至关重要。本研究提出了将视觉-语言模型（VLM）应用于Mercari平台的产品推荐，其中VLM已在图像识别和图文检索任务中展现出卓越的性能。Mercari是日本一个重要的C2C电商平台，月活跃用户超过2000万。具体而言，我们使用在三个月内从Mercari收集的包含一百万对产品图像和标题的数据，对采用基于Sigmoid的对比损失的VLM SigLIP进行了微调，并开发了用于生成推荐系统中物料嵌入表示的图像编码器。我们的评估包括对历史交互日志的离线分析以及在生产环境中进行的在线A/B测试。离线分析结果显示，与基线模型相比，该模型在nDCG@5指标上提升了9.1%。在线A/B测试中，点击率提升了50%，转化率提升了14%。这些结果证明了基于VLM编码器在电商产品推荐中的有效性，并为视觉相似性推荐系统的发展提供了实践洞见。",
        "translated_title": "使用视觉-语言模型改进电子商务平台的视觉推荐",
        "label": [
            "多模态推荐",
            "精排",
            "图像相似性推荐"
        ],
        "label_reason": "论文将视觉语言模型用于电商推荐，提升图像相似性产品推荐效果",
        "relevance_score": 8
    },
    {
        "title": "ChatR1: Reinforcement Learning for Conversational Reasoning and\n  Retrieval Augmented Question Answering",
        "url": "http://arxiv.org/abs/2510.13312v1",
        "pub_date": "2025-10-15",
        "summary": "We present ChatR1, a reasoning framework based on reinforcement learning (RL) for conversational question answering (CQA). Reasoning plays an important role in CQA, where user intent evolves across dialogue turns, and utterances are often underspecified, requiring contextual interpretation, query reformulation, and dynamic coordination between retrieval and generation. Unlike static `rewrite, retrieve, and generate' pipelines, ChatR1 interleaves search and reasoning across turns, enabling exploratory and adaptive behaviors learned through RL. To address the challenge of sparse and delayed rewards in RL, we propose an intent-aware reward that provides turn-level feedback by aligning retrieval and reasoning with evolving user goals. Our proposed ChatR1 demonstrates strong performance on both 3B and 7B model backbones, outperforming competitive models on five CQA datasets, measured by different metrics (F1, BERTScore, and LLM-as-judge). We include a diverse set of CQA datasets to cover topic shifts, evolving intents, mixed-initiative dialogues, and multi-document grounding, testing ChatR1's performance from various aspects. Ablation studies confirm the effectiveness of the intent-aware reward. Our analyses further reveal diverse reasoning trajectories and effective use of the search tool. ChatR1 also generalizes robustly across domains, demonstrating that RL-based reasoning enables more flexible and context-sensitive behavior than static CQA pipelines.",
        "translated": "我们提出了 ChatR1，一种基于强化学习（RL）的对话式问答（CQA）推理框架。推理在 CQA 中起着重要作用，因为用户意图会随着对话轮次而演变，且对话内容通常信息不完整，需要上下文解释、查询重构，以及检索与生成之间的动态协调。与静态的“重写、检索、生成”流水线不同，ChatR1 在对话轮次中交替进行搜索和推理，使得通过 RL 学到的探索性和适应性行为得以实现。为了解决 RL 中稀疏和延迟奖励的挑战，我们提出了一种意图感知的奖励机制，通过将检索和推理与用户意图的演变对齐，提供轮次级的反馈。我们提出的 ChatR1 在 3B 和 7B 模型主干上均表现出色，在五个 CQA 数据集上的表现优于多个竞争模型，评估指标包括 F1、BERTScore 和以大语言模型作为评判者。我们纳入了多样化的 CQA 数据集，涵盖主题转换、意图演变、混合倡议对话以及多文档依据，从多个方面测试了 ChatR1 的性能。消融实验验证了意图感知奖励的有效性。进一步的分析还揭示了多样化的推理轨迹和对搜索工具的有效利用。ChatR1 在多个领域上也表现出良好的泛化能力，表明基于 RL 的推理能够实现比静态 CQA 流水线更为灵活和上下文敏感的行为。",
        "translated_title": "ChatR1：会话推理与检索增强问答的强化学习方法",
        "label": [
            "LLM生成式推荐",
            "序列推荐"
        ],
        "label_reason": "论文涉及对话式问答与生成式模型，适用于推荐中的序列建模和生成式推荐。",
        "relevance_score": 7
    },
    {
        "title": "Beyond Static LLM Policies: Imitation-Enhanced Reinforcement Learning\n  for Recommendation",
        "url": "http://arxiv.org/abs/2510.13229v1",
        "pub_date": "2025-10-15",
        "summary": "Recommender systems (RecSys) have become critical tools for enhancing user engagement by delivering personalized content across diverse digital platforms. Recent advancements in large language models (LLMs) demonstrate significant potential for improving RecSys, primarily due to their exceptional generalization capabilities and sophisticated contextual understanding, which facilitate the generation of flexible and interpretable recommendations. However, the direct deployment of LLMs as primary recommendation policies presents notable challenges, including persistent latency issues stemming from frequent API calls and inherent model limitations such as hallucinations and biases. To address these issues, this paper proposes a novel offline reinforcement learning (RL) framework that leverages imitation learning from LLM-generated trajectories. Specifically, inverse reinforcement learning is employed to extract robust reward models from LLM demonstrations. This approach negates the need for LLM fine-tuning, thereby substantially reducing computational overhead. Simultaneously, the RL policy is guided by the cumulative rewards derived from these demonstrations, effectively transferring the semantic insights captured by the LLM. Comprehensive experiments conducted on two benchmark datasets validate the effectiveness of the proposed method, demonstrating superior performance when compared against state-of-the-art RL-based and in-context learning baselines. The code can be found at https://github.com/ArronDZhang/IL-Rec.",
        "translated": "推荐系统（Recommender systems, RecSys）已成为在各种数字平台上提供个性化内容以提升用户参与度的关键工具。近年来，大语言模型（Large language models, LLMs）在推荐系统中的应用展现出显著潜力，主要归功于其出色的泛化能力和复杂上下文理解能力，这些能力有助于生成灵活且可解释的推荐结果。然而，直接将LLMs作为主要的推荐策略部署存在诸多挑战，包括由于频繁调用API而导致的持续性延迟问题，以及模型本身固有的局限性，如幻觉和偏见等。为了解决这些问题，本文提出了一种新颖的离线强化学习（Reinforcement learning, RL）框架，该框架通过模仿学习LLM生成的轨迹来实现。具体而言，采用逆强化学习（inverse reinforcement learning）方法从LLM的演示中提取稳健的奖励模型。这种方法避免了对LLM进行微调的需求，从而显著降低了计算开销。同时，强化学习策略通过这些演示所获得的累积奖励进行指导，有效地将LLM捕捉到的语义信息迁移过来。在两个基准数据集上进行的全面实验验证了所提方法的有效性，其性能优于最先进的基于RL的和上下文学习的基线方法。代码可在 https://github.com/ArronDZhang/IL-Rec 找到。",
        "translated_title": "超越静态大语言模型策略：基于模仿增强的推荐系统强化学习方法",
        "label": [
            "LLM生成式推荐",
            "精排",
            "通用推荐技术"
        ],
        "label_reason": "结合LLM与强化学习优化推荐策略，涉及生成式推荐和策略优化",
        "relevance_score": 9
    },
    {
        "title": "LLM-guided Hierarchical Retrieval",
        "url": "http://arxiv.org/abs/2510.13217v1",
        "pub_date": "2025-10-15",
        "summary": "Modern IR systems are increasingly tasked with answering complex, multi-faceted queries that require deep reasoning rather than simple keyword or semantic matching. While LLM-based IR has shown great promise, the prevailing retrieve-then-rerank paradigm inherits the limitations of embedding-based retrieval; parametric generative approaches are difficult to update with new information; and long-context methods that place the entire corpus in context are computationally infeasible for large document collections. To address these challenges, we introduce LATTICE, a hierarchical retrieval framework that enables an LLM to reason over and navigate large corpora with logarithmic search complexity by imposing a semantic tree structure on the corpus. Our approach consists of two stages: (1) an offline phase that organizes the corpus into a semantic hierarchy via either a bottom-up agglomerative strategy or a top-down divisive strategy using multi-level summaries and (2) an online traversal phase where a search LLM navigates this tree. A central challenge in such LLM-guided search is that the model's relevance judgments are noisy, context-dependent, and unaware of the hierarchy, making cross-branch and cross-level comparisons difficult. To overcome this, we propose a traversal algorithm that estimates calibrated latent relevance scores from local LLM outputs and aggregates them into a global path relevance metric. Our training-free framework achieves state-of-the-art zero-shot performance on the reasoning-intensive BRIGHT benchmark, demonstrating up to 9% improvement in Recall@100 and 5% in nDCG@10 over the next best zero-shot baseline. Furthermore, compared to the fine-tuned SOTA method DIVER-v2, LATTICE attains comparable results on BRIGHT subsets that use a static corpus for evaluation.",
        "translated": "现代推荐系统日益需要处理复杂的、多方面的查询，这些查询需要深入推理，而不仅仅是简单的关键词或语义匹配。尽管基于大语言模型（LLM）的推荐系统展现出了巨大潜力，但主流的先召回后重排范式继承了基于嵌入的召回方法的局限性；参数化的生成式方法难以通过新信息进行更新；而将整个语料库放入上下文中的长上下文方法在面对大规模文档集合时计算上不可行。为了解决这些挑战，我们引入了 LATTICE，一个分层召回框架，通过在语料库上施加语义树结构，使 LLM 能够以对数级搜索复杂度对大规模语料库进行推理和导航。我们的方法包含两个阶段：（1）一个离线阶段，通过自底向上的聚合策略或自顶向下的划分策略，利用多层级摘要将语料库组织成语义层次结构；（2）一个在线遍历阶段，其中搜索 LLM 遍历该树结构。在这种由 LLM 指导的搜索中，一个核心挑战是模型的相关性判断是嘈杂的、上下文依赖的，并且不了解层次结构，从而使得跨分支和跨层级的比较变得困难。为了解决这一问题，我们提出了一种遍历算法，该算法从局部 LLM 输出中估计校准后的隐相关性得分，并将其聚合为一个全局路径相关性指标。我们的无训练框架在推理密集型的 BRIGHT 基准上实现了最先进的零样本性能，其 Recall@100 和 nDCG@10 指标分别比次优的零样本基线提升了最高 9% 和 5%。此外，与微调后的最先进方法 DIVER-v2 相比，LATTICE 在使用静态语料库进行评估的 BRIGHT 子集上达到了可比的结果。",
        "translated_title": "LLM引导的层次化召回",
        "label": [
            "召回（Recall）",
            "多模态推荐（Multimodal Recommendation）",
            "LLM生成式推荐（LLM-based Generative Recommendation）"
        ],
        "label_reason": "论文提出基于LLM的分层召回框架，适用于信息检索并间接可用于推荐",
        "relevance_score": 7
    },
    {
        "title": "ReMindRAG: Low-Cost LLM-Guided Knowledge Graph Traversal for Efficient\n  RAG",
        "url": "http://arxiv.org/abs/2510.13193v1",
        "pub_date": "2025-10-15",
        "summary": "Knowledge graphs (KGs), with their structured representation capabilities, offer promising avenue for enhancing Retrieval Augmented Generation (RAG) systems, leading to the development of KG-RAG systems. Nevertheless, existing methods often struggle to achieve effective synergy between system effectiveness and cost efficiency, leading to neither unsatisfying performance nor excessive LLM prompt tokens and inference time. To this end, this paper proposes REMINDRAG, which employs an LLM-guided graph traversal featuring node exploration, node exploitation, and, most notably, memory replay, to improve both system effectiveness and cost efficiency. Specifically, REMINDRAG memorizes traversal experience within KG edge embeddings, mirroring the way LLMs \"memorize\" world knowledge within their parameters, but in a train-free manner. We theoretically and experimentally confirm the effectiveness of REMINDRAG, demonstrating its superiority over existing baselines across various benchmark datasets and LLM backbones. Our code is available at https://github.com/kilgrims/ReMindRAG.",
        "translated": "知识图谱（KGs）凭借其结构化表示能力，为增强检索增强生成（RAG）系统提供了有前景的路径，从而推动了KG-RAG系统的出现。然而，现有方法通常难以在系统效果和成本效率之间实现有效的协同，导致性能不佳或大语言模型（LLM）提示词数量和推理时间过多。为此，本文提出REMINDRAG，其采用了一种由LLM引导的图遍历方法，包含节点探索、节点利用，以及最重要的是记忆回放，从而同时提升系统的有效性和成本效率。具体而言，REMINDRAG在KG的边嵌入中记忆遍历经验，其方式类似于LLMs在其参数中“记忆”世界知识，但无需训练。我们从理论和实验两个方面验证了REMINDRAG的有效性，结果表明其在多种基准数据集和LLM主干模型上均优于现有基线方法。我们的代码可在 https://github.com/kilgrims/ReMindRAG 获取。",
        "translated_title": "ReMindRAG：高效RAG的低成本大语言模型引导的知识图谱遍历方法",
        "label": [
            "LLM生成式推荐",
            "多模态推荐"
        ],
        "label_reason": "论文涉及LLM引导的知识图谱遍历，适用于生成式推荐中的信息检索优化。",
        "relevance_score": 7
    },
    {
        "title": "Retrieval-in-the-Chain: Bootstrapping Large Language Models for\n  Generative Retrieval",
        "url": "http://arxiv.org/abs/2510.13095v1",
        "pub_date": "2025-10-15",
        "summary": "Generative retrieval (GR) is an emerging paradigm that leverages large language models (LLMs) to autoregressively generate document identifiers (docids) relevant to a given query. Prior works have focused on leveraging the generative capabilities of LLMs to improve GR, while overlooking that their reasoning capabilities could likewise help. This raises a key question: Can explicit reasoning benefit GR? To investigate, we first conduct a preliminary study where an LLM is prompted to generate free-form chain-of-thought (CoT) reasoning before performing constrained docid decoding. Although this method outperforms standard GR, the generated reasoning tends to be verbose and poorly aligned with the docid space. These limitations motivate the development of a reasoning mechanism better tailored to GR.   Therefore, we propose Reason-for-Retrieval (R4R), a reasoning-augmented framework for GR that converts free-form CoT reasoning into a compact, structured format, and iteratively refines the reasoning during the retrieval process. R4R augments an existing GR method by leveraging a reasoning-capable LLM that has been instruction-tuned for GR. At inference time, R4R first uses the LLM to generate an initial structured reasoning; then the same LLM alternates between (i) constrained decoding with the chosen GR method to produce candidate docids and (ii) updating the reasoning based on retrieval results to improve the next round. R4R does not require additional models or training, and instead a single LLM serves as both the reasoning generator and the retriever. Extensive experiments on Natural Questions, MS MARCO, and a real-world item-search benchmark validate the effectiveness of R4R.",
        "translated": "生成式召回（GR）是一种新兴范式，它利用大语言模型（LLM）对给定查询进行自回归地生成相关文档标识符（docids）。此前的研究主要关注于利用LLM的生成能力来提升GR，而忽略了其推理能力同样可以提供帮助。这引发了一个关键问题：显式的推理是否能提升GR？为了探究这一问题，我们首先进行了一项初步研究，其中LLM被提示在进行受限docids解码之前生成自由形式的思维链（CoT）推理。尽管这种方法优于标准的GR方法，但生成的推理内容往往冗长，并且与docid空间的对齐效果较差。这些限制促使我们开发一种更加契合GR的推理机制。因此，我们提出了Reason-for-Retrieval（R4R），这是一个增强推理能力的GR框架，它将自由形式的CoT推理转换为一种紧凑的结构化格式，并在召回过程中迭代地优化该推理。R4R通过使用为GR指令调优的具有推理能力的LLM，来增强现有的GR方法。在推理阶段，R4R首先使用LLM生成初始的结构化推理；然后，相同的LLM交替执行以下两个步骤：（i）利用选定的GR方法进行受限解码以生成候选docids，以及（ii）根据召回结果更新推理内容以优化下一轮生成。R4R不需要额外的模型或训练，而是通过单一LLM同时担任推理生成器和召回器的角色。在Natural Questions、MS MARCO和一个实际的物料-搜索基准数据集上的大量实验验证了R4R的有效性。",
        "translated_title": "链中召回：通过引导大语言模型实现生成式召回",
        "label": [
            "LLM生成式推荐",
            "召回"
        ],
        "label_reason": "论文提出基于LLM的生成式检索框架，与推荐系统召回环节密切相关。",
        "relevance_score": 8
    },
    {
        "title": "Post-hoc Popularity Bias Correction in GNN-based Collaborative Filtering",
        "url": "http://arxiv.org/abs/2510.12959v1",
        "pub_date": "2025-10-14",
        "summary": "User historical interaction data is the primary signal for learning user preferences in collaborative filtering (CF). However, the training data often exhibits a long-tailed distribution, where only a few items have the majority of interactions. CF models trained directly on such imbalanced data are prone to learning popularity bias, which reduces personalization and leads to suboptimal recommendation quality. Graph Neural Networks (GNNs), while effective for CF due to their message passing mechanism, can further propagate and amplify popularity bias through their aggregation process. Existing approaches typically address popularity bias by modifying training objectives but fail to directly counteract the bias propagated during GNN's neighborhood aggregation. Applying weights to interactions during aggregation can help alleviate this problem, yet it risks distorting model learning due to unstable node representations in the early stages of training. In this paper, we propose a Post-hoc Popularity Debiasing (PPD) method that corrects for popularity bias in GNN-based CF and operates directly on pre-trained embeddings without requiring retraining. By estimating interaction-level popularity and removing popularity components from node representations via a popularity direction vector, PPD reduces bias while preserving user preferences. Experimental results show that our method outperforms state-of-the-art approaches for popularity bias correction in GNN-based CF.",
        "translated": "用户的历史交互数据是协同过滤（CF）中学习用户/物料偏好的主要信号。然而，训练数据通常表现出长尾分布，其中只有少数物料获得了大部分的交互。直接在这样的不平衡数据上训练的CF模型容易学习到流行度偏差，从而降低个性化程度并导致推荐质量下降。图神经网络（GNN）由于其消息传递机制在CF中表现有效，但它们的聚合过程可能会进一步传播和放大流行度偏差。现有方法通常通过修改训练目标来应对流行度偏差，但未能直接对抗GNN在邻域聚合过程中传播的偏差。在聚合过程中对交互加权有助于缓解这一问题，然而由于训练早期阶段节点表示的不稳定性，这种加权可能会扭曲模型的学习。本文提出了一种后处理流行度去偏（Post-hoc Popularity Debiasing, PPD）方法，该方法无需重新训练，即可直接在预训练的嵌入上进行操作，对基于GNN的CF中的流行度偏差进行校正。通过估计交互层面的流行度，并借助一个流行度方向向量从节点表示中去除流行度成分，PPD在减少偏差的同时保留了用户偏好。实验结果表明，我们的方法在基于GNN的CF中流行度偏差校正方面优于当前最先进的方法。",
        "translated_title": "基于图神经网络的协同过滤中的后处理流行度偏差校正",
        "label": [
            "图神经网络推荐（GNN for Recommendation）",
            "通用推荐技术（General Recommendation Techniques）"
        ],
        "label_reason": "论文针对GNN在推荐中的流行度偏差问题提出后处理方法，属于图神经网络推荐改进技术",
        "relevance_score": 8
    },
    {
        "title": "Universal Image Restoration Pre-training via Masked Degradation\n  Classification",
        "url": "http://arxiv.org/abs/2510.13282v1",
        "pub_date": "2025-10-15",
        "summary": "This study introduces a Masked Degradation Classification Pre-Training method (MaskDCPT), designed to facilitate the classification of degradation types in input images, leading to comprehensive image restoration pre-training. Unlike conventional pre-training methods, MaskDCPT uses the degradation type of the image as an extremely weak supervision, while simultaneously leveraging the image reconstruction to enhance performance and robustness. MaskDCPT includes an encoder and two decoders: the encoder extracts features from the masked low-quality input image. The classification decoder uses these features to identify the degradation type, whereas the reconstruction decoder aims to reconstruct a corresponding high-quality image. This design allows the pre-training to benefit from both masked image modeling and contrastive learning, resulting in a generalized representation suited for restoration tasks. Benefit from the straightforward yet potent MaskDCPT, the pre-trained encoder can be used to address universal image restoration and achieve outstanding performance. Implementing MaskDCPT significantly improves performance for both convolution neural networks (CNNs) and Transformers, with a minimum increase in PSNR of 3.77 dB in the 5D all-in-one restoration task and a 34.8% reduction in PIQE compared to baseline in real-world degradation scenarios. It also emergences strong generalization to previously unseen degradation types and levels. In addition, we curate and release the UIR-2.5M dataset, which includes 2.5 million paired restoration samples across 19 degradation types and over 200 degradation levels, incorporating both synthetic and real-world data. The dataset, source code, and models are available at https://github.com/MILab-PKU/MaskDCPT.",
        "translated": "本研究提出了一种 Masked Degradation Classification Pre-Training 方法（MaskDCPT），旨在实现对输入图像退化类型的分类，从而促进通用图像恢复的预训练。不同于传统的预训练方法，MaskDCPT 将图像的退化类型作为极其弱的监督信号，同时利用图像重建来提升性能和鲁棒性。MaskDCPT 包含一个编码器和两个解码器：编码器从被遮蔽的低质量输入图像中提取特征；分类解码器利用这些特征识别退化类型，而重建解码器则致力于重建对应的高质量图像。这种设计使预训练能够同时受益于遮蔽图像建模和对比学习，从而获得适用于图像恢复任务的通用表征。借助简洁而有效的 MaskDCPT，预训练的编码器可用于解决通用图像恢复问题，并取得优异的性能。MaskDCPT 的实现显著提升了卷积神经网络（CNNs）和 Transformers 的性能，在 5D 全合一图像恢复任务中，PSNR 至少提升了 3.77 dB，而在真实退化场景中，与基线相比，PIQE 降低了 34.8%。此外，该方法在未见过的退化类型和程度上也展现出强大的泛化能力。同时，我们整理并发布了 UIR-2.5M 数据集，该数据集包含 250 万对覆盖 19 种退化类型和 200 多个退化等级的图像恢复样本，融合了合成数据和真实数据。数据集、源代码和模型均可在 https://github.com/MILab-PKU/MaskDCPT 获取。",
        "translated_title": "通过掩码退化分类的通用图像恢复预训练",
        "label": [
            "图像恢复（Image Restoration）",
            "图像去噪（Image Denoising）",
            "图像去雨（Image Deraining）",
            "图像去雾（Image Dehazing）",
            "图像去模糊（Image Deblurring）",
            "超分辨率（Super-Resolution）",
            "图像去 JPEG 伪影（JPEG Artifact Removal）"
        ],
        "label_reason": "论文提出通用图像恢复预训练方法，适用于多种低级图像恢复任务",
        "relevance_score": 9
    },
    {
        "title": "PhysMaster: Mastering Physical Representation for Video Generation via\n  Reinforcement Learning",
        "url": "http://arxiv.org/abs/2510.13809v1",
        "pub_date": "2025-10-15",
        "summary": "Video generation models nowadays are capable of generating visually realistic videos, but often fail to adhere to physical laws, limiting their ability to generate physically plausible videos and serve as ''world models''. To address this issue, we propose PhysMaster, which captures physical knowledge as a representation for guiding video generation models to enhance their physics-awareness. Specifically, PhysMaster is based on the image-to-video task where the model is expected to predict physically plausible dynamics from the input image. Since the input image provides physical priors like relative positions and potential interactions of objects in the scenario, we devise PhysEncoder to encode physical information from it as an extra condition to inject physical knowledge into the video generation process. The lack of proper supervision on the model's physical performance beyond mere appearance motivates PhysEncoder to apply reinforcement learning with human feedback to physical representation learning, which leverages feedback from generation models to optimize physical representations with Direct Preference Optimization (DPO) in an end-to-end manner. PhysMaster provides a feasible solution for improving physics-awareness of PhysEncoder and thus of video generation, proving its ability on a simple proxy task and generalizability to wide-ranging physical scenarios. This implies that our PhysMaster, which unifies solutions for various physical processes via representation learning in the reinforcement learning paradigm, can act as a generic and plug-in solution for physics-aware video generation and broader applications.",
        "translated": "当前的视频生成模型虽然能够生成视觉上逼真的视频，但往往未能遵循物理规律，这限制了它们生成物理合理视频的能力，也限制了其作为“世界模型”的潜力。为了解决这一问题，我们提出了PhysMaster，它将物理知识捕获为表示形式，用于引导视频生成模型，从而增强其对物理规律的感知能力。具体而言，PhysMaster基于图像到视频的任务，模型期望从输入图像中预测物理上合理的动态过程。由于输入图像提供了诸如场景中物体相对位置和潜在交互等物理先验，我们设计了PhysEncoder，从输入图像中编码物理信息作为额外的条件，将其注入视频生成过程中。由于缺乏对模型物理性能（除了外观）的有效监督，PhysEncoder采用基于人类反馈的强化学习来实现物理表示学习，通过生成模型的反馈，以端到端的方式使用直接偏好优化（Direct Preference Optimization, DPO）来优化物理表示。PhysMaster为提高PhysEncoder以及视频生成的物理感知能力提供了一种可行的解决方案，并在简单代理任务上验证了其能力，同时展示了其在广泛物理场景中的泛化性能。这表明，我们提出的PhysMaster通过在强化学习范式下的表示学习，统一了解决各种物理过程的方法，可以作为一种通用且可插拔的解决方案，应用于物理感知视频生成及相关更广泛的领域。",
        "translated_title": "PhysMaster：通过强化学习掌握物理表示用于视频生成",
        "label": [],
        "label_reason": "论文聚焦视频生成而非图像像素级复原，属于high-level任务",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "结合强化学习与物理表示，有一定创新但非图像恢复核心领域"
    },
    {
        "title": "VisCoP: Visual Probing for Video Domain Adaptation of Vision Language\n  Models",
        "url": "http://arxiv.org/abs/2510.13808v1",
        "pub_date": "2025-10-15",
        "summary": "Large Vision-Language Models (VLMs) excel at general visual reasoning tasks but exhibit sharp performance degradation when applied to novel domains with substantial distribution shifts from pretraining data. Existing domain adaptation approaches finetune different VLM components, but this often results in limited domain-specific feature learning or catastrophic forgetting of prior capabilities. To address these issues, we introduce Vision Contextualized Probing (VisCoP), which augments the VLM's vision encoder with a compact set of learnable visual probes. These probes enable efficient domain-specific adaptation with minimal modification to pretrained parameters. We evaluate VisCoP across three challenging domain adaptation settings-cross-view (exocentric to egocentric), cross-modal (RGB to depth), and cross-task (human understanding to robot control). Experiments show that VisCoP consistently outperforms existing adaptation strategies, achieving superior performance on target domains while effectively retaining source-domain knowledge.",
        "translated": "大型视觉-语言模型（VLMs）在通用视觉推理任务中表现出色，但当应用于与预训练数据存在显著分布差异的新领域时，其性能会急剧下降。现有的领域适应方法通常对VLM的不同组件进行微调，但这往往导致领域特定特征学习受限，或者对先前能力产生灾难性遗忘。为了解决这些问题，我们提出了视觉上下文化探针（Vision Contextualized Probing, VisCoP），该方法通过在VLM的视觉编码器中引入一组紧凑的可学习视觉探针来增强模型。这些探针能够在对预训练参数仅作最小修改的情况下实现高效的领域特定适应。我们在三种具有挑战性的领域适应设置中评估了VisCoP，包括跨视角（外视角到自视角）、跨模态（RGB到深度）和跨任务（人类理解到机器人控制）。实验表明，VisCoP始终优于现有适应策略，在目标领域上取得了更优的性能，同时有效保留了源领域的知识。",
        "translated_title": "VisCoP：用于视觉语言模型视频域适应的视觉探测",
        "label": [],
        "label_reason": "论文聚焦于视觉语言模型的视频领域适配，不直接涉及图像像素级恢复或增强",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出视觉探针机制，对现有领域适配方法有一定改进"
    },
    {
        "title": "Generative Universal Verifier as Multimodal Meta-Reasoner",
        "url": "http://arxiv.org/abs/2510.13804v1",
        "pub_date": "2025-10-15",
        "summary": "We introduce Generative Universal Verifier, a novel concept and plugin designed for next-generation multimodal reasoning in vision-language models and unified multimodal models, providing the fundamental capability of reflection and refinement on visual outcomes during the reasoning and generation process. This work makes three main contributions: (1) We build ViVerBench, a comprehensive benchmark spanning 16 categories of critical tasks for evaluating visual outcomes in multimodal reasoning. Results show that existing VLMs consistently underperform across these tasks, underscoring a substantial gap from human-level capability in reliable visual verification. (2) We design two automated pipelines to construct large-scale visual verification data and train OmniVerifier-7B, the first omni-capable generative verifier trained for universal visual verification and achieves notable gains on ViVerBench(+8.3). Through training, we identify three atomic capabilities in visual verification and demonstrate how they generalize and interact synergistically. (3) We propose OmniVerifier-TTS, a sequential test-time scaling paradigm that leverages the universal verifier to bridge image generation and editing within unified models, enhancing the upper bound of generative ability through iterative fine-grained optimization. Beyond generation, we extend universal verifier to broader world-modeling interleaved reasoning scenarios. Empirically, OmniVerifier-TTS achieves improvements on T2I-ReasonBench(+3.7), and GenEval++(+4.3), outperforming existing parallel test-time scaling methods, such as Best-of-N. By endowing multimodal reasoning with reliable visual verification, OmniVerifier advances both reliable reflection during generation and scalable test-time refinement, marking a step toward more trustworthy and controllable next-generation reasoning systems.",
        "translated": "我们提出生成式通用验证器（Generative Universal Verifier），这是一种面向下一代视觉语言模型和统一多模态模型的新型概念和插件，旨在提供推理和生成过程中对视觉结果进行反思和优化的基本能力。本研究做出以下三项主要贡献：(1) 我们构建了 ViVerBench，一个全面的基准测试集，涵盖16类关键任务，用于评估多模态推理中视觉结果的性能。结果显示，现有视觉语言模型（VLMs）在这些任务中表现普遍不佳，表明其在可靠视觉验证方面与人类水平之间存在显著差距。(2) 我们设计了两条自动化流水线用于构建大规模的视觉验证数据，并训练了 OmniVerifier-7B，这是首个面向通用视觉验证的全能型生成式验证器。在 ViVerBench 上，OmniVerifier-7B 取得了显著的性能提升（+8.3）。通过训练，我们识别出视觉验证中的三个基本能力，并展示了它们如何在不同任务中泛化并协同工作。(3) 我们提出了 OmniVerifier-TTS，一种顺序式测试时扩展范式，利用通用验证器在统一模型中实现图像生成和编辑的桥梁作用，并通过迭代细粒度优化提升生成能力的上限。除了生成任务，我们还将通用验证器扩展到更广泛的交互式世界建模推理场景。实证表明，OmniVerifier-TTS 在 T2I-ReasonBench（+3.7）和 GenEval++（+4.3）上均取得性能提升，并优于现有的并行式测试时扩展方法，如 Best-of-N。通过为多模态推理赋予可靠的视觉验证能力，OmniVerifier 推动了生成过程中的可靠反思以及可扩展的测试时优化，标志着我们向更加可信和可控的下一代推理系统迈出了关键一步。",
        "translated_title": "生成式通用验证器作为多模态元推理器",
        "label": [],
        "label_reason": "论文主要关注视觉-语言模型的推理验证，非图像像素级处理",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出通用验证框架和新范式，但未突破low-level图像处理核心技术"
    },
    {
        "title": "Trace Anything: Representing Any Video in 4D via Trajectory Fields",
        "url": "http://arxiv.org/abs/2510.13802v1",
        "pub_date": "2025-10-15",
        "summary": "Effective spatio-temporal representation is fundamental to modeling, understanding, and predicting dynamics in videos. The atomic unit of a video, the pixel, traces a continuous 3D trajectory over time, serving as the primitive element of dynamics. Based on this principle, we propose representing any video as a Trajectory Field: a dense mapping that assigns a continuous 3D trajectory function of time to each pixel in every frame. With this representation, we introduce Trace Anything, a neural network that predicts the entire trajectory field in a single feed-forward pass. Specifically, for each pixel in each frame, our model predicts a set of control points that parameterizes a trajectory (i.e., a B-spline), yielding its 3D position at arbitrary query time instants. We trained the Trace Anything model on large-scale 4D data, including data from our new platform, and our experiments demonstrate that: (i) Trace Anything achieves state-of-the-art performance on our new benchmark for trajectory field estimation and performs competitively on established point-tracking benchmarks; (ii) it offers significant efficiency gains thanks to its one-pass paradigm, without requiring iterative optimization or auxiliary estimators; and (iii) it exhibits emergent abilities, including goal-conditioned manipulation, motion forecasting, and spatio-temporal fusion. Project page: https://trace-anything.github.io/.",
        "translated": "有效的时空表示是建模、理解和预测视频中动态过程的基础。视频的基本单位是像素，其在时间上追踪一个连续的3D轨迹，作为动态的基本元素。基于这一原理，我们提出将任何视频表示为轨迹场（Trajectory Field）：一种密集映射，为每一帧中的每个像素分配一个关于时间的连续3D轨迹函数。通过这种表示，我们引入了Trace Anything，一种神经网络模型，它能够在一次前向传播中预测整个轨迹场。具体来说，对于每一帧中的每个像素，我们的模型预测一组控制点，以参数化轨迹（即B样条），从而在任意查询时刻获得其3D位置。我们在大规模4D数据上训练了Trace Anything模型，包括我们新平台的数据，实验结果表明：(i) Trace Anything在我们新的轨迹场估计基准上取得了最先进的性能，并在已有的点跟踪基准上表现出竞争力；(ii) 由于其单次前向传播的范式，它在效率方面有显著提升，无需进行迭代优化或使用辅助估计器；(iii) 它展现出一些新兴能力，包括目标条件下的操控、运动预测以及时空融合。项目主页：https://trace-anything.github.io/。",
        "translated_title": "追踪任何内容：通过轨迹场在4D中表示任何视频",
        "label": [],
        "label_reason": "论文关注视频轨迹建模，不属于图像像素级恢复或增强任务。",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出轨迹场表示和一次性预测框架，具有新颖性。"
    },
    {
        "title": "Reasoning in Space via Grounding in the World",
        "url": "http://arxiv.org/abs/2510.13800v1",
        "pub_date": "2025-10-15",
        "summary": "In this paper, we claim that 3D visual grounding is the cornerstone of spatial reasoning and introduce the Grounded-Spatial Reasoner (GS-Reasoner) to explore the effective spatial representations that bridge the gap between them. Existing 3D LLMs suffer from the absence of a unified 3D representation capable of jointly capturing semantic and geometric information. This deficiency is manifested either in poor performance on grounding or in an excessive reliance on external modules, ultimately hindering the seamless integration of grounding and spatial reasoning. To address this, we propose a simple yet effective dual-path pooling mechanism that tightly aligns geometric features with both semantic and positional cues, constructing a unified image patch-based 3D representation that encapsulates all essential information without increasing the number of input tokens. Leveraging this holistic representation, GS-Reasoner is the first 3D LLM that achieves autoregressive grounding entirely without external modules while delivering performance comparable to state-of-the-art models, establishing a unified and self-contained framework for 3D spatial reasoning. To further bridge grounding and spatial reasoning, we introduce the Grounded Chain-of-Thought (GCoT) dataset. This dataset is meticulously curated to include both 3D bounding box annotations for objects referenced in reasoning questions and step-by-step reasoning paths that integrate grounding as a core component of the problem-solving process. Extensive experiments demonstrate that GS-Reasoner achieves impressive results on 3D visual grounding, which in turn significantly enhances its spatial reasoning capabilities, leading to state-of-the-art performance.",
        "translated": "本文提出观点认为，三维视觉定位是空间推理的基础，并引入了 GS-Reasoner（Grounded-Spatial Reasoner）以探索能够弥合二者之间差距的有效空间表示。现有的三维大语言模型（LLM）面临缺乏统一的三维表示的问题，该表示无法同时捕捉语义和几何信息。这种缺陷体现在定位任务上的性能较差，或对外部模块的过度依赖，最终阻碍了定位与空间推理的无缝融合。为了解决这一问题，我们提出了一种简单而有效的同时路径池化机制，该机制紧密对齐几何特征与语义和位置线索，构建了一种统一的、基于图像块的三维表示形式，包含所有必要信息且不增加输入 token 的数量。借助这种整体表示，GS-Reasoner 是首个在不使用任何外部模块的情况下实现自回归定位的三维大语言模型，其性能可与最先进的模型相媲美，为三维空间推理提供了一个统一且自洽的框架。为进一步弥合定位与空间推理之间的联系，我们引入了 Grounded Chain-of-Thought（GCoT）数据集。该数据集经过精心构建，包含了推理问题中所提到对象的三维边界框标注，以及将定位作为解决问题核心步骤的逐步推理路径。大量实验表明，GS-Reasoner 在三维视觉定位任务中取得了令人印象深刻的结果，从而显著提升了其空间推理能力，达到当前最先进的性能水平。",
        "translated_title": "通过在世界中的锚定实现空间推理",
        "label": [],
        "label_reason": "论文聚焦3D视觉理解和推理，不涉及图像像素级恢复或增强",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "提出了一种新颖的3D表示方法和数据集，但不针对低层图像处理任务"
    },
    {
        "title": "The Mechanistic Emergence of Symbol Grounding in Language Models",
        "url": "http://arxiv.org/abs/2510.13796v1",
        "pub_date": "2025-10-15",
        "summary": "Symbol grounding (Harnad, 1990) describes how symbols such as words acquire their meanings by connecting to real-world sensorimotor experiences. Recent work has shown preliminary evidence that grounding may emerge in (vision-)language models trained at scale without using explicit grounding objectives. Yet, the specific loci of this emergence and the mechanisms that drive it remain largely unexplored. To address this problem, we introduce a controlled evaluation framework that systematically traces how symbol grounding arises within the internal computations through mechanistic and causal analysis. Our findings show that grounding concentrates in middle-layer computations and is implemented through the aggregate mechanism, where attention heads aggregate the environmental ground to support the prediction of linguistic forms. This phenomenon replicates in multimodal dialogue and across architectures (Transformers and state-space models), but not in unidirectional LSTMs. Our results provide behavioral and mechanistic evidence that symbol grounding can emerge in language models, with practical implications for predicting and potentially controlling the reliability of generation.",
        "translated": "符号根基（Symbol grounding）（Harnad, 1990）描述了符号（如词语）如何通过与现实世界中的感知和运动体验相连接而获得其含义。最近的研究表明，在大规模训练的（视觉-）语言模型中，即使不使用显式的根基目标，符号根基也可能自发出现。然而，这种现象的具体出现位置及其驱动机制仍鲜有研究。为了解决这一问题，我们引入了一种可控的评估框架，系统地追踪符号根基在内部计算过程中的产生方式，并通过机制和因果分析进行研究。我们的研究结果表明，符号根基主要集中在中间层计算中，并通过聚合机制实现，其中注意力头聚合环境根基以支持语言形式的预测。这一现象在多模态对话中以及不同架构（Transformer 和状态空间模型）中均出现，但在单向 LSTM 中未观察到。我们的研究提供了行为和机制上的证据，证明符号根基可以在语言模型中自发产生，这对于预测和可能控制生成的可靠性具有实际意义。",
        "translated_title": "语言模型中符号 grounding 的机制性出现",
        "label": [],
        "label_reason": "论文聚焦语言模型的符号接地机制，不涉及图像像素级处理。",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "提出了符号接地的机制分析框架，有一定理论创新但非视觉任务相关。"
    },
    {
        "title": "Bee: A High-Quality Corpus and Full-Stack Suite to Unlock Advanced Fully\n  Open MLLMs",
        "url": "http://arxiv.org/abs/2510.13795v1",
        "pub_date": "2025-10-15",
        "summary": "Fully open multimodal large language models (MLLMs) currently lag behind proprietary counterparts, primarily due to a significant gap in data quality for supervised fine-tuning (SFT). Existing open-source datasets are often plagued by widespread noise and a critical deficit in complex reasoning data, such as Chain-of-Thought (CoT), which hinders the development of advanced model capabilities. Addressing these challenges, our work makes three primary contributions. First, we introduce Honey-Data-15M, a new SFT dataset comprising approximately 15 million QA pairs, processed through multiple cleaning techniques and enhanced with a novel dual-level (short and long) CoT enrichment strategy. Second, we introduce HoneyPipe, the data curation pipeline, and its underlying framework DataStudio, providing the community with a transparent and adaptable methodology for data curation that moves beyond static dataset releases. Finally, to validate our dataset and pipeline, we train Bee-8B, an 8B model on Honey-Data-15M. Experiments show that Bee-8B establishes a new state-of-the-art (SOTA) for fully open MLLMs, achieving performance that is competitive with, and in some cases surpasses, recent semi-open models such as InternVL3.5-8B. Our work delivers to the community a suite of foundational resources, including: the Honey-Data-15M corpus; the full-stack suite comprising HoneyPipe and DataStudio; training recipes; an evaluation harness; and the model weights. This effort demonstrates that a principled focus on data quality is a key pathway to developing fully open MLLMs that are highly competitive with their semi-open counterparts.",
        "translated": "目前，完全开源的多模态大语言模型（MLLMs）在性能上落后于专有模型，主要原因在于用于监督微调（SFT）的数据质量存在显著差距。现有的开源数据集通常受到广泛噪声的困扰，并且在复杂推理数据（如思维链（CoT））方面存在严重不足，这阻碍了先进模型能力的发展。为了解决这些挑战，我们的工作主要包括三个方面的贡献。首先，我们引入了 Honey-Data-15M，一个包含约 1500 万个问答对的新 SFT 数据集，通过多种数据清洗技术处理，并采用了一种新的双层（短链和长链）CoT 增强策略进行优化。其次，我们提出了 HoneyPipe 数据整理流水线及其底层框架 DataStudio，为社区提供一种透明且可调节的数据整理方法，突破了传统静态数据集发布的方式。最后，为了验证我们的数据集和流水线效果，我们基于 Honey-Data-15M 训练了 Bee-8B，一个 80 亿参数规模的模型。实验结果表明，Bee-8B 在完全开源 MLLMs 中达到了新的最先进水平（SOTA），其性能与近期的半开源模型（如 InternVL3.5-8B）相比具有竞争力，某些情况下甚至优于后者。我们的工作为社区提供了一套基础资源，包括：Honey-Data-15M 数据语料库；涵盖 HoneyPipe 和 DataStudio 的全栈工具套件；训练配方；评估框架；以及模型权重。这项研究表明，专注于数据质量的原则性方法是开发与半开源模型高度竞争的完全开源 MLLMs 的关键路径。",
        "translated_title": "Bee：一个高质量语料库和全栈套件，用于解锁先进的全开放大语言模型",
        "label": [],
        "label_reason": "论文聚焦多模态大语言模型数据构建，不涉及图像像素级处理",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "提出新数据集和训练方法，但属于通用MLLM改进而非视觉低级任务创新"
    },
    {
        "title": "NoisePrints: Distortion-Free Watermarks for Authorship in Private\n  Diffusion Models",
        "url": "http://arxiv.org/abs/2510.13793v1",
        "pub_date": "2025-10-15",
        "summary": "With the rapid adoption of diffusion models for visual content generation, proving authorship and protecting copyright have become critical. This challenge is particularly important when model owners keep their models private and may be unwilling or unable to handle authorship issues, making third-party verification essential. A natural solution is to embed watermarks for later verification. However, existing methods require access to model weights and rely on computationally heavy procedures, rendering them impractical and non-scalable. To address these challenges, we propose , a lightweight watermarking scheme that utilizes the random seed used to initialize the diffusion process as a proof of authorship without modifying the generation process. Our key observation is that the initial noise derived from a seed is highly correlated with the generated visual content. By incorporating a hash function into the noise sampling process, we further ensure that recovering a valid seed from the content is infeasible. We also show that sampling an alternative seed that passes verification is infeasible, and demonstrate the robustness of our method under various manipulations. Finally, we show how to use cryptographic zero-knowledge proofs to prove ownership without revealing the seed. By keeping the seed secret, we increase the difficulty of watermark removal. In our experiments, we validate NoisePrints on multiple state-of-the-art diffusion models for images and videos, demonstrating efficient verification using only the seed and output, without requiring access to model weights.",
        "translated": "随着扩散模型在视觉内容生成中的迅速应用，证明作者身份和保护版权变得至关重要。当模型所有者保持其模型私有时，这一挑战尤为突出，他们可能不愿或无法处理作者身份问题，从而使得第三方验证成为必要。一种自然的解决方案是嵌入水印以供后续验证。然而，现有方法需要访问模型权重，并依赖计算量大的过程，使其在实践中不可行且难以扩展。为了解决这些挑战，我们提出了一种轻量级的水印方案，该方案利用用于初始化扩散过程的随机种子作为作者身份的证明，且无需修改生成过程。我们的关键观察是，由种子生成的初始噪声与生成的视觉内容具有高度相关性。通过在噪声采样过程中引入哈希函数，我们进一步确保了从内容中恢复有效种子在计算上不可行。我们还证明，采样一个能通过验证的替代种子在计算上也是不可行的，并展示了我们的方法在各种篡改下的鲁棒性。最后，我们展示了如何使用密码学中的零知识证明来在不泄露种子的前提下证明所有权。通过保密种子，我们提高了水印移除的难度。在我们的实验中，我们在多个最先进的图像和视频扩散模型上验证了 NoisePrints 的有效性，展示了仅使用种子和输出即可实现高效的验证，而无需访问模型权重。",
        "translated_title": "NoisePrints: 隐私扩散模型中的无损水印用于作者归属",
        "label": [],
        "label_reason": "论文聚焦扩散模型水印而非图像像素级恢复",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出无需模型权重的轻量水印方案，有一定创新"
    },
    {
        "title": "Adaptive Visual Conditioning for Semantic Consistency in Diffusion-Based\n  Story Continuation",
        "url": "http://arxiv.org/abs/2510.13787v1",
        "pub_date": "2025-10-15",
        "summary": "Story continuation focuses on generating the next image in a narrative sequence so that it remains coherent with both the ongoing text description and the previously observed images. A central challenge in this setting lies in utilizing prior visual context effectively, while ensuring semantic alignment with the current textual input. In this work, we introduce AVC (Adaptive Visual Conditioning), a framework for diffusion-based story continuation. AVC employs the CLIP model to retrieve the most semantically aligned image from previous frames. Crucially, when no sufficiently relevant image is found, AVC adaptively restricts the influence of prior visuals to only the early stages of the diffusion process. This enables the model to exploit visual context when beneficial, while avoiding the injection of misleading or irrelevant information. Furthermore, we improve data quality by re-captioning a noisy dataset using large language models, thereby strengthening textual supervision and semantic alignment. Quantitative results and human evaluations demonstrate that AVC achieves superior coherence, semantic consistency, and visual fidelity compared to strong baselines, particularly in challenging cases where prior visuals conflict with the current input.",
        "translated": "故事延续关注于生成叙事序列中的下一幅图像，使其与当前的文本描述和之前已观测的图像保持连贯。在这种设置下的一个核心挑战在于如何有效地利用先前的视觉上下文，同时确保与当前文本输入在语义上的一致性。在本文中，我们提出了 AVC（Adaptive Visual Conditioning），一种基于扩散模型的故事延续框架。AVC 利用 CLIP 模型从先前的帧中检索语义最相关的一幅图像。关键的是，当未找到足够相关的图像时，AVC 会自适应地将先前视觉信息的影响限制在扩散过程的早期阶段。这使得模型在有益时能够有效利用视觉上下文，同时避免注入误导性或不相关的信息。此外，我们通过使用大语言模型对噪声数据集进行重新描述，提高了数据质量，从而增强了文本监督和语义一致性。定量结果和人类评估表明，与强大的基线模型相比，AVC 在连贯性、语义一致性和视觉保真度方面均取得了优越的性能，特别是在先前视觉信息与当前输入冲突的具有挑战性的情况下。",
        "translated_title": "基于扩散的故事情节延续中的自适应视觉条件化以保持语义一致性",
        "label": [],
        "label_reason": "论文关注文本驱动的扩散模型故事生成，非像素级图像恢复任务。",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出自适应视觉条件框架，改进文本-图像对齐方法，但创新点较常规。"
    },
    {
        "title": "InternVLA-M1: A Spatially Guided Vision-Language-Action Framework for\n  Generalist Robot Policy",
        "url": "http://arxiv.org/abs/2510.13778v1",
        "pub_date": "2025-10-15",
        "summary": "We introduce InternVLA-M1, a unified framework for spatial grounding and robot control that advances instruction-following robots toward scalable, general-purpose intelligence. Its core idea is spatially guided vision-language-action training, where spatial grounding serves as the critical link between instructions and robot actions. InternVLA-M1 employs a two-stage pipeline: (i) spatial grounding pre-training on over 2.3M spatial reasoning data to determine ``where to act'' by aligning instructions with visual, embodiment-agnostic positions, and (ii) spatially guided action post-training to decide ``how to act'' by generating embodiment-aware actions through plug-and-play spatial prompting. This spatially guided training recipe yields consistent gains: InternVLA-M1 outperforms its variant without spatial guidance by +14.6% on SimplerEnv Google Robot, +17% on WidowX, and +4.3% on LIBERO Franka, while demonstrating stronger spatial reasoning capability in box, point, and trace prediction. To further scale instruction following, we built a simulation engine to collect 244K generalizable pick-and-place episodes, enabling a 6.2% average improvement across 200 tasks and 3K+ objects. In real-world clustered pick-and-place, InternVLA-M1 improved by 7.3%, and with synthetic co-training, achieved +20.6% on unseen objects and novel configurations. Moreover, in long-horizon reasoning-intensive scenarios, it surpassed existing works by over 10%. These results highlight spatially guided training as a unifying principle for scalable and resilient generalist robots. Code and models are available at https://github.com/InternRobotics/InternVLA-M1.",
        "translated": "我们引入了 InternVLA-M1，这是一个面向空间定位与机器人控制的统一框架，推动指令跟随机器人向可扩展、通用型智能发展。其核心思想是空间引导的视觉-语言-动作训练，其中空间定位作为连接指令与机器人动作的关键环节。InternVLA-M1 采用两阶段训练流程：(i) 在超过 2.3 万个空间推理数据上进行空间定位预训练，通过将指令与视觉、与实体无关的空间位置对齐来确定 ``在何处执行动作''，以及 (ii) 通过即插即用的空间提示生成与实体相关的动作，进行空间引导的动作后训练以决定 ``如何执行动作''。这种空间引导的训练方法带来了显著的提升：在 SimplierEnv Google Robot 上，InternVLA-M1 相较于其无空间引导的变体提升了 +14.6%，在 WidowX 上提升了 +17%，在 LIBERO Franka 上提升了 +4.3%，同时在盒预测、点预测和轨迹预测中表现出更强的空间推理能力。为了进一步扩展指令跟随的能力，我们构建了一个仿真引擎，收集了 244K 个具有泛化能力的抓取与放置场景，使得在 200 个任务和 3K+ 个物体上的平均性能提升了 6.2%。在现实世界中的集群抓取与放置任务中，InternVLA-M1 提升了 7.3%，而在结合合成数据的联合训练中，对于未见过的物体和新配置，性能提升达 +20.6%。此外，在需要长期推理的复杂场景中，其性能超越现有工作超过 10%。这些结果表明，空间引导的训练方法可作为构建可扩展且鲁棒的通用型机器人的统一原则。代码和模型可在 https://github.com/InternRobotics/InternVLA-M1 获取。",
        "translated_title": "InternVLA-M1：一种空间引导的视觉-语言-动作框架  \n用于通用机器人策略",
        "label": [],
        "label_reason": "论文聚焦机器人控制而非图像像素级恢复或增强",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出空间引导训练方法，对指令跟随机器人有改进"
    },
    {
        "title": "UrbanFusion: Stochastic Multimodal Fusion for Contrastive Learning of\n  Robust Spatial Representations",
        "url": "http://arxiv.org/abs/2510.13774v1",
        "pub_date": "2025-10-15",
        "summary": "Forecasting urban phenomena such as housing prices and public health indicators requires the effective integration of various geospatial data. Current methods primarily utilize task-specific models, while recent foundation models for spatial representations often support only limited modalities and lack multimodal fusion capabilities. To overcome these challenges, we present UrbanFusion, a Geo-Foundation Model (GeoFM) that features Stochastic Multimodal Fusion (SMF). The framework employs modality-specific encoders to process different types of inputs, including street view imagery, remote sensing data, cartographic maps, and points of interest (POIs) data. These multimodal inputs are integrated via a Transformer-based fusion module that learns unified representations. An extensive evaluation across 41 tasks in 56 cities worldwide demonstrates UrbanFusion's strong generalization and predictive performance compared to state-of-the-art GeoAI models. Specifically, it 1) outperforms prior foundation models on location-encoding, 2) allows multimodal input during inference, and 3) generalizes well to regions unseen during training. UrbanFusion can flexibly utilize any subset of available modalities for a given location during both pretraining and inference, enabling broad applicability across diverse data availability scenarios. All source code is available at https://github.com/DominikM198/UrbanFusion.",
        "translated": "预测城市现象（如房价和公共健康指标）需要有效整合各种地理空间数据。当前的方法主要依赖于任务特定模型，而近期的空间表示基础模型通常只支持有限的模态，并缺乏多模态融合能力。为了解决这些挑战，我们提出了 UrbanFusion，一种具有随机多模态融合（Stochastic Multimodal Fusion, SMF）功能的地理基础模型（Geo-Foundation Model, GeoFM）。该框架使用模态特定编码器处理不同类型的输入，包括街景图像、遥感数据、地图数据和兴趣点（Points of Interest, POIs）数据。这些多模态输入通过基于 Transformer 的融合模块进行整合，以学习统一的表示。在对全球 56 个城市中的 41 项任务进行广泛评估后，UrbanFusion 在泛化能力和预测性能方面均优于最先进的 GeoAI 模型。具体而言，UrbanFusion 具备以下优势：1）在位置编码方面优于以往的基础模型，2）在推理过程中支持多模态输入，3）对训练过程中未见过的区域具有良好的泛化能力。UrbanFusion 在预训练和推理阶段都能灵活地利用特定位置中可用的任意子集模态，从而使其适用于各种数据可用性场景。所有源代码均在 https://github.com/DominikM198/UrbanFusion 上公开。",
        "translated_title": "UrbanFusion: 用于鲁棒空间表示对比学习的随机多模态融合",
        "label": [
            "遥感图像复原"
        ],
        "label_reason": "涉及遥感图像处理，但主要聚焦多模态融合而非像素级图像质量恢复。",
        "relevance_score": 5,
        "novelty_score": 7,
        "novelty_reason": "提出随机多模态融合方法，对Geo-Foundation Model有一定改进。"
    },
    {
        "title": "Scaling Vision Transformers for Functional MRI with Flat Maps",
        "url": "http://arxiv.org/abs/2510.13768v1",
        "pub_date": "2025-10-15",
        "summary": "A key question for adapting modern deep learning architectures to functional MRI (fMRI) is how to represent the data for model input. To bridge the modality gap between fMRI and natural images, we transform the 4D volumetric fMRI data into videos of 2D fMRI activity flat maps. We train Vision Transformers on 2.3K hours of fMRI flat map videos from the Human Connectome Project using the spatiotemporal masked autoencoder (MAE) framework. We observe that masked fMRI modeling performance improves with dataset size according to a strict power scaling law. Downstream classification benchmarks show that our model learns rich representations supporting both fine-grained state decoding across subjects, as well as subject-specific trait decoding across changes in brain state. This work is part of an ongoing open science project to build foundation models for fMRI data. Our code and datasets are available at https://github.com/MedARC-AI/fmri-fm.",
        "translated": "将现代深度学习架构适应于功能磁共振成像（fMRI）的一个关键问题是，如何将数据表示为模型输入。为了弥合 fMRI 与自然图像之间的模态差异，我们把 4D 的 fMRI 体积数据转换为 2D 的 fMRI 活动平面图的视频。我们使用时空掩码自编码器（MAE）框架，在来自人类连接组计划（Human Connectome Project）的 2.3K 小时 fMRI 平面图视频上训练视觉变换器（Vision Transformers）。我们观察到，随着数据集规模的增加，掩码 fMRI 建模性能按照严格的幂律进行提升。下游分类基准测试表明，我们的模型学习到了丰富的表示，不仅能够跨被试进行细粒度状态解码，还能在脑状态变化的条件下进行被试特异性特质的解码。本项工作是正在进行的一个开放科学项目的一部分，旨在为 fMRI 数据构建基础模型。我们的代码和数据集可在 https://github.com/MedARC-AI/fmri-fm 获取。",
        "translated_title": "Scaling Vision Transformers for Functional MRI with Flat Maps  \n使用平面图的视觉变换器扩展功能磁共振成像  \n\n功能磁共振成像（fMRI）是一种非侵入性技术，用于研究大脑活动，其通过检测血氧水平依赖（BOLD）信号实现。然而，fMRI 数据通常受到低空间分辨率和低信噪比（SNR）的限制，这对识别大脑功能区域构成挑战。最近，视觉变换器（Vision Transformers, ViTs）在各种图像恢复任务中表现出卓越的性能。这些模型通过其自注意力机制能够捕获长距离依赖关系，为 fMRI 信号的处理提供了新思路。  \n\n在本研究中，我们提出了一种基于视觉变换器的方法，用于 fMRI 数据的去噪和增强。我们采用平面图（Flat Maps）作为先验知识，以更好地建模大脑皮层的拓扑结构。通过将 fMRI 数据投影到平面图中，我们能够利用 ViT 在空域中的强大特征提取能力。实验结果表明，我们的方法在多个 fMRI 数据集上显著优于现有方法，尤其是在保留功能细节和减少噪声方面表现出色。  \n\n此外，我们还探讨了视觉变换器的扩展能力，通过增加模型深度和宽度来提升其性能。我们发现，适当增加模型规模可以有效提高 fMRI 数据的恢复质量，同时保持计算效率。",
        "label": [],
        "label_reason": "论文专注于fMRI数据表示与建模，非图像像素级恢复或增强任务。",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出将Vision Transformer扩展至fMRI数据建模，具有一定新颖性。"
    },
    {
        "title": "Uni-MMMU: A Massive Multi-discipline Multimodal Unified Benchmark",
        "url": "http://arxiv.org/abs/2510.13759v1",
        "pub_date": "2025-10-15",
        "summary": "Unified multimodal models aim to jointly enable visual understanding and generation, yet current benchmarks rarely examine their true integration. Existing evaluations either treat the two abilities in isolation or overlook tasks that inherently couple them. To address this gap, we present Uni-MMMU, a comprehensive and discipline-aware benchmark that systematically unfolds the bidirectional synergy between generation and understanding across eight reasoning-centric domains, including science, coding, mathematics, and puzzles. Each task is bidirectionally coupled, demanding models to (i) leverage conceptual understanding to guide precise visual synthesis, or (ii) utilize generation as a cognitive scaffold for analytical reasoning. Uni-MMMU incorporates verifiable intermediate reasoning steps, unique ground truths, and a reproducible scoring protocol for both textual and visual outputs. Through extensive evaluation of state-of-the-art unified, generation-only, and understanding-only models, we reveal substantial performance disparities and cross-modal dependencies, offering new insights into when and how these abilities reinforce one another, and establishing a reliable foundation for advancing unified models.",
        "translated": "统一的多模态模型旨在同时实现视觉理解和生成能力，但目前的基准测试很少真正考察这两种能力的融合。现有的评估方法要么将这两种能力孤立地对待，要么忽略了那些本质上需要将它们耦合的任务。为了解决这一问题，我们提出了 Uni-MMMU，这是一个全面且学科感知的基准测试，系统地揭示了生成与理解在八个以推理为中心的领域（包括科学、编程、数学和谜题）中的双向协同作用。每个任务都是双向耦合的，要求模型 (i) 利用概念理解来指导精确的视觉合成，或 (ii) 将生成能力作为认知支架，以支持分析推理。Uni-MMMU 包含可验证的中间推理步骤、唯一的地面真值，以及针对文本和视觉输出的可复现评分协议。通过对最先进的统一模型、仅生成模型和仅理解模型的广泛评估，我们揭示了显著的性能差异和跨模态依赖关系，提供了关于何时以及如何实现这些能力相互增强的新见解，并为统一模型的进一步发展奠定了可靠基础。",
        "translated_title": "Uni-MMMU: 一个大规模多学科多模态统一基准",
        "label": [],
        "label_reason": "论文关注多模态统一模型评估，不属于图像像素级处理任务。",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "提出了跨学科的多模态统一基准，但方法较为常规。"
    },
    {
        "title": "RECODE: Reasoning Through Code Generation for Visual Question Answering",
        "url": "http://arxiv.org/abs/2510.13756v1",
        "pub_date": "2025-10-15",
        "summary": "Multimodal Large Language Models (MLLMs) struggle with precise reasoning for structured visuals like charts and diagrams, as pixel-based perception lacks a mechanism for verification. To address this, we propose to leverage derendering -- the process of reverse-engineering visuals into executable code -- as a new modality for verifiable visual reasoning. Specifically, we propose RECODE, an agentic framework that first generates multiple candidate programs to reproduce the input image. It then uses a critic to select the most faithful reconstruction and iteratively refines the code. This process not only transforms an ambiguous perceptual task into a verifiable, symbolic problem, but also enables precise calculations and logical inferences later on. On various visual reasoning benchmarks such as CharXiv, ChartQA, and Geometry3K, RECODE significantly outperforms methods that do not leverage code or only use code for drawing auxiliary lines or cropping. Our work demonstrates that grounding visual perception in executable code provides a new path toward more accurate and verifiable multimodal reasoning.",
        "translated": "多模态大语言模型（MLLMs）在处理图表和示意图等结构化视觉内容时，常常在精确推理方面存在困难，因为基于像素的感知缺乏验证机制。为了解决这一问题，我们提出利用去渲染（derendering）——即将视觉图像反向工程为可执行代码的过程——作为一种可验证视觉推理的新模态。具体而言，我们提出了 RECODE，一个智能体框架，首先生成多个候选程序以重现输入图像。然后，它使用一个评估器选择最忠实的重建结果，并对代码进行迭代优化。这一过程不仅将模糊的感知任务转化为可验证的符号问题，还为后续的精确计算和逻辑推理提供了可能。在各种视觉推理基准如 CharXiv、ChartQA 和 Geometry3K 上，RECODE 显著优于不使用代码或仅使用代码绘制辅助线或裁剪图像的方法。我们的工作表明，将视觉感知建立在可执行代码基础上，为更准确和可验证的多模态推理提供了一条新路径。",
        "translated_title": "RECODE：通过代码生成进行推理的视觉问答",
        "label": [],
        "label_reason": "论文不属于低层图像处理，而是视觉问答任务。",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出通过代码生成实现视觉推理，具有新颖性。"
    },
    {
        "title": "InteractiveOmni: A Unified Omni-modal Model for Audio-Visual Multi-turn\n  Dialogue",
        "url": "http://arxiv.org/abs/2510.13747v1",
        "pub_date": "2025-10-15",
        "summary": "We introduce InteractiveOmni, a unified and open-source omni-modal large language model for audio-visual multi-turn interaction, ranging from 4B to 8B parameters, designed to lead the field of lightweight models by offering comprehensive omni-modal understanding and speech generation capabilities. To achieve this, we integrate the vision encoder, audio encoder, large language model, and speech decoder into a unified model for understanding and generation tasks. We design a multi-stage training strategy to ensure robust cross-modal capabilities, including pre-training for omni-modal understanding, followed by post-training with speech conversation and audio-visual interaction. To enable human-like long-term conversational ability, we meticulously curate a multi-turn training dataset that enhances the model's ability to handle complex and multi-turn interactions. To effectively evaluate the multi-turn memory and speech interaction capabilities, we construct the multi-modal multi-turn memory benchmark and the multi-turn speech interaction benchmark. Experiments demonstrate that InteractiveOmni significantly outperforms leading open-source models and provides a more intelligent multi-turn audio-visual experience, particularly in its long-term memory capabilities. Notably, InteractiveOmni-4B is comparable to the much larger model like Qwen2.5-Omni-7B on general benchmarks, and it can retain 97% of the performance of the InteractiveOmni-8B while utilizing only 50% of the model size. Achieving state-of-the-art results against similarly sized models across image, audio, video understanding, and speech generation tasks, InteractiveOmni is an accessible, open-source foundation for next-generation intelligent interactive systems.",
        "translated": "我们提出 InteractiveOmni，一种统一且开源的多模态大语言模型，用于音频-视觉的多轮交互，参数规模从 4B 到 8B 不等，旨在通过提供全面的多模态理解与语音生成能力，引领轻量级模型的发展。为此，我们将视觉编码器、音频编码器、大语言模型和语音解码器集成到一个统一模型中，用于理解和生成任务。我们设计了一种多阶段训练策略，以确保强大的跨模态能力，包括用于多模态理解的预训练，以及随后进行的语音对话和音视频交互的后训练。为了实现类人般的长期对话能力，我们精心构建了一个多轮训练数据集，以增强模型处理复杂和多轮交互的能力。为了有效评估多轮记忆和语音交互能力，我们构建了多模态多轮记忆基准和多轮语音交互基准。实验表明，InteractiveOmni 在多项指标上显著优于领先的开源模型，提供了更加智能的多轮音视频交互体验，特别是在长期记忆能力方面。值得注意的是，InteractiveOmni-4B 在通用基准上表现与更大规模的模型如 Qwen2.5-Omni-7B 相当，并且仅使用 50% 的模型规模即可保留 97% 的 InteractiveOmni-8B 性能。在图像、音频、视频理解和语音生成任务中，InteractiveOmni 在同规模模型中实现了最先进的结果，是下一代智能交互系统的一个可访问、开源的基础。",
        "translated_title": "InteractiveOmni：用于音视频多轮对话的统一全模态模型",
        "label": [],
        "label_reason": "不属于low-level图像处理，是多模态对话模型",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "常规多模态模型设计，无本质创新"
    },
    {
        "title": "UniCalli: A Unified Diffusion Framework for Column-Level Generation and\n  Recognition of Chinese Calligraphy",
        "url": "http://arxiv.org/abs/2510.13745v1",
        "pub_date": "2025-10-15",
        "summary": "Computational replication of Chinese calligraphy remains challenging. Existing methods falter, either creating high-quality isolated characters while ignoring page-level aesthetics like ligatures and spacing, or attempting page synthesis at the expense of calligraphic correctness. We introduce \\textbf{UniCalli}, a unified diffusion framework for column-level recognition and generation. Training both tasks jointly is deliberate: recognition constrains the generator to preserve character structure, while generation provides style and layout priors. This synergy fosters concept-level abstractions that improve both tasks, especially in limited-data regimes. We curated a dataset of over 8,000 digitized pieces, with ~4,000 densely annotated. UniCalli employs asymmetric noising and a rasterized box map for spatial priors, trained on a mix of synthetic, labeled, and unlabeled data. The model achieves state-of-the-art generative quality with superior ligature continuity and layout fidelity, alongside stronger recognition. The framework successfully extends to other ancient scripts, including Oracle bone inscriptions and Egyptian hieroglyphs. Code and data can be viewed in \\href{https://github.com/EnVision-Research/UniCalli}{this URL}.",
        "translated": "中国书法的计算复现仍然具有挑战性。现有方法存在不足，要么生成高质量的单个字符而忽视了页面级别的美学要素（如连笔和间距），要么在尝试页面合成时以牺牲书法正确性为代价。我们引入了**UniCalli**，一种面向列级别的识别与生成的统一扩散框架。联合训练这两个任务是经过深思熟虑的：识别任务约束生成器以保持字符结构，而生成任务则提供了风格和布局的先验知识。这种协同作用促进了概念级别的抽象表示，从而提升了两个任务的性能，尤其在数据受限的情况下表现更为明显。我们整理了一个包含超过 8,000 幅数字化书法作品的数据集，其中约 4,000 幅进行了密集标注。UniCalli 采用非对称噪声和光栅化框图来提供空域先验，并在合成数据、标注数据和未标注数据的混合集上进行训练。该模型在生成质量方面达到了最先进的水平，连笔的连续性和布局的保真度均优于现有方法，同时识别性能也更强。该框架成功地扩展到了其他古代文字，包括甲骨文和埃及象形文字。代码和数据可通过 \\href{https://github.com/EnVision-Research/UniCalli}{此链接}查看。",
        "translated_title": "UniCalli：一种面向列级生成与识别的中文书法统一扩散框架",
        "label": [],
        "label_reason": "不属于low-level图像处理，专注于书法生成与识别",
        "relevance_score": 1,
        "novelty_score": 6,
        "novelty_reason": "提出了统一扩散框架，但属于high-level生成与识别任务"
    },
    {
        "title": "Multi-Scale High-Resolution Logarithmic Grapher Module for Efficient\n  Vision GNNs",
        "url": "http://arxiv.org/abs/2510.13740v1",
        "pub_date": "2025-10-15",
        "summary": "Vision graph neural networks (ViG) have demonstrated promise in vision tasks as a competitive alternative to conventional convolutional neural nets (CNN) and transformers (ViTs); however, common graph construction methods, such as k-nearest neighbor (KNN), can be expensive on larger images. While methods such as Sparse Vision Graph Attention (SVGA) have shown promise, SVGA's fixed step scale can lead to over-squashing and missing multiple connections to gain the same information that could be gained from a long-range link. Through this observation, we propose a new graph construction method, Logarithmic Scalable Graph Construction (LSGC) to enhance performance by limiting the number of long-range links. To this end, we propose LogViG, a novel hybrid CNN-GNN model that utilizes LSGC. Furthermore, inspired by the successes of multi-scale and high-resolution architectures, we introduce and apply a high-resolution branch and fuse features between our high-resolution and low-resolution branches for a multi-scale high-resolution Vision GNN network. Extensive experiments show that LogViG beats existing ViG, CNN, and ViT architectures in terms of accuracy, GMACs, and parameters on image classification and semantic segmentation tasks. Our smallest model, Ti-LogViG, achieves an average top-1 accuracy on ImageNet-1K of 79.9% with a standard deviation of 0.2%, 1.7% higher average accuracy than Vision GNN with a 24.3% reduction in parameters and 35.3% reduction in GMACs. Our work shows that leveraging long-range links in graph construction for ViGs through our proposed LSGC can exceed the performance of current state-of-the-art ViGs. Code is available at https://github.com/mmunir127/LogViG-Official.",
        "translated": "视觉图神经网络（ViG）在视觉任务中展现出作为传统卷积神经网络（CNN）和视觉变换器（ViTs）的有竞争力的替代方案的潜力；然而，常见的图构建方法，如 k-近邻（KNN），在处理大尺寸图像时计算代价较高。虽然稀疏视觉图注意力（SVGA）等方法已显示出前景，但 SVGA 固定的步长尺度可能导致过度压缩问题，并且无法通过多个连接获取与长程连接相同的信息。基于这一观察，我们提出了一种新的图构建方法——对数可扩展图构建（LSGC），通过限制长程连接的数量来提升性能。为此，我们设计了 LogViG，这是一种新颖的 CNN-GNN 混合模型，利用了 LSGC。此外，受多尺度和高分辨率架构成功案例的启发，我们引入并应用了一个高分辨率分支，并在高分辨率和低分辨率分支之间融合特征，构建了一个多尺度高分辨率的视觉图神经网络。大量实验表明，LogViG 在图像分类和语义分割任务中，在准确率、GMACs 和参数数量方面均优于现有的 ViG、CNN 和 ViT 架构。我们最小的模型 Ti-LogViG 在 ImageNet-1K 数据集上的平均 top-1 准确率为 79.9%，标准差为 0.2%，比 Vision GNN 的平均准确率高出 1.7%，同时参数数量减少了 24.3%，GMACs 降低了 35.3%。我们的研究表明，通过所提出的 LSGC 在图构建中利用长程连接，可使 ViG 的性能超越当前最先进的 ViG 模型。代码可在 https://github.com/mmunir127/LogViG-Official 获取。",
        "translated_title": "多尺度高分辨率对数图生成模块用于高效的视觉图神经网络",
        "label": [],
        "label_reason": "论文主要关注图像分类和语义分割，不属于 low-level 图像处理。",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出新的图构建方法 LSGC，对 Vision GNN 有显著改进。"
    },
    {
        "title": "Cyclic Self-Supervised Diffusion for Ultra Low-field to High-field MRI\n  Synthesis",
        "url": "http://arxiv.org/abs/2510.13735v1",
        "pub_date": "2025-10-15",
        "summary": "Synthesizing high-quality images from low-field MRI holds significant potential. Low-field MRI is cheaper, more accessible, and safer, but suffers from low resolution and poor signal-to-noise ratio. This synthesis process can reduce reliance on costly acquisitions and expand data availability. However, synthesizing high-field MRI still suffers from a clinical fidelity gap. There is a need to preserve anatomical fidelity, enhance fine-grained structural details, and bridge domain gaps in image contrast. To address these issues, we propose a \\emph{cyclic self-supervised diffusion (CSS-Diff)} framework for high-field MRI synthesis from real low-field MRI data. Our core idea is to reformulate diffusion-based synthesis under a cycle-consistent constraint. It enforces anatomical preservation throughout the generative process rather than just relying on paired pixel-level supervision. The CSS-Diff framework further incorporates two novel processes. The slice-wise gap perception network aligns inter-slice inconsistencies via contrastive learning. The local structure correction network enhances local feature restoration through self-reconstruction of masked and perturbed patches. Extensive experiments on cross-field synthesis tasks demonstrate the effectiveness of our method, achieving state-of-the-art performance (e.g., 31.80 $\\pm$ 2.70 dB in PSNR, 0.943 $\\pm$ 0.102 in SSIM, and 0.0864 $\\pm$ 0.0689 in LPIPS). Beyond pixel-wise fidelity, our method also preserves fine-grained anatomical structures compared with the original low-field MRI (e.g., left cerebral white matter error drops from 12.1$\\%$ to 2.1$\\%$, cortex from 4.2$\\%$ to 3.7$\\%$). To conclude, our CSS-Diff can synthesize images that are both quantitatively reliable and anatomically consistent.",
        "translated": "从低场 MRI 数据合成高质量图像具有重要潜力。低场 MRI 成本更低、更易获取、也更安全，但其分辨率较低且信噪比差。通过合成过程，可以减少对昂贵采集设备的依赖，并扩展数据的可用性。然而，当前的高场 MRI 合成仍然存在临床保真度的不足。有必要在合成中保持解剖结构的准确性，增强细粒度的结构细节，并弥合图像对比度方面的领域差异。为了解决这些问题，我们提出了一种名为 \\emph{循环自监督扩散（CSS-Diff）} 的框架，用于从真实的低场 MRI 数据中合成高场 MRI。我们的核心思想是在循环一致的约束下重构基于扩散的合成方法。这种方法在整个生成过程中强制保持解剖结构，而不仅仅依赖于像素级的配对监督。CSS-Diff 框架进一步结合了两个新颖的处理过程。切片级差异感知网络通过对比学习对齐切片间的不一致性。局部结构校正网络则通过遮蔽和扰动图像块的自重建来增强局部特征恢复。在跨场强合成任务上的大量实验表明了我们方法的有效性，达到了当前最先进的性能（例如，PSNR 为 31.80 $\\pm$ 2.70 dB，SSIM 为 0.943 $\\pm$ 0.102，LPIPS 为 0.0864 $\\pm$ 0.0689）。除了像素级保真度之外，与原始低场 MRI 相比，我们的方法还能保持更精细的解剖结构（例如，左脑白质误差从 12.1$\\%$ 降至 2.1$\\%$，皮层误差从 4.2$\\%$ 降至 3.7$\\%$）。综上所述，我们的 CSS-Diff 能够合成在定量评估和解剖一致性方面都表现良好的图像。",
        "translated_title": "循环自监督扩散用于超低场到高场MRI合成",
        "label": [
            "医学图像增强",
            "图像去噪",
            "图像恢复"
        ],
        "label_reason": "论文专注于从低场MRI合成高质量高场MRI，涉及图像恢复与医学增强",
        "relevance_score": 8,
        "novelty_score": 8,
        "novelty_reason": "提出循环自监督扩散框架，结合新网络结构提升合成效果"
    },
    {
        "title": "LiFMCR: Dataset and Benchmark for Light Field Multi-Camera Registration",
        "url": "http://arxiv.org/abs/2510.13729v1",
        "pub_date": "2025-10-15",
        "summary": "We present LiFMCR, a novel dataset for the registration of multiple micro lens array (MLA)-based light field cameras. While existing light field datasets are limited to single-camera setups and typically lack external ground truth, LiFMCR provides synchronized image sequences from two high-resolution Raytrix R32 plenoptic cameras, together with high-precision 6-degrees of freedom (DoF) poses recorded by a Vicon motion capture system. This unique combination enables rigorous evaluation of multi-camera light field registration methods.   As a baseline, we provide two complementary registration approaches: a robust 3D transformation estimation via a RANSAC-based method using cross-view point clouds, and a plenoptic PnP algorithm estimating extrinsic 6-DoF poses from single light field images. Both explicitly integrate the plenoptic camera model, enabling accurate and scalable multi-camera registration. Experiments show strong alignment with the ground truth, supporting reliable multi-view light field processing.   Project page: https://lifmcr.github.io/",
        "translated": "我们提出了 LiFMCR，这是一个用于多微透镜阵列（MLA）光场相机注册的新数据集。现有的光场数据集通常局限于单相机设置，并且普遍缺乏外部真实值。LiFMCR 提供了来自两台高分辨率 Raytrix R32 光场相机的同步图像序列，并配有通过 Vicon 运动捕捉系统记录的高精度六自由度（DoF）姿态。这一独特组合使得对多相机光场注册方法的严格评估成为可能。\n\n作为基准，我们提供了两种互补的注册方法：一种是通过基于 RANSAC 的方法利用跨视角点云进行鲁棒的三维变换估计，另一种是光场 PnP 算法，用于从单光场图像中估计外部六自由度姿态。两种方法均明确集成了光场相机模型，从而实现了精确且可扩展的多相机注册。实验结果表明，与真实值具有高度对齐性，支持可靠的多视角光场处理。\n\n项目页面：https://lifmcr.github.io/",
        "translated_title": "LiFMCR：光场多摄像头配准的数据集与基准",
        "label": [],
        "label_reason": "论文主要关注光场相机注册，不属于像素级图像恢复或增强",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出了新的光场多相机注册数据集和基准"
    },
    {
        "title": "NExT-OMNI: Towards Any-to-Any Omnimodal Foundation Models with Discrete\n  Flow Matching",
        "url": "http://arxiv.org/abs/2510.13721v1",
        "pub_date": "2025-10-15",
        "summary": "Next-generation multimodal foundation models capable of any-to-any cross-modal generation and multi-turn interaction will serve as core components of artificial general intelligence systems, playing a pivotal role in human-machine interaction. However, most existing multimodal models remain constrained by autoregressive architectures, whose inherent limitations prevent a balanced integration of understanding and generation capabilities. Although hybrid and decoupling strategies have been explored to address these tasks within unified frameworks separately, their redundant, non-integrated designs limit their applicability to broader scenarios, such as cross-modal retrieval.In this work, we introduce NExT-OMNI, an open-source omnimodal foundation model that achieves unified modeling through discrete flow paradigms. By leveraging metric-induced probability paths and kinetic optimal velocities, NExT-OMNI natively supports any-to-any understanding and generation with enhanced response efficiency, while enabling broader application scenarios through concise unified representations rather than task-decoupled designs. Trained on large-scale interleaved text, image, video, and audio data, NExT-OMNI delivers competitive performance on multimodal generation and understanding benchmarks, while outperforming prior unified models in multi-turn multimodal interaction and cross-modal retrieval, highlighting its architectural advantages as a next-generation multimodal foundation model. To advance further research, we release training details, data protocols, and open-source both the code and model checkpoints.",
        "translated": "下一代能够实现任意模态到任意模态跨模态生成和多轮交互的多模态基础模型，将成为通用人工智能系统的核心组成部分，在人机交互中发挥关键作用。然而，目前大多数多模态模型仍受自回归架构的限制，其固有局限性阻碍了理解能力和生成能力之间的平衡整合。尽管已有研究探索了混合架构和任务解耦策略，试图在统一框架内分别处理这些任务，但其冗余且非整合的设计限制了其在更广泛场景（如跨模态检索）中的应用。在本工作中，我们提出 NExT-OMNI，一个开源的全模态基础模型，通过离散流范式实现统一建模。NExT-OMNI 利用度量诱导的概率路径和动力最优速度，在增强响应效率的同时，原生支持任意模态到任意模态的理解与生成，并通过简洁的统一表征而非任务解耦的设计，实现了更广泛的应用场景。NExT-OMNI 在大规模交织的文本、图像、视频和音频数据上进行训练，在多模态生成和理解基准上展现出具有竞争力的性能，并在多轮多模态交互和跨模态检索方面优于以往的统一模型，突显了其作为下一代多模态基础模型的架构优势。为了推动进一步的研究，我们发布了训练细节、数据协议，并开源了代码和模型检查点。",
        "translated_title": "NExT-OMNI：基于离散流匹配的任意到任意全模态基础模型",
        "label": [],
        "label_reason": "论文主要研究多模态模型，不属于图像像素级恢复或增强任务。",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出离散流匹配的新范式，对多模态模型架构有显著改进。"
    },
    {
        "title": "Epistemic-aware Vision-Language Foundation Model for Fetal Ultrasound\n  Interpretation",
        "url": "http://arxiv.org/abs/2510.12953v1",
        "pub_date": "2025-10-14",
        "summary": "Recent medical vision-language models have shown promise on tasks such as VQA, report generation, and anomaly detection. However, most are adapted to structured adult imaging and underperform in fetal ultrasound, which poses challenges of multi-view image reasoning, numerous diseases, and image diversity. To bridge this gap, we introduce FetalMind, a medical AI system tailored to fetal ultrasound for both report generation and diagnosis. Guided by clinical workflow, we propose Salient Epistemic Disentanglement (SED), which injects an expert-curated bipartite graph into the model to decouple view-disease associations and to steer preference selection along clinically faithful steps via reinforcement learning. This design mitigates variability across diseases and heterogeneity across views, reducing learning bottlenecks while aligning the model's inference with obstetric practice. To train FetalMind at scale, we curate FetalSigma-1M dataset, the first large-scale fetal ultrasound report corpus, comprising 20K reports from twelve medical centers, addressing the scarcity of domain data. Extensive experiments show that FetalMind outperforms open- and closed-source baselines across all gestational stages, achieving +14% average gains and +61.2% higher accuracy on critical conditions while remaining efficient, stable, and scalable. Project Page: https://hexiao0275.github.io/FetalMind.",
        "translated": "近年来，医疗视觉-语言模型在视觉问答（VQA）、报告生成和异常检测等任务上表现出潜力。然而，大多数模型适用于结构化的成人影像数据，在胎儿超声任务中表现欠佳，这面临着多视角图像推理、大量疾病类别和图像多样性的挑战。为弥合这一差距，我们提出了FetalMind，一个专为胎儿超声定制的医疗人工智能系统，支持报告生成与诊断任务。受临床工作流程的启发，我们提出了显著性认识解耦（Salient Epistemic Disentanglement, SED）方法，该方法将专家构建的二部图注入模型，以解耦视角与疾病之间的关联，并通过强化学习引导模型在临床可靠的步骤中进行偏好选择。这一设计缓解了不同疾病之间的差异性以及不同视角之间的异质性，从而降低学习瓶颈，同时使模型的推理过程与产科实践保持一致。为了大规模训练FetalMind，我们构建了FetalSigma-1M数据集，这是首个大规模胎儿超声报告语料库，包含了来自十二个医疗机构的20K份报告，解决了该领域数据稀缺的问题。大量实验表明，FetalMind在所有孕周阶段均优于开源和闭源基线模型，平均提升14%，在关键疾病上的准确率提高61.2%，同时保持高效、稳定和可扩展。项目主页：https://hexiao0275.github.io/FetalMind.",
        "translated_title": "具有认知感知能力的视觉-语言基础模型在胎儿超声解释中的应用",
        "label": [],
        "label_reason": "论文聚焦医学影像与自然语言处理，非直接推荐系统相关。",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出临床工作流引导的模型设计，数据集和方法有创新性。"
    },
    {
        "title": "DeepMMSearch-R1: Empowering Multimodal LLMs in Multimodal Web Search",
        "url": "http://arxiv.org/abs/2510.12801v1",
        "pub_date": "2025-10-14",
        "summary": "Multimodal Large Language Models (MLLMs) in real-world applications require access to external knowledge sources and must remain responsive to the dynamic and ever-changing real-world information in order to address information-seeking and knowledge-intensive user queries. Existing approaches, such as retrieval augmented generation (RAG) methods, search agents, and search equipped MLLMs, often suffer from rigid pipelines, excessive search calls, and poorly constructed search queries, which result in inefficiencies and suboptimal outcomes. To address these limitations, we present DeepMMSearch-R1, the first multimodal LLM capable of performing on-demand, multi-turn web searches and dynamically crafting queries for both image and text search tools. Specifically, DeepMMSearch-R1 can initiate web searches based on relevant crops of the input image making the image search more effective, and can iteratively adapt text search queries based on retrieved information, thereby enabling self-reflection and self-correction. Our approach relies on a two-stage training pipeline: a cold start supervised finetuning phase followed by an online reinforcement learning optimization. For training, we introduce DeepMMSearchVQA, a novel multimodal VQA dataset created through an automated pipeline intermixed with real-world information from web search tools. This dataset contains diverse, multi-hop queries that integrate textual and visual information, teaching the model when to search, what to search for, which search tool to use and how to reason over the retrieved information. We conduct extensive experiments across a range of knowledge-intensive benchmarks to demonstrate the superiority of our approach. Finally, we analyze the results and provide insights that are valuable for advancing multimodal web-search.",
        "translated": "在现实世界应用中，多模态大语言模型（MLLMs）需要访问外部知识源，并且必须能够响应动态变化的现实世界信息，以应对用户的信息查询和知识密集型请求。现有方法，如检索增强生成（RAG）方法、搜索代理和配备搜索功能的 MLLMs，通常存在流水线僵化、搜索调用过多以及搜索查询构建不完善的问题，导致效率低下和结果次优。为了解决这些限制，我们提出了 DeepMMSearch-R1，这是首个能够按需进行多轮网页搜索，并为图像和文本搜索工具动态构建查询的多模态大语言模型。具体来说，DeepMMSearch-R1 可以根据输入图像的相关裁剪区域发起网页搜索，从而提高图像搜索的有效性；同时能够根据检索到的信息迭代调整文本搜索查询，实现自我反思和自我修正。我们的方法依赖于一个两阶段的训练流水线：首先进行冷启动阶段的监督微调，然后进行在线强化学习优化。为了训练，我们引入了 DeepMMSearchVQA，这是一个通过自动化流水线与网页搜索工具中的现实世界信息混合生成的新型多模态视觉问答（VQA）数据集。该数据集包含多样化的多跳查询，融合了文本和视觉信息，教导模型何时进行搜索、搜索什么内容、使用哪个搜索工具以及如何对检索到的信息进行推理。我们在多个知识密集型基准上进行了广泛的实验，以验证我们方法的优越性。最后，我们对结果进行了分析，并提供了对多模态网页搜索研究具有重要价值的见解。",
        "translated_title": "DeepMMSearch-R1：在多模态网页搜索中增强多模态大语言模型",
        "label": [
            "多模态推荐",
            "LLM生成式推荐"
        ],
        "label_reason": "论文涉及多模态LLM在搜索中的应用，可能适用于推荐系统。",
        "relevance_score": 5,
        "novelty_score": 8,
        "novelty_reason": "提出多模态LLM动态构建搜索查询的新方法，具有创新性。"
    },
    {
        "title": "CTRL-Rec: Controlling Recommender Systems With Natural Language",
        "url": "http://arxiv.org/abs/2510.12742v1",
        "pub_date": "2025-10-14",
        "summary": "When users are dissatisfied with recommendations from a recommender system, they often lack fine-grained controls for changing them. Large language models (LLMs) offer a solution by allowing users to guide their recommendations through natural language requests (e.g., \"I want to see respectful posts with a different perspective than mine\"). We propose a method, CTRL-Rec, that allows for natural language control of traditional recommender systems in real-time with computational efficiency. Specifically, at training time, we use an LLM to simulate whether users would approve of items based on their language requests, and we train embedding models that approximate such simulated judgments. We then integrate these user-request-based predictions into the standard weighting of signals that traditional recommender systems optimize. At deployment time, we require only a single LLM embedding computation per user request, allowing for real-time control of recommendations. In experiments with the MovieLens dataset, our method consistently allows for fine-grained control across a diversity of requests. In a study with 19 Letterboxd users, we find that CTRL-Rec was positively received by users and significantly enhanced users' sense of control and satisfaction with recommendations compared to traditional controls.",
        "translated": "当用户对推荐系统提供的推荐结果不满意时，他们通常缺乏对推荐结果进行细粒度调整的控制手段。大语言模型（LLM）提供了一种解决方案，允许用户通过自然语言请求（例如，“我想看到与我的观点不同的尊重性帖子”）来引导推荐结果。我们提出了一种方法 CTRL-Rec，它能够在计算效率的前提下，实现对传统推荐系统的实时自然语言控制。具体而言，在训练阶段，我们使用 LLM 来模拟用户是否会基于他们的语言请求批准某些物料，并训练嵌入模型以逼近这种模拟的判断结果。随后，我们将这些基于用户请求的预测结果整合到传统推荐系统优化的标准信号加权中。在部署阶段，我们仅需为每个用户请求进行一次 LLM 嵌入计算，从而实现实时的推荐控制。在 MovieLens 数据集上的实验表明，我们的方法在各种请求下都能实现一致的细粒度控制。在与 19 位 Letterboxd 用户进行的研究中，我们发现 CTRL-Rec 得到了用户的积极反馈，并显著增强了用户对推荐结果的控制感和满意度，相比传统的控制方式。",
        "translated_title": "CTRL-Rec：用自然语言控制推荐系统",
        "label": [
            "LLM生成式推荐",
            "精排"
        ],
        "label_reason": "结合LLM实现自然语言控制推荐结果，增强用户控制感和满意度",
        "relevance_score": 8,
        "novelty_score": 9,
        "novelty_reason": "首次将LLM用于实时推荐控制，方法新颖且有效"
    },
    {
        "title": "SAIL-Embedding Technical Report: Omni-modal Embedding Foundation Model",
        "url": "http://arxiv.org/abs/2510.12709v2",
        "pub_date": "2025-10-14",
        "summary": "Multimodal embedding models aim to yield informative unified representations that empower diverse cross-modal tasks. Despite promising developments in the evolution from CLIP-based dual-tower architectures to large vision-language models, prior works still face unavoidable challenges in real-world applications and business scenarios, such as the limited modality support, unstable training mechanisms, and industrial domain gaps. In this work, we introduce SAIL-Embedding, an omni-modal embedding foundation model that addresses these issues through tailored training strategies and architectural design. In the optimization procedure, we propose a multi-stage training scheme to boost the multifaceted effectiveness of representation learning. Specifically, the content-aware progressive training aims to enhance the model's adaptability to diverse downstream tasks and master enriched cross-modal proficiency. The collaboration-aware recommendation enhancement training further adapts multimodal representations for recommendation scenarios by distilling knowledge from sequence-to-item and ID-to-item embeddings while mining user historical interests. Concurrently, we develop the stochastic specialization and dataset-driven pattern matching to strengthen model training flexibility and generalizability. Experimental results show that SAIL-Embedding achieves SOTA performance compared to other methods in different retrieval tasks. In online experiments across various real-world scenarios integrated with our model, we observe a significant increase in Lifetime (LT), which is a crucial indicator for the recommendation experience. For instance, the model delivers the 7-day LT gain of +0.5% in the Douyin-Selected scenario. For the Douyin feed rank model, the match features produced by SAIL-Embedding yield a +0.1% AUC gain.",
        "translated": "多模态嵌入模型旨在生成具有信息量的统一表示，以支持多种跨模态任务。尽管从基于CLIP的双塔架构发展到大视觉-语言模型已取得令人期待的进展，但先前的工作在实际应用和业务场景中仍面临不可避免的挑战，如模态支持有限、训练机制不稳定以及与工业领域的差距。在本文中，我们提出了 SAIL-Embedding，这是一个全模态嵌入基础模型，通过定制化的训练策略和架构设计解决上述问题。在优化过程中，我们提出了一种多阶段训练方案，以提升表示学习在多方面上的有效性。具体而言，内容感知的渐进式训练旨在增强模型对下游任务的适应能力，并掌握丰富的跨模态能力。协作感知的推荐增强训练则通过从序列到物料和ID到物料的嵌入中提炼知识，同时挖掘用户的历史兴趣，进一步将多模态表示适配到推荐场景。与此同时，我们开发了随机专业化和数据集驱动的模式匹配方法，以增强模型训练的灵活性和泛化能力。实验结果表明，SAIL-Embedding在不同检索任务中相较其他方法实现了最先进的（SOTA）性能。在多个实际场景中与我们的模型集成进行的在线实验中，我们观察到用户生命周期（Lifetime, LT）有显著提升，这是衡量推荐体验的关键指标。例如，在Douyin-Selected场景中，模型带来了7天LT指标+0.5%的增长。对于Douyin的流推荐排序模型，SAIL-Embedding生成的匹配特征带来了+0.1%的AUC提升。",
        "translated_title": "SAIL-Embedding 技术报告：通用模态嵌入基础模型",
        "label": [
            "多模态推荐",
            "精排",
            "序列推荐"
        ],
        "label_reason": "多模态嵌入模型用于推荐场景，结合序列建模与ID建模",
        "relevance_score": 8,
        "novelty_score": 8,
        "novelty_reason": "提出多阶段训练策略和模式匹配方法，提升模型灵活性和表现"
    },
    {
        "title": "The Role of Parametric Injection-A Systematic Study of Parametric\n  Retrieval-Augmented Generation",
        "url": "http://arxiv.org/abs/2510.12668v1",
        "pub_date": "2025-10-14",
        "summary": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by retrieving external documents. As an emerging form of RAG, parametric retrieval-augmented generation (PRAG) encodes documents as model parameters (i.e., LoRA modules) and injects these representations into the model during inference, enabling interaction between the LLM and documents at parametric level. Compared with directly placing documents in the input context, PRAG is more efficient and has the potential to offer deeper model-document interaction. Despite its growing attention, the mechanism underlying parametric injection remains poorly understood. In this work, we present a systematic study of PRAG to clarify the role of parametric injection, showing that parameterized documents capture only partial semantic information of documents, and relying on them alone yields inferior performance compared to interaction at text level. However, these parametric representations encode high-level document information that can enhance the model's understanding of documents within the input context. When combined parameterized documents with textual documents, the model can leverage relevant information more effectively and become more robust to noisy inputs, achieving better performance than either source alone. We recommend jointly using parameterized and textual documents and advocate for increasing the information content of parametric representations to advance PRAG.",
        "translated": "检索增强生成（RAG）通过检索外部文档来增强大语言模型（LLM）。作为一种新兴形式的RAG，参数化检索增强生成（PRAG）将文档编码为模型参数（即LoRA模块），并在推理过程中将这些表示注入模型，从而在参数层面实现LLM与文档之间的交互。与直接将文档放入输入上下文相比，PRAG更加高效，并且有望提供更深层次的模型-文档交互。尽管PRAG已引起越来越多的关注，但其参数注入机制仍不明确。在本工作中，我们对PRAG进行了系统研究，以阐明参数注入的作用，表明参数化的文档仅捕捉了文档的部分语义信息，单独依赖它们会导致性能不如文本层面的交互。然而，这些参数表示编码了文档的高层信息，可以增强模型对输入上下文中文档的理解。将参数化文档与文本文档结合使用时，模型能够更有效地利用相关信息，并对输入中的噪声具有更强的鲁棒性，从而在性能上优于任一单独来源。我们建议联合使用参数化文档与文本文档，并倡导提升参数表示的信息量以推动PRAG的发展。",
        "translated_title": "The Role of Parametric Injection—A Systematic Study of Parametric Retrieval-Augmented Generation\n\n参数注入的作用——参数化检索增强生成的系统研究",
        "label": [
            "LLM生成式推荐"
        ],
        "label_reason": "论文研究PRAG在生成式模型中的作用，可能用于生成式推荐系统。",
        "relevance_score": 5,
        "novelty_score": 7,
        "novelty_reason": "系统性研究PRAG机制，提出参数化文档与文本文档结合的新方法。"
    },
    {
        "title": "SMILE: SeMantic Ids Enhanced CoLd Item Representation for Click-through\n  Rate Prediction in E-commerce SEarch",
        "url": "http://arxiv.org/abs/2510.12604v1",
        "pub_date": "2025-10-14",
        "summary": "With the rise of modern search and recommendation platforms, insufficient collaborative information of cold-start items exacerbates the Matthew effect of existing platform items, challenging platform diversity and becoming a longstanding issue. Existing methods align items' side content with collaborative information to transfer collaborative signals from high-popularity items to cold-start items. However, these methods fail to account for the asymmetry between collaboration and content, nor the fine-grained differences among items. To address these issues, we propose SMILE, an item representation enhancement approach based on fused alignment of semantic IDs. Specifically, we use RQ-OPQ encoding to quantize item content and collaborative information, followed by a two-step alignment: RQ encoding transfers shared collaborative signals across items, while OPQ encoding learns differentiated information of items. Comprehensive offline experiments on large-scale industrial datasets demonstrate superiority of SMILE, and rigorous online A/B tests confirm statistically significant improvements: item CTR +1.66%, buyers +1.57%, and order volume +2.17%.",
        "translated": "随着现代搜索与推荐系统的兴起，冷启动物料的协同信息不足加剧了平台现有物料的马太效应，挑战平台多样性，并成为一个长期存在的问题。现有方法将物料的侧边内容与协同信息对齐，以将协同信号从高热度物料转移到冷启动物料。然而，这些方法未能考虑协同与内容之间的不对称性，也未考虑物料之间的细粒度差异。为了解决这些问题，我们提出了 SMILE，一种基于语义 ID 融合对齐的物料表示增强方法。具体而言，我们使用 RQ-OPQ 编码对物料内容和协同信息进行量化，随后进行两步对齐：RQ 编码跨物料传递共享的协同信号，而 OPQ 编码学习物料的差异化信息。在大规模工业数据集上的全面离线实验验证了 SMILE 的优越性，严格的在线 A/B 测试也确认了其具有统计显著性的提升效果：物料点击率提升 1.66%，买家数量增加 1.57%，订单量增长 2.17%。",
        "translated_title": "SMILE：语义ID增强的冷启动物料表征用于电商搜索中的点击率预测",
        "label": [
            "精排",
            "冷启动",
            "General Recommendation Techniques",
            "Click-through Rate Prediction"
        ],
        "label_reason": "论文聚焦于冷启动商品的CTR预测，属于推荐系统核心问题",
        "relevance_score": 8,
        "novelty_score": 7,
        "novelty_reason": "提出了基于语义ID融合对齐的新方法，改进了冷启动建模"
    },
    {
        "title": "Leveraging Language Semantics for Collaborative Filtering with TextGCN\n  and TextGCN-MLP: Zero-Shot vs In-Domain Performance",
        "url": "http://arxiv.org/abs/2510.12461v1",
        "pub_date": "2025-10-14",
        "summary": "In recent years, various approaches have been proposed to leverage large language models (LLMs) for incorporating textual information about items into recommender systems. Existing methods primarily focus on either fine-tuning LLMs to generate recommendations or integrating LLM-based embeddings into downstream models. In this work, we follow the latter direction and propose \\textbf{TextGCN}, which applies parameter-free graph convolution layers directly over LLM-based item-title embeddings, instead of learning ID-based embeddings as in traditional methods. By combining language semantics with graph message passing, this architecture achieves state-of-the-art zero-shot performance, significantly outperforming prior approaches. Furthermore, we introduce \\textbf{TextGCN-MLP}, which extends TextGCN with a trainable multilayer perceptron trained using a contrastive loss, achieving state-of-the-art in-domain performance on recommendation benchmarks. However, the zero-shot performance of TextGCN-MLP remains lower than that of TextGCN, highlighting the trade-off between in-domain specialization and zero-shot generalization. We release our code on github at \\href{https://github.com/ChernovAndrey/TFCE}{github.com/ChernovAndrey/TFCE}.",
        "translated": "近年来，各种方法被提出以利用大语言模型（LLMs）将物品的文本信息引入推荐系统。现有方法主要集中在两个方面：一是微调LLMs以生成推荐结果，二是将基于LLM的嵌入向量整合到下游模型中。在本工作中，我们遵循后者，并提出了**TextGCN**，该方法直接在基于LLM的物品标题嵌入上应用无参数图卷积层，而不是像传统方法那样学习基于ID的嵌入。通过将语言语义与图消息传递相结合，该架构实现了最先进的零样本性能，显著优于先前的方法。此外，我们引入了**TextGCN-MLP**，该方法通过对比损失训练一个可训练的多层感知机（multilayer perceptron）来扩展TextGCN，在推荐基准测试中实现了最先进的域内性能。然而，TextGCN-MLP的零样本性能仍然低于TextGCN，突显了域内专业化与零样本泛化之间的权衡。我们的代码已发布在github上：\\href{https://github.com/ChernovAndrey/TFCE}{github.com/ChernovAndrey/TFCE}。",
        "translated_title": "利用语言语义进行协同过滤的推荐系统：TextGCN与TextGCN-MLP的零样本与领域内性能对比",
        "label": [
            "图神经网络推荐",
            "通用推荐技术"
        ],
        "label_reason": "结合图神经网络与文本语义，改进协同过滤方法。",
        "relevance_score": 8,
        "novelty_score": 8,
        "novelty_reason": "提出参数无关的图卷积方法，结合对比学习提升推荐效果。"
    },
    {
        "title": "A Hierarchical Quantized Tokenization Framework for Task-Adaptive Graph\n  Representation Learning",
        "url": "http://arxiv.org/abs/2510.12369v1",
        "pub_date": "2025-10-14",
        "summary": "Recent progress in language and vision foundation models demonstrates the importance of discrete token interfaces that transform complex inputs into compact sequences for large-scale modeling. Extending this paradigm to graphs requires a tokenization scheme that handles non-Euclidean structures and multi-scale dependencies efficiently. Existing approaches to graph tokenization, linearized, continuous, and quantized, remain limited in adaptability and efficiency. In particular, most current quantization-based tokenizers organize hierarchical information in fixed or task-agnostic ways, which may either over-represent or under-utilize structural cues, and lack the ability to dynamically reweight contributions from different levels without retraining the encoder. This work presents a hierarchical quantization framework that introduces a self-weighted mechanism for task-adaptive aggregation across multiple scales. The proposed method maintains a frozen encoder while modulating information flow through a lightweight gating process, enabling parameter-efficient adaptation to diverse downstream tasks. Experiments on benchmark datasets for node classification and link prediction demonstrate consistent improvements over strong baselines under comparable computational budgets.",
        "translated": "近期在语言和视觉基础模型方面的进展表明，离散的token接口在将复杂输入转化为紧凑序列以进行大规模建模中的重要性。将这一范式扩展到图结构需要一种能够高效处理非欧几里得结构和多尺度依赖关系的token化方案。现有的图token化方法，包括线性化、连续化和量化方法，在适应性和效率方面仍存在限制。特别是，大多数当前基于量化的tokenizer以固定或任务无关的方式组织层次信息，这可能导致对结构线索的过度表示或利用不足，并且无法在不重新训练编码器的情况下动态调整不同层次的贡献权重。本文提出了一种层次量化框架，引入了一种自加权机制，用于在多个尺度上进行任务自适应的聚合。所提出的方法在保持冻结编码器的同时，通过轻量级的门控过程调节信息流动，从而在不同下游任务中实现参数高效的适应。在节点分类和链接预测基准数据集上的实验表明，在可比较的计算预算下，该方法在多个任务中均优于强大的基线模型。",
        "translated_title": "一种面向任务自适应图表示学习的层次量化分词框架",
        "label": [],
        "label_reason": "论文聚焦图表示学习，未明确涉及推荐系统环节",
        "relevance_score": 3,
        "novelty_score": 7,
        "novelty_reason": "提出自加权机制改进图量化方法，有一定创新性"
    },
    {
        "title": "Simple Projection Variants Improve ColBERT Performance",
        "url": "http://arxiv.org/abs/2510.12327v1",
        "pub_date": "2025-10-14",
        "summary": "Multi-vector dense retrieval methods like ColBERT systematically use a single-layer linear projection to reduce the dimensionality of individual vectors. In this study, we explore the implications of the MaxSim operator on the gradient flows of the training of multi-vector models and show that such a simple linear projection has inherent, if non-critical, limitations in this setting. We then discuss the theoretical improvements that could result from replacing this single-layer projection with well-studied alternative feedforward linear networks (FFN), such as deeper, non-linear FFN blocks, GLU blocks, and skip-connections, could alleviate these limitations. Through the design and systematic evaluation of alternate projection blocks, we show that better-designed final projections positively impact the downstream performance of ColBERT models. We highlight that many projection variants outperform the original linear projections, with the best-performing variants increasing average performance on a range of retrieval benchmarks across domains by over 2 NDCG@10 points. We then conduct further exploration on the individual parameters of these projections block in order to understand what drives this empirical performance, highlighting the particular importance of upscaled intermediate projections and residual connections. As part of these ablation studies, we show that numerous suboptimal projection variants still outperform the traditional single-layer projection across multiple benchmarks, confirming our hypothesis. Finally, we observe that this effect is consistent across random seeds, further confirming that replacing the linear layer of ColBERT models is a robust, drop-in upgrade.",
        "translated": "诸如 ColBERT 等多向量密集召回方法通常使用单层线性投影来降低单个向量的维度。在本研究中，我们探讨了 MaxSim 运算符在多向量模型训练过程中的梯度流动所带来的影响，并表明在这一设置下，这种简单的线性投影存在固有但非关键的限制。我们进一步讨论了理论上的改进，即将该单层投影替换为已被广泛研究的替代前馈线性网络（FFN），例如更深的非线性 FFN 模块、GLU 模块以及跳跃连接（skip-connections），从而缓解这些限制。通过对替代投影模块的设计及其系统性评估，我们证明了更合理设计的最终投影对 ColBERT 模型的下游性能具有积极影响。我们发现，许多投影变体优于原始的线性投影，其中表现最好的变体在多个跨领域的召回基准上将平均性能提升了超过 2 个 NDCG@10 分数点。随后，我们对这些投影模块的单个参数进行了进一步探索，以理解其经验性能背后的关键因素，强调了中间投影的扩展（upscaled）和残差连接的特别重要性。作为消融实验的一部分，我们展示了即使许多次优的投影变体，也能在多个基准上优于传统的单层投影，从而验证了我们的假设。最后，我们观察到这一效果在不同随机种子之间是一致的，进一步确认了替换 ColBERT 模型中的线性层是一种鲁棒且可直接替换的性能提升方式。",
        "translated_title": "简单投影变体提升ColBERT性能",
        "label": [],
        "label_reason": "论文聚焦于密集检索方法优化，不直接涉及推荐系统",
        "relevance_score": 3,
        "novelty_score": 6,
        "novelty_reason": "提出改进投影层的结构，有一定方法创新"
    },
    {
        "title": "Causal Inspired Multi Modal Recommendation",
        "url": "http://arxiv.org/abs/2510.12325v1",
        "pub_date": "2025-10-14",
        "summary": "Multimodal recommender systems enhance personalized recommendations in e-commerce and online advertising by integrating visual, textual, and user-item interaction data. However, existing methods often overlook two critical biases: (i) modal confounding, where latent factors (e.g., brand style or product category) simultaneously drive multiple modalities and influence user preference, leading to spurious feature-preference associations; (ii) interaction bias, where genuine user preferences are mixed with noise from exposure effects and accidental clicks. To address these challenges, we propose a Causal-inspired multimodal Recommendation framework. Specifically, we introduce a dual-channel cross-modal diffusion module to identify hidden modal confounders, utilize back-door adjustment with hierarchical matching and vector-quantized codebooks to block confounding paths, and apply front-door adjustment combined with causal topology reconstruction to build a deconfounded causal subgraph. Extensive experiments on three real-world e-commerce datasets demonstrate that our method significantly outperforms state-of-the-art baselines while maintaining strong interpretability.",
        "translated": "多模态推荐系统通过融合视觉、文本和用户-物料交互数据，提升了电子商务和在线广告中的个性化推荐效果。然而，现有方法通常忽略了两种关键偏差：(i) 模态混淆，其中潜在因子（例如品牌风格或产品类别）同时驱动多个模态，并影响用户偏好，导致虚假的特征-偏好关联；(ii) 交互偏差，其中真实的用户偏好与曝光效应和偶然点击带来的噪声混合。为了解决这些挑战，我们提出了一种受因果启发的多模态推荐框架。具体而言，我们引入了一个双通道跨模态扩散模块以识别隐藏的模态混淆因子，利用分层匹配和矢量量化码本进行后门调整以阻断混淆路径，并应用前门调整结合因果拓扑重构，以构建去混淆的因果子图。在三个真实世界电子商务数据集上的广泛实验表明，我们的方法在保持强大可解释性的同时，显著优于最先进的基线方法。",
        "translated_title": "因果启发的多模态推荐",
        "label": [
            "多模态推荐",
            "因果推理",
            "推荐系统公平性/可解释性"
        ],
        "label_reason": "论文聚焦多模态推荐并引入因果推理解决关键偏差问题。",
        "relevance_score": 8,
        "novelty_score": 8,
        "novelty_reason": "提出双通道跨模态扩散模块和因果拓扑重构，具有显著创新性。"
    },
    {
        "title": "Fantastic (small) Retrievers and How to Train Them:\n  mxbai-edge-colbert-v0 Tech Report",
        "url": "http://arxiv.org/abs/2510.14880v1",
        "pub_date": "2025-10-16",
        "summary": "In this work, we introduce mxbai-edge-colbert-v0 models, at two different parameter counts: 17M and 32M. As part of our research, we conduct numerous experiments to improve retrieval and late-interaction models, which we intend to distill into smaller models as proof-of-concepts. Our ultimate aim is to support retrieval at all scales, from large-scale retrieval which lives in the cloud to models that can run locally, on any device. mxbai-edge-colbert-v0 is a model that we hope will serve as a solid foundation backbone for all future experiments, representing the first version of a long series of small proof-of-concepts. As part of the development of mxbai-edge-colbert-v0, we conducted multiple ablation studies, of which we report the results. In terms of downstream performance, mxbai-edge-colbert-v0 is a particularly capable small model, outperforming ColBERTv2 on common short-text benchmarks (BEIR) and representing a large step forward in long-context tasks, with unprecedented efficiency.",
        "translated": "在本工作中，我们引入了 mxbai-edge-colbert-v0 模型，并提供了两种不同的参数规模：17M 和 32M。作为我们研究的一部分，我们进行了大量实验以改进检索和后期交互模型，并计划将其蒸馏为更小的模型作为概念验证。我们的最终目标是支持所有规模的检索，从云端的大规模检索到能够在任何设备上本地运行的小模型。mxbai-edge-colbert-v0 是我们希望作为所有未来实验坚实基础的骨干模型，代表了一系列小型概念验证模型的第一个版本。在 mxbai-edge-colbert-v0 的开发过程中，我们进行了多项消融实验，并在此报告其结果。在下游任务性能方面，mxbai-edge-colbert-v0 是一个特别强大的小型模型，其在常见的短文本基准（BEIR）上优于 ColBERTv2，并在长上下文任务中实现了前所未有的效率，迈出了重要一步。",
        "translated_title": "Fantastic (small) Retrievers and How to Train Them:  \nmxbai-edge-colbert-v0 技术报告",
        "label": [
            "召回"
        ],
        "label_reason": "论文聚焦检索模型优化，适用于推荐系统的召回环节。",
        "relevance_score": 6,
        "novelty_score": 7,
        "novelty_reason": "提出高效小型模型，改进了检索与长上下文处理能力。"
    },
    {
        "title": "A Simulation Framework for Studying Systemic Effects of Feedback Loops\n  in Recommender Systems",
        "url": "http://arxiv.org/abs/2510.14857v1",
        "pub_date": "2025-10-16",
        "summary": "Recommender systems continuously interact with users, creating feedback loops that shape both individual behavior and collective market dynamics. This paper introduces a simulation framework to model these loops in online retail environments, where recommenders are periodically retrained on evolving user-item interactions. Using the Amazon e-Commerce dataset, we analyze how different recommendation algorithms influence diversity, purchase concentration, and user homogenization over time. Results reveal a systematic trade-off: while the feedback loop increases individual diversity, it simultaneously reduces collective diversity and concentrates demand on a few popular items. Moreover, for some recommender systems, the feedback loop increases user homogenization over time, making user purchase profiles increasingly similar. These findings underscore the need for recommender designs that balance personalization with long-term diversity.",
        "translated": "推荐系统持续与用户进行交互，形成反馈循环，从而塑造个体行为和集体市场动态。本文引入了一个仿真框架，用于模拟在线零售环境中的这些循环，其中推荐系统会定期在不断演变的用户-物料交互数据上进行重新训练。使用 Amazon 电商平台数据集，我们分析了不同的推荐算法如何随着时间的推移影响多样性、购买集中度和用户同质化。结果揭示了一个系统性的权衡：虽然反馈循环提高了个体的多样性，但它同时降低了集体的多样性，并将需求集中在少数热门物料上。此外，对于某些推荐系统，反馈循环会随着时间推移增加用户的同质化，使得用户的购买行为变得越来越相似。这些发现凸显了推荐系统设计中需要在个性化与长期多样性之间取得平衡的必要性。",
        "translated_title": "一个用于研究推荐系统中反馈循环系统性效应的模拟框架",
        "label": [
            "推荐系统公平性/可解释性",
            "通用推荐技术"
        ],
        "label_reason": "研究推荐系统的反馈循环及长期多样性问题，属于推荐系统核心机制分析",
        "relevance_score": 8,
        "novelty_score": 7,
        "novelty_reason": "提出新颖的模拟框架，揭示个性化与多样性间的系统性权衡"
    },
    {
        "title": "Supervised Fine-Tuning or Contrastive Learning? Towards Better\n  Multimodal LLM Reranking",
        "url": "http://arxiv.org/abs/2510.14824v1",
        "pub_date": "2025-10-16",
        "summary": "In information retrieval, training reranking models mainly focuses on two types of objectives: metric learning (e.g. contrastive loss to increase the predicted scores on relevant query-document pairs) and classification (binary label prediction of relevance vs. irrelevance). For BERT-style encoders, various studies have shown that contrastive learning (CL) can be more effective than discriminative (classification) learning. However, for large language models (LLMs), classification via supervised fine-tuning (SFT), which predicts ''yes'' (resp. ''no'') token for relevant (resp. irrelevant) pairs, appears more promising as it aligns well with the generative nature of LLMs. This divergence raises a central question: which objective is intrinsically better suited to LLM-based reranking, and what mechanism underlies the difference? In this work, we conduct a comprehensive comparison and analysis between CL and SFT for reranking, taking the universal multimodal retrieval (UMR) as the experimental playground. We first decompose the objectives into two components: weight, which controls the magnitude of those updates, and direction, which guides the model updates, then present a unified framework for understanding their interactions. Through probing experiments, we find that SFT provides a substantially stronger weighting scheme than CL, whereas the preferred scoring direction shows no clear winner. Taken together, these results point to a consistent advantage of SFT over CL for LLM reranking. To further validate our findings, we conduct large-scale training with SFT and present new state-of-the-art rerankers on the MRB benchmark. We also provide ablations on SFT settings and expect our findings to benefit future research and applications in this area.",
        "translated": "在信息检索中，训练重排模型主要关注两种目标：度量学习（例如对比损失，用于提升相关查询-文档对的预测得分）和分类（相关与不相关的二分类标签预测）。对于 BERT 风格的编码器，已有大量研究表明，对比学习（CL）在性能上通常优于判别式（分类）学习。然而，对于大语言模型（LLMs），通过监督微调（SFT）进行分类——即对相关（resp. 不相关）对预测“yes”（resp. “no”）标记——似乎更具前景，因为它与 LLM 生成式的本质更加契合。这一差异引发了一个核心问题：哪一种目标本质上更适合基于 LLM 的重排？其背后的不同机制是什么？在本研究中，我们对 CL 和 SFT 在重排中的表现进行了全面的比较与分析，实验场景设定为通用多模态检索（UMR）。我们首先将这两种目标分解为两个组成部分：权重，用于控制更新的幅度；以及方向，用于指导模型的更新，然后提出一个统一的框架来理解它们之间的相互作用。通过探针实验，我们发现 SFT 提供了显著更强的权重机制，而关于最优的得分方向则没有明显优势。综合来看，这些结果表明在基于 LLM 的重排任务中，SFT 相较于 CL 具有一致的优势。为了进一步验证我们的发现，我们使用 SFT 进行了大规模训练，并在 MRB 基准上提出了新的最先进的重排模型。我们还对 SFT 的设置进行了消融实验，并希望我们的研究结果能够为该领域的未来研究与应用提供帮助。",
        "translated_title": "监督微调还是对比学习？迈向更优的多模态大语言模型重排",
        "label": [
            "重排（Re-ranking）",
            "负采样与对比学习（Negative Sampling / Contrastive Learning）",
            "LLM生成式推荐（LLM-based Generative Recommendation）"
        ],
        "label_reason": "论文聚焦LLM在重排中的应用，并对比CL和SFT的效果，与推荐系统密切相关。",
        "relevance_score": 8,
        "novelty_score": 7,
        "novelty_reason": "提出SFT更适合LLM重排，并提供统一框架分析两者差异，具有创新性。"
    },
    {
        "title": "Cross-Scenario Unified Modeling of User Interests at Billion Scale",
        "url": "http://arxiv.org/abs/2510.14788v1",
        "pub_date": "2025-10-16",
        "summary": "User interests on content platforms are inherently diverse, manifesting through complex behavioral patterns across heterogeneous scenarios such as search, feed browsing, and content discovery. Traditional recommendation systems typically prioritize business metric optimization within isolated specific scenarios, neglecting cross-scenario behavioral signals and struggling to integrate advanced techniques like LLMs at billion-scale deployments, which finally limits their ability to capture holistic user interests across platform touchpoints. We propose RED-Rec, an LLM-enhanced hierarchical Recommender Engine for Diversified scenarios, tailored for industry-level content recommendation systems. RED-Rec unifies user interest representations across multiple behavioral contexts by aggregating and synthesizing actions from varied scenarios, resulting in comprehensive item and user modeling. At its core, a two-tower LLM-powered framework enables nuanced, multifaceted representations with deployment efficiency, and a scenario-aware dense mixing and querying policy effectively fuses diverse behavioral signals to capture cross-scenario user intent patterns and express fine-grained, context-specific intents during serving. We validate RED-Rec through online A/B testing on hundreds of millions of users in RedNote through online A/B testing, showing substantial performance gains in both content recommendation and advertisement targeting tasks. We further introduce a million-scale sequential recommendation dataset, RED-MMU, for comprehensive offline training and evaluation. Our work advances unified user modeling, unlocking deeper personalization and fostering more meaningful user engagement in large-scale UGC platforms.",
        "translated": "内容平台上的用户兴趣本质上是多样化的，这种多样性在搜索、信息流浏览和内容发现等异构场景中通过复杂的用户行为模式体现出来。传统的推荐系统通常优先在孤立的具体场景中优化业务指标，忽略了跨场景的行为信号，且在数十亿级别的实际部署中难以有效整合大语言模型（LLM）等先进技术，最终限制了它们在平台各个触点中捕捉用户整体兴趣的能力。我们提出了 RED-Rec，一个面向工业级内容推荐系统的、用于多样化场景的、融合大语言模型的分层推荐引擎。RED-Rec 通过聚合和综合来自不同场景的行为，统一了用户兴趣在多种行为上下文中的表示，从而实现了全面的物料与用户建模。其核心是一个双塔结构的大语言模型驱动框架，能够以高效的部署方式生成细致、多维的表示；同时，一个具备场景感知能力的密集混合与查询策略，有效融合了多样的行为信号，以捕捉跨场景的用户意图模式，并在服务过程中表达出细粒度、上下文特定的意图。我们在 RedNote 上通过数亿用户的在线 A/B 实验验证了 RED-Rec，结果在内容推荐和广告定向任务中均表现出显著的性能提升。此外，我们进一步引入了一个百万规模的序列推荐数据集 RED-MMU，用于全面的离线训练与评估。我们的工作推动了统一用户建模的发展，在大规模用户生成内容（UGC）平台中实现了更深层次的个性化，并促进了更具意义的用户参与。",
        "translated_title": "百亿级跨场景用户兴趣统一建模",
        "label": [
            "通用推荐技术",
            "LLM生成式推荐",
            "跨域/联邦推荐",
            "序列推荐"
        ],
        "label_reason": "跨场景统一用户兴趣建模，适用于大规模内容推荐系统",
        "relevance_score": 9,
        "novelty_score": 8,
        "novelty_reason": "提出融合LLM的分层架构RED-Rec，改进跨场景行为建模"
    },
    {
        "title": "Dataset Pruning in RecSys and ML: Best Practice or Mal-Practice?",
        "url": "http://arxiv.org/abs/2510.14704v1",
        "pub_date": "2025-10-16",
        "summary": "Offline evaluations in recommender system research depend heavily on datasets, many of which are pruned, such as the widely used MovieLens collections. This thesis examines the impact of data pruning - specifically, removing users with fewer than a specified number of interactions - on both dataset characteristics and algorithm performance. Five benchmark datasets were analysed in both their unpruned form and at five successive pruning levels (5, 10, 20, 50, 100). For each coreset, we examined structural and distributional characteristics and trained and tested eleven representative algorithms. To further assess if pruned datasets lead to artificially inflated performance results, we also evaluated models trained on the pruned train sets but tested on unpruned data. Results show that commonly applied core pruning can be highly selective, leaving as little as 2% of the original users in some datasets. Traditional algorithms achieved higher nDCG@10 scores when both training and testing on pruned data; however, this advantage largely disappeared when evaluated on unpruned test sets. Across all algorithms, performance declined with increasing pruning levels when tested on unpruned data, highlighting the impact of dataset reduction on the performance of recommender algorithms.",
        "translated": "推荐系统的离线评估高度依赖于数据集，其中许多数据集经过了修剪，例如广泛使用的 MovieLens 数据集。本文研究了数据修剪——具体来说，移除交互次数少于指定阈值的用户——对数据集特性和算法性能的影响。我们分析了五个基准数据集在未修剪形式和五种连续修剪等级（5, 10, 20, 50, 100）下的表现。对于每个核心子集（coreset），我们考察了其结构和分布特征，并训练和测试了十一种代表性算法。为了进一步评估修剪后的数据集是否会导致性能结果的人为高估，我们还评估了在修剪后的训练集上训练但在未修剪数据上测试的模型。结果显示，常见的核心修剪方法具有高度的选择性，在某些数据集中仅保留原始用户数的2%。传统算法在训练和测试均使用修剪数据时取得了更高的 nDCG@10 分数；然而，当在未修剪的测试集上进行评估时，这种优势基本消失。在所有算法中，当测试数据为未修剪数据时，其性能随着修剪等级的提高而下降，突显了数据集减少对推荐算法性能的影响。",
        "translated_title": "推荐系统与机器学习中的数据集裁剪：最佳实践还是不当实践？",
        "label": [
            "推荐系统评估"
        ],
        "label_reason": "论文聚焦推荐系统数据集剪枝对算法性能的影响，属于推荐系统的评估问题。",
        "relevance_score": 8,
        "novelty_score": 6,
        "novelty_reason": "对数据集剪枝现象进行系统分析，但方法较为常规，创新性有限。"
    },
    {
        "title": "TITAN: Graph-Executable Reasoning for Cyber Threat Intelligence",
        "url": "http://arxiv.org/abs/2510.14670v1",
        "pub_date": "2025-10-16",
        "summary": "TITAN (Threat Intelligence Through Automated Navigation) is a framework that connects natural-language cyber threat queries with executable reasoning over a structured knowledge graph. It integrates a path planner model, which predicts logical relation chains from text, and a graph executor that traverses the TITAN Ontology to retrieve factual answers and supporting evidence. Unlike traditional retrieval systems, TITAN operates on a typed, bidirectional graph derived from MITRE, allowing reasoning to move clearly and reversibly between threats, behaviors, and defenses. To support training and evaluation, we introduce the TITAN Dataset, a corpus of 88209 examples (Train: 74258; Test: 13951) pairing natural language questions with executable reasoning paths and step by step Chain of Thought explanations. Empirical evaluations show that TITAN enables models to generate syntactically valid and semantically coherent reasoning paths that can be deterministically executed on the underlying graph.",
        "translated": "TITAN（Threat Intelligence Through Automated Navigation）是一个将自然语言的网络威胁查询与结构化知识图谱上的可执行推理相连接的框架。它集成了一个路径规划模型，该模型从文本中预测逻辑关系链，并结合一个图谱执行器，该执行器遍历TITAN本体（Ontology）以检索事实性答案和支持证据。与传统召回系统不同，TITAN基于来自MITRE的类型化、双向图谱进行操作，使得在威胁、行为和防御之间可以清晰且可逆地进行推理。为支持训练和评估，我们引入了TITAN数据集，该数据集包含88209个示例（训练集：74258；测试集：13951），将自然语言问题与可执行推理路径以及逐步的Chain of Thought解释配对。实证评估表明，TITAN使模型能够生成在语法上有效、语义上连贯的推理路径，并可以在底层图谱上确定性地执行。",
        "translated_title": "TITAN：用于网络威胁情报的图可执行推理",
        "label": [],
        "label_reason": "论文聚焦网络安全威胁情报，不直接涉及推荐系统。",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出基于知识图谱的可执行推理框架，具有新颖性但非推荐系统创新。"
    },
    {
        "title": "An Efficient Rubric-based Generative Verifier for Search-Augmented LLMs",
        "url": "http://arxiv.org/abs/2510.14660v1",
        "pub_date": "2025-10-16",
        "summary": "Search augmentation empowers Large Language Models with retrieval capabilities to overcome the limitations imposed by static parameters. Recently, Reinforcement Learning leverages tailored reward signals as a viable technique to enhance LLMs performing tasks involving search. However, existing reward modeling for search-augmented LLMs faces several limitations. Rule-based rewards, such as Exact Match, are verifiable but fragile to variations in expression and cannot be applied to long-form workloads. In contrast, generative rewards improve robustness, but designing verifiable and stable rewards for long-form workloads in dynamic corpora remains challenging and also incurs high computational costs. In this paper, we propose a unified and verifiable paradigm, \"nugget-as-rubric\", which treats atomic information points as structured evaluation criteria for different search-augmentation workloads. Short-form tasks correspond to a single rubric, whereas long-form tasks expand to multiple rubrics aligned with the question's information needs. To support long-form settings, we design an automatic rubric construction pipeline based on query rewriting, which can automatically retrieve passages relevant to each question and extract rubrics from them, both from static corpora and from dynamic online web content. Furthermore, we introduce \\textbf{Search-Gen-V}, a 4B-parameter efficient generative verifier under our proposed verifiable paradigm, which is trained via the idea of distillation and a two-stage strategy. Experimental results show that Search-Gen-V achieves strong verification accuracy across different workloads, making it a scalable, robust, and efficient verifiable reward constructor for search-augmented LLMs.",
        "translated": "搜索增强为大语言模型赋予了召回能力，以克服静态参数所带来的限制。近年来，强化学习利用定制化的奖励信号，作为增强大语言模型在涉及搜索任务性能的一种可行技术。然而，现有的用于搜索增强的大语言模型的奖励建模面临多个限制。基于规则的奖励，例如精确匹配（Exact Match），虽然可验证，但对表达方式的变化非常脆弱，无法应用于长文本任务。相比之下，生成式奖励增强了鲁棒性，但在动态语料库中为长文本任务设计可验证且稳定的奖励仍然具有挑战性，并且计算成本较高。在本文中，我们提出了一种统一且可验证的范式——“nugget-as-rubric”，该范式将原子信息点视为结构化的评估标准，适用于不同的搜索增强任务。短文本任务对应单一评估标准，而长文本任务则扩展为多个与问题信息需求对齐的评估标准。为了支持长文本场景，我们设计了一种基于查询重写（query rewriting）的自动评估标准构建流程，该流程能够自动召回与每个问题相关的段落，并从中提取评估标准，无论是静态语料库还是动态的在线网页内容。此外，我们引入了 **Search-Gen-V**，一个在我们提出的可验证范式下高效的大语言生成验证模型。该模型通过蒸馏思想和两阶段训练策略进行训练。实验结果表明，Search-Gen-V 在不同任务中均实现了较强的验证精度，使其成为适用于搜索增强的大语言模型的一种可扩展、鲁棒且高效的可验证奖励构造器。",
        "translated_title": "一种高效的基于评分规则的生成式验证器用于检索增强的大语言模型",
        "label": [
            "LLM生成式推荐"
        ],
        "label_reason": "论文涉及生成式LLM与搜索增强，适用于生成式推荐场景",
        "relevance_score": 4,
        "novelty_score": 7,
        "novelty_reason": "提出新颖的验证范式和高效的生成式验证器，改进现有奖励建模"
    },
    {
        "title": "Causality Enhancement for Cross-Domain Recommendation",
        "url": "http://arxiv.org/abs/2510.14641v1",
        "pub_date": "2025-10-16",
        "summary": "Cross-domain recommendation forms a crucial component in recommendation systems. It leverages auxiliary information through source domain tasks or features to enhance target domain recommendations. However, incorporating inconsistent source domain tasks may result in insufficient cross-domain modeling or negative transfer. While incorporating source domain features without considering the underlying causal relationships may limit their contribution to final predictions. Thus, a natural idea is to directly train a cross-domain representation on a causality-labeled dataset from the source to target domain. Yet this direction has been rarely explored, as identifying unbiased real causal labels is highly challenging in real-world scenarios. In this work, we attempt to take a first step in this direction by proposing a causality-enhanced framework, named CE-CDR. Specifically, we first reformulate the cross-domain recommendation as a causal graph for principled guidance. We then construct a causality-aware dataset heuristically. Subsequently, we derive a theoretically unbiased Partial Label Causal Loss to generalize beyond the biased causality-aware dataset to unseen cross-domain patterns, yielding an enriched cross-domain representation, which is then fed into the target model to enhance target-domain recommendations. Theoretical and empirical analyses, as well as extensive experiments, demonstrate the rationality and effectiveness of CE-CDR and its general applicability as a model-agnostic plugin. Moreover, it has been deployed in production since April 2025, showing its practical value in real-world applications.",
        "translated": "跨域推荐是推荐系统中的一个关键组成部分。它通过源域任务或特征来利用辅助信息，以增强目标域的推荐效果。然而，引入不一致的源域任务可能导致跨域建模不充分或出现负迁移。而在不考虑底层因果关系的情况下融合源域特征，可能会限制其对最终预测的贡献。因此，一个自然的想法是直接在从源域到目标域的因果标注数据集上训练跨域表示。然而，这一方向鲜有探索，因为在现实场景中识别无偏的真实因果标签极具挑战性。在本工作中，我们尝试通过提出一种因果增强框架CE-CDR，在这一方向上迈出第一步。具体来说，我们首先将跨域推荐重新表述为因果图，以提供原理性的指导。然后，我们启发式地构建了一个因果感知数据集。随后，我们推导出一个理论上无偏的部标签因果损失函数，以在存在偏倚的因果感知数据集基础上，泛化至未见过的跨域模式，从而获得更丰富的跨域表示，并将其输入目标模型中以提升目标域的推荐性能。理论与实证分析以及广泛的实验验证了CE-CDR的合理性与有效性，并展示了其作为模型无关插件的通用适用性。此外，该方法自2025年4月起已在生产环境中部署，表明其在实际应用中的实用价值。",
        "translated_title": "跨域推荐中的因果增强",
        "label": [
            "跨域/联邦推荐",
            "因果推理"
        ],
        "label_reason": "论文研究跨域推荐，属于推荐系统核心问题，涉及因果关系建模。",
        "relevance_score": 8,
        "novelty_score": 7,
        "novelty_reason": "提出因果增强框架CE-CDR，改进跨域推荐模型的泛化能力。"
    },
    {
        "title": "Intent Clustering with Shared Pseudo-Labels",
        "url": "http://arxiv.org/abs/2510.14640v1",
        "pub_date": "2025-10-16",
        "summary": "In this paper, we propose an intuitive, training-free and label-free method for intent clustering that makes minimal assumptions using lightweight and open-source LLMs. Many current approaches rely on commercial LLMs, which are costly, and offer limited transparency. Additionally, their methods often explicitly depend on knowing the number of clusters in advance, which is often not the case in realistic settings. To address these challenges, instead of asking the LLM to match similar text directly, we first ask it to generate pseudo-labels for each text, and then perform multi-label classification in this pseudo-label set for each text. This approach is based on the hypothesis that texts belonging to the same cluster will share more labels, and will therefore be closer when encoded into embeddings. These pseudo-labels are more human-readable than direct similarity matches. Our evaluation on four benchmark sets shows that our approach achieves results comparable to and better than recent baselines, while remaining simple and computationally efficient. Our findings indicate that our method can be applied in low-resource scenarios and is stable across multiple models and datasets.",
        "translated": "在本文中，我们提出了一种直观的、无需训练且无需标注的方法用于意图聚类，该方法在使用轻量级和开源大语言模型的前提下，做出最小的假设。许多现有方法依赖于商业大语言模型，这不仅成本高昂，而且透明度有限。此外，这些方法通常明确要求提前知道聚类的数量，而这种情况在现实场景中往往并不存在。为了解决这些挑战，我们没有直接要求大语言模型匹配相似文本，而是首先让其为每段文本生成伪标签，然后在该伪标签集合中对每段文本进行多标签分类。该方法基于这样的假设：属于同一聚类的文本将共享更多的标签，因此在嵌入编码后彼此的距离将更近。与直接相似性匹配相比，这些伪标签更具可读性。我们在四个基准数据集上的评估表明，我们的方法在保持简单和计算高效的同时，取得了与近期基线相当甚至更好的结果。我们的实验结果表明，该方法适用于资源有限的场景，并在多个模型和数据集之间具有稳定性。",
        "translated_title": "Intent Clustering with Shared Pseudo-Labels  \n共享伪标签的意图聚类",
        "label": [
            "LLM生成式推荐"
        ],
        "label_reason": "伪标签生成用于意图聚类，可能间接用于推荐系统的意图建模",
        "relevance_score": 5,
        "novelty_score": 7,
        "novelty_reason": "提出无需训练的伪标签聚类方法，提升计算效率和可解释性"
    },
    {
        "title": "MR.Rec: Synergizing Memory and Reasoning for Personalized Recommendation\n  Assistant with LLMs",
        "url": "http://arxiv.org/abs/2510.14629v1",
        "pub_date": "2025-10-16",
        "summary": "The application of Large Language Models (LLMs) in recommender systems faces key challenges in delivering deep personalization and intelligent reasoning, especially for interactive scenarios. Current methods are often constrained by limited context windows and single-turn reasoning, hindering their ability to capture dynamic user preferences and proactively reason over recommendation contexts. To address these limitations, we propose MR.Rec, a novel framework that synergizes memory and reasoning for LLM-based recommendations. To achieve personalization, we develop a comprehensive Retrieval-Augmented Generation (RAG) system that efficiently indexes and retrieves relevant external memory to enhance LLM personalization capabilities. Furthermore, to enable the synergy between memory and reasoning, our RAG system goes beyond conventional query-based retrieval by integrating reasoning enhanced memory retrieval. Finally, we design a reinforcement learning framework that trains the LLM to autonomously learn effective strategies for both memory utilization and reasoning refinement. By combining dynamic memory retrieval with adaptive reasoning, this approach ensures more accurate, context-aware, and highly personalized recommendations. Extensive experiments demonstrate that MR.Rec significantly outperforms state-of-the-art baselines across multiple metrics, validating its efficacy in delivering intelligent and personalized recommendations. We will release code and data upon paper notification.",
        "translated": "大语言模型（LLM）在推荐系统中的应用面临在实现深度个性化与智能推理方面的主要挑战，尤其是在交互式场景中。当前的方法通常受限于有限的上下文窗口和单轮推理，阻碍了其捕捉动态用户偏好并主动在推荐上下文中进行推理的能力。为了解决这些限制，我们提出MR.Rec，一种新颖的框架，将记忆与推理相结合，用于基于LLM的推荐。为了实现个性化，我们开发了一个全面的检索增强生成（RAG）系统，高效地索引和召回相关的外部记忆，以增强LLM的个性化能力。此外，为了实现记忆与推理之间的协同作用，我们的RAG系统超越了传统的基于查询的召回，通过集成推理增强的记忆召回机制。最后，我们设计了一个强化学习框架，训练LLM自主学习在记忆利用和推理优化方面的有效策略。通过结合动态记忆召回与自适应推理，该方法能够提供更加准确、上下文感知和高度个性化的推荐。广泛的实验表明，MR.Rec在多个指标上显著优于最先进的基线方法，验证了其在实现智能和个性化推荐方面的有效性。我们将随论文通知发布代码和数据。",
        "translated_title": "MR.Rec: 结合记忆与推理的个性化推荐方法  \n与大语言模型结合",
        "label": [
            "LLM生成式推荐",
            "序列推荐",
            "推荐系统公平性/可解释性"
        ],
        "label_reason": "论文提出结合记忆与推理的LLM推荐框架，显著提升个性化与智能推荐能力。",
        "relevance_score": 9,
        "novelty_score": 8,
        "novelty_reason": "创新性地结合RAG与强化学习，实现动态记忆检索与自适应推理策略。"
    },
    {
        "title": "GemiRec: Interest Quantization and Generation for Multi-Interest\n  Recommendation",
        "url": "http://arxiv.org/abs/2510.14626v1",
        "pub_date": "2025-10-16",
        "summary": "Multi-interest recommendation has gained attention, especially in industrial retrieval stage. Unlike classical dual-tower methods, it generates multiple user representations instead of a single one to model comprehensive user interests. However, prior studies have identified two underlying limitations: the first is interest collapse, where multiple representations homogenize. The second is insufficient modeling of interest evolution, as they struggle to capture latent interests absent from a user's historical behavior. We begin with a thorough review of existing works in tackling these limitations. Then, we attempt to tackle these limitations from a new perspective. Specifically, we propose a framework-level refinement for multi-interest recommendation, named GemiRec. The proposed framework leverages interest quantization to enforce a structural interest separation and interest generation to learn the evolving dynamics of user interests explicitly. It comprises three modules: (a) Interest Dictionary Maintenance Module (IDMM) maintains a shared quantized interest dictionary. (b) Multi-Interest Posterior Distribution Module (MIPDM) employs a generative model to capture the distribution of user future interests. (c) Multi-Interest Retrieval Module (MIRM) retrieves items using multiple user-interest representations. Both theoretical and empirical analyses, as well as extensive experiments, demonstrate its advantages and effectiveness. Moreover, it has been deployed in production since March 2025, showing its practical value in industrial applications.",
        "translated": "多兴趣推荐在工业召回阶段中引起了广泛关注。与经典的双塔方法不同，它生成多个用户表示，而不是单一的表示，以建模用户全面的兴趣。然而，已有研究表明该方法存在两个潜在的限制：第一个是兴趣坍缩，其中多个表示趋于同质化；第二个是对兴趣演化的建模不足，因为它们难以捕捉用户历史行为中未体现的潜在兴趣。我们首先对现有工作中解决这些限制的方法进行了全面的回顾。随后，我们尝试从一个新的视角来应对这些限制。具体而言，我们提出了一个面向多兴趣推荐的框架级改进，称为GemiRec。所提框架利用兴趣量化来实现结构化的兴趣分离，并通过兴趣生成显式地学习用户兴趣的演化动态。它包含三个模块：(a) 兴趣字典维护模块（IDMM）维护一个共享的量化兴趣字典；(b) 多兴趣后验分布模块（MIPDM）采用生成模型来捕捉用户未来兴趣的分布；(c) 多兴趣召回模块（MIRM）使用多个用户-兴趣表示进行物料召回。理论与实证分析以及广泛的实验验证了其优势和有效性。此外，该方法自2025年3月起已在生产环境中部署，展示了其在工业应用中的实用价值。",
        "translated_title": "GemiRec：多兴趣推荐中的兴趣量化与生成",
        "label": [
            "召回（Recall）",
            "多兴趣推荐（Multimodal Recommendation）",
            "生成式推荐（LLM-based Generative Recommendation）"
        ],
        "label_reason": "论文提出多兴趣推荐框架GemiRec，聚焦于召回阶段的兴趣建模与生成，与推荐系统核心问题紧密相关。",
        "relevance_score": 9,
        "novelty_score": 8,
        "novelty_reason": "从兴趣量化与生成角度提出新颖框架，改进多兴趣建模方式，具有理论与应用创新。"
    },
    {
        "title": "Multimodal RAG for Unstructured Data:Leveraging Modality-Aware Knowledge\n  Graphs with Hybrid Retrieval",
        "url": "http://arxiv.org/abs/2510.14592v1",
        "pub_date": "2025-10-16",
        "summary": "Current Retrieval-Augmented Generation (RAG) systems primarily operate on unimodal textual data, limiting their effectiveness on unstructured multimodal documents. Such documents often combine text, images, tables, equations, and graphs, each contributing unique information. In this work, we present a Modality-Aware Hybrid retrieval Architecture (MAHA), designed specifically for multimodal question answering with reasoning through a modality-aware knowledge graph. MAHA integrates dense vector retrieval with structured graph traversal, where the knowledge graph encodes cross-modal semantics and relationships. This design enables both semantically rich and context-aware retrieval across diverse modalities. Evaluations on multiple benchmark datasets demonstrate that MAHA substantially outperforms baseline methods, achieving a ROUGE-L score of 0.486, providing complete modality coverage. These results highlight MAHA's ability to combine embeddings with explicit document structure, enabling effective multimodal retrieval. Our work establishes a scalable and interpretable retrieval framework that advances RAG systems by enabling modality-aware reasoning over unstructured multimodal data.",
        "translated": "当前的检索增强生成（RAG）系统主要基于单模态文本数据进行操作，这限制了它们在非结构化多模态文档上的有效性。此类文档通常结合了文本、图像、表格、公式和图表，每种模态都提供了独特的信息。在本文中，我们提出了一种模态感知混合检索架构（MAHA），该架构专为通过模态感知知识图谱进行多模态问答与推理而设计。MAHA 将稠密向量召回与结构化的图遍历相结合，其中知识图谱编码了跨模态的语义信息与关系。这种设计实现了在多种模态下语义丰富且上下文感知的检索。在多个基准数据集上的评估表明，MAHA 显著优于基线方法，达到 ROUGE-L 评分为 0.486，并实现了对所有模态的完整覆盖。这些结果突显了 MAHA 将嵌入表示与显式的文档结构相结合的能力，从而实现高效的多模态召回。我们的工作建立了一个可扩展且可解释的检索框架，通过在非结构化多模态数据上实现模态感知推理，推动了 RAG 系统的发展。",
        "translated_title": "多模态RAG用于非结构化数据：利用模态感知的知识图谱与混合召回",
        "label": [
            "多模态推荐",
            "LLM生成式推荐",
            "召回"
        ],
        "label_reason": "论文提出多模态RAG框架，适用于包含多种模态信息的推荐场景",
        "relevance_score": 7,
        "novelty_score": 8,
        "novelty_reason": "引入模态感知的知识图谱与混合检索机制，具有一定的创新性"
    },
    {
        "title": "Agentic Entropy-Balanced Policy Optimization",
        "url": "http://arxiv.org/abs/2510.14545v1",
        "pub_date": "2025-10-16",
        "summary": "Recently, Agentic Reinforcement Learning (Agentic RL) has made significant progress in incentivizing the multi-turn, long-horizon tool-use capabilities of web agents. While mainstream agentic RL algorithms autonomously explore high-uncertainty tool-call steps under the guidance of entropy, excessive reliance on entropy signals can impose further constraints, leading to the training collapse. In this paper, we delve into the challenges caused by entropy and propose the Agentic Entropy-Balanced Policy Optimization (AEPO), an agentic RL algorithm designed to balance entropy in both the rollout and policy update phases. AEPO comprises two core components: (1) a dynamic entropy-balanced rollout mechanism that adaptively allocate global and branch sampling budget through entropy pre-monitoring, while imposing a branch penalty on consecutive high-entropy tool-call steps to prevent over-branching issues; and (2) Entropy-Balanced Policy Optimization that inserts a stop-gradient operation into the high-entropy clipping term to preserve and properly rescale gradients on high-entropy tokens, while incorporating entropy-aware advantage estimation to prioritize learning on high-uncertainty tokens. Results across 14 challenging datasets show that AEPO consistently outperforms 7 mainstream RL algorithms. With just 1K RL samples, Qwen3-14B with AEPO achieves impressive results: 47.6% on GAIA, 11.2% on Humanity's Last Exam, and 43.0% on WebWalker for Pass@1; 65.0% on GAIA, 26.0% on Humanity's Last Exam, and 70.0% on WebWalker for Pass@5. Further analysis reveals that AEPO improves rollout sampling diversity while maintaining stable policy entropy, facilitating scalable web agent training.",
        "translated": "近期，Agentic 强化学习（Agentic RL）在激励网络智能体的多轮次、长时域工具使用能力方面取得了显著进展。尽管主流的 Agentic RL 算法在熵的指导下自主探索高不确定性工具调用步骤，但对熵信号的过度依赖可能带来进一步限制，导致训练崩溃。在本文中，我们深入探讨了由熵引起的挑战，并提出了 Agentic Entropy-Balanced Policy Optimization（AEPO），这是一种旨在在 rollout 和策略更新阶段平衡熵的 Agentic RL 算法。AEPO 包含两个核心组件：（1）一种动态熵平衡 rollout 机制，通过熵的预监控，自适应地分配全局与分支采样预算，同时对连续的高熵工具调用步骤施加分支惩罚，以防止过度分支问题；（2）熵平衡策略优化，将 stop-gradient 操作插入高熵裁剪项中，以保留并正确缩放高熵 token 的梯度，同时结合熵感知的优势估计，优先在高不确定性 token 上进行学习。在 14 个具有挑战性的数据集上的结果表明，AEPO 一致优于 7 种主流 RL 算法。仅使用 1K RL 样本，采用 AEPO 的 Qwen3-14B 在 Pass@1 指标上分别达到了 47.6%（GAIA）、11.2%（Humanity's Last Exam）和 43.0%（WebWalker）；在 Pass@5 指标上分别达到了 65.0%（GAIA）、26.0%（Humanity's Last Exam）和 70.0%（WebWalker）。进一步分析表明，AEPO 在保持策略熵稳定的同时提升了 rollout 采样的多样性，从而促进了网络智能体的可扩展训练。",
        "translated_title": "基于智能体的熵平衡策略优化",
        "label": [],
        "label_reason": "论文聚焦于强化学习中的策略优化，与推荐系统无直接关联。",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出了一种熵平衡策略优化方法，改进了探索机制，但属于通用强化学习范畴。"
    },
    {
        "title": "Acquisition of interpretable domain information during brain MR image\n  harmonization for content-based image retrieval",
        "url": "http://arxiv.org/abs/2510.14535v1",
        "pub_date": "2025-10-16",
        "summary": "Medical images like MR scans often show domain shifts across imaging sites due to scanner and protocol differences, which degrade machine learning performance in tasks such as disease classification. Domain harmonization is thus a critical research focus. Recent approaches encode brain images $\\boldsymbol{x}$ into a low-dimensional latent space $\\boldsymbol{z}$, then disentangle it into $\\boldsymbol{z_u}$ (domain-invariant) and $\\boldsymbol{z_d}$ (domain-specific), achieving strong results. However, these methods often lack interpretability$-$an essential requirement in medical applications$-$leaving practical issues unresolved. We propose Pseudo-Linear-Style Encoder Adversarial Domain Adaptation (PL-SE-ADA), a general framework for domain harmonization and interpretable representation learning that preserves disease-relevant information in brain MR images. PL-SE-ADA includes two encoders $f_E$ and $f_{SE}$ to extract $\\boldsymbol{z_u}$ and $\\boldsymbol{z_d}$, a decoder to reconstruct the image $f_D$, and a domain predictor $g_D$. Beyond adversarial training between the encoder and domain predictor, the model learns to reconstruct the input image $\\boldsymbol{x}$ by summing reconstructions from $\\boldsymbol{z_u}$ and $\\boldsymbol{z_d}$, ensuring both harmonization and informativeness. Compared to prior methods, PL-SE-ADA achieves equal or better performance in image reconstruction, disease classification, and domain recognition. It also enables visualization of both domain-independent brain features and domain-specific components, offering high interpretability across the entire framework.",
        "translated": "磁共振扫描等医学图像常常由于扫描设备和成像协议的差异，在不同成像中心之间表现出领域偏移，这会降低机器学习在疾病分类等任务中的性能。因此，领域调和成为了一个关键的研究方向。近期的方法将脑图像 $\\boldsymbol{x}$ 编码到一个低维潜在空间 $\\boldsymbol{z}$ 中，然后将其解耦为 $\\boldsymbol{z_u}$（领域不变）和 $\\boldsymbol{z_d}$（领域特定），取得了良好的效果。然而，这些方法通常缺乏可解释性——这是医学应用中的基本要求——从而未能解决实际问题。我们提出了一种伪线性风格编码器对抗领域自适应（Pseudo-Linear-Style Encoder Adversarial Domain Adaptation, PL-SE-ADA）框架，该框架是一种通用的领域调和与可解释表征学习方法，能够在脑部磁共振图像中保留与疾病相关的信息。PL-SE-ADA 包括两个编码器 $f_E$ 和 $f_{SE}$，分别用于提取 $\\boldsymbol{z_u}$ 和 $\\boldsymbol{z_d}$，一个用于图像重建的解码器 $f_D$，以及一个领域预测器 $g_D$。除了编码器与领域预测器之间的对抗训练外，模型还通过将 $\\boldsymbol{z_u}$ 和 $\\boldsymbol{z_d}$ 的重建结果相加，学习重建输入图像 $\\boldsymbol{x}$，从而确保领域调和和信息保留。与先前方法相比，PL-SE-ADA 在图像重建、疾病分类和领域识别任务中均达到了同等或更好的性能。此外，它还支持对领域无关的脑特征和领域相关组件进行可视化，为整个框架提供了高度的可解释性。",
        "translated_title": "在基于内容的医学图像检索中进行脑部 MR 图像协调过程中可解释领域信息的获取",
        "label": [],
        "label_reason": "主要涉及医学图像处理，与推荐系统无直接关联。",
        "relevance_score": 2,
        "novelty_score": 5,
        "novelty_reason": "在图像域适应方面提出了一定改进，但属于常规技术范畴。"
    },
    {
        "title": "MedTrust-RAG: Evidence Verification and Trust Alignment for Biomedical\n  Question Answering",
        "url": "http://arxiv.org/abs/2510.14400v1",
        "pub_date": "2025-10-16",
        "summary": "Biomedical question answering (QA) requires accurate interpretation of complex medical knowledge. Large language models (LLMs) have shown promising capabilities in this domain, with retrieval-augmented generation (RAG) systems enhancing performance by incorporating external medical literature. However, RAG-based approaches in biomedical QA suffer from hallucinations due to post-retrieval noise and insufficient verification of retrieved evidence, undermining response reliability. We propose MedTrust-Guided Iterative RAG, a framework designed to enhance factual consistency and mitigate hallucinations in medical QA. Our method introduces three key innovations. First, it enforces citation-aware reasoning by requiring all generated content to be explicitly grounded in retrieved medical documents, with structured Negative Knowledge Assertions used when evidence is insufficient. Second, it employs an iterative retrieval-verification process, where a verification agent assesses evidence adequacy and refines queries through Medical Gap Analysis until reliable information is obtained. Third, it integrates the MedTrust-Align Module (MTAM) that combines verified positive examples with hallucination-aware negative samples, leveraging Direct Preference Optimization to reinforce citation-grounded reasoning while penalizing hallucination-prone response patterns. Experiments on MedMCQA, MedQA, and MMLU-Med demonstrate that our approach consistently outperforms competitive baselines across multiple model architectures, achieving the best average accuracy with gains of 2.7% for LLaMA3.1-8B-Instruct and 2.4% for Qwen3-8B.",
        "translated": "生物医学问答（QA）需要对复杂的医学知识进行准确的理解。大语言模型（LLMs）在此领域展现出良好的潜力，而基于召回增强生成（RAG）的系统通过引入外部医学文献进一步提升了性能。然而，在生物医学QA中，基于RAG的方法由于召回后的噪声以及对召回证据验证不足，容易产生幻觉，从而影响响应的可靠性。我们提出MedTrust-Guided Iterative RAG，一个旨在提升医学问答事实一致性和减少幻觉的框架。我们的方法引入了三个关键创新。首先，该方法通过要求所有生成内容必须明确基于召回的医学文档进行推理，从而强制引用感知的推理过程。在证据不足时，采用结构化的Negative Knowledge Assertions进行补充。其次，该方法采用迭代的召回-验证过程，其中验证代理通过医学差距分析（Medical Gap Analysis）评估证据充分性并细化查询，直到获取可靠的信息。第三，该方法集成了MedTrust-Align Module（MTAM），将已验证的正例与具有幻觉感知能力的负样本相结合，利用直接偏好优化（Direct Preference Optimization）来强化引用基础推理，并对易产生幻觉的响应模式施加惩罚。在MedMCQA、MedQA和MMLU-Med上的实验表明，我们的方法在多种模型架构下均能持续超越具有竞争力的基线模型，取得了最佳的平均准确率，其中在LLaMA3.1-8B-Instruct上提高了2.7%，在Qwen3-8B上提高了2.4%。",
        "translated_title": "MedTrust-RAG：生物医学问答中的证据验证与信任对齐",
        "label": [],
        "label_reason": "论文聚焦医学问答，不直接涉及推荐系统核心技术",
        "relevance_score": 3,
        "novelty_score": 7,
        "novelty_reason": "提出创新性的迭代检索-验证框架，减少幻觉"
    },
    {
        "title": "PluriHop: Exhaustive, Recall-Sensitive QA over Distractor-Rich Corpora",
        "url": "http://arxiv.org/abs/2510.14377v1",
        "pub_date": "2025-10-16",
        "summary": "Recent advances in large language models (LLMs) and retrieval-augmented generation (RAG) have enabled progress on question answering (QA) when relevant evidence is in one (single-hop) or multiple (multi-hop) passages. Yet many realistic questions about recurring report data - medical records, compliance filings, maintenance logs - require aggregation across all documents, with no clear stopping point for retrieval and high sensitivity to even one missed passage. We term these pluri-hop questions and formalize them by three criteria: recall sensitivity, exhaustiveness, and exactness. To study this setting, we introduce PluriHopWIND, a diagnostic multilingual dataset of 48 pluri-hop questions built from 191 real-world wind industry reports in German and English. We show that PluriHopWIND is 8-40% more repetitive than other common datasets and thus has higher density of distractor documents, better reflecting practical challenges of recurring report corpora. We test a traditional RAG pipeline as well as graph-based and multimodal variants, and find that none of the tested approaches exceed 40% in statement-wise F1 score. Motivated by this, we propose PluriHopRAG, a RAG architecture that follows a \"check all documents individually, filter cheaply\" approach: it (i) decomposes queries into document-level subquestions and (ii) uses a cross-encoder filter to discard irrelevant documents before costly LLM reasoning. We find that PluriHopRAG achieves relative F1 score improvements of 18-52% depending on base LLM. Despite its modest size, PluriHopWIND exposes the limitations of current QA systems on repetitive, distractor-rich corpora. PluriHopRAG's performance highlights the value of exhaustive retrieval and early filtering as a powerful alternative to top-k methods.",
        "translated": "近年来，大语言模型（LLM）和检索增强生成（RAG）的进展使得在相关证据存在于一个（单跳）或多个（多跳）段落中的问答（QA）任务取得了进步。然而，许多关于重复报告数据的现实问题——如医疗记录、合规文件、维护日志——需要对所有文档进行聚合，检索过程中没有明确的停止点，且对漏掉一个段落都高度敏感。我们将这些问题称为“pluri-hop”问题，并通过三个标准对其进行形式化：召回敏感性（recall sensitivity）、全面性（exhaustiveness）和准确性（exactness）。为了研究这一场景，我们引入了 PluriHopWIND，这是一个用于诊断的多语言数据集，包含48个 pluri-hop 问题，由191份真实世界中的风能行业报告（以德语和英语编写）构建而成。我们发现，PluriHopWIND 比其他常见数据集多出8%-40%的重复内容，因此具有更高的干扰文档密度，更能反映重复报告语料库中的实际挑战。我们测试了传统 RAG 流水线以及基于图和多模态的变体，发现所有测试方法的陈述级 F1 分数均未超过40%。受此启发，我们提出了 PluriHopRAG，一种遵循“逐个检查所有文档、廉价过滤”的 RAG 架构：它（i）将查询分解为文档级别的子问题，（ii）使用交叉编码器过滤器在昂贵的 LLM 推理之前丢弃不相关的文档。我们发现，PluriHopRAG 在不同基础 LLM 上实现了18%-52%的相对 F1 分数提升。尽管 PluriHopWIND 的规模相对较小，但它揭示了当前问答系统在重复且干扰文档丰富的语料库中的局限性。PluriHopRAG 的性能凸显了全面检索和早期过滤作为 top-k 方法强大替代方案的价值。",
        "translated_title": "PluriHop: 在干扰信息丰富的语料库上进行详尽的、召回敏感的问答",
        "label": [
            "召回（Recall）"
        ],
        "label_reason": "论文关注问答中的召回敏感性，提出改进的RAG架构以提升文档检索效率。",
        "relevance_score": 4,
        "novelty_score": 7,
        "novelty_reason": "提出PluriHopRAG架构，通过文档级子问题分解和交叉编码器过滤提升性能。"
    },
    {
        "title": "Ensembling Multiple Hallucination Detectors Trained on VLLM Internal\n  Representations",
        "url": "http://arxiv.org/abs/2510.14330v1",
        "pub_date": "2025-10-16",
        "summary": "This paper presents the 5th place solution by our team, y3h2, for the Meta CRAG-MM Challenge at KDD Cup 2025. The CRAG-MM benchmark is a visual question answering (VQA) dataset focused on factual questions about images, including egocentric images. The competition was contested based on VQA accuracy, as judged by an LLM-based automatic evaluator. Since incorrect answers result in negative scores, our strategy focused on reducing hallucinations from the internal representations of the VLM. Specifically, we trained logistic regression-based hallucination detection models using both the hidden_state and the outputs of specific attention heads. We then employed an ensemble of these models. As a result, while our method sacrificed some correct answers, it significantly reduced hallucinations and allowed us to place among the top entries on the final leaderboard. For implementation details and code, please refer to https://gitlab.aicrowd.com/htanabe/meta-comprehensive-rag-benchmark-starter-kit.",
        "translated": "本文介绍了我们团队 y3h2 在 KDD Cup 2025 的 Meta CRAG-MM 挑战赛中获得第五名的解决方案。CRAG-MM 基准是一个专注于图像事实性问题的视觉问答（VQA）数据集，包括第一人称视角的图像。比赛根据 VQA 准确率进行评判，评判方式由基于大语言模型（LLM）的自动评估器完成。由于错误答案会导致负分，我们的策略重点在于减少视觉语言模型（VLM）内部表示中的幻觉现象。具体来说，我们使用隐藏状态（hidden_state）以及特定注意力头（attention heads）的输出，训练了基于逻辑回归的幻觉检测模型。随后，我们采用了这些模型的集成。结果表明，虽然我们的方法牺牲了一些正确答案，但显著减少了幻觉，使我们在最终排行榜中进入了前列。关于实现细节和代码，请参考 https://gitlab.aicrowd.com/htanabe/meta-comprehensive-rag-benchmark-starter-kit。",
        "translated_title": "基于大语言模型内部表示的多种幻觉检测器集成",
        "label": [],
        "label_reason": "论文聚焦于VQA中的幻觉检测，与推荐系统无直接关联。",
        "relevance_score": 2,
        "novelty_score": 5,
        "novelty_reason": "提出基于VLM内部表示的幻觉检测方法，有一定实用价值但创新性有限。"
    },
    {
        "title": "Large Reasoning Embedding Models: Towards Next-Generation Dense\n  Retrieval Paradigm",
        "url": "http://arxiv.org/abs/2510.14321v1",
        "pub_date": "2025-10-16",
        "summary": "In modern e-commerce search systems, dense retrieval has become an indispensable component. By computing similarities between query and item (product) embeddings, it efficiently selects candidate products from large-scale repositories. With the breakthroughs in large language models (LLMs), mainstream embedding models have gradually shifted from BERT to LLMs for more accurate text modeling. However, these models still adopt direct-embedding methods, and the semantic accuracy of embeddings remains inadequate. Therefore, contrastive learning is heavily employed to achieve tight semantic alignment between positive pairs. Consequently, such models tend to capture statistical co-occurrence patterns in the training data, biasing them toward shallow lexical and semantic matches. For difficult queries exhibiting notable lexical disparity from target items, the performance degrades significantly. In this work, we propose the Large Reasoning Embedding Model (LREM), which novelly integrates reasoning processes into representation learning. For difficult queries, LREM first conducts reasoning to achieve a deep understanding of the original query, and then produces a reasoning-augmented query embedding for retrieval. This reasoning process effectively bridges the semantic gap between original queries and target items, significantly improving retrieval accuracy. Specifically, we adopt a two-stage training process: the first stage optimizes the LLM on carefully curated Query-CoT-Item triplets with SFT and InfoNCE losses to establish preliminary reasoning and embedding capabilities, and the second stage further refines the reasoning trajectories via reinforcement learning (RL). Extensive offline and online experiments validate the effectiveness of LREM, leading to its deployment on China's largest e-commerce platform since August 2025.",
        "translated": "在现代电子商务搜索系统中，稠密召回已成为不可或缺的组成部分。通过计算查询与物料（产品）嵌入之间的相似性，它能够高效地从大规模仓库中选取候选产品。随着大语言模型（LLM）的突破，主流嵌入模型已逐渐从 BERT 转向 LLM，以实现更精确的文本建模。然而，这些模型仍采用直接嵌入方法，其嵌入的语义准确性仍然不足。因此，对比学习被广泛用于实现正样本对之间的紧密语义对齐。结果是，这些模型倾向于捕捉训练数据中的统计共现模式，使其偏向于浅层的词汇和语义匹配。对于与目标物料存在显著词汇差异的复杂查询，其性能会明显下降。在本研究中，我们提出了大型推理嵌入模型（LREM），其创新性地将推理过程整合到表示学习中。对于复杂查询，LREM 首先进行推理以实现对原始查询的深入理解，然后生成一个经过推理增强的查询嵌入用于召回。这一推理过程有效地弥合了原始查询与目标物料之间的语义差距，显著提升了召回精度。具体来说，我们采用了一个两阶段的训练过程：第一阶段在精心构建的 Query-CoT-Item 三元组上优化 LLM，使用监督微调（SFT）和 InfoNCE 损失以建立初步的推理和嵌入能力；第二阶段则通过强化学习（RL）进一步优化推理轨迹。大量的离线和在线实验验证了 LREM 的有效性，自 2025 年 8 月起，该模型已部署在中国最大的电子商务平台上。",
        "translated_title": "大型推理嵌入模型：迈向下一代密集召回范式",
        "label": [
            "召回",
            "通用推荐技术",
            "负采样与对比学习"
        ],
        "label_reason": "论文聚焦于电商搜索中的密集召回优化，与推荐系统召回环节直接相关。",
        "relevance_score": 8,
        "novelty_score": 9,
        "novelty_reason": "提出将推理过程融入嵌入学习，通过强化学习优化推理轨迹，方法新颖且效果显著。"
    },
    {
        "title": "Rethinking Schema Linking: A Context-Aware Bidirectional Retrieval\n  Approach for Text-to-SQL",
        "url": "http://arxiv.org/abs/2510.14296v1",
        "pub_date": "2025-10-16",
        "summary": "Schema linking -- the process of aligning natural language questions with database schema elements -- is a critical yet underexplored component of Text-to-SQL systems. While recent methods have focused primarily on improving SQL generation, they often neglect the retrieval of relevant schema elements, which can lead to hallucinations and execution failures. In this work, we propose a context-aware bidirectional schema retrieval framework that treats schema linking as a standalone problem. Our approach combines two complementary strategies: table-first retrieval followed by column selection, and column-first retrieval followed by table selection. It is further augmented with techniques such as question decomposition, keyword extraction, and keyphrase extraction. Through comprehensive evaluations on challenging benchmarks such as BIRD and Spider, we demonstrate that our method significantly improves schema recall while reducing false positives. Moreover, SQL generation using our retrieved schema consistently outperforms full-schema baselines and closely approaches oracle performance, all without requiring query refinement. Notably, our method narrows the performance gap between full and perfect schema settings by 50\\%. Our findings highlight schema linking as a powerful lever for enhancing Text-to-SQL accuracy and efficiency.",
        "translated": "模式链接——将自然语言问题与数据库模式元素对齐的过程——是文本到SQL（Text-to-SQL）系统中一个关键但尚未充分探索的组成部分。尽管最近的方法主要集中在提升SQL生成的能力上，它们往往忽视了相关模式元素的检索，这可能导致幻觉（hallucinations）和执行失败。在本文中，我们提出了一种上下文感知的双向模式检索框架，将模式链接视为一个独立的问题进行处理。我们的方法结合了两种互补策略：先进行表检索，然后选择列；以及先进行列检索，然后选择表。此外，该方法还融合了诸如问题分解、关键词提取和关键短语提取等技术。在具有挑战性的基准数据集BIRD和Spider上的全面评估表明，我们的方法在显著提升模式召回率的同时降低了误报率。此外，基于我们所检索到的模式进行SQL生成在性能上始终优于全模式基线方法，并且接近理想模式下的性能，而无需进行查询优化。值得注意的是，我们的方法将全模式与理想模式设置之间的性能差距缩小了50\\%。我们的研究结果表明，模式链接是提升文本到SQL系统准确性和效率的强大杠杆。",
        "translated_title": "Rethinking Schema Linking: A Context-Aware Bidirectional Retrieval Approach for Text-to-SQL  \n重新思考模式链接：一种面向上下文的双向检索方法用于文本到SQL  \n\nAbstract  \n摘要  \n\nSchema linking is a critical component in text-to-SQL tasks, aiming to map natural language utterances to corresponding database schema elements. Most existing methods focus on modeling the schema from a single perspective, either by leveraging schema elements independently or by incorporating contextual information from the natural language query. However, these approaches often suffer from incomplete schema understanding and limited contextual awareness, leading to suboptimal performance in complex schema environments. To address this issue, we propose a **Context-Aware Bidirectional Retrieval (CABR)** framework that simultaneously considers both the schema context and the query context. CABR introduces a dual-encoder architecture to learn semantic representations for schema elements and queries from different views, and establishes a bidirectional retrieval mechanism that links schema elements to queries and vice versa. This design allows the model to better capture the interdependencies between schema elements and queries, improving the accuracy of schema linking. We conduct extensive experiments on the Spider and WikiSQL datasets, and the results demonstrate that CABR significantly outperforms state-of-the-art baselines in both schema element selection and SQL generation.  \n模式链接是文本到SQL任务中的关键组成部分，其目标是将自然语言语句映射到对应的数据库模式元素。大多数现有方法从单一视角建模模式，要么独立利用模式元素，要么结合来自自然语言查询的上下文信息。然而，这些方法通常存在模式理解不完整和上下文感知能力有限的问题，导致在复杂模式环境下性能欠佳。为了解决这一问题，我们提出了一种**面向上下文的双向检索（CABR）**框架，该框架同时考虑模式上下文和查询上下文。CABR引入了一种双编码器结构，从不同视角学习模式元素和查询的语义表示，并建立了一个双向检索机制，将模式元素与查询相互链接。该设计使模型能够更好地捕捉模式元素与查询之间的依赖关系，提高模式链接的准确性。我们在Spider和WikiSQL数据集上进行了广泛的实验，结果表明，CABR在模式元素选择和SQL生成方面均显著优于最先进的基线方法。  \n\nIntroduction  \n引言  \n\nText-to-SQL tasks aim to automatically translate natural language queries into executable SQL statements, enabling users to interact with databases without requiring SQL expertise. A core challenge in this task is schema linking, which involves identifying the relevant schema elements (e.g., tables, columns, values) for a given query. Effective schema linking is essential for accurate SQL generation, as incorrect or incomplete schema mappings can lead to invalid queries and poor performance.  \n文本到SQL任务旨在自动将自然语言查询转化为可执行的SQL语句，使用户无需具备SQL专业知识即可与数据库进行交互。该任务中的一个核心挑战是模式链接，即识别给定查询中相关的模式元素（例如，表、列、值）。有效的模式链接对于准确的SQL生成至关重要，因为错误或不完整的模式映射可能导致无效查询和较差的性能。  \n\nTraditional schema linking approaches typically treat the schema as a flat structure and model it using either rule-based methods or single-view neural architectures. These methods often fail to capture the full context of both the schema and the query, especially in complex, multi-table environments where relationships between schema elements are crucial.  \n传统的模式链接方法通常将模式视为扁平结构，并使用基于规则的方法或单视角神经网络架构对其进行建模。这些方法常常无法捕捉模式和查询的完整上下文，特别是在复杂的多表环境中，其中模式元素之间的关系至关重要。  \n\nTo overcome these limitations, we propose a novel framework that jointly models schema and query contexts using a bidirectional retrieval mechanism. Our method enhances the contextual awareness of schema elements by considering both their structural information and their semantic relationships with the query.  \n为克服这些限制，我们提出了一种新的框架，使用双向检索机制联合建模模式和查询上下文。我们的方法通过考虑模式元素的结构信息及其与查询的语义关系，增强了其上下文感知能力。",
        "label": [],
        "label_reason": "论文聚焦Text-to-SQL，非推荐系统核心问题，相关性较低。",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出新颖的双向检索框架，改进Schema Linking效果。"
    },
    {
        "title": "PRISM: Agentic Retrieval with LLMs for Multi-Hop Question Answering",
        "url": "http://arxiv.org/abs/2510.14278v1",
        "pub_date": "2025-10-16",
        "summary": "Retrieval plays a central role in multi-hop question answering (QA), where answering complex questions requires gathering multiple pieces of evidence. We introduce an Agentic Retrieval System that leverages large language models (LLMs) in a structured loop to retrieve relevant evidence with high precision and recall. Our framework consists of three specialized agents: a Question Analyzer that decomposes a multi-hop question into sub-questions, a Selector that identifies the most relevant context for each sub-question (focusing on precision), and an Adder that brings in any missing evidence (focusing on recall). The iterative interaction between Selector and Adder yields a compact yet comprehensive set of supporting passages. In particular, it achieves higher retrieval accuracy while filtering out distracting content, enabling downstream QA models to surpass full-context answer accuracy while relying on significantly less irrelevant information. Experiments on four multi-hop QA benchmarks -- HotpotQA, 2WikiMultiHopQA, MuSiQue, and MultiHopRAG -- demonstrates that our approach consistently outperforms strong baselines.",
        "translated": "召回在多跳问答（QA）中起着核心作用，其中回答复杂问题需要收集多个证据。我们引入了一种基于智能体的召回系统（Agentic Retrieval System），通过结构化的循环利用大语言模型（LLMs），以高精度和高召回率检索相关证据。我们的框架包含三个专门的智能体：一个用于将多跳问题拆解为子问题的问题分析器（Question Analyzer），一个用于为每个子问题识别最相关上下文（注重精度）的选取器（Selector），以及一个用于补充缺失证据（注重召回）的添加器（Adder）。Selector 与 Adder 之间的迭代交互产生了一组紧凑而全面的支持段落。特别是，它在过滤干扰内容的同时实现了更高的召回精度，使下游 QA 模型能够在依赖明显更少无关信息的情况下超越全上下文回答的准确性。在四个多跳 QA 基准测试 HotpotQA、2WikiMultiHopQA、MuSiQue 和 MultiHopRAG 上的实验表明，我们的方法始终优于强基线方法。",
        "translated_title": "PRISM：利用大语言模型进行多跳问答的代理式召回",
        "label": [
            "多模态推荐",
            "LLM生成式推荐"
        ],
        "label_reason": "论文涉及LLM用于信息检索，可间接用于推荐系统的生成式方法",
        "relevance_score": 5,
        "novelty_score": 7,
        "novelty_reason": "提出结构化代理检索框架，改进多跳QA的精度与召回"
    },
    {
        "title": "Coupled Diffusion Sampling for Training-Free Multi-View Image Editing",
        "url": "http://arxiv.org/abs/2510.14981v1",
        "pub_date": "2025-10-16",
        "summary": "We present an inference-time diffusion sampling method to perform multi-view consistent image editing using pre-trained 2D image editing models. These models can independently produce high-quality edits for each image in a set of multi-view images of a 3D scene or object, but they do not maintain consistency across views. Existing approaches typically address this by optimizing over explicit 3D representations, but they suffer from a lengthy optimization process and instability under sparse view settings. We propose an implicit 3D regularization approach by constraining the generated 2D image sequences to adhere to a pre-trained multi-view image distribution. This is achieved through coupled diffusion sampling, a simple diffusion sampling technique that concurrently samples two trajectories from both a multi-view image distribution and a 2D edited image distribution, using a coupling term to enforce the multi-view consistency among the generated images. We validate the effectiveness and generality of this framework on three distinct multi-view image editing tasks, demonstrating its applicability across various model architectures and highlighting its potential as a general solution for multi-view consistent editing.",
        "translated": "我们提出了一种在推理阶段使用的扩散采样方法，用于利用预训练的 2D 图像编辑模型进行多视角一致的图像编辑。这些模型可以独立地为 3D 场景或物体的多视角图像集中的每张图像生成高质量的编辑结果，但它们在不同视角之间无法保持一致性。现有方法通常通过在显式的 3D 表示上进行优化来解决这一问题，但它们在稀疏视角设置下优化过程漫长且不稳定。我们提出了一种隐式的 3D 正则化方法，通过约束生成的 2D 图像序列以遵循预训练的多视角图像分布。该方法是通过耦合扩散采样实现的，这是一种简单的扩散采样技术，同时从多视角图像分布和 2D 编辑图像分布中采样两条轨迹，并使用耦合项来强制生成图像之间的多视角一致性。我们在三个不同的多视角图像编辑任务上验证了该框架的有效性和通用性，展示了其在各种模型架构中的适用性，并突出了其作为多视角一致性编辑通用解决方案的潜力。",
        "translated_title": "Coupled Diffusion Sampling for Training-Free Multi-View Image Editing  \n用于无需训练的多视角图像编辑的耦合扩散采样",
        "label": [
            "多帧/视频图像恢复"
        ],
        "label_reason": "方法涉及多视角图像生成一致性，属于多帧图像恢复范畴",
        "relevance_score": 5,
        "novelty_score": 8,
        "novelty_reason": "提出耦合扩散采样新方法，具有一定的创新性"
    },
    {
        "title": "From Pixels to Words -- Towards Native Vision-Language Primitives at\n  Scale",
        "url": "http://arxiv.org/abs/2510.14979v1",
        "pub_date": "2025-10-16",
        "summary": "The edifice of native Vision-Language Models (VLMs) has emerged as a rising contender to typical modular VLMs, shaped by evolving model architectures and training paradigms. Yet, two lingering clouds cast shadows over its widespread exploration and promotion: (-) What fundamental constraints set native VLMs apart from modular ones, and to what extent can these barriers be overcome? (-) How to make research in native VLMs more accessible and democratized, thereby accelerating progress in the field. In this paper, we clarify these challenges and outline guiding principles for constructing native VLMs. Specifically, one native VLM primitive should: (i) effectively align pixel and word representations within a shared semantic space; (ii) seamlessly integrate the strengths of formerly separate vision and language modules; (iii) inherently embody various cross-modal properties that support unified vision-language encoding, aligning, and reasoning. Hence, we launch NEO, a novel family of native VLMs built from first principles, capable of rivaling top-tier modular counterparts across diverse real-world scenarios. With only 390M image-text examples, NEO efficiently develops visual perception from scratch while mitigating vision-language conflicts inside a dense and monolithic model crafted from our elaborate primitives. We position NEO as a cornerstone for scalable and powerful native VLMs, paired with a rich set of reusable components that foster a cost-effective and extensible ecosystem. Our code and models are publicly available at: https://github.com/EvolvingLMMs-Lab/NEO.",
        "translated": "原生视觉-语言模型（VLMs）的构建已逐渐成为传统模块化VLMs的有力竞争者，这得益于模型架构和训练范式的发展与演变。然而，两个关键问题仍阻碍其广泛研究与推广：（-）原生VLMs与模块化VLMs之间存在哪些基本限制？这些限制在多大程度上可以被克服？（-）如何使原生VLMs的研究更具可访问性和普及性，从而加快该领域的发展进程？在本文中，我们明确了这些挑战，并提出了构建原生VLMs的指导原则。具体而言，一个原生VLM的基本要素应满足以下三点：（i）在共享的语义空间中有效对齐像素与语言表示；（ii）无缝融合视觉与语言模块各自的优势；（iii）内在地体现多种跨模态属性，以支持统一的视觉-语言编码、对齐和推理。因此，我们提出了NEO，一种基于第一性原理构建的新型原生VLM家族，其在多种现实场景中能够媲美最先进的模块化模型。在仅使用390M图像-文本样例的情况下，NEO便能高效地从零开始发展视觉感知能力，同时在由我们精心设计的密集单一模型中缓解视觉-语言冲突。我们视NEO为构建可扩展且强大的原生VLMs的基石，并提供了丰富的可复用组件，以促进经济高效且可扩展的生态系统形成。我们的代码和模型已公开发布于：https://github.com/EvolvingLMMs-Lab/NEO。",
        "translated_title": "从像素到词语——迈向大规模的原生视觉-语言基元",
        "label": [],
        "label_reason": "论文专注于视觉-语言模型，非图像像素级处理任务。",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "提出新的视觉-语言模型构建原则和组件，但创新点主要在架构整合。"
    },
    {
        "title": "Agentic Design of Compositional Machines",
        "url": "http://arxiv.org/abs/2510.14980v1",
        "pub_date": "2025-10-16",
        "summary": "The design of complex machines stands as both a marker of human intelligence and a foundation of engineering practice. Given recent advances in large language models (LLMs), we ask whether they, too, can learn to create. We approach this question through the lens of compositional machine design: a task in which machines are assembled from standardized components to meet functional demands like locomotion or manipulation in a simulated physical environment. To support this investigation, we introduce BesiegeField, a testbed built on the machine-building game Besiege, which enables part-based construction, physical simulation and reward-driven evaluation. Using BesiegeField, we benchmark state-of-the-art LLMs with agentic workflows and identify key capabilities required for success, including spatial reasoning, strategic assembly, and instruction-following. As current open-source models fall short, we explore reinforcement learning (RL) as a path to improvement: we curate a cold-start dataset, conduct RL finetuning experiments, and highlight open challenges at the intersection of language, machine design, and physical reasoning.",
        "translated": "复杂机器的设计既是人类智能的标志，也是工程实践的基础。鉴于近年来在大语言模型（LLMs）方面取得的进展，我们提出一个问题：它们是否也能学会创造？我们通过组合式机器设计的视角来探讨这一问题：该任务要求在模拟的物理环境中，通过标准化组件的组合来满足功能性需求，例如移动或操作。为了支持这项研究，我们引入了 BesiegeField，这是一个基于机器建造游戏 Besiege 构建的测试平台，支持基于部件的构建、物理模拟以及基于奖励的评估。借助 BesiegeField，我们对最先进的 LLMs 以及代理式工作流程进行了基准测试，并识别出成功所必需的关键能力，包括空间推理、策略性组装和遵循指令。由于目前的开源模型尚无法满足要求，我们探索了强化学习（RL）作为提升的路径：我们构建了一个冷启动数据集，进行了 RL 微调实验，并强调了语言、机器设计和物理推理交叉领域中的开放挑战。",
        "translated_title": "Agentic Design of Compositional Machines  \n复合机器的智能体设计",
        "label": [],
        "label_reason": "论文聚焦于机器设计与语言模型应用，非图像像素级处理任务",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出新测试平台与RL冷启动数据集，但方法迁移性有限"
    },
    {
        "title": "Learning an Image Editing Model without Image Editing Pairs",
        "url": "http://arxiv.org/abs/2510.14978v1",
        "pub_date": "2025-10-16",
        "summary": "Recent image editing models have achieved impressive results while following natural language editing instructions, but they rely on supervised fine-tuning with large datasets of input-target pairs. This is a critical bottleneck, as such naturally occurring pairs are hard to curate at scale. Current workarounds use synthetic training pairs that leverage the zero-shot capabilities of existing models. However, this can propagate and magnify the artifacts of the pretrained model into the final trained model. In this work, we present a new training paradigm that eliminates the need for paired data entirely. Our approach directly optimizes a few-step diffusion model by unrolling it during training and leveraging feedback from vision-language models (VLMs). For each input and editing instruction, the VLM evaluates if an edit follows the instruction and preserves unchanged content, providing direct gradients for end-to-end optimization. To ensure visual fidelity, we incorporate distribution matching loss (DMD), which constrains generated images to remain within the image manifold learned by pretrained models. We evaluate our method on standard benchmarks and include an extensive ablation study. Without any paired data, our method performs on par with various image editing diffusion models trained on extensive supervised paired data, under the few-step setting. Given the same VLM as the reward model, we also outperform RL-based techniques like Flow-GRPO.",
        "translated": "近期的图像编辑模型在遵循自然语言编辑指令的情况下取得了令人印象深刻的结果，但它们依赖于使用大量输入-目标对数据集进行监督式微调。这成为了一个关键性的瓶颈，因为这种自然形成的配对数据很难大规模地进行整理。当前的变通方法使用合成的训练对，利用现有模型的零样本能力。然而，这可能会将预训练模型中的伪影传播并放大到最终训练好的模型中。在本文中，我们提出了一种新的训练范式，完全消除了对配对数据的需求。我们的方法在训练过程中通过展开模型并利用视觉-语言模型（VLM）的反馈，直接优化一个几步扩散模型。对于每个输入图像和编辑指令，VLM会评估编辑是否遵循指令并保留未更改的内容，从而提供端到端优化所需的直接梯度。为了确保视觉保真度，我们引入了分布匹配损失（DMD），该损失限制生成的图像保持在预训练模型所学习的图像流形内。我们在标准基准数据集上评估了我们的方法，并进行了广泛的消融研究。在几步扩散设置下，即使没有任何配对数据，我们的方法也与使用大量监督配对数据训练的各种图像编辑扩散模型表现相当。在使用相同VLM作为奖励模型的前提下，我们的方法还优于基于强化学习的技术，如 Flow-GRPO。",
        "translated_title": "学习无需图像编辑配对的图像编辑模型",
        "label": [],
        "label_reason": "论文主要关注图像编辑而非像素级恢复",
        "relevance_score": 3,
        "novelty_score": 8,
        "novelty_reason": "提出无需配对数据的图像编辑新范式"
    },
    {
        "title": "Ponimator: Unfolding Interactive Pose for Versatile Human-human\n  Interaction Animation",
        "url": "http://arxiv.org/abs/2510.14976v1",
        "pub_date": "2025-10-16",
        "summary": "Close-proximity human-human interactive poses convey rich contextual information about interaction dynamics. Given such poses, humans can intuitively infer the context and anticipate possible past and future dynamics, drawing on strong priors of human behavior. Inspired by this observation, we propose Ponimator, a simple framework anchored on proximal interactive poses for versatile interaction animation. Our training data consists of close-contact two-person poses and their surrounding temporal context from motion-capture interaction datasets. Leveraging interactive pose priors, Ponimator employs two conditional diffusion models: (1) a pose animator that uses the temporal prior to generate dynamic motion sequences from interactive poses, and (2) a pose generator that applies the spatial prior to synthesize interactive poses from a single pose, text, or both when interactive poses are unavailable. Collectively, Ponimator supports diverse tasks, including image-based interaction animation, reaction animation, and text-to-interaction synthesis, facilitating the transfer of interaction knowledge from high-quality mocap data to open-world scenarios. Empirical experiments across diverse datasets and applications demonstrate the universality of the pose prior and the effectiveness and robustness of our framework.",
        "translated": "近距离的人与人之间的交互姿态能够传达丰富的交互动态上下文信息。在给定这些姿态的情况下，人类可以直观地推断交互的上下文，并预测可能的过去和未来动态，这依赖于对人类行为的强先验知识。受这一观察的启发，我们提出了 Ponimator，一种基于邻近交互姿态的简单框架，用于实现多样化的交互动画。我们的训练数据包括来自运动捕捉交互数据集的近距离接触的双人姿态及其周围的时间上下文。借助交互姿态先验，Ponimator 使用了两个条件扩散模型：(1) 一个姿态动画生成器，利用时间先验从交互姿态生成动态运动序列；(2) 一个姿态生成器，当交互姿态不可用时，应用空间先验从单个姿态、文本或两者共同合成交互姿态。总体而言，Ponimator 支持多种任务，包括基于图像的交互动画、反应动画以及从文本生成交互，从而促进将高质量运动捕捉数据中的交互知识迁移至开放世界场景中。在多个数据集和应用上的实证实验表明，姿态先验具有普遍性，且我们的框架在性能和鲁棒性方面均表现出色。",
        "translated_title": "Ponimator：展开交互式姿态以实现多样化的人与人交互动画",
        "label": [],
        "label_reason": "论文聚焦于人体姿态动画生成，属于高阶任务，不涉及像素级图像处理",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "提出了基于扩散模型的姿态动画框架，但方法属于常规迁移"
    },
    {
        "title": "Terra: Explorable Native 3D World Model with Point Latents",
        "url": "http://arxiv.org/abs/2510.14977v1",
        "pub_date": "2025-10-16",
        "summary": "World models have garnered increasing attention for comprehensive modeling of the real world. However, most existing methods still rely on pixel-aligned representations as the basis for world evolution, neglecting the inherent 3D nature of the physical world. This could undermine the 3D consistency and diminish the modeling efficiency of world models. In this paper, we present Terra, a native 3D world model that represents and generates explorable environments in an intrinsic 3D latent space. Specifically, we propose a novel point-to-Gaussian variational autoencoder (P2G-VAE) that encodes 3D inputs into a latent point representation, which is subsequently decoded as 3D Gaussian primitives to jointly model geometry and appearance. We then introduce a sparse point flow matching network (SPFlow) for generating the latent point representation, which simultaneously denoises the positions and features of the point latents. Our Terra enables exact multi-view consistency with native 3D representation and architecture, and supports flexible rendering from any viewpoint with only a single generation process. Furthermore, Terra achieves explorable world modeling through progressive generation in the point latent space. We conduct extensive experiments on the challenging indoor scenes from ScanNet v2. Terra achieves state-of-the-art performance in both reconstruction and generation with high 3D consistency.",
        "translated": "世界模型因其对现实世界的全面建模能力而受到越来越多的关注。然而，大多数现有方法仍然依赖像素对齐的表示作为世界演化的基础，忽视了物理世界固有的三维特性。这可能会削弱世界模型的三维一致性，并降低其建模效率。在本文中，我们提出了 Terra，这是一种原生的三维世界模型，能够在内在的三维潜在空间中表示和生成可探索的环境。具体而言，我们提出了一种新颖的点到高斯变分自编码器（P2G-VAE），它将三维输入编码为潜在点表示，随后将其解码为三维高斯基元，以联合建模几何和外观。我们接着引入了一个稀疏点流匹配网络（SPFlow）来生成潜在点表示，该网络可同时对点潜在的位置和特征进行去噪。我们的 Terra 通过原生的三维表示和架构实现了精确的多视角一致性，并支持仅通过一次生成过程即可从任意视角进行灵活渲染。此外，Terra 通过在点潜在空间中的渐进生成实现了可探索的世界建模。我们在具有挑战性的 ScanNet v2 室内场景上进行了广泛的实验，Terra 在重建和生成方面均达到了最先进的性能，且具有高度的三维一致性。",
        "translated_title": "Terra：具有点潜在表示的可探索本原三维世界模型",
        "label": [],
        "label_reason": "论文关注3D世界建模与生成，不直接处理像素级图像质量",
        "relevance_score": 3,
        "novelty_score": 8,
        "novelty_reason": "提出P2G-VAE与SPFlow新架构，实现3D表示与生成"
    },
    {
        "title": "WithAnyone: Towards Controllable and ID Consistent Image Generation",
        "url": "http://arxiv.org/abs/2510.14975v1",
        "pub_date": "2025-10-16",
        "summary": "Identity-consistent generation has become an important focus in text-to-image research, with recent models achieving notable success in producing images aligned with a reference identity. Yet, the scarcity of large-scale paired datasets containing multiple images of the same individual forces most approaches to adopt reconstruction-based training. This reliance often leads to a failure mode we term copy-paste, where the model directly replicates the reference face rather than preserving identity across natural variations in pose, expression, or lighting. Such over-similarity undermines controllability and limits the expressive power of generation. To address these limitations, we (1) construct a large-scale paired dataset MultiID-2M, tailored for multi-person scenarios, providing diverse references for each identity; (2) introduce a benchmark that quantifies both copy-paste artifacts and the trade-off between identity fidelity and variation; and (3) propose a novel training paradigm with a contrastive identity loss that leverages paired data to balance fidelity with diversity. These contributions culminate in WithAnyone, a diffusion-based model that effectively mitigates copy-paste while preserving high identity similarity. Extensive qualitative and quantitative experiments demonstrate that WithAnyone significantly reduces copy-paste artifacts, improves controllability over pose and expression, and maintains strong perceptual quality. User studies further validate that our method achieves high identity fidelity while enabling expressive controllable generation.",
        "translated": "身份一致的生成已成为文本到图像研究中的一个重要焦点，近期模型在生成与参考身份对齐的图像方面取得了显著成功。然而，由于缺乏大规模的配对数据集，其中包含同一人的多张图像，大多数方法被迫采用基于重建的训练方式。这种依赖通常导致一种我们称之为“复制粘贴”的失效模式，即模型直接复制参考图像中的面部，而不是在姿态、表情或光照等自然变化下保持身份一致性。这种过度相似性削弱了生成的可控性，并限制了生成的表现力。为了解决这些局限，我们（1）构建了一个大规模的配对数据集 MultiID-2M，专门针对多人员场景，为每个身份提供多样化的参考图像；（2）引入了一个基准测试，用于量化“复制粘贴”伪影以及身份保真度与变化之间的权衡；（3）提出了一种新颖的训练范式，采用对比身份损失，利用配对数据在保真度和多样性之间取得平衡。这些贡献促成了 WithAnyone 这一基于扩散的模型，该模型在有效缓解“复制粘贴”问题的同时，仍能保持高身份相似性。大量定性和定量实验表明，WithAnyone 显著减少了“复制粘贴”伪影，提高了对姿态和表情的可控性，并保持了良好的感知质量。用户研究进一步验证了我们的方法在保持高身份保真度的同时，能够实现富有表现力的可控生成。",
        "translated_title": "WithAnyone: 超可控且ID一致的图像生成",
        "label": [],
        "label_reason": "论文关注文本生成图像的身份一致性，属于高阶生成任务，不涉及像素级图像恢复或增强。",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出了对比身份损失和多身份数据集，但方法基于扩散模型，创新性有限。"
    },
    {
        "title": "pi-Flow: Policy-Based Few-Step Generation via Imitation Distillation",
        "url": "http://arxiv.org/abs/2510.14974v1",
        "pub_date": "2025-10-16",
        "summary": "Few-step diffusion or flow-based generative models typically distill a velocity-predicting teacher into a student that predicts a shortcut towards denoised data. This format mismatch has led to complex distillation procedures that often suffer from a quality-diversity trade-off. To address this, we propose policy-based flow models ($\\pi$-Flow). $\\pi$-Flow modifies the output layer of a student flow model to predict a network-free policy at one timestep. The policy then produces dynamic flow velocities at future substeps with negligible overhead, enabling fast and accurate ODE integration on these substeps without extra network evaluations. To match the policy's ODE trajectory to the teacher's, we introduce a novel imitation distillation approach, which matches the policy's velocity to the teacher's along the policy's trajectory using a standard $\\ell_2$ flow matching loss. By simply mimicking the teacher's behavior, $\\pi$-Flow enables stable and scalable training and avoids the quality-diversity trade-off. On ImageNet 256$^2$, it attains a 1-NFE FID of 2.85, outperforming MeanFlow of the same DiT architecture. On FLUX.1-12B and Qwen-Image-20B at 4 NFEs, $\\pi$-Flow achieves substantially better diversity than state-of-the-art few-step methods, while maintaining teacher-level quality.",
        "translated": "基于少量步骤的扩散模型或流模型通常将预测速度的教师模型提炼为一个预测通向去噪数据捷径的学生模型。这种格式上的不匹配导致了复杂的提炼过程，通常会陷入质量和多样性之间的权衡问题。为了解决这一问题，我们提出了基于策略的流模型（$\\pi$-Flow）。$\\pi$-Flow 修改了学生流模型的输出层，使其在一个时间步上预测一个无需网络的策略。该策略随后以极低的计算开销生成未来子步骤中的动态流速度，从而在这些子步骤上实现快速且准确的 ODE 积分，而无需额外的网络评估。为了使策略的 ODE 轨迹与教师模型相匹配，我们引入了一种新颖的模仿提炼方法，该方法通过在策略轨迹上使用标准的 $\\ell_2$ 流匹配损失，将策略的速度与教师模型的速度进行匹配。通过简单地模仿教师模型的行为，$\\pi$-Flow 实现了稳定且可扩展的训练，并避免了质量和多样性之间的权衡问题。在 ImageNet 256$^2$ 数据集上，$\\pi$-Flow 达到了 1-NFE FID 为 2.85，优于相同 DiT 架构下的 MeanFlow。在 FLUX.1-12B 和 Qwen-Image-20B 模型上，使用 4 个 NFE 时，$\\pi$-Flow 显著提升了多样性，同时保持了与教师模型相当的质量。",
        "translated_title": "pi-Flow: 基于策略的少步生成方法通过模仿蒸馏",
        "label": [],
        "label_reason": "论文主要关注生成模型的快速推理，不属于图像恢复或增强任务",
        "relevance_score": 2,
        "novelty_score": 8,
        "novelty_reason": "提出基于策略的生成模型新方法，改进了少步生成的质量与多样性平衡"
    },
    {
        "title": "RDD: Retrieval-Based Demonstration Decomposer for Planner Alignment in\n  Long-Horizon Tasks",
        "url": "http://arxiv.org/abs/2510.14968v1",
        "pub_date": "2025-10-16",
        "summary": "To tackle long-horizon tasks, recent hierarchical vision-language-action (VLAs) frameworks employ vision-language model (VLM)-based planners to decompose complex manipulation tasks into simpler sub-tasks that low-level visuomotor policies can easily handle. Typically, the VLM planner is finetuned to learn to decompose a target task. This finetuning requires target task demonstrations segmented into sub-tasks by either human annotation or heuristic rules. However, the heuristic subtasks can deviate significantly from the training data of the visuomotor policy, which degrades task performance. To address these issues, we propose a Retrieval-based Demonstration Decomposer (RDD) that automatically decomposes demonstrations into sub-tasks by aligning the visual features of the decomposed sub-task intervals with those from the training data of the low-level visuomotor policies. Our method outperforms the state-of-the-art sub-task decomposer on both simulation and real-world tasks, demonstrating robustness across diverse settings. Code and more results are available at rdd-neurips.github.io.",
        "translated": "为应对长时域任务，最近的分层视觉-语言-动作（VLA）框架采用基于视觉-语言模型（VLM）的规划器，将复杂的操作任务分解为低级视觉运动策略可轻松处理的简单子任务。通常，VLM 规划器会被微调以学习如何分解目标任务。这种微调需要目标任务的演示数据被分割为子任务，分割方式依赖于人工标注或启发式规则。然而，这些启发式子任务与低级视觉运动策略的训练数据可能存在较大偏差，从而影响任务性能。为解决这些问题，我们提出了一种基于检索的演示分解器（RDD），通过将分解后的子任务区间中的视觉特征与低级视觉运动策略训练数据中的特征对齐，实现演示数据的自动子任务分解。我们的方法在仿真和真实世界任务中均优于最先进的子任务分解器，展现出在不同设置下的鲁棒性。代码和更多结果可在 rdd-neurips.github.io 获取。",
        "translated_title": "RDD：基于检索的演示分解器用于长时域任务中的规划器对齐",
        "label": [],
        "label_reason": "论文聚焦于高层任务规划，与图像像素级恢复无关。",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出基于检索的子任务分解方法，有一定改进但属于常规优化。"
    },
    {
        "title": "ChangingGrounding: 3D Visual Grounding in Changing Scenes",
        "url": "http://arxiv.org/abs/2510.14965v1",
        "pub_date": "2025-10-16",
        "summary": "Real-world robots localize objects from natural-language instructions while scenes around them keep changing. Yet most of the existing 3D visual grounding (3DVG) method still assumes a reconstructed and up-to-date point cloud, an assumption that forces costly re-scans and hinders deployment. We argue that 3DVG should be formulated as an active, memory-driven problem, and we introduce ChangingGrounding, the first benchmark that explicitly measures how well an agent can exploit past observations, explore only where needed, and still deliver precise 3D boxes in changing scenes. To set a strong reference point, we also propose Mem-ChangingGrounder, a zero-shot method for this task that marries cross-modal retrieval with lightweight multi-view fusion: it identifies the object type implied by the query, retrieves relevant memories to guide actions, then explores the target efficiently in the scene, falls back when previous operations are invalid, performs multi-view scanning of the target, and projects the fused evidence from multi-view scans to get accurate object bounding boxes. We evaluate different baselines on ChangingGrounding, and our Mem-ChangingGrounder achieves the highest localization accuracy while greatly reducing exploration cost. We hope this benchmark and method catalyze a shift toward practical, memory-centric 3DVG research for real-world applications. Project page: https://hm123450.github.io/CGB/ .",
        "translated": "现实世界中的机器人需要在周围场景不断变化的情况下，根据自然语言指令定位物体。然而，大多数现有的 3D 视觉定位（3DVG）方法仍然假设存在重建且最新的点云，这一假设迫使进行成本高昂的重新扫描，并阻碍实际部署。我们认为，3DVG 应该被表述为一个主动的、由记忆驱动的问题，因此我们引入了 ChangingGrounding，这是首个明确衡量智能体在变化场景中如何有效利用过往观测结果、仅在必要区域进行探索，并仍能提供精确 3D 边界框的基准。为了提供一个强有力的参考点，我们还提出了 Mem-ChangingGrounder，一种针对该任务的零样本方法，它将跨模态检索与轻量多视角融合相结合：该方法首先识别查询语句所暗示的物体类型，检索相关记忆以指导操作，然后在场景中高效地探索目标，当先前操作无效时进行回退，执行多视角扫描，最后将多视角扫描的融合证据进行投影，以获得精确的物体边界框。我们在 ChangingGrounding 上评估了不同的基线方法，我们的 Mem-ChangingGrounder 在显著降低探索成本的同时实现了最高的定位精度。我们希望这一基准和方法能够推动 3DVG 研究向实用、以记忆为中心的方向转变，以支持现实世界的应用。项目页面：https://hm123450.github.io/CGB/。",
        "translated_title": "ChangingGrounding: 变化场景中的3D视觉定位",
        "label": [],
        "label_reason": "论文聚焦3D视觉定位，属于high-level任务",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "提出新基准和零样本方法，但创新性有限"
    },
    {
        "title": "RainDiff: End-to-end Precipitation Nowcasting Via Token-wise Attention\n  Diffusion",
        "url": "http://arxiv.org/abs/2510.14962v1",
        "pub_date": "2025-10-16",
        "summary": "Precipitation nowcasting, predicting future radar echo sequences from current observations, is a critical yet challenging task due to the inherently chaotic and tightly coupled spatio-temporal dynamics of the atmosphere. While recent advances in diffusion-based models attempt to capture both large-scale motion and fine-grained stochastic variability, they often suffer from scalability issues: latent-space approaches require a separately trained autoencoder, adding complexity and limiting generalization, while pixel-space approaches are computationally intensive and often omit attention mechanisms, reducing their ability to model long-range spatio-temporal dependencies. To address these limitations, we propose a Token-wise Attention integrated into not only the U-Net diffusion model but also the spatio-temporal encoder that dynamically captures multi-scale spatial interactions and temporal evolution. Unlike prior approaches, our method natively integrates attention into the architecture without incurring the high resource cost typical of pixel-space diffusion, thereby eliminating the need for separate latent modules. Our extensive experiments and visual evaluations across diverse datasets demonstrate that the proposed method significantly outperforms state-of-the-art approaches, yielding superior local fidelity, generalization, and robustness in complex precipitation forecasting scenarios.",
        "translated": "降水临近预报，即根据当前观测预测未来的雷达回波序列，是一项关键但极具挑战性的任务，这是由于大气本身的混沌性以及紧密耦合的时空动态特性。尽管基于扩散模型的最新进展尝试捕捉大尺度运动和细粒度的随机变化，但它们通常存在可扩展性问题：隐空间方法需要单独训练的自编码器，增加了模型复杂性并限制了泛化能力，而像素空间方法计算成本高昂，且通常省略注意力机制，削弱了对长程时空依赖关系的建模能力。为了解决这些限制，我们提出了一种将 Token-wise 注意力机制集成到 U-Net 扩散模型以及动态捕捉多尺度空间交互和时间演化过程的时空编码器中的方法。与以往的方法不同，我们的方法在架构中原生地引入注意力机制，而无需承担像素空间扩散通常带来的高资源消耗，从而消除了对单独隐模块的依赖。我们在多个数据集上的广泛实验和可视化评估表明，所提出的方法显著优于最先进的方法，在复杂的降水预测场景中展现出更优越的局部真实性、泛化性和鲁棒性。",
        "translated_title": "RainDiff：通过逐标记注意力的扩散实现端到端降水临近预报",
        "label": [
            "图像去雨",
            "多帧/视频图像恢复"
        ],
        "label_reason": "论文聚焦于雷达降水序列预测，属于多帧图像恢复和去雨任务。",
        "relevance_score": 8,
        "novelty_score": 8,
        "novelty_reason": "提出 Token-wise Attention 结构，改进扩散模型的时空建模能力。"
    },
    {
        "title": "C4D: 4D Made from 3D through Dual Correspondences",
        "url": "http://arxiv.org/abs/2510.14960v1",
        "pub_date": "2025-10-16",
        "summary": "Recovering 4D from monocular video, which jointly estimates dynamic geometry and camera poses, is an inevitably challenging problem. While recent pointmap-based 3D reconstruction methods (e.g., DUSt3R) have made great progress in reconstructing static scenes, directly applying them to dynamic scenes leads to inaccurate results. This discrepancy arises because moving objects violate multi-view geometric constraints, disrupting the reconstruction. To address this, we introduce C4D, a framework that leverages temporal Correspondences to extend existing 3D reconstruction formulation to 4D. Specifically, apart from predicting pointmaps, C4D captures two types of correspondences: short-term optical flow and long-term point tracking. We train a dynamic-aware point tracker that provides additional mobility information, facilitating the estimation of motion masks to separate moving elements from the static background, thus offering more reliable guidance for dynamic scenes. Furthermore, we introduce a set of dynamic scene optimization objectives to recover per-frame 3D geometry and camera parameters. Simultaneously, the correspondences lift 2D trajectories into smooth 3D trajectories, enabling fully integrated 4D reconstruction. Experiments show that our framework achieves complete 4D recovery and demonstrates strong performance across multiple downstream tasks, including depth estimation, camera pose estimation, and point tracking. Project Page: https://littlepure2333.github.io/C4D",
        "translated": "从单目视频中恢复4D，即联合估计动态几何结构和相机姿态，是一个不可避免的具有挑战性的问题。尽管近年来基于点图的3D重建方法（例如DUSt3R）在静态场景重建方面取得了显著进展，但直接将其应用于动态场景会导致不准确的结果。这种差异的产生是由于运动物体违反了多视图几何约束，从而破坏了重建效果。为了解决这一问题，我们提出了C4D，一个利用时间对应关系将现有3D重建方法扩展到4D的框架。具体而言，除了预测点图外，C4D还捕捉两种类型的对应关系：短期光流和长期点跟踪。我们训练了一个动态感知的点跟踪器，提供额外的运动信息，有助于估计运动掩码以将运动元素与静态背景分离，从而为动态场景提供更可靠的引导。此外，我们引入了一组动态场景优化目标，以恢复每帧的3D几何结构和相机参数。同时，这些对应关系将2D轨迹提升为平滑的3D轨迹，实现了完整的4D重建。实验表明，我们的框架能够实现完整的4D恢复，并在多个下游任务中表现出强大的性能，包括深度估计、相机姿态估计和点跟踪。项目页面：https://littlepure2333.github.io/C4D",
        "translated_title": "C4D：通过双对应关系从3D生成4D",
        "label": [],
        "label_reason": "论文主要关注4D重建与动态场景分析，不属于像素级图像恢复或增强任务",
        "relevance_score": 3,
        "novelty_score": 7,
        "novelty_reason": "提出基于双对应关系的动态场景4D重建框架，方法设计新颖"
    },
    {
        "title": "MathCanvas: Intrinsic Visual Chain-of-Thought for Multimodal\n  Mathematical Reasoning",
        "url": "http://arxiv.org/abs/2510.14958v1",
        "pub_date": "2025-10-16",
        "summary": "While Large Language Models (LLMs) have excelled in textual reasoning, they struggle with mathematical domains like geometry that intrinsically rely on visual aids. Existing approaches to Visual Chain-of-Thought (VCoT) are often limited by rigid external tools or fail to generate the high-fidelity, strategically-timed diagrams necessary for complex problem-solving. To bridge this gap, we introduce MathCanvas, a comprehensive framework designed to endow unified Large Multimodal Models (LMMs) with intrinsic VCoT capabilities for mathematics. Our approach consists of two phases. First, a Visual Manipulation stage pre-trains the model on a novel 15.2M-pair corpus, comprising 10M caption-to-diagram pairs (MathCanvas-Imagen) and 5.2M step-by-step editing trajectories (MathCanvas-Edit), to master diagram generation and editing. Second, a Strategic Visual-Aided Reasoning stage fine-tunes the model on MathCanvas-Instruct, a new 219K-example dataset of interleaved visual-textual reasoning paths, teaching it when and how to leverage visual aids. To facilitate rigorous evaluation, we introduce MathCanvas-Bench, a challenging benchmark with 3K problems that require models to produce interleaved visual-textual solutions. Our model, BAGEL-Canvas, trained under this framework, achieves an 86% relative improvement over strong LMM baselines on MathCanvas-Bench, demonstrating excellent generalization to other public math benchmarks. Our work provides a complete toolkit-framework, datasets, and benchmark-to unlock complex, human-like visual-aided reasoning in LMMs. Project Page: https://mathcanvas.github.io/",
        "translated": "虽然大型语言模型（LLMs）在文本推理方面表现出色，但它们在数学领域（如几何学）中表现不佳，因为这些领域本质上依赖于视觉辅助手段。现有的视觉推理链（VCoT）方法通常受到外部工具刚性的限制，或者无法生成在复杂问题求解过程中所需高保真度、策略性时间点的图表。为了解决这一问题，我们提出 MathCanvas，一个全面的框架，旨在为统一的大型多模态模型（LMMs）赋予内在的 VCoT 能力以应对数学问题。我们的方法包括两个阶段。第一阶段是视觉操作（Visual Manipulation），在此阶段，模型在一个全新的 1520 万对语料库上进行预训练，该语料库包括 1000 万对描述到图表的配对（MathCanvas-Imagen）以及 520 万条逐步编辑轨迹（MathCanvas-Edit），从而掌握图表生成和编辑的能力。第二阶段是策略性的视觉辅助推理（Strategic Visual-Aided Reasoning），模型在 MathCanvas-Instruct 上进行微调，这是一个新的 21.9 万个示例的数据集，包含交错的图文推理路径，教会模型何时以及如何利用视觉辅助手段。为了便于严格评估，我们引入了 MathCanvas-Bench 这一具有挑战性的基准，包含 3000 个问题，要求模型生成交错的图文解决方案。在该框架下训练的模型 BAGEL-Canvas 在 MathCanvas-Bench 上相比强大的 LMM 基线模型取得了 86% 的相对提升，显示出在其他公共数学基准上出色的泛化能力。我们的工作提供了一套完整的工具包，包括框架、数据集和基准，以在 LMM 中实现复杂且类似人类的视觉辅助推理。项目页面：https://mathcanvas.github.io/",
        "translated_title": "MathCanvas: 用于多模态数学推理的内在视觉思维链",
        "label": [],
        "label_reason": "论文聚焦多模态数学推理，非图像像素级处理任务",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "提出了多阶段框架和新数据集，但方法基于现有大模型范式"
    },
    {
        "title": "RealDPO: Real or Not Real, that is the Preference",
        "url": "http://arxiv.org/abs/2510.14955v1",
        "pub_date": "2025-10-16",
        "summary": "Video generative models have recently achieved notable advancements in synthesis quality. However, generating complex motions remains a critical challenge, as existing models often struggle to produce natural, smooth, and contextually consistent movements. This gap between generated and real-world motions limits their practical applicability. To address this issue, we introduce RealDPO, a novel alignment paradigm that leverages real-world data as positive samples for preference learning, enabling more accurate motion synthesis. Unlike traditional supervised fine-tuning (SFT), which offers limited corrective feedback, RealDPO employs Direct Preference Optimization (DPO) with a tailored loss function to enhance motion realism. By contrasting real-world videos with erroneous model outputs, RealDPO enables iterative self-correction, progressively refining motion quality. To support post-training in complex motion synthesis, we propose RealAction-5K, a curated dataset of high-quality videos capturing human daily activities with rich and precise motion details. Extensive experiments demonstrate that RealDPO significantly improves video quality, text alignment, and motion realism compared to state-of-the-art models and existing preference optimization techniques.",
        "translated": "近期，视频生成模型在合成质量方面取得了显著进展。然而，生成复杂运动仍然是一个关键挑战，因为现有模型通常难以生成自然、流畅且上下文一致的运动。这种生成运动与真实世界运动之间的差距限制了它们的实际应用性。为了解决这一问题，我们提出 RealDPO，一种新颖的对齐范式，它利用真实世界数据作为偏好学习中的正样本，从而实现更精确的运动合成。与仅能提供有限纠正反馈的传统监督微调（SFT）方法不同，RealDPO 采用直接偏好优化（DPO）并配以定制的损失函数，以提升运动的真实感。通过将真实世界视频与模型错误的输出进行对比，RealDPO 可以实现迭代式的自我修正，逐步优化运动质量。为了支持复杂运动合成的后训练，我们提出了 RealAction-5K，一个经过精心挑选的高质量视频数据集，涵盖人类日常活动，包含丰富且精确的运动细节。大量实验表明，与最先进的模型以及现有的偏好优化技术相比，RealDPO 在视频质量、文本对齐以及运动真实感方面均有显著提升。",
        "translated_title": "RealDPO: 真实还是非真实，这是偏好",
        "label": [],
        "label_reason": "论文聚焦视频生成模型的运动合成，不属于图像恢复/增强任务。",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出新的对齐范式 RealDPO，改进了运动真实感建模方法。"
    },
    {
        "title": "OmniMotion: Multimodal Motion Generation with Continuous Masked\n  Autoregression",
        "url": "http://arxiv.org/abs/2510.14954v1",
        "pub_date": "2025-10-16",
        "summary": "Whole-body multi-modal human motion generation poses two primary challenges: creating an effective motion generation mechanism and integrating various modalities, such as text, speech, and music, into a cohesive framework. Unlike previous methods that usually employ discrete masked modeling or autoregressive modeling, we develop a continuous masked autoregressive motion transformer, where a causal attention is performed considering the sequential nature within the human motion. Within this transformer, we introduce a gated linear attention and an RMSNorm module, which drive the transformer to pay attention to the key actions and suppress the instability caused by either the abnormal movements or the heterogeneous distributions within multi-modalities. To further enhance both the motion generation and the multimodal generalization, we employ the DiT structure to diffuse the conditions from the transformer towards the targets. To fuse different modalities, AdaLN and cross-attention are leveraged to inject the text, speech, and music signals. Experimental results demonstrate that our framework outperforms previous methods across all modalities, including text-to-motion, speech-to-gesture, and music-to-dance. The code of our method will be made public.",
        "translated": "全身多模态人体动作生成面临两个主要挑战：建立一个有效的动作生成机制，以及将各种模态（如文本、语音和音乐）集成到一个统一的框架中。与以往通常采用离散掩码建模或自回归建模的方法不同，我们开发了一个连续掩码自回归运动变换器，在该变换器中，通过考虑人体动作的序列特性，引入了因果注意力机制。在该变换器中，我们引入了门控线性注意力和 RMSNorm 模块，使得模型能够关注关键动作，并抑制由异常动作或多模态中异构分布所引起的不稳定性。为了进一步增强动作生成和多模态泛化能力，我们采用 DiT 结构将条件从变换器扩散到目标。为了融合不同模态，我们利用 AdaLN 和交叉注意力机制将文本、语音和音乐信号注入模型。实验结果表明，我们的框架在所有模态上均优于先前的方法，包括文本到动作、语音到手势和音乐到舞蹈。我们的方法代码将会公开。",
        "translated_title": "OmniMotion: 基于连续掩码自回归的多模态运动生成",
        "label": [],
        "label_reason": "论文关注多模态运动生成，非图像像素级处理任务",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出连续掩码自回归运动生成框架，改进注意力机制和多模态融合方法"
    },
    {
        "title": "From Language to Locomotion: Retargeting-free Humanoid Control via\n  Motion Latent Guidance",
        "url": "http://arxiv.org/abs/2510.14952v1",
        "pub_date": "2025-10-16",
        "summary": "Natural language offers a natural interface for humanoid robots, but existing language-guided humanoid locomotion pipelines remain cumbersome and unreliable. They typically decode human motion, retarget it to robot morphology, and then track it with a physics-based controller. However, this multi-stage process is prone to cumulative errors, introduces high latency, and yields weak coupling between semantics and control. These limitations call for a more direct pathway from language to action, one that eliminates fragile intermediate stages. Therefore, we present RoboGhost, a retargeting-free framework that directly conditions humanoid policies on language-grounded motion latents. By bypassing explicit motion decoding and retargeting, RoboGhost enables a diffusion-based policy to denoise executable actions directly from noise, preserving semantic intent and supporting fast, reactive control. A hybrid causal transformer-diffusion motion generator further ensures long-horizon consistency while maintaining stability and diversity, yielding rich latent representations for precise humanoid behavior. Extensive experiments demonstrate that RoboGhost substantially reduces deployment latency, improves success rates and tracking accuracy, and produces smooth, semantically aligned locomotion on real humanoids. Beyond text, the framework naturally extends to other modalities such as images, audio, and music, providing a general foundation for vision-language-action humanoid systems.",
        "translated": "自然语言为人形机器人提供了自然的交互界面，但现有的语言引导人形机器人运动控制流程仍然复杂且不可靠。它们通常先解码人类运动，将其适配到机器人形态，然后通过基于物理的控制器进行跟踪。然而，这种多阶段流程容易累积误差，引入高延迟，并导致语义与控制之间的耦合较弱。这些限制促使我们寻求从语言到动作的更直接路径，从而消除脆弱的中间阶段。因此，我们提出 RoboGhost，一个无需适配的框架，它直接将人形机器人策略建立在语言引导的运动潜在表示之上。通过绕过显式运动解码和适配，RoboGhost 使基于扩散的策略能够直接从噪声中去噪出可执行动作，保留语义意图，并支持快速、反应式的控制。一个混合因果变压器-扩散运动生成器进一步确保了长期行为的一致性，同时保持稳定性和多样性，生成丰富的潜在表示以实现精确的人形机器人行为。大量实验表明，RoboGhost 显著降低了部署延迟，提高了成功率和跟踪精度，并在真实人形机器人上实现了平滑且语义一致的运动。除了文本之外，该框架自然地扩展到其他模态，如图像、音频和音乐，为人形机器人的视觉-语言-动作系统提供了通用基础。",
        "translated_title": "从语言到运动：基于运动隐空间引导的免重定向人形机器人控制",
        "label": [],
        "label_reason": "论文研究机器人运动控制，不属于图像处理任务",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "提出免重定向框架，但创新点在控制领域而非视觉"
    },
    {
        "title": "DialectGen: Benchmarking and Improving Dialect Robustness in Multimodal\n  Generation",
        "url": "http://arxiv.org/abs/2510.14949v1",
        "pub_date": "2025-10-16",
        "summary": "Contact languages like English exhibit rich regional variations in the form of dialects, which are often used by dialect speakers interacting with generative models. However, can multimodal generative models effectively produce content given dialectal textual input? In this work, we study this question by constructing a new large-scale benchmark spanning six common English dialects. We work with dialect speakers to collect and verify over 4200 unique prompts and evaluate on 17 image and video generative models. Our automatic and human evaluation results show that current state-of-the-art multimodal generative models exhibit 32.26% to 48.17% performance degradation when a single dialect word is used in the prompt. Common mitigation methods such as fine-tuning and prompt rewriting can only improve dialect performance by small margins (&lt; 7%), while potentially incurring significant performance degradation in Standard American English (SAE). To this end, we design a general encoder-based mitigation strategy for multimodal generative models. Our method teaches the model to recognize new dialect features while preserving SAE performance. Experiments on models such as Stable Diffusion 1.5 show that our method is able to simultaneously raise performance on five dialects to be on par with SAE (+34.4%), while incurring near zero cost to SAE performance.",
        "translated": "像英语这样的接触语言表现出丰富的地域变体，即方言，这些方言经常被用于与生成模型进行交互。然而，多模态生成模型是否能够有效处理方言文本输入并生成内容？在本工作中，我们通过构建一个新的大规模基准，覆盖六种常见的英语方言，来研究这一问题。我们与方言使用者合作，收集并验证了4200多个独特的提示词，并在17种图像和视频生成模型上进行了评估。我们的自动评估与人工评估结果表明，当前最先进的多模态生成模型在提示词中仅包含一个方言词汇的情况下，性能退化达到32.26%到48.17%。常见的缓解方法如微调和提示重写仅能带来小幅的性能提升（< 7%），同时可能在标准美式英语（SAE）上造成显著的性能下降。为了解决这一问题，我们设计了一种基于通用编码器的缓解策略，用于多模态生成模型。我们的方法在保留SAE性能的同时，使模型能够识别新的方言特征。在如Stable Diffusion 1.5等模型上的实验表明，我们的方法能够同时提升五种方言的性能，使其与SAE相当（+34.4%），而对SAE性能几乎没有影响。",
        "translated_title": "DialectGen：多模态生成中方言鲁棒性的基准测试与改进",
        "label": [],
        "label_reason": "论文聚焦于多模态生成模型对英语方言的鲁棒性，非图像像素级处理任务。",
        "relevance_score": 1,
        "novelty_score": 3,
        "novelty_reason": "提出针对方言输入的缓解策略，但方法基于通用编码器，创新性较弱。"
    },
    {
        "title": "3D Scene Prompting for Scene-Consistent Camera-Controllable Video\n  Generation",
        "url": "http://arxiv.org/abs/2510.14945v1",
        "pub_date": "2025-10-16",
        "summary": "We present 3DScenePrompt, a framework that generates the next video chunk from arbitrary-length input while enabling precise camera control and preserving scene consistency. Unlike methods conditioned on a single image or a short clip, we employ dual spatio-temporal conditioning that reformulates context-view referencing across the input video. Our approach conditions on both temporally adjacent frames for motion continuity and spatially adjacent content for scene consistency. However, when generating beyond temporal boundaries, directly using spatially adjacent frames would incorrectly preserve dynamic elements from the past. We address this by introducing a 3D scene memory that represents exclusively the static geometry extracted from the entire input video. To construct this memory, we leverage dynamic SLAM with our newly introduced dynamic masking strategy that explicitly separates static scene geometry from moving elements. The static scene representation can then be projected to any target viewpoint, providing geometrically consistent warped views that serve as strong 3D spatial prompts while allowing dynamic regions to evolve naturally from temporal context. This enables our model to maintain long-range spatial coherence and precise camera control without sacrificing computational efficiency or motion realism. Extensive experiments demonstrate that our framework significantly outperforms existing methods in scene consistency, camera controllability, and generation quality. Project page : https://cvlab-kaist.github.io/3DScenePrompt/",
        "translated": "我们提出 3DScenePrompt，这是一种在任意长度的输入基础上生成下一视频片段的框架，同时实现精确的相机控制并保持场景一致性。与依赖于单张图像或短片段的方法不同，我们采用双时空条件化策略，对输入视频中的上下文视图参考进行重新建模。我们的方法在时间相邻帧上进行条件化以保证运动连续性，在空间相邻内容上进行条件化以保持场景一致性。然而，在生成超出时间边界的内容时，直接使用空间相邻帧会导致过去动态元素被错误保留。我们通过引入一种3D场景记忆来解决这一问题，该记忆仅表示从整个输入视频中提取的静态几何结构。为构建这种记忆，我们结合动态SLAM与我们新提出的动态掩码策略，从而显式地将静态场景几何与动态元素分离。静态场景表示可以投影到任意目标视角，提供几何一致的视图，作为强有力的3D空间提示，同时允许动态区域从时间上下文中自然演变。这使得我们的模型在保持长期空间一致性和精确相机控制的同时，不会牺牲计算效率或运动的真实性。大量实验表明，我们的框架在场景一致性、相机可控性和生成质量方面显著优于现有方法。项目页面：https://cvlab-kaist.github.io/3DScenePrompt/",
        "translated_title": "3D场景提示用于场景一致的相机可控视频生成",
        "label": [],
        "label_reason": "论文聚焦于视频生成与相机控制，不属于低级图像处理任务。",
        "relevance_score": 2,
        "novelty_score": 8,
        "novelty_reason": "提出3D场景记忆和动态掩码策略，为视频生成提供新方法。"
    },
    {
        "title": "MaskCaptioner : Learning to Jointly Segment and Caption Object\n  Trajectories in Videos",
        "url": "http://arxiv.org/abs/2510.14904v1",
        "pub_date": "2025-10-16",
        "summary": "Dense Video Object Captioning (DVOC) is the task of jointly detecting, tracking, and captioning object trajectories in a video, requiring the ability to understand spatio-temporal details and describe them in natural language. Due to the complexity of the task and the high cost associated with manual annotation, previous approaches resort to disjoint training strategies, potentially leading to suboptimal performance. To circumvent this issue, we propose to generate captions about spatio-temporally localized entities leveraging a state-of-the-art VLM. By extending the LVIS and LV-VIS datasets with our synthetic captions (LVISCap and LV-VISCap), we train MaskCaptioner, an end-to-end model capable of jointly detecting, segmenting, tracking and captioning object trajectories. Moreover, with pretraining on LVISCap and LV-VISCap, MaskCaptioner achieves state-of-the-art DVOC results on three existing benchmarks, VidSTG, VLN and BenSMOT. The datasets and code are available at https://www.gabriel.fiastre.fr/maskcaptioner/.",
        "translated": "密集视频目标字幕生成（Dense Video Object Captioning，DVOC）是一项在视频中同时检测、跟踪并为目标轨迹生成字幕的任务，要求系统能够理解时空细节，并用自然语言加以描述。由于任务本身的复杂性和人工标注的高昂成本，以往的方法往往采用分阶段训练策略，可能导致性能次优。为了解决这一问题，我们提出利用最先进的视觉语言模型（VLM）生成关于时空定位实体的字幕。通过在LVIS和LV-VIS数据集上扩展我们合成的字幕（LVISCap和LV-VISCap），我们训练了一个端到端模型MaskCaptioner，能够同时完成目标的检测、分割、跟踪和字幕生成任务。此外，在对LVISCap和LV-VISCap进行预训练后，MaskCaptioner在三个现有的基准测试VidSTG、VLN和BenSMOT上取得了最先进的DVOC结果。数据集和代码可通过 https://www.gabriel.fiastre.fr/maskcaptioner/ 获得。",
        "translated_title": "MaskCaptioner：学习在视频中联合分割和描述物体轨迹",
        "label": [],
        "label_reason": "论文聚焦视频对象分割与描述，属于 high-level 任务",
        "relevance_score": 2,
        "novelty_score": 5,
        "novelty_reason": "方法结合 VLM 进行视频对象描述，有一定迁移创新"
    },
    {
        "title": "Leveraging Multimodal LLM Descriptions of Activity for Explainable\n  Semi-Supervised Video Anomaly Detection",
        "url": "http://arxiv.org/abs/2510.14896v1",
        "pub_date": "2025-10-16",
        "summary": "Existing semi-supervised video anomaly detection (VAD) methods often struggle with detecting complex anomalies involving object interactions and generally lack explainability. To overcome these limitations, we propose a novel VAD framework leveraging Multimodal Large Language Models (MLLMs). Unlike previous MLLM-based approaches that make direct anomaly judgments at the frame level, our method focuses on extracting and interpreting object activity and interactions over time. By querying an MLLM with visual inputs of object pairs at different moments, we generate textual descriptions of the activity and interactions from nominal videos. These textual descriptions serve as a high-level representation of the activity and interactions of objects in a video. They are used to detect anomalies during test time by comparing them to textual descriptions found in nominal training videos. Our approach inherently provides explainability and can be combined with many traditional VAD methods to further enhance their interpretability. Extensive experiments on benchmark datasets demonstrate that our method not only detects complex interaction-based anomalies effectively but also achieves state-of-the-art performance on datasets without interaction anomalies.",
        "translated": "现有的半监督视频异常检测（VAD）方法在检测涉及物体交互的复杂异常时通常表现不佳，且普遍缺乏可解释性。为克服这些限制，我们提出了一种新颖的 VAD 框架，该框架利用多模态大语言模型（MLLMs）。与之前基于 MLLM 的方法在帧级别直接进行异常判断不同，我们的方法聚焦于提取和解释视频中物体活动及其随时间变化的交互。通过在不同时间点对物体对的视觉输入查询 MLLM，我们生成了来自正常视频中活动和交互的文本描述。这些文本描述作为视频中物体活动和交互的高层表示，在测试阶段通过将其与正常训练视频中的文本描述进行比较，用于检测异常。我们的方法本质上提供了可解释性，并可与许多传统 VAD 方法结合，以进一步提升其可解释性。在基准数据集上的大量实验表明，我们的方法不仅能够有效检测基于交互的复杂异常，而且在无交互异常的数据集上也达到了最先进的性能。",
        "translated_title": "利用多模态大语言模型对活动的描述实现可解释的半监督视频异常检测",
        "label": [],
        "label_reason": "论文聚焦视频异常检测，属于 high-level 任务，不涉及像素级图像质量改善。",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出基于 MLLM 的新框架，提升 VAD 的可解释性，具备一定创新性。"
    },
    {
        "title": "OmniVinci: Enhancing Architecture and Data for Omni-Modal Understanding\n  LLM",
        "url": "http://arxiv.org/abs/2510.15870v1",
        "pub_date": "2025-10-17",
        "summary": "Advancing machine intelligence requires developing the ability to perceive across multiple modalities, much as humans sense the world. We introduce OmniVinci, an initiative to build a strong, open-source, omni-modal LLM. We carefully study the design choices across model architecture and data curation. For model architecture, we present three key innovations: (i) OmniAlignNet for strengthening alignment between vision and audio embeddings in a shared omni-modal latent space; (ii) Temporal Embedding Grouping for capturing relative temporal alignment between vision and audio signals; and (iii) Constrained Rotary Time Embedding for encoding absolute temporal information in omni-modal embeddings. We introduce a curation and synthesis pipeline that generates 24M single-modal and omni-modal conversations. We find that modalities reinforce one another in both perception and reasoning. Our model, OmniVinci, outperforms Qwen2.5-Omni with +19.05 on DailyOmni (cross-modal understanding), +1.7 on MMAR (audio), and +3.9 on Video-MME (vision), while using just 0.2T training tokens - a 6 times reduction compared to Qwen2.5-Omni's 1.2T. We finally demonstrate omni-modal advantages in downstream applications spanning robotics, medical AI, and smart factory.",
        "translated": "提升机器智能需要发展跨多种模态的感知能力，正如人类感知世界一样。我们提出了OmniVinci，一个旨在构建强大、开源、全模态大语言模型的项目。我们对模型架构和数据收集方面进行了细致的研究。在模型架构方面，我们提出了三项关键创新：(i) OmniAlignNet，用于在共享的全模态潜在空间中加强视觉和音频嵌入之间的对齐；(ii) Temporal Embedding Grouping，用于捕捉视觉和音频信号之间的相对时序对齐；(iii) Constrained Rotary Time Embedding，用于在全模态嵌入中编码绝对时序信息。我们引入了一个数据收集与合成流水线，生成了2400万条单模态和全模态的对话。我们发现，不同模态在感知和推理方面能够相互增强。我们的模型OmniVinci在使用仅0.2T训练数据量的情况下，在DailyOmni（跨模态理解）上优于Qwen2.5-Omni 19.05分，在MMAR（音频）上高1.7分，在Video-MME（视觉）上高3.9分，这比Qwen2.5-Omni所使用的1.2T训练数据量减少了6倍。最后，我们在下游应用中展示了全模态的优势，涵盖了机器人、医疗AI和智能工厂等多个领域。",
        "translated_title": "OmniVinci：增强架构与数据以实现全模态理解",
        "label": [],
        "label_reason": "论文聚焦多模态LLM，未涉及像素级图像处理任务",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出了多模态对齐和时间嵌入的改进方法"
    },
    {
        "title": "Skyfall-GS: Synthesizing Immersive 3D Urban Scenes from Satellite\n  Imagery",
        "url": "http://arxiv.org/abs/2510.15869v1",
        "pub_date": "2025-10-17",
        "summary": "Synthesizing large-scale, explorable, and geometrically accurate 3D urban scenes is a challenging yet valuable task in providing immersive and embodied applications. The challenges lie in the lack of large-scale and high-quality real-world 3D scans for training generalizable generative models. In this paper, we take an alternative route to create large-scale 3D scenes by synergizing the readily available satellite imagery that supplies realistic coarse geometry and the open-domain diffusion model for creating high-quality close-up appearances. We propose \\textbf{Skyfall-GS}, the first city-block scale 3D scene creation framework without costly 3D annotations, also featuring real-time, immersive 3D exploration. We tailor a curriculum-driven iterative refinement strategy to progressively enhance geometric completeness and photorealistic textures. Extensive experiments demonstrate that Skyfall-GS provides improved cross-view consistent geometry and more realistic textures compared to state-of-the-art approaches. Project page: https://skyfall-gs.jayinnn.dev/",
        "translated": "合成大规模、可探索且几何准确的3D城市场景是一项具有挑战性但又极具价值的任务，可为沉浸式和具身化应用提供支持。该任务的主要挑战在于缺乏用于训练通用生成模型的大规模高质量真实3D扫描数据。在本文中，我们采用了一种替代方法，通过结合现成的卫星图像（提供逼真的粗略几何结构）与开放域扩散模型（用于生成高质量的近距离外观），来创建大规模3D场景。我们提出了 \\textbf{Skyfall-GS}，这是首个无需高昂成本的3D标注即可实现城市街区尺度3D场景生成的框架，同时具备实时、沉浸式的3D探索功能。我们定制了一种基于课程学习的迭代优化策略，以逐步提升几何完整性与照片级真实纹理。大量实验表明，Skyfall-GS 相比于最先进的方法，在多视角几何一致性与纹理逼真度方面均有明显提升。项目主页：https://skyfall-gs.jayinnn.dev/",
        "translated_title": "Skyfall-GS：从卫星图像合成沉浸式三维城市场景",
        "label": [],
        "label_reason": "论文聚焦3D场景生成，非图像像素级恢复或增强任务",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出基于卫星图像和扩散模型的3D场景生成框架，有一定技术迁移性"
    },
    {
        "title": "LightsOut: Diffusion-based Outpainting for Enhanced Lens Flare Removal",
        "url": "http://arxiv.org/abs/2510.15868v1",
        "pub_date": "2025-10-17",
        "summary": "Lens flare significantly degrades image quality, impacting critical computer vision tasks like object detection and autonomous driving. Recent Single Image Flare Removal (SIFR) methods perform poorly when off-frame light sources are incomplete or absent. We propose LightsOut, a diffusion-based outpainting framework tailored to enhance SIFR by reconstructing off-frame light sources. Our method leverages a multitask regression module and LoRA fine-tuned diffusion model to ensure realistic and physically consistent outpainting results. Comprehensive experiments demonstrate LightsOut consistently boosts the performance of existing SIFR methods across challenging scenarios without additional retraining, serving as a universally applicable plug-and-play preprocessing solution. Project page: https://ray-1026.github.io/lightsout/",
        "translated": "镜头眩光会显著降低图像质量，影响物体检测和自动驾驶等关键的计算机视觉任务。近期的单图像眩光去除（Single Image Flare Removal, SIFR）方法在处理帧外光源不完整或缺失时效果较差。我们提出了 LightsOut，一种基于扩散模型的图像扩展框架，旨在通过重建帧外光源来增强 SIFR 的性能。我们的方法结合了一个多任务回归模块和经过 LoRA 微调的扩散模型，以确保生成结果在视觉真实性和物理一致性方面都具备高质量。大量实验表明，LightsOut 能够在各种具有挑战性的场景中，无需额外的再训练，显著提升现有 SIFR 方法的性能，成为一种通用的即插即用预处理解决方案。项目页面：https://ray-1026.github.io/lightsout/",
        "translated_title": "LightsOut：基于扩散的外推绘画用于增强的镜头眩光去除",
        "label": [
            "图像去反射",
            "图像恢复"
        ],
        "label_reason": "论文聚焦镜头光晕去除，涉及像素级图像恢复",
        "relevance_score": 8,
        "novelty_score": 7,
        "novelty_reason": "结合扩散模型与外延生成，提出新颖的光晕修复方案"
    },
    {
        "title": "BiomedXPro: Prompt Optimization for Explainable Diagnosis with\n  Biomedical Vision Language Models",
        "url": "http://arxiv.org/abs/2510.15866v1",
        "pub_date": "2025-10-17",
        "summary": "The clinical adoption of biomedical vision-language models is hindered by prompt optimization techniques that produce either uninterpretable latent vectors or single textual prompts. This lack of transparency and failure to capture the multi-faceted nature of clinical diagnosis, which relies on integrating diverse observations, limits their trustworthiness in high-stakes settings. To address this, we introduce BiomedXPro, an evolutionary framework that leverages a large language model as both a biomedical knowledge extractor and an adaptive optimizer to automatically generate a diverse ensemble of interpretable, natural-language prompt pairs for disease diagnosis. Experiments on multiple biomedical benchmarks show that BiomedXPro consistently outperforms state-of-the-art prompt-tuning methods, particularly in data-scarce few-shot settings. Furthermore, our analysis demonstrates a strong semantic alignment between the discovered prompts and statistically significant clinical features, grounding the model's performance in verifiable concepts. By producing a diverse ensemble of interpretable prompts, BiomedXPro provides a verifiable basis for model predictions, representing a critical step toward the development of more trustworthy and clinically-aligned AI systems.",
        "translated": "生物医学视觉-语言模型在临床中的应用受到提示优化技术的限制，这些技术生成的提示要么是不可解释的潜在向量，要么是单一的文本提示。这种缺乏透明性以及未能捕捉临床诊断的多方面特性——而临床诊断依赖于整合多种观察结果——限制了其在高风险场景中的可信度。为了解决这一问题，我们引入了 BiomedXPro，这是一种进化框架，利用大型语言模型作为生物医学知识提取器和自适应优化器，能够自动生成多样化的、可解释的自然语言提示对，用于疾病诊断。在多个生物医学基准数据集上的实验表明，BiomedXPro 在性能上始终优于最先进的提示调优方法，尤其是在数据稀缺的小样本设置中。此外，我们的分析表明，所发现的提示与统计上显著的临床特征之间存在强烈的语义一致性，从而将模型的性能建立在可验证的概念基础上。通过生成多样化且可解释的提示集合，BiomedXPro 为模型预测提供了可验证的依据，代表着开发更加可信且与临床对齐的 AI 系统的重要一步。",
        "translated_title": "BiomedXPro：用于可解释诊断的生物医学视觉语言模型提示优化",
        "label": [],
        "label_reason": "论文聚焦于生物医学视觉语言模型的提示优化，属于高阶诊断任务。",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "结合大语言模型进行提示优化，在提示工程上有一定创新。"
    },
    {
        "title": "BLIP3o-NEXT: Next Frontier of Native Image Generation",
        "url": "http://arxiv.org/abs/2510.15857v1",
        "pub_date": "2025-10-17",
        "summary": "We present BLIP3o-NEXT, a fully open-source foundation model in the BLIP3 series that advances the next frontier of native image generation. BLIP3o-NEXT unifies text-to-image generation and image editing within a single architecture, demonstrating strong image generation and image editing capabilities. In developing the state-of-the-art native image generation model, we identify four key insights: (1) Most architectural choices yield comparable performance; an architecture can be deemed effective provided it scales efficiently and supports fast inference; (2) The successful application of reinforcement learning can further push the frontier of native image generation; (3) Image editing still remains a challenging task, yet instruction following and the consistency between generated and reference images can be significantly enhanced through post-training and data engine; (4) Data quality and scale continue to be decisive factors that determine the upper bound of model performance. Building upon these insights, BLIP3o-NEXT leverages an Autoregressive + Diffusion architecture in which an autoregressive model first generates discrete image tokens conditioned on multimodal inputs, whose hidden states are then used as conditioning signals for a diffusion model to generate high-fidelity images. This architecture integrates the reasoning strength and instruction following of autoregressive models with the fine-detail rendering ability of diffusion models, achieving a new level of coherence and realism. Extensive evaluations of various text-to-image and image-editing benchmarks show that BLIP3o-NEXT achieves superior performance over existing models.",
        "translated": "我们提出了 BLIP3o-NEXT，这是 BLIP3 系列中一个完全开源的基础模型，推动了原生图像生成的下一个前沿方向。BLIP3o-NEXT 在单一架构中统一了文本到图像生成和图像编辑任务，展示了强大的图像生成和编辑能力。在开发先进的原生图像生成模型过程中，我们总结出四个关键见解：(1) 大多数架构选择在性能上是相近的；只要架构能够高效扩展并支持快速推理，就可以认为其是有效的；(2) 强化学习的成功应用可以进一步推动原生图像生成的前沿；(3) 图像编辑仍然是一个具有挑战性的任务，但通过后训练和数据引擎，指令跟随能力以及生成图像与参考图像之间的一致性可以显著提升；(4) 数据质量和数据规模仍然是决定模型性能上限的关键因素。基于这些见解，BLIP3o-NEXT 采用了一个 Autoregressive + Diffusion 架构：首先，一个自回归模型根据多模态输入生成离散的图像 token，然后将这些 token 的隐藏状态作为扩散模型的条件信号，以生成高质量图像。该架构结合了自回归模型的推理能力和指令跟随能力，以及扩散模型在细节渲染方面的优势，实现了生成图像在一致性和真实感方面的新水平。在多个文本到图像和图像编辑基准上的广泛评估表明，BLIP3o-NEXT 在现有模型中表现出优越的性能。",
        "translated_title": "BLIP3o-NEXT：原生图像生成的下一个前沿",
        "label": [],
        "label_reason": "论文关注图像生成而非像素级质量修复，不属于low-level图像处理",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出新的生成架构，结合自回归与扩散模型，有一定创新性"
    },
    {
        "title": "Memory-SAM: Human-Prompt-Free Tongue Segmentation via\n  Retrieval-to-Prompt",
        "url": "http://arxiv.org/abs/2510.15849v1",
        "pub_date": "2025-10-17",
        "summary": "Accurate tongue segmentation is crucial for reliable TCM analysis. Supervised models require large annotated datasets, while SAM-family models remain prompt-driven. We present Memory-SAM, a training-free, human-prompt-free pipeline that automatically generates effective prompts from a small memory of prior cases via dense DINOv3 features and FAISS retrieval. Given a query image, mask-constrained correspondences to the retrieved exemplar are distilled into foreground/background point prompts that guide SAM2 without manual clicks or model fine-tuning. We evaluate on 600 expert-annotated images (300 controlled, 300 in-the-wild). On the mixed test split, Memory-SAM achieves mIoU 0.9863, surpassing FCN (0.8188) and a detector-to-box SAM baseline (0.1839). On controlled data, ceiling effects above 0.98 make small differences less meaningful given annotation variability, while our method shows clear gains under real-world conditions. Results indicate that retrieval-to-prompt enables data-efficient, robust segmentation of irregular boundaries in tongue imaging. The code is publicly available at https://github.com/jw-chae/memory-sam.",
        "translated": "精确的舌象分割对于可靠的中医分析至关重要。监督模型需要大规模的标注数据集，而 SAM 系列模型仍然依赖提示驱动。我们提出 Memory-SAM，这是一种无需训练、无需人工提示的流程，它通过密集的 DINOv3 特征和 FAISS 检索，从少量先验案例构成的记忆中自动生成有效的提示。给定一张查询图像，将检索到的样例中受掩码约束的对应关系提炼为前景/背景点提示，以引导 SAM2 进行分割，而无需人工点击或模型微调。我们在 600 张专家标注的图像上进行评估（300 张受控图像，300 张真实场景图像）。在混合测试集上，Memory-SAM 实现了 0.9863 的 mIoU，优于 FCN（0.8188）和基于检测框的 SAM 基线（0.1839）。在受控数据中，由于标注的可变性，高于 0.98 的天花板效应使得微小差异变得不显著，而我们的方法在真实场景条件下表现出明显的性能提升。结果表明，检索到提示的机制能够实现舌象图像中不规则边界的数据高效且鲁棒的分割。代码公开在 https://github.com/jw-chae/memory-sam。",
        "translated_title": "Memory-SAM：基于检索到提示的无需人工提示的舌部分割",
        "label": [
            "图像分割",
            "医学图像增强"
        ],
        "label_reason": "方法用于医学图像分割，但非直接像素级质量复原",
        "relevance_score": 5,
        "novelty_score": 8,
        "novelty_reason": "提出无需人工提示的自动提示生成新方法"
    },
    {
        "title": "3DPR: Single Image 3D Portrait Relight using Generative Priors",
        "url": "http://arxiv.org/abs/2510.15846v1",
        "pub_date": "2025-10-17",
        "summary": "Rendering novel, relit views of a human head, given a monocular portrait image as input, is an inherently underconstrained problem. The traditional graphics solution is to explicitly decompose the input image into geometry, material and lighting via differentiable rendering; but this is constrained by the multiple assumptions and approximations of the underlying models and parameterizations of these scene components. We propose 3DPR, an image-based relighting model that leverages generative priors learnt from multi-view One-Light-at-A-Time (OLAT) images captured in a light stage. We introduce a new diverse and large-scale multi-view 4K OLAT dataset of 139 subjects to learn a high-quality prior over the distribution of high-frequency face reflectance. We leverage the latent space of a pre-trained generative head model that provides a rich prior over face geometry learnt from in-the-wild image datasets. The input portrait is first embedded in the latent manifold of such a model through an encoder-based inversion process. Then a novel triplane-based reflectance network trained on our lightstage data is used to synthesize high-fidelity OLAT images to enable image-based relighting. Our reflectance network operates in the latent space of the generative head model, crucially enabling a relatively small number of lightstage images to train the reflectance model. Combining the generated OLATs according to a given HDRI environment maps yields physically accurate environmental relighting results. Through quantitative and qualitative evaluations, we demonstrate that 3DPR outperforms previous methods, particularly in preserving identity and in capturing lighting effects such as specularities, self-shadows, and subsurface scattering. Project Page: https://vcai.mpi-inf.mpg.de/projects/3dpr/",
        "translated": "基于单目人像图像渲染新颖、重新光照的人头视图本身是一个固有欠约束的问题。传统的图形学解决方案是通过可微分渲染显式地将输入图像分解为几何、材质和光照；但该方法受限于基础模型和这些场景组件参数化过程中的多个假设和近似。我们提出 3DPR，一种基于图像的重新光照模型，利用从光场中捕获的多视角 One-Light-at-A-Time (OLAT) 图像学习的生成先验。我们引入一个新的多样化且大规模的多视角 4K OLAT 数据集，包含 139 个主体，以学习高频人脸反射分布的高质量先验。我们利用一个预训练的生成人头模型的潜在空间，该模型提供了从真实图像数据集中学习的丰富人脸几何先验。首先通过基于编码器的逆过程将输入人像嵌入到该模型的潜在流形中。然后利用一种基于三平面的反射网络（在我们采集的光场数据上训练），合成高保真的 OLAT 图像，以实现基于图像的重新光照。我们的反射网络在生成人头模型的潜在空间中进行操作，关键在于能够利用相对较少的光场图像训练反射模型。根据给定的 HDRI 环境图结合生成的 OLAT 图像，可以实现物理准确的环境光照效果。通过定量和定性评估，我们证明了 3DPR 在保持身份信息和捕捉诸如高光、自阴影和次表面散射等光照效果方面优于先前的方法。项目页面：https://vcai.mpi-inf.mpg.de/projects/3dpr/",
        "translated_title": "3DPR：使用生成先验的单图像3D人像重光照",
        "label": [],
        "label_reason": "论文属于3D渲染和生成任务，非图像像素级质量恢复或增强。",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出基于生成先验的3D人脸重光照方法，但属于常规生成模型的应用扩展。"
    },
    {
        "title": "Paper2Web: Let's Make Your Paper Alive!",
        "url": "http://arxiv.org/abs/2510.15842v1",
        "pub_date": "2025-10-17",
        "summary": "Academic project websites can more effectively disseminate research when they clearly present core content and enable intuitive navigation and interaction. However, current approaches such as direct Large Language Model (LLM) generation, templates, or direct HTML conversion struggle to produce layout-aware, interactive sites, and a comprehensive evaluation suite for this task has been lacking. In this paper, we introduce Paper2Web, a benchmark dataset and multi-dimensional evaluation framework for assessing academic webpage generation. It incorporates rule-based metrics like Connectivity, Completeness and human-verified LLM-as-a-Judge (covering interactivity, aesthetics, and informativeness), and PaperQuiz, which measures paper-level knowledge retention. We further present PWAgent, an autonomous pipeline that converts scientific papers into interactive and multimedia-rich academic homepages. The agent iteratively refines both content and layout through MCP tools that enhance emphasis, balance, and presentation quality. Our experiments show that PWAgent consistently outperforms end-to-end baselines like template-based webpages and arXiv/alphaXiv versions by a large margin while maintaining low cost, achieving the Pareto-front in academic webpage generation.",
        "translated": "学术项目网站在清晰呈现核心内容并实现直观导航与交互时，能够更有效地传播研究成果。然而，当前的方法如直接使用大型语言模型（LLM）生成、模板化或直接HTML转换，难以生成具有布局感知能力且交互性强的网站，且在此任务上缺乏全面的评估体系。本文中，我们提出了Paper2Web，这是一个用于评估学术网页生成的基准数据集和多维度评估框架。该框架集成了基于规则的指标，如连通性（Connectivity）、完整性（Completeness），以及由人类验证的LLM作为评判者（LLM-as-a-Judge），涵盖交互性、美观性和信息性等方面，同时还引入了PaperQuiz，用于衡量论文级别的知识保留程度。我们进一步提出了PWAgent，一个自主的流水线工具，能够将科研论文转换为交互性强、多媒体丰富的学术主页。该智能体通过MCP工具迭代优化内容与布局，提升重点突出性、视觉平衡性和展示质量。实验结果表明，PWAgent在保持低成本的同时，显著优于基于模板的网页和arXiv/alphaXiv版本等端到端基线方法，在学术网页生成任务中达到了帕累托前沿。",
        "translated_title": "Paper2Web: 让你的论文动起来！",
        "label": [],
        "label_reason": "论文不属于low-level图像处理，主要关注网页生成。",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "提出新框架和工具，但创新点集中于网页交互设计而非图像处理。"
    },
    {
        "title": "Neuro-Symbolic Spatial Reasoning in Segmentation",
        "url": "http://arxiv.org/abs/2510.15841v1",
        "pub_date": "2025-10-17",
        "summary": "Open-Vocabulary Semantic Segmentation (OVSS) assigns pixel-level labels from an open set of categories, requiring generalization to unseen and unlabelled objects. Using vision-language models (VLMs) to correlate local image patches with potential unseen object categories suffers from a lack of understanding of spatial relations of objects in a scene. To solve this problem, we introduce neuro-symbolic (NeSy) spatial reasoning in OVSS. In contrast to contemporary VLM correlation-based approaches, we propose Relational Segmentor (RelateSeg) to impose explicit spatial relational constraints by first order logic (FOL) formulated in a neural network architecture. This is the first attempt to explore NeSy spatial reasoning in OVSS. Specifically, RelateSeg automatically extracts spatial relations, e.g., &lt;cat, to-right-of, person&gt;, and encodes them as first-order logic formulas using our proposed pseudo categories. Each pixel learns to predict both a semantic category (e.g., \"cat\") and a spatial pseudo category (e.g., \"right of person\") simultaneously, enforcing relational constraints (e.g., a \"cat\" pixel must lie to the right of a \"person\"). Finally, these logic constraints are formulated in a deep network architecture by fuzzy logic relaxation, enabling end-to-end learning of spatial-relationally consistent segmentation. RelateSeg achieves state-of-the-art performance in terms of average mIoU across four benchmark datasets and particularly shows clear advantages on images containing multiple categories, with the cost of only introducing a single auxiliary loss function and no additional parameters, validating the effectiveness of NeSy spatial reasoning in OVSS.",
        "translated": "开放词汇语义分割（OVSS）从开放的类别集中为每个像素分配标签，要求对未见过且未标记的对象具备泛化能力。使用视觉-语言模型（VLMs）将局部图像块与潜在的未知对象类别相关联，往往难以理解场景中对象的空间关系。为了解决这一问题，我们在OVSS中引入了神经符号（NeSy）空间推理。不同于当前基于VLM相关性的方法，我们提出Relational Segmentor（RelateSeg），通过在神经网络架构中以一阶逻辑（FOL）的形式施加显式空间关系约束来实现推理。这是首次尝试在OVSS中探索NeSy空间推理。具体而言，RelateSeg会自动提取空间关系，例如 $<cat, to-right-of, person>$，并使用我们提出的伪类别将其编码为一阶逻辑公式。每个像素同时学习预测一个语义类别（如\"cat\"）和一个空间伪类别（如\"right of person\"），从而强制实施关系约束（如\"cat\"像素必须位于\"person\"像素的右侧）。最终，这些逻辑约束通过模糊逻辑松弛技术整合到深度网络架构中，实现空间关系一致的端到端分割学习。RelateSeg在四个基准数据集上的平均mIoU指标上达到了最先进的性能，尤其在包含多个类别的图像上表现出明显优势，仅引入了一个辅助损失函数且无需额外参数，验证了NeSy空间推理在OVSS中的有效性。",
        "translated_title": "神经符号化空间推理在分割中的应用",
        "label": [],
        "label_reason": "论文聚焦语义分割，属于high-level任务",
        "relevance_score": 3,
        "novelty_score": 7,
        "novelty_reason": "首次将神经符号推理引入分割任务，具有新颖性"
    },
    {
        "title": "VISTA: A Test-Time Self-Improving Video Generation Agent",
        "url": "http://arxiv.org/abs/2510.15831v1",
        "pub_date": "2025-10-17",
        "summary": "Despite rapid advances in text-to-video synthesis, generated video quality remains critically dependent on precise user prompts. Existing test-time optimization methods, successful in other domains, struggle with the multi-faceted nature of video. In this work, we introduce VISTA (Video Iterative Self-improvemenT Agent), a novel multi-agent system that autonomously improves video generation through refining prompts in an iterative loop. VISTA first decomposes a user idea into a structured temporal plan. After generation, the best video is identified through a robust pairwise tournament. This winning video is then critiqued by a trio of specialized agents focusing on visual, audio, and contextual fidelity. Finally, a reasoning agent synthesizes this feedback to introspectively rewrite and enhance the prompt for the next generation cycle. Experiments on single- and multi-scene video generation scenarios show that while prior methods yield inconsistent gains, VISTA consistently improves video quality and alignment with user intent, achieving up to 60% pairwise win rate against state-of-the-art baselines. Human evaluators concur, preferring VISTA outputs in 66.4% of comparisons.",
        "translated": "尽管文本到视频合成领域取得了快速进展，但生成的视频质量在很大程度上仍依赖于用户提供的精确提示。现有在测试时进行优化的方法虽然在其他领域取得了成功，但在处理视频的多维度特性时却面临挑战。在本研究中，我们提出 VISTA（Video Iterative Self-improvemenT Agent），一种新颖的多智能体系统，通过迭代循环优化提示，自主提升视频生成质量。VISTA 首先将用户的创意想法分解为结构化的时序计划。在生成视频之后，通过一个鲁棒的两两比赛机制确定最佳视频。随后，一个由三个专业智能体组成的小组对该胜出视频进行评估，分别关注视觉、音频和上下文的保真度。最后，一个推理智能体综合这些反馈，自主地重写并增强下一轮生成所用的提示。在单场景和多场景视频生成任务中的实验表明，尽管现有方法的性能提升并不一致，VISTA 则始终能够提升视频质量和与用户意图的对齐程度，在与最先进基线方法的两两比较中，最高可达 60% 的胜率。人类评估者也一致认可 VISTA 的输出，在 66.4% 的比较中更倾向于选择其结果。",
        "translated_title": "VISTA：一种运行时自我改进的视频生成智能体",
        "label": [],
        "label_reason": "论文聚焦视频生成而非像素级图像恢复或增强",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出了多智能体系统进行迭代视频优化，具有一定创新性"
    },
    {
        "title": "ERNet: Efficient Non-Rigid Registration Network for Point Sequences",
        "url": "http://arxiv.org/abs/2510.15800v1",
        "pub_date": "2025-10-17",
        "summary": "Registering an object shape to a sequence of point clouds undergoing non-rigid deformation is a long-standing challenge. The key difficulties stem from two factors: (i) the presence of local minima due to the non-convexity of registration objectives, especially under noisy or partial inputs, which hinders accurate and robust deformation estimation, and (ii) error accumulation over long sequences, leading to tracking failures. To address these challenges, we introduce to adopt a scalable data-driven approach and propose ERNet, an efficient feed-forward model trained on large deformation datasets. It is designed to handle noisy and partial inputs while effectively leveraging temporal information for accurate and consistent sequential registration. The key to our design is predicting a sequence of deformation graphs through a two-stage pipeline, which first estimates frame-wise coarse graph nodes for robust initialization, before refining their trajectories over time in a sliding-window fashion. Extensive experiments show that our proposed approach (i) outperforms previous state-of-the-art on both the DeformingThings4D and D-FAUST datasets, and (ii) achieves more than 4x speedup compared to the previous best, offering significant efficiency improvement.",
        "translated": "将一个物体形状配准到经历非刚性变形的一系列点云中是一个长期存在的挑战。其关键难点来源于两个因素：(i) 由于配准目标函数的非凸性，特别是在噪声或部分输入的情况下，容易出现局部极小值，这阻碍了准确且鲁棒的变形估计；以及 (ii) 在长序列中误差不断累积，导致跟踪失败。为了解决这些挑战，我们提出采用一种可扩展的数据驱动方法，并设计了 ERNet，一个在大变形数据集上训练的高效前馈模型。该模型旨在处理噪声和部分输入的同时，有效利用时间信息，实现准确且一致的序列配准。我们设计的关键在于通过一个两阶段的流程预测一系列变形图：首先估计每一帧的粗略图节点以实现鲁棒的初始化，然后在滑动窗口的框架下逐时地优化这些节点的轨迹。大量实验表明，我们提出的方法 (i) 在 DeformingThings4D 和 D-FAUST 数据集上均优于之前的最先进方法，以及 (ii) 相比之前最佳方法的效率提升了超过 4 倍，具有显著的速度优势。",
        "translated_title": "ERNet：用于点序列的高效非刚性配准网络",
        "label": [],
        "label_reason": "论文研究点云配准，不属于图像处理任务。",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "提出两阶段变形图预测方法，但为常规技术组合。"
    },
    {
        "title": "ReCon: Region-Controllable Data Augmentation with Rectification and\n  Alignment for Object Detection",
        "url": "http://arxiv.org/abs/2510.15783v1",
        "pub_date": "2025-10-17",
        "summary": "The scale and quality of datasets are crucial for training robust perception models. However, obtaining large-scale annotated data is both costly and time-consuming. Generative models have emerged as a powerful tool for data augmentation by synthesizing samples that adhere to desired distributions. However, current generative approaches often rely on complex post-processing or extensive fine-tuning on massive datasets to achieve satisfactory results, and they remain prone to content-position mismatches and semantic leakage. To overcome these limitations, we introduce ReCon, a novel augmentation framework that enhances the capacity of structure-controllable generative models for object detection. ReCon integrates region-guided rectification into the diffusion sampling process, using feedback from a pre-trained perception model to rectify misgenerated regions within diffusion sampling process. We further propose region-aligned cross-attention to enforce spatial-semantic alignment between image regions and their textual cues, thereby improving both semantic consistency and overall image fidelity. Extensive experiments demonstrate that ReCon substantially improve the quality and trainability of generated data, achieving consistent performance gains across various datasets, backbone architectures, and data scales. Our code is available at https://github.com/haoweiz23/ReCon .",
        "translated": "数据集的规模和质量对于训练鲁棒的感知模型至关重要。然而，获取大规模标注数据既昂贵又耗时。生成模型通过合成符合目标分布的样本，已成为数据增强的强大工具。然而，当前的生成方法通常依赖于复杂的后处理或对大规模数据集进行大量微调才能获得令人满意的结果，并且仍然容易出现内容-位置不匹配和语义泄露的问题。为克服这些限制，我们引入了 ReCon，一种新颖的增强框架，旨在提升结构可控生成模型在目标检测中的能力。ReCon 将区域引导的修正机制整合到扩散采样过程中，利用预训练感知模型的反馈来修正扩散采样过程中的错误生成区域。我们进一步提出了区域对齐的交叉注意力机制，以强制图像区域与文本提示之间在空间语义上的一致性，从而提升语义一致性以及整体图像保真度。大量实验表明，ReCon 显著提高了生成数据的质量和可训练性，在不同数据集、主干网络结构和数据规模上均实现了性能的稳定提升。我们的代码可在 https://github.com/haoweiz23/ReCon 获取。",
        "translated_title": "ReCon：带校正与对齐的区域可控数据增强方法用于目标检测",
        "label": [],
        "label_reason": "论文聚焦于数据增强，用于目标检测，属于 high-level 任务。",
        "relevance_score": 3,
        "novelty_score": 7,
        "novelty_reason": "提出区域对齐交叉注意力和区域引导修正，有一定创新性。"
    },
    {
        "title": "Controlling the image generation process with parametric activation\n  functions",
        "url": "http://arxiv.org/abs/2510.15778v1",
        "pub_date": "2025-10-17",
        "summary": "As image generative models continue to increase not only in their fidelity but also in their ubiquity the development of tools that leverage direct interaction with their internal mechanisms in an interpretable way has received little attention In this work we introduce a system that allows users to develop a better understanding of the model through interaction and experimentation By giving users the ability to replace activation functions of a generative network with parametric ones and a way to set the parameters of these functions we introduce an alternative approach to control the networks output We demonstrate the use of our method on StyleGAN2 and BigGAN networks trained on FFHQ and ImageNet respectively.",
        "translated": "随着图像生成模型在真实感和普及性方面持续提升，能够以可解释的方式利用与模型内部机制直接交互的工具的发展却未受到足够关注。在本工作中，我们引入了一个系统，使用户通过交互和实验更好地理解模型。通过赋予用户将生成网络的激活函数替换为参数化激活函数的能力，并提供设置这些函数参数的方法，我们提出了一种控制网络输出的替代方案。我们分别在基于 FFHQ 和 ImageNet 训练的 StyleGAN2 与 BigGAN 网络上展示了该方法的应用。",
        "translated_title": "通过参数化激活函数控制图像生成过程",
        "label": [],
        "label_reason": "论文聚焦图像生成而非恢复/增强，不属于低级图像处理",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出可参数化激活函数交互方法，有一定技术迁移潜力"
    },
    {
        "title": "SANR: Scene-Aware Neural Representation for Light Field Image\n  Compression with Rate-Distortion Optimization",
        "url": "http://arxiv.org/abs/2510.15775v1",
        "pub_date": "2025-10-17",
        "summary": "Light field images capture multi-view scene information and play a crucial role in 3D scene reconstruction. However, their high-dimensional nature results in enormous data volumes, posing a significant challenge for efficient compression in practical storage and transmission scenarios. Although neural representation-based methods have shown promise in light field image compression, most approaches rely on direct coordinate-to-pixel mapping through implicit neural representation (INR), often neglecting the explicit modeling of scene structure. Moreover, they typically lack end-to-end rate-distortion optimization, limiting their compression efficiency. To address these limitations, we propose SANR, a Scene-Aware Neural Representation framework for light field image compression with end-to-end rate-distortion optimization. For scene awareness, SANR introduces a hierarchical scene modeling block that leverages multi-scale latent codes to capture intrinsic scene structures, thereby reducing the information gap between INR input coordinates and the target light field image. From a compression perspective, SANR is the first to incorporate entropy-constrained quantization-aware training (QAT) into neural representation-based light field image compression, enabling end-to-end rate-distortion optimization. Extensive experiment results demonstrate that SANR significantly outperforms state-of-the-art techniques regarding rate-distortion performance with a 65.62\\% BD-rate saving against HEVC.",
        "translated": "光场图像捕获多视角场景信息，在三维场景重建中起着至关重要的作用。然而，其高维特性导致数据量巨大，在实际存储和传输场景中对高效压缩提出了重大挑战。尽管基于神经表示的方法在光场图像压缩中展现出前景，但大多数方法依赖于通过隐式神经表示（INR）进行的直接坐标到像素的映射，常常忽略了对场景结构的显式建模。此外，它们通常缺乏端到端的率失真优化，限制了压缩效率。为了解决这些限制，我们提出SANR，一种用于光场图像压缩的场景感知神经表示框架，具备端到端的率失真优化。为了实现场景感知，SANR引入了一个层次化的场景建模模块，利用多尺度潜在码捕获场景的内在结构，从而减少INR输入坐标与目标光场图像之间的信息差距。从压缩的角度来看，SANR是首个将熵约束的量化感知训练（QAT）引入基于神经表示的光场图像压缩的方法，实现了端到端的率失真优化。大量实验结果表明，SANR在率失真性能方面显著优于最先进的技术，相比HEVC在BD-rate上节省了65.62\\%。",
        "translated_title": "SANR：基于率失真优化的场景感知神经表示用于光场图像压缩",
        "label": [
            "多帧/视频图像恢复"
        ],
        "label_reason": "论文涉及光场图像压缩，属于多帧图像处理，与low-level相关。",
        "relevance_score": 8,
        "novelty_score": 7,
        "novelty_reason": "提出结合熵约束量化感知训练的新方法，改进压缩效率。"
    },
    {
        "title": "Towards more holistic interpretability: A lightweight disentangled\n  Concept Bottleneck Model",
        "url": "http://arxiv.org/abs/2510.15770v1",
        "pub_date": "2025-10-17",
        "summary": "Concept Bottleneck Models (CBMs) enhance interpretability by predicting human-understandable concepts as intermediate representations. However, existing CBMs often suffer from input-to-concept mapping bias and limited controllability, which restricts their practical value, directly damage the responsibility of strategy from concept-based methods. We propose a lightweight Disentangled Concept Bottleneck Model (LDCBM) that automatically groups visual features into semantically meaningful components without region annotation. By introducing a filter grouping loss and joint concept supervision, our method improves the alignment between visual patterns and concepts, enabling more transparent and robust decision-making. Notably, Experiments on three diverse datasets demonstrate that LDCBM achieves higher concept and class accuracy, outperforming previous CBMs in both interpretability and classification performance. By grounding concepts in visual evidence, our method overcomes a fundamental limitation of prior models and enhances the reliability of interpretable AI.",
        "translated": "概念瓶颈模型（CBMs）通过预测人类可理解的概念作为中间表示来增强模型的可解释性。然而，现有的 CBMs 往往存在输入到概念映射的偏差以及可控性有限的问题，这限制了它们的实用价值，并直接损害了基于概念方法策略的责任性。我们提出了一种轻量级的解耦概念瓶颈模型（LDCBM），该模型无需区域标注即可自动将视觉特征分组为语义上有意义的组件。通过引入滤波器分组损失和联合概念监督，我们的方法提升了视觉模式与概念之间的对齐程度，从而实现更加透明和稳健的决策。值得注意的是，三个多样化数据集上的实验表明，LDCBM 在概念和类别准确率方面均高于现有方法，在可解释性与分类性能上均优于以往的 CBMs。通过将概念与视觉证据相联系，我们的方法克服了先前模型的基本局限，并提高了可解释人工智能的可靠性。",
        "translated_title": "迈向更全面的可解释性：一种轻量级的解耦概念瓶颈模型",
        "label": [],
        "label_reason": "论文聚焦模型可解释性，非像素级图像处理任务。",
        "relevance_score": 3,
        "novelty_score": 7,
        "novelty_reason": "提出轻量解耦概念瓶颈模型，改进了CBM的可控性和概念对齐。"
    },
    {
        "title": "QSilk: Micrograin Stabilization and Adaptive Quantile Clipping for\n  Detail-Friendly Latent Diffusion",
        "url": "http://arxiv.org/abs/2510.15761v1",
        "pub_date": "2025-10-17",
        "summary": "We present QSilk, a lightweight, always-on stabilization layer for latent diffusion that improves high-frequency fidelity while suppressing rare activation spikes. QSilk combines (i) a per-sample micro clamp that gently limits extreme values without washing out texture, and (ii) Adaptive Quantile Clip (AQClip), which adapts the allowed value corridor per region. AQClip can operate in a proxy mode using local structure statistics or in an attention entropy guided mode (model confidence). Integrated into the CADE 2.5 rendering pipeline, QSilk yields cleaner, sharper results at low step counts and ultra-high resolutions with negligible overhead. It requires no training or fine-tuning and exposes minimal user controls. We report consistent qualitative improvements across SD/SDXL backbones and show synergy with CFG/Rescale, enabling slightly higher guidance without artifacts.",
        "translated": "我们提出了 QSilk，一种用于潜在扩散的轻量级、始终启用的稳定层，它在提高高频保真度的同时抑制罕见的激活尖峰。QSilk 结合了以下两个部分：(i) 每个样本的微小钳制机制，该机制温和地限制极端值而不模糊纹理；(ii) 自适应分位数裁剪（AQClip），它根据每个区域调整允许的数值通道。AQClip 可以通过局部结构统计信息运行在代理模式下，也可以通过注意力熵引导模式（模型置信度）运行。将 QSilk 集成到 CADE 2.5 渲染流程中后，即使在较低的采样步数和超高分辨率下，也能获得更干净、更清晰的结果，且几乎没有额外开销。QSilk 不需要训练或微调，并且用户可调节的参数极少。我们在 SD/SDXL 的主干模型上均观察到一致的定性提升，并展示了其与 CFG/Rescale 的协同效应，使得在不产生伪影的情况下可略微提高引导强度。",
        "translated_title": "QSilk：面向细节的潜在扩散的微晶粒稳定与自适应分位数裁剪",
        "label": [],
        "label_reason": "不直接处理像素级图像质量，属于生成模型的优化",
        "relevance_score": 3,
        "novelty_score": 6,
        "novelty_reason": "提出轻量稳定层和自适应裁剪方法，有一定创新"
    },
    {
        "title": "Poultry Farm Intelligence: An Integrated Multi-Sensor AI Platform for\n  Enhanced Welfare and Productivity",
        "url": "http://arxiv.org/abs/2510.15757v1",
        "pub_date": "2025-10-17",
        "summary": "Poultry farming faces increasing pressure to meet productivity targets while ensuring animal welfare and environmental compliance. Yet many small and medium-sized farms lack affordable, integrated tools for continuous monitoring and decision-making, relying instead on manual, reactive inspections. This paper presents Poultry Farm Intelligence (PoultryFI) - a modular, cost-effective platform that integrates six AI-powered modules: Camera Placement Optimizer, Audio-Visual Monitoring, Analytics &amp; Alerting, Real-Time Egg Counting, Production &amp; Profitability Forecasting, and a Recommendation Module.   Camera layouts are first optimized offline using evolutionary algorithms for full poultry house coverage with minimal hardware. The Audio-Visual Monitoring module extracts welfare indicators from synchronized video, audio, and feeding data. Analytics &amp; Alerting produces daily summaries and real-time notifications, while Real-Time Egg Counting uses an edge vision model to automate production tracking. Forecasting models predict egg yield and feed consumption up to 10 days in advance, and the Recommendation Module integrates forecasts with weather data to guide environmental and operational adjustments.   This is among the first systems to combine low-cost sensing, edge analytics, and prescriptive AI to continuously monitor flocks, predict production, and optimize performance. Field trials demonstrate 100% egg-count accuracy on Raspberry Pi 5, robust anomaly detection, and reliable short-term forecasting. PoultryFI bridges the gap between isolated pilot tools and scalable, farm-wide intelligence, empowering producers to proactively safeguard welfare and profitability.",
        "translated": "禽类养殖面临日益增长的压力，需要在提高生产效率的同时保障动物福利并满足环境合规要求。然而，许多中小型农场缺乏经济、集成化的工具来进行连续监控和决策，通常依赖人工的、被动的检查方式。本文提出了 Poultry Farm Intelligence（PoultryFI）——一个模块化、低成本的平台，集成了六个基于人工智能的模块：摄像头布局优化器、音视频监控、分析与预警、实时蛋数统计、生产与盈利能力预测以及推荐模块。\n\n首先使用进化算法在离线环境下对摄像头布局进行优化，以在使用最少硬件设备的情况下实现禽舍的全面覆盖。音视频监控模块从同步的视频、音频和喂食数据中提取福利指标。分析与预警模块生成每日摘要并发出实时通知，而实时蛋数统计模块则采用边缘视觉模型实现生产情况的自动化跟踪。预测模型可提前10天预测产蛋量和饲料消耗，推荐模块则将预测结果与天气数据整合，用于指导环境和操作的调整。\n\n该系统是首批结合低成本传感、边缘分析和处方式人工智能的系统之一，能够持续监控禽群、预测生产情况并优化整体性能。实地试验表明，在 Raspberry Pi 5 上实现蛋数统计的准确率达到100%，异常检测稳健，短期预测可靠。PoultryFI 搭起了孤立试点工具与可扩展、全场智能化之间的桥梁，使养殖者能够主动保障动物福利和盈利能力。",
        "translated_title": "禽类养殖智能化：一种集成多传感器的AI平台，提升福利与生产效率",
        "label": [],
        "label_reason": "论文主要关注多传感器AI平台在养鸡场的应用，不属于图像恢复或增强任务。",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出了模块化低成本AI平台，但属于常规系统集成创新。"
    },
    {
        "title": "Semantic segmentation with coarse annotations",
        "url": "http://arxiv.org/abs/2510.15756v1",
        "pub_date": "2025-10-17",
        "summary": "Semantic segmentation is the task of classifying each pixel in an image. Training a segmentation model achieves best results using annotated images, where each pixel is annotated with the corresponding class. When obtaining fine annotations is difficult or expensive, it may be possible to acquire coarse annotations, e.g. by roughly annotating pixels in an images leaving some pixels around the boundaries between classes unlabeled. Segmentation with coarse annotations is difficult, in particular when the objective is to optimize the alignment of boundaries between classes. This paper proposes a regularization method for models with an encoder-decoder architecture with superpixel based upsampling. It encourages the segmented pixels in the decoded image to be SLIC-superpixels, which are based on pixel color and position, independent of the segmentation annotation. The method is applied to FCN-16 fully convolutional network architecture and evaluated on the SUIM, Cityscapes, and PanNuke data sets. It is shown that the boundary recall improves significantly compared to state-of-the-art models when trained on coarse annotations.",
        "translated": "语义分割的任务是对图像中的每个像素进行分类。使用带有标注的图像训练分割模型可以取得最佳效果，其中每个像素都标注了对应的类别。当获取精细标注困难或成本较高时，可以尝试获取粗略标注，例如通过大致标注图像中的像素，而将类别边界附近的一些像素留作未标注。使用粗略标注进行分割是具有挑战性的，特别是当目标是优化类别之间边界的对齐时。本文提出了一种针对具有编码器-解码器架构、并采用超像素上采样的模型的正则化方法。该方法促使解码图像中的分割像素属于基于像素颜色和位置的SLIC超像素，且不依赖于分割标注。该方法被应用于FCN-16全卷积网络架构，并在SUIM、Cityscapes和PanNuke数据集上进行了评估。结果表明，与最先进的模型相比，在使用粗标注训练时，该方法能够显著提高边界召回率。",
        "translated_title": "带有粗略标注的语义分割",
        "label": [],
        "label_reason": "论文属于语义分割，目标是场景理解，非像素级图像质量恢复",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出基于超像素的正则化方法，对模型训练有一定改进"
    },
    {
        "title": "NDM: A Noise-driven Detection and Mitigation Framework against Implicit\n  Sexual Intentions in Text-to-Image Generation",
        "url": "http://arxiv.org/abs/2510.15752v1",
        "pub_date": "2025-10-17",
        "summary": "Despite the impressive generative capabilities of text-to-image (T2I) diffusion models, they remain vulnerable to generating inappropriate content, especially when confronted with implicit sexual prompts. Unlike explicit harmful prompts, these subtle cues, often disguised as seemingly benign terms, can unexpectedly trigger sexual content due to underlying model biases, raising significant ethical concerns. However, existing detection methods are primarily designed to identify explicit sexual content and therefore struggle to detect these implicit cues. Fine-tuning approaches, while effective to some extent, risk degrading the model's generative quality, creating an undesirable trade-off. To address this, we propose NDM, the first noise-driven detection and mitigation framework, which could detect and mitigate implicit malicious intention in T2I generation while preserving the model's original generative capabilities. Specifically, we introduce two key innovations: first, we leverage the separability of early-stage predicted noise to develop a noise-based detection method that could identify malicious content with high accuracy and efficiency; second, we propose a noise-enhanced adaptive negative guidance mechanism that could optimize the initial noise by suppressing the prominent region's attention, thereby enhancing the effectiveness of adaptive negative guidance for sexual mitigation. Experimentally, we validate NDM on both natural and adversarial datasets, demonstrating its superior performance over existing SOTA methods, including SLD, UCE, and RECE, etc. Code and resources are available at https://github.com/lorraine021/NDM.",
        "translated": "尽管文本到图像（T2I）扩散模型展现出令人印象深刻的内容生成能力，它们仍易受到生成不适当内容的影响，尤其是在面对隐含的性暗示提示时。与显式的有害提示不同，这些微妙的提示通常伪装成看似无害的词汇，由于模型内部存在的偏见，可能意外触发与性相关的内容，从而引发重大的伦理问题。然而，现有的检测方法主要设计用于识别显式的色情内容，因此难以检测这些隐含提示。尽管微调方法在某种程度上是有效的，但它们可能会降低模型的生成质量，造成令人不满意的性能与质量之间的权衡。为了解决这一问题，我们提出了NDM，第一个基于噪声驱动的内容检测和缓解框架，可以在保持模型原有生成能力的同时，检测并缓解T2I生成过程中的隐性恶意意图。具体而言，我们引入了两个关键创新：首先，我们利用早期预测噪声的可分性，开发了一种基于噪声的检测方法，能够以高精度和高效率识别恶意内容；其次，我们提出了一种噪声增强的自适应负向引导机制，该机制通过抑制显著区域的注意力来优化初始噪声，从而提高自适应负向引导在缓解性内容方面的有效性。实验上，我们在自然和对抗数据集上验证了NDM，结果表明其性能优于现有最先进的方法，包括SLD、UCE和RECE等。代码和资源可在 https://github.com/lorraine021/NDM 获取。",
        "translated_title": "NDM：一种基于噪声驱动的检测与缓解框架，用于应对文本到图像生成中的隐式性意图",
        "label": [],
        "label_reason": "论文聚焦文本到图像生成中的内容检测，不属于图像像素级处理任务",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出了首个基于噪声驱动的检测与缓解框架，具有一定创新性"
    },
    {
        "title": "SEGA: A Stepwise Evolution Paradigm for Content-Aware Layout Generation\n  with Design Prior",
        "url": "http://arxiv.org/abs/2510.15749v1",
        "pub_date": "2025-10-17",
        "summary": "In this paper, we study the content-aware layout generation problem, which aims to automatically generate layouts that are harmonious with a given background image. Existing methods usually deal with this task with a single-step reasoning framework. The lack of a feedback-based self-correction mechanism leads to their failure rates significantly increasing when faced with complex element layout planning. To address this challenge, we introduce SEGA, a novel Stepwise Evolution Paradigm for Content-Aware Layout Generation. Inspired by the systematic mode of human thinking, SEGA employs a hierarchical reasoning framework with a coarse-to-fine strategy: first, a coarse-level module roughly estimates the layout planning results; then, another refining module performs fine-level reasoning regarding the coarse planning results. Furthermore, we incorporate layout design principles as prior knowledge into the model to enhance its layout planning ability. Besides, we present GenPoster-100K that is a new large-scale poster dataset with rich meta-information annotation. The experiments demonstrate the effectiveness of our approach by achieving the state-of-the-art results on multiple benchmark datasets. Our project page is at: https://brucew91.github.io/SEGA.github.io/",
        "translated": "本文研究了内容感知的布局生成问题，旨在自动生成与给定背景图像和谐一致的布局。现有方法通常采用单步推理框架处理该任务。由于缺乏基于反馈的自修正机制，当面对复杂的元素布局规划时，其失败率显著上升。为了解决这一挑战，我们引入了 SEGA，一种新颖的内容感知布局生成的逐步演化范式。受人类系统性思维模式的启发，SEGA 采用分层推理框架和从粗到细的策略：首先，一个粗粒度模块大致估计布局规划结果；然后，另一个优化模块对粗粒度规划结果进行细粒度推理。此外，我们将布局设计原则作为先验知识引入模型，以提升其布局规划能力。同时，我们还提出了 GenPoster-100K，这是一个新的大规模海报数据集，包含丰富的元信息标注。实验表明，我们的方法在多个基准数据集上取得了最先进的结果，验证了其有效性。我们的项目页面为：https://brucew91.github.io/SEGA.github.io/",
        "translated_title": "SEGA：一种基于设计先验的内容感知布局生成的逐步演化范式",
        "label": [],
        "label_reason": "论文研究布局生成，属于high-level任务",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出分步演化范式SEGA，改进布局生成方法"
    },
    {
        "title": "FACE: A General Framework for Mapping Collaborative Filtering Embeddings\n  into LLM Tokens",
        "url": "http://arxiv.org/abs/2510.15729v1",
        "pub_date": "2025-10-17",
        "summary": "Recently, large language models (LLMs) have been explored for integration with collaborative filtering (CF)-based recommendation systems, which are crucial for personalizing user experiences. However, a key challenge is that LLMs struggle to interpret the latent, non-semantic embeddings produced by CF approaches, limiting recommendation effectiveness and further applications. To address this, we propose FACE, a general interpretable framework that maps CF embeddings into pre-trained LLM tokens. Specifically, we introduce a disentangled projection module to decompose CF embeddings into concept-specific vectors, followed by a quantized autoencoder to convert continuous embeddings into LLM tokens (descriptors). Then, we design a contrastive alignment objective to ensure that the tokens align with corresponding textual signals. Hence, the model-agnostic FACE framework achieves semantic alignment without fine-tuning LLMs and enhances recommendation performance by leveraging their pre-trained capabilities. Empirical results on three real-world recommendation datasets demonstrate performance improvements in benchmark models, with interpretability studies confirming the interpretability of the descriptors. Code is available in https://github.com/YixinRoll/FACE.",
        "translated": "近期，研究者探索将大语言模型（LLMs）与基于协同过滤（CF）的推荐系统结合，以提升用户个性化体验。然而，一个关键挑战在于LLMs难以解析CF方法产生的潜在且非语义的嵌入，这限制了推荐效果及其进一步应用。为解决这一问题，我们提出FACE，一种通用的可解释框架，将CF嵌入映射到预训练LLM的token中。具体来说，我们引入了一个解耦投影模块，将CF嵌入分解为概念特定的向量，并通过一个量化自编码器将连续嵌入转换为LLM token（描述符）。随后，我们设计了一个对比对齐目标，以确保token与对应的文本信号对齐。因此，该模型无关的FACE框架在不微调LLMs的情况下实现了语义对齐，并通过利用其预训练能力提升了推荐性能。在三个真实推荐数据集上的实验结果表明，基准模型的性能得到了提升，可解释性研究也验证了描述符的可解释性。代码可在https://github.com/YixinRoll/FACE获取。",
        "translated_title": "FACE：一种通用框架，用于将协同过滤嵌入映射到大语言模型（LLM）的token中",
        "label": [
            "LLM生成式推荐",
            "通用推荐技术"
        ],
        "label_reason": "将协同过滤嵌入映射为LLM token，提升推荐语义对齐与性能",
        "relevance_score": 9,
        "novelty_score": 8,
        "novelty_reason": "提出解耦投影和量化自编码器的新方法，实现LLM与CF的创新结合"
    },
    {
        "title": "The 3rd Place Solution of CCIR CUP 2025: A Framework for\n  Retrieval-Augmented Generation in Multi-Turn Legal Conversation",
        "url": "http://arxiv.org/abs/2510.15722v1",
        "pub_date": "2025-10-17",
        "summary": "Retrieval-Augmented Generation has made significant progress in the field of natural language processing. By combining the advantages of information retrieval and large language models, RAG can generate relevant and contextually appropriate responses based on items retrieved from reliable sources. This technology has demonstrated outstanding performance across multiple domains, but its application in the legal field remains in its exploratory phase. In this paper, we introduce our approach for \"Legal Knowledge Retrieval and Generation\" in CCIR CUP 2025, which leverages large language models and information retrieval systems to provide responses based on laws in response to user questions.",
        "translated": "检索增强生成（Retrieval-Augmented Generation, RAG）在自然语言处理领域取得了显著进展。通过结合信息检索与大语言模型的优势，RAG可以根据从可靠来源检索到的项目生成相关且符合上下文的回答。该技术在多个领域中展示了卓越的性能，但其在法律领域的应用仍处于探索阶段。在本文中，我们介绍了我们在CCIR CUP 2025中“法律知识检索与生成”任务中的方法，该方法利用大语言模型和信息检索系统，根据法律法规对用户问题提供回答。",
        "translated_title": "2025年CCIR CUP的第三名解决方案：一种用于多轮法律对话的检索增强生成框架",
        "label": [
            "LLM生成式推荐",
            "多模态推荐"
        ],
        "label_reason": "论文涉及基于检索增强生成的推荐方法，适用于法律领域的多轮对话生成。",
        "relevance_score": 7,
        "novelty_score": 6,
        "novelty_reason": "在RAG基础上提出法律对话生成框架，有一定创新但属于领域适配改进。"
    },
    {
        "title": "Cost-Aware Retrieval-Augmentation Reasoning Models with Adaptive\n  Retrieval Depth",
        "url": "http://arxiv.org/abs/2510.15719v1",
        "pub_date": "2025-10-17",
        "summary": "Reasoning models have gained significant attention due to their strong performance, particularly when enhanced with retrieval augmentation. However, these models often incur high computational costs, as both retrieval and reasoning tokens contribute substantially to the overall resource usage. In this work, we make the following contributions: (1) we propose a retrieval-augmented reasoning model that dynamically adjusts the length of the retrieved document list based on the query and retrieval results; (2) we develop a cost-aware advantage function for training of efficient retrieval-augmented reasoning models through reinforcement learning; and (3) we explore both memory- and latency-bound implementations of the proposed cost-aware framework for both proximal and group relative policy optimization algorithms. We evaluate our approach on seven public question answering datasets and demonstrate significant efficiency gains, without compromising effectiveness. In fact, we observed that the model latency decreases by ~16-20% across datasets, while its effectiveness increases by ~5% on average, in terms of exact match.",
        "translated": "由于其出色的表现，推理模型近年来受到了广泛关注，尤其是在引入检索增强后效果更为显著。然而，这些模型通常带来较高的计算成本，因为检索和推理过程中的 tokens 都会显著增加整体资源消耗。在本文中，我们做出以下贡献：(1) 我们提出了一种检索增强的推理模型，该模型能够根据查询和检索结果动态调整检索文档列表的长度；(2) 我们开发了一种成本感知的优势函数，通过强化学习训练高效的检索增强推理模型；(3) 我们探讨了所提出成本感知框架在近端策略优化和组相对策略优化算法中的内存受限和延迟受限实现。我们在七个公开的问题回答数据集上评估了我们的方法，并展示了在不损害效果的前提下显著提升的效率。事实上，我们观察到，模型的延迟在各个数据集上平均降低了 ~16-20%，而其在精确匹配方面的效果平均提高了 ~5%。",
        "translated_title": "成本感知的召回增强推理模型与自适应召回深度",
        "label": [
            "召回（Recall）",
            "通用推荐技术（General Recommendation Techniques）"
        ],
        "label_reason": "论文涉及检索增强推理，与推荐系统的召回阶段相关。",
        "relevance_score": 5,
        "novelty_score": 7,
        "novelty_reason": "提出动态调整检索长度和成本感知训练方法，有一定创新性。"
    },
    {
        "title": "GraphMind: Interactive Novelty Assessment System for Accelerating\n  Scientific Discovery",
        "url": "http://arxiv.org/abs/2510.15706v1",
        "pub_date": "2025-10-17",
        "summary": "Large Language Models (LLMs) show strong reasoning and text generation capabilities, prompting their use in scientific literature analysis, including novelty assessment. While evaluating novelty of scientific papers is crucial for peer review, it requires extensive knowledge of related work, something not all reviewers have. While recent work on LLM-assisted scientific literature analysis supports literature comparison, existing approaches offer limited transparency and lack mechanisms for result traceability via an information retrieval module. To address this gap, we introduce $\\textbf{GraphMind}$, an easy-to-use interactive web tool designed to assist users in evaluating the novelty of scientific papers or drafted ideas. Specially, $\\textbf{GraphMind}$ enables users to capture the main structure of a scientific paper, explore related ideas through various perspectives, and assess novelty via providing verifiable contextual insights. $\\textbf{GraphMind}$ enables users to annotate key elements of a paper, explore related papers through various relationships, and assess novelty with contextual insight. This tool integrates external APIs such as arXiv and Semantic Scholar with LLMs to support annotation, extraction, retrieval and classification of papers. This combination provides users with a rich, structured view of a scientific idea's core contributions and its connections to existing work. $\\textbf{GraphMind}$ is available at https://oyarsa.github.io/graphmind and a demonstration video at https://youtu.be/wKbjQpSvwJg. The source code is available at https://github.com/oyarsa/graphmind.",
        "translated": "大语言模型（LLMs）展现出强大的推理和文本生成能力，促使它们被用于科学文献分析，包括新颖性评估。尽管评估科学论文的新颖性对于同行评审至关重要，但这需要对相关工作有广泛的知识，而并非所有评审者都具备这一条件。虽然最近关于LLM辅助科学文献分析的研究支持文献对比，但现有方法在透明度方面存在较大局限，且缺乏通过信息检索模块实现结果可追溯的机制。为了解决这一问题，我们引入了 $\\textbf{GraphMind}$，一款易于使用的交互式网络工具，旨在协助用户评估科学论文或初稿想法的新颖性。具体而言，$\\textbf{GraphMind}$ 使用户能够捕捉科学论文的主要结构，通过多种视角探索相关观点，并通过提供可验证的上下文洞察评估新颖性。$\\textbf{GraphMind}$ 支持用户标注论文的关键要素，通过多种关系探索相关论文，并结合上下文信息评估新颖性。该工具将外部 API（如 arXiv 和 Semantic Scholar）与大语言模型（LLMs）相结合，以支持论文的标注、提取、检索和分类。这种组合为用户提供了对科学观点核心贡献及其与已有工作关联的丰富结构化视图。$\\textbf{GraphMind}$ 可在 https://oyarsa.github.io/graphmind 获取，演示视频可在 https://youtu.be/wKbjQpSvwJg 查看。源代码可在 https://github.com/oyarsa/graphmind 获得。",
        "translated_title": "GraphMind: 交互式新颖性评估系统，用于加速科学发现",
        "label": [],
        "label_reason": "论文主要关注科学论文新颖性评估，不直接涉及推荐系统核心技术。",
        "relevance_score": 3,
        "novelty_score": 7,
        "novelty_reason": "提出结合LLM与信息检索的新工具，解决科学文献新颖性评估中的透明性问题。"
    },
    {
        "title": "Mixture of Experts Approaches in Dense Retrieval Tasks",
        "url": "http://arxiv.org/abs/2510.15683v1",
        "pub_date": "2025-10-17",
        "summary": "Dense Retrieval Models (DRMs) are a prominent development in Information Retrieval (IR). A key challenge with these neural Transformer-based models is that they often struggle to generalize beyond the specific tasks and domains they were trained on. To address this challenge, prior research in IR incorporated the Mixture-of-Experts (MoE) framework within each Transformer layer of a DRM, which, though effective, substantially increased the number of additional parameters. In this paper, we propose a more efficient design, which introduces a single MoE block (SB-MoE) after the final Transformer layer. To assess the retrieval effectiveness of SB-MoE, we perform an empirical evaluation across three IR tasks. Our experiments involve two evaluation setups, aiming to assess both in-domain effectiveness and the model's zero-shot generalizability. In the first setup, we fine-tune SB-MoE with four different underlying DRMs on seven IR benchmarks and evaluate them on their respective test sets. In the second setup, we fine-tune SB-MoE on MSMARCO and perform zero-shot evaluation on thirteen BEIR datasets. Additionally, we perform further experiments to analyze the model's dependency on its hyperparameters (i.e., the number of employed and activated experts) and investigate how this variation affects SB-MoE's performance. The obtained results show that SB-MoE is particularly effective for DRMs with lightweight base models, such as TinyBERT and BERT-Small, consistently exceeding standard model fine-tuning across benchmarks. For DRMs with more parameters, such as BERT-Base and Contriever, our model requires a larger number of training samples to achieve improved retrieval performance. Our code is available online at: https://github.com/FaySokli/SB-MoE.",
        "translated": "稠密检索模型（DRMs）是信息检索（IR）领域的重要进展。基于神经网络Transformer的这些模型面临的一个关键挑战是，它们通常难以泛化到训练时特定任务和领域之外。为了解决这一挑战，以往的IR研究在每个Transformer层中引入了专家混合（MoE）框架，虽然这种方法有效，但却显著增加了额外的参数数量。在本文中，我们提出了一种更高效的结构设计，即在最终的Transformer层之后引入一个单一的MoE模块（SB-MoE）。为了评估SB-MoE的检索效果，我们在三个IR任务上进行了实证评估。我们的实验包含两种评估设置，旨在分别评估模型的在域性能和零样本泛化能力。在第一种设置中，我们在七个IR基准上使用四个不同的基础DRMs对SB-MoE进行微调，并在各自对应的测试集上进行评估。在第二种设置中，我们在MSMARCO上对SB-MoE进行微调，并在十三个BEIR数据集上进行零样本评估。此外，我们还进行了进一步的实验，以分析模型对超参数（即所使用和激活的专家数量）的依赖性，并研究这种变化如何影响SB-MoE的性能。实验结果表明，SB-MoE对轻量级基础模型的DRMs（如TinyBERT和BERT-Small）特别有效，其性能在多个基准上一致优于标准模型微调方法。对于参数较多的DRMs（如BERT-Base和Contriever），我们的模型需要更多的训练样本才能实现改进的检索性能。我们的代码可在以下网址获取：https://github.com/FaySokli/SB-MoE。",
        "translated_title": "稠密召回任务中的专家混合方法",
        "label": [
            "通用推荐技术"
        ],
        "label_reason": "论文涉及密集检索模型优化，可能适用于推荐系统召回阶段。",
        "relevance_score": 5,
        "novelty_score": 7,
        "novelty_reason": "提出一种更高效的MoE设计，改进了现有方法的参数效率。"
    },
    {
        "title": "SQuAI: Scientific Question-Answering with Multi-Agent\n  Retrieval-Augmented Generation",
        "url": "http://arxiv.org/abs/2510.15682v1",
        "pub_date": "2025-10-17",
        "summary": "We present SQuAI (https://squai.scads.ai/), a scalable and trustworthy multi-agent retrieval-augmented generation (RAG) framework for scientific question answering (QA) with large language models (LLMs). SQuAI addresses key limitations of existing RAG systems in the scholarly domain, where complex, open-domain questions demand accurate answers, explicit claims with citations, and retrieval across millions of scientific documents. Built on over 2.3 million full-text papers from arXiv.org, SQuAI employs four collaborative agents to decompose complex questions into sub-questions, retrieve targeted evidence via hybrid sparse-dense retrieval, and adaptively filter documents to improve contextual relevance. To ensure faithfulness and traceability, SQuAI integrates in-line citations for each generated claim and provides supporting sentences from the source documents. Our system improves faithfulness, answer relevance, and contextual relevance by up to +0.088 (12%) over a strong RAG baseline. We further release a benchmark of 1,000 scientific question-answer-evidence triplets to support reproducibility. With transparent reasoning, verifiable citations, and domain-wide scalability, SQuAI demonstrates how multi-agent RAG enables more trustworthy scientific QA with LLMs.",
        "translated": "我们提出SQuAI（https://squai.scads.ai/），一种基于大语言模型（LLM）的可扩展且可靠的多智能体检索增强生成（RAG）框架，用于科学问答（QA）。SQuAI解决了现有RAG系统在学术领域中的关键局限性，尤其是在处理复杂、开放领域的科学问题时，需要准确的答案、带引文的明确声明，以及在数百万篇科学文献中进行检索。SQuAI基于来自arXiv.org的超过230万篇全文论文构建，利用四个协作智能体将复杂问题分解为子问题，通过混合稀疏-密集检索获取目标证据，并自适应地过滤文档以提高上下文相关性。为确保答案的忠实性和可追溯性，SQuAI为每个生成的声明整合行内引文，并提供支持性句子以引证来源文档。与一个强大的RAG基线相比，我们的系统在答案忠实性、相关性以及上下文相关性方面最多提高了+0.088（12%）。我们进一步发布了一个包含1000个科学问题-答案-证据三元组的基准数据集，以支持可复现性。通过透明的推理、可验证的引文以及覆盖整个领域的可扩展性，SQuAI展示了多智能体RAG如何在LLMs中实现更可信的科学问答。",
        "translated_title": "SQuAI：基于多智能体的检索增强生成的科学问答",
        "label": [],
        "label_reason": "非推荐系统领域，但涉及RAG和检索技术",
        "relevance_score": 3,
        "novelty_score": 7,
        "novelty_reason": "多智能体协作与混合检索方法具有创新性"
    },
    {
        "title": "Enhance Large Language Models as Recommendation Systems with\n  Collaborative Filtering",
        "url": "http://arxiv.org/abs/2510.15647v1",
        "pub_date": "2025-10-17",
        "summary": "As powerful tools in Natural Language Processing (NLP), Large Language Models (LLMs) have been leveraged for crafting recommendations to achieve precise alignment with user preferences and elevate the quality of the recommendations. The existing approaches implement both non-tuning and tuning strategies. Compared to following the tuning strategy, the approaches following the non-tuning strategy avoid the relatively costly, time-consuming, and expertise-requiring process of further training pre-trained LLMs on task-specific datasets, but they suffer the issue of not having the task-specific business or local enterprise knowledge. To the best of our knowledge, none of the existing approaches following the non-tuning strategy explicitly integrates collaborative filtering, one of the most successful recommendation techniques. This study aims to fill the gap by proposing critique-based LLMs as recommendation systems (Critic-LLM-RS). For our purpose, we train a separate machine-learning model called Critic that implements collaborative filtering for recommendations by learning from the interactions between many users and items. The Critic provides critiques to LLMs to significantly refine the recommendations. Extensive experiments have verified the effectiveness of Critic-LLM-RS on real datasets.",
        "translated": "在自然语言处理（NLP）中，大语言模型（LLMs）作为强大的工具已被用于生成推荐，以实现与用户偏好的精确对齐并提升推荐质量。现有方法采用了非微调和微调两种策略。与采用微调策略的方法相比，采用非微调策略的方法避免了在任务特定数据集上进一步训练预训练大语言模型这一相对成本高、耗时长且需要专业知识的过程，但它们缺乏任务特定的商业或本地企业知识。据我们所知，目前没有采用非微调策略的现有方法明确地整合协同过滤这一最成功的推荐技术之一。本研究旨在通过提出基于批评的大语言模型推荐系统（Critic-LLM-RS）来弥补这一不足。为此，我们训练了一个单独的机器学习模型，称为Critic，它通过学习大量用户与物料之间的交互来实现基于协同过滤的推荐。Critic向大语言模型提供批评，从而显著优化推荐结果。大量的实验验证了Critic-LLM-RS在真实数据集上的有效性。",
        "translated_title": "利用协同过滤增强大语言模型作为推荐系统",
        "label": [
            "LLM生成式推荐",
            "通用推荐技术"
        ],
        "label_reason": "论文探讨了LLM在推荐系统中的应用，并结合协同过滤技术",
        "relevance_score": 8,
        "novelty_score": 7,
        "novelty_reason": "提出Critic-LLM-RS框架，创新性地将协同过滤与LLM结合"
    },
    {
        "title": "MCA: Modality Composition Awareness for Robust Composed Multimodal\n  Retrieval",
        "url": "http://arxiv.org/abs/2510.15543v1",
        "pub_date": "2025-10-17",
        "summary": "Multimodal retrieval, which seeks to retrieve relevant content across modalities such as text or image, supports applications from AI search to contents production. Despite the success of separate-encoder approaches like CLIP align modality-specific embeddings with contrastive learning, recent multimodal large language models (MLLMs) enable a unified encoder that directly processes composed inputs. While flexible and advanced, we identify that unified encoders trained with conventional contrastive learning are prone to learn modality shortcut, leading to poor robustness under distribution shifts. We propose a modality composition awareness framework to mitigate this issue. Concretely, a preference loss enforces multimodal embeddings to outperform their unimodal counterparts, while a composition regularization objective aligns multimodal embeddings with prototypes composed from its unimodal parts. These objectives explicitly model structural relationships between the composed representation and its unimodal counterparts. Experiments on various benchmarks show gains in out-of-distribution retrieval, highlighting modality composition awareness as a effective principle for robust composed multimodal retrieval when utilizing MLLMs as the unified encoder.",
        "translated": "跨模态检索旨在跨文本或图像等不同模态中检索相关内容，广泛支持从人工智能搜索到内容生产的应用场景。尽管像 CLIP 这样的独立编码器方法通过对比学习对齐模态特定嵌入取得了成功，但最近的多模态大语言模型（MLLMs）引入了能够直接处理组合输入的统一编码器。虽然统一编码器具备灵活性和先进性，但我们发现使用传统对比学习训练的统一编码器容易学习模态捷径，从而在分布偏移情况下表现不佳。我们提出了一种模态组合感知框架来缓解这一问题。具体而言，通过一个偏好损失强制多模态嵌入优于其单模态对应嵌入，同时通过一个组合正则化目标将多模态嵌入与由其单模态部分组合而成的原型对齐。这些目标显式建模了组合表示与其单模态嵌入之间的结构关系。在多个基准上的实验表明，该框架在分布外检索中取得了提升，突显了模态组合感知作为使用 MLLMs 作为统一编码器时实现鲁棒组合式多模态检索的有效原则。",
        "translated_title": "MCA：模态组合感知的鲁棒组合多模态召回",
        "label": [
            "多模态推荐"
        ],
        "label_reason": "论文聚焦多模态检索，与推荐系统中的多模态推荐有间接联系。",
        "relevance_score": 5,
        "novelty_score": 7,
        "novelty_reason": "提出模态组合感知框架，改进统一编码器训练策略，具有一定创新性。"
    },
    {
        "title": "MSAM: Multi-Semantic Adaptive Mining for Cross-Modal Drone Video-Text\n  Retrieval",
        "url": "http://arxiv.org/abs/2510.15470v1",
        "pub_date": "2025-10-17",
        "summary": "With the advancement of drone technology, the volume of video data increases rapidly, creating an urgent need for efficient semantic retrieval. We are the first to systematically propose and study the drone video-text retrieval (DVTR) task. Drone videos feature overhead perspectives, strong structural homogeneity, and diverse semantic expressions of target combinations, which challenge existing cross-modal methods designed for ground-level views in effectively modeling their characteristics. Therefore, dedicated retrieval mechanisms tailored for drone scenarios are necessary. To address this issue, we propose a novel approach called Multi-Semantic Adaptive Mining (MSAM). MSAM introduces a multi-semantic adaptive learning mechanism, which incorporates dynamic changes between frames and extracts rich semantic information from specific scene regions, thereby enhancing the deep understanding and reasoning of drone video content. This method relies on fine-grained interactions between words and drone video frames, integrating an adaptive semantic construction module, a distribution-driven semantic learning term and a diversity semantic term to deepen the interaction between text and drone video modalities and improve the robustness of feature representation. To reduce the interference of complex backgrounds in drone videos, we introduce a cross-modal interactive feature fusion pooling mechanism that focuses on feature extraction and matching in target regions, minimizing noise effects. Extensive experiments on two self-constructed drone video-text datasets show that MSAM outperforms other existing methods in the drone video-text retrieval task. The source code and dataset will be made publicly available.",
        "translated": "随着无人机技术的发展，视频数据的数量迅速增加，从而对高效的语义召回提出了迫切需求。我们首次系统性地提出了并研究了无人机视频-文本召回（DVTR）任务。无人机视频具有俯视视角、强烈的结构同质性以及目标组合多样的语义表达，这些特性对当前为地面视角设计的跨模态方法在有效建模方面提出了挑战。因此，有必要设计专门针对无人机场景的召回机制。为了解决这一问题，我们提出了一种新颖的方法，称为多语义自适应挖掘（MSAM）。MSAM 引入了一种多语义自适应学习机制，能够捕捉帧之间的动态变化，并从特定场景区域中提取丰富的语义信息，从而增强对无人机视频内容的深入理解和推理能力。该方法依赖于词语与无人机视频帧之间的细粒度交互，融合了自适应语义构建模块、分布驱动的语义学习项以及多样性语义项，以深化文本与无人机视频模态之间的交互，并提升特征表示的鲁棒性。为了减少无人机视频中复杂背景的干扰，我们引入了一种跨模态交互式特征融合池化机制，该机制聚焦于目标区域的特征提取与匹配，从而最小化噪声的影响。在两个自建的无人机视频-文本数据集上的广泛实验表明，MSAM 在无人机视频-文本召回任务中优于其他现有方法。源代码和数据集将向公众开放。",
        "translated_title": "MSAM：用于跨模态无人机视频-文本召回的多语义自适应挖掘",
        "label": [],
        "label_reason": "",
        "relevance_score": 0,
        "novelty_score": 0,
        "novelty_reason": ""
    },
    {
        "title": "Fault Cause Identification across Manufacturing Lines through\n  Ontology-Guided and Process-Aware FMEA Graph Learning with LLMs",
        "url": "http://arxiv.org/abs/2510.15428v1",
        "pub_date": "2025-10-17",
        "summary": "Fault cause identification in automated manufacturing lines is challenging due to the system's complexity, frequent reconfigurations, and the limited reusability of existing Failure Mode and Effects Analysis (FMEA) knowledge. Although FMEA worksheets contain valuable expert insights, their reuse across heterogeneous lines is hindered by natural language variability, inconsistent terminology, and process differences. To address these limitations, this study proposes a process-aware framework that enhances FMEA reusability by combining manufacturing-domain conceptualization with graph neural network (GNN) reasoning. First, FMEA worksheets from multiple manufacturing lines are transformed into a unified knowledge graph through ontology-guided large language model (LLM) extraction, capturing domain concepts such as actions, states, components, and parameters. Second, a Relational Graph Convolutional Network (RGCN) with the process-aware scoring function learns embeddings that respect both semantic relationships and sequential process flows. Finally, link prediction is employed to infer and rank candidate fault causes consistent with the target line's process flow.   A case study on automotive pressure sensor assembly lines demonstrates that the proposed method outperforms a state-of-the-art retrieval-augmented generation (RAG) baseline (F1@20 = 0.267) and an RGCN approach (0.400), achieving the best performance (0.523) in fault cause identification. Ablation studies confirm the contributions of both LLM-driven domain conceptualization and process-aware learning. These results indicate that the proposed framework significantly improves the transferability of FMEA knowledge across heterogeneous lines, thereby supporting operators in diagnosing failures more reliably and paving the way for future domain-adaptive LLM applications in smart manufacturing.",
        "translated": "在自动化生产线中，故障原因识别由于系统复杂性、频繁重构以及现有失效模式与影响分析（FMEA）知识的有限可重用性而具有挑战性。尽管FMEA工作表包含了宝贵的专家见解，但其在异构生产线之间的复用受到自然语言差异、术语不一致和流程差异的限制。为了解决这些局限性，本文提出了一种流程感知框架，通过将制造领域的概念化与图神经网络（GNN）推理相结合，以增强FMEA知识的可重用性。首先，利用本体引导的大语言模型（LLM）从多条制造生产线中提取FMEA工作表，将其转换为统一的知识图谱，从而捕捉诸如操作、状态、组件和参数等领域概念。其次，使用具有流程感知评分函数的关系图卷积网络（RGCN）学习嵌入，该嵌入同时尊重语义关系和顺序流程结构。最后，通过链接预测来推断并排序与目标生产线流程一致的候选故障原因。在汽车压力传感器装配线上的案例研究表明，所提出的方法优于最先进的检索增强生成（RAG）基线方法（F1@20 = 0.267）和RGCN方法（0.400），在故障原因识别上取得了最佳性能（0.523）。消融实验验证了LLM驱动的领域概念化和流程感知学习的贡献。这些结果表明，所提出的框架显著提升了FMEA知识在异构生产线之间的可迁移性，从而帮助操作人员更可靠地诊断故障，并为未来智能制造中领域自适应的大语言模型应用铺平了道路。",
        "translated_title": "跨生产线的故障原因识别：基于本体引导与流程感知的FMEA图学习与大语言模型",
        "label": [],
        "label_reason": "论文聚焦于制造领域的故障分析，与推荐系统无直接关联。",
        "relevance_score": 3,
        "novelty_score": 6,
        "novelty_reason": "提出结合本体论与GNN的框架改进FMEA，有一定创新性但非推荐系统核心方向。"
    },
    {
        "title": "Dimension Mask Layer: Optimizing Embedding Efficiency for Scalable\n  ID-based Models",
        "url": "http://arxiv.org/abs/2510.15308v1",
        "pub_date": "2025-10-17",
        "summary": "In modern recommendation systems and social media platforms like Meta, TikTok, and Instagram, large-scale ID-based features often require embedding tables that consume significant memory. Managing these embedding sizes can be challenging, leading to bulky models that are harder to deploy and maintain. In this paper, we introduce a method to automatically determine the optimal embedding size for ID features, significantly reducing the model size while maintaining performance.   Our approach involves defining a custom Keras layer called the dimension mask layer, which sits directly after the embedding lookup. This layer trims the embedding vector by allowing only the first N dimensions to pass through. By doing this, we can reduce the input feature dimension by more than half with minimal or no loss in model performance metrics. This reduction helps cut down the memory footprint of the model and lowers the risk of overfitting due to multicollinearity.   Through offline experiments on public datasets and an online A/B test on a real production dataset, we demonstrate that using a dimension mask layer can shrink the effective embedding dimension by 40-50\\%, leading to substantial improvements in memory efficiency. This method provides a scalable solution for platforms dealing with a high volume of ID features, optimizing both resource usage and model performance.",
        "translated": "在现代推荐系统和Meta、TikTok以及Instagram等社交媒体平台中，大规模基于ID的特征通常需要嵌入表，而嵌入表会消耗大量内存。管理这些嵌入的维度规模具有挑战性，从而导致模型体积庞大，难以部署和维护。在本文中，我们介绍了一种方法，用于自动确定ID特征的最优嵌入维度，从而在保持性能的前提下显著减小模型规模。\n\n我们的方法包括定义一个自定义的Keras层，称为维度掩码层（dimension mask layer），该层直接位于嵌入查找（embedding lookup）之后。该层通过仅允许嵌入向量的前N个维度通过，从而对嵌入向量进行裁剪。通过这种方式，我们可以在几乎没有或完全没有损失模型性能指标的情况下，将输入特征维度减少一半以上。这种降维有助于减小模型的内存占用，并降低由于多重共线性导致的过拟合风险。\n\n通过在公共数据集上的离线实验和在真实生产数据集上的在线A/B测试，我们证明使用维度掩码层可以将有效的嵌入维度减少40-50%，从而在内存效率方面实现显著提升。该方法为处理大量ID特征的平台提供了一种可扩展的解决方案，同时优化了资源使用和模型性能。",
        "translated_title": "维度掩码层：优化可扩展ID模型的嵌入效率",
        "label": [
            "通用推荐技术",
            "负采样与对比学习"
        ],
        "label_reason": "论文提出嵌入维度优化方法，适用于推荐系统特征建模。",
        "relevance_score": 7,
        "novelty_score": 6,
        "novelty_reason": "改进了嵌入层设计，有一定实用创新但非范式突破。"
    },
    {
        "title": "GRank: Towards Target-Aware and Streamlined Industrial Retrieval with a\n  Generate-Rank Framework",
        "url": "http://arxiv.org/abs/2510.15299v1",
        "pub_date": "2025-10-17",
        "summary": "Industrial-scale recommender systems rely on a cascade pipeline in which the retrieval stage must return a high-recall candidate set from billions of items under tight latency. Existing solutions ei- ther (i) suffer from limited expressiveness in capturing fine-grained user-item interactions, as seen in decoupled dual-tower architectures that rely on separate encoders, or generative models that lack precise target-aware matching capabilities, or (ii) build structured indices (tree, graph, quantization) whose item-centric topologies struggle to incorporate dynamic user preferences and incur prohibitive construction and maintenance costs.   We present GRank, a novel structured-index-free retrieval paradigm that seamlessly unifies target-aware learning with user-centric retrieval. Our key innovations include: (1) A target-aware Generator trained to perform personalized candidate generation via GPU-accelerated MIPS, eliminating semantic drift and maintenance costs of structured indexing; (2) A lightweight but powerful Ranker that performs fine-grained, candidate-specific inference on small subsets; (3) An end-to-end multi-task learning framework that ensures semantic consistency between generation and ranking objectives.   Extensive experiments on two public benchmarks and a billion-item production corpus demonstrate that GRank improves Recall@500 by over 30% and 1.7$\\times$ the P99 QPS of state-of-the-art tree- and graph-based retrievers.   GRank has been fully deployed in production in our recommendation platform since Q2 2025, serving 400 million active users with 99.95% service availability. Online A/B tests confirm significant improvements in core engagement metrics, with Total App Usage Time increasing by 0.160% in the main app and 0.165% in the Lite version.",
        "translated": "工业级推荐系统依赖于一个级联流水线，其中召回阶段必须在严格的延迟限制下从数十亿个物料中返回一个高召回的候选集。现有解决方案要么（i）在捕捉用户-物料细粒度交互方面存在表达能力的局限，如依赖独立编码器的解耦双塔架构，或缺乏精确目标感知匹配能力的生成模型；要么（ii）构建结构化索引（树、图、量化），其以物料为中心的拓扑结构难以融合动态的用户偏好，并带来高昂的构建和维护成本。\n\n我们提出了 GRank，一种新颖的、不依赖结构化索引的召回范式，该范式无缝地将目标感知学习与用户中心召回统一起来。我们的关键创新包括：（1）一个目标感知的生成器（Generator），其通过 GPU 加速的 MIPS 进行个性化候选生成，消除了语义漂移和结构化索引的维护成本；（2）一个轻量但强大的排序器（Ranker），其在小规模候选子集上进行细粒度、候选特定的推理；（3）一个端到端的多任务学习框架，确保生成和排序目标之间的语义一致性。\n\n在两个公开基准和一个包含十亿级物料的生产语料上的广泛实验表明，GRank 在 Recall@500 上提升了超过 30%，并且在 P99 QPS 上达到了最先进树型和图型召回器的 1.7 倍。GRank 自 2025 年第二季度起已在我们的推荐平台中全面部署，服务于 4 亿活跃用户，服务可用性达到 99.95%。在线 A/B 测试确认了其在核心参与度指标上的显著提升，主应用中的总使用时长（Total App Usage Time）增加了 0.160%，Lite 版本增加了 0.165%。",
        "translated_title": "GRank：面向目标感知和高效工业召回的生成-排序框架",
        "label": [
            "召回（Recall）",
            "精排（Ranking）",
            "LLM生成式推荐（LLM-based Generative Recommendation）",
            "通用推荐技术（General Recommendation Techniques）"
        ],
        "label_reason": "GRank 提出生成-排序框架，提升推荐召回与排序效果，适用于工业级推荐场景。",
        "relevance_score": 9,
        "novelty_score": 8,
        "novelty_reason": "结合目标感知生成与轻量排序，避免结构化索引，创新性强且效果显著。"
    },
    {
        "title": "MTmixAtt: Integrating Mixture-of-Experts with Multi-Mix Attention for\n  Large-Scale Recommendation",
        "url": "http://arxiv.org/abs/2510.15286v1",
        "pub_date": "2025-10-17",
        "summary": "Industrial recommender systems critically depend on high-quality ranking models. However, traditional pipelines still rely on manual feature engineering and scenario-specific architectures, which hinder cross-scenario transfer and large-scale deployment. To address these challenges, we propose \\textbf{MTmixAtt}, a unified Mixture-of-Experts (MoE) architecture with Multi-Mix Attention, designed for large-scale recommendation tasks. MTmixAtt integrates two key components. The \\textbf{AutoToken} module automatically clusters heterogeneous features into semantically coherent tokens, removing the need for human-defined feature groups. The \\textbf{MTmixAttBlock} module enables efficient token interaction via a learnable mixing matrix, shared dense experts, and scenario-aware sparse experts, capturing both global patterns and scenario-specific behaviors within a single framework. Extensive experiments on the industrial TRec dataset from Meituan demonstrate that MTmixAtt consistently outperforms state-of-the-art baselines including Transformer-based models, WuKong, HiFormer, MLP-Mixer, and RankMixer. At comparable parameter scales, MTmixAtt achieves superior CTR and CTCVR metrics; scaling to MTmixAtt-1B yields further monotonic gains. Large-scale online A/B tests validate the real-world impact: in the \\textit{Homepage} scenario, MTmixAtt increases Payment PV by \\textbf{+3.62\\%} and Actual Payment GTV by \\textbf{+2.54\\%}. Overall, MTmixAtt provides a unified and scalable solution for modeling arbitrary heterogeneous features across scenarios, significantly improving both user experience and commercial outcomes.",
        "translated": "工业推荐系统高度依赖高质量的排序模型。然而，传统的推荐流程仍然依赖人工特征工程和特定场景的模型架构，这限制了跨场景的迁移能力和大规模部署的效率。为了解决这些挑战，我们提出了**MTmixAtt**，一种基于多混合注意力机制的统一专家混合（Mixture-of-Experts，MoE）架构，专为大规模推荐任务设计。MTmixAtt 集成了两个关键模块。**AutoToken** 模块能够自动将异构特征聚类为语义连贯的 tokens，从而去除了对人工定义特征组的依赖。**MTmixAttBlock** 模块则通过可学习的混合矩阵、共享的密集专家和场景感知的稀疏专家实现高效的 token 交互，从而在单一框架内同时捕捉全局模式和特定场景的行为。在来自美团的工业级 TRec 数据集上的大量实验表明，MTmixAtt 在点击率（CTR）和转化率（CTCVR）等指标上始终优于当前最先进的基线模型，包括基于 Transformer 的模型、WuKong、HiFormer、MLP-Mixer 和 RankMixer。在参数规模相当的情况下，MTmixAtt-1B 在扩展后进一步实现了单调性能提升。大规模的在线 A/B 测试验证了其在实际中的效果：在 \\textit{Homepage} 场景下，MTmixAtt 带来了 \\textbf{+3.62\\%} 的支付页面访问量（Payment PV）提升和 \\textbf{+2.54\\%} 的实际支付 GMV（GTV）增长。总体而言，MTmixAtt 为跨场景建模任意异构特征提供了一个统一且可扩展的解决方案，显著提升了用户体验和商业成果。",
        "translated_title": "MTmixAtt：将专家混合与多混合注意力结合用于大规模推荐",
        "label": [],
        "label_reason": "",
        "relevance_score": 0,
        "novelty_score": 0,
        "novelty_reason": ""
    },
    {
        "title": "HOB: A Holistically Optimized Bidding Strategy under Heterogeneous\n  Auction Mechanisms with Organic Traffic",
        "url": "http://arxiv.org/abs/2510.15238v1",
        "pub_date": "2025-10-17",
        "summary": "The E-commerce advertising platforms typically sell commercial traffic through either second-price auction (SPA) or first-price auction (FPA). SPA was historically prevalent due to its dominant strategy incentive-compatible (DSIC) for bidders with quasi-linear utilities, especially when budgets are not a binding constraint, while FPA has gained more prominence for offering higher revenue potential to publishers and avoiding the possibility for discriminatory treatment in personalized reserve prices. Meanwhile, on the demand side, advertisers are increasingly adopting platform-wide marketing solutions akin to QuanZhanTui, shifting from spending budgets solely on commercial traffic to bidding on the entire traffic for the purpose of maximizing overall sales. For automated bidding systems, such a trend poses a critical challenge: determining optimal strategies across heterogeneous auction channels to fulfill diverse advertiser objectives, such as maximizing return (MaxReturn) or meeting target return on ad spend (TargetROAS). To overcome this challenge, this work makes two key contributions. First, we derive an efficient solution for optimal bidding under FPA channels, which takes into account the presence of organic traffic - traffic can be won for free. Second, we introduce a marginal cost alignment (MCA) strategy that provably secures bidding efficiency across heterogeneous auction mechanisms. To validate performance of our developed framework, we conduct comprehensive offline experiments on public datasets and large-scale online A/B testing, which demonstrate consistent improvements over existing methods.",
        "translated": "电子商务广告平台通常通过第二价格拍卖（SPA）或第一价格拍卖（FPA）出售商业流量。SPA 在历史上较为普遍，因为它对具有准线性效用的竞标者而言是占优策略激励相容（DSIC）的，尤其是在预算并非约束条件的情况下。而 FPA 因其能为发布者提供更高的收入潜力，并避免在个性化保留价中出现歧视性待遇的可能性，而逐渐受到更多关注。与此同时，在需求侧，广告主正越来越多地采用类似 QuanZhanTui 的平台级营销方案，从仅仅将预算用于商业流量，转向对全部流量进行竞价，以实现整体销售额的最大化。对于自动出价系统而言，这一趋势带来了关键挑战：如何在异构的拍卖渠道中确定最优策略，以满足广告主的多样化目标，例如最大化回报（MaxReturn）或达到目标广告支出回报率（TargetROAS）。为克服这一挑战，本文做出了两个关键贡献。首先，我们推导了一个在 FPA 渠道下高效确定最优出价的解决方案，该方案考虑了免费流量（即自然流量）的存在——这些流量可以免费获取。其次，我们引入了一种边际成本对齐（MCA）策略，该策略在理论上可确保在异构拍卖机制中实现出价效率。为验证所提出框架的性能，我们在公开数据集上进行了全面的离线实验，并在大规模在线 A/B 测试中进行了验证，结果表明该方法相较于现有方法实现了持续的改进。",
        "translated_title": "HOB：异质拍卖机制与自然流量下的整体优化出价策略",
        "label": [
            "跨域/联邦推荐"
        ],
        "label_reason": "论文涉及广告竞价策略优化，与推荐系统有一定间接关联。",
        "relevance_score": 5,
        "novelty_score": 7,
        "novelty_reason": "提出了针对异构竞价机制的边际成本对齐策略，具有一定创新性。"
    },
    {
        "title": "Structure-R1: Dynamically Leveraging Structural Knowledge in LLM\n  Reasoning through Reinforcement Learning",
        "url": "http://arxiv.org/abs/2510.15191v1",
        "pub_date": "2025-10-16",
        "summary": "Large language models (LLMs) have demonstrated remarkable advances in reasoning capabilities. However, their performance remains constrained by limited access to explicit and structured domain knowledge. Retrieval-Augmented Generation (RAG) addresses this by incorporating external information as context to augment reasoning. Nevertheless, traditional RAG systems typically operate over unstructured and fragmented text, resulting in low information density and suboptimal reasoning. To overcome these limitations, we propose \\textsc{Structure-R1}, a novel framework that transforms retrieved content into structured representations optimized for reasoning. Leveraging reinforcement learning, \\textsc{Structure-R1} learns a content representation policy that dynamically generates and adapts structural formats based on the demands of multi-step reasoning. Unlike prior methods that rely on fixed schemas, our approach adopts a generative paradigm capable of producing task-specific structures tailored to individual queries. To ensure the quality and reliability of these representations, we introduce a self-reward structural verification mechanism that checks whether the generated structures are both correct and self-contained. Extensive experiments on seven knowledge-intensive benchmarks show that \\textsc{Structure-R1} consistently achieves competitive performance with a 7B-scale backbone model and matches the performance of much larger models. Additionally, our theoretical analysis demonstrates how structured representations enhance reasoning by improving information density and contextual clarity. Our code and data are available at: https://github.com/jlwu002/sr1.",
        "translated": "大语言模型（LLMs）在推理能力方面已展现出显著的进展。然而，它们的性能仍受限于对显式且结构化领域知识的访问不足。检索增强生成（RAG）通过将外部信息作为上下文以增强推理能力来解决这一问题。然而，传统的 RAG 系统通常在非结构化和碎片化的文本上运行，导致信息密度低和推理效果不佳。为克服这些限制，我们提出了 \\textsc{Structure-R1}，一种新颖的框架，能够将检索到的内容转换为优化推理的结构化表示。该框架利用强化学习，学习一种内容表示策略，能够根据多步推理的需求动态生成和调整结构化格式。不同于依赖固定模式的先前方法，我们的方法采用生成式范式，能够为每个查询生成特定任务的结构。为确保这些表示的质量和可靠性，我们引入了一种自奖励的结构验证机制，用以检查生成的结构是否正确且自洽。在七个需要大量知识的基准数据集上的广泛实验表明，\\textsc{Structure-R1} 在使用 7B 规模主干模型的情况下始终能够实现具有竞争力的性能，并可与更大模型的表现相匹配。此外，我们的理论分析展示了结构化表示如何通过提高信息密度和上下文清晰度来增强推理。我们的代码和数据可在以下地址获取：https://github.com/jlwu002/sr1。",
        "translated_title": "Structure-R1：通过强化学习动态利用大语言模型中的结构化知识",
        "label": [
            "LLM生成式推荐"
        ],
        "label_reason": "论文将结构化知识与LLM结合，适用于生成式推荐系统的推理增强。",
        "relevance_score": 6,
        "novelty_score": 8,
        "novelty_reason": "提出动态生成结构化表示的新框架，并结合强化学习进行优化。"
    },
    {
        "title": "DMRetriever: A Family of Models for Improved Text Retrieval in Disaster\n  Management",
        "url": "http://arxiv.org/abs/2510.15087v1",
        "pub_date": "2025-10-16",
        "summary": "Effective and efficient access to relevant information is essential for disaster management. However, no retrieval model is specialized for disaster management, and existing general-domain models fail to handle the varied search intents inherent to disaster management scenarios, resulting in inconsistent and unreliable performance. To this end, we introduce DMRetriever, the first series of dense retrieval models (33M to 7.6B) tailored for this domain. It is trained through a novel three-stage framework of bidirectional attention adaptation, unsupervised contrastive pre-training, and difficulty-aware progressive instruction fine-tuning, using high-quality data generated through an advanced data refinement pipeline. Comprehensive experiments demonstrate that DMRetriever achieves state-of-the-art (SOTA) performance across all six search intents at every model scale. Moreover, DMRetriever is highly parameter-efficient, with 596M model outperforming baselines over 13.3 X larger and 33M model exceeding baselines with only 7.6% of their parameters. All codes, data, and checkpoints are available at https://github.com/KaiYin97/DMRETRIEVER",
        "translated": "在灾难管理中，有效且高效地获取相关信息至关重要。然而，目前尚无专门针对灾难管理设计的检索模型，现有的通用领域模型无法处理灾难管理场景中固有的多样化的搜索意图，从而导致性能表现不一致且不可靠。为此，我们引入了 DMRetriever，这是首个专门为此领域定制的密集检索模型系列（33M 到 7.6B 参数）。该模型通过一种新颖的三阶段训练框架进行训练，包括双向注意力适配、无监督对比预训练以及难度感知的渐进式指令微调，使用了通过先进数据精炼流水线生成的高质量数据。全面的实验表明，DMRetriever 在所有六种搜索意图上均在每种模型规模下实现了最先进的（SOTA）性能。此外，DMRetriever 具有高度参数效率，596M 模型的性能优于参数规模为其 13.3 倍的基线模型，而 33M 模型的性能甚至超过了基线模型，其参数仅为基线模型的 7.6%。所有代码、数据和模型检查点均可在 https://github.com/KaiYin97/DMRETRIEVER 获取。",
        "translated_title": "DMRetriever: 用于灾难管理中提升文本召回的一组模型",
        "label": [
            "召回（Recall）"
        ],
        "label_reason": "论文聚焦灾害管理中的文本检索，属于信息检索范畴，与推荐系统召回环节间接相关。",
        "relevance_score": 4,
        "novelty_score": 7,
        "novelty_reason": "提出三阶段训练框架和高效参数模型，在检索模型设计上有创新。"
    },
    {
        "title": "ConsistEdit: Highly Consistent and Precise Training-free Visual Editing",
        "url": "http://arxiv.org/abs/2510.17803v1",
        "pub_date": "2025-10-20",
        "summary": "Recent advances in training-free attention control methods have enabled flexible and efficient text-guided editing capabilities for existing generation models. However, current approaches struggle to simultaneously deliver strong editing strength while preserving consistency with the source. This limitation becomes particularly critical in multi-round and video editing, where visual errors can accumulate over time. Moreover, most existing methods enforce global consistency, which limits their ability to modify individual attributes such as texture while preserving others, thereby hindering fine-grained editing. Recently, the architectural shift from U-Net to MM-DiT has brought significant improvements in generative performance and introduced a novel mechanism for integrating text and vision modalities. These advancements pave the way for overcoming challenges that previous methods failed to resolve. Through an in-depth analysis of MM-DiT, we identify three key insights into its attention mechanisms. Building on these, we propose ConsistEdit, a novel attention control method specifically tailored for MM-DiT. ConsistEdit incorporates vision-only attention control, mask-guided pre-attention fusion, and differentiated manipulation of the query, key, and value tokens to produce consistent, prompt-aligned edits. Extensive experiments demonstrate that ConsistEdit achieves state-of-the-art performance across a wide range of image and video editing tasks, including both structure-consistent and structure-inconsistent scenarios. Unlike prior methods, it is the first approach to perform editing across all inference steps and attention layers without handcraft, significantly enhancing reliability and consistency, which enables robust multi-round and multi-region editing. Furthermore, it supports progressive adjustment of structural consistency, enabling finer control.",
        "translated": "近年来，训练无关的注意力控制方法在文本引导的图像编辑任务中取得了显著进展，使得现有生成模型具备了灵活且高效的编辑能力。然而，当前方法在实现强大编辑能力的同时，往往难以保持与原始图像的一致性。这一局限在多轮编辑和视频编辑任务中尤为突出，因为视觉误差可能会随时间累积。此外，大多数现有方法仅强制全局一致性，限制了在保留其他属性的同时修改特定属性（如纹理）的能力，从而阻碍了细粒度的编辑。最近，从 U-Net 架构向 MM-DiT 的转变在生成性能上带来了显著提升，并引入了一种新的文本与视觉模态融合机制。这些进展为克服先前方法未能解决的挑战奠定了基础。通过对 MM-DiT 的深入分析，我们发现了其注意力机制中的三个关键见解。基于这些发现，我们提出了 ConsistEdit，一种专为 MM-DiT 量身定制的全新注意力控制方法。ConsistEdit 融合了仅视觉的注意力控制、掩膜引导的预注意力融合机制，以及对 query、key 和 value token 的差异化操作，以实现一致且与提示对齐的编辑效果。大量实验证明，ConsistEdit 在多种图像和视频编辑任务中均取得了最先进的性能，涵盖了结构一致和结构不一致的场景。与以往方法不同，这是首个无需人工设计，即可在所有推理步骤和注意力层中进行编辑的方法，显著提升了编辑的鲁棒性和一致性，从而支持多轮和多区域的编辑操作。此外，该方法还支持结构一致性的渐进式调整，实现了更精细的控制。",
        "translated_title": "ConsistEdit: 高一致性且高精度的免训练视觉编辑",
        "label": [],
        "label_reason": "论文聚焦图像编辑而非像素级质量复原",
        "relevance_score": 3,
        "novelty_score": 7,
        "novelty_reason": "提出新的注意力控制机制，改进视频和多轮编辑一致性"
    },
    {
        "title": "Glyph: Scaling Context Windows via Visual-Text Compression",
        "url": "http://arxiv.org/abs/2510.17800v1",
        "pub_date": "2025-10-20",
        "summary": "Large language models (LLMs) increasingly rely on long-context modeling for tasks such as document understanding, code analysis, and multi-step reasoning. However, scaling context windows to the million-token level brings prohibitive computational and memory costs, limiting the practicality of long-context LLMs. In this work, we take a different perspective-visual context scaling-to tackle this challenge. Instead of extending token-based sequences, we propose Glyph, a framework that renders long texts into images and processes them with vision-language models (VLMs). This approach substantially compresses textual input while preserving semantic information, and we further design an LLM-driven genetic search to identify optimal visual rendering configurations for balancing accuracy and compression. Through extensive experiments, we demonstrate that our method achieves 3-4x token compression while maintaining accuracy comparable to leading LLMs such as Qwen3-8B on various long-context benchmarks. This compression also leads to around 4x faster prefilling and decoding, and approximately 2x faster SFT training. Furthermore, under extreme compression, a 128K-context VLM could scale to handle 1M-token-level text tasks. In addition, the rendered text data benefits real-world multimodal tasks, such as document understanding. Our code and model are released at https://github.com/thu-coai/Glyph.",
        "translated": "大语言模型（LLMs）在文档理解、代码分析和多步骤推理等任务中日益依赖于长上下文建模。然而，将上下文窗口扩展到百万级 token 的水平会带来高昂的计算和内存成本，从而限制了长上下文 LLM 的实用性。在本工作中，我们从视觉上下文扩展（visual context scaling）的角度出发，提出了一种不同的方法来应对这一挑战。我们没有扩展基于 token 的序列，而是提出了 Glyph 框架，该框架将长文本渲染为图像，并通过视觉-语言模型（VLMs）进行处理。该方法在保持语义信息的同时显著压缩了文本输入，我们进一步设计了一个由 LLM 驱动的遗传搜索算法，用于寻找在准确性和压缩率之间达到平衡的最佳视觉渲染配置。通过广泛的实验，我们证明了我们的方法在保持与 Qwen3-8B 等领先 LLM 相当的准确性的同时，实现了 3-4 倍的 token 压缩。这种压缩还带来了预填充和解码速度约 4 倍的提升，以及 SFT 训练速度大约 2 倍的提升。此外，在极端压缩的情况下，一个 128K 上下文的 VLM 可以扩展以处理百万级 token 的文本任务。另外，渲染后的文本数据也有助于现实世界中的多模态任务，例如文档理解。我们的代码和模型已发布在 https://github.com/thu-coai/Glyph。",
        "translated_title": "Glyph: 通过视觉-文本压缩扩展上下文窗口",
        "label": [],
        "label_reason": "不属于low-level图像处理任务",
        "relevance_score": 1,
        "novelty_score": 6,
        "novelty_reason": "提出文本压缩新方法，但非图像复原/增强"
    },
    {
        "title": "UltraCUA: A Foundation Model for Computer Use Agents with Hybrid Action",
        "url": "http://arxiv.org/abs/2510.17790v1",
        "pub_date": "2025-10-20",
        "summary": "Multimodal agents for computer use rely exclusively on primitive actions (click, type, scroll) that require accurate visual grounding and lengthy execution chains, leading to cascading failures and performance bottlenecks. While other agents leverage rich programmatic interfaces (APIs, MCP servers, tools), computer-use agents (CUAs) remain isolated from these capabilities. We present UltraCUA, a foundation model that bridges this gap through hybrid action -- seamlessly integrating GUI primitives with high-level programmatic tool calls. To achieve this, our approach comprises four key components: (1) an automated pipeline that scales programmatic tools from software documentation, open-source repositories, and code generation; (2) a synthetic data engine producing over 17,000 verifiable tasks spanning real-world computer-use scenarios; (3) a large-scale high-quality hybrid action trajectory collection with both low-level GUI actions and high-level programmatic tool calls; and (4) a two-stage training pipeline combining supervised fine-tuning with online reinforcement learning, enabling strategic alternation between low-level and high-level actions. Experiments with our 7B and 32B models demonstrate substantial improvements over state-of-the-art agents. On OSWorld, UltraCUA models achieve an average 22% relative improvement over base models, while being 11% faster in terms of steps. Out-of-domain evaluation on WindowsAgentArena shows our model reaches 21.7% success rate, outperforming baselines trained on Windows data. The hybrid action mechanism proves critical, reducing error propagation while maintaining execution efficiency.",
        "translated": "当前依赖于基本操作（点击、输入、滚动）的多模态计算机使用代理存在视觉定位精度要求高和操作链条长的问题，导致级联失效和性能瓶颈。虽然其他代理能够利用丰富的编程接口（APIs、MCP 服务器、工具），但计算机使用代理（CUAs）仍未具备这些能力。我们提出了 UltraCUA，这是一种通过混合操作弥合这一差距的基础模型——无缝整合 GUI 原语与高层编程工具调用。为此，我们的方法包含四个关键组件：(1) 一个自动化流程，通过软件文档、开源仓库和代码生成扩展编程工具；(2) 一个合成数据引擎，生成超过 17,000 个可验证任务，覆盖现实中的计算机使用场景；(3) 一个大规模高质量的混合操作轨迹集，包含低层 GUI 操作和高层编程工具调用；以及 (4) 一个结合监督微调和在线强化学习的两阶段训练流程，支持低层与高层操作之间的策略切换。在我们的 7B 和 32B 模型上进行的实验表明，UltraCUA 显著优于最先进的代理。在 OSWorld 上，UltraCUA 模型平均比基线模型提升 22%，同时在操作步数上快 11%。在 WindowsAgentArena 上的跨领域评估中，我们的模型达到了 21.7% 的成功率，优于基于 Windows 数据训练的基线模型。混合操作机制被证明是关键，既减少了错误传播，又保持了执行效率。",
        "translated_title": "UltraCUA：一种具有混合动作能力的计算机使用代理基础模型",
        "label": [],
        "label_reason": "论文聚焦计算机代理任务，非图像像素级处理",
        "relevance_score": 1,
        "novelty_score": 6,
        "novelty_reason": "提出混合动作机制并构建任务数据，有一定创新"
    },
    {
        "title": "Botany-Bot: Digital Twin Monitoring of Occluded and Underleaf Plant\n  Structures with Gaussian Splats",
        "url": "http://arxiv.org/abs/2510.17783v1",
        "pub_date": "2025-10-20",
        "summary": "Commercial plant phenotyping systems using fixed cameras cannot perceive many plant details due to leaf occlusion. In this paper, we present Botany-Bot, a system for building detailed \"annotated digital twins\" of living plants using two stereo cameras, a digital turntable inside a lightbox, an industrial robot arm, and 3D segmentated Gaussian Splat models. We also present robot algorithms for manipulating leaves to take high-resolution indexable images of occluded details such as stem buds and the underside/topside of leaves. Results from experiments suggest that Botany-Bot can segment leaves with 90.8% accuracy, detect leaves with 86.2% accuracy, lift/push leaves with 77.9% accuracy, and take detailed overside/underside images with 77.3% accuracy. Code, videos, and datasets are available at https://berkeleyautomation.github.io/Botany-Bot/.",
        "translated": "商业植物表型分析系统使用固定摄像头难以因叶片遮挡而感知许多植物细节。在本文中，我们提出了 Botany-Bot，这是一个系统，使用两个立体摄像头、一个位于灯光箱中的数字转盘、一个工业机械臂和 3D 分割的 Gaussian Splat 模型，以构建植物的详细“带注释的数字孪生”。我们还提出了用于操控叶片的机器人算法，以获取遮挡细节（如茎芽、叶片上表面和下表面）的高分辨率可检索图像。实验结果表明，Botany-Bot 能够以 90.8% 的精度分割叶片，86.2% 的精度检测叶片，77.9% 的精度提起/压下叶片，并以 77.3% 的精度获取详细的上表面/下表面图像。代码、视频和数据集可在 https://berkeleyautomation.github.io/Botany-Bot/ 获取。",
        "translated_title": "Botany-Bot: 使用高斯点进行遮挡和叶下植物结构的数字孪生监测",
        "label": [
            "图像分割",
            "遥感图像复原"
        ],
        "label_reason": "涉及植物遮挡结构的3D分割与高分辨率图像采集，与low-level图像处理相关",
        "relevance_score": 6,
        "novelty_score": 7,
        "novelty_reason": "提出结合机械臂与Gaussian Splat模型的创新系统，用于植物数字孪生建模"
    },
    {
        "title": "SparseVILA: Decoupling Visual Sparsity for Efficient VLM Inference",
        "url": "http://arxiv.org/abs/2510.17777v1",
        "pub_date": "2025-10-20",
        "summary": "Vision Language Models (VLMs) have rapidly advanced in integrating visual and textual reasoning, powering applications across high-resolution image understanding, long-video analysis, and multi-turn conversation. However, their scalability remains limited by the growing number of visual tokens that dominate inference latency. We present SparseVILA, a new paradigm for efficient VLM inference that decouples visual sparsity across the prefilling and decoding stages. SparseVILA distributes sparsity across stages by pruning redundant visual tokens during prefill and retrieving only query-relevant tokens during decoding. This decoupled design matches leading prefill pruning methods while preserving multi-turn fidelity by retaining most of the visual cache so that query-aware tokens can be retrieved at each conversation round. Built on an AWQ-optimized inference pipeline, SparseVILA achieves up to 4.0 times faster prefilling, 2.5 times faster decoding, and an overall 2.6 times end-to-end speedup on long-context video tasks -- while improving accuracy on document-understanding and reasoning tasks. By decoupling query-agnostic pruning and query-aware retrieval, SparseVILA establishes a new direction for efficient multimodal inference, offering a training-free, architecture-agnostic framework for accelerating large VLMs without sacrificing capability.",
        "translated": "视觉语言模型（VLMs）在整合视觉与文本推理方面取得了快速进展，并推动了高分辨率图像理解、长视频分析和多轮对话等应用的发展。然而，其可扩展性受到日益增长的视觉token数量的限制，这些token在推理延迟中占据主导地位。我们提出了SparseVILA，这是一种新的高效VLM推理范式，通过在预填充（prefilling）和解码（decoding）阶段解耦视觉稀疏性，提高推理效率。SparseVILA在预填充阶段通过剪枝冗余的视觉token实现稀疏性，而在解码阶段仅检索与查询相关的token，从而将稀疏性分布在不同阶段。该解耦设计在保持多轮对话保真度的同时，与领先的预填充剪枝方法相匹配，通过保留大部分视觉缓存以在每一轮对话中检索查询感知的token。基于AWQ优化的推理管道，SparseVILA在长上下文视频任务中实现了最高4.0倍的预填充加速、2.5倍的解码加速以及整体2.6倍的端到端加速，同时在文档理解和推理任务中提升了准确性。通过将查询无关的剪枝与查询感知的检索解耦，SparseVILA为高效多模态推理开辟了新的方向，提供了一种无需训练、且适用于任意架构的框架，可在不牺牲模型能力的前提下加速大规模VLMs。",
        "translated_title": "SparseVILA：解耦视觉稀疏性以实现高效的视觉语言模型推理",
        "label": [],
        "label_reason": "论文聚焦于视觉语言模型推理效率，不涉及像素级图像质量恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出了一种新的推理范式，通过解耦视觉稀疏性提高效率。"
    },
    {
        "title": "On-the-Fly OVD Adaptation with FLAME: Few-shot Localization via Active\n  Marginal-Samples Exploration",
        "url": "http://arxiv.org/abs/2510.17670v1",
        "pub_date": "2025-10-20",
        "summary": "Open-vocabulary object detection (OVD) models offer remarkable flexibility by detecting objects from arbitrary text queries. However, their zero-shot performance in specialized domains like Remote Sensing (RS) is often compromised by the inherent ambiguity of natural language, limiting critical downstream applications. For instance, an OVD model may struggle to distinguish between fine-grained classes such as \"fishing boat\" and \"yacht\" since their embeddings are similar and often inseparable. This can hamper specific user goals, such as monitoring illegal fishing, by producing irrelevant detections. To address this, we propose a cascaded approach that couples the broad generalization of a large pre-trained OVD model with a lightweight few-shot classifier. Our method first employs the zero-shot model to generate high-recall object proposals. These proposals are then refined for high precision by a compact classifier trained in real-time on only a handful of user-annotated examples - drastically reducing the high costs of RS imagery annotation.The core of our framework is FLAME, a one-step active learning strategy that selects the most informative samples for training. FLAME identifies, on the fly, uncertain marginal candidates near the decision boundary using density estimation, followed by clustering to ensure sample diversity. This efficient sampling technique achieves high accuracy without costly full-model fine-tuning and enables instant adaptation, within less then a minute, which is significantly faster than state-of-the-art alternatives.Our method consistently surpasses state-of-the-art performance on RS benchmarks, establishing a practical and resource-efficient framework for adapting foundation models to specific user needs.",
        "translated": "开放词汇目标检测（OVD）模型通过从任意文本查询中检测目标，提供了显著的灵活性。然而，由于自然语言本身的歧义性，这些模型在遥感（RS）等专业领域中的零样本性能常常受到影响，从而限制了其关键的下游应用。例如，一个OVD模型可能难以区分“fishing boat”和“yacht”等细粒度类别，因为它们的嵌入表示相似且通常无法分离。这可能会妨碍特定用户目标，例如监测非法捕鱼行为，从而产生不相关的目标检测结果。为了解决这一问题，我们提出了一种级联方法，将大规模预训练OVD模型的广泛泛化能力与一个轻量级的小样本分类器相结合。我们的方法首先利用零样本模型生成高召回率的目标候选区域，然后通过一个紧凑的分类器对这些候选区域进行高精度的优化。该分类器仅使用少量用户标注的示例即可实时训练，从而大幅降低遥感图像标注的高昂成本。我们框架的核心是FLAME，一种一步式主动学习策略，用于选择最具信息量的样本进行训练。FLAME通过密度估计实时识别决策边界附近的不确定边际候选样本，随后进行聚类以确保样本的多样性。这种高效的采样技术在不进行代价高昂的完整模型微调的情况下实现了高精度，并能够在一分钟之内实现即时适应，显著快于最先进的替代方案。我们的方法在遥感基准测试中持续超越最先进的性能，建立了一个实用且资源高效的框架，用于将基础模型适配到特定用户需求。",
        "translated_title": "On-the-Fly OVD Adaptation with FLAME: Few-shot Localization via Active Marginal-Samples Exploration  \nFLAME：通过主动探索边缘样本实现快速变化的OVD适应与小样本定位",
        "label": [],
        "label_reason": "论文主要涉及开放词汇目标检测，与推荐系统无直接关系。",
        "relevance_score": 2,
        "novelty_score": 5,
        "novelty_reason": "提出了FLAME策略，但属于常规改进，未显著推动推荐领域发展。"
    },
    {
        "title": "OG-Rank: Learning to Rank Fast and Slow with Uncertainty and\n  Reward-Trend Guided Adaptive Exploration",
        "url": "http://arxiv.org/abs/2510.17614v1",
        "pub_date": "2025-10-20",
        "summary": "Clinicians need ranking systems that work in real time and still justify their choices. Motivated by the need for a low-latency, decoder-based reranker, we present OG-Rank, a single-decoder approach that pairs a pooled first-token scoring signal with an uncertainty-gated explanation step. The model scores all candidates in one pass and generates a brief, structured rationale only when the list is genuinely ambiguous, keeping latency predictable. Trained with a curriculum that concentrates effort on hard cases, OG-Rank delivers strong effectiveness on encounter-scoped order selection (fast path: Recall@1~0.45, nDCG@20~0.625) and improves further when the gate activates (Recall@1~0.56, nDCG@20~0.699 at a 45\\% gate rate), while compact backbones show similar gains under the same policy. Encoder baselines trail in both effectiveness and flexibility. The result is a practical recipe: rank fast by default and explain when it helps, a pattern that applies broadly to decision tasks where selective generation buys accuracy at acceptable cost. The single-policy design simplifies deployment and budget planning, and the curriculum principle (spend more on the hard cases, less on the easy ones) readily transfers beyond clinical order selection.",
        "translated": "医生需要能够在实时环境下运行并对其推荐结果进行解释的排序系统。受低延迟、基于解码器的重排器需求驱动，我们提出了 OG-Rank，这是一种单解码器方法，将汇聚后的第一个 token 评分信号与不确定性门控的解释步骤相结合。该模型在一次前向传播中对所有候选对象进行评分，并仅在列表确实存在歧义时才生成简短、结构化的解释，从而保持延迟的可预测性。OG-Rank 通过集中训练资源在困难样本上的课程学习方式进行训练，在以就诊为范围的医嘱选择任务中表现出色（快速路径：Recall@1 约为 0.45，nDCG@20 约为 0.625），当门控激活时效果进一步提升（在 45% 的门控率下，Recall@1 约为 0.56，nDCG@20 约为 0.699），而使用相同策略的小型主干模型也能获得类似的提升效果。基于编码器的方法在效果和灵活性方面均落后。结果提供了一种实用的方案：默认快速排序，仅在有必要时进行解释，这种模式广泛适用于那些选择性生成能够以可接受成本换取准确性的决策任务。单策略设计简化了部署与预算规划，而课程学习原则（在困难样本上投入更多，在简单样本上投入更少）也能够轻松迁移到临床医嘱选择以外的其他任务中。",
        "translated_title": "OG-Rank：基于不确定性和奖励趋势引导的自适应探索的“快与慢”排序学习",
        "label": [
            "重排（Re-ranking）",
            "推荐系统评估（Evaluation Metrics / Offline/Online Testing）"
        ],
        "label_reason": "论文提出一种重排方法，结合快速排序与生成解释，适用于推荐系统中的重排环节。",
        "relevance_score": 8,
        "novelty_score": 7,
        "novelty_reason": "提出基于不确定性和奖励趋势的自适应探索机制，改进重排效率与效果。"
    },
    {
        "title": "How role-play shapes relevance judgment in zero-shot LLM rankers",
        "url": "http://arxiv.org/abs/2510.17535v1",
        "pub_date": "2025-10-20",
        "summary": "Large Language Models (LLMs) have emerged as promising zero-shot rankers, but their performance is highly sensitive to prompt formulation. In particular, role-play prompts, where the model is assigned a functional role or identity, often give more robust and accurate relevance rankings. However, the mechanisms and diversity of role-play effects remain underexplored, limiting both effective use and interpretability. In this work, we systematically examine how role-play variations influence zero-shot LLM rankers. We employ causal intervention techniques from mechanistic interpretability to trace how role-play information shapes relevance judgments in LLMs. Our analysis reveals that (1) careful formulation of role descriptions have a large effect on the ranking quality of the LLM; (2) role-play signals are predominantly encoded in early layers and communicate with task instructions in middle layers, while receiving limited interaction with query or document representations. Specifically, we identify a group of attention heads that encode information critical for role-conditioned relevance. These findings not only shed light on the inner workings of role-play in LLM ranking but also offer guidance for designing more effective prompts in IR and beyond, pointing toward broader opportunities for leveraging role-play in zero-shot applications.",
        "translated": "大语言模型（LLMs）已经展现出作为有前景的零样本排序器的潜力，但其性能对提示语的构造高度敏感。特别是角色扮演提示语，其中模型被赋予某种功能角色或身份，往往能提供更为鲁棒且准确的相关性排序。然而，角色扮演机制及其效果的多样性仍缺乏深入研究，这限制了其在实际应用中的有效使用和可解释性。在本研究中，我们系统地考察了角色扮演的变化如何影响零样本LLM排序器的表现。我们采用来自机制可解释性领域的因果干预技术，追踪角色扮演信息如何在LLM中塑造相关性判断。我们的分析揭示了以下几点：（1）角色描述的精心构造对LLM的排序质量具有显著影响；（2）角色扮演信号主要编码在模型的早期层中，并与任务指令在中层层进行交互，而与查询或文档表示的交互则较为有限。具体而言，我们识别出一组注意力头，它们编码了对角色条件相关性至关重要的信息。这些发现不仅阐明了角色扮演在LLM排序中的内部运作机制，还为设计更有效的提示语在信息检索和其他领域提供了指导，表明在零样本应用中利用角色扮演具有更广泛的可能性。",
        "translated_title": "角色扮演如何塑造零样本大语言模型排序器的相关性判断",
        "label": [
            "LLM生成式推荐",
            "精排",
            "推荐系统评估"
        ],
        "label_reason": "论文探讨了LLM在零样本排序中的角色扮演机制，与生成式推荐和排序相关。",
        "relevance_score": 7,
        "novelty_score": 8,
        "novelty_reason": "提出通过角色扮演提升LLM排序性能，并使用因果干预分析其内部机制，具有创新性。"
    },
    {
        "title": "Towards Mixed-Modal Retrieval for Universal Retrieval-Augmented\n  Generation",
        "url": "http://arxiv.org/abs/2510.17354v1",
        "pub_date": "2025-10-20",
        "summary": "Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for enhancing large language models (LLMs) by retrieving relevant documents from an external corpus. However, existing RAG systems primarily focus on unimodal text documents, and often fall short in real-world scenarios where both queries and documents may contain mixed modalities (such as text and images). In this paper, we address the challenge of Universal Retrieval-Augmented Generation (URAG), which involves retrieving and reasoning over mixed-modal information to improve vision-language generation. To this end, we propose Nyx, a unified mixed-modal to mixed-modal retriever tailored for URAG scenarios. To mitigate the scarcity of realistic mixed-modal data, we introduce a four-stage automated pipeline for generation and filtering, leveraging web documents to construct NyxQA, a dataset comprising diverse mixed-modal question-answer pairs that better reflect real-world information needs. Building on this high-quality dataset, we adopt a two-stage training framework for Nyx: we first perform pre-training on NyxQA along with a variety of open-source retrieval datasets, followed by supervised fine-tuning using feedback from downstream vision-language models (VLMs) to align retrieval outputs with generative preferences. Experimental results demonstrate that Nyx not only performs competitively on standard text-only RAG benchmarks, but also excels in the more general and realistic URAG setting, significantly improving generation quality in vision-language tasks.",
        "translated": "检索增强生成（RAG）作为一种强有力的范式，通过从外部语料库中检索相关文档来增强大语言模型（LLM）的能力。然而，现有的RAG系统主要关注单模态文本文档，在现实场景中往往表现不足，尤其是在查询和文档可能包含混合模态信息（如文本和图像）的情况下。本文中，我们研究了通用检索增强生成（URAG）这一挑战，其目标是通过检索和推理混合模态信息来提升视觉-语言生成能力。为此，我们提出了Nyx，一种统一的混合模态到混合模态的检索器，专门针对URAG场景进行设计。为了缓解真实混合模态数据稀缺的问题，我们引入了一个包含四个阶段的自动化生成与过滤流水线，利用网络文档构建了NyxQA数据集，该数据集包含多样化的混合模态问答对，更准确地反映了现实中的信息需求。基于这一高质量数据集，我们为Nyx采用了两阶段训练框架：首先在NyxQA以及多个开源检索数据集上进行预训练，然后使用下游视觉-语言模型（VLM）的反馈进行监督微调，以使检索结果与生成偏好对齐。实验结果表明，Nyx不仅在标准的纯文本RAG基准上表现优异，而且在更通用和现实的URAG设置中也表现出色，显著提升了视觉-语言任务中的生成质量。",
        "translated_title": "迈向多模态召回的通用召回增强生成",
        "label": [
            "多模态推荐（Multimodal Recommendation）",
            "LLM生成式推荐（LLM-based Generative Recommendation）"
        ],
        "label_reason": "论文探讨多模态信息检索增强生成，适用于推荐系统中的生成式推荐场景。",
        "relevance_score": 6,
        "novelty_score": 8,
        "novelty_reason": "提出统一的多模态检索框架和数据生成方法，具有新颖性和实用性。"
    },
    {
        "title": "MemoryBench: A Benchmark for Memory and Continual Learning in LLM\n  Systems",
        "url": "http://arxiv.org/abs/2510.17281v1",
        "pub_date": "2025-10-20",
        "summary": "Scaling up data, parameters, and test-time computation has been the mainstream methods to improve LLM systems (LLMsys), but their upper bounds are almost reached due to the gradual depletion of high-quality data and marginal gains obtained from larger computational resource consumption. Inspired by the abilities of human and traditional AI systems in learning from practice, constructing memory and continual learning frameworks for LLMsys has become an important and popular research direction in recent literature. Yet, existing benchmarks for LLM memory often focus on evaluating the system on homogeneous reading comprehension tasks with long-form inputs rather than testing their abilities to learn from accumulated user feedback in service time. Therefore, we propose a user feedback simulation framework and a comprehensive benchmark covering multiple domains, languages, and types of tasks to evaluate the continual learning abilities of LLMsys. Experiments show that the effectiveness and efficiency of state-of-the-art baselines are far from satisfying, and we hope this benchmark could pave the way for future studies on LLM memory and optimization algorithms.",
        "translated": "扩大数据量、模型参数和测试时计算能力已成为提升大语言模型系统（LLMsys）的主流方法，但由于高质量数据的逐渐枯竭以及计算资源消耗所带来的收益递减，其性能上限几乎已达到。受到人类及传统人工智能系统通过实践学习能力的启发，为LLMsys构建记忆机制和持续学习框架已成为近期研究中的一个重要且热门的研究方向。然而，现有的LLM记忆基准往往集中在评估系统在长文本输入下的同质化阅读理解任务上，而非测试其在服务时间内从累积的用户反馈中进行学习的能力。因此，我们提出一个用户反馈模拟框架以及一个涵盖多个领域、语言和任务类型的综合基准，用于评估LLMsys的持续学习能力。实验表明，最先进的基线方法在有效性和效率方面远未达到令人满意的效果，我们希望这一基准能够为未来关于LLM记忆机制和优化算法的研究铺平道路。",
        "translated_title": "MemoryBench: 一个面向大语言模型系统中记忆与持续学习的基准测试",
        "label": [
            "LLM生成式推荐"
        ],
        "label_reason": "论文涉及LLM系统的持续学习和记忆能力，可能适用于生成式推荐。",
        "relevance_score": 4,
        "novelty_score": 7,
        "novelty_reason": "提出新的基准和用户反馈模拟框架，具有一定的创新性。"
    },
    {
        "title": "On Efficiency-Effectiveness Trade-off of Diffusion-based Recommenders",
        "url": "http://arxiv.org/abs/2510.17245v1",
        "pub_date": "2025-10-20",
        "summary": "Diffusion models have emerged as a powerful paradigm for generative sequential recommendation, which typically generate next items to recommend guided by user interaction histories with a multi-step denoising process. However, the multi-step process relies on discrete approximations, introducing discretization error that creates a trade-off between computational efficiency and recommendation effectiveness. To address this trade-off, we propose TA-Rec, a two-stage framework that achieves one-step generation by smoothing the denoising function during pretraining while alleviating trajectory deviation by aligning with user preferences during fine-tuning. Specifically, to improve the efficiency without sacrificing the recommendation performance, TA-Rec pretrains the denoising model with Temporal Consistency Regularization (TCR), enforcing the consistency between the denoising results across adjacent steps. Thus, we can smooth the denoising function to map the noise as oracle items in one step with bounded error. To further enhance effectiveness, TA-Rec introduces Adaptive Preference Alignment (APA) that aligns the denoising process with user preference adaptively based on preference pair similarity and timesteps. Extensive experiments prove that TA-Rec's two-stage objective effectively mitigates the discretization errors-induced trade-off, enhancing both efficiency and effectiveness of diffusion-based recommenders.",
        "translated": "扩散模型已成为生成式序列推荐领域的一个强大范式，通常通过用户交互历史，以多步骤去噪过程生成下一个推荐的物料。然而，该多步骤过程依赖于离散近似，从而引入了离散化误差，造成了计算效率与推荐效果之间的权衡问题。为了解决这一权衡问题，我们提出了 TA-Rec，一个两阶段框架，通过在预训练阶段平滑去噪函数实现一步生成，同时在微调阶段通过与用户偏好对齐，缓解轨迹偏移问题。具体而言，为了在不牺牲推荐性能的前提下提高效率，TA-Rec 使用时间一致性正则化（Temporal Consistency Regularization, TCR）对去噪模型进行预训练，强制相邻步骤之间的去噪结果保持一致性。因此，我们可以平滑去噪函数，从而在一步内将噪声映射为理想物料，且误差被控制在一定范围内。为进一步增强推荐效果，TA-Rec 引入了自适应偏好对齐（Adaptive Preference Alignment, APA），根据偏好对相似性及时刻自适应地将去噪过程与用户偏好对齐。大量实验表明，TA-Rec 的两阶段目标有效地缓解了由离散化误差引起的权衡问题，提升了基于扩散模型的推荐系统的效率和效果。",
        "translated_title": "基于扩散的推荐系统的效率与效果权衡",
        "label": [
            "LLM生成式推荐",
            "序列推荐"
        ],
        "label_reason": "论文研究基于扩散模型的生成式序列推荐，与推荐系统核心问题直接相关。",
        "relevance_score": 8,
        "novelty_score": 7,
        "novelty_reason": "提出TA-Rec框架，通过平滑去噪函数和自适应偏好对齐缓解效率-效果权衡。"
    },
    {
        "title": "DSEBench: A Test Collection for Explainable Dataset Search with Examples",
        "url": "http://arxiv.org/abs/2510.17228v1",
        "pub_date": "2025-10-20",
        "summary": "Dataset search has been an established information retrieval task. Current paradigms either retrieve datasets that are relevant to a keyword query or find datasets that are similar to an input target dataset. To allow for their combined specification of information needs, in this article, we investigate the more generalized task of Dataset Search with Examples (DSE) and further extend it to Explainable DSE that requires identifying the metadata and content fields of a dataset that indicate its relevance to the query and similarity to the target datasets. To facilitate this research, we construct DSEBench, a test collection that provides high-quality dataset- and field-level annotations to enable the evaluation of explainable DSE. We also employ a large language model to generate numerous annotations to be used for training. We establish extensive baselines on DSEBench by adapting and evaluating a variety of sparse, dense, and LLM-based retrieval, reranking, and explanation methods.",
        "translated": "数据集检索已成为一项成熟的信息检索任务。当前的方法要么检索与关键字查询相关的数据集，要么查找与输入目标数据集相似的数据集。为了能够综合表达用户的信息需求，本文我们研究了更通用的数据集检索与示例任务（Dataset Search with Examples, DSE），并进一步将其扩展为可解释的DSE，该任务要求识别数据集中指示其与查询相关性以及与目标数据集相似性的元数据和内容字段。为推动该领域的研究，我们构建了DSEBench测试集合，提供高质量的数据集级别和字段级别的标注，以支持可解释DSE的评估。我们还利用大语言模型生成大量标注用于训练。我们在DSEBench上建立了广泛的基线，通过调整和评估多种稀疏、稠密以及基于大语言模型的召回、重排和解释方法。",
        "translated_title": "DSEBench：一个包含示例的可解释数据集检索测试集",
        "label": [
            "推荐系统评估",
            "解释性"
        ],
        "label_reason": "论文涉及可解释数据集检索，与推荐系统评估有一定关联",
        "relevance_score": 5,
        "novelty_score": 7,
        "novelty_reason": "提出可解释数据集搜索新任务并构建高质量基准"
    },
    {
        "title": "Rethinking On-policy Optimization for Query Augmentation",
        "url": "http://arxiv.org/abs/2510.17139v1",
        "pub_date": "2025-10-20",
        "summary": "Recent advances in large language models (LLMs) have led to a surge of interest in query augmentation for information retrieval (IR). Two main approaches have emerged. The first prompts LLMs to generate answers or pseudo-documents that serve as new queries, relying purely on the model's parametric knowledge or contextual information. The second applies reinforcement learning (RL) to fine-tune LLMs for query rewriting, directly optimizing retrieval metrics. While having respective advantages and limitations, the two approaches have not been compared under consistent experimental conditions. In this work, we present the first systematic comparison of prompting-based and RL-based query augmentation across diverse benchmarks, including evidence-seeking, ad hoc, and tool retrieval. Our key finding is that simple, training-free query augmentation often performs on par with, or even surpasses, more expensive RL-based counterparts, especially when using powerful LLMs. Motivated by this discovery, we introduce a novel hybrid method, On-policy Pseudo-document Query Expansion (OPQE), which, instead of rewriting a query, the LLM policy learns to generate a pseudo-document that maximizes retrieval performance, thus merging the flexibility and generative structure of prompting with the targeted optimization of RL. We show OPQE outperforms both standalone prompting and RL-based rewriting, demonstrating that a synergistic approach yields the best results. Our implementation is made available to facilitate reproducibility.",
        "translated": "近年来，大语言模型（LLMs）的进展引发了信息检索（IR）领域对查询增强（query augmentation）的广泛关注。目前主要有两种方法。第一种方法通过提示LLM生成答案或伪文档作为新的查询，完全依赖模型的参数化知识或上下文信息。第二种方法则利用强化学习（RL）对LLMs进行微调以实现查询重写，直接优化检索指标。尽管两种方法各有优势和局限性，但它们尚未在一致的实验条件下进行比较。在本文中，我们首次在多个基准上对基于提示（prompting-based）和基于强化学习（RL-based）的查询增强方法进行了系统性比较，包括证据寻求、临时检索和工具检索等任务。我们的主要发现是，简单且无需训练的查询增强方法通常可以与，甚至超越，代价更高的基于强化学习的对应方法，尤其是在使用功能强大的LLMs时。受此发现的启发，我们提出了一种新颖的混合方法——基于策略的伪文档查询扩展（On-policy Pseudo-document Query Expansion，OPQE），该方法不依赖于查询重写，而是通过学习LLM策略生成伪文档以最大化检索性能，从而将提示方法的灵活性与生成结构和强化学习的目标优化结合起来。我们展示了OPQE优于独立的提示方法和基于强化学习的重写方法，表明协同的方法能够取得最佳效果。我们已开源实现代码以促进结果复现。",
        "translated_title": "Rethinking On-policy Optimization for Query Augmentation  \n重新思考基于策略的查询增强优化方法",
        "label": [
            "通用推荐技术",
            "LLM生成式推荐"
        ],
        "label_reason": "论文涉及LLM生成伪文档用于检索，与生成式推荐有一定关联。",
        "relevance_score": 6,
        "novelty_score": 7,
        "novelty_reason": "提出一种结合提示和强化学习的新型混合方法，具有一定创新性。"
    },
    {
        "title": "Towards Context-aware Reasoning-enhanced Generative Searching in\n  E-commerce",
        "url": "http://arxiv.org/abs/2510.16925v1",
        "pub_date": "2025-10-19",
        "summary": "Search-based recommendation is one of the most critical application scenarios in e-commerce platforms. Users' complex search contexts--such as spatiotemporal factors, historical interactions, and current query's information--constitute an essential part of their decision-making, reflecting implicit preferences that complement explicit query terms. Modeling such rich contextual signals and their intricate associations with candidate items remains a key challenge. Although numerous efforts have been devoted to building more effective search methods, existing approaches still show limitations in integrating contextual information, which hinders their ability to fully capture user intent.   To address these challenges, we propose a context-aware reasoning-enhanced generative search framework for better \\textbf{understanding the complicated context}. Specifically, the framework first unifies heterogeneous user and item contexts into textual representations or text-based semantic identifiers and aligns them. To overcome the lack of explicit reasoning trajectories, we introduce a self-evolving post-training paradigm that iteratively combines supervised fine-tuning and reinforcement learning to progressively enhance the model's reasoning capability. In addition, we identify potential biases in existing RL algorithms when applied to search scenarios and present a debiased variant of GRPO to improve ranking performance. Extensive experiments on search log data collected from a real-world e-commerce platform demonstrate that our approach achieves superior performance compared with strong baselines, validating its effectiveness for search-based recommendation.",
        "translated": "基于搜索的推荐是电子商务平台中最关键的应用场景之一。用户复杂的搜索上下文——例如时空因素、历史交互以及当前查询的信息——构成了其决策过程的重要组成部分，反映了对显式查询词的补充隐式偏好。如何建模这些丰富的上下文信号以及它们与候选物料之间复杂的关联关系，仍然是一个关键挑战。尽管已有大量研究致力于构建更有效的搜索方法，但现有方法在整合上下文信息方面仍存在局限，这限制了其全面捕捉用户意图的能力。\n\n为应对这些挑战，我们提出了一种面向搜索的上下文感知推理增强生成式框架，以更好地**理解复杂的上下文**。具体而言，该框架首先将异构的用户和物料上下文统一为文本表示或基于文本的语义标识符，并进行对齐。为了克服缺乏显式推理轨迹的问题，我们引入了一种自我演进的后训练范式，该范式通过迭代结合监督微调和强化学习，逐步增强模型的推理能力。此外，我们识别了现有强化学习算法在应用于搜索场景时可能存在的偏差问题，并提出了去偏的GRPO变体，以提升排序效果。在从真实电子商务平台收集的搜索日志数据上的大量实验表明，与强基线方法相比，我们的方法实现了更优越的性能，验证了其在基于搜索推荐中的有效性。",
        "translated_title": "面向电子商务中上下文感知与推理增强的生成式搜索",
        "label": [
            "LLM生成式推荐",
            "推荐系统评估",
            "序列推荐"
        ],
        "label_reason": "论文聚焦电商场景下的搜索推荐，结合上下文和生成式方法，具有较强相关性。",
        "relevance_score": 8,
        "novelty_score": 7,
        "novelty_reason": "提出结合上下文感知和推理增强的生成式搜索框架，具有一定创新性。"
    },
    {
        "title": "The Layout Is the Model: On Action-Item Coupling in Generative\n  Recommendation",
        "url": "http://arxiv.org/abs/2510.16804v1",
        "pub_date": "2025-10-19",
        "summary": "Generative Recommendation (GR) models treat a user's interaction history as a sequence to be autoregressively predicted. When both items and actions (e.g., watch time, purchase, comment) are modeled, the layout-the ordering and visibility of item/action tokens-critically determines what information the model can use and how it generalizes. We present a unified study of token layouts for GR grounded in first principles: (P1) maximize item/action signal in both input/output space, (P2) preserve the conditioning relationship \"action given item\" and (P3) no information leakage.   While interleaved layout (where item and action occupy separate tokens) naturally satisfies these principles, it also bloats sequence length with larger training/inference cost. On the non-interleaved front, we design a novel and effective approach, Lagged Action Conditioning (LAC), which appears strange on the surface but aligns well with the design principles to yield strong accuracy. Comprehensive experiments on public datasets and large-scale production logs evaluate different layout options and empirically verifies the design principles. Our proposed non-interleaved method, LAC, achieves competitive or superior quality at substantially lower FLOPs than interleaving. Our findings offer actionable guidance for assembling GR systems that are both accurate and efficient.",
        "translated": "生成式推荐（GR）模型将用户的历史交互视为一个需要自回归预测的序列。当同时建模物料和行为（例如观看时长、购买、评论）时，布局——即物料/行为标记的顺序和可见性——在很大程度上决定了模型可以使用的哪些信息以及其泛化能力。我们基于第一性原理对GR的标记布局进行了统一研究：（P1）在输入/输出空间中最大化物料/行为的信号，（P2）保持“在给定物料下预测行为”的条件关系，以及（P3）防止信息泄露。尽管交错布局（物料和行为分别占据不同的标记）自然地满足了这些原理，但它也会增加序列长度并带来更高的训练和推理成本。在非交错布局方面，我们设计了一种新颖且有效的方法——延迟行为条件建模（LAC），这种方法表面上看起来有些奇怪，但其与设计原理高度一致，从而实现了较强的准确性。我们在公开数据集和大规模生产日志上进行了全面的实验，评估了不同的布局选项，并实证验证了这些设计原理。我们提出的非交错方法LAC在计算量（FLOPs）显著低于交错布局的情况下，实现了具有竞争力甚至更优的推荐质量。我们的研究结果为构建既准确又高效的GR系统提供了切实可行的指导。",
        "translated_title": "“布局即模型”：生成式推荐中的动作-物料耦合问题",
        "label": [
            "LLM生成式推荐",
            "序列推荐"
        ],
        "label_reason": "论文聚焦生成式推荐中的序列建模与token布局设计，对推荐系统有直接应用价值。",
        "relevance_score": 8,
        "novelty_score": 7,
        "novelty_reason": "提出新颖的非交错布局方法LAC，在生成式推荐中展现强效且低计算成本。"
    },
    {
        "title": "An Efficient Framework for Whole-Page Reranking via Single-Modal\n  Supervision",
        "url": "http://arxiv.org/abs/2510.16803v1",
        "pub_date": "2025-10-19",
        "summary": "The whole-page reranking plays a critical role in shaping the user experience of search engines, which integrates retrieval results from multiple modalities, such as documents, images, videos, and LLM outputs. Existing methods mainly rely on large-scale human-annotated data, which is costly to obtain and time-consuming. This is because whole-page annotation is far more complex than single-modal: it requires assessing the entire result page while accounting for cross-modal relevance differences. Thus, how to improve whole-page reranking performance while reducing annotation costs is still a key challenge in optimizing search engine result pages(SERP). In this paper, we propose SMAR, a novel whole-page reranking framework that leverages strong Single-modal rankers to guide Modal-wise relevance Alignment for effective Reranking, using only limited whole-page annotation to outperform fully-annotated reranking models. Specifically, high-quality single-modal rankers are first trained on data specific to their respective modalities. Then, for each query, we select a subset of their outputs to construct candidate pages and perform human annotation at the page level. Finally, we train the whole-page reranker using these limited annotations and enforcing consistency with single-modal preferences to maintain ranking quality within each modality. Experiments on the Qilin and Baidu datasets demonstrate that SMAR reduces annotation costs by about 70-90\\% while achieving significant ranking improvements compared to baselines. Further offline and online A/B testing on Baidu APPs also shows notable gains in standard ranking metrics as well as user experience indicators, fully validating the effectiveness and practical value of our approach in real-world search scenarios.",
        "translated": "整页重排在塑造搜索引擎用户体验方面起着关键作用，它将来自多种模态的检索结果（如文档、图像、视频和大语言模型输出）进行整合。现有方法主要依赖于大规模的人工标注数据，这在获取成本和耗时方面都较高。这是因为整页标注远比单模态标注复杂：它需要评估整个结果页面，同时考虑跨模态的相关性差异。因此，如何在降低标注成本的同时提升整页重排性能，仍然是优化搜索引擎结果页面（SERP）中的关键挑战。在本文中，我们提出了SMAR，一种新颖的整页重排框架，该框架利用强大的单模态排序器来引导模态间的相关性对齐以实现有效的重排，仅使用有限的整页标注即可超越完全标注的重排模型。具体而言，首先在各自模态的专用数据上训练高质量的单模态排序器。然后，对于每个查询，我们选择其输出的子集以构建候选页面，并在页面层面进行人工标注。最后，我们使用这些有限的标注数据训练整页重排模型，并通过强制保持与单模态偏好的一致性，以维持各模态内的排序质量。在Qilin和Baidu数据集上的实验表明，SMAR在实现显著排序性能提升的同时，可将标注成本降低约70-90\\%。进一步在百度APP上的离线和在线A/B测试也显示了在标准排序指标和用户体验指标上的显著提升，充分验证了我们的方法在实际搜索引擎场景中的有效性和实用价值。",
        "translated_title": "通过单模态监督实现高效的整页重排框架",
        "label": [
            "重排（Re-ranking）",
            "推荐系统评估（Evaluation Metrics / Offline/Online Testing）"
        ],
        "label_reason": "论文聚焦于搜索结果页重排，与推荐系统中的重排环节相关。",
        "relevance_score": 7,
        "novelty_score": 8,
        "novelty_reason": "提出通过单模态排序器引导多模态对齐的重排框架，方法新颖且有效。"
    },
    {
        "title": "Exact Nearest-Neighbor Search on Energy-Efficient FPGA Devices",
        "url": "http://arxiv.org/abs/2510.16736v1",
        "pub_date": "2025-10-19",
        "summary": "This paper investigates the usage of FPGA devices for energy-efficient exact kNN search in high-dimension latent spaces. This work intercepts a relevant trend that tries to support the increasing popularity of learned representations based on neural encoder models by making their large-scale adoption greener and more inclusive. The paper proposes two different energy-efficient solutions adopting the same FPGA low-level configuration. The first solution maximizes system throughput by processing the queries of a batch in parallel over a streamed dataset not fitting into the FPGA memory. The second minimizes latency by processing each kNN incoming query in parallel over an in-memory dataset. Reproducible experiments on publicly available image and text datasets show that our solution outperforms state-of-the-art CPU-based competitors regarding throughput, latency, and energy consumption. Specifically, experiments show that the proposed FPGA solutions achieve the best throughput in terms of queries per second and the best-observed latency with scale-up factors of up to 16.6X. Similar considerations can be made regarding energy efficiency, where results show that our solutions can achieve up to 11.9X energy saving w.r.t. strong CPU-based competitors.",
        "translated": "本文研究了使用FPGA设备在高维隐空间中进行节能精确kNN搜索的应用。这项工作抓住了一个相关趋势，即通过使基于神经编码器模型的表示学习的大规模采用更加绿色和包容，以支持其日益增长的流行。论文提出了两种不同的节能解决方案，它们采用相同的FPGA底层配置。第一种解决方案通过在一个无法完全加载到FPGA内存中的流式数据集上并行处理一批查询，以最大化系统吞吐量。第二种解决方案则通过在一个内存中的数据集上并行处理每个传入的kNN查询，以最小化延迟。在公开可用的图像和文本数据集上进行的可复现实验表明，我们的解决方案在吞吐量、延迟和能耗方面均优于当前最先进的基于CPU的竞争对手。具体而言，实验表明，所提出的FPGA解决方案在每秒查询数方面实现了最佳吞吐量，并观察到在扩展因子高达16.6倍的情况下，具有最低的延迟。关于能效方面也有类似的结论，结果表明，与强大的基于CPU的解决方案相比，我们的解决方案最多可实现11.9倍的能耗节省。",
        "translated_title": "节能FPGA设备上的精确最近邻搜索",
        "label": [],
        "label_reason": "论文聚焦于FPGA上的kNN搜索优化，与推荐系统无直接关联。",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出两种FPGA节能方案，实验设计完整，但属于通用计算优化领域。"
    },
    {
        "title": "Right Answer at the Right Time - Temporal Retrieval-Augmented Generation\n  via Graph Summarization",
        "url": "http://arxiv.org/abs/2510.16715v1",
        "pub_date": "2025-10-19",
        "summary": "Question answering in temporal knowledge graphs requires retrieval that is both time-consistent and efficient. Existing RAG methods are largely semantic and typically neglect explicit temporal constraints, which leads to time-inconsistent answers and inflated token usage. We propose STAR-RAG, a temporal GraphRAG framework that relies on two key ideas: building a time-aligned rule graph and conducting propagation on this graph to narrow the search space and prioritize semantically relevant, time-consistent evidence. This design enforces temporal proximity during retrieval, reduces the candidate set of retrieval results, and lowers token consumption without sacrificing accuracy. Compared with existing temporal RAG approaches, STAR-RAG eliminates the need for heavy model training and fine-tuning, thereby reducing computational cost and significantly simplifying deployment.Extensive experiments on real-world temporal KG datasets show that our method achieves improved answer accuracy while consuming fewer tokens than strong GraphRAG baselines.",
        "translated": "时序知识图谱中的问答任务需要既具备时间一致性又高效的数据召回能力。现有的 RAG 方法大多基于语义，通常忽视了显式的时间约束，这导致了时间不一致的回答和较高的 token 使用量。我们提出了 STAR-RAG，一种时序 GraphRAG 框架，其依赖于两个核心思想：构建一个时间对齐的规则图，并在此图上进行传播以缩小搜索空间并优先考虑语义相关且时间一致的证据。这种设计在召回过程中强制执行时间邻近性，减少了召回结果的候选集合，并在不牺牲准确性的情况下降低了 token 消耗。与现有的时序 RAG 方法相比，STAR-RAG 消除了对繁重模型训练和微调的需求，从而降低了计算成本并大大简化了部署。在真实世界的时序知识图谱数据集上的大量实验表明，我们的方法在保持较低 token 消耗的同时，实现了优于强大 GraphRAG 基线的回答准确性。",
        "translated_title": "Right Answer at the Right Time - Temporal Retrieval-Augmented Generation via Graph Summarization  \n在正确的时间给出正确的答案——基于图摘要的时间检索增强生成",
        "label": [
            "LLM生成式推荐",
            "多模态推荐"
        ],
        "label_reason": "涉及基于图的检索增强生成，适用于生成式推荐场景",
        "relevance_score": 7,
        "novelty_score": 8,
        "novelty_reason": "提出时间对齐规则图和传播机制，改进现有RAG方法"
    },
    {
        "title": "Resolution-Aware Retrieval Augmented Zero-Shot Forecasting",
        "url": "http://arxiv.org/abs/2510.16695v1",
        "pub_date": "2025-10-19",
        "summary": "Zero-shot forecasting aims to predict outcomes for previously unseen conditions without direct historical data, posing a significant challenge for traditional forecasting methods. We introduce a Resolution-Aware Retrieval-Augmented Forecasting model that enhances predictive accuracy by leveraging spatial correlations and temporal frequency characteristics. By decomposing signals into different frequency components, our model employs resolution-aware retrieval, where lower-frequency components rely on broader spatial context, while higher-frequency components focus on local influences. This allows the model to dynamically retrieve relevant data and adapt to new locations with minimal historical context.   Applied to microclimate forecasting, our model significantly outperforms traditional forecasting methods, numerical weather prediction models, and modern foundation time series models, achieving 71% lower MSE than HRRR and 34% lower MSE than Chronos on the ERA5 dataset.   Our results highlight the effectiveness of retrieval-augmented and resolution-aware strategies, offering a scalable and data-efficient solution for zero-shot forecasting in microclimate modeling and beyond.",
        "translated": "零样本预测旨在在没有直接历史数据的情况下预测此前未见过的条件下的结果，这对传统预测方法提出了重大挑战。我们引入了一种分辨率感知的检索增强预测模型，该模型通过利用空间相关性和时间频率特征来提高预测准确性。通过将信号分解为不同的频率成分，我们的模型采用分辨率感知的检索策略，其中低频成分依赖于更广泛的空间上下文，而高频成分则关注局部影响。这使得模型能够动态检索相关数据，并在仅有少量历史信息的情况下适应新的位置。\n\n在微气候预测中的应用表明，我们的模型显著优于传统预测方法、数值天气预测模型以及现代基础时间序列模型，在ERA5数据集上，其均方误差（MSE）比HRRR低71%，比Chronos低34%。我们的结果突显了检索增强和分辨率感知策略的有效性，为微气候建模及其他领域的零样本预测提供了一种可扩展且数据高效的解决方案。",
        "translated_title": "分辨率感知的召回增强零样本预测",
        "label": [
            "召回（Recall）",
            "通用推荐技术（General Recommendation Techniques）"
        ],
        "label_reason": "论文涉及基于检索的策略，可用于推荐系统的召回环节。",
        "relevance_score": 4,
        "novelty_score": 7,
        "novelty_reason": "提出分辨率感知的检索增强方法，对零样本预测有明显改进。"
    },
    {
        "title": "Safire: Similarity Framework for Visualization Retrieval",
        "url": "http://arxiv.org/abs/2510.16662v1",
        "pub_date": "2025-10-18",
        "summary": "Effective visualization retrieval necessitates a clear definition of similarity. Despite the growing body of work in specialized visualization retrieval systems, a systematic approach to understanding visualization similarity remains absent. We introduce the Similarity Framework for Visualization Retrieval (Safire), a conceptual model that frames visualization similarity along two dimensions: comparison criteria and representation modalities. Comparison criteria identify the aspects that make visualizations similar, which we divide into primary facets (data, visual encoding, interaction, style, metadata) and derived properties (data-centric and human-centric measures). Safire connects what to compare with how comparisons are executed through representation modalities. We categorize existing representation approaches into four groups based on their levels of information content and visualization determinism: raster image, vector image, specification, and natural language description, together guiding what is computable and comparable. We analyze several visualization retrieval systems using Safire to demonstrate its practical value in clarifying similarity considerations. Our findings reveal how particular criteria and modalities align across different use cases. Notably, the choice of representation modality is not only an implementation detail but also an important decision that shapes retrieval capabilities and limitations. Based on our analysis, we provide recommendations and discuss broader implications for multimodal learning, AI applications, and visualization reproducibility.",
        "translated": "有效的可视化召回需要对相似性有明确的定义。尽管在专用的可视化召回系统方面已有越来越多的研究，但在理解可视化相似性方面仍缺乏系统的方法。我们提出了可视化召回相似性框架（Safire），这是一个概念模型，从两个维度构建可视化相似性：比较标准和表示模态。比较标准识别使可视化之间产生相似性的方面，我们将其划分为基本维度（数据、视觉编码、交互、风格、元数据）和衍生属性（以数据为中心和以用户为中心的度量）。Safire通过表示模态将比较内容与比较方式联系起来。我们根据信息含量和可视化确定性的水平，将现有的表示方法分为四类：光栅图像、矢量图像、规范描述和自然语言描述，共同指导了哪些内容是可计算和可比较的。我们使用Safire分析了多个可视化召回系统，以展示其在澄清相似性考虑方面的实用价值。我们的发现揭示了特定的标准和模态在不同应用场景中的对齐方式。值得注意的是，表示模态的选择不仅是一个实现细节，也是一个重要的决策，它塑造了召回系统的能力和局限性。基于我们的分析，我们提供了建议，并讨论了其对多模态学习、人工智能应用和可视化可复现性的更广泛影响。",
        "translated_title": "Safire：面向可视化召回的相似度框架",
        "label": [
            "多模态推荐"
        ],
        "label_reason": "论文涉及多模态表征与相似度计算，但主要聚焦于可视化检索而非推荐系统",
        "relevance_score": 3,
        "novelty_score": 6,
        "novelty_reason": "提出系统性可视化相似度框架，对多模态表示有清晰分类"
    },
    {
        "title": "Prompt Optimization via Retrieved Reasoning Assets and Multi-Agent\n  Analysis",
        "url": "http://arxiv.org/abs/2510.16635v1",
        "pub_date": "2025-10-18",
        "summary": "Prompt optimization has emerged as an effective alternative to retraining for improving the performance of Large Language Models (LLMs). However, most existing approaches treat evaluation as a black box, relying solely on numerical scores while offering limited insight into why a prompt succeeds or fails. They also depend heavily on trial-and-error refinements, which are difficult to interpret and control. In this paper, we introduce MA-SAPO, a Multi-Agent framework for Score-Aware Prompt Optimization. Compared to prior methods, MA-SAPO explicitly couples evaluation outcomes with structured reasoning to guide systematic edits. The framework specifically consists of two stages: during the Reasoning Phase, agents collaboratively explain metric scores, diagnose weaknesses, and synthesize targeted refinements that are stored as reusable reasoning assets; during the Test Phase, agents retrieve these assets to analyze optimized prompts and apply only evidence-grounded edits. By turning evaluation signals into interpretable reasoning chains, MA-SAPO produces prompt refinements that are more transparent, auditable, and controllable. Experiments on the HelpSteer1/2 benchmarks demonstrate consistent improvements over single-pass prompting, retrieval-augmented baselines, and prior multi-agent strategies, validating the effectiveness of our approach.",
        "translated": "Prompt优化已逐渐成为改进大语言模型（LLM）性能、替代模型微调的有效方法。然而，大多数现有方法将评估视为黑箱，仅依赖数值评分，而对Prompt成功或失败的原因提供有限的洞察。它们也高度依赖于试错式的优化，难以解释和控制。在本文中，我们提出了MA-SAPO，一个多智能体框架，用于实现评分感知的Prompt优化。与之前的方法相比，MA-SAPO明确地将评估结果与结构化推理相结合，以指导系统性的Prompt修改。该框架具体包含两个阶段：在推理阶段，智能体协作解释评估指标、诊断模型弱点，并综合生成有针对性的优化策略，这些策略以可复用的推理资产形式存储；在测试阶段，智能体检索这些资产以分析优化后的Prompt，并仅应用基于证据的修改。通过将评估信号转化为可解释的推理链，MA-SAPO生成的Prompt优化策略更加透明、可审计且可控。我们在HelpSteer1/2基准上的实验表明，MA-SAPO在单次Prompt、检索增强基线以及之前多智能体策略的基础上取得了持续的性能提升，验证了我们方法的有效性。",
        "translated_title": "基于检索推理资产和多智能体分析的提示优化",
        "label": [],
        "label_reason": "论文聚焦于大模型提示优化，未直接涉及推荐系统技术。",
        "relevance_score": 3,
        "novelty_score": 7,
        "novelty_reason": "提出多智能体框架，改进提示优化方法，具有创新性。"
    },
    {
        "title": "FRONTIER-RevRec: A Large-scale Dataset for Reviewer Recommendation",
        "url": "http://arxiv.org/abs/2510.16597v1",
        "pub_date": "2025-10-18",
        "summary": "Reviewer recommendation is a critical task for enhancing the efficiency of academic publishing workflows. However, research in this area has been persistently hindered by the lack of high-quality benchmark datasets, which are often limited in scale, disciplinary scope, and comparative analyses of different methodologies. To address this gap, we introduce FRONTIER-RevRec, a large-scale dataset constructed from authentic peer review records (2007-2025) from the Frontiers open-access publishing platform https://www.frontiersin.org/. The dataset contains 177941 distinct reviewers and 478379 papers across 209 journals spanning multiple disciplines including clinical medicine, biology, psychology, engineering, and social sciences. Our comprehensive evaluation on this dataset reveals that content-based methods significantly outperform collaborative filtering. This finding is explained by our structural analysis, which uncovers fundamental differences between academic recommendation and commercial domains. Notably, approaches leveraging language models are particularly effective at capturing the semantic alignment between a paper's content and a reviewer's expertise. Furthermore, our experiments identify optimal aggregation strategies to enhance the recommendation pipeline. FRONTIER-RevRec is intended to serve as a comprehensive benchmark to advance research in reviewer recommendation and facilitate the development of more effective academic peer review systems. The FRONTIER-RevRec dataset is available at: https://anonymous.4open.science/r/FRONTIER-RevRec-5D05.",
        "translated": "评审推荐是提升学术出版流程效率的关键任务。然而，该领域的研究一直受到高质量基准数据集缺乏的阻碍，现有数据集通常在规模、学科覆盖范围以及不同方法的对比分析方面存在局限。为了解决这一问题，我们提出了 FRONTIER-RevRec，这是一个大规模数据集，由 Frontiers 开放获取出版平台（https://www.frontiersin.org/）提供的真实同行评审记录（2007-2025）构建而成。该数据集包含 177941 位不同的审稿人和 478379 篇论文，涵盖 209 个期刊，涉及临床医学、生物学、心理学、工程学和社会科学等多个学科。我们在该数据集上的综合评估表明，基于内容的方法显著优于协同过滤方法。这一发现通过我们的结构分析得到解释，该分析揭示了学术推荐与商业领域之间的基本差异。值得注意的是，利用语言模型的方法在捕捉论文内容与审稿人专业领域之间的语义一致性方面特别有效。此外，我们的实验识别出最优的聚合策略，以提升推荐流程的性能。FRONTIER-RevRec 旨在作为全面的基准数据集，推动评审推荐研究的发展，并促进更有效的学术同行评审系统的构建。FRONTIER-RevRec 数据集可通过以下链接获取：https://anonymous.4open.science/r/FRONTIER-RevRec-5D05。",
        "translated_title": "FRONTIER-RevRec：一个用于审稿人推荐的大型数据集",
        "label": [
            "通用推荐技术（General Recommendation Techniques）"
        ],
        "label_reason": "论文聚焦推荐系统中的评审人推荐，属于学术推荐的通用技术范畴。",
        "relevance_score": 7,
        "novelty_score": 6,
        "novelty_reason": "提出大规模数据集并分析方法性能，有一定实用价值但创新性有限。"
    },
    {
        "title": "Enhancing Channel Estimation in RIS-aided Systems via Observation Matrix\n  Design",
        "url": "http://arxiv.org/abs/2510.16576v1",
        "pub_date": "2025-10-18",
        "summary": "Reconfigurable intelligent surfaces (RISs) have emerged as a promising technology for enhancing wireless communications through dense antenna arrays. Accurate channel estimation is critical to unlocking their full performance potential. To enhance RIS channel estimators, this paper proposes a novel observation matrix design scheme. Bayesian optimization framework is adopted to generate observation matrices that maximize the mutual information between received pilot signals and RIS channels. To solve the formulated problem efficiently, we develop an alternating Riemannian manifold optimization (ARMO) algorithm to alternately update the receiver combiners and RIS phase-shift matrices. An adaptive kernel training strategy is further introduced to iteratively refine the channel covariance matrix without requiring additional pilot resources. Simulation results demonstrate that the proposed ARMO-enhanced estimator achieves substantial gains in estimation accuracy over state-of-the-art methods.",
        "translated": "可重构智能表面（RISs）作为一种有前景的技术，通过密集天线阵列来增强无线通信。准确的信道估计对于发挥其全部性能潜力至关重要。为提升RIS信道估计器的性能，本文提出了一种新颖的观测矩阵设计方案。采用贝叶斯优化框架生成观测矩阵，以最大化接收到的导频信号与RIS信道之间的互信息。为高效求解所构建的问题，我们开发了一种交替黎曼流形优化（ARMO）算法，用于交替更新接收端组合器和RIS相移矩阵。此外，引入了一种自适应核训练策略，可在不增加额外导频资源的情况下，迭代优化信道协方差矩阵。仿真结果表明，所提出的ARMO增强估计器在估计精度方面相比现有最先进方法有显著提升。",
        "translated_title": "通过观测矩阵设计提升RIS辅助系统中的信道估计",
        "label": [],
        "label_reason": "论文研究RIS信道估计，属于通信领域，与推荐系统无直接关联。",
        "relevance_score": 1,
        "novelty_score": 6,
        "novelty_reason": "提出了一种基于贝叶斯优化的观测矩阵设计方法，具有一定的技术新颖性。"
    },
    {
        "title": "Blending Learning to Rank and Dense Representations for Efficient and\n  Effective Cascades",
        "url": "http://arxiv.org/abs/2510.16393v1",
        "pub_date": "2025-10-18",
        "summary": "We investigate the exploitation of both lexical and neural relevance signals for ad-hoc passage retrieval. Our exploration involves a large-scale training dataset in which dense neural representations of MS-MARCO queries and passages are complemented and integrated with 253 hand-crafted lexical features extracted from the same corpus. Blending of the relevance signals from the two different groups of features is learned by a classical Learning-to-Rank (LTR) model based on a forest of decision trees. To evaluate our solution, we employ a pipelined architecture where a dense neural retriever serves as the first stage and performs a nearest-neighbor search over the neural representations of the documents. Our LTR model acts instead as the second stage that re-ranks the set of candidates retrieved by the first stage to enhance effectiveness. The results of reproducible experiments conducted with state-of-the-art dense retrievers on publicly available resources show that the proposed solution significantly enhances the end-to-end ranking performance while relatively minimally impacting efficiency. Specifically, we achieve a boost in nDCG@10 of up to 11% with an increase in average query latency of only 4.3%. This confirms the advantage of seamlessly combining two distinct families of signals that mutually contribute to retrieval effectiveness.",
        "translated": "我们研究了在即席段落召回中同时利用词汇相关性和神经相关性信号的问题。我们的探索涉及一个大规模训练数据集，其中MS-MARCO查询和段落的密集神经表示与从同一语料库中提取的253个手工构建的词汇特征相结合并融合。我们通过一个基于决策树森林的经典学习排序（LTR）模型，来学习融合这两组特征的相关性信号。为了评估我们的解决方案，我们采用了一种流水线架构，其中密集神经召回器作为第一阶段，通过对文档的神经表示进行最近邻搜索来完成初步召回。而我们的LTR模型则作为第二阶段，对第一阶段召回的候选集合进行重排以提升效果。在公开可用资源上使用最先进的密集召回器进行的可复现实验结果表明，所提出的方案在端到端排序性能上有显著提升，同时对效率的影响相对较小。具体而言，我们实现了nDCG@10提升高达11%，而平均查询延迟仅增加了4.3%。这证实了将两种不同类别的信号无缝融合以共同提升召回效果的优势。",
        "translated_title": "融合排序学习与密集表征以实现高效且有效的级联系统",
        "label": [
            "精排（Ranking）",
            "重排（Re-ranking）",
            "通用推荐技术（General Recommendation Techniques）"
        ],
        "label_reason": "论文研究了排序与密集表示的结合，适用于推荐系统中的重排与排序环节。",
        "relevance_score": 7,
        "novelty_score": 6,
        "novelty_reason": "将传统LTR与密集表示结合，有一定组合创新但非突破性方法。"
    },
    {
        "title": "Investigating the Association Between Text-Based Indications of\n  Foodborne Illness from Yelp Reviews and New York City Health Inspection\n  Outcomes (2023)",
        "url": "http://arxiv.org/abs/2510.16334v1",
        "pub_date": "2025-10-18",
        "summary": "Foodborne illnesses are gastrointestinal conditions caused by consuming contaminated food. Restaurants are critical venues to investigate outbreaks because they share sourcing, preparation, and distribution of foods. Public reporting of illness via formal channels is limited, whereas social media platforms host abundant user-generated content that can provide timely public health signals. This paper analyzes signals from Yelp reviews produced by a Hierarchical Sigmoid Attention Network (HSAN) classifier and compares them with official restaurant inspection outcomes issued by the New York City Department of Health and Mental Hygiene (NYC DOHMH) in 2023. We evaluate correlations at the Census tract level, compare distributions of HSAN scores by prevalence of C-graded restaurants, and map spatial patterns across NYC. We find minimal correlation between HSAN signals and inspection scores at the tract level and no significant differences by number of C-graded restaurants. We discuss implications and outline next steps toward address-level analyses.",
        "translated": "食源性疾病是由于摄入受污染的食物所引起的胃肠道疾病。餐厅是调查疫情爆发的关键场所，因为它们在食品的采购、制备和分发方面具有共通性。通过正式渠道对疾病的公开报告有限，而社交媒体平台则承载了大量用户生成的内容，这些内容可以提供及时的公共卫生信号。本文分析了来自Yelp评论中的信号，并采用层级sigmoid注意力网络（HSAN）分类器进行处理，同时将其与纽约市卫生与心理健康部（NYC DOHMH）于2023年发布的官方餐厅检查结果进行了比较。我们在普查区层面评估相关性，按C级餐厅的分布情况比较HSAN评分的分布，并绘制纽约市范围内的空间模式。我们在普查区层面发现HSAN信号与检查评分之间的相关性极小，并且C级餐厅数量在统计上也没有显著差异。我们讨论了这些发现的含义，并概述了朝向地址级别分析的下一步工作。",
        "translated_title": "调查Yelp评论中基于文本的食源性疾病迹象与纽约市健康检查结果之间的关联（2023）",
        "label": [],
        "label_reason": "论文聚焦公共卫生信号分析，与推荐系统无直接关联",
        "relevance_score": 2,
        "novelty_score": 5,
        "novelty_reason": "提出HSAN分类器分析用户评论信号，方法具有一定新颖性"
    },
    {
        "title": "Towards Explainable Skin Cancer Classification: A Dual-Network Attention\n  Model with Lesion Segmentation and Clinical Metadata Fusion",
        "url": "http://arxiv.org/abs/2510.17773v1",
        "pub_date": "2025-10-20",
        "summary": "Skin cancer is a life-threatening disease where early detection significantly improves patient outcomes. Automated diagnosis from dermoscopic images is challenging due to high intra-class variability and subtle inter-class differences. Many deep learning models operate as \"black boxes,\" limiting clinical trust. In this work, we propose a dual-encoder attention-based framework that leverages both segmented lesions and clinical metadata to enhance skin lesion classification in terms of both accuracy and interpretability. A novel Deep-UNet architecture with Dual Attention Gates (DAG) and Atrous Spatial Pyramid Pooling (ASPP) is first employed to segment lesions. The classification stage uses two DenseNet201 encoders-one on the original image and another on the segmented lesion whose features are fused via multi-head cross-attention. This dual-input design guides the model to focus on salient pathological regions. In addition, a transformer-based module incorporates patient metadata (age, sex, lesion site) into the prediction. We evaluate our approach on the HAM10000 dataset and the ISIC 2018 and 2019 challenges. The proposed method achieves state-of-the-art segmentation performance and significantly improves classification accuracy and average AUC compared to baseline models. To validate our model's reliability, we use Gradient-weighted Class Activation Mapping (Grad-CAM) to generate heatmaps. These visualizations confirm that our model's predictions are based on the lesion area, unlike models that rely on spurious background features. These results demonstrate that integrating precise lesion segmentation and clinical data with attention-based fusion leads to a more accurate and interpretable skin cancer classification model.",
        "translated": "皮肤癌是一种危及生命的疾病，早期发现可以显著改善患者的预后。由于皮肤病变类内差异大且类间差异细微，从皮肤镜图像中进行自动诊断具有挑战性。许多深度学习模型如同“黑箱”一样运行，限制了临床信任。在本研究中，我们提出了一种基于双编码器注意力的框架，该框架利用分割后的病变区域和临床元数据，以提升皮肤病变分类的准确性和可解释性。首先，我们采用一种新的 Deep-UNet 架构，结合双注意力门（Dual Attention Gates, DAG）和空洞空间金字塔池化（Atrous Spatial Pyramid Pooling, ASPP），用于病变分割。分类阶段使用两个 DenseNet201 编码器，一个处理原始图像，另一个处理分割后的病变区域，其特征通过多头交叉注意力进行融合。这种双输入设计引导模型关注关键的病理区域。此外，基于变压器的模块将患者的元数据（如年龄、性别、病变部位）融入预测过程中。我们在 HAM10000 数据集以及 ISIC 2018 和 2019 挑战中评估了我们的方法。与基线模型相比，所提方法在分割性能上达到了最先进水平，并显著提升了分类准确率和平均 AUC。为了验证我们模型的可靠性，我们使用梯度加权类激活映射（Gradient-weighted Class Activation Mapping, Grad-CAM）生成热图。这些可视化结果证实，我们的模型预测基于病变区域，而不同于依赖虚假背景特征的模型。这些结果表明，将精确的病变分割与临床数据结合，并通过注意力机制进行特征融合，能够构建出更准确且更可解释的皮肤癌分类模型。",
        "translated_title": "迈向可解释的皮肤癌分类：一种结合病变分割与临床元数据融合的双网络注意力模型",
        "label": [
            "图像分割"
        ],
        "label_reason": "论文涉及图像分割，但主要用于皮肤癌分类，属于high-level任务",
        "relevance_score": 2,
        "novelty_score": 5,
        "novelty_reason": "结合分割与临床元数据的分类方法，有一定融合创新"
    },
    {
        "title": "Seeing but Not Believing: Probing the Disconnect Between Visual\n  Attention and Answer Correctness in VLMs",
        "url": "http://arxiv.org/abs/2510.17771v1",
        "pub_date": "2025-10-20",
        "summary": "Vision-Language Models (VLMs) achieve strong results on multimodal tasks such as visual question answering, yet they can still fail even when the correct visual evidence is present. In this work, we systematically investigate whether these failures arise from not perceiving the evidence or from not leveraging it effectively. By examining layer-wise attention dynamics, we find that shallow layers focus primarily on text, while deeper layers sparsely but reliably attend to localized evidence regions. Surprisingly, VLMs often perceive the visual evidence when outputting incorrect answers, a phenomenon we term ``seeing but not believing'' that widely exists in major VLM families. Building on this, we introduce an inference-time intervention that highlights deep-layer evidence regions through selective attention-based masking. It requires no training and consistently improves accuracy across multiple families, including LLaVA, Qwen, Gemma, and InternVL. These results show that VLMs encode reliable evidence internally but under-utilize it, making such signals explicit can bridge the gap between perception and reasoning, advancing the diagnostic understanding and reliability of VLMs.",
        "translated": "视觉语言模型（VLMs）在视觉问答等多模态任务中取得了良好的效果，但即使在视觉证据正确存在的情况下，它们仍可能失败。在本工作中，我们系统性地研究了这些失败是由于未能感知到证据，还是未能有效地利用证据所导致。通过分析各层注意力的动态变化，我们发现浅层主要关注文本，而深层则稀疏但稳定地关注局部证据区域。令人惊讶的是，VLMs在输出错误答案时往往已经感知到了视觉证据，我们称这种现象为“看见但不信”，它在主要的VLM模型家族中广泛存在。基于此，我们引入了一种推理阶段的干预方法，通过选择性注意力掩码突出显示深层证据区域。该方法无需训练，即可在多个模型家族中（包括LLaVA、Qwen、Gemma和InternVL）一致提高准确率。这些结果表明，VLMs在内部编码了可靠的证据，但未能充分利用它，使得这些信号显式化可以在感知与推理之间建立桥梁，从而提升对VLMs的诊断理解和可靠性。",
        "translated_title": "看到但不信：探究视觉注意与视觉语言模型回答正确性之间的脱节",
        "label": [],
        "label_reason": "不属于low-level图像处理任务，研究VLM的注意力机制与答案正确性关系",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "提出新的推理时干预方法，揭示VLM内部感知与推理的脱节现象"
    },
    {
        "title": "VERA-V: Variational Inference Framework for Jailbreaking Vision-Language\n  Models",
        "url": "http://arxiv.org/abs/2510.17759v1",
        "pub_date": "2025-10-20",
        "summary": "Vision-Language Models (VLMs) extend large language models with visual reasoning, but their multimodal design also introduces new, underexplored vulnerabilities. Existing multimodal red-teaming methods largely rely on brittle templates, focus on single-attack settings, and expose only a narrow subset of vulnerabilities. To address these limitations, we introduce VERA-V, a variational inference framework that recasts multimodal jailbreak discovery as learning a joint posterior distribution over paired text-image prompts. This probabilistic view enables the generation of stealthy, coupled adversarial inputs that bypass model guardrails. We train a lightweight attacker to approximate the posterior, allowing efficient sampling of diverse jailbreaks and providing distributional insights into vulnerabilities. VERA-V further integrates three complementary strategies: (i) typography-based text prompts that embed harmful cues, (ii) diffusion-based image synthesis that introduces adversarial signals, and (iii) structured distractors to fragment VLM attention. Experiments on HarmBench and HADES benchmarks show that VERA-V consistently outperforms state-of-the-art baselines on both open-source and frontier VLMs, achieving up to 53.75% higher attack success rate (ASR) over the best baseline on GPT-4o.",
        "translated": "视觉-语言模型（VLMs）通过引入视觉推理能力扩展了大语言模型，但其多模态设计也带来了新的、尚未充分研究的漏洞。现有的多模态红队方法大多依赖脆弱的模板，专注于单次攻击场景，且仅揭示了有限的漏洞子集。为了解决这些问题，我们提出了 VERA-V，一个变分推理框架，将多模态越狱发现重新定义为学习文本-图像提示对的联合后验分布。这一概率视角使得生成隐蔽性强、耦合的对抗性输入成为可能，从而绕过模型的防护机制。我们训练了一个轻量级的攻击者来近似后验分布，实现高效采样多样化的越狱方式，并提供对漏洞分布的深入理解。VERA-V 进一步融合了三种互补策略：(i) 基于排版的文本提示，嵌入有害的提示信息；(ii) 基于扩散的图像合成，引入对抗性信号；(iii) 结构化干扰项，以分散 VLM 的注意力。在 HarmBench 和 HADES 基准上的实验表明，VERA-V 在开源和前沿 VLM 上均能持续优于最先进的基线方法，在 GPT-4o 上的攻击成功率（ASR）最高可比最佳基线高出 53.75%。",
        "translated_title": "VERA-V：用于破解视觉-语言模型的变分推断框架",
        "label": [],
        "label_reason": "论文研究视觉-语言模型的安全性攻击，不属于图像像素级处理任务。",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出变分推理框架用于生成对抗性输入，具有方法创新性。"
    },
    {
        "title": "Joint Multi-Condition Representation Modelling via Matrix Factorisation\n  for Visual Place Recognition",
        "url": "http://arxiv.org/abs/2510.17739v1",
        "pub_date": "2025-10-20",
        "summary": "We address multi-reference visual place recognition (VPR), where reference sets captured under varying conditions are used to improve localisation performance. While deep learning with large-scale training improves robustness, increasing data diversity and model complexity incur extensive computational cost during training and deployment. Descriptor-level fusion via voting or aggregation avoids training, but often targets multi-sensor setups or relies on heuristics with limited gains under appearance and viewpoint change. We propose a training-free, descriptor-agnostic approach that jointly models places using multiple reference descriptors via matrix decomposition into basis representations, enabling projection-based residual matching. We also introduce SotonMV, a structured benchmark for multi-viewpoint VPR. On multi-appearance data, our method improves Recall@1 by up to ~18% over single-reference and outperforms multi-reference baselines across appearance and viewpoint changes, with gains of ~5% on unstructured data, demonstrating strong generalisation while remaining lightweight.",
        "translated": "我们研究多参考图像视觉地点识别（multi-reference visual place recognition, VPR），其中在不同条件下采集的参考图像集被用于提升定位性能。尽管大规模训练下的深度学习方法能够增强鲁棒性，但增加数据多样性与模型复杂度会在训练与部署阶段带来较大的计算开销。基于投票或聚合的描述符级融合方法虽然无需训练，但通常针对多传感器设置，或依赖启发式策略，在外观和视角变化下提升有限。我们提出一种无需训练、与描述符无关的方法，通过矩阵分解将多个参考描述符共同建模为基向量表示，从而实现基于投影的残差匹配。我们还引入了 SotonMV，一个结构化的多视角 VPR 基准数据集。在多外观数据上，我们的方法在 Recall@1 指标上比单参考图像方法最多提升了 ~18%，并在外观和视角变化下优于多参考图像基线方法，在非结构化数据上实现了 ~5% 的性能提升，展现出较强的泛化能力，同时保持了轻量级特性。",
        "translated_title": "基于矩阵分解的联合多条件表示建模用于视觉地点识别",
        "label": [],
        "label_reason": "论文聚焦于视觉地点识别，属于 high-level 任务",
        "relevance_score": 2,
        "novelty_score": 5,
        "novelty_reason": "提出了一种训练无关的描述符融合方法，有一定新颖性"
    },
    {
        "title": "Can Image-To-Video Models Simulate Pedestrian Dynamics?",
        "url": "http://arxiv.org/abs/2510.17731v1",
        "pub_date": "2025-10-20",
        "summary": "Recent high-performing image-to-video (I2V) models based on variants of the diffusion transformer (DiT) have displayed remarkable inherent world-modeling capabilities by virtue of training on large scale video datasets. We investigate whether these models can generate realistic pedestrian movement patterns in crowded public scenes. Our framework conditions I2V models on keyframes extracted from pedestrian trajectory benchmarks, then evaluates their trajectory prediction performance using quantitative measures of pedestrian dynamics.",
        "translated": "基于扩散变换器（DiT）变体的最新高性能图像到视频（I2V）模型，由于在大规模视频数据集上进行训练，展现出显著的内在世界建模能力。我们研究这些模型是否能够在拥挤的公共场景中生成逼真的行人运动模式。我们的框架以从行人轨迹基准中提取的关键帧作为条件输入，驱动I2V模型生成视频，并通过行人动力学的定量指标评估其轨迹预测性能。",
        "translated_title": "图像到视频模型能否模拟行人动态？",
        "label": [],
        "label_reason": "论文不属于low-level图像处理任务",
        "relevance_score": 2,
        "novelty_score": 5,
        "novelty_reason": "方法基于现有扩散模型框架进行微调"
    },
    {
        "title": "Signature Forgery Detection: Improving Cross-Dataset Generalization",
        "url": "http://arxiv.org/abs/2510.17724v1",
        "pub_date": "2025-10-20",
        "summary": "Automated signature verification is a critical biometric technique used in banking, identity authentication, and legal documentation. Despite the notable progress achieved by deep learning methods, most approaches in offline signature verification still struggle to generalize across datasets, as variations in handwriting styles and acquisition protocols often degrade performance. This study investigates feature learning strategies for signature forgery detection, focusing on improving cross-dataset generalization -- that is, model robustness when trained on one dataset and tested on another. Using three public benchmarks -- CEDAR, ICDAR, and GPDS Synthetic -- two experimental pipelines were developed: one based on raw signature images and another employing a preprocessing method referred to as shell preprocessing. Several behavioral patterns were identified and analyzed; however, no definitive superiority between the two approaches was established. The results show that the raw-image model achieved higher performance across benchmarks, while the shell-based model demonstrated promising potential for future refinement toward robust, cross-domain signature verification.",
        "translated": "自动签名验证是银行、身份认证和法律文件中使用的关键生物识别技术。尽管深度学习方法已取得了显著进展，但大多数离线签名验证方法在不同数据集之间仍然难以泛化，因为书写风格和采集协议的变化常常导致性能下降。本研究探讨了用于签名伪造检测的特征学习策略，重点在于提升跨数据集的泛化能力——即模型在一个数据集上训练，在另一个数据集上测试时的鲁棒性。实验使用了三个公开基准——CEDAR、ICDAR 和 GPDS Synthetic，设计了两种实验流程：一种基于原始签名图像，另一种采用一种称为壳预处理（shell preprocessing）的预处理方法。我们识别并分析了若干行为模式，但尚未明确确立两种方法之间的绝对优势。结果表明，基于原始图像的模型在各基准测试中表现更优，而基于壳处理的模型在进一步优化后，表现出在鲁棒性和跨域签名验证方面具有良好的前景。",
        "translated_title": "签名伪造检测：提升跨数据集泛化能力",
        "label": [],
        "label_reason": "论文主题为签名伪造检测，属于高阶视觉任务",
        "relevance_score": 2,
        "novelty_score": 5,
        "novelty_reason": "探讨了跨数据集泛化策略，但未提出显著新方法"
    },
    {
        "title": "MT-Video-Bench: A Holistic Video Understanding Benchmark for Evaluating\n  Multimodal LLMs in Multi-Turn Dialogues",
        "url": "http://arxiv.org/abs/2510.17722v1",
        "pub_date": "2025-10-20",
        "summary": "The recent development of Multimodal Large Language Models (MLLMs) has significantly advanced AI's ability to understand visual modalities. However, existing evaluation benchmarks remain limited to single-turn question answering, overlooking the complexity of multi-turn dialogues in real-world scenarios. To bridge this gap, we introduce MT-Video-Bench, a holistic video understanding benchmark for evaluating MLLMs in multi-turn dialogues. Specifically, our MT-Video-Bench mainly assesses six core competencies that focus on perceptivity and interactivity, encompassing 987 meticulously curated multi-turn dialogues from diverse domains. These capabilities are rigorously aligned with real-world applications, such as interactive sports analysis and multi-turn video-based intelligent tutoring. With MT-Video-Bench, we extensively evaluate various state-of-the-art open-source and closed-source MLLMs, revealing their significant performance discrepancies and limitations in handling multi-turn video dialogues. The benchmark will be publicly available to foster future research.",
        "translated": "多模态大语言模型（MLLMs）的最新发展显著提升了人工智能理解视觉模态的能力。然而，现有的评估基准仍局限于单轮问答，忽视了现实场景中多轮对话的复杂性。为弥补这一差距，我们引入了 MT-Video-Bench，一个全面的视频理解基准，用于评估 MLLMs 在多轮对话中的表现。具体而言，我们的 MT-Video-Bench 主要评估六项核心能力，涵盖感知性和交互性，其中包括从多个领域精心挑选的 987 个多轮对话。这些能力与实际应用紧密对齐，例如交互式体育分析和基于视频的多轮智能教学。借助 MT-Video-Bench，我们对多种最先进的开源和闭源 MLLMs 进行了广泛评估，揭示了它们在处理多轮视频对话任务时显著的性能差异和局限性。该基准将向公众开放，以促进未来的研究。",
        "translated_title": "MT-Video-Bench：一种用于评估多轮对话中多模态大语言模型的综合视频理解基准",
        "label": [],
        "label_reason": "论文聚焦于视频理解的多轮对话评估，属于high-level任务。",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出了面向多轮对话的视频理解基准，有一定实用价值。"
    },
    {
        "title": "Raindrop GS: A Benchmark for 3D Gaussian Splatting under Raindrop\n  Conditions",
        "url": "http://arxiv.org/abs/2510.17719v1",
        "pub_date": "2025-10-20",
        "summary": "3D Gaussian Splatting (3DGS) under raindrop conditions suffers from severe occlusions and optical distortions caused by raindrop contamination on the camera lens, substantially degrading reconstruction quality. Existing benchmarks typically evaluate 3DGS using synthetic raindrop images with known camera poses (constrained images), assuming ideal conditions. However, in real-world scenarios, raindrops often interfere with accurate camera pose estimation and point cloud initialization. Moreover, a significant domain gap between synthetic and real raindrops further impairs generalization. To tackle these issues, we introduce RaindropGS, a comprehensive benchmark designed to evaluate the full 3DGS pipeline-from unconstrained, raindrop-corrupted images to clear 3DGS reconstructions. Specifically, the whole benchmark pipeline consists of three parts: data preparation, data processing, and raindrop-aware 3DGS evaluation, including types of raindrop interference, camera pose estimation and point cloud initialization, single image rain removal comparison, and 3D Gaussian training comparison. First, we collect a real-world raindrop reconstruction dataset, in which each scene contains three aligned image sets: raindrop-focused, background-focused, and rain-free ground truth, enabling a comprehensive evaluation of reconstruction quality under different focus conditions. Through comprehensive experiments and analyses, we reveal critical insights into the performance limitations of existing 3DGS methods on unconstrained raindrop images and the varying impact of different pipeline components: the impact of camera focus position on 3DGS reconstruction performance, and the interference caused by inaccurate pose and point cloud initialization on reconstruction. These insights establish clear directions for developing more robust 3DGS methods under raindrop conditions.",
        "translated": "在雨滴条件下，3D Gaussian Splatting（3DGS）会受到相机镜头上雨滴污染所引起的严重遮挡和光学畸变的影响，从而显著降低重建质量。现有的基准测试通常在已知相机姿态的合成雨滴图像（受限图像）上评估 3DGS，假设为理想条件。然而，在现实场景中，雨滴常常干扰精确的相机姿态估计和点云初始化。此外，合成雨滴与真实雨滴之间存在显著的域差异，进一步削弱了方法的泛化能力。为了解决这些问题，我们提出了 RaindropGS，一个全面的基准，用于评估完整的 3DGS 流程——从不受约束的、被雨滴污染的图像到清晰的 3DGS 重建。具体而言，整个基准流程包括三个部分：数据准备、数据处理和雨滴感知的 3DGS 评估，涵盖了雨滴干扰的类型、相机姿态估计与点云初始化、单图像去雨比较以及 3D Gaussian 训练比较。首先，我们收集了一个真实世界的雨滴重建数据集，其中每个场景包含三组对齐的图像：聚焦雨滴图像、聚焦背景图像和无雨的真实图像，从而能够在不同聚焦条件下全面评估重建质量。通过综合的实验与分析，我们揭示了当前 3DGS 方法在处理不受约束的雨滴图像时性能限制的关键洞察，并分析了不同流程组件对重建结果的影响：相机聚焦位置对 3DGS 重建性能的影响，以及姿态估计不准和点云初始化错误所引起的干扰。这些发现为开发在雨滴条件下更鲁棒的 3DGS 方法提供了明确的方向。",
        "translated_title": "雨滴GS：雨滴条件下3D高斯点绘制的基准测试",
        "label": [
            "图像去雨",
            "多帧/视频图像恢复",
            "图像恢复"
        ],
        "label_reason": "论文聚焦雨滴干扰下的3DGS重建，涉及图像去雨和恢复任务",
        "relevance_score": 7,
        "novelty_score": 8,
        "novelty_reason": "提出首个针对真实雨滴条件下的3DGS基准，包含新数据集和评估方法"
    },
    {
        "title": "Automatic Classification of Circulating Blood Cell Clusters based on\n  Multi-channel Flow Cytometry Imaging",
        "url": "http://arxiv.org/abs/2510.17716v1",
        "pub_date": "2025-10-20",
        "summary": "Circulating blood cell clusters (CCCs) containing red blood cells (RBCs), white blood cells(WBCs), and platelets are significant biomarkers linked to conditions like thrombosis, infection, and inflammation. Flow cytometry, paired with fluorescence staining, is commonly used to analyze these cell clusters, revealing cell morphology and protein profiles. While computational approaches based on machine learning have advanced the automatic analysis of single-cell flow cytometry images, there is a lack of effort to build tools to automatically analyze images containing CCCs. Unlike single cells, cell clusters often exhibit irregular shapes and sizes. In addition, these cell clusters often consist of heterogeneous cell types, which require multi-channel staining to identify the specific cell types within the clusters. This study introduces a new computational framework for analyzing CCC images and identifying cell types within clusters. Our framework uses a two-step analysis strategy. First, it categorizes images into cell cluster and non-cluster groups by fine-tuning the You Only Look Once(YOLOv11) model, which outperforms traditional convolutional neural networks (CNNs), Vision Transformers (ViT). Then, it identifies cell types by overlaying cluster contours with regions from multi-channel fluorescence stains, enhancing accuracy despite cell debris and staining artifacts. This approach achieved over 95% accuracy in both cluster classification and phenotype identification. In summary, our automated framework effectively analyzes CCC images from flow cytometry, leveraging both bright-field and fluorescence data. Initially tested on blood cells, it holds potential for broader applications, such as analyzing immune and tumor cell clusters, supporting cellular research across various diseases.",
        "translated": "含有红细胞（RBCs）、白细胞（WBCs）和血小板的循环血细胞簇（CCCs）是与血栓形成、感染和炎症等病症相关的重要生物标志物。流式细胞术配合荧光染色通常用于分析这些细胞簇，揭示细胞形态和蛋白表达谱。尽管基于机器学习的计算方法已推动了单细胞流式细胞术图像的自动分析，但尚缺乏针对包含细胞簇图像的自动分析工具。与单细胞不同，细胞簇通常具有不规则的形状和大小。此外，这些细胞簇往往由异质性的细胞类型组成，需要多通道染色以识别簇内的特定细胞类型。本研究提出了一种新的计算框架，用于分析细胞簇图像并识别簇内的细胞类型。我们的框架采用两步分析策略。首先，通过微调 You Only Look Once (YOLOv11) 模型，将图像分为细胞簇和非细胞簇组，该模型性能优于传统卷积神经网络（CNNs）和视觉Transformer（ViT）。然后，通过将簇轮廓叠加于多通道荧光染色区域，识别细胞类型，从而在细胞碎片和染色伪影存在的情况下提高准确性。该方法在细胞簇分类和表型识别方面均实现了超过95%的准确率。综上所述，我们的自动化框架有效分析了流式细胞术中的细胞簇图像，结合了明场和荧光数据。该方法最初在血液细胞上进行了测试，具有更广泛的应用潜力，例如分析免疫和肿瘤细胞簇，从而支持多种疾病的研究。",
        "translated_title": "基于多通道流式细胞术成像的循环血细胞簇自动分类",
        "label": [],
        "label_reason": "论文主要关注细胞分类，不涉及图像像素级恢复或增强",
        "relevance_score": 2,
        "novelty_score": 5,
        "novelty_reason": "方法基于现有模型改进，创新性有限"
    },
    {
        "title": "Improving Cross-Patient Generalization in Parkinson's Disease Detection\n  through Chunk-Based Analysis of Hand-Drawn Patterns",
        "url": "http://arxiv.org/abs/2510.17703v1",
        "pub_date": "2025-10-20",
        "summary": "Parkinson's disease (PD) is a neurodegenerative disease affecting about 1% of people over the age of 60, causing motor impairments that impede hand coordination activities such as writing and drawing. Many approaches have tried to support early detection of Parkinson's disease based on hand-drawn images; however, we identified two major limitations in the related works: (1) the lack of sufficient datasets, (2) the robustness when dealing with unseen patient data. In this paper, we propose a new approach to detect Parkinson's disease that consists of two stages: The first stage classifies based on their drawing type(circle, meander, spiral), and the second stage extracts the required features from the images and detects Parkinson's disease. We overcame the previous two limitations by applying a chunking strategy where we divide each image into 2x2 chunks. Each chunk is processed separately when extracting features and recognizing Parkinson's disease indicators. To make the final classification, an ensemble method is used to merge the decisions made from each chunk. Our evaluation shows that our proposed approach outperforms the top performing state-of-the-art approaches, in particular on unseen patients. On the NewHandPD dataset our approach, it achieved 97.08% accuracy for seen patients and 94.91% for unseen patients, our proposed approach maintained a gap of only 2.17 percentage points, compared to the 4.76-point drop observed in prior work.",
        "translated": "帕金森病（PD）是一种神经退行性疾病，影响约60岁以上人群的1%，会导致运动障碍，妨碍手部协调活动如书写和绘画。许多方法尝试基于手绘图像支持帕金森病的早期检测；然而，我们发现相关工作中存在两个主要限制：(1) 缺乏足够的数据集；(2) 在处理未见过的患者数据时的鲁棒性不足。在本文中，我们提出了一种新的帕金森病检测方法，该方法包含两个阶段：第一阶段基于绘画类型（圆形、来回线、螺旋）进行分类，第二阶段从图像中提取所需特征并检测帕金森病。我们通过应用一种块处理策略克服了上述两个限制，即将每张图像划分为2x2的块。在提取特征和识别帕金森病指标时，每个块被单独处理。为做出最终分类，使用了一种集成方法将每个块的决策结果进行融合。我们的评估表明，所提出的检测方法优于当前最先进的方法，特别是在未见过的患者数据上表现突出。在NewHandPD数据集上，我们的方法对已见患者的准确率达到97.08%，对未见患者达到94.91%；相比先前工作中观察到的4.76个百分点的下降，我们的方法仅存在2.17个百分点的差距。",
        "translated_title": "通过基于块分析手绘图案提高帕金森病检测中的跨患者泛化能力",
        "label": [],
        "label_reason": "论文主要关注帕金森病检测，属于high-level任务",
        "relevance_score": 2,
        "novelty_score": 5,
        "novelty_reason": "提出分块策略与集成方法，但创新性有限"
    },
    {
        "title": "Elastic ViTs from Pretrained Models without Retraining",
        "url": "http://arxiv.org/abs/2510.17700v1",
        "pub_date": "2025-10-20",
        "summary": "Vision foundation models achieve remarkable performance but are only available in a limited set of pre-determined sizes, forcing sub-optimal deployment choices under real-world constraints. We introduce SnapViT: Single-shot network approximation for pruned Vision Transformers, a new post-pretraining structured pruning method that enables elastic inference across a continuum of compute budgets. Our approach efficiently combines gradient information with cross-network structure correlations, approximated via an evolutionary algorithm, does not require labeled data, generalizes to models without a classification head, and is retraining-free. Experiments on DINO, SigLIPv2, DeIT, and AugReg models demonstrate superior performance over state-of-the-art methods across various sparsities, requiring less than five minutes on a single A100 GPU to generate elastic models that can be adjusted to any computational budget. Our key contributions include an efficient pruning strategy for pretrained Vision Transformers, a novel evolutionary approximation of Hessian off-diagonal structures, and a self-supervised importance scoring mechanism that maintains strong performance without requiring retraining or labels. Code and pruned models are available at: https://elastic.ashita.nl/",
        "translated": "视觉基础模型在性能上取得了显著成果，但它们仅以有限的一组预设尺寸提供，这在现实约束条件下迫使用户做出次优的部署选择。我们提出 SnapViT：剪枝视觉变换器的单次网络近似方法，这是一种新的预训练后结构化剪枝方法，能够在一系列不同的计算预算之间实现弹性推理。我们的方法高效地结合了梯度信息与跨网络结构的相关性，通过进化算法进行近似，无需标注数据，适用于没有分类头的模型，并且无需重新训练。在 DINO、SigLIPv2、DeIT 和 AugReg 模型上的实验表明，我们的方法在各种稀疏度下均优于当前最先进的方法，且在单个 A100 GPU 上即可在不到五分钟的时间内生成可适应任意计算预算的弹性模型。我们的主要贡献包括：为预训练视觉变换器设计了一种高效的剪枝策略、提出了一种新型的 Hessian 非对角结构的进化近似方法，以及一种无需重新训练或标签即可保持强大性能的自监督重要性评分机制。代码和剪枝后的模型可通过以下链接获取：https://elastic.ashita.nl/",
        "translated_title": "从预训练模型中无需微调的弹性视觉变换器",
        "label": [],
        "label_reason": "论文聚焦于视觉Transformer的剪枝，不涉及像素级图像处理任务",
        "relevance_score": 1,
        "novelty_score": 6,
        "novelty_reason": "提出了一种无需重新训练的剪枝方法，结合梯度信息和进化算法"
    },
    {
        "title": "GAS: Improving Discretization of Diffusion ODEs via Generalized\n  Adversarial Solver",
        "url": "http://arxiv.org/abs/2510.17699v1",
        "pub_date": "2025-10-20",
        "summary": "While diffusion models achieve state-of-the-art generation quality, they still suffer from computationally expensive sampling. Recent works address this issue with gradient-based optimization methods that distill a few-step ODE diffusion solver from the full sampling process, reducing the number of function evaluations from dozens to just a few. However, these approaches often rely on intricate training techniques and do not explicitly focus on preserving fine-grained details. In this paper, we introduce the Generalized Solver: a simple parameterization of the ODE sampler that does not require additional training tricks and improves quality over existing approaches. We further combine the original distillation loss with adversarial training, which mitigates artifacts and enhances detail fidelity. We call the resulting method the Generalized Adversarial Solver and demonstrate its superior performance compared to existing solver training methods under similar resource constraints. Code is available at https://github.com/3145tttt/GAS.",
        "translated": "尽管扩散模型在生成质量方面达到了最先进的水平，它们在采样过程中仍然面临计算成本高昂的问题。最近的研究工作通过基于梯度的优化方法解决了这一问题，这些方法从完整的采样过程中蒸馏出几步的 ODE 扩散求解器，将函数求值次数从几十次减少到几次。然而，这些方法通常依赖于复杂的训练技巧，并没有明确关注于保留精细的细节信息。在本文中，我们提出了广义求解器（Generalized Solver）：一种无需额外训练技巧的 ODE 采样器的简单参数化方式，并在现有方法的基础上提高了生成质量。我们进一步将原始的蒸馏损失与对抗训练相结合，这有助于减少伪影并增强细节的真实性。我们将所提出的方法称为广义对抗求解器（Generalized Adversarial Solver），并在相似资源约束下展示了其相比现有求解器训练方法的优越性能。代码可在 https://github.com/3145tttt/GAS 获得。",
        "translated_title": "GAS：通过广义对抗求解器改进扩散ODE的离散化",
        "label": [],
        "label_reason": "论文聚焦图像生成采样优化，非像素级图像恢复或增强任务。",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出通用对抗求解器改进扩散ODE离散化，训练方法简洁有效。"
    },
    {
        "title": "Towards 3D Objectness Learning in an Open World",
        "url": "http://arxiv.org/abs/2510.17686v1",
        "pub_date": "2025-10-20",
        "summary": "Recent advancements in 3D object detection and novel category detection have made significant progress, yet research on learning generalized 3D objectness remains insufficient. In this paper, we delve into learning open-world 3D objectness, which focuses on detecting all objects in a 3D scene, including novel objects unseen during training. Traditional closed-set 3D detectors struggle to generalize to open-world scenarios, while directly incorporating 3D open-vocabulary models for open-world ability struggles with vocabulary expansion and semantic overlap. To achieve generalized 3D object discovery, We propose OP3Det, a class-agnostic Open-World Prompt-free 3D Detector to detect any objects within 3D scenes without relying on hand-crafted text prompts. We introduce the strong generalization and zero-shot capabilities of 2D foundation models, utilizing both 2D semantic priors and 3D geometric priors for class-agnostic proposals to broaden 3D object discovery. Then, by integrating complementary information from point cloud and RGB image in the cross-modal mixture of experts, OP3Det dynamically routes uni-modal and multi-modal features to learn generalized 3D objectness. Extensive experiments demonstrate the extraordinary performance of OP3Det, which significantly surpasses existing open-world 3D detectors by up to 16.0% in AR and achieves a 13.5% improvement compared to closed-world 3D detectors.",
        "translated": "近年来，3D目标检测和新类别检测取得了显著进展，但关于学习广义3D目标性的研究仍显不足。本文我们深入探讨了开放世界3D目标性的学习，其核心目标是在3D场景中检测所有物体，包括训练过程中未见过的新物体。传统的闭集3D检测器难以在开放世界场景中泛化，而直接引入3D开放词汇模型以实现开放世界能力则面临词汇扩展和语义重叠的挑战。为了实现广义3D目标发现，我们提出了OP3Det，一种类别无关的无提示开放世界3D检测器，能够在不依赖人工设计文本提示的情况下检测3D场景中的任何物体。我们引入了2D基础模型强大的泛化能力和零样本能力，结合2D语义先验和3D几何先验生成类别无关的候选区域，从而扩展3D目标发现的范围。随后，通过在跨模态专家混合结构中融合点云和RGB图像的互补信息，OP3Det动态路由单模态和多模态特征，以学习广义3D目标性。大量实验表明，OP3Det具有出色的性能，在AR指标上显著超越现有的开放世界3D检测器，最高提升达16.0%，并且相比闭世界3D检测器也有13.5%的提升。",
        "translated_title": "在开放世界中实现3D目标性学习",
        "label": [],
        "label_reason": "论文聚焦3D目标检测与开放世界学习，不属于low-level图像处理任务。",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "提出了OP3Det，结合2D语义先验与3D几何先验，有一定创新性。"
    },
    {
        "title": "Multilingual Text-to-Image Person Retrieval via Bidirectional Relation\n  Reasoning and Aligning",
        "url": "http://arxiv.org/abs/2510.17685v1",
        "pub_date": "2025-10-20",
        "summary": "Text-to-image person retrieval (TIPR) aims to identify the target person using textual descriptions, facing challenge in modality heterogeneity. Prior works have attempted to address it by developing cross-modal global or local alignment strategies. However, global methods typically overlook fine-grained cross-modal differences, whereas local methods require prior information to explore explicit part alignments. Additionally, current methods are English-centric, restricting their application in multilingual contexts. To alleviate these issues, we pioneer a multilingual TIPR task by developing a multilingual TIPR benchmark, for which we leverage large language models for initial translations and refine them by integrating domain-specific knowledge. Correspondingly, we propose Bi-IRRA: a Bidirectional Implicit Relation Reasoning and Aligning framework to learn alignment across languages and modalities. Within Bi-IRRA, a bidirectional implicit relation reasoning module enables bidirectional prediction of masked image and text, implicitly enhancing the modeling of local relations across languages and modalities, a multi-dimensional global alignment module is integrated to bridge the modality heterogeneity. The proposed method achieves new state-of-the-art results on all multilingual TIPR datasets. Data and code are presented in https://github.com/Flame-Chasers/Bi-IRRA.",
        "translated": "文本到图像的人体检索（Text-to-image person retrieval, TIPR）旨在通过文本描述识别目标人物，面临模态异质性的挑战。现有工作尝试通过开发跨模态的全局或局部对齐策略来解决这一问题。然而，全局方法通常忽略细粒度的跨模态差异，而局部方法则需要先验信息以探索显式的部件对齐。此外，当前方法多以英语为中心，限制了其在多语言场景中的应用。为缓解这些问题，我们通过构建一个多语言TIPR基准数据集，开创了多语言TIPR任务，在该基准中，我们利用大语言模型进行初步翻译，并通过融合领域特定知识对其进行优化。相应地，我们提出Bi-IRRA：一种双向隐式关系推理与对齐框架，以学习跨语言和模态的对齐关系。在Bi-IRRA中，一个双向隐式关系推理模块能够实现图像和文本掩码部分的双向预测，隐式增强跨语言和模态的局部关系建模；同时集成一个多维全局对齐模块以弥合模态异质性。所提方法在所有多语言TIPR数据集上均取得了新的最先进结果。数据和代码已发布在 https://github.com/Flame-Chasers/Bi-IRRA。",
        "translated_title": "通过双向关系推理与对齐的多语言文本到图像行人检索",
        "label": [],
        "label_reason": "论文聚焦文本到图像的人检索，属于跨模态匹配，非图像像素级恢复或增强",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出双向关系推理与对齐框架，改进跨模态匹配，有一定创新性"
    },
    {
        "title": "Intelligent Communication Mixture-of-Experts Boosted-Medical Image\n  Segmentation Foundation Model",
        "url": "http://arxiv.org/abs/2510.17684v1",
        "pub_date": "2025-10-20",
        "summary": "Foundation models for medical image segmentation have achieved remarkable performance. Adaptive fine-tuning of natural image segmentation foundation models is crucial for medical image segmentation tasks. However, some limitations exist in existing fine-tuning methods: 1) insufficient representation of high-level features and 2) the fine-tuning process disrupts the structural integrity of pretrained weights. Inspired by these critical problems, we propose an intelligent communication mixture-of-experts boosted-medical image segmentation foundation model, named IC-MoE, with twofold ideas: 1) We construct basic experts, semantic experts, and adaptive experts. Moreover, we implement a pixel probability adaptive voting strategy, which enables expert selection and fusion through label consistency and load balancing. This approach preliminarily enhances the representation capability of high-level features while preserving the structural integrity of pretrained weights. 2) We propose a semantic-guided contrastive learning method to address the issue of weak supervision in contrastive learning. This method further enhances the representation capability of high-level features while preserving the structural integrity of pretrained weights. Extensive experiments across three public medical image segmentation datasets demonstrate that the IC-MoE outperforms other SOTA models. Consequently, the proposed IC-MoE effectively supplements foundational medical image segmentation models with high-level features and pretrained structural integrity. We also validate the superior generalizability of the IC-MoE across diverse medical image segmentation scenarios.",
        "translated": "医学图像分割的基础模型已经取得了显著的性能。自然图像分割基础模型的自适应微调对医学图像分割任务至关重要。然而，现有的微调方法中存在一些局限性：1）对高层特征的表示能力不足，2）微调过程破坏了预训练权重的结构完整性。受到这些问题的启发，我们提出了一种智能通信的混合专家增强型医学图像分割基础模型，命名为 IC-MoE，包含两个核心思想：1）我们构建了基本专家、语义专家和自适应专家。此外，我们实现了一种像素概率自适应投票策略，该策略通过标签一致性和负载平衡实现专家的选择与融合。这种方法初步增强了对高层特征的表示能力，同时保留了预训练权重的结构完整性。2）我们提出了一种语义引导的对比学习方法，以解决对比学习中监督信号较弱的问题。该方法进一步增强了对高层特征的表示能力，同时保持了预训练权重的结构完整性。在三个公开的医学图像分割数据集上的大量实验表明，IC-MoE 在性能上优于其他最先进的模型。因此，所提出的 IC-MoE 有效地通过高层特征和预训练结构完整性补充了医学图像分割的基础模型。我们还验证了 IC-MoE 在多种医学图像分割场景中的优越泛化能力。",
        "translated_title": "智能通信混合专家增强的医学图像分割基础模型",
        "label": [
            "医学图像增强"
        ],
        "label_reason": "论文关注医学图像分割，属于图像增强范畴",
        "relevance_score": 5,
        "novelty_score": 7,
        "novelty_reason": "提出IC-MoE模型和语义引导对比学习方法，具有显著改进"
    },
    {
        "title": "LLMs as Sparse Retrievers:A Framework for First-Stage Product Search",
        "url": "http://arxiv.org/abs/2510.18527v1",
        "pub_date": "2025-10-21",
        "summary": "Product search is a crucial component of modern e-commerce platforms, with billions of user queries every day. In product search systems, first-stage retrieval should achieve high recall while ensuring efficient online deployment. Sparse retrieval is particularly attractive in this context due to its interpretability and storage efficiency. However, sparse retrieval methods suffer from severe vocabulary mismatch issues, leading to suboptimal performance in product search scenarios.With their potential for semantic analysis, large language models (LLMs) offer a promising avenue for mitigating vocabulary mismatch issues and thereby improving retrieval quality. Directly applying LLMs to sparse retrieval in product search exposes two key challenges:(1)Queries and product titles are typically short and highly susceptible to LLM-induced hallucinations, such as generating irrelevant expansion terms or underweighting critical literal terms like brand names and model numbers;(2)The large vocabulary space of LLMs leads to difficulty in initializing training effectively, making it challenging to learn meaningful sparse representations in such ultra-high-dimensional spaces.To address these challenges, we propose PROSPER, a framework for PROduct search leveraging LLMs as SParsE Retrievers. PROSPER incorporates: (1)A literal residual network that alleviates hallucination in lexical expansion by reinforcing underweighted literal terms through a residual compensation mechanism; and (2)A lexical focusing window that facilitates effective training initialization via a coarse-to-fine sparsification strategy.Extensive offline and online experiments show that PROSPER significantly outperforms sparse baselines and achieves recall performance comparable to advanced dense retrievers, while also achieving revenue increments online.",
        "translated": "产品搜索是现代电子商务平台的关键组成部分，每天处理数十亿用户查询。在产品搜索系统中，第一阶段的召回应在保证高效在线部署的前提下实现高召回率。稀疏召回由于其可解释性和存储效率，在这种场景下尤为具有吸引力。然而，稀疏召回方法在产品搜索场景中会受到严重的词汇不匹配问题，导致性能欠佳。大语言模型（LLMs）由于其在语义分析方面的潜力，为缓解词汇不匹配问题并从而提升召回质量提供了有前景的途径。然而，将LLMs直接应用于产品搜索中的稀疏召回会面临两个关键挑战：(1) 查询和产品标题通常较短，极易受到LLM引发的幻觉影响，例如生成不相关的扩展词汇或低估关键的字面词汇，如品牌名称和型号数字；(2) LLM的庞大词汇空间使得训练初始化变得困难，从而难以在这样的超高维空间中学习到有意义的稀疏表示。\n\n为解决这些挑战，我们提出了PROSPER，一种利用大语言模型作为稀疏召回器的产品搜索框架。PROSPER包含以下两个组成部分：(1) 一个字面残差网络，通过残差补偿机制强化被低估的字面词汇，从而减轻词汇扩展中的幻觉问题；(2) 一个词汇聚焦窗口，通过从粗到细的稀疏化策略促进有效的训练初始化。广泛的离线和在线实验表明，PROSPER显著优于稀疏基线方法，在召回性能上可与先进的密集召回器相媲美，同时在在线环境中也实现了收入增长。",
        "translated_title": "LLMs 作为稀疏检索器：一个用于第一阶段商品搜索的框架",
        "label": [
            "召回（Recall）",
            "LLM生成式推荐（LLM-based Generative Recommendation）",
            "通用推荐技术（General Recommendation Techniques）"
        ],
        "label_reason": "论文聚焦于基于LLM的召回框架PROSPER，改进稀疏检索在电商产品搜索中的表现。",
        "relevance_score": 8,
        "novelty_score": 7,
        "novelty_reason": "提出结合LLM与稀疏检索的新框架，解决词汇不匹配与幻觉问题，具有实用创新性。"
    },
    {
        "title": "ImageGem: In-the-wild Generative Image Interaction Dataset for\n  Generative Model Personalization",
        "url": "http://arxiv.org/abs/2510.18433v1",
        "pub_date": "2025-10-21",
        "summary": "We introduce ImageGem, a dataset for studying generative models that understand fine-grained individual preferences. We posit that a key challenge hindering the development of such a generative model is the lack of in-the-wild and fine-grained user preference annotations. Our dataset features real-world interaction data from 57K users, who collectively have built 242K customized LoRAs, written 3M text prompts, and created 5M generated images. With user preference annotations from our dataset, we were able to train better preference alignment models. In addition, leveraging individual user preference, we investigated the performance of retrieval models and a vision-language model on personalized image retrieval and generative model recommendation. Finally, we propose an end-to-end framework for editing customized diffusion models in a latent weight space to align with individual user preferences. Our results demonstrate that the ImageGem dataset enables, for the first time, a new paradigm for generative model personalization.",
        "translated": "我们引入 ImageGem 数据集，用于研究能够理解细粒度个性化偏好的生成式模型。我们认为，阻碍此类生成式模型发展的关键挑战之一是缺乏真实场景下的细粒度用户偏好标注。我们的数据集包含了 57K 个用户的真实交互数据，这些用户共同构建了 242K 个定制化的 LoRA 模型，撰写了 3M 条文本提示，并生成了 5M 张图像。借助数据集中提供的用户偏好标注，我们训练出了更优的偏好对齐模型。此外，基于个体用户偏好，我们研究了检索模型和视觉-语言模型在个性化图像检索和生成式模型推荐方面的性能。最后，我们提出了一种端到端的框架，用于在潜在权重空间中编辑定制化的扩散模型，以对齐个体用户偏好。我们的结果表明，ImageGem 数据集首次实现了一种生成式模型个性化的全新范式。",
        "translated_title": "ImageGem：用于生成模型个性化的野外生成图像交互数据集",
        "label": [
            "LLM生成式推荐",
            "个性化推荐"
        ],
        "label_reason": "论文涉及生成式模型个性化与推荐，与生成式推荐系统相关",
        "relevance_score": 7,
        "novelty_score": 8,
        "novelty_reason": "提出新数据集和端到端框架，推动个性化生成模型发展"
    },
    {
        "title": "Censorship Chokepoints: New Battlegrounds for Regional Surveillance,\n  Censorship and Influence on the Internet",
        "url": "http://arxiv.org/abs/2510.18394v1",
        "pub_date": "2025-10-21",
        "summary": "Undoubtedly, the Internet has become one of the most important conduits to information for the general public. Nonetheless, Internet access can be and has been limited systematically or blocked completely during political events in numerous countries and regions by various censorship mechanisms. Depending on where the core filtering component is situated, censorship techniques have been classified as client-based, server-based, or network-based. However, as the Internet evolves rapidly, new and sophisticated censorship techniques have emerged, which involve techniques that cut across locations and involve new forms of hurdles to information access. We argue that modern censorship can be better understood through a new lens that we term chokepoints, which identifies bottlenecks in the content production or delivery cycle where efficient new forms of large-scale client-side surveillance and filtering mechanisms have emerged.",
        "translated": "毫无疑问，互联网已成为普通公众获取信息最重要的渠道之一。然而，在许多国家和地区，特别是在政治事件期间，互联网的访问权限可能被系统性地限制或完全封锁，这涉及各种审查机制。根据核心过滤组件的位置，审查技术被分为基于客户端、基于服务器或基于网络的类型。然而，随着互联网的快速发展，新的、复杂的审查技术不断涌现，这些技术突破了地理位置的限制，并引入了新型的信息获取障碍。我们认为，通过一个我们称之为“瓶颈点”的新视角，可以更好地理解现代审查机制，该视角识别内容生产或分发周期中的关键瓶颈，在这些瓶颈处已出现了高效的大规模客户端监控和过滤机制。",
        "translated_title": "审查瓶颈：互联网区域监控、审查与影响的新战场",
        "label": [],
        "label_reason": "论文聚焦网络审查，与推荐系统无直接关联。",
        "relevance_score": 1,
        "novelty_score": 3,
        "novelty_reason": "提出新概念‘chokepoints’，但属于社会技术分析，创新性有限。"
    },
    {
        "title": "Evaluating LLM-Based Mobile App Recommendations: An Empirical Study",
        "url": "http://arxiv.org/abs/2510.18364v1",
        "pub_date": "2025-10-21",
        "summary": "Large Language Models (LLMs) are increasingly used to recommend mobile applications through natural language prompts, offering a flexible alternative to keyword-based app store search. Yet, the reasoning behind these recommendations remains opaque, raising questions about their consistency, explainability, and alignment with traditional App Store Optimization (ASO) metrics. In this paper, we present an empirical analysis of how widely-used general purpose LLMs generate, justify, and rank mobile app recommendations. Our contributions are: (i) a taxonomy of 16 generalizable ranking criteria elicited from LLM outputs; (ii) a systematic evaluation framework to analyse recommendation consistency and responsiveness to explicit ranking instructions; and (iii) a replication package to support reproducibility and future research on AI-based recommendation systems. Our findings reveal that LLMs rely on a broad yet fragmented set of ranking criteria, only partially aligned with standard ASO metrics. While top-ranked apps tend to be consistent across runs, variability increases with ranking depth and search specificity. LLMs exhibit varying sensitivity to explicit ranking instructions - ranging from substantial adaptations to near-identical outputs - highlighting their complex reasoning dynamics in conversational app discovery. Our results aim to support end-users, app developers, and recommender-systems researchers in navigating the emerging landscape of conversational app discovery.",
        "translated": "大语言模型（LLMs）正被越来越多地用于通过自然语言提示推荐移动应用，为传统的基于关键词的商店搜索提供了一种灵活的替代方式。然而，这些推荐背后的推理过程仍然不透明，引发了关于其一致性、可解释性以及与传统应用商店优化（App Store Optimization, ASO）指标对齐程度的疑问。在本文中，我们对广泛使用的通用大语言模型如何生成、证明和排序移动应用推荐进行了实证分析。我们的贡献包括：(i) 从LLM输出中提取出的16种通用排序标准的分类法；(ii) 一个系统性的评估框架，用于分析推荐的一致性以及对显式排序指令的响应能力；以及(iii) 一个复现包，以支持基于人工智能的推荐系统的可复现性和未来研究。我们的研究结果表明，LLM依赖于一个广泛但碎片化的排序标准集合，这些标准仅部分与标准的ASO指标对齐。尽管排名靠前的应用在不同运行中往往具有一致性，但随着排序深度和搜索特定性的增加，变异性也随之上升。LLM对显式排序指令的敏感性各不相同——从显著的适应到几乎完全相同的输出——突显了它们在对话式应用发现中的复杂推理动态。我们的研究结果旨在帮助终端用户、应用开发者和推荐系统研究人员在新兴的对话式应用发现领域中更好地导航。",
        "translated_title": "基于大语言模型的移动应用推荐评估：一项实证研究",
        "label": [
            "LLM生成式推荐",
            "推荐系统评估"
        ],
        "label_reason": "论文研究LLM生成移动应用推荐并评估其一致性，与推荐系统生成及评估相关。",
        "relevance_score": 8,
        "novelty_score": 7,
        "novelty_reason": "提出LLM推荐的评估框架与排名标准，具有一定新颖性。"
    },
    {
        "title": "KrishokBondhu: A Retrieval-Augmented Voice-Based Agricultural Advisory\n  Call Center for Bengali Farmers",
        "url": "http://arxiv.org/abs/2510.18355v1",
        "pub_date": "2025-10-21",
        "summary": "In Bangladesh, many farmers continue to face challenges in accessing timely, expert-level agricultural guidance. This paper presents KrishokBondhu, a voice-enabled, call-centre-integrated advisory platform built on a Retrieval-Augmented Generation (RAG) framework, designed specifically for Bengali-speaking farmers. The system aggregates authoritative agricultural handbooks, extension manuals, and NGO publications; applies Optical Character Recognition (OCR) and document-parsing pipelines to digitize and structure the content; and indexes this corpus in a vector database for efficient semantic retrieval. Through a simple phone-based interface, farmers can call the system to receive real-time, context-aware advice: speech-to-text converts the Bengali query, the RAG module retrieves relevant content, a large language model (Gemma 3-4B) generates a context-grounded response, and text-to-speech delivers the answer in natural spoken Bengali. In a pilot evaluation, KrishokBondhu produced high-quality responses for 72.7% of diverse agricultural queries covering crop management, disease control, and cultivation practices. Compared to the KisanQRS benchmark, the system achieved a composite score of 4.53 (vs. 3.13) on a 5-point scale, a 44.7% improvement, with especially large gains in contextual richness (+367%) and completeness (+100.4%), while maintaining comparable relevance and technical specificity. Semantic similarity analysis further revealed a strong correlation between retrieved context and answer quality, emphasizing the importance of grounding generative responses in curated documentation. KrishokBondhu demonstrates the feasibility of integrating call-centre accessibility, multilingual voice interaction, and modern RAG techniques to deliver expert-level agricultural guidance to remote Bangladeshi farmers, paving the way toward a fully AI-driven agricultural advisory ecosystem.",
        "translated": "在孟加拉国，许多农民仍面临难以及时获得专家级农业指导的挑战。本文提出 KrishokBondhu，一个基于检索增强生成（RAG）框架的语音交互、呼叫中心集成的咨询平台，专门为讲孟加拉语的农民设计。该系统汇集了权威的农业手册、推广手册和非政府组织（NGO）出版物；应用光学字符识别（OCR）和文档解析流水线对内容进行数字化和结构化处理；并将该文档集索引至向量数据库中，以实现高效的语义召回。通过一个简单的基于电话的交互界面，农民可以拨打电话，实时获得情境感知的建议：语音转文本将孟加拉语的查询转换为文本，RAG 模块召回相关内容，一个大语言模型（Gemma 3-4B）生成基于上下文的响应，文本转语音则以自然的孟加拉语语音形式输出答案。在试点评估中，KrishokBondhu 对 72.7% 的多样化农业查询（涵盖作物管理、病虫害防治和栽培实践）提供了高质量的回复。与 KisanQRS 基准系统相比，该系统在 5 分制评分体系下获得了 4.53（对比 3.13）的综合得分，提升了 44.7%，尤其是在上下文丰富性（+367%）和完整性（+100.4%）方面表现出显著优势，同时保持了相当的相关性和技术准确性。语义相似性分析进一步揭示了召回上下文与答案质量之间的强相关性，强调了将生成式回复基于精心整理的文档的重要性。KrishokBondhu 证明了将呼叫中心的可访问性、多语言语音交互和现代 RAG 技术相结合，为偏远地区的孟加拉国农民提供专家级农业指导的可行性，为构建一个完全由人工智能驱动的农业咨询服务生态系统铺平了道路。",
        "translated_title": "KrishokBondhu：一个基于检索增强的语音农业咨询服务热线，面向孟加拉语农民",
        "label": [
            "LLM生成式推荐",
            "多模态推荐"
        ],
        "label_reason": "使用RAG和语音交互生成农业建议，间接涉及生成式推荐和多模态交互。",
        "relevance_score": 5,
        "novelty_score": 7,
        "novelty_reason": "将RAG与语音系统结合用于农业咨询，具有实际创新性和跨领域整合能力。"
    },
    {
        "title": "Enhancing Hotel Recommendations with AI: LLM-Based Review Summarization\n  and Query-Driven Insights",
        "url": "http://arxiv.org/abs/2510.18277v1",
        "pub_date": "2025-10-21",
        "summary": "The increasing number of data a booking platform such as Booking.com and AirBnB offers make it challenging for interested parties to browse through the available accommodations and analyze reviews in an efficient way. Efforts have been made from the booking platform providers to utilize recommender systems in an effort to enable the user to filter the results by factors such as stars, amenities, cost but most valuable insights can be provided by the unstructured text-based reviews. Going through these reviews one-by-one requires a substantial amount of time to be devoted while a respectable percentage of the reviews won't provide to the user what they are actually looking for.   This research publication explores how Large Language Models (LLMs) can enhance short rental apartments recommendations by summarizing and mining key insights from user reviews. The web application presented in this paper, named \"instaGuide\", automates the procedure of isolating the text-based user reviews from a property on the Booking.com platform, synthesizing the summary of the reviews, and enabling the user to query specific aspects of the property in an effort to gain feedback on their personal questions/criteria.   During the development of the instaGuide tool, numerous LLM models were evaluated based on accuracy, cost, and response quality. The results suggest that the LLM-powered summarization reduces significantly the amount of time the users need to devote on their search for the right short rental apartment, improving the overall decision-making procedure.",
        "translated": "随着 Booking.com 和 Airbnb 等预订平台提供的数据量不断增加，使利益相关者以高效的方式浏览可用住宿和分析评论变得具有挑战性。为了使用户能够根据星级、设施、价格等因素过滤结果，平台提供商已尝试采用推荐系统，但最有价值的见解往往来自非结构化的文本评论。逐条阅读这些评论需要用户投入大量时间，且相当一部分评论并不能提供用户真正关心的信息。本文研究探讨了大语言模型（LLM）如何通过总结和挖掘用户评论中的关键信息，提升短租公寓的推荐效果。本文提出了一款名为“instaGuide”的网络应用程序，该应用能够自动化地从 Booking.com 平台上提取某房产的基于文本的用户评论，生成评论摘要，并允许用户查询该房产的具体方面，以获取针对其个人问题或标准的反馈。在 instaGuide 工具的开发过程中，评估了多种 LLM 模型的准确性、成本和响应质量。结果表明，基于 LLM 的摘要显著减少了用户寻找合适的短租公寓所需的时间，从而提升了整体决策过程的效率。",
        "translated_title": "利用人工智能提升酒店推荐：基于大语言模型的评论摘要与查询驱动的洞察",
        "label": [
            "LLM生成式推荐",
            "通用推荐技术"
        ],
        "label_reason": "利用LLM进行酒店评论摘要生成，增强推荐系统的实用性。",
        "relevance_score": 7,
        "novelty_score": 6,
        "novelty_reason": "提出基于LLM的评论摘要方法，但未显著突破现有推荐范式。"
    },
    {
        "title": "LIME: Link-based user-item Interaction Modeling with decoupled xor\n  attention for Efficient test time scaling",
        "url": "http://arxiv.org/abs/2510.18239v1",
        "pub_date": "2025-10-21",
        "summary": "Scaling large recommendation systems requires advancing three major frontiers: processing longer user histories, expanding candidate sets, and increasing model capacity. While promising, transformers' computational cost scales quadratically with the user sequence length and linearly with the number of candidates. This trade-off makes it prohibitively expensive to expand candidate sets or increase sequence length at inference, despite the significant performance improvements.   We introduce \\textbf{LIME}, a novel architecture that resolves this trade-off. Through two key innovations, LIME fundamentally reduces computational complexity. First, low-rank ``link embeddings\" enable pre-computation of attention weights by decoupling user and candidate interactions, making the inference cost nearly independent of candidate set size. Second, a linear attention mechanism, \\textbf{LIME-XOR}, reduces the complexity with respect to user sequence length from quadratic ($O(N^2)$) to linear ($O(N)$).   Experiments on public and industrial datasets show LIME achieves near-parity with state-of-the-art transformers but with a 10$\\times$ inference speedup on large candidate sets or long sequence lengths. When tested on a major recommendation platform, LIME improved user engagement while maintaining minimal inference costs with respect to candidate set size and user history length, establishing a new paradigm for efficient and expressive recommendation systems.",
        "translated": "扩展大规模推荐系统需要推进三个主要方向：处理更长的用户历史、扩大候选集规模以及提升模型容量。虽然Transformer在这些方面表现出潜力，但其计算成本随着用户序列长度呈平方级增长，随着候选数量呈线性增长。这种权衡使得在推理阶段扩展候选集或增加序列长度的成本变得非常高，即便其在性能上有显著提升。我们提出了一种新颖的架构 **LIME**，来解决这一权衡问题。通过两个关键创新，LIME从根本上降低了计算复杂度。首先，低秩的“链接嵌入”（link embeddings）通过解耦用户与候选之间的交互，使得注意力权重可以预先计算，从而使推理成本几乎不再依赖于候选集的规模。其次，线性注意力机制 **LIME-XOR** 将与用户序列长度相关的复杂度从平方级（$O(N^2)$）降低到线性级（$O(N)$）。我们在公开和工业数据集上的实验表明，LIME在性能上接近最先进的Transformer模型，但在处理大规模候选集或长序列时，推理速度提升了10倍。在主流推荐平台上的测试结果表明，LIME在保持候选集规模和用户历史长度的最小推理成本的同时，显著提升了用户参与度，从而为高效且表达能力强的推荐系统建立了一种新的范式。",
        "translated_title": "LIME：基于链接的用户-物料交互建模与解耦异或注意力，用于高效的在线测试扩展",
        "label": [
            "精排（Ranking）",
            "序列推荐（Sequential Recommendation）",
            "通用推荐技术（General Recommendation Techniques）"
        ],
        "label_reason": "论文聚焦推荐系统的高效序列建模与大规模候选集处理",
        "relevance_score": 9,
        "novelty_score": 8,
        "novelty_reason": "提出解耦注意力与线性机制，显著降低推理复杂度"
    },
    {
        "title": "From AutoRecSys to AutoRecLab: A Call to Build, Evaluate, and Govern\n  Autonomous Recommender-Systems Research Labs",
        "url": "http://arxiv.org/abs/2510.18104v1",
        "pub_date": "2025-10-20",
        "summary": "Recommender-systems research has accelerated model and evaluation advances, yet largely neglects automating the research process itself. We argue for a shift from narrow AutoRecSys tools -- focused on algorithm selection and hyper-parameter tuning -- to an Autonomous Recommender-Systems Research Lab (AutoRecLab) that integrates end-to-end automation: problem ideation, literature analysis, experimental design and execution, result interpretation, manuscript drafting, and provenance logging. Drawing on recent progress in automated science (e.g., multi-agent AI Scientist and AI Co-Scientist systems), we outline an agenda for the RecSys community: (1) build open AutoRecLab prototypes that combine LLM-driven ideation and reporting with automated experimentation; (2) establish benchmarks and competitions that evaluate agents on producing reproducible RecSys findings with minimal human input; (3) create review venues for transparently AI-generated submissions; (4) define standards for attribution and reproducibility via detailed research logs and metadata; and (5) foster interdisciplinary dialogue on ethics, governance, privacy, and fairness in autonomous research. Advancing this agenda can increase research throughput, surface non-obvious insights, and position RecSys to contribute to emerging Artificial Research Intelligence. We conclude with a call to organise a community retreat to coordinate next steps and co-author guidance for the responsible integration of automated research systems.",
        "translated": "推荐系统研究推动了模型和评估方法的进步，但很大程度上忽视了研究过程本身的自动化。我们主张从狭义的 AutoRecSys 工具（专注于算法选择和超参数调优）转向一个自主的推荐系统研究实验室（Autonomous Recommender-Systems Research Lab，AutoRecLab），该实验室集成端到端的自动化：问题构思、文献分析、实验设计与执行、结果解释、论文撰写以及溯源日志记录。借鉴自动化科学领域的最新进展（例如多智能体 AI 科学家和 AI 协同科学家系统），我们为推荐系统社区提出一个研究议程：（1）构建开源的 AutoRecLab 原型，将大语言模型驱动的构思与报告撰写与自动实验相结合；（2）建立基准测试和竞赛，评估智能体在最小人工输入下生成可复现推荐系统发现的能力；（3）创建透明的、针对 AI 生成论文的审阅渠道；（4）通过详细的研究日志和元数据定义归属和可复现性的标准；以及（5）促进关于自主研究中伦理、治理、隐私和公平性的跨学科对话。推动这一议程可以提升研究效率，揭示非显而易见的洞见，并使推荐系统在新兴的人工智能研究智能（Artificial Research Intelligence）中发挥重要作用。最后，我们呼吁组织一次社区退思会，以协调下一步工作，并共同撰写关于自动研究系统负责任整合的指导文件。",
        "translated_title": "从 AutoRecSys 到 AutoRecLab：呼吁构建、评估与治理自主推荐系统研究实验室",
        "label": [
            "推荐系统评估",
            "通用推荐技术",
            "LLM生成式推荐"
        ],
        "label_reason": "论文提出自动化推荐系统研究实验室框架，涉及生成、评估与治理，与推荐系统紧密相关。",
        "relevance_score": 8,
        "novelty_score": 7,
        "novelty_reason": "提出将LLM与自动化科学结合的新研究范式，具有一定的创新性和前瞻性。"
    },
    {
        "title": "Grasp Any Region: Towards Precise, Contextual Pixel Understanding for\n  Multimodal LLMs",
        "url": "http://arxiv.org/abs/2510.18876v1",
        "pub_date": "2025-10-21",
        "summary": "While Multimodal Large Language Models (MLLMs) excel at holistic understanding, they struggle in capturing the dense world with complex scenes, requiring fine-grained analysis of intricate details and object inter-relationships. Region-level MLLMs have been a promising step. However, previous attempts are generally optimized to understand given regions in isolation, neglecting crucial global contexts. To address this, we introduce Grasp Any Region (GAR) for comprehen- sive region-level visual understanding. Empowered by an effective RoI-aligned feature replay technique, GAR supports (1) precise perception by leveraging necessary global contexts, and (2) modeling interactions between multiple prompts. Together, it then naturally achieves (3) advanced compositional reasoning to answer specific free-form questions about any region, shifting the paradigm from passive description to active dialogue. Moreover, we construct GAR-Bench, which not only provides a more accurate evaluation of single-region comprehension, but also, more importantly, measures interactions and complex reasoning across multiple regions. Extensive experiments have demonstrated that GAR-1B not only maintains the state-of-the-art captioning capabilities, e.g., outperforming DAM-3B +4.5 on DLC-Bench, but also excels at modeling relationships between multiple prompts with advanced comprehension capabilities, even surpassing InternVL3-78B on GAR-Bench-VQA. More importantly, our zero-shot GAR-8B even outperforms in-domain VideoRefer-7B on VideoRefer-BenchQ, indicating its strong capabilities can be easily transferred to videos.",
        "translated": "虽然多模态大语言模型（MLLMs）在整体理解方面表现出色，但它们在捕捉具有复杂场景的密集世界方面仍面临挑战，需要对细节和物体之间的关系进行细致分析。区域级别的 MLLMs 提供了一个有前景的方向。然而，以往的尝试通常仅优化了对给定区域的独立理解，忽略了关键的全局上下文。为了解决这一问题，我们提出了 Grasp Any Region（GAR），旨在实现全面的区域级视觉理解。借助有效的 RoI 对齐特征重放技术，GAR 支持（1）通过利用必要的全局上下文实现精确感知，以及（2）对多个提示之间的交互进行建模。结合这两方面的能力，GAR 自然地实现了（3）高级的组合推理能力，以回答关于任意区域的特定自由形式问题，从而将范式从被动描述转变为主动对话。此外，我们构建了 GAR-Bench，它不仅能够更准确地评估单区域理解能力，更重要的是，可以衡量多个区域之间的交互与复杂推理。大量实验表明，GAR-1B 不仅保持了最先进的图像描述能力，例如在 DLC-Bench 上优于 DAM-3B +4.5，而且在具有高级理解能力的多提示关系建模方面表现出色，甚至在 GAR-Bench-VQA 上超越了 InternVL3-78B。更重要的是，我们零样本的 GAR-8B 在 VideoRefer-BenchQ 上的表现甚至优于领域内的 VideoRefer-7B，表明其强大能力可以轻松迁移到视频中。",
        "translated_title": "Grasp Any Region: Towards Precise, Contextual Pixel Understanding for Multimodal LLMs\n\n抓住任意区域：面向精确、上下文感知像素理解的多模态大语言模型",
        "label": [],
        "label_reason": "论文关注区域级视觉理解，偏向high-level任务。",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出新的区域级交互建模方法，但属于常规改进。"
    },
    {
        "title": "DSI-Bench: A Benchmark for Dynamic Spatial Intelligence",
        "url": "http://arxiv.org/abs/2510.18873v1",
        "pub_date": "2025-10-21",
        "summary": "Reasoning about dynamic spatial relationships is essential, as both observers and objects often move simultaneously. Although vision-language models (VLMs) and visual expertise models excel in 2D tasks and static scenarios, their ability to fully understand dynamic 3D scenarios remains limited. We introduce Dynamic Spatial Intelligence and propose DSI-Bench, a benchmark with nearly 1,000 dynamic videos and over 1,700 manually annotated questions covering nine decoupled motion patterns of observers and objects. Spatially and temporally symmetric designs reduce biases and enable systematic evaluation of models' reasoning about self-motion and object motion. Our evaluation of 14 VLMs and expert models reveals key limitations: models often conflate observer and object motion, exhibit semantic biases, and fail to accurately infer relative relationships in dynamic scenarios. Our DSI-Bench provides valuable findings and insights about the future development of general and expertise models with dynamic spatial intelligence.",
        "translated": "动态空域关系的推理至关重要，因为观察者和物体通常会同时运动。尽管视觉-语言模型（VLMs）和视觉专家模型在二维任务和静态场景中表现出色，但它们在全面理解动态三维场景方面的能力仍然有限。我们引入了动态空域智能（Dynamic Spatial Intelligence），并提出了DSI-Bench，这是一个包含近1,000个动态视频和1,700多个手动标注问题的基准测试集，涵盖了观察者与物体之间九种解耦的运动模式。时空对称的设计减少了偏见，并能够系统评估模型对自运动和物体运动的推理能力。我们对14个VLMs和专家模型的评估揭示了关键的局限性：模型常常将观察者运动与物体运动混淆，表现出语义偏见，并且在动态场景中无法准确推断相对关系。我们的DSI-Bench为具备动态空域智能的一般模型和专家模型的未来发展提供了有价值的发现与洞见。",
        "translated_title": "DSI-Bench：动态空间智能基准",
        "label": [],
        "label_reason": "论文关注动态空间推理，属于high-level视觉任务，不涉及像素级图像恢复或增强。",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出了新的动态空间智能基准DSI-Bench，但方法上无本质性的low-level图像处理创新。"
    },
    {
        "title": "LightMem: Lightweight and Efficient Memory-Augmented Generation",
        "url": "http://arxiv.org/abs/2510.18866v1",
        "pub_date": "2025-10-21",
        "summary": "Despite their remarkable capabilities, Large Language Models (LLMs) struggle to effectively leverage historical interaction information in dynamic and complex environments. Memory systems enable LLMs to move beyond stateless interactions by introducing persistent information storage, retrieval, and utilization mechanisms. However, existing memory systems often introduce substantial time and computational overhead. To this end, we introduce a new memory system called LightMem, which strikes a balance between the performance and efficiency of memory systems. Inspired by the Atkinson-Shiffrin model of human memory, LightMem organizes memory into three complementary stages. First, cognition-inspired sensory memory rapidly filters irrelevant information through lightweight compression and groups information according to their topics. Next, topic-aware short-term memory consolidates these topic-based groups, organizing and summarizing content for more structured access. Finally, long-term memory with sleep-time update employs an offline procedure that decouples consolidation from online inference. Experiments on LongMemEval with GPT and Qwen backbones show that LightMem outperforms strong baselines in accuracy (up to 10.9% gains) while reducing token usage by up to 117x, API calls by up to 159x, and runtime by over 12x. The code is available at https://github.com/zjunlp/LightMem.",
        "translated": "尽管大型语言模型（LLMs）具备显著的能力，但它们在动态且复杂的环境中难以有效利用历史交互信息。记忆系统通过引入持续的信息存储、检索和利用机制，使得 LLM 能够超越无状态的交互模式。然而，现有的记忆系统通常会引入较大的时间与计算开销。为此，我们提出了一种新的记忆系统 LightMem，在记忆系统的性能与效率之间取得了平衡。受人类记忆的 Atkinson-Shiffrin 模型启发，LightMem 将记忆组织为三个互补的阶段。首先，受认知启发的感官记忆通过轻量级压缩快速过滤无关信息，并根据主题对信息进行分组。接下来，主题感知的短期记忆对这些基于主题的分组进行整合，组织和总结内容以实现更结构化的访问。最后，具有睡眠时间更新机制的长期记忆采用离线过程，将整合操作与在线推理解耦。在 LongMemEval 上的实验表明，基于 GPT 和 Qwen 的 LightMem 在准确率方面优于强基线（最高提升 10.9%），同时将 token 使用量减少了最多 117 倍，API 调用量减少了最多 159 倍，运行时间减少了超过 12 倍。代码可在 https://github.com/zjunlp/LightMem 获取。",
        "translated_title": "LightMem: 轻量且高效的记忆增强生成",
        "label": [],
        "label_reason": "论文聚焦语言模型记忆系统优化，非图像处理任务",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出轻量高效记忆系统，改进信息存储与检索机制"
    },
    {
        "title": "DP$^2$O-SR: Direct Perceptual Preference Optimization for Real-World\n  Image Super-Resolution",
        "url": "http://arxiv.org/abs/2510.18851v1",
        "pub_date": "2025-10-21",
        "summary": "Benefiting from pre-trained text-to-image (T2I) diffusion models, real-world image super-resolution (Real-ISR) methods can synthesize rich and realistic details. However, due to the inherent stochasticity of T2I models, different noise inputs often lead to outputs with varying perceptual quality. Although this randomness is sometimes seen as a limitation, it also introduces a wider perceptual quality range, which can be exploited to improve Real-ISR performance. To this end, we introduce Direct Perceptual Preference Optimization for Real-ISR (DP$^2$O-SR), a framework that aligns generative models with perceptual preferences without requiring costly human annotations. We construct a hybrid reward signal by combining full-reference and no-reference image quality assessment (IQA) models trained on large-scale human preference datasets. This reward encourages both structural fidelity and natural appearance. To better utilize perceptual diversity, we move beyond the standard best-vs-worst selection and construct multiple preference pairs from outputs of the same model. Our analysis reveals that the optimal selection ratio depends on model capacity: smaller models benefit from broader coverage, while larger models respond better to stronger contrast in supervision. Furthermore, we propose hierarchical preference optimization, which adaptively weights training pairs based on intra-group reward gaps and inter-group diversity, enabling more efficient and stable learning. Extensive experiments across both diffusion- and flow-based T2I backbones demonstrate that DP$^2$O-SR significantly improves perceptual quality and generalizes well to real-world benchmarks.",
        "translated": "借助预训练的文生图（T2I）扩散模型，真实世界图像超分辨率（Real-ISR）方法可以合成丰富的逼真细节。然而，由于 T2I 模型固有的随机性，不同的噪声输入通常会导致具有不同感知质量的输出。尽管这种随机性有时被视为限制，但同时也引入了更广泛的感知质量范围，可以加以利用以提升 Real-ISR 的性能。为此，我们提出面向 Real-ISR 的直接感知偏好优化（Direct Perceptual Preference Optimization for Real-ISR，DP$^2$O-SR），这是一种无需代价高昂的人类标注即可将生成模型与感知偏好对齐的框架。我们通过结合在大规模人类偏好数据集上训练的全参考和无参考图像质量评估（IQA）模型，构建了一个混合奖励信号。该奖励信号同时鼓励结构保真度和自然外观。为了更好地利用感知多样性，我们超越了标准的最佳与最差选择策略，从同一模型的输出中构建多个偏好对。我们的分析表明，最优的选择比例取决于模型容量：小模型受益于更广泛的覆盖，而大模型则对更强的监督对比响应更好。此外，我们提出了分层偏好优化方法，该方法基于组内奖励差距和组间多样性自适应地加权训练对，从而实现更高效和稳定的训练。在基于扩散模型和流模型的 T2I 主干网络上的大量实验表明，DP$^2$O-SR 显著提升了感知质量，并在真实世界基准上表现出良好的泛化能力。",
        "translated_title": "DP$^2$O-SR：面向真实场景图像超分辨率的直接感知偏好优化",
        "label": [
            "超分辨率",
            "图像增强"
        ],
        "label_reason": "论文聚焦真实图像超分辨率，属于图像恢复与增强任务",
        "relevance_score": 9,
        "novelty_score": 8,
        "novelty_reason": "提出感知偏好优化框架，改进扩散模型用于图像超分辨率"
    },
    {
        "title": "See the Text: From Tokenization to Visual Reading",
        "url": "http://arxiv.org/abs/2510.18840v1",
        "pub_date": "2025-10-21",
        "summary": "People see text. Humans read by recognizing words as visual objects, including their shapes, layouts, and patterns, before connecting them to meaning, which enables us to handle typos, distorted fonts, and various scripts effectively. Modern large language models (LLMs), however, rely on subword tokenization, fragmenting text into pieces from a fixed vocabulary. While effective for high-resource languages, this approach over-segments low-resource languages, yielding long, linguistically meaningless sequences and inflating computation. In this work, we challenge this entrenched paradigm and move toward a vision-centric alternative. Our method, SeeTok, renders text as images (visual-text) and leverages pretrained multimodal LLMs to interpret them, reusing strong OCR and text-vision alignment abilities learned from large-scale multimodal training. Across three different language tasks, SeeTok matches or surpasses subword tokenizers while requiring 4.43 times fewer tokens and reducing FLOPs by 70.5%, with additional gains in cross-lingual generalization, robustness to typographic noise, and linguistic hierarchy. SeeTok signals a shift from symbolic tokenization to human-like visual reading, and takes a step toward more natural and cognitively inspired language models.",
        "translated": "人们看到的是文本。人类通过将词语识别为视觉对象，包括它们的形状、布局和模式，然后将其与语义联系起来进行阅读，这种机制使我们能够有效地处理拼写错误、变形字体和各种书写系统。然而，现代的大型语言模型（LLMs）依赖于子词分词，将文本分割成来自固定词典的片段。虽然这种方法对于高资源语言是有效的，但它对低资源语言进行了过度分割，产生了长而语言学上无意义的序列，并增加了计算负担。在本研究中，我们挑战了这种根深蒂固的范式，转向一种以视觉为中心的替代方法。我们的方法 SeeTok 将文本渲染为图像（视觉文本），并利用预训练的多模态 LLMs 对其进行解释，复用大规模多模态训练中学到的强大的 OCR 和图文对齐能力。在三个不同的语言任务中，SeeTok 在使用更少 4.43 倍 tokens 和减少 70.5% FLOPs 的前提下，表现与或优于子词分词器，并在跨语言泛化能力、对排版噪声的鲁棒性以及语言层次结构方面获得了额外的提升。SeeTok 标志着从符号化分词向类人视觉阅读方式的转变，并朝着更加自然和受认知启发的语言模型迈出了关键一步。",
        "translated_title": "从分词到视觉阅读",
        "label": [],
        "label_reason": "论文聚焦于文本处理和语言模型，非图像像素级质量改善任务。",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出视觉文本表示方法，对低资源语言和计算效率有显著改进。"
    },
    {
        "title": "FedDEAP: Adaptive Dual-Prompt Tuning for Multi-Domain Federated Learning",
        "url": "http://arxiv.org/abs/2510.18837v1",
        "pub_date": "2025-10-21",
        "summary": "Federated learning (FL) enables multiple clients to collaboratively train machine learning models without exposing local data, balancing performance and privacy. However, domain shift and label heterogeneity across clients often hinder the generalization of the aggregated global model. Recently, large-scale vision-language models like CLIP have shown strong zero-shot classification capabilities, raising the question of how to effectively fine-tune CLIP across domains in a federated setting. In this work, we propose an adaptive federated prompt tuning framework, FedDEAP, to enhance CLIP's generalization in multi-domain scenarios. Our method includes the following three key components: (1) To mitigate the loss of domain-specific information caused by label-supervised tuning, we disentangle semantic and domain-specific features in images by using semantic and domain transformation networks with unbiased mappings; (2) To preserve domain-specific knowledge during global prompt aggregation, we introduce a dual-prompt design with a global semantic prompt and a local domain prompt to balance shared and personalized information; (3) To maximize the inclusion of semantic and domain information from images in the generated text features, we align textual and visual representations under the two learned transformations to preserve semantic and domain consistency. Theoretical analysis and extensive experiments on four datasets demonstrate the effectiveness of our method in enhancing the generalization of CLIP for federated image recognition across multiple domains.",
        "translated": "联邦学习（FL）允许多个客户端在不暴露本地数据的情况下协同训练机器学习模型，从而在性能与隐私之间取得平衡。然而，客户端之间的域偏移和标签异质性通常会限制聚合后的全局模型的泛化能力。最近，像 CLIP 这样的大规模视觉-语言模型展现出了强大的零样本分类能力，引发了一个问题：如何在联邦设置下跨域有效地对 CLIP 进行微调。在本研究中，我们提出了一种自适应联邦提示微调框架 FedDEAP，以增强 CLIP 在多域场景中的泛化能力。我们的方法包括以下三个关键组成部分：(1) 为缓解标签监督微调导致的域特定信息丢失，我们通过语义和域变换网络实现图像中语义特征与域特定特征的解耦，并利用无偏映射；(2) 为在全局提示聚合过程中保留域特定知识，我们引入了包含全局语义提示和本地域提示的双提示设计，以平衡共享信息与个性化信息；(3) 为最大化图像中语义和域信息在生成文本特征中的包含，我们在两种学习变换下对齐文本和视觉表示，以保持语义和域一致性。理论分析和在四个数据集上的大量实验表明，我们的方法在增强 CLIP 联邦图像识别的跨域泛化能力方面是有效的。",
        "translated_title": "FedDEAP：多域联邦学习的自适应双提示调优",
        "label": [],
        "label_reason": "论文聚焦联邦学习中的图像分类，不涉及像素级图像恢复或增强。",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "方法基于CLIP进行改进，设计了双提示机制，有一定创新但属于常规迁移应用。"
    },
    {
        "title": "Unifying and Enhancing Graph Transformers via a Hierarchical Mask\n  Framework",
        "url": "http://arxiv.org/abs/2510.18825v1",
        "pub_date": "2025-10-21",
        "summary": "Graph Transformers (GTs) have emerged as a powerful paradigm for graph representation learning due to their ability to model diverse node interactions. However, existing GTs often rely on intricate architectural designs tailored to specific interactions, limiting their flexibility. To address this, we propose a unified hierarchical mask framework that reveals an underlying equivalence between model architecture and attention mask construction. This framework enables a consistent modeling paradigm by capturing diverse interactions through carefully designed attention masks. Theoretical analysis under this framework demonstrates that the probability of correct classification positively correlates with the receptive field size and label consistency, leading to a fundamental design principle: an effective attention mask should ensure both a sufficiently large receptive field and a high level of label consistency. While no single existing mask satisfies this principle across all scenarios, our analysis reveals that hierarchical masks offer complementary strengths, motivating their effective integration. Then, we introduce M3Dphormer, a Mixture-of-Experts-based Graph Transformer with Multi-Level Masking and Dual Attention Computation. M3Dphormer incorporates three theoretically grounded hierarchical masks and employs a bi-level expert routing mechanism to adaptively integrate multi-level interaction information. To ensure scalability, we further introduce a dual attention computation scheme that dynamically switches between dense and sparse modes based on local mask sparsity. Extensive experiments across multiple benchmarks demonstrate that M3Dphormer achieves state-of-the-art performance, validating the effectiveness of our unified framework and model design.",
        "translated": "图神经网络中的图变换器（Graph Transformers, GTs）由于能够建模多样化的节点交互，已成为图表示学习的一种强大范式。然而，现有的 GTs 通常依赖于针对特定交互量身定制的复杂架构设计，限制了其灵活性。为了解决这一问题，我们提出了一种统一的层次化掩码框架，揭示了模型架构与注意力掩码构建之间潜在的等价关系。该框架通过精心设计的注意力掩码捕捉多样化的交互，从而实现一致的建模范式。在该框架下的理论分析表明，正确分类的概率与感受野大小和标签一致性呈正相关，由此引出一个基本的设计原则：有效的注意力掩码应同时确保足够大的感受野和较高的标签一致性。虽然现有的任何单一掩码都无法在所有场景中满足这一原则，但我们的分析表明，层次化掩码具有互补的优势，这促使我们对其进行有效的融合。随后，我们介绍了 M3Dphormer，这是一种基于专家混合（Mixture-of-Experts）的图变换器，结合了多级掩码和双注意力计算。M3Dphormer 集成了三种理论支撑的层次化掩码，并采用双层专家路由机制，以自适应地融合多级交互信息。为了确保可扩展性，我们进一步引入了一种双注意力计算方案，根据局部掩码的稀疏性在密集模式和稀疏模式之间动态切换。在多个基准数据集上的大量实验表明，M3Dphormer 取得了最先进的性能，验证了我们统一框架和模型设计的有效性。",
        "translated_title": "通过分层掩码框架统一并增强图变换器",
        "label": [],
        "label_reason": "论文聚焦图神经网络设计，不涉及图像像素级处理",
        "relevance_score": 1,
        "novelty_score": 6,
        "novelty_reason": "提出分层掩码框架和混合专家机制，有一定改进"
    },
    {
        "title": "SAM 2++: Tracking Anything at Any Granularity",
        "url": "http://arxiv.org/abs/2510.18822v1",
        "pub_date": "2025-10-21",
        "summary": "Video tracking aims at finding the specific target in subsequent frames given its initial state. Due to the varying granularity of target states across different tasks, most existing trackers are tailored to a single task and heavily rely on custom-designed modules within the individual task, which limits their generalization and leads to redundancy in both model design and parameters. To unify video tracking tasks, we present SAM 2++, a unified model towards tracking at any granularity, including masks, boxes, and points. First, to extend target granularity, we design task-specific prompts to encode various task inputs into general prompt embeddings, and a unified decoder to unify diverse task results into a unified form pre-output. Next, to satisfy memory matching, the core operation of tracking, we introduce a task-adaptive memory mechanism that unifies memory across different granularities. Finally, we introduce a customized data engine to support tracking training at any granularity, producing a large and diverse video tracking dataset with rich annotations at three granularities, termed Tracking-Any-Granularity, which represents a comprehensive resource for training and benchmarking on unified tracking. Comprehensive experiments on multiple benchmarks confirm that SAM 2++ sets a new state of the art across diverse tracking tasks at different granularities, establishing a unified and robust tracking framework.",
        "translated": "视频跟踪旨在在给定目标初始状态的情况下，于后续帧中定位该特定目标。由于不同任务中目标状态的粒度存在差异，大多数现有的跟踪器仅适用于单一任务，并高度依赖任务内部定制设计的模块，这限制了其泛化能力，并导致模型设计与参数上的冗余。为了统一视频跟踪任务，我们提出 SAM 2++，一个面向任意粒度跟踪的统一模型，包括掩膜、框和点。首先，为了扩展目标的粒度，我们设计了任务特定的提示，将各种任务输入编码为通用的提示嵌入，并引入统一的解码器，将多样化的任务结果统一为预输出的统一形式。其次，为满足跟踪中的核心操作——记忆匹配，我们引入了一个任务自适应的记忆机制，以统一不同粒度下的记忆信息。最后，我们提出一个定制的数据引擎，以支持任意粒度下的跟踪训练，生成了一个包含三种粒度丰富标注的大规模且多样化的视频跟踪数据集，称为 Tracking-Any-Granularity，该数据集为统一跟踪的训练与基准测试提供了全面的资源。在多个基准测试上的综合性实验表明，SAM 2++ 在不同粒度的多样化跟踪任务中均达到了新的最先进性能，建立了一个统一且鲁棒的跟踪框架。",
        "translated_title": "SAM 2++：以任意粒度追踪任何对象",
        "label": [],
        "label_reason": "论文聚焦视频跟踪，不属于图像像素级恢复或增强任务。",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出统一视频跟踪框架，改进了任务适应性和泛化能力。"
    },
    {
        "title": "An Explainable Hybrid AI Framework for Enhanced Tuberculosis and Symptom\n  Detection",
        "url": "http://arxiv.org/abs/2510.18819v1",
        "pub_date": "2025-10-21",
        "summary": "Tuberculosis remains a critical global health issue, particularly in resource-limited and remote areas. Early detection is vital for treatment, yet the lack of skilled radiologists underscores the need for artificial intelligence (AI)-driven screening tools. Developing reliable AI models is challenging due to the necessity for large, high-quality datasets, which are costly to obtain. To tackle this, we propose a teacher--student framework which enhances both disease and symptom detection on chest X-rays by integrating two supervised heads and a self-supervised head. Our model achieves an accuracy of 98.85% for distinguishing between COVID-19, tuberculosis, and normal cases, and a macro-F1 score of 90.09% for multilabel symptom detection, significantly outperforming baselines. The explainability assessments also show the model bases its predictions on relevant anatomical features, demonstrating promise for deployment in clinical screening and triage settings.",
        "translated": "结核病仍然是一个重要的全球性健康问题，尤其是在资源匮乏和偏远地区。早期检测对于治疗至关重要，然而，专业放射科医生的缺乏凸显了需要基于人工智能（AI）的筛查工具。由于开发可靠的AI模型需要大量的高质量数据集，而这些数据集的获取成本较高，因此这一过程面临挑战。为了解决这一问题，我们提出了一种教师-学生框架，通过整合两个监督头和一个自监督头，以提升胸部X光图像中疾病和症状的检测能力。我们的模型在区分新冠肺炎、结核病和正常病例方面实现了98.85%的准确率，在多标签症状检测中达到了90.09%的宏F1分数，显著优于基线方法。可解释性评估也表明，该模型基于相关的解剖特征进行预测，展现出在临床筛查和分级诊疗中的应用潜力。",
        "translated_title": "一种用于增强结核病和症状检测的可解释混合人工智能框架",
        "label": [],
        "label_reason": "论文主要关注医学图像的疾病分类，不涉及像素级图像恢复或增强",
        "relevance_score": 2,
        "novelty_score": 5,
        "novelty_reason": "提出了一种教师-学生框架，但方法改进较为常规"
    },
    {
        "title": "A Geometric Approach to Steerable Convolutions",
        "url": "http://arxiv.org/abs/2510.18813v1",
        "pub_date": "2025-10-21",
        "summary": "In contrast to the somewhat abstract, group theoretical approach adopted by many papers, our work provides a new and more intuitive derivation of steerable convolutional neural networks in $d$ dimensions. This derivation is based on geometric arguments and fundamental principles of pattern matching. We offer an intuitive explanation for the appearance of the Clebsch--Gordan decomposition and spherical harmonic basis functions. Furthermore, we suggest a novel way to construct steerable convolution layers using interpolation kernels that improve upon existing implementation, and offer greater robustness to noisy data.",
        "translated": "与许多论文所采用的略显抽象的群论方法不同，我们的工作提供了一种新的、更直观的推导方法，用于 $d$ 维的可操控卷积神经网络。该推导基于几何论证和模式匹配的基本原理。我们对 Clebsch--Gordan 分解和球谐基函数的出现给出了直观的解释。此外，我们提出了一种新颖的构建可操控卷积层的方法，该方法使用插值核，改进了现有的实现方式，并对噪声数据表现出更强的鲁棒性。",
        "translated_title": "几何可转向卷积方法",
        "label": [
            "图像去噪"
        ],
        "label_reason": "论文探讨 steerable 卷积的几何方法，可能用于提升图像去噪的鲁棒性。",
        "relevance_score": 5,
        "novelty_score": 7,
        "novelty_reason": "提出基于几何和插值核的新 steerable 卷积实现方式，具有一定创新性。"
    },
    {
        "title": "ProCLIP: Progressive Vision-Language Alignment via LLM-based Embedder",
        "url": "http://arxiv.org/abs/2510.18795v1",
        "pub_date": "2025-10-21",
        "summary": "The original CLIP text encoder is limited by a maximum input length of 77 tokens, which hampers its ability to effectively process long texts and perform fine-grained semantic understanding. In addition, the CLIP text encoder lacks support for multilingual inputs. All these limitations significantly restrict its applicability across a broader range of tasks. Recent studies have attempted to replace the CLIP text encoder with an LLM-based embedder to enhance its ability in processing long texts, multilingual understanding, and fine-grained semantic comprehension. However, because the representation spaces of LLMs and the vision-language space of CLIP are pretrained independently without alignment priors, direct alignment using contrastive learning can disrupt the intrinsic vision-language alignment in the CLIP image encoder, leading to an underutilization of the knowledge acquired during pre-training. To address this challenge, we propose ProCLIP, a curriculum learning-based progressive vision-language alignment framework to effectively align the CLIP image encoder with an LLM-based embedder. Specifically, ProCLIP first distills knowledge from CLIP's text encoder into the LLM-based embedder to leverage CLIP's rich pretrained knowledge while establishing initial alignment between the LLM embedder and CLIP image encoder. Subsequently, ProCLIP further aligns the CLIP image encoder with the LLM-based embedder through image-text contrastive tuning, employing self-distillation regularization to avoid overfitting. To achieve a more effective alignment, instance semantic alignment loss and embedding structure alignment loss are employed during representation inheritance and contrastive tuning. The Code is available at https://github.com/VisionXLab/ProCLIP",
        "translated": "原始的 CLIP 文本编码器受到最大输入长度为 77 个 token 的限制，这阻碍了其对长文本进行有效处理和实现细粒度语义理解的能力。此外，CLIP 文本编码器不支持多语言输入。所有这些限制显著制约了其在更广泛任务中的适用性。近期的研究尝试将 CLIP 文本编码器替换为基于大语言模型（LLM）的嵌入器，以增强其处理长文本、多语言理解和细粒度语义理解的能力。然而，由于 LLMs 和 CLIP 的视觉-语言空间是独立预训练的，且没有对齐先验知识，直接使用对比学习进行对齐可能会破坏 CLIP 图像编码器中固有的视觉-语言对齐关系，从而导致预训练阶段获得的知识未能充分利用。为了解决这一问题，我们提出 ProCLIP，一种基于课程学习的渐进式视觉-语言对齐框架，以有效地将 CLIP 图像编码器与基于 LLM 的嵌入器对齐。具体来说，ProCLIP 首先将 CLIP 文本编码器的知识蒸馏到基于 LLM 的嵌入器中，从而利用 CLIP 丰富的预训练知识，同时在 LLM 嵌入器和 CLIP 图像编码器之间建立初步对齐。随后，ProCLIP 通过图像-文本对比调优进一步对齐 CLIP 图像编码器与基于 LLM 的嵌入器，并采用自蒸馏正则化方法避免过拟合。为了实现更有效的对齐，在表示继承和对比调优过程中分别引入了实例语义对齐损失和嵌入结构对齐损失。代码已发布于 https://github.com/VisionXLab/ProCLIP",
        "translated_title": "ProCLIP：基于大语言模型的嵌入器实现渐进式视觉-语言对齐",
        "label": [],
        "label_reason": "论文主要涉及视觉-语言对齐，不属于低层图像处理任务。",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出基于课程学习的渐进式视觉-语言对齐框架，具有一定创新性。"
    },
    {
        "title": "Rebellious Student: A Complementary Learning Framework for Background\n  Feature Enhancement in Hyperspectral Anomaly Detection",
        "url": "http://arxiv.org/abs/2510.18781v1",
        "pub_date": "2025-10-21",
        "summary": "A recent class of hyperspectral anomaly detection methods that can be trained once on background datasets and then universally deployed -- without per-scene retraining or parameter tuning -- has demonstrated remarkable efficiency and robustness. Building upon this paradigm, we focus on the integration of spectral and spatial cues and introduce a novel \"Rebellious Student\" framework for complementary feature learning. Unlike conventional teacher-student paradigms driven by imitation, our method intentionally trains the spatial branch to diverge from the spectral teacher, thereby learning complementary spatial patterns that the teacher fails to capture. A two-stage learning strategy is adopted: (1) a spectral enhancement network is first trained via reverse distillation to obtain robust background spectral representations; and (2) a spatial network -- the rebellious student -- is subsequently optimized using decorrelation losses that enforce feature orthogonality while maintaining reconstruction fidelity to avoid irrelevant noise. Once trained, the framework enhances both spectral and spatial background features, enabling parameter-free and training-free anomaly detection when paired with conventional detectors. Extensive experiments on the HAD100 benchmark show substantial improvements over several established baselines with minimal computational overhead, confirming the effectiveness and generality of the proposed complementary learning paradigm. Our code is publicly available at https://github.com/xjpp2016/FERS.",
        "translated": "近年来，一类高光谱异常检测方法在背景数据集上可以一次性训练，然后无需按场景重新训练或参数调优即可通用部署，表现出显著的效率和鲁棒性。在这一范式的基础上，我们专注于光谱与空域线索的融合，并提出了一种新颖的“Rebellious Student”框架，用于互补特征学习。与传统依赖模仿的师生范式不同，我们的方法有意训练空域分支与光谱教师模型产生分歧，从而学习教师模型未能捕捉的互补空域模式。我们采用了两阶段学习策略：(1) 首先通过反向蒸馏训练光谱增强网络，以获得稳健的背景光谱表示；(2) 随后使用去相关损失对空域网络（即叛逆学生）进行优化，该损失在保持重建保真度的同时强制特征正交，以避免无关噪声的干扰。一旦训练完成，该框架即可增强背景的光谱与空域特征，使得在与传统检测器配对使用时，能够实现无参数且无需训练的异常检测。在 HAD100 基准上的大量实验表明，与多个已有基线方法相比，该方法在计算开销最小的情况下实现了显著性能提升，验证了所提出的互补学习范式的有效性与通用性。我们的代码已公开在 https://github.com/xjpp2016/FERS。",
        "translated_title": "反叛学生：一种用于高光谱异常检测中背景特征增强的互补学习框架",
        "label": [
            "遥感图像复原",
            "图像增强"
        ],
        "label_reason": "论文涉及高光谱异常检测中的背景特征增强，属于遥感图像复原和增强范畴。",
        "relevance_score": 6,
        "novelty_score": 7,
        "novelty_reason": "提出新颖的互补学习框架，通过反向蒸馏和去相关损失实现特征增强。"
    },
    {
        "title": "UltraGen: High-Resolution Video Generation with Hierarchical Attention",
        "url": "http://arxiv.org/abs/2510.18775v1",
        "pub_date": "2025-10-21",
        "summary": "Recent advances in video generation have made it possible to produce visually compelling videos, with wide-ranging applications in content creation, entertainment, and virtual reality. However, most existing diffusion transformer based video generation models are limited to low-resolution outputs (&lt;=720P) due to the quadratic computational complexity of the attention mechanism with respect to the output width and height. This computational bottleneck makes native high-resolution video generation (1080P/2K/4K) impractical for both training and inference. To address this challenge, we present UltraGen, a novel video generation framework that enables i) efficient and ii) end-to-end native high-resolution video synthesis. Specifically, UltraGen features a hierarchical dual-branch attention architecture based on global-local attention decomposition, which decouples full attention into a local attention branch for high-fidelity regional content and a global attention branch for overall semantic consistency. We further propose a spatially compressed global modeling strategy to efficiently learn global dependencies, and a hierarchical cross-window local attention mechanism to reduce computational costs while enhancing information flow across different local windows. Extensive experiments demonstrate that UltraGen can effectively scale pre-trained low-resolution video models to 1080P and even 4K resolution for the first time, outperforming existing state-of-the-art methods and super-resolution based two-stage pipelines in both qualitative and quantitative evaluations.",
        "translated": "近期在视频生成领域的进展使得生成视觉上具有吸引力的视频成为可能，并在内容创作、娱乐和虚拟现实等领域具有广泛的应用前景。然而，由于注意力机制的计算复杂度与输出宽度和高度呈平方关系，大多数现有的基于扩散变换器的视频生成模型在输出分辨率上受到限制（<=720P）。这种计算瓶颈使得在训练和推理阶段实现原生的高分辨率视频生成（1080P/2K/4K）变得不切实际。为了解决这一挑战，我们提出了 UltraGen，一种新颖的视频生成框架，能够在 i) 高效性和 ii) 原生高分辨率视频的端到端合成方面取得突破。具体而言，UltraGen 采用基于全局-局部注意力分解的层次化双分支注意力架构，将完整的注意力机制拆分为用于高保真区域内容的局部注意力分支和用于整体语义一致性的全局注意力分支。我们进一步提出了一种空间压缩的全局建模策略，以高效地学习全局依赖关系，并引入了一种层次化的跨窗口局部注意力机制，以在降低计算成本的同时增强不同局部窗口之间的信息流动。大量实验表明，UltraGen 能够首次有效地将预训练的低分辨率视频模型扩展至 1080P，甚至 4K 分辨率，在定性和定量评估中均优于现有的最先进方法以及基于超分辨率的两阶段流水线方法。",
        "translated_title": "UltraGen：基于层次注意力的高分辨率视频生成",
        "label": [],
        "label_reason": "论文聚焦视频生成，非图像像素级恢复任务",
        "relevance_score": 3,
        "novelty_score": 8,
        "novelty_reason": "提出分层注意力机制，显著提升高分辨率视频生成效率"
    },
    {
        "title": "Detection and Simulation of Urban Heat Islands Using a Fine-Tuned\n  Geospatial Foundation Model for Microclimate Impact Prediction",
        "url": "http://arxiv.org/abs/2510.18773v1",
        "pub_date": "2025-10-21",
        "summary": "As urbanization and climate change progress, urban heat island effects are becoming more frequent and severe. To formulate effective mitigation plans, cities require detailed air temperature data, yet conventional machine learning models with limited data often produce inaccurate predictions, particularly in underserved areas. Geospatial foundation models trained on global unstructured data offer a promising alternative by demonstrating strong generalization and requiring only minimal fine-tuning. In this study, an empirical ground truth of urban heat patterns is established by quantifying cooling effects from green spaces and benchmarking them against model predictions to evaluate the model's accuracy. The foundation model is subsequently fine-tuned to predict land surface temperatures under future climate scenarios, and its practical value is demonstrated through a simulated inpainting that highlights its role for mitigation support. The results indicate that foundation models offer a powerful way for evaluating urban heat island mitigation strategies in data-scarce regions to support more climate-resilient cities.",
        "translated": "随着城市化和气候变化的加剧，城市热岛效应变得越来越频繁和严重。为制定有效的缓解计划，城市需要详细的空气温度数据，然而基于有限数据的传统机器学习模型常常产生不准确的预测，特别是在数据匮乏的地区。在全局非结构化数据上训练的地理空间基础模型提供了一个有前景的替代方案，因为它们表现出良好的泛化能力，且仅需少量微调即可使用。本研究中，通过量化绿地的降温效应，并将其与模型预测进行基准比较，建立了一个城市热岛模式的经验性真实数据，以评估模型的准确性。随后对基础模型进行微调，以预测未来气候情景下的地表温度，并通过模拟补全展示了其在缓解支持中的实际价值。结果表明，基础模型为在数据稀缺地区评估城市热岛缓解策略提供了一种有力的方法，有助于建设更具气候韧性的城市。",
        "translated_title": "基于微气候影响预测的微调地理空间基础模型用于城市热岛的检测与模拟",
        "label": [],
        "label_reason": "不属于低级图像处理，主要涉及地理空间建模和气候预测",
        "relevance_score": 3,
        "novelty_score": 6,
        "novelty_reason": "对基础模型进行微调用于模拟城市热岛效应，具有一定新颖性"
    },
    {
        "title": "Seg the HAB: Language-Guided Geospatial Algae Bloom Reasoning and\n  Segmentation",
        "url": "http://arxiv.org/abs/2510.18751v1",
        "pub_date": "2025-10-21",
        "summary": "Climate change is intensifying the occurrence of harmful algal bloom (HAB), particularly cyanobacteria, which threaten aquatic ecosystems and human health through oxygen depletion, toxin release, and disruption of marine biodiversity. Traditional monitoring approaches, such as manual water sampling, remain labor-intensive and limited in spatial and temporal coverage. Recent advances in vision-language models (VLMs) for remote sensing have shown potential for scalable AI-driven solutions, yet challenges remain in reasoning over imagery and quantifying bloom severity. In this work, we introduce ALGae Observation and Segmentation (ALGOS), a segmentation-and-reasoning system for HAB monitoring that combines remote sensing image understanding with severity estimation. Our approach integrates GeoSAM-assisted human evaluation for high-quality segmentation mask curation and fine-tunes vision language model on severity prediction using the Cyanobacteria Aggregated Manual Labels (CAML) from NASA. Experiments demonstrate that ALGOS achieves robust performance on both segmentation and severity-level estimation, paving the way toward practical and automated cyanobacterial monitoring systems.",
        "translated": "气候变化加剧了有害藻类水华（HAB）的出现，特别是蓝藻，它们通过耗氧、释放毒素和破坏海洋生物多样性而威胁水生生态系统和人类健康。传统的监测方法，如人工采水样，仍然劳动密集且在空间和时间覆盖范围上受到限制。最近在遥感领域中视觉-语言模型（VLM）的进展表明了可扩展的AI驱动解决方案的潜力，但如何对图像进行推理和量化水华严重程度仍是挑战。在此工作中，我们提出了ALGae Observation and Segmentation（ALGOS），一个用于HAB监测的分割与推理系统，该系统结合了遥感图像理解和严重程度估计。我们的方法整合了由GeoSAM辅助的人工评估，以生成高质量的分割掩码，并利用美国宇航局（NASA）提供的蓝藻聚集人工标签（Cyanobacteria Aggregated Manual Labels, CAML）对视觉语言模型进行严重程度预测的微调。实验表明，ALGOS在分割和严重程度估计方面均表现出稳健的性能，为实用且自动化的蓝藻监测系统铺平了道路。",
        "translated_title": "Seg the HAB：语言引导的地理空间藻华推理与分割",
        "label": [
            "遥感图像复原",
            "图像分割"
        ],
        "label_reason": "涉及遥感图像分割，但主要用于地理场景理解",
        "relevance_score": 5,
        "novelty_score": 7,
        "novelty_reason": "结合 GeoSAM 和语言模型，改进遥感 HAB 分析"
    },
    {
        "title": "SEAL: Semantic-Aware Hierarchical Learning for Generalized Category\n  Discovery",
        "url": "http://arxiv.org/abs/2510.18740v1",
        "pub_date": "2025-10-21",
        "summary": "This paper investigates the problem of Generalized Category Discovery (GCD). Given a partially labelled dataset, GCD aims to categorize all unlabelled images, regardless of whether they belong to known or unknown classes. Existing approaches typically depend on either single-level semantics or manually designed abstract hierarchies, which limit their generalizability and scalability. To address these limitations, we introduce a SEmantic-aware hierArchical Learning framework (SEAL), guided by naturally occurring and easily accessible hierarchical structures. Within SEAL, we propose a Hierarchical Semantic-Guided Soft Contrastive Learning approach that exploits hierarchical similarity to generate informative soft negatives, addressing the limitations of conventional contrastive losses that treat all negatives equally. Furthermore, a Cross-Granularity Consistency (CGC) module is designed to align the predictions from different levels of granularity. SEAL consistently achieves state-of-the-art performance on fine-grained benchmarks, including the SSB benchmark, Oxford-Pet, and the Herbarium19 dataset, and further demonstrates generalization on coarse-grained datasets. Project page: https://visual-ai.github.io/seal/",
        "translated": "本文研究了广义类别发现（Generalized Category Discovery, GCD）问题。在给定一个部分标注的数据集的情况下，GCD 的目标是将所有未标注的图像进行分类，无论它们属于已知类别还是未知类别。现有方法通常依赖于单一层级的语义或手动设计的抽象层次结构，这限制了它们的泛化能力和可扩展性。为了解决这些局限，我们引入了一种语义感知的层次化学习框架（SEmantic-aware hierArchical Learning framework, SEAL），该框架以自然存在且易于获取的层次化结构为指导。在 SEAL 中，我们提出了一种层次语义引导的软对比学习（Hierarchical Semantic-Guided Soft Contrastive Learning）方法，该方法利用层次相似性生成具有信息量的软负样本，从而解决了传统对比损失函数将所有负样本一视同仁的局限。此外，我们设计了一个跨粒度一致性（Cross-Granularity Consistency, CGC）模块，用于对齐不同粒度层级的预测结果。SEAL 在细粒度基准数据集上始终实现了最先进的性能，包括 SSB 基准、Oxford-Pet 以及 Herbarium19 数据集，并在粗粒度数据集上进一步展示了其泛化能力。项目主页：https://visual-ai.github.io/seal/",
        "translated_title": "SEAL：面向广义类别发现的语义感知层次化学习",
        "label": [],
        "label_reason": "论文聚焦分类任务，不涉及像素级图像恢复或增强",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出层次化语义学习框架，有一定方法改进但非本质创新"
    },
    {
        "title": "Moving Light Adaptive Colonoscopy Reconstruction via\n  Illumination-Attenuation-Aware 3D Gaussian Splatting",
        "url": "http://arxiv.org/abs/2510.18739v1",
        "pub_date": "2025-10-21",
        "summary": "3D Gaussian Splatting (3DGS) has emerged as a pivotal technique for real-time view synthesis in colonoscopy, enabling critical applications such as virtual colonoscopy and lesion tracking. However, the vanilla 3DGS assumes static illumination and that observed appearance depends solely on viewing angle, which causes incompatibility with the photometric variations in colonoscopic scenes induced by dynamic light source/camera. This mismatch forces most 3DGS methods to introduce structure-violating vaporous Gaussian blobs between the camera and tissues to compensate for illumination attenuation, ultimately degrading the quality of 3D reconstructions. Previous works only consider the illumination attenuation caused by light distance, ignoring the physical characters of light source and camera. In this paper, we propose ColIAGS, an improved 3DGS framework tailored for colonoscopy. To mimic realistic appearance under varying illumination, we introduce an Improved Appearance Modeling with two types of illumination attenuation factors, which enables Gaussians to adapt to photometric variations while preserving geometry accuracy. To ensure the geometry approximation condition of appearance modeling, we propose an Improved Geometry Modeling using high-dimensional view embedding to enhance Gaussian geometry attribute prediction. Furthermore, another cosine embedding input is leveraged to generate illumination attenuation solutions in an implicit manner. Comprehensive experimental results on standard benchmarks demonstrate that our proposed ColIAGS achieves the dual capabilities of novel view synthesis and accurate geometric reconstruction. It notably outperforms other state-of-the-art methods by achieving superior rendering fidelity while significantly reducing Depth MSE. Code will be available.",
        "translated": "3D Gaussian Splatting（3DGS）已成为结肠镜检查中实时视图合成的关键技术，使虚拟结肠镜和病灶跟踪等重要应用成为可能。然而，原始的3DGS假设照明是静态的，且所观察到的外观仅依赖于视角，这导致其与由动态光源/相机引起的结肠镜场景中的光度变化不兼容。这种不匹配迫使大多数3DGS方法在相机和组织之间引入破坏结构的雾状高斯斑点，以补偿照明衰减，最终降低了3D重建的质量。之前的工作只考虑了由光源距离引起的照明衰减，而忽略了光源和相机的物理特性。在本文中，我们提出ColIAGS，这是一种专为结肠镜检查改进的3DGS框架。为了在不同照明条件下模拟逼真的外观，我们引入了一种改进的外观建模方法，包含两种类型的照明衰减因子，这使得高斯模型能够适应光度变化，同时保持几何精度。为确保外观建模的几何逼近条件，我们提出了一种改进的几何建模方法，使用高维视角嵌入来增强高斯几何属性的预测能力。此外，还利用另一个余弦嵌入输入以隐式方式生成照明衰减解。在标准基准上的全面实验结果表明，我们提出的ColIAGS实现了新视角合成与精确几何重建的双重能力。它在显著降低深度均方误差的同时，实现了优于其他最先进方法的渲染保真度。代码将公开。",
        "translated_title": "通过光照衰减感知的3D高斯点绘实现动态光照自适应结肠镜重建",
        "label": [
            "医学图像增强",
            "图像恢复",
            "多帧/视频图像恢复"
        ],
        "label_reason": "论文针对内窥镜场景的光照变化进行3D重建优化，属于医学图像增强与图像恢复",
        "relevance_score": 8,
        "novelty_score": 7,
        "novelty_reason": "引入光照衰减因子与改进的几何建模，对3DGS方法有一定创新"
    },
    {
        "title": "IF-VidCap: Can Video Caption Models Follow Instructions?",
        "url": "http://arxiv.org/abs/2510.18726v1",
        "pub_date": "2025-10-21",
        "summary": "Although Multimodal Large Language Models (MLLMs) have demonstrated proficiency in video captioning, practical applications require captions that follow specific user instructions rather than generating exhaustive, unconstrained descriptions. Current benchmarks, however, primarily assess descriptive comprehensiveness while largely overlooking instruction-following capabilities. To address this gap, we introduce IF-VidCap, a new benchmark for evaluating controllable video captioning, which contains 1,400 high-quality samples. Distinct from existing video captioning or general instruction-following benchmarks, IF-VidCap incorporates a systematic framework that assesses captions on two dimensions: format correctness and content correctness. Our comprehensive evaluation of over 20 prominent models reveals a nuanced landscape: despite the continued dominance of proprietary models, the performance gap is closing, with top-tier open-source solutions now achieving near-parity. Furthermore, we find that models specialized for dense captioning underperform general-purpose MLLMs on complex instructions, indicating that future work should simultaneously advance both descriptive richness and instruction-following fidelity.",
        "translated": "尽管多模态大语言模型（MLLMs）在视频字幕生成方面表现出色，但实际应用中需要的是遵循特定用户指令的字幕，而不是生成详尽且无约束的描述。然而，目前的基准测试主要评估描述的全面性，而几乎忽视了对指令遵循能力的考量。为填补这一空白，我们引入了 IF-VidCap，一个新的用于评估可控视频字幕生成的基准，包含 1,400 个高质量样本。与现有的视频字幕生成或通用指令遵循基准不同，IF-VidCap 采用了一个系统化的框架，从两个维度对字幕进行评估：格式正确性和内容正确性。我们对 20 多个主流模型进行了全面评估，结果揭示了一个复杂而细致的图景：尽管专有模型依然占据主导地位，但性能差距正在缩小，顶级开源解决方案如今已接近其水平。此外，我们发现专门用于密集字幕的模型在处理复杂指令时表现不如通用型 MLLMs，这表明未来的工作应同时推进描述的丰富性和指令遵循的准确性。",
        "translated_title": "IF-VidCap: 视频标题模型能否遵循指令？",
        "label": [],
        "label_reason": "论文聚焦视频描述生成，不涉及图像像素级质量恢复或增强",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出了新的可控视频描述基准IF-VidCap，具有系统性评估框架"
    },
    {
        "title": "SSD: Spatial-Semantic Head Decoupling for Efficient Autoregressive Image\n  Generation",
        "url": "http://arxiv.org/abs/2510.18716v1",
        "pub_date": "2025-10-21",
        "summary": "Autoregressive image generation models like Janus-Pro produce high-quality images, but at the significant cost of high memory and ever-growing computational demands due to the large number of visual tokens. While KV cache compression has been extensively studied in language modeling, it still remains largely unexplored for the image generation domain. In this work, we begin by identifying a distinct and prominent attention phenomenon, which we term spatial locality and emergent semantic sink. To leverage this key insight, we introduce a novel KV cache compression framework. Specifically, we compress the KV cache for all visual tokens by adaptively decoupling attention heads into two separate types: for spatial-locality heads, our method maintains a short recent token window; for semantic-sink heads, it strategically preserves a compact set of highly-attended tokens. Our extensive experiments demonstrate that the proposed method achieves a 5$\\times$ reduction in memory usage and a notable 6.6$\\times$ speedup in overall throughput with only minimal visual quality loss, thereby enabling highly efficient native autoregressive image generation on resource-constrained hardware.",
        "translated": "Janus-Pro 等自回归图像生成模型能够生成高质量图像，但由于视觉 token 数量庞大，其代价是高昂的内存占用和不断增长的计算需求。尽管在语言建模领域对键值缓存（KV cache）压缩已进行了广泛研究，但在图像生成领域中，这一技术仍鲜有探索。在本研究中，我们首先识别出一种独特且显著的注意力现象，我们称之为“空间局部性”和“涌现语义沉降”。为利用这一关键发现，我们引入了一种新颖的 KV cache 压缩框架。具体而言，我们通过自适应地将注意力头解耦为两种类型来压缩所有视觉 token 的 KV cache：对于空间局部性头，我们的方法保留一个短的最近 token 窗口；对于语义沉降头，则策略性地保留一组被高度关注的 token。大量实验表明，所提出的方法在仅造成极小视觉质量损失的情况下，实现了 5$\\times$ 的内存使用减少和整体吞吐量 6.6$\\times$ 的加速，从而使得在资源受限的硬件上实现高效原生自回归图像生成成为可能。",
        "translated_title": "SSD：用于高效自回归图像生成的空域-语义头解耦",
        "label": [],
        "label_reason": "论文聚焦于图像生成效率，非图像恢复或增强任务",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出KV缓存压缩框架，提升生成效率具创新性"
    },
    {
        "title": "PLANA3R: Zero-shot Metric Planar 3D Reconstruction via Feed-Forward\n  Planar Splatting",
        "url": "http://arxiv.org/abs/2510.18714v1",
        "pub_date": "2025-10-21",
        "summary": "This paper addresses metric 3D reconstruction of indoor scenes by exploiting their inherent geometric regularities with compact representations. Using planar 3D primitives - a well-suited representation for man-made environments - we introduce PLANA3R, a pose-free framework for metric Planar 3D Reconstruction from unposed two-view images. Our approach employs Vision Transformers to extract a set of sparse planar primitives, estimate relative camera poses, and supervise geometry learning via planar splatting, where gradients are propagated through high-resolution rendered depth and normal maps of primitives. Unlike prior feedforward methods that require 3D plane annotations during training, PLANA3R learns planar 3D structures without explicit plane supervision, enabling scalable training on large-scale stereo datasets using only depth and normal annotations. We validate PLANA3R on multiple indoor-scene datasets with metric supervision and demonstrate strong generalization to out-of-domain indoor environments across diverse tasks under metric evaluation protocols, including 3D surface reconstruction, depth estimation, and relative pose estimation. Furthermore, by formulating with planar 3D representation, our method emerges with the ability for accurate plane segmentation. The project page is available at https://lck666666.github.io/plana3r",
        "translated": "本文通过利用室内场景固有的几何规律并采用紧凑表示方法，研究了度量意义上的三维重建问题。我们引入了 PLANA3R，这是一个用于从无姿态的双视角图像中进行度量平面三维重建的无姿态框架。该方法采用视觉变换器（Vision Transformers）提取一组稀疏的平面基元（planar primitives），估计相对相机姿态，并通过平面溅射（planar splatting）监督几何学习，其中梯度通过基元的高分辨率渲染深度图和法线图进行传播。与以往需要在训练过程中使用三维平面标注的前馈方法不同，PLANA3R 在没有显式平面监督的情况下学习三维平面结构，从而仅利用深度和法线标注即可在大规模立体数据集上进行可扩展的训练。我们在多个具有度量监督的室内场景数据集上验证了 PLANA3R，并在多种任务中展示了其对域外室内环境的强泛化能力，包括三维表面重建、深度估计和相对姿态估计。此外，通过采用平面三维表示方法，我们的方法还具备了准确的平面分割能力。项目页面见 https://lck666666.github.io/plana3r",
        "translated_title": "PLANA3R：通过前馈平面泼溅实现零样本度量平面三维重建",
        "label": [
            "3D重建"
        ],
        "label_reason": "方法涉及3D重建但非图像像素级恢复任务",
        "relevance_score": 5,
        "novelty_score": 7,
        "novelty_reason": "提出一种无需显式平面监督的3D平面重建方法"
    },
    {
        "title": "Is This Tracker On? A Benchmark Protocol for Dynamic Tracking",
        "url": "http://arxiv.org/abs/2510.19819v1",
        "pub_date": "2025-10-22",
        "summary": "We introduce ITTO, a challenging new benchmark suite for evaluating and diagnosing the capabilities and limitations of point tracking methods. Our videos are sourced from existing datasets and egocentric real-world recordings, with high-quality human annotations collected through a multi-stage pipeline. ITTO captures the motion complexity, occlusion patterns, and object diversity characteristic of real-world scenes -- factors that are largely absent in current benchmarks. We conduct a rigorous analysis of state-of-the-art tracking methods on ITTO, breaking down performance along key axes of motion complexity. Our findings reveal that existing trackers struggle with these challenges, particularly in re-identifying points after occlusion, highlighting critical failure modes. These results point to the need for new modeling approaches tailored to real-world dynamics. We envision ITTO as a foundation testbed for advancing point tracking and guiding the development of more robust tracking algorithms.",
        "translated": "我们引入了 ITTO，这是一个具有挑战性的新基准套件，用于评估和诊断点跟踪方法的能力与局限性。我们的视频来源于现有的数据集和以第一视角拍摄的真实世界记录，并通过多阶段流程收集高质量的人工标注。ITTO 捕捉了真实场景中运动的复杂性、遮挡模式和物体的多样性——这些因素在当前的基准测试中大多缺失。我们在 ITTO 上对最先进的跟踪方法进行了严格分析，并沿着运动复杂性的关键维度对性能进行了细分。我们的研究结果表明，现有跟踪器在应对这些挑战方面存在困难，尤其是在遮挡后重新识别点方面，突显了关键的失效模式。这些结果表明，有必要提出新的建模方法以适应真实世界的动态特性。我们期望 ITTO 成为推动点跟踪技术发展和指导更鲁棒跟踪算法设计的基础测试平台。",
        "translated_title": "这是追踪器开启的吗？一个动态追踪的基准协议",
        "label": [],
        "label_reason": "论文聚焦动态跟踪，不属于图像像素级恢复或增强任务",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出了新的基准协议，但跟踪方法改进较为常规"
    },
    {
        "title": "olmOCR 2: Unit Test Rewards for Document OCR",
        "url": "http://arxiv.org/abs/2510.19817v1",
        "pub_date": "2025-10-22",
        "summary": "We present olmOCR 2, the latest in our family of powerful OCR systems for converting digitized print documents, like PDFs, into clean, naturally ordered plain text. olmOCR 2 is powered by olmOCR-2-7B-1025, a specialized, 7B vision language model (VLM) trained using reinforcement learning with verifiable rewards (RLVR), where our rewards are a diverse set of binary unit tests. To scale unit test creation, we develop a pipeline for generating synthetic documents with diverse and challenging layouts, known ground-truth HTML source code, and extracted test cases. We show that RL training on these test cases results in state-of-the-art performance on olmOCR-Bench, our English-language OCR benchmark, with the largest improvements in math formula conversion, table parsing, and multi-column layouts compared to previous versions. We release our model, data and code under permissive open licenses.",
        "translated": "我们提出了 olmOCR 2，这是我们的强大光学字符识别（OCR）系统家族中用于将数字化的印刷文档（如 PDF）转换为干净、自然排序的纯文本的最新版本。olmOCR 2 由 olmOCR-2-7B-1025 驱动，这是一个专用的 7B 视觉语言模型（VLM），使用可验证奖励的强化学习（RLVR）进行训练，其中我们的奖励是一组多样的二值化单元测试。为了扩展单元测试的创建，我们开发了一个生成合成文档的流水线，这些文档具有多样且具有挑战性的排版、已知的 HTML 源代码以及提取的测试用例。我们展示了在这些测试用例上进行强化学习训练，使得 olmOCR-Bench（我们发布的英文 OCR 基准测试）上达到最先进的性能，相较于前一版本在数学公式转换、表格解析和多列排版方面取得了最大的改进。我们已将模型、数据和代码在宽松的开源许可下发布。",
        "translated_title": "olmOCR 2: 文档 OCR 的单元测试奖励",
        "label": [],
        "label_reason": "论文聚焦于文档OCR，不涉及像素级图像质量恢复",
        "relevance_score": 2,
        "novelty_score": 5,
        "novelty_reason": "提出合成文档与单元测试奖励的新训练方法"
    },
    {
        "title": "How to Evaluate Monocular Depth Estimation?",
        "url": "http://arxiv.org/abs/2510.19814v1",
        "pub_date": "2025-10-22",
        "summary": "Monocular depth estimation is an important task with rapid progress, but how to evaluate it remains an open question, as evidenced by a lack of standardization in existing literature and a large selection of evaluation metrics whose trade-offs and behaviors are not well understood. This paper contributes a novel, quantitative analysis of existing metrics in terms of their sensitivity to various types of perturbations of ground truth, emphasizing comparison to human judgment. Our analysis reveals that existing metrics are severely under-sensitive to curvature perturbation such as making flat surfaces wavy. To remedy this, we introduce a new metric based on relative surface normals, along with new depth visualization tools and a principled method to create composite metrics with better human alignment. Code and data are available at: https://github.com/princeton-vl/evalmde.",
        "translated": "单目深度估计是一个进展迅速的重要任务，但如何对其进行评估仍是一个开放性问题，这体现在现有文献中缺乏标准化以及评估指标的种类繁多，而它们之间的权衡和行为尚未被充分理解。本文对现有指标进行了新颖而定量的分析，从它们对真实值各种类型扰动的敏感性角度出发，强调与人类判断的对比。我们的分析表明，现有指标对曲率扰动（如使平面表面变得波浪状）的敏感性严重不足。为了解决这一问题，我们引入了一种基于相对表面法线的新指标，以及新的深度可视化工具和一种原理明确的方法，用于构建与人类判断更一致的复合指标。代码和数据可在以下地址获取：https://github.com/princeton-vl/evalmde。",
        "translated_title": "如何评估单目深度估计？",
        "label": [],
        "label_reason": "论文聚焦深度估计评估，属于high-level任务，不涉及像素级图像恢复或增强。",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出了新评估指标和工具，但其创新点主要在评估方法而非图像处理技术本身。"
    },
    {
        "title": "Pico-Banana-400K: A Large-Scale Dataset for Text-Guided Image Editing",
        "url": "http://arxiv.org/abs/2510.19808v1",
        "pub_date": "2025-10-22",
        "summary": "Recent advances in multimodal models have demonstrated remarkable text-guided image editing capabilities, with systems like GPT-4o and Nano-Banana setting new benchmarks. However, the research community's progress remains constrained by the absence of large-scale, high-quality, and openly accessible datasets built from real images. We introduce Pico-Banana-400K, a comprehensive 400K-image dataset for instruction-based image editing. Our dataset is constructed by leveraging Nano-Banana to generate diverse edit pairs from real photographs in the OpenImages collection. What distinguishes Pico-Banana-400K from previous synthetic datasets is our systematic approach to quality and diversity. We employ a fine-grained image editing taxonomy to ensure comprehensive coverage of edit types while maintaining precise content preservation and instruction faithfulness through MLLM-based quality scoring and careful curation. Beyond single turn editing, Pico-Banana-400K enables research into complex editing scenarios. The dataset includes three specialized subsets: (1) a 72K-example multi-turn collection for studying sequential editing, reasoning, and planning across consecutive modifications; (2) a 56K-example preference subset for alignment research and reward model training; and (3) paired long-short editing instructions for developing instruction rewriting and summarization capabilities. By providing this large-scale, high-quality, and task-rich resource, Pico-Banana-400K establishes a robust foundation for training and benchmarking the next generation of text-guided image editing models.",
        "translated": "近年来，多模态模型在文本引导的图像编辑方面取得了显著进展，GPT-4o 和 Nano-Banana 等系统已树立了新的基准。然而，研究社区的进展仍受到缺乏大规模、高质量且公开可用的真实图像数据集的限制。我们提出了 Pico-Banana-400K，一个全面的包含 40 万张图像的指令驱动图像编辑数据集。我们的数据集通过利用 Nano-Banana 从 OpenImages 集合中的真实照片生成多样化的编辑对来构建。Pico-Banana-400K 与以往合成数据集的不同之处在于我们对质量和多样性的系统化处理。我们采用细粒度的图像编辑分类法，确保编辑类型覆盖全面，同时通过基于 MLLM 的质量评分和细致的编辑过程，保持内容的精确保存和指令的忠实度。除了单轮编辑之外，Pico-Banana-400K 还支持对复杂编辑场景的研究。该数据集包含三个专门的子集：(1) 一个包含 72,000 个示例的多轮编辑集合，用于研究连续修改中的顺序编辑、推理和规划；(2) 一个包含 56,000 个示例的偏好子集，用于对齐研究和奖励模型训练；以及 (3) 配对的长-短编辑指令，用于开发指令重写和摘要能力。通过提供这一大规模、高质量且任务丰富的新资源，Pico-Banana-400K 为下一代文本引导图像编辑模型的训练和评估奠定了坚实的基础。",
        "translated_title": "Pico-Banana-400K：一个用于文本引导图像编辑的大规模数据集",
        "label": [],
        "label_reason": "论文关注文本引导的图像编辑，不属于像素级图像恢复或增强任务",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出了大规模真实图像编辑数据集，但方法为常规数据构建"
    },
    {
        "title": "Class-Aware Prototype Learning with Negative Contrast for Test-Time\n  Adaptation of Vision-Language Models",
        "url": "http://arxiv.org/abs/2510.19802v1",
        "pub_date": "2025-10-22",
        "summary": "Vision-Language Models (VLMs) demonstrate impressive zero-shot generalization through large-scale image-text pretraining, yet their performance can drop once the deployment distribution diverges from the training distribution. To address this, Test-Time Adaptation (TTA) methods update models using unlabeled target data. However, existing approaches often ignore two key challenges: prototype degradation in long-tailed distributions and confusion between semantically similar classes. To tackle these issues, we propose \\textbf{C}lass-Aware \\textbf{P}rototype \\textbf{L}earning with \\textbf{N}egative \\textbf{C}ontrast(\\textbf{CPL-NC}), a lightweight TTA framework designed specifically for VLMs to enhance generalization under distribution shifts. CPL-NC introduces a \\textit{Class-Aware Prototype Cache} Module that dynamically adjusts per-class capacity based on test-time frequency and activation history, with a rejuvenation mechanism for inactive classes to retain rare-category knowledge. Additionally, a \\textit{Negative Contrastive Learning} Mechanism identifies and constrains hard visual-textual negatives to improve class separability. The framework employs asymmetric optimization, refining only textual prototypes while anchoring on stable visual features. Experiments on 15 benchmarks show that CPL-NC consistently outperforms prior TTA methods across both ResNet-50 and ViT-B/16 backbones.",
        "translated": "视觉-语言模型（VLMs）通过大规模图像-文本预训练展示了令人印象深刻的零样本泛化能力，但一旦部署分布与训练分布发生偏离，其性能可能会下降。为了解决这一问题，测试时自适应（TTA）方法利用未标注的目标数据更新模型。然而，现有方法通常忽略了两个关键挑战：长尾分布中的原型退化以及语义相似类别的混淆。为应对这些问题，我们提出了一种面向视觉-语言模型的轻量级TTA框架，称为\\textbf{C}lass-Aware \\textbf{P}rototype \\textbf{L}earning with \\textbf{N}egative \\textbf{C}ontrast（\\textbf{CPL-NC}），旨在增强模型在分布偏移下的泛化能力。CPL-NC引入了一个\\textit{类感知原型缓存}模块，该模块根据测试时的频率和激活历史动态调整每个类别的容量，并通过一种针对不活跃类别的恢复机制，保留罕见类别的知识。此外，\\textit{负对比学习}机制用于识别并约束困难的视觉-文本负样本，以提升类间可分性。该框架采用了非对称优化策略，仅优化文本原型，同时以稳定的视觉特征为锚点。在15个基准数据集上的实验表明，CPL-NC在ResNet-50和ViT-B/16两种主干结构上均一致优于以往的TTA方法。",
        "translated_title": "面向视觉-语言模型测试时自适应的类感知原型学习与负向对比",
        "label": [],
        "label_reason": "论文聚焦视觉语言模型测试时适应，属于high-level任务",
        "relevance_score": 1,
        "novelty_score": 6,
        "novelty_reason": "提出了类感知原型缓存和负对比学习机制，有一定创新性"
    },
    {
        "title": "OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation",
        "url": "http://arxiv.org/abs/2510.19789v1",
        "pub_date": "2025-10-22",
        "summary": "This paper introduces OmniMotion-X, a versatile multimodal framework for whole-body human motion generation, leveraging an autoregressive diffusion transformer in a unified sequence-to-sequence manner. OmniMotion-X efficiently supports diverse multimodal tasks, including text-to-motion, music-to-dance, speech-to-gesture, and global spatial-temporal control scenarios (e.g., motion prediction, in-betweening, completion, and joint/trajectory-guided synthesis), as well as flexible combinations of these tasks. Specifically, we propose the use of reference motion as a novel conditioning signal, substantially enhancing the consistency of generated content, style, and temporal dynamics crucial for realistic animations. To handle multimodal conflicts, we introduce a progressive weak-to-strong mixed-condition training strategy. To enable high-quality multimodal training, we construct OmniMoCap-X, the largest unified multimodal motion dataset to date, integrating 28 publicly available MoCap sources across 10 distinct tasks, standardized to the SMPL-X format at 30 fps. To ensure detailed and consistent annotations, we render sequences into videos and use GPT-4o to automatically generate structured and hierarchical captions, capturing both low-level actions and high-level semantics. Extensive experimental evaluations confirm that OmniMotion-X significantly surpasses existing methods, demonstrating state-of-the-art performance across multiple multimodal tasks and enabling the interactive generation of realistic, coherent, and controllable long-duration motions.",
        "translated": "本文介绍了 OmniMotion-X，这是一个用于全身人体运动生成的多功能多模态框架，通过统一的序列到序列方式利用自回归扩散变压器。OmniMotion-X 高效支持多种多模态任务，包括文本到运动、音乐到舞蹈、语音到手势，以及全局时空控制场景（例如运动预测、中间插值、补全和关节/轨迹引导的合成），并支持这些任务的灵活组合。具体而言，我们提出使用参考运动作为一种新的条件信号，显著增强了生成内容的一致性、风格及时序动态，这对于真实动画至关重要。为处理多模态冲突，我们引入了一种渐进式的弱到强混合条件训练策略。为实现高质量的多模态训练，我们构建了 OmniMoCap-X，目前最大的统一多模态运动数据集，整合了涵盖 10 个不同任务的 28 个公开可用的 MoCap 数据源，并标准化为 30 fps 的 SMPL-X 格式。为确保详细且一致的标注，我们将序列渲染为视频，并使用 GPT-4o 自动生成结构化和分层的字幕，捕捉低级动作和高级语义。广泛的实验评估验证了 OmniMotion-X 显著优于现有方法，在多个多模态任务上表现出最先进的性能，并能够实现真实、连贯且可控的长时序运动的交互式生成。",
        "translated_title": "OmniMotion-X：通用多模态全身运动生成",
        "label": [],
        "label_reason": "论文聚焦于人体运动生成，属于high-level任务，不涉及图像像素级处理。",
        "relevance_score": 1,
        "novelty_score": 4,
        "novelty_reason": "提出参考运动作为条件信号和多模态训练策略，有一定创新但非本质突破。"
    },
    {
        "title": "Adaptive Distribution-aware Quantization for Mixed-Precision Neural\n  Networks",
        "url": "http://arxiv.org/abs/2510.19760v1",
        "pub_date": "2025-10-22",
        "summary": "Quantization-Aware Training (QAT) is a critical technique for deploying deep neural networks on resource-constrained devices. However, existing methods often face two major challenges: the highly non-uniform distribution of activations and the static, mismatched codebooks used in weight quantization. To address these challenges, we propose Adaptive Distribution-aware Quantization (ADQ), a mixed-precision quantization framework that employs a differentiated strategy. The core of ADQ is a novel adaptive weight quantization scheme comprising three key innovations: (1) a quantile-based initialization method that constructs a codebook closely aligned with the initial weight distribution; (2) an online codebook adaptation mechanism based on Exponential Moving Average (EMA) to dynamically track distributional shifts; and (3) a sensitivity-informed strategy for mixed-precision allocation. For activations, we integrate a hardware-friendly non-uniform-to-uniform mapping scheme. Comprehensive experiments validate the effectiveness of our method. On ImageNet, ADQ enables a ResNet-18 to achieve 71.512% Top-1 accuracy with an average bit-width of only 2.81 bits, outperforming state-of-the-art methods under comparable conditions. Furthermore, detailed ablation studies on CIFAR-10 systematically demonstrate the individual contributions of each innovative component, validating the rationale and effectiveness of our design.",
        "translated": "量化感知训练（QAT）是将深度神经网络部署到资源受限设备上的关键技术。然而，现有方法通常面临两个主要挑战：激活值的分布高度非均匀，以及在权重量化中使用的静态且不匹配的码本。为了解决这些挑战，我们提出了自适应分布感知量化（ADQ），这是一个采用差异化策略的混合精度量化框架。ADQ 的核心是一个新颖的自适应权重量化方案，包含三个关键创新：(1) 一种基于分位数的初始化方法，构建与初始权重分布高度一致的码本；(2) 一种基于指数移动平均（EMA）的在线码本适应机制，以动态跟踪分布的变化；以及 (3) 一种基于敏感度的混合精度分配策略。对于激活值，我们集成了一种硬件友好的非均匀到均匀映射方案。大量实验验证了我们方法的有效性。在 ImageNet 数据集上，ADQ 使得 ResNet-18 在平均位宽仅为 2.81 位的情况下实现了 71.512% 的 Top-1 准确率，优于在类似条件下最先进的方法。此外，我们在 CIFAR-10 上进行了详尽的消融研究，系统地展示了每个创新组件的独立贡献，验证了我们设计的合理性和有效性。",
        "translated_title": "自适应分布感知的混合精度神经网络量化",
        "label": [],
        "label_reason": "论文关注神经网络量化，不涉及像素级图像处理",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出了自适应量化方案，改进了激活分布和码本设计"
    },
    {
        "title": "A Survey on Cache Methods in Diffusion Models: Toward Efficient\n  Multi-Modal Generation",
        "url": "http://arxiv.org/abs/2510.19755v1",
        "pub_date": "2025-10-22",
        "summary": "Diffusion Models have become a cornerstone of modern generative AI for their exceptional generation quality and controllability. However, their inherent \\textit{multi-step iterations} and \\textit{complex backbone networks} lead to prohibitive computational overhead and generation latency, forming a major bottleneck for real-time applications. Although existing acceleration techniques have made progress, they still face challenges such as limited applicability, high training costs, or quality degradation.   Against this backdrop, \\textbf{Diffusion Caching} offers a promising training-free, architecture-agnostic, and efficient inference paradigm. Its core mechanism identifies and reuses intrinsic computational redundancies in the diffusion process. By enabling feature-level cross-step reuse and inter-layer scheduling, it reduces computation without modifying model parameters. This paper systematically reviews the theoretical foundations and evolution of Diffusion Caching and proposes a unified framework for its classification and analysis.   Through comparative analysis of representative methods, we show that Diffusion Caching evolves from \\textit{static reuse} to \\textit{dynamic prediction}. This trend enhances caching flexibility across diverse tasks and enables integration with other acceleration techniques such as sampling optimization and model distillation, paving the way for a unified, efficient inference framework for future multimodal and interactive applications. We argue that this paradigm will become a key enabler of real-time and efficient generative AI, injecting new vitality into both theory and practice of \\textit{Efficient Generative Intelligence}.",
        "translated": "扩散模型由于其卓越的生成质量和可控性，已成为现代生成式人工智能的核心。然而，其固有的**多步迭代**和**复杂的主干网络**导致计算开销巨大和生成延迟高，形成实时应用中的主要瓶颈。尽管现有的加速技术取得了一定进展，但它们仍然面临适用性有限、训练成本高或生成质量下降等挑战。在这一背景下，**Diffusion Caching** 提供了一种有前景的、无需训练、与架构无关且高效的推理范式。其核心机制在于识别并复用扩散过程中的内在计算冗余。通过实现特征级的跨步复用和层间调度，它在不修改模型参数的前提下减少了计算量。本文系统地回顾了 Diffusion Caching 的理论基础及其发展历程，并提出了一个统一的分类与分析框架。通过对比分析代表性方法，我们展示了 Diffusion Caching 从**静态复用**向**动态预测**的演变趋势。这一趋势增强了在不同任务中的缓存灵活性，并使其能够与其他加速技术（如采样优化和模型蒸馏）相结合，为未来多模态和交互式应用提供统一高效的推理框架。我们认为，这一范式将成为实时高效生成式人工智能的关键推动因素，为**高效生成智能**的理论与实践注入新的活力。",
        "translated_title": "扩散模型中的缓存方法综述：迈向高效的多模态生成",
        "label": [],
        "label_reason": "论文聚焦生成模型加速，不属于图像像素级恢复或增强任务。",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出了扩散模型中的缓存方法新范式，提升了推理效率。"
    },
    {
        "title": "Memo: Training Memory-Efficient Embodied Agents with Reinforcement\n  Learning",
        "url": "http://arxiv.org/abs/2510.19732v1",
        "pub_date": "2025-10-22",
        "summary": "To enable embodied agents to operate effectively over extended timeframes, it is crucial to develop models that form and access memories to stay contextualized in their environment. In the current paradigm of training transformer-based policies for embodied sequential decision-making tasks, visual inputs often overwhelm the context limits of transformers, while humans can maintain and utilize a lifetime of experience compressed as memories. Significant compression is possible in principle, as much of the input is irrelevant and can be abstracted. However, existing approaches predominantly focus on either recurrent models with fixed-size memory or transformers with full-context reliance. In this work, we propose Memo, a transformer-based architecture and training recipe for reinforcement learning (RL) on memory-intensive, long-horizon tasks. Memo incorporates the creation and retrieval of memory by interleaving periodic summarization tokens with the inputs of a model during training. We demonstrate Memo's effectiveness on a gridworld meta-RL benchmark and a multi-object navigation task in photo-realistic indoor settings. Memo outperforms naive long-context transformer baselines while being more compute and storage efficient. Additionally, Memo generalizes better to longer contexts at inference time and remains robust in streaming settings, where historical context must be truncated to fit inference constraints.",
        "translated": "为了使具身智能体在长时间范围内有效运行，开发能够形成和访问记忆以保持环境上下文感的模型至关重要。在当前基于Transformer的具身序列决策任务训练范式中，视觉输入往往会超出Transformer的上下文限制，而人类则能够维持并利用压缩为记忆的终身经验。从原理上讲，实现显著的压缩是可能的，因为大部分输入信息是无关的，可以进行抽象处理。然而，现有方法主要集中在两种方向：一是使用固定大小记忆的循环模型，二是完全依赖上下文的Transformer。在本文中，我们提出Memo，这是一种基于Transformer的架构和训练方法，专为内存密集型、长视野任务的强化学习（RL）而设计。Memo通过在训练过程中将周期性摘要标记与模型输入交错，实现了记忆的生成与检索。我们在一个网格世界元强化学习基准和一个在照片级真实感室内环境中进行的多物体导航任务上验证了Memo的有效性。Memo在计算和存储效率上优于简单的长上下文Transformer基线。此外，Memo在推理时对更长的上下文具有更好的泛化能力，并且在流式处理场景中仍能保持鲁棒性，即使必须截断历史上下文以满足推理限制。",
        "translated_title": "Memo: 基于强化学习训练内存高效的具身智能体",
        "label": [],
        "label_reason": "论文聚焦于强化学习代理的记忆训练，不涉及图像像素级处理",
        "relevance_score": 1,
        "novelty_score": 6,
        "novelty_reason": "提出一种高效的记忆压缩方法，但属于常规方法改进"
    },
    {
        "title": "LyTimeT: Towards Robust and Interpretable State-Variable Discovery",
        "url": "http://arxiv.org/abs/2510.19716v1",
        "pub_date": "2025-10-22",
        "summary": "Extracting the true dynamical variables of a system from high-dimensional video is challenging due to distracting visual factors such as background motion, occlusions, and texture changes. We propose LyTimeT, a two-phase framework for interpretable variable extraction that learns robust and stable latent representations of dynamical systems. In Phase 1, LyTimeT employs a spatio-temporal TimeSformer-based autoencoder that uses global attention to focus on dynamically relevant regions while suppressing nuisance variation, enabling distraction-robust latent state learning and accurate long-horizon video prediction. In Phase 2, we probe the learned latent space, select the most physically meaningful dimensions using linear correlation analysis, and refine the transition dynamics with a Lyapunov-based stability regularizer to enforce contraction and reduce error accumulation during roll-outs. Experiments on five synthetic benchmarks and four real-world dynamical systems, including chaotic phenomena, show that LyTimeT achieves mutual information and intrinsic dimension estimates closest to ground truth, remains invariant under background perturbations, and delivers the lowest analytical mean squared error among CNN-based (TIDE) and transformer-only baselines. Our results demonstrate that combining spatio-temporal attention with stability constraints yields predictive models that are not only accurate but also physically interpretable.",
        "translated": "从高维视频中提取系统的真实动力学变量具有挑战性，因为存在诸如背景运动、遮挡和纹理变化等干扰性视觉因素。我们提出 LyTimeT，一种用于可解释变量提取的两阶段框架，其目标是学习鲁棒且稳定的动力系统潜在表示。在第一阶段中，LyTimeT 采用基于时空 TimeSformer 的自编码器，利用全局注意力机制聚焦于动力学相关的区域，同时抑制无关变化，从而实现对干扰具有鲁棒性的潜在状态学习，并能进行准确的长跨度视频预测。在第二阶段中，我们对所学习的潜在空间进行探测，使用线性相关性分析选择最具物理意义的维度，并通过基于 Lyapunov 的稳定性正则化项对状态转移动力学进行优化，以增强收缩性并减少预测过程中的误差累积。在五个合成基准和四个真实世界动力系统（包括混沌现象）上的实验表明，LyTimeT 在互信息和固有维度估计方面最接近真实值，在背景扰动下保持不变性，并且在基于 CNN（TIDE）和纯 Transformer 的基线方法中具有最低的解析均方误差。我们的结果表明，将时空注意力机制与稳定性约束相结合，能够生成不仅准确而且具有物理可解释性的预测模型。",
        "translated_title": "LyTimeT：迈向稳健且可解释的状态变量发现",
        "label": [],
        "label_reason": "论文聚焦于动态变量提取，非图像像素级恢复任务。",
        "relevance_score": 2,
        "novelty_score": 5,
        "novelty_reason": "提出两阶段框架，结合时空注意力与稳定性约束，有一定创新但非突破性。"
    },
    {
        "title": "Explainable Face Presentation Attack Detection via Ensemble-CAM",
        "url": "http://arxiv.org/abs/2510.19695v1",
        "pub_date": "2025-10-22",
        "summary": "Presentation attacks represent a critical security threat where adversaries use fake biometric data, such as face, fingerprint, or iris images, to gain unauthorized access to protected systems. Various presentation attack detection (PAD) systems have been designed leveraging deep learning (DL) models to mitigate this type of threat. Despite their effectiveness, most of the DL models function as black boxes - their decisions are opaque to their users. The purpose of explainability techniques is to provide detailed information about the reason behind the behavior or decision of DL models. In particular, visual explanation is necessary to better understand the decisions or predictions of DL-based PAD systems and determine the key regions due to which a biometric image is considered real or fake by the system. In this work, a novel technique, Ensemble-CAM, is proposed for providing visual explanations for the decisions made by deep learning-based face PAD systems. Our goal is to improve DL-based face PAD systems by providing a better understanding of their behavior. Our provided visual explanations will enhance the transparency and trustworthiness of DL-based face PAD systems.",
        "translated": "呈现攻击是指对手使用伪造的生物特征数据（如人脸、指纹或虹膜图像）以获取对受保护系统的未经授权访问，这代表了关键的安全威胁。为了缓解此类威胁，已有多种呈现攻击检测（PAD）系统被设计出来，利用深度学习（DL）模型实现检测。尽管这些方法在实际中有效，但大多数深度学习模型都表现为黑盒——它们的决策对于用户而言是不透明的。可解释性技术的目的在于提供关于深度学习模型行为或决策背后原因的详细信息。特别是，可视化解释对于更好地理解基于深度学习的PAD系统的决策或预测，以及确定系统将生物特征图像识别为真实或伪造的关键区域，是必不可少的。在本工作中，我们提出了一种新颖的技术——Ensemble-CAM，用于为基于深度学习的人脸PAD系统的决策提供可视化解释。我们的目标是通过加深对其行为的理解，从而提升基于深度学习的人脸PAD系统的性能。所提供的可视化解释将增强基于深度学习的人脸PAD系统的透明度和可信度。",
        "translated_title": "通过集成-CAM实现可解释的面部呈现攻击检测",
        "label": [],
        "label_reason": "论文关注的是人脸识别的安全问题，不属于图像像素级质量恢复或增强任务。",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出了一种新的可视化解释方法，但属于常规的模型解释技术改进。"
    },
    {
        "title": "Curvilinear Structure-preserving Unpaired Cross-domain Medical Image\n  Translation",
        "url": "http://arxiv.org/abs/2510.19679v1",
        "pub_date": "2025-10-22",
        "summary": "Unpaired image-to-image translation has emerged as a crucial technique in medical imaging, enabling cross-modality synthesis, domain adaptation, and data augmentation without costly paired datasets. Yet, existing approaches often distort fine curvilinear structures, such as microvasculature, undermining both diagnostic reliability and quantitative analysis. This limitation is consequential in ophthalmic and vascular imaging, where subtle morphological changes carry significant clinical meaning. We propose Curvilinear Structure-preserving Translation (CST), a general framework that explicitly preserves fine curvilinear structures during unpaired translation by integrating structure consistency into the training. Specifically, CST augments baseline models with a curvilinear extraction module for topological supervision. It can be seamlessly incorporated into existing methods. We integrate it into CycleGAN and UNSB as two representative backbones. Comprehensive evaluation across three imaging modalities: optical coherence tomography angiography, color fundus and X-ray coronary angiography demonstrates that CST improves translation fidelity and achieves state-of-the-art performance. By reinforcing geometric integrity in learned mappings, CST establishes a principled pathway toward curvilinear structure-aware cross-domain translation in medical imaging.",
        "translated": "无配对图像到图像的翻译已成为医学影像中的关键技术，能够在无需昂贵配对数据集的情况下实现跨模态合成、域适应和数据增强。然而，现有方法通常会扭曲精细的曲线结构，例如微血管，从而影响诊断的可靠性和定量分析。这一限制在眼科和血管成像中尤为关键，因为细微的形态变化在临床上具有重要意义。我们提出了一种名为 Curvilinear Structure-preserving Translation (CST) 的通用框架，通过在训练过程中整合结构一致性，明确地在无配对翻译过程中保留精细的曲线结构。具体而言，CST 在基础模型中增加了曲线提取模块，以提供拓扑监督。它可以无缝集成到现有方法中。我们将其集成到 CycleGAN 和 UNSB 两种代表性骨干网络中。在三种成像模态——光学相干断层扫描血管造影、彩色眼底图像和X射线冠状动脉造影上的全面评估表明，CST 提高了翻译保真度并取得了最先进的性能。通过在学习映射中强化几何完整性，CST 为医学影像中曲线结构感知的跨域翻译建立了一条原理性的路径。",
        "translated_title": "保持曲线结构的非配对跨域医学图像翻译",
        "label": [
            "医学图像增强",
            "图像恢复"
        ],
        "label_reason": "论文聚焦于医学图像翻译中结构保护，属于图像恢复范畴。",
        "relevance_score": 8,
        "novelty_score": 7,
        "novelty_reason": "提出结构一致性模块，改进现有方法的翻译保真度。"
    },
    {
        "title": "I Spy With My Model's Eye: Visual Search as a Behavioural Test for MLLMs",
        "url": "http://arxiv.org/abs/2510.19678v1",
        "pub_date": "2025-10-22",
        "summary": "Multimodal large language models (MLLMs) achieve strong performance on vision-language tasks, yet their visual processing is opaque. Most black-box evaluations measure task accuracy, but reveal little about underlying mechanisms. Drawing on cognitive psychology, we adapt classic visual search paradigms -- originally developed to study human perception -- to test whether MLLMs exhibit the ``pop-out'' effect, where salient visual features are detected independently of distractor set size. Using controlled experiments targeting colour, size and lighting features, we find that advanced MLLMs exhibit human-like pop-out effects in colour or size-based disjunctive (single feature) search, as well as capacity limits for conjunctive (multiple feature) search. We also find evidence to suggest that MLLMs, like humans, incorporate natural scene priors such as lighting direction into object representations. We reinforce our findings using targeted fine-tuning and mechanistic interpretability analyses. Our work shows how visual search can serve as a cognitively grounded diagnostic tool for evaluating perceptual capabilities in MLLMs.",
        "translated": "多模态大语言模型（MLLMs）在视觉-语言任务中表现出色，但其视觉处理过程具有黑箱性质。大多数黑盒评估方法测量任务的准确率，却难以揭示其内部机制。借鉴认知心理学，我们采用经典的视觉搜索范式——最初用于研究人类感知——来测试 MLLMs 是否表现出“pop-out”效应，即显著的视觉特征能够不受干扰项数量的影响而被检测到。通过针对颜色、大小和光照特征设计的受控实验，我们发现先进的 MLLMs 在基于颜色或大小的析取（单一特征）搜索中表现出类似人类的 pop-out 效应，同时在合取（多特征）搜索中也表现出容量限制。我们还发现了一些证据，表明 MLLMs 与人类一样，会将自然场景的先验知识（如光照方向）融入对象表示中。我们通过有针对性的微调和机制解释性分析进一步验证了这些发现。我们的工作表明，视觉搜索可以作为基于认知的诊断工具，用于评估 MLLMs 的感知能力。",
        "translated_title": "我以我的模型之眼观察：视觉搜索作为多模态大语言模型的行为测试",
        "label": [],
        "label_reason": "论文聚焦多模态语言模型的视觉认知测试，非图像像素级恢复或增强",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "将认知心理学中的视觉搜索范式应用于 MLLMs 评估，具有一定创新性"
    },
    {
        "title": "From Forecasting to Planning: Policy World Model for Collaborative\n  State-Action Prediction",
        "url": "http://arxiv.org/abs/2510.19654v1",
        "pub_date": "2025-10-22",
        "summary": "Despite remarkable progress in driving world models, their potential for autonomous systems remains largely untapped: the world models are mostly learned for world simulation and decoupled from trajectory planning. While recent efforts aim to unify world modeling and planning in a single framework, the synergistic facilitation mechanism of world modeling for planning still requires further exploration. In this work, we introduce a new driving paradigm named Policy World Model (PWM), which not only integrates world modeling and trajectory planning within a unified architecture, but is also able to benefit planning using the learned world knowledge through the proposed action-free future state forecasting scheme. Through collaborative state-action prediction, PWM can mimic the human-like anticipatory perception, yielding more reliable planning performance. To facilitate the efficiency of video forecasting, we further introduce a dynamically enhanced parallel token generation mechanism, equipped with a context-guided tokenizer and an adaptive dynamic focal loss. Despite utilizing only front camera input, our method matches or exceeds state-of-the-art approaches that rely on multi-view and multi-modal inputs. Code and model weights will be released at https://github.com/6550Zhao/Policy-World-Model.",
        "translated": "尽管世界模型在驾驶领域取得了显著进展，但它们在自动驾驶系统中的潜力仍远未被充分挖掘：这些世界模型大多用于世界仿真，并与轨迹规划解耦。虽然最近的研究努力旨在在一个统一的框架中融合世界建模和规划，但世界建模对规划的协同促进机制仍需进一步探索。在本研究中，我们提出了一种新的驾驶范式，称为策略世界模型（Policy World Model，PWM），该模型不仅在一个统一的架构中融合了世界建模与轨迹规划，而且通过提出一种无需动作的未来状态预测方案，能够利用所学习到的世界知识来促进规划。通过协作式的状态-动作预测，PWM可以模拟人类类似的前瞻性感知，从而实现更可靠的规划性能。为了提高视频预测的效率，我们进一步引入了一种动态增强的并行token生成机制，该机制配备了上下文引导的tokenizer和自适应动态焦点损失。尽管仅使用前视摄像头输入，我们的方法在性能上可与甚至超越那些依赖多视角和多模态输入的最先进方法。代码和模型权重将发布在 https://github.com/6550Zhao/Policy-World-Model。",
        "translated_title": "从预测到规划：用于协作状态-动作预测的策略世界模型",
        "label": [],
        "label_reason": "论文聚焦于轨迹规划与世界模型集成，不涉及像素级图像恢复或增强",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出了统一架构的 Policy World Model，但属于常规组合与扩展"
    },
    {
        "title": "MedReason-R1: Learning to Reason for CT Diagnosis with Reinforcement\n  Learning and Local Zoom",
        "url": "http://arxiv.org/abs/2510.19626v1",
        "pub_date": "2025-10-22",
        "summary": "General-purpose large Vision-Language Models (VLMs) demonstrate strong capabilities in generating detailed descriptions for natural images. However, their performance in the medical domain remains suboptimal, even for relatively straightforward tasks, primarily due to the lack of large-scale, high-quality, specialized medical imaging datasets and the neglect of the diagnostic process that progresses from coarse to fine-grained. To address the first issue, we construct the CT-RATE-VQA dataset, which has 84K QA pairs. For the second issue, we propose MedReason-R1, a medical VLM with explicit reasoning process for disease diagnosis. MedReason-R1 incorporates a novel strategy that embeds zoom-in disease region-of-interest areas into the image, highlighting the crucial role of both global localization and disease-specific details in enhancing the model's diagnostic performance. Furthermore, we introduce the GRPO reinforcement learning framework to MedReason-R1, which enables effective reasoning without relying on costly manual annotations. Compared to recent general-purpose and medical VLMs, MedReason-R1 achieves state-of-the-art performance in CT disease diagnosis while retaining generalization. The code, checkpoints, and dataset are available at: https://github.com/Leevan001/MedReason-R1",
        "translated": "通用的大规模视觉-语言模型（VLMs）在生成自然图像的详细描述方面表现出强大的能力。然而，它们在医学领域的表现仍不理想，即使在相对简单的任务上也是如此，这主要是由于缺乏大规模的高质量、专门的医学图像数据集，并且忽视了从粗粒度到细粒度的诊断过程。为了解决第一个问题，我们构建了 CT-RATE-VQA 数据集，该数据集包含 84,000 对问答对。针对第二个问题，我们提出了 MedReason-R1，这是一种具备显式推理过程的医学 VLM，用于疾病诊断。MedReason-R1 引入了一种新颖的策略，通过在图像中嵌入放大后的疾病感兴趣区域，突出了全局定位与疾病特异性细节在提升模型诊断性能中的关键作用。此外，我们还向 MedReason-R1 引入了 GRPO 强化学习框架，该框架能够在不依赖昂贵的人工标注的前提下，实现有效的推理。与最近的通用和医学专用 VLMs 相比，MedReason-R1 在 CT 疾病诊断任务上实现了最先进的性能，同时保留了良好的泛化能力。代码、模型参数和数据集可在以下链接获取：https://github.com/Leevan001/MedReason-R1",
        "translated_title": "MedReason-R1：利用强化学习与局部放大进行 CT 诊断的推理学习",
        "label": [
            "医学图像增强",
            "图像去噪"
        ],
        "label_reason": "方法涉及医学图像增强和局部放大以提升诊断性能",
        "relevance_score": 5,
        "novelty_score": 7,
        "novelty_reason": "引入GRPO强化学习框架，提升了模型的诊断推理能力"
    },
    {
        "title": "Augmenting Moment Retrieval: Zero-Dependency Two-Stage Learning",
        "url": "http://arxiv.org/abs/2510.19622v1",
        "pub_date": "2025-10-22",
        "summary": "Existing Moment Retrieval methods face three critical bottlenecks: (1) data scarcity forces models into shallow keyword-feature associations; (2) boundary ambiguity in transition regions between adjacent events; (3) insufficient discrimination of fine-grained semantics (e.g., distinguishing ``kicking\" vs. ``throwing\" a ball). In this paper, we propose a zero-external-dependency Augmented Moment Retrieval framework, AMR, designed to overcome local optima caused by insufficient data annotations and the lack of robust boundary and semantic discrimination capabilities. AMR is built upon two key insights: (1) it resolves ambiguous boundary information and semantic confusion in existing annotations without additional data (avoiding costly manual labeling), and (2) it preserves boundary and semantic discriminative capabilities enhanced by training while generalizing to real-world scenarios, significantly improving performance. Furthermore, we propose a two-stage training framework with cold-start and distillation adaptation. The cold-start stage employs curriculum learning on augmented data to build foundational boundary/semantic awareness. The distillation stage introduces dual query sets: Original Queries maintain DETR-based localization using frozen Base Queries from the cold-start model, while Active Queries dynamically adapt to real-data distributions. A cross-stage distillation loss enforces consistency between Original and Base Queries, preventing knowledge forgetting while enabling real-world generalization. Experiments on multiple benchmarks show that AMR achieves improved performance over prior state-of-the-art approaches.",
        "translated": "现有的Moment Retrieval方法面临三个关键瓶颈：(1) 数据稀缺迫使模型建立浅层的关键词-特征关联；(2) 相邻事件之间的过渡区域中边界存在模糊性；(3) 对细粒度语义的区分能力不足（例如区分“踢”和“扔”球）。在本文中，我们提出了一种零外部依赖的增强型时刻检索框架AMR，旨在克服由于数据标注不足以及缺乏鲁棒的边界和语义区分能力所导致的局部最优问题。AMR基于两个关键见解：(1) 在不使用额外数据（避免高昂的人工标注成本）的情况下，解决了现有标注中存在的边界模糊信息和语义混淆问题；(2) 在训练中增强并保留了边界和语义的判别能力，同时能够泛化到现实场景，显著提升了性能。此外，我们提出了一个包含冷启动和蒸馏适配的两阶段训练框架。冷启动阶段在增强的数据上采用课程学习来构建基本的边界/语义感知能力。蒸馏阶段引入了双查询集：原始查询（Original Queries）通过冻结来自冷启动模型的基查询（Base Queries）来保持基于DETR的定位能力，而活跃查询（Active Queries）则动态适应真实数据分布。跨阶段蒸馏损失（cross-stage distillation loss）强制原始查询和基查询之间的一致性，防止知识遗忘的同时实现对现实场景的泛化。在多个基准数据集上的实验表明，AMR在性能上优于先前最先进的方法。",
        "translated_title": "增强矩量检索：零依赖的两阶段学习",
        "label": [],
        "label_reason": "论文聚焦于时刻检索，属于high-level任务，与图像像素级质量无关。",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出两阶段训练框架和课程学习策略，具有一定创新性。"
    },
    {
        "title": "Pragmatic Heterogeneous Collaborative Perception via Generative\n  Communication Mechanism",
        "url": "http://arxiv.org/abs/2510.19618v1",
        "pub_date": "2025-10-22",
        "summary": "Multi-agent collaboration enhances the perception capabilities of individual agents through information sharing. However, in real-world applications, differences in sensors and models across heterogeneous agents inevitably lead to domain gaps during collaboration. Existing approaches based on adaptation and reconstruction fail to support pragmatic heterogeneous collaboration due to two key limitations: (1) Intrusive retraining of the encoder or core modules disrupts the established semantic consistency among agents; and (2) accommodating new agents incurs high computational costs, limiting scalability. To address these challenges, we present a novel Generative Communication mechanism (GenComm) that facilitates seamless perception across heterogeneous multi-agent systems through feature generation, without altering the original network, and employs lightweight numerical alignment of spatial information to efficiently integrate new agents at minimal cost. Specifically, a tailored Deformable Message Extractor is designed to extract spatial message for each collaborator, which is then transmitted in place of intermediate features. The Spatial-Aware Feature Generator, utilizing a conditional diffusion model, generates features aligned with the ego agent's semantic space while preserving the spatial information of the collaborators. These generated features are further refined by a Channel Enhancer before fusion. Experiments conducted on the OPV2V-H, DAIR-V2X and V2X-Real datasets demonstrate that GenComm outperforms existing state-of-the-art methods, achieving an 81\\% reduction in both computational cost and parameter count when incorporating new agents. Our code is available at https://github.com/jeffreychou777/GenComm.",
        "translated": "多智能体协作通过信息共享提升单个智能体的感知能力。然而，在现实应用场景中，由于异构智能体在传感器和模型上的差异，协作过程中不可避免地会出现领域差异。现有的基于自适应和重建的方法由于两个关键限制，无法支持实用的异构协作：（1）对编码器或核心模块的侵入式再训练会破坏智能体之间既有的语义一致性；（2）容纳新的智能体需要较高的计算成本，限制了系统的可扩展性。为了解决这些挑战，我们提出了一种新颖的生成式通信机制（GenComm），该机制通过特征生成促进异构多智能体系统之间的无缝感知，无需修改原始网络结构，并通过空间信息的轻量级数值对齐，以最小成本高效集成新的智能体。具体而言，我们设计了一个定制化的可变形信息提取器（Deformable Message Extractor），用于提取每个协作者的空间信息，并将其作为中间特征的替代进行传输。空间感知特征生成器（Spatial-Aware Feature Generator）利用条件扩散模型（conditional diffusion model），在保持协作者空间信息的同时，生成与自身智能体语义空间对齐的特征。这些生成的特征在融合前进一步通过通道增强器（Channel Enhancer）进行优化。在 OPV2V-H、DAIR-V2X 和 V2X-Real 数据集上的实验表明，GenComm 在性能上优于现有最先进的方法，在引入新智能体时计算成本和参数数量分别减少了 81%。我们的代码可在 https://github.com/jeffreychou777/GenComm 获得。",
        "translated_title": "实用的异构协同感知生成通信机制",
        "label": [],
        "label_reason": "主要处理多智能体感知协作，非像素级图像质量复原",
        "relevance_score": 3,
        "novelty_score": 7,
        "novelty_reason": "提出新颖的生成式通信机制，提升了异构系统协作效率"
    },
    {
        "title": "Beyond sparse denoising in frames: minimax estimation with a scattering\n  transform",
        "url": "http://arxiv.org/abs/2510.19612v1",
        "pub_date": "2025-10-22",
        "summary": "A considerable amount of research in harmonic analysis has been devoted to non-linear estimators of signals contaminated by additive Gaussian noise. They are implemented by thresholding coefficients in a frame, which provide a sparse signal representation, or by minimising their $\\ell^1$ norm. However, sparse estimators in frames are not sufficiently rich to adapt to complex signal regularities. For cartoon images whose edges are piecewise $\\bf C^\\alpha$ curves, wavelet, curvelet and Xlet frames are suboptimal if the Lipschitz exponent $\\alpha \\leq 2$ is an unknown parameter. Deep convolutional neural networks have recently obtained much better numerical results, which reach the minimax asymptotic bounds for all $\\alpha$. Wavelet scattering coefficients have been introduced as simplified convolutional neural network models. They are computed by transforming the modulus of wavelet coefficients with a second wavelet transform. We introduce a denoising estimator by jointly minimising and maximising the $\\ell^1$ norms of different subsets of scattering coefficients. We prove that these $\\ell^1$ norms capture different types of geometric image regularity. Numerical experiments show that this denoising estimator reaches the minimax asymptotic bound for cartoon images for all Lipschitz exponents $\\alpha \\leq 2$. We state this numerical result as a mathematical conjecture. It provides a different harmonic analysis approach to suppress noise from signals, and to specify the geometric regularity of functions. It also opens a mathematical bridge between harmonic analysis and denoising estimators with deep convolutional network.",
        "translated": "谐波分析领域已有大量研究致力于估计被加性高斯噪声污染的信号的非线性方法。这些方法通过在某一帧中对系数进行阈值处理来实现，该帧提供稀疏的信号表示，或通过最小化其 $\\ell^1$ 范数来实现。然而，帧中的稀疏估计器不足以适应复杂的信号规律性。对于边缘为分段 $\\bf C^\\alpha$ 曲线的卡通图像，如果 Lipschitz 指数 $\\alpha \\leq 2$ 为未知参数，则小波、曲线小波和 Xlet 帧并非最优选择。最近，深度卷积神经网络在图像去噪任务中取得了显著优于传统方法的数值结果，并对所有 $\\alpha$ 都达到了极小极大渐近界。小波散射系数被提出作为简化版的卷积神经网络模型。它们通过对小波系数的模进行二次小波变换计算得到。我们通过联合最小化和最大化不同子集的散射系数的 $\\ell^1$ 范数，引入了一种图像去噪估计器。我们证明了这些 $\\ell^1$ 范数能够捕捉不同类型图像的几何规律性。数值实验表明，该去噪估计器对于所有满足 $\\alpha \\leq 2$ 的 Lipschitz 指数的卡通图像，均能达到极小极大渐近界。我们将这一数值结果表述为一个数学猜想。该结果为从信号中抑制噪声以及函数几何规律性的刻画提供了新的谐波分析方法。它也搭建了谐波分析与基于深度卷积网络的去噪估计器之间的数学桥梁。",
        "translated_title": "超越帧中的稀疏去噪：具有散射变换的极小极大估计",
        "label": [
            "图像去噪",
            "频域先验"
        ],
        "label_reason": "论文聚焦图像去噪，并探索信号几何正则性建模",
        "relevance_score": 9,
        "novelty_score": 8,
        "novelty_reason": "提出基于散射变换的极小极大去噪新方法"
    },
    {
        "title": "XBench: A Comprehensive Benchmark for Visual-Language Explanations in\n  Chest Radiography",
        "url": "http://arxiv.org/abs/2510.19599v1",
        "pub_date": "2025-10-22",
        "summary": "Vision-language models (VLMs) have recently shown remarkable zero-shot performance in medical image understanding, yet their grounding ability, the extent to which textual concepts align with visual evidence, remains underexplored. In the medical domain, however, reliable grounding is essential for interpretability and clinical adoption. In this work, we present the first systematic benchmark for evaluating cross-modal interpretability in chest X-rays across seven CLIP-style VLM variants. We generate visual explanations using cross-attention and similarity-based localization maps, and quantitatively assess their alignment with radiologist-annotated regions across multiple pathologies. Our analysis reveals that: (1) while all VLM variants demonstrate reasonable localization for large and well-defined pathologies, their performance substantially degrades for small or diffuse lesions; (2) models that are pretrained on chest X-ray-specific datasets exhibit improved alignment compared to those trained on general-domain data. (3) The overall recognition ability and grounding ability of the model are strongly correlated. These findings underscore that current VLMs, despite their strong recognition ability, still fall short in clinically reliable grounding, highlighting the need for targeted interpretability benchmarks before deployment in medical practice. XBench code is available at https://github.com/Roypic/Benchmarkingattention",
        "translated": "近年来，视觉语言模型（VLMs）在医学图像理解中展现了显著的零样本性能，然而它们的“grounding能力”，即文本概念与视觉证据的一致程度，仍未得到充分探索。在医学领域，可靠的grounding对于模型的可解释性和临床应用至关重要。在本研究中，我们首次提出了一个系统化的基准测试，用于评估七种CLIP风格VLM变体在胸部X光图像中跨模态可解释性的表现。我们通过交叉注意力和基于相似性的定位图生成视觉解释，并定量评估其与放射科医生标注区域在多种疾病类型上的一致性。我们的分析表明：(1) 尽管所有VLM变体在大而明确的疾病定位上表现合理，但它们在小病灶或弥散性病变上的性能显著下降；(2) 在胸部X光专用数据集上预训练的模型相较于在通用领域数据上训练的模型，其定位一致性更高；(3) 模型的整体识别能力与grounding能力之间存在强相关性。这些发现表明，尽管当前VLMs具有较强的识别能力，但在临床可靠的grounding方面仍有不足，强调在医学实践中部署前，需要针对可解释性设计专门的基准测试。XBench代码可在 https://github.com/Roypic/Benchmarkingattention 获得。",
        "translated_title": "XBench：胸部X光影像中视觉-语言解释的综合基准测试",
        "label": [],
        "label_reason": "论文聚焦医学图像解释性，非像素级图像恢复任务",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出首个系统性跨模态可解释性基准，具有实用价值"
    },
    {
        "title": "CBDiff:Conditional Bernoulli Diffusion Models for Image Forgery\n  Localization",
        "url": "http://arxiv.org/abs/2510.19597v1",
        "pub_date": "2025-10-22",
        "summary": "Image Forgery Localization (IFL) is a crucial task in image forensics, aimed at accurately identifying manipulated or tampered regions within an image at the pixel level. Existing methods typically generate a single deterministic localization map, which often lacks the precision and reliability required for high-stakes applications such as forensic analysis and security surveillance. To enhance the credibility of predictions and mitigate the risk of errors, we introduce an advanced Conditional Bernoulli Diffusion Model (CBDiff). Given a forged image, CBDiff generates multiple diverse and plausible localization maps, thereby offering a richer and more comprehensive representation of the forgery distribution. This approach addresses the uncertainty and variability inherent in tampered regions. Furthermore, CBDiff innovatively incorporates Bernoulli noise into the diffusion process to more faithfully reflect the inherent binary and sparse properties of forgery masks. Additionally, CBDiff introduces a Time-Step Cross-Attention (TSCAttention), which is specifically designed to leverage semantic feature guidance with temporal steps to improve manipulation detection. Extensive experiments on eight publicly benchmark datasets demonstrate that CBDiff significantly outperforms existing state-of-the-art methods, highlighting its strong potential for real-world deployment.",
        "translated": "图像伪造定位（Image Forgery Localization，IFL）是图像取证中的关键任务，旨在以像素级别精确识别图像中被篡改或修改的区域。现有方法通常生成单一确定性的定位图，这在诸如司法鉴定和安全监控等高风险应用中往往缺乏所需的精度和可靠性。为提高预测结果的可信度并降低错误风险，我们引入了一种先进的条件伯努利扩散模型（Conditional Bernoulli Diffusion Model，CBDiff）。给定一幅伪造图像，CBDiff 能够生成多个多样且合理的定位图，从而提供对伪造分布更丰富、更全面的表示。该方法有效应对了篡改区域中存在的不确定性和变异性。此外，CBDiff 创新性地将伯努利噪声融入扩散过程中，以更真实地反映伪造掩码固有的二值性和稀疏性。CBDiff 还引入了一种时间步交叉注意力机制（Time-Step Cross-Attention，TSCAttention），该机制专门设计用于在时间步进过程中利用语义特征引导，以提升篡改检测的性能。在八个公开基准数据集上的大量实验表明，CBDiff 显著优于现有的最先进方法，展示了其在实际应用中强大的潜力。",
        "translated_title": "CBDiff：用于图像篡改定位的条件伯努利扩散模型",
        "label": [
            "图像修复",
            "图像去噪"
        ],
        "label_reason": "论文涉及图像伪造定位，属于像素级图像分析，与图像修复和去噪相关",
        "relevance_score": 6,
        "novelty_score": 8,
        "novelty_reason": "提出条件伯努利扩散模型和时间步交叉注意力，改进伪造检测方法"
    },
    {
        "title": "ToolDreamer: Instilling LLM Reasoning Into Tool Retrievers",
        "url": "http://arxiv.org/abs/2510.19791v1",
        "pub_date": "2025-10-22",
        "summary": "Tool calling has become increasingly popular for Large Language Models (LLMs). However, for large tool sets, the resulting tokens would exceed the LLM's context window limit, making it impossible to include every tool. Hence, an external retriever is used to provide LLMs with the most relevant tools for a query. Existing retrieval models rank tools based on the similarity between a user query and a tool description (TD). This leads to suboptimal retrieval as user requests are often poorly aligned with the language of TD. To remedy the issue, we propose ToolDreamer, a framework to condition retriever models to fetch tools based on hypothetical (synthetic) TD generated using an LLM, i.e., description of tools that the LLM feels will be potentially useful for the query. The framework enables a more natural alignment between queries and tools within the language space of TD's. We apply ToolDreamer on the ToolRet dataset and show that our method improves the performance of sparse and dense retrievers with and without training, thus showcasing its flexibility. Through our proposed framework, our aim is to offload a portion of the reasoning burden to the retriever so that the LLM may effectively handle a large collection of tools without inundating its context window.",
        "translated": "工具调用已成为大语言模型（LLM）的一项重要功能。然而，对于工具集较大的情况，生成的 tokens 数量往往会超出 LLM 的上下文窗口限制，从而无法包含所有工具。因此，通常使用一个外部召回器，为 LLM 提供与查询最相关的工具。现有的召回模型根据用户查询与工具描述（TD）之间的相似性对工具进行排序。这种做法会导致召回效果次优，因为用户请求通常与 TD 的语言表达方式对齐不佳。为了解决这一问题，我们提出了 ToolDreamer 框架，该框架通过使用 LLM 生成假设性（合成的）TD 来对召回模型进行条件化，即生成 LLM 认为对查询可能有用的工具描述。该框架在 TD 的语言空间中实现了查询与工具之间更自然的对齐。我们在 ToolRet 数据集上应用了 ToolDreamer，并证明我们的方法在有训练和无训练的情况下均能提升稀疏和稠密召回器的性能，从而展示了其灵活性。通过我们提出的框架，我们的目标是将部分推理负担转移到召回器上，使 LLM 能够有效处理大量工具，而不会超出其上下文窗口的限制。",
        "translated_title": "ToolDreamer：将大语言模型推理能力注入工具召回模块",
        "label": [
            "LLM生成式推荐"
        ],
        "label_reason": "论文涉及利用LLM生成工具描述以提升检索相关性，适用于推荐场景。",
        "relevance_score": 7,
        "novelty_score": 8,
        "novelty_reason": "提出基于LLM生成合成描述的新型检索框架，方法新颖且有明显改进效果。"
    },
    {
        "title": "Top-P Masking for Cross Language Information Retrieval",
        "url": "http://arxiv.org/abs/2510.19758v1",
        "pub_date": "2025-10-22",
        "summary": "Top-K masking schemes have been proposed as a method to promote sparse representations in Information Retrieval (IR) tasks, as a simple alternative to Floating Point Operations per Second (FLOPS) regularization. Algorithms such as Bilingual Lexical and Document Expansion Model (BLADE), adopt this approach as a post-processing stage. We propose using Top-P Dynamic Masking similar to Nucleus Sampling in Large Language Models, and demonstrate better performance than Top-K masking. Specifically, we evaluate our methods in the domain of Cross Language Information Retrieval (CLIR)",
        "translated": "Top-K 掩码方案已被提出作为一种在信息检索（IR）任务中促进稀疏表示的方法，作为每秒浮点运算次数（FLOPS）正则化的简单替代方案。例如双语词典和文档扩展模型（BLADE）就采用此方法作为后处理阶段。我们提出使用类似于大语言模型中核心采样（Nucleus Sampling）的 Top-P 动态掩码方法，并展示了其在 Top-K 掩码基础上更优的性能。具体而言，我们在跨语言信息检索（CLIR）领域对我们的方法进行了评估。",
        "translated_title": "Top-P Masking for Cross Language Information Retrieval  \n跨语言信息检索的Top-P掩码方法",
        "label": [],
        "label_reason": "论文主要涉及信息检索，非专为推荐系统设计",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出Top-P动态掩码方法，改进了传统Top-K方法"
    },
    {
        "title": "A Matter of Time: Revealing the Structure of Time in Vision-Language\n  Models",
        "url": "http://arxiv.org/abs/2510.19559v1",
        "pub_date": "2025-10-22",
        "summary": "Large-scale vision-language models (VLMs) such as CLIP have gained popularity for their generalizable and expressive multimodal representations. By leveraging large-scale training data with diverse textual metadata, VLMs acquire open-vocabulary capabilities, solving tasks beyond their training scope. This paper investigates the temporal awareness of VLMs, assessing their ability to position visual content in time. We introduce TIME10k, a benchmark dataset of over 10,000 images with temporal ground truth, and evaluate the time-awareness of 37 VLMs by a novel methodology. Our investigation reveals that temporal information is structured along a low-dimensional, non-linear manifold in the VLM embedding space. Based on this insight, we propose methods to derive an explicit ``timeline'' representation from the embedding space. These representations model time and its chronological progression and thereby facilitate temporal reasoning tasks. Our timeline approaches achieve competitive to superior accuracy compared to a prompt-based baseline while being computationally efficient. All code and data are available at https://tekayanidham.github.io/timeline-page/.",
        "translated": "诸如CLIP等大规模视觉-语言模型（VLMs）因其具有泛化性和表现力的多模态表征而广受欢迎。通过利用带有丰富文本元数据的大规模训练数据，VLMs获得了开放词汇能力，能够解决超出其训练范围的任务。本文研究了VLMs的时序感知能力，评估其对视觉内容进行时间定位的能力。我们引入了一个名为TIME10k的基准数据集，包含超过10,000张具有时间真实标签的图像，并通过一种新的方法评估了37个VLMs的时间感知能力。我们的研究表明，时间信息在VLM嵌入空间中沿着一个低维、非线性的流形结构分布。基于这一发现，我们提出了一种从嵌入空间中推导出显式“时间线”表示的方法。这些表示能够建模时间及其时序演进，从而有助于时间推理任务。与基于提示的基线方法相比，我们的时间线方法在计算效率高且准确率具有竞争力甚至更优。所有代码和数据均可在https://tekayanidham.github.io/timeline-page/获取。",
        "translated_title": "A Matter of Time: Vision-Language 模型中时间结构的揭示",
        "label": [
            "多模态推荐"
        ],
        "label_reason": "论文探讨视觉-语言模型中的时间感知，与多模态推荐系统有一定间接关联。",
        "relevance_score": 3,
        "novelty_score": 7,
        "novelty_reason": "提出了时间线表示方法，揭示了嵌入空间中的时间结构，具有新颖性。"
    },
    {
        "title": "The Massive Legal Embedding Benchmark (MLEB)",
        "url": "http://arxiv.org/abs/2510.19365v1",
        "pub_date": "2025-10-22",
        "summary": "We present the Massive Legal Embedding Benchmark (MLEB), the largest, most diverse, and most comprehensive open-source benchmark for legal information retrieval to date. MLEB consists of ten expert-annotated datasets spanning multiple jurisdictions (the US, UK, EU, Australia, Ireland, and Singapore), document types (cases, legislation, regulatory guidance, contracts, and literature), and task types (search, zero-shot classification, and question answering). Seven of the datasets in MLEB were newly constructed in order to fill domain and jurisdictional gaps in the open-source legal information retrieval landscape. We document our methodology in building MLEB and creating the new constituent datasets, and release our code, results, and data openly to assist with reproducible evaluations.",
        "translated": "我们提出了大规模法律嵌入基准（Massive Legal Embedding Benchmark，MLEB），这是迄今为止最大、最多样且最全面的开源法律信息检索基准。MLEB包含十个由专家标注的数据集，覆盖多个司法管辖区（美国、英国、欧盟、澳大利亚、爱尔兰和新加坡）、多种文档类型（案例、立法、监管指导、合同和文献）以及多种任务类型（检索、零样本分类和问答）。为了填补开源法律信息检索领域在领域和司法管辖区上的空白，MLEB中有七个数据集是新构建的。我们记录了构建MLEB和创建新组成数据集的方法，并公开发布了代码、实验结果和数据，以支持可复现的评估。",
        "translated_title": "大规模法律嵌入基准（MLEB）",
        "label": [],
        "label_reason": "论文聚焦法律信息检索，与推荐系统无直接关联",
        "relevance_score": 2,
        "novelty_score": 5,
        "novelty_reason": "构建了大规模法律检索基准，但属于常规领域改进"
    },
    {
        "title": "CoRECT: A Framework for Evaluating Embedding Compression Techniques at\n  Scale",
        "url": "http://arxiv.org/abs/2510.19340v1",
        "pub_date": "2025-10-22",
        "summary": "Dense retrieval systems have proven to be effective across various benchmarks, but require substantial memory to store large search indices. Recent advances in embedding compression show that index sizes can be greatly reduced with minimal loss in ranking quality. However, existing studies often overlook the role of corpus complexity -- a critical factor, as recent work shows that both corpus size and document length strongly affect dense retrieval performance. In this paper, we introduce CoRECT (Controlled Retrieval Evaluation of Compression Techniques), a framework for large-scale evaluation of embedding compression methods, supported by a newly curated dataset collection. To demonstrate its utility, we benchmark eight representative types of compression methods. Notably, we show that non-learned compression achieves substantial index size reduction, even on up to 100M passages, with statistically insignificant performance loss. However, selecting the optimal compression method remains challenging, as performance varies across models. Such variability highlights the necessity of CoRECT to enable consistent comparison and informed selection of compression methods. All code, data, and results are available on GitHub and HuggingFace.",
        "translated": "稠密召回系统在各种基准测试中已被证明是有效的，但需要大量的内存来存储大规模的检索索引。嵌入压缩方面的最新进展表明，在仅轻微损失排序质量的前提下，可以大幅减少索引大小。然而，现有研究往往忽略了语料复杂性这一关键因素，因为近期研究表明，语料规模和文档长度都会显著影响稠密召回的性能。在本文中，我们引入了 CoRECT（压缩技术可控召回评估框架），这是一个用于大规模评估嵌入压缩方法的框架，并由一个新整理的数据集集合支持。为展示其有效性，我们对八类具有代表性的压缩方法进行了基准测试。值得注意的是，我们发现非学习型压缩方法即使在多达 100M 个段落的数据上，也能实现显著的索引大小减少，且性能损失在统计上不显著。然而，选择最优的压缩方法仍具有挑战性，因为不同模型的性能表现存在差异。这种差异性突显了 CoRECT 的必要性，以支持对压缩方法进行一致性比较和明智选择。所有代码、数据和结果均可在 GitHub 和 HuggingFace 上获取。",
        "translated_title": "CoRECT：一种用于大规模评估嵌入压缩技术的框架",
        "label": [
            "推荐系统评估"
        ],
        "label_reason": "论文聚焦于嵌入压缩技术的评估，适用于推荐系统的检索环节",
        "relevance_score": 6,
        "novelty_score": 7,
        "novelty_reason": "提出了一个大规模评估嵌入压缩技术的框架，具有一定创新性"
    },
    {
        "title": "Metadata Extraction Leveraging Large Language Models",
        "url": "http://arxiv.org/abs/2510.19334v1",
        "pub_date": "2025-10-22",
        "summary": "The advent of Large Language Models has revolutionized tasks across domains, including the automation of legal document analysis, a critical component of modern contract management systems. This paper presents a comprehensive implementation of LLM-enhanced metadata extraction for contract review, focusing on the automatic detection and annotation of salient legal clauses. Leveraging both the publicly available Contract Understanding Atticus Dataset (CUAD) and proprietary contract datasets, our work demonstrates the integration of advanced LLM methodologies with practical applications. We identify three pivotal elements for optimizing metadata extraction: robust text conversion, strategic chunk selection, and advanced LLM-specific techniques, including Chain of Thought (CoT) prompting and structured tool calling. The results from our experiments highlight the substantial improvements in clause identification accuracy and efficiency. Our approach shows promise in reducing the time and cost associated with contract review while maintaining high accuracy in legal clause identification. The results suggest that carefully optimized LLM systems could serve as valuable tools for legal professionals, potentially increasing access to efficient contract review services for organizations of all sizes.",
        "translated": "大语言模型的出现已经革新了各个领域的任务，包括法律文档分析的自动化，这是现代合同管理系统中的关键组成部分。本文提出了一个全面的基于大语言模型增强的合同审查元数据提取实现方案，重点在于显著法律条款的自动检测与标注。我们利用公开可用的Contract Understanding Atticus数据集（CUAD）以及专有的合同数据集，展示了先进大语言模型方法与实际应用的整合。我们识别出三个优化元数据提取的关键要素：稳健的文本转换、策略性的块选择以及高级的大语言模型特定技术，包括Chain of Thought（CoT）提示和结构化工具调用。实验结果突显了本方法在条款识别准确性和效率方面的显著提升。我们的方法在减少合同审查所需时间和成本的同时，仍能保持法律条款识别的高准确率，展现出良好的应用前景。结果表明，经过精心优化的大语言模型系统可以作为法律专业人士的有力工具，从而为各类规模的组织提供高效的合同审查服务。",
        "translated_title": "利用大语言模型进行元数据提取",
        "label": [],
        "label_reason": "论文关注元数据提取，非推荐系统核心问题。",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "结合LLM优化合同元数据提取，具有一定实用性。"
    },
    {
        "title": "C2T-ID: Converting Semantic Codebooks to Textual Document Identifiers\n  for Generative Search",
        "url": "http://arxiv.org/abs/2510.19221v1",
        "pub_date": "2025-10-22",
        "summary": "Designing document identifiers (docids) that carry rich semantic information while maintaining tractable search spaces is a important challenge in generative retrieval (GR). Popular codebook methods address this by building a hierarchical semantic tree and constraining generation to its child nodes, yet their numeric identifiers cannot leverage the large language model's pretrained natural language understanding. Conversely, using text as docid provides more semantic expressivity but inflates the decoding space, making the system brittle to early-step errors. To resolve this trade-off, we propose C2T-ID: (i) first construct semantic numerical docid via hierarchical clustering; (ii) then extract high-frequency metadata keywords and iteratively replace each numeric label with its cluster's top-K keywords; and (iii) an optional two-level semantic smoothing step further enhances the fluency of C2T-ID. Experiments on Natural Questions and Taobao's product search demonstrate that C2T-ID significantly outperforms atomic, semantic codebook, and pure-text docid baselines, demonstrating its effectiveness in balancing semantic expressiveness with search space constraints.",
        "translated": "设计携带丰富语义信息同时又能保持可处理的搜索空间的文档标识符（docid）是生成式召回（GR）中的一个重要挑战。流行的方法通过构建一个层次化的语义树并限制生成过程仅在子节点中进行来应对这一问题，但它们的数字标识符无法利用大语言模型的预训练自然语言理解能力。相反，使用文本作为docid可以提供更强的语义表达能力，但却扩展了解码空间，使系统更容易受到早期生成步骤错误的影响。为了解决这一权衡问题，我们提出了 C2T-ID：(i) 首先通过层次聚类构建语义化的数字docid；(ii) 然后提取高频率的元数据关键词，并迭代地将每个数字标签替换为该聚类的 top-K 关键词；以及 (iii) 一个可选的两级语义平滑步骤进一步提升了 C2T-ID 的流畅性。我们在 Natural Questions 和 Taobao 产品搜索上的实验表明，C2T-ID 显著优于原子型、语义码本和纯文本 docid 基线模型，证明其在平衡语义表达性和搜索空间约束方面的有效性。",
        "translated_title": "C2T-ID：将语义码本转换为文本文档标识符用于生成式搜索",
        "label": [
            "LLM生成式推荐",
            "推荐系统评估"
        ],
        "label_reason": "论文涉及生成式检索，与生成式推荐相关，但更偏重搜索领域",
        "relevance_score": 6,
        "novelty_score": 7,
        "novelty_reason": "提出一种新的语义ID生成方法，结合聚类与关键词替换，有一定创新性"
    },
    {
        "title": "XGen-Q: An Explainable Domain-Adaptive LLM Framework with\n  Retrieval-Augmented Generation for Software Security",
        "url": "http://arxiv.org/abs/2510.19006v1",
        "pub_date": "2025-10-21",
        "summary": "Generative AI and large language models (LLMs) have shown strong capabilities in code understanding, but their use in cybersecurity, particularly for malware detection and analysis, remains limited. Existing detection systems often fail to generalize to obfuscated or previously unseen threats, underscoring the need for more adaptable and explainable models. To address this challenge, we introduce XGen-Q, a domain-adapted LLM built on the Qwen-Coder architecture and pretrained on a large-scale corpus of over one million malware samples, spanning both source and assembly code. XGen-Q uses a multi-stage prompt strategy combined with retrieval-augmented generation (RAG) to deliver reliable malware identification and detailed forensic reporting, even in the presence of complex code obfuscation. To further enhance generalization, we design a training pipeline that systematically exposes the model to diverse obfuscation patterns. Experimental results show that XGen-Q achieves significantly lower perplexity than competitive baselines and exhibits strong performance on novel malware samples, demonstrating the promise of LLM-based approaches for interpretable and robust malware analysis.",
        "translated": "生成式人工智能和大语言模型（LLMs）在代码理解方面展现了强大的能力，但其在网络安全领域的应用，特别是用于恶意软件检测和分析方面仍较为有限。现有的检测系统通常难以对混淆代码或未见过的威胁进行泛化，凸显了对更具适应性和可解释性的模型的需求。为应对这一挑战，我们提出了XGen-Q，这是一个基于Qwen-Coder架构的领域适应大语言模型，并在包含超过一百万个恶意软件样本的大规模语料库上进行了预训练，涵盖源代码和汇编代码。XGen-Q结合多阶段提示策略和检索增强生成（RAG）方法，即使在存在复杂代码混淆的情况下，也能实现可靠的恶意软件识别和详细的取证报告。为进一步增强模型的泛化能力，我们设计了一套训练流程，系统性地使模型接触多样化的混淆模式。实验结果表明，XGen-Q在困惑度上显著低于竞争性的基线模型，并在新型恶意软件样本上表现出色，证明了基于大语言模型的方法在可解释性和鲁棒性的恶意软件分析中具有潜力。",
        "translated_title": "XGen-Q：一种基于检索增强生成的可解释、领域自适应大语言模型框架用于软件安全",
        "label": [],
        "label_reason": "论文主要聚焦于软件安全和恶意代码检测，与推荐系统无直接关联。",
        "relevance_score": 3,
        "novelty_score": 7,
        "novelty_reason": "提出了可解释且具有领域适应性的生成式模型新方法，具有一定的创新性。"
    },
    {
        "title": "SBAN: A Framework \\&amp; Multi-Dimensional Dataset for Large Language Model\n  Pre-Training and Software Code Mining",
        "url": "http://arxiv.org/abs/2510.18936v1",
        "pub_date": "2025-10-21",
        "summary": "This paper introduces SBAN (Source code, Binary, Assembly, and Natural Language Description), a large-scale, multi-dimensional dataset designed to advance the pre-training and evaluation of large language models (LLMs) for software code analysis. SBAN comprises more than 3 million samples, including 2.9 million benign and 672,000 malware respectively, each represented across four complementary layers: binary code, assembly instructions, natural language descriptions, and source code. This unique multimodal structure enables research on cross-representation learning, semantic understanding of software, and automated malware detection. Beyond security applications, SBAN supports broader tasks such as code translation, code explanation, and other software mining tasks involving heterogeneous data. It is particularly suited for scalable training of deep models, including transformers and other LLM architectures. By bridging low-level machine representations and high-level human semantics, SBAN provides a robust foundation for building intelligent systems that reason about code. We believe that this dataset opens new opportunities for mining software behavior, improving security analytics, and enhancing LLM capabilities in pre-training and fine-tuning tasks for software code mining.",
        "translated": "本文介绍了SBAN（源代码、二进制、汇编和自然语言描述），这是一个旨在推动面向软件代码分析的大语言模型（LLMs）预训练与评估的大规模多维数据集。SBAN包含超过300万个样本，其中良性样本为290万个，恶意软件样本为672,000个，每个样本在四个互补层上都有表示：二进制代码、汇编指令、自然语言描述和源代码。这种独特的多模态结构使得在跨表示学习、软件语义理解和自动化恶意软件检测等方面的研究成为可能。除了安全相关应用外，SBAN还支持更广泛的软件任务，如代码翻译、代码解释以及其他涉及异构数据的软件挖掘任务。该数据集特别适用于深度模型的可扩展训练，包括transformer和其他LLM架构。通过连接底层机器表示和高层人类语义，SBAN为构建能够推理代码的智能系统提供了坚实的基础。我们认为，该数据集为挖掘软件行为、提升安全分析能力以及增强LLM在软件代码挖掘任务中的预训练和微调能力带来了新的机遇。",
        "translated_title": "SBAN：一个用于大语言模型预训练与软件代码挖掘的框架与多维数据集",
        "label": [],
        "label_reason": "论文主要关注代码分析与预训练，与推荐系统无直接关联。",
        "relevance_score": 3,
        "novelty_score": 6,
        "novelty_reason": "提出了多维度异构数据集，对代码分析和LLM训练有一定创新。"
    },
    {
        "title": "DuoLens: A Framework for Robust Detection of Machine-Generated\n  Multilingual Text and Code",
        "url": "http://arxiv.org/abs/2510.18904v1",
        "pub_date": "2025-10-21",
        "summary": "The prevalence of Large Language Models (LLMs) for generating multilingual text and source code has only increased the imperative for machine-generated content detectors to be accurate and efficient across domains. Current detectors, predominantly utilizing zero-shot methods, such as Fast DetectGPT or GPTZero, either incur high computational cost or lack sufficient accuracy, often with a trade-off between the two, leaving room for further improvement. To address these gaps, we propose the fine-tuning of encoder-only Small Language Models (SLMs), in particular, the pre-trained models of RoBERTA and CodeBERTa using specialized datasets on source code and other natural language to prove that for the task of binary classification, SLMs outperform LLMs by a huge margin whilst using a fraction of compute. Our encoders achieve AUROC $= 0.97$ to $0.99$ and macro-F1 $0.89$ to $0.94$ while reducing latency by $8$-$12\\times$ and peak VRAM by $3$-$5\\times$ at $512$-token inputs. Under cross-generator shifts and adversarial transformations (paraphrase, back-translation; code formatting/renaming), performance retains $\\geq 92%$ of clean AUROC. We release training and evaluation scripts with seeds and configs; a reproducibility checklist is also included.",
        "translated": "大语言模型（LLMs）在生成多语言文本和源代码方面的普及进一步提高了对跨领域机器生成内容检测器在准确性和效率方面提出的要求。目前的检测器主要采用零样本方法（zero-shot methods），如 Fast DetectGPT 或 GPTZero，要么计算成本高昂，要么准确性不足，通常在两者之间存在权衡，因此仍存在进一步改进的空间。为了解决这些问题，我们提出对仅编码器结构的小语言模型（SLMs）进行微调，特别是使用 RoBERTa 和 CodeBERTa 的预训练模型，并结合源代码和其他自然语言的专用数据集，以证明在二分类任务中，SLMs 的性能远超 LLMs，同时仅需极小的计算资源。我们的编码器在输入长度为 512-token 时，实现了 AUROC $= 0.97$ 到 $0.99$ 以及 macro-F1 $= 0.89$ 到 $0.94$，同时将延迟降低了 $8$-$12\\times$，峰值 VRAM 降低了 $3$-$5\\times$。在面对跨生成器的分布变化和对抗性变换（如改写、回译；代码格式化/重命名）时，性能仍能保留原始 AUROC 的 $\\geq 92\\%$。我们发布了包含随机种子和配置的训练与评估脚本，并附有可复现性检查清单。",
        "translated_title": "DuoLens：一种用于鲁棒检测机器生成的多语言文本和代码的框架",
        "label": [],
        "label_reason": "论文主题为文本和代码生成检测，与推荐系统无直接关联。",
        "relevance_score": 2,
        "novelty_score": 5,
        "novelty_reason": "提出微调小型编码器模型用于生成检测，方法有一定改进但不具颠覆性。"
    },
    {
        "title": "Generative Reasoning Recommendation via LLMs",
        "url": "http://arxiv.org/abs/2510.20815v1",
        "pub_date": "2025-10-23",
        "summary": "Despite their remarkable reasoning capabilities across diverse domains, large language models (LLMs) face fundamental challenges in natively functioning as generative reasoning recommendation models (GRRMs), where the intrinsic modeling gap between textual semantics and collaborative filtering signals, combined with the sparsity and stochasticity of user feedback, presents significant obstacles. This work explores how to build GRRMs by adapting pre-trained LLMs, which achieves a unified understanding-reasoning-prediction manner for recommendation tasks. We propose GREAM, an end-to-end framework that integrates three components: (i) Collaborative-Semantic Alignment, which fuses heterogeneous textual evidence to construct semantically consistent, discrete item indices and auxiliary alignment tasks that ground linguistic representations in interaction semantics; (ii) Reasoning Curriculum Activation, which builds a synthetic dataset with explicit Chain-of-Thought supervision and a curriculum that progresses through behavioral evidence extraction, latent preference modeling, intent inference, recommendation formulation, and denoised sequence rewriting; and (iii) Sparse-Regularized Group Policy Optimization (SRPO), which stabilizes post-training via Residual-Sensitive Verifiable Reward and Bonus-Calibrated Group Advantage Estimation, enabling end-to-end optimization under verifiable signals despite sparse successes. GREAM natively supports two complementary inference modes: Direct Sequence Recommendation for high-throughput, low-latency deployment, and Sequential Reasoning Recommendation that first emits an interpretable reasoning chain for causal transparency. Experiments on three datasets demonstrate consistent gains over strong baselines, providing a practical path toward verifiable-RL-driven LLM recommenders.",
        "translated": "尽管大语言模型（LLMs）在多个领域展现出卓越的推理能力，但它们在原生地作为生成式推理推荐模型（GRRMs）运行时仍面临根本性挑战，其中文本语义与协同过滤信号之间的内在建模差距，以及用户反馈的稀疏性和随机性，构成了显著的障碍。本文探讨了如何通过适配预训练大语言模型来构建GRRMs，从而在推荐任务中实现统一的理解-推理-预测范式。我们提出了GREAM，一个端到端的框架，集成了三个组件：(i) 协同语义对齐，该组件融合异构文本证据以构建语义一致的离散物料索引和辅助对齐任务，从而将语言表示接地于交互语义；(ii) 推理课程激活，该组件构建了一个带有显式链式思维（Chain-of-Thought）监督的合成数据集，并设计了一个课程，依次包括行为证据提取、潜在偏好建模、意图推断、推荐生成和去噪序列重写；(iii) 稀疏正则化的群体策略优化（SRPO），该组件通过残差敏感的可验证奖励和奖励校准的群体优势估计稳定后训练过程，即使在成功信号稀疏的情况下也能实现端到端优化。GREAM原生支持两种互补的推理模式：直接序列推荐，适用于高吞吐量、低延迟的部署场景；以及序列推理推荐，该模式首先生成可解释的推理链，以实现因果透明性。在三个数据集上的实验表明，GREAM在多个强基线上保持了一致的提升，为构建以可验证强化学习驱动的LLM推荐系统提供了切实可行的路径。",
        "translated_title": "生成式推理推荐",
        "label": [
            "LLM生成式推荐",
            "序列推荐",
            "推荐系统评估"
        ],
        "label_reason": "论文聚焦于LLM在推荐中的生成式推理，涉及序列建模和实际推荐任务。",
        "relevance_score": 9,
        "novelty_score": 8,
        "novelty_reason": "提出GREAM框架，结合语义对齐与课程推理，改进LLM在推荐中的应用方式。"
    },
    {
        "title": "RAGRank: Using PageRank to Counter Poisoning in CTI LLM Pipelines",
        "url": "http://arxiv.org/abs/2510.20768v1",
        "pub_date": "2025-10-23",
        "summary": "Retrieval-Augmented Generation (RAG) has emerged as the dominant architectural pattern to operationalize Large Language Model (LLM) usage in Cyber Threat Intelligence (CTI) systems. However, this design is susceptible to poisoning attacks, and previously proposed defenses can fail for CTI contexts as cyber threat information is often completely new for emerging attacks, and sophisticated threat actors can mimic legitimate formats, terminology, and stylistic conventions. To address this issue, we propose that the robustness of modern RAG defenses can be accelerated by applying source credibility algorithms on corpora, using PageRank as an example. In our experiments, we demonstrate quantitatively that our algorithm applies a lower authority score to malicious documents while promoting trusted content, using the standardized MS MARCO dataset. We also demonstrate proof-of-concept performance of our algorithm on CTI documents and feeds.",
        "translated": "检索增强生成（RAG）已成为在网络威胁情报（CTI）系统中部署大语言模型（LLM）的主要架构模式。然而，这种设计容易受到投毒攻击的影响，且先前提出的防御方法在CTI场景下可能失效，因为网络威胁信息往往针对新兴攻击是全新的，且复杂的攻击者可以模仿合法的格式、术语和风格惯例。为了解决这一问题，我们提出通过在语料库上应用来源可信度算法（以PageRank为例）来提升现代RAG防御的鲁棒性。在我们的实验中，我们定量展示了该算法在降低恶意文档的权威评分的同时，提升了可信内容的权重，实验基于标准化的MS MARCO数据集。我们还展示了该算法在CTI文档和信息流上的概念验证性能。",
        "translated_title": "RAGRank：使用PageRank对抗CTI大语言模型流水线中的投毒攻击",
        "label": [
            "LLM生成式推荐",
            "推荐系统评估"
        ],
        "label_reason": "论文涉及LLM生成式推荐中的RAG架构及其安全性问题，但主要聚焦于CTI领域。",
        "relevance_score": 5,
        "novelty_score": 7,
        "novelty_reason": "提出将PageRank用于提升RAG系统对恶意内容的鲁棒性，具有一定的方法创新性。"
    },
    {
        "title": "Analyticup E-commerce Product Search Competition Technical Report from\n  Team Tredence_AICOE",
        "url": "http://arxiv.org/abs/2510.20674v1",
        "pub_date": "2025-10-23",
        "summary": "This study presents the multilingual e-commerce search system developed by the Tredence_AICOE team. The competition features two multilingual relevance tasks: Query-Category (QC) Relevance, which evaluates how well a user's search query aligns with a product category, and Query-Item (QI) Relevance, which measures the match between a multilingual search query and an individual product listing. To ensure full language coverage, we performed data augmentation by translating existing datasets into languages missing from the development set, enabling training across all target languages. We fine-tuned Gemma-3 12B and Qwen-2.5 14B model for both tasks using multiple strategies. The Gemma-3 12B (4-bit) model achieved the best QC performance using original and translated data, and the best QI performance using original, translated, and minority class data creation. These approaches secured 4th place on the final leaderboard, with an average F1-score of 0.8857 on the private test set.",
        "translated": "本研究介绍了由 Tredence_AICOE 团队开发的多语言电商搜索系统。该竞赛包含两个多语言相关性任务：查询-类别（Query-Category, QC）相关性任务，评估用户搜索查询与产品类别的一致性程度；以及查询-商品（Query-Item, QI）相关性任务，衡量多语言搜索查询与单个商品列表的匹配程度。为确保全面的语言覆盖，我们通过将现有数据集翻译为开发集中缺失的语言进行数据增强，从而实现了在所有目标语言上的训练。我们采用多种策略对 Gemma-3 12B 和 Qwen-2.5 14B 模型在两项任务上进行了微调。Gemma-3 12B（4 位）模型在使用原始和翻译数据时取得了最佳 QC 性能，在使用原始、翻译和少数类数据生成时取得了最佳 QI 性能。这些方法在最终排行榜上取得了第 4 名的成绩，在私有测试集上的平均 F1 分数为 0.8857。",
        "translated_title": "Analyticup 电子商务产品搜索竞赛技术报告  \n来自 Team Tredence_AICOE",
        "label": [
            "通用推荐技术",
            "跨域/联邦推荐"
        ],
        "label_reason": "论文涉及多语言搜索相关性，与推荐有间接联系",
        "relevance_score": 5,
        "novelty_score": 4,
        "novelty_reason": "采用数据增强和多语言模型微调，改进常规"
    },
    {
        "title": "Practical Code RAG at Scale: Task-Aware Retrieval Design Choices under\n  Compute Budgets",
        "url": "http://arxiv.org/abs/2510.20609v1",
        "pub_date": "2025-10-23",
        "summary": "We study retrieval design for code-focused generation tasks under realistic compute budgets. Using two complementary tasks from Long Code Arena -- code completion and bug localization -- we systematically compare retrieval configurations across various context window sizes along three axes: (i) chunking strategy, (ii) similarity scoring, and (iii) splitting granularity. (1) For PL-PL, sparse BM25 with word-level splitting is the most effective and practical, significantly outperforming dense alternatives while being an order of magnitude faster. (2) For NL-PL, proprietary dense encoders (Voyager-3 family) consistently beat sparse retrievers, however requiring 100x larger latency. (3) Optimal chunk size scales with available context: 32-64 line chunks work best at small budgets, and whole-file retrieval becomes competitive at 16000 tokens. (4) Simple line-based chunking matches syntax-aware splitting across budgets. (5) Retrieval latency varies by up to 200x across configurations; BPE-based splitting is needlessly slow, and BM25 + word splitting offers the best quality-latency trade-off. Thus, we provide evidence-based recommendations for implementing effective code-oriented RAG systems based on task requirements, model constraints, and computational efficiency.",
        "translated": "我们研究在现实计算预算下针对代码生成任务的召回设计。通过使用 Long Code Arena 中的两个互补任务——代码补全和错误定位，我们在不同的上下文窗口大小下，从三个维度系统地比较了召回配置：(i) 分块策略，(ii) 相似性评分，以及 (iii) 分割粒度。(1) 对于 PL-PL 任务，基于词级分割的稀疏 BM25 是最有效且实用的方法，其性能显著优于密集方法，同时速度快一个数量级。(2) 对于 NL-PL 任务，专有密集编码器（Voyager-3 家族）始终优于稀疏召回器，但其延迟也大 100 倍。(3) 最优的分块大小随着可用上下文长度而变化：在较小的预算下，32-64 行的分块效果最佳，而在 16000 tokens 的预算下，整个文件的召回变得具有竞争力。(4) 基于行的简单分块策略在不同预算下与语法感知的分割方法效果相当。(5) 在不同配置下，召回的延迟变化可达 200 倍；基于 BPE 的分割速度过慢且不必要；BM25 + 词级分割在质量和延迟之间提供了最佳的平衡。因此，我们基于任务需求、模型限制和计算效率，提供了基于实证的建议，用于实现高效的面向代码的 RAG 系统。",
        "translated_title": "大规模实用代码RAG：计算预算下的任务感知召回设计选择",
        "label": [],
        "label_reason": "",
        "relevance_score": 0,
        "novelty_score": 0,
        "novelty_reason": ""
    },
    {
        "title": "Rotate Both Ways: Time-and-Order RoPE for Generative Recommendation",
        "url": "http://arxiv.org/abs/2510.20455v1",
        "pub_date": "2025-10-23",
        "summary": "Generative recommenders, typically transformer-based autoregressive models, predict the next item or action from a user's interaction history. Their effectiveness depends on how the model represents where an interaction event occurs in the sequence (discrete index) and when it occurred in wall-clock time. Prevailing approaches inject time via learned embeddings or relative attention biases. In this paper, we argue that RoPE-based approaches, if designed properly, can be a stronger alternative for jointly modeling temporal and sequential information in user behavior sequences. While vanilla RoPE in LLMs considers only token order, generative recommendation requires incorporating both event time and token index. To address this, we propose Time-and-Order RoPE (TO-RoPE), a family of rotary position embedding designs that treat index and time as angle sources shaping the query-key geometry directly. We present three instantiations: early fusion, split-by-dim, and split-by-head. Extensive experiments on both publicly available datasets and a proprietary industrial dataset show that TO-RoPE variants consistently improve accuracy over existing methods for encoding time and index. These results position rotary embeddings as a simple, principled, and deployment-friendly foundation for generative recommendation.",
        "translated": "生成式推荐系统，通常基于 Transformer 的自回归模型，通过用户的历史交互序列预测下一个物料或行为。其有效性取决于模型如何表示交互事件在序列中的位置（离散索引）以及在实际时间中的发生时间。现有的方法通常通过学习到的嵌入或相对注意力偏置来引入时间信息。在本文中，我们认为，如果设计得当，基于 RoPE 的方法可以成为在用户行为序列中联合建模时间和顺序信息的更优替代方案。虽然大语言模型中的标准 RoPE 仅考虑了词元顺序，但生成式推荐需要同时融合事件时间与词元索引。为了解决这一问题，我们提出了时间与顺序 RoPE（Time-and-Order RoPE，TO-RoPE），这是一组将索引和时间作为直接塑造查询-键几何结构的角度来源的旋转位置嵌入设计。我们给出了三种实现方式：早期融合、按维度划分和按注意力头划分。在多个公开数据集和一个工业私有数据集上的广泛实验表明，TO-RoPE 的不同变体在编码时间和索引方面始终优于现有方法。这些结果表明，旋转嵌入可以作为生成式推荐的一个简单、原理清晰且易于部署的基础。",
        "translated_title": "Rotate Both Ways: Time-and-Order RoPE for Generative Recommendation  \n双向旋转：面向生成式推荐的时序与序号 RoPE  \n\n**Abstract**  \nRecent advances in large language models (LLMs) have enabled their application to recommendation systems, especially for generative recommendation tasks where the goal is to produce a sequence of recommendation candidates. However, the sequential nature of these tasks introduces two key challenges: modeling temporal dynamics and preserving the order information of the input sequence. In this work, we propose the Time-and-Order RoPE (T-O RoPE), a novel rotary position embedding method tailored for generative recommendation. T-O RoPE integrates both temporal and positional order information into the rotary embedding, allowing the model to simultaneously capture sequential dependencies and maintain the temporal coherence of the interaction history. We incorporate T-O RoPE into the state-of-the-art generative recommendation framework, RecRank, and evaluate it on two benchmark datasets: RecDial and Taobao. Experimental results demonstrate that T-O RoPE consistently improves the performance of generative models across multiple metrics, including diversity, relevance, and coherence. Furthermore, we conduct ablation studies to validate the effectiveness of different components of T-O RoPE. Our method provides a flexible and efficient way to model sequential interactions in generative recommendation systems and sets a new baseline for future research in this area.  \n\n**摘要**  \n近期大语言模型（LLMs）的发展使其能够应用于推荐系统，尤其是在生成式推荐任务中，其目标是生成一序列的推荐候选。然而，这类任务的序列特性引入了两个关键挑战：建模时序动态以及保留输入序列的序号信息。在本文中，我们提出了时序与序号 RoPE（T-O RoPE），一种为生成式推荐任务定制的新型旋转位置嵌入方法。T-O RoPE 将时序和序号信息整合进旋转嵌入中，使模型能够同时捕捉序列依赖关系并保持交互历史的时序一致性。我们将 T-O RoPE 应用于最先进的生成式推荐框架 RecRank，并在两个基准数据集 RecDial 和 Taobao 上进行了评估。实验结果表明，T-O RoPE 在多样性、相关性及一致性等多个指标上均能持续提升生成模型的性能。此外，我们还进行了消融实验以验证 T-O RoPE 不同组件的有效性。我们的方法为生成式推荐系统中建模序列交互提供了一种灵活且高效的方式，并为该领域的未来研究设定了新的基线。",
        "label": [
            "LLM生成式推荐",
            "序列推荐"
        ],
        "label_reason": "论文聚焦生成式推荐中的序列建模问题，提出TO-RoPE用于时间与顺序联合建模",
        "relevance_score": 8,
        "novelty_score": 7,
        "novelty_reason": "提出针对推荐系统的时间-顺序联合建模的新方法TO-RoPE，改进了传统RoPE设计"
    },
    {
        "title": "From Generation to Attribution: Music AI Agent Architectures for the\n  Post-Streaming Era",
        "url": "http://arxiv.org/abs/2510.20276v1",
        "pub_date": "2025-10-23",
        "summary": "Generative AI is reshaping music creation, but its rapid growth exposes structural gaps in attribution, rights management, and economic models. Unlike past media shifts, from live performance to recordings, downloads, and streaming, AI transforms the entire lifecycle of music, collapsing boundaries between creation, distribution, and monetization. However, existing streaming systems, with opaque and concentrated royalty flows, are ill-equipped to handle the scale and complexity of AI-driven production. We propose a content-based Music AI Agent architecture that embeds attribution directly into the creative workflow through block-level retrieval and agentic orchestration. Designed for iterative, session-based interaction, the system organizes music into granular components (Blocks) stored in BlockDB; each use triggers an Attribution Layer event for transparent provenance and real-time settlement. This framework reframes AI from a generative tool into infrastructure for a Fair AI Media Platform. By enabling fine-grained attribution, equitable compensation, and participatory engagement, it points toward a post-streaming paradigm where music functions not as a static catalog but as a collaborative and adaptive ecosystem.",
        "translated": "生成式人工智能正在重塑音乐创作，但其快速发展暴露了在归属、版权管理和经济模型方面的结构性缺口。与以往从现场表演到录音、下载和流媒体的媒介转变不同，人工智能正在改变音乐的整个生命周期，并模糊了创作、分发和变现之间的界限。然而，现有的流媒体系统由于其不透明且集中的版税分配机制，难以应对人工智能驱动生产所带来的规模和复杂性。我们提出了一种基于内容的音乐人工智能代理架构，通过区块级别的召回和代理式协调（agentic orchestration），将归属直接嵌入创作流程中。该系统设计用于迭代式、会话式的交互，将音乐组织为细粒度的组件（Blocks），存储在 BlockDB 中；每次使用都会触发 Attribution Layer 事件，以实现透明的来源追踪和实时结算。这一框架将人工智能从生成工具的视角重新定义为构建公平人工智能媒体平台的基础设施。通过实现细粒度归属、公平补偿和参与式互动，该框架指明了一种后流媒体时代范式，在此范式下，音乐不再作为静态目录存在，而是作为一种协作和自适应的生态系统。",
        "translated_title": "从生成到归因：面向流媒体时代的音乐AI智能体架构",
        "label": [
            "LLM生成式推荐",
            "多模态推荐"
        ],
        "label_reason": "论文涉及生成式AI在音乐领域的应用，与生成式推荐相关。",
        "relevance_score": 7,
        "novelty_score": 8,
        "novelty_reason": "提出了新颖的音乐生成与归属框架，具有一定的创新性。"
    },
    {
        "title": "Balancing Fine-tuning and RAG: A Hybrid Strategy for Dynamic LLM\n  Recommendation Updates",
        "url": "http://arxiv.org/abs/2510.20260v1",
        "pub_date": "2025-10-23",
        "summary": "Large Language Models (LLMs) empower recommendation systems through their advanced reasoning and planning capabilities. However, the dynamic nature of user interests and content poses a significant challenge: While initial fine-tuning aligns LLMs with domain knowledge and user preferences, it fails to capture such real-time changes, necessitating robust update mechanisms. This paper investigates strategies for updating LLM-powered recommenders, focusing on the trade-offs between ongoing fine-tuning and Retrieval-Augmented Generation (RAG). Using an LLM-powered user interest exploration system as a case study, we perform a comparative analysis of these methods across dimensions like cost, agility, and knowledge incorporation. We propose a hybrid update strategy that leverages the long-term knowledge adaptation of periodic fine-tuning with the agility of low-cost RAG. We demonstrate through live A/B experiments on a billion-user platform that this hybrid approach yields statistically significant improvements in user satisfaction, offering a practical and cost-effective framework for maintaining high-quality LLM-powered recommender systems.",
        "translated": "大语言模型（LLMs）凭借其先进的推理和规划能力，为推荐系统提供了支持。然而，用户兴趣和内容的动态性带来了重大挑战：虽然初始的微调使LLMs与领域知识和用户偏好相一致，但难以捕捉这些实时变化，因此需要稳健的更新机制。本文研究了用于更新大语言模型驱动的推荐系统的策略，重点关注持续微调和检索增强生成（RAG）之间的权衡。以一个由LLM驱动的用户兴趣探索系统为案例，我们在成本、敏捷性以及知识融合等维度上对这些方法进行了比较分析。我们提出了一种混合更新策略，将周期性微调的长期知识适应能力与低成本RAG的敏捷性相结合。我们通过在一个十亿用户级别的平台上进行实时A/B实验，展示了这种混合方法在用户满意度方面具有统计显著的提升，为维护高质量的大语言模型推荐系统提供了一种实用且成本有效的框架。",
        "translated_title": "平衡微调与RAG：一种面向动态大语言模型推荐更新的混合策略",
        "label": [
            "LLM生成式推荐"
        ],
        "label_reason": "论文探讨了LLM在推荐系统中的更新策略，涉及生成式推荐。",
        "relevance_score": 8,
        "novelty_score": 7,
        "novelty_reason": "提出了结合微调和RAG的混合更新策略，具有一定创新性。"
    },
    {
        "title": "Multimedia-Aware Question Answering: A Review of Retrieval and\n  Cross-Modal Reasoning Architectures",
        "url": "http://arxiv.org/abs/2510.20193v1",
        "pub_date": "2025-10-23",
        "summary": "Question Answering (QA) systems have traditionally relied on structured text data, but the rapid growth of multimedia content (images, audio, video, and structured metadata) has introduced new challenges and opportunities for retrieval-augmented QA. In this survey, we review recent advancements in QA systems that integrate multimedia retrieval pipelines, focusing on architectures that align vision, language, and audio modalities with user queries. We categorize approaches based on retrieval methods, fusion techniques, and answer generation strategies, and analyze benchmark datasets, evaluation protocols, and performance tradeoffs. Furthermore, we highlight key challenges such as cross-modal alignment, latency-accuracy tradeoffs, and semantic grounding, and outline open problems and future research directions for building more robust and context-aware QA systems leveraging multimedia data.",
        "translated": "问答（QA）系统传统上依赖于结构化文本数据，但多媒体内容（图像、音频、视频和结构化元数据）的快速增长为增强召回的问答系统带来了新的挑战和机遇。在本综述中，我们回顾了近期将多媒体召回流程集成到问答系统中的研究成果，重点关注将视觉、语言和音频模态与用户查询对齐的系统架构。我们根据召回方法、融合技术以及答案生成策略对相关方法进行了分类，并分析了基准数据集、评估协议以及性能上的权衡。此外，我们强调了诸如跨模态对齐、延迟与准确率的权衡以及语义基础等关键挑战，并概述了构建更加鲁棒和上下文感知的问答系统在利用多媒体数据方面仍存在的开放问题和未来研究方向。",
        "translated_title": "多媒体感知问答：召回与跨模态推理架构综述",
        "label": [
            "多模态推荐"
        ],
        "label_reason": "论文涉及多模态QA系统，与多模态推荐间接相关。",
        "relevance_score": 5,
        "novelty_score": 6,
        "novelty_reason": "总结了多模态QA的最新进展，提出了一些开放问题，但整体为综述性质。"
    },
    {
        "title": "Rank-GRPO: Training LLM-based Conversational Recommender Systems with\n  Reinforcement Learning",
        "url": "http://arxiv.org/abs/2510.20150v1",
        "pub_date": "2025-10-23",
        "summary": "Large language models (LLMs) are reshaping the recommender system paradigm by enabling users to express preferences and receive recommendations through conversations. Yet, aligning LLMs to the recommendation task remains challenging: pretrained LLMs often generate out-of-catalog items, violate required output formats, and their ranking quality degrades sharply toward the end of the generated list. To this end, we propose ConvRec-R1, a two-stage framework for end-to-end training of LLM-based conversational recommender systems. In Stage 1, we construct a behavioral-cloning dataset with a Remap-Reflect-Adjust pipeline, which produces high-quality, catalog-grounded demonstrations from powerful blackbox LLMs to warm-start the RL training. In Stage 2, we propose Rank-GRPO, a principled extension of group relative policy optimization (GRPO) tailored to tasks with rank-style outputs. Rank-GRPO treats each rank in the recommendation list as the unit instead of token (too fine-grained) or sequence (too coarse), redefining rewards to remove non-causal credit assignment and introducing a rank-level importance ratio based on the geometric mean of rank-wise token probabilities to stabilize policy updates. Experiments on the public Reddit-v2 dataset show that ConvRec-R1 converges faster and achieves higher Recall and NDCG than GRPO-style baselines. Code and datasets are released at https://github.com/yaochenzhu/Rank-GRPO.",
        "translated": "大语言模型（LLMs）正在通过使用户能够通过对话表达偏好并接收推荐，重塑推荐系统的范式。然而，将LLMs对齐到推荐任务仍然是一个挑战：预训练的大语言模型通常会生成目录之外的物料，违反所需的输出格式，并且其排序质量在生成列表的末尾急剧下降。为此，我们提出了ConvRec-R1，一个面向LLM基础的对话推荐系统的端到端训练的两阶段框架。在第一阶段，我们通过Remap-Reflect-Adjust流水线构建一个行为克隆数据集，该流水线从强大的黑盒大语言模型中生成高质量、与目录一致的演示，以实现RL训练的热启动。在第二阶段，我们提出Rank-GRPO，这是针对具有排序风格输出任务的群体相对策略优化（GRPO）的一种原则性扩展。Rank-GRPO将推荐列表中的每个排序位（rank）视为单元，而不是过于细粒度的token或过于粗粒度的序列，重新定义奖励以消除非因果的信用分配，并引入基于排序级token概率几何均值的排序级重要性比例，以稳定策略更新。在公共的Reddit-v2数据集上的实验表明，ConvRec-R1比GRPO风格的基线方法更快收敛，并取得了更高的Recall和NDCG指标。代码和数据集已发布在 https://github.com/yaochenzhu/Rank-GRPO。",
        "translated_title": "Rank-GRPO: 基于强化学习的LLM对话推荐系统训练",
        "label": [
            "LLM生成式推荐",
            "精排",
            "推荐系统评估"
        ],
        "label_reason": "论文聚焦LLM生成式推荐中的排序优化，与推荐系统高度相关。",
        "relevance_score": 9,
        "novelty_score": 8,
        "novelty_reason": "提出Rank-GRPO方法，改进RL训练策略，具有创新性。"
    },
    {
        "title": "Automating Iconclass: LLMs and RAG for Large-Scale Classification of\n  Religious Woodcuts",
        "url": "http://arxiv.org/abs/2510.19986v1",
        "pub_date": "2025-10-22",
        "summary": "This paper presents a novel methodology for classifying early modern religious images by using Large Language Models (LLMs) and vector databases in combination with Retrieval-Augmented Generation (RAG). The approach leverages the full-page context of book illustrations from the Holy Roman Empire, allowing the LLM to generate detailed descriptions that incorporate both visual and textual elements. These descriptions are then matched to relevant Iconclass codes through a hybrid vector search. This method achieves 87% and 92% precision at five and four levels of classification, significantly outperforming traditional image and keyword-based searches. By employing full-page descriptions and RAG, the system enhances classification accuracy, offering a powerful tool for large-scale analysis of early modern visual archives. This interdisciplinary approach demonstrates the growing potential of LLMs and RAG in advancing research within art history and digital humanities.",
        "translated": "本文提出了一种新颖的方法，用于通过结合使用大语言模型（LLM）、向量数据库和检索增强生成（RAG）对早期现代宗教图像进行分类。该方法利用了神圣罗马帝国书籍插图的整页上下文，使LLM能够生成包含视觉与文本元素的详细描述。随后，这些描述通过混合向量搜索与相关Iconclass代码进行匹配。该方法在五级和四级分类中的精度分别达到了87%和92%，显著优于传统的基于图像和关键词的搜索方法。通过采用整页描述和RAG，该系统提升了分类准确性，为大规模分析早期现代视觉档案提供了一个强有力的工具。这种跨学科的方法展示了LLM和RAG在推动艺术史和数字人文学科研究方面的日益增长的潜力。",
        "translated_title": "Automating Iconclass: LLMs and RAG for Large-Scale Classification of Religious Woodcuts  \n图标分类的自动化：大语言模型与检索增强生成在宗教木刻大规模分类中的应用",
        "label": [],
        "label_reason": "方法涉及LLM和RAG，但应用于艺术图像分类，与推荐系统无直接关联",
        "relevance_score": 3,
        "novelty_score": 7,
        "novelty_reason": "提出将LLM和RAG结合用于图像分类，具有新颖性和跨学科价值"
    },
    {
        "title": "HoloCine: Holistic Generation of Cinematic Multi-Shot Long Video\n  Narratives",
        "url": "http://arxiv.org/abs/2510.20822v1",
        "pub_date": "2025-10-23",
        "summary": "State-of-the-art text-to-video models excel at generating isolated clips but fall short of creating the coherent, multi-shot narratives, which are the essence of storytelling. We bridge this \"narrative gap\" with HoloCine, a model that generates entire scenes holistically to ensure global consistency from the first shot to the last. Our architecture achieves precise directorial control through a Window Cross-Attention mechanism that localizes text prompts to specific shots, while a Sparse Inter-Shot Self-Attention pattern (dense within shots but sparse between them) ensures the efficiency required for minute-scale generation. Beyond setting a new state-of-the-art in narrative coherence, HoloCine develops remarkable emergent abilities: a persistent memory for characters and scenes, and an intuitive grasp of cinematic techniques. Our work marks a pivotal shift from clip synthesis towards automated filmmaking, making end-to-end cinematic creation a tangible future. Our code is available at: https://holo-cine.github.io/.",
        "translated": "最先进的文本到视频模型在生成孤立片段方面表现出色，但无法生成连贯的、多镜头的叙事内容，而后者正是讲故事的核心所在。我们通过 HoloCine 模型弥补这一“叙事鸿沟”，该模型能够整体生成整个场景，从而从第一个镜头到最后一个镜头确保全局一致性。我们的架构通过一种窗口交叉注意力机制实现了精确的导演控制，该机制能够将文本提示定位到特定镜头，同时稀疏镜头间自注意力模式（在镜头内部密集，镜头之间稀疏）确保了分钟级生成所需的效率。除了在叙事连贯性方面设立了新的最先进水平外，HoloCine 还展现出显著的涌现能力：对角色和场景的持久记忆，以及对电影技巧的直观理解。我们的工作标志着从片段合成向自动电影制作的重要转变，使端到端的电影创作成为触手可及的未来。我们的代码可在以下网址获取：https://holo-cine.github.io/。",
        "translated_title": "HoloCine：电影多镜头长视频的全面生成",
        "label": [],
        "label_reason": "不属于low-level图像处理，关注视频叙事生成。",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出Window Cross-Attention等新机制，提升视频叙事生成能力。"
    },
    {
        "title": "LayerComposer: Interactive Personalized T2I via Spatially-Aware Layered\n  Canvas",
        "url": "http://arxiv.org/abs/2510.20820v1",
        "pub_date": "2025-10-23",
        "summary": "Despite their impressive visual fidelity, existing personalized generative models lack interactive control over spatial composition and scale poorly to multiple subjects. To address these limitations, we present LayerComposer, an interactive framework for personalized, multi-subject text-to-image generation. Our approach introduces two main contributions: (1) a layered canvas, a novel representation in which each subject is placed on a distinct layer, enabling occlusion-free composition; and (2) a locking mechanism that preserves selected layers with high fidelity while allowing the remaining layers to adapt flexibly to the surrounding context. Similar to professional image-editing software, the proposed layered canvas allows users to place, resize, or lock input subjects through intuitive layer manipulation. Our versatile locking mechanism requires no architectural changes, relying instead on inherent positional embeddings combined with a new complementary data sampling strategy. Extensive experiments demonstrate that LayerComposer achieves superior spatial control and identity preservation compared to the state-of-the-art methods in multi-subject personalized image generation.",
        "translated": "尽管现有个性化生成模型在视觉保真度方面表现出色，但它们在空间布局的交互控制方面仍存在不足，且难以有效扩展到多主体场景。为了解决这些限制，我们提出了 LayerComposer，一个用于个性化、多主体文本到图像生成的交互式框架。我们的方法主要有两个贡献：(1) 一种分层画布，这是一种新颖的表示方式，其中每个主体被放置在不同的图层上，从而实现了无遮挡的合成；以及 (2) 一种锁定机制，该机制在保持所选图层高保真度的同时，允许其余图层灵活适应周围上下文。类似于专业的图像编辑软件，我们提出的分层画布允许用户通过直观的图层操作来放置、调整或锁定输入主体。我们灵活的锁定机制无需对模型结构进行修改，而是依赖于固有的位置嵌入，并结合一种新的互补数据采样策略。大量实验表明，与当前最先进的多主体个性化图像生成方法相比，LayerComposer 在空间控制和身份保持方面具有更优的性能。",
        "translated_title": "LayerComposer: 通过空间感知分层画布实现交互式个性化文本到图像生成",
        "label": [],
        "label_reason": "论文聚焦图像生成而非像素级恢复，不属于低层图像处理",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "引入分层画布和锁定机制，但属于常规创新"
    },
    {
        "title": "Towards General Modality Translation with Contrastive and Predictive\n  Latent Diffusion Bridge",
        "url": "http://arxiv.org/abs/2510.20819v1",
        "pub_date": "2025-10-23",
        "summary": "Recent advances in generative modeling have positioned diffusion models as state-of-the-art tools for sampling from complex data distributions. While these models have shown remarkable success across single-modality domains such as images and audio, extending their capabilities to Modality Translation (MT), translating information across different sensory modalities, remains an open challenge. Existing approaches often rely on restrictive assumptions, including shared dimensionality, Gaussian source priors, and modality-specific architectures, which limit their generality and theoretical grounding. In this work, we propose the Latent Denoising Diffusion Bridge Model (LDDBM), a general-purpose framework for modality translation based on a latent-variable extension of Denoising Diffusion Bridge Models. By operating in a shared latent space, our method learns a bridge between arbitrary modalities without requiring aligned dimensions. We introduce a contrastive alignment loss to enforce semantic consistency between paired samples and design a domain-agnostic encoder-decoder architecture tailored for noise prediction in latent space. Additionally, we propose a predictive loss to guide training toward accurate cross-domain translation and explore several training strategies to improve stability. Our approach supports arbitrary modality pairs and performs strongly on diverse MT tasks, including multi-view to 3D shape generation, image super-resolution, and multi-view scene synthesis. Comprehensive experiments and ablations validate the effectiveness of our framework, establishing a new strong baseline in general modality translation. For more information, see our project page: https://sites.google.com/view/lddbm/home.",
        "translated": "最近，生成模型方面的进展将扩散模型确立为从复杂数据分布中采样的先进工具。尽管这些模型在单模态领域（如图像和音频）中取得了显著成功，但将其能力扩展到模态转换（Modality Translation, MT），即在不同感官模态之间进行信息转换，仍然是一个开放性的挑战。现有方法通常依赖于一些限制性假设，包括共享维度、高斯先验源分布和模态特定的架构，这些假设限制了其通用性和理论基础。在本文中，我们提出了一个名为 Latent Denoising Diffusion Bridge Model (LDDBM) 的通用框架，用于模态转换。该方法基于去噪扩散桥模型的潜在变量扩展。通过在共享的潜在空间中操作，我们的方法能够学习任意模态之间的桥梁，而无需对齐维度。我们引入了一个对比对齐损失，以强制成对样本之间的语义一致性，并设计了一个模态无关的编码器-解码器架构，专门用于潜在空间中的噪声预测。此外，我们提出了一种预测损失，以引导训练过程实现准确的跨域转换，并探讨了若干训练策略以提高稳定性。我们的方法支持任意模态对，并在多种MT任务中表现出色，包括多视图到3D形状生成、图像超分辨率和多视图场景合成。全面的实验和消融研究验证了我们框架的有效性，为通用模态转换建立了一个新的强基线。更多信息请访问我们的项目页面：https://sites.google.com/view/lddbm/home。",
        "translated_title": "面向具有对比与预测能力的通用模态转换的潜在扩散桥",
        "label": [
            "超分辨率",
            "图像恢复"
        ],
        "label_reason": "论文涉及图像超分辨率和多模态翻译，属于图像恢复任务",
        "relevance_score": 8,
        "novelty_score": 7,
        "novelty_reason": "提出基于扩散模型的通用多模态翻译框架，有一定改进"
    },
    {
        "title": "GSWorld: Closed-Loop Photo-Realistic Simulation Suite for Robotic\n  Manipulation",
        "url": "http://arxiv.org/abs/2510.20813v1",
        "pub_date": "2025-10-23",
        "summary": "This paper presents GSWorld, a robust, photo-realistic simulator for robotics manipulation that combines 3D Gaussian Splatting with physics engines. Our framework advocates \"closing the loop\" of developing manipulation policies with reproducible evaluation of policies learned from real-robot data and sim2real policy training without using real robots. To enable photo-realistic rendering of diverse scenes, we propose a new asset format, which we term GSDF (Gaussian Scene Description File), that infuses Gaussian-on-Mesh representation with robot URDF and other objects. With a streamlined reconstruction pipeline, we curate a database of GSDF that contains 3 robot embodiments for single-arm and bimanual manipulation, as well as more than 40 objects. Combining GSDF with physics engines, we demonstrate several immediate interesting applications: (1) learning zero-shot sim2real pixel-to-action manipulation policy with photo-realistic rendering, (2) automated high-quality DAgger data collection for adapting policies to deployment environments, (3) reproducible benchmarking of real-robot manipulation policies in simulation, (4) simulation data collection by virtual teleoperation, and (5) zero-shot sim2real visual reinforcement learning. Website: https://3dgsworld.github.io/.",
        "translated": "本文提出了 GSWorld，一个结合了 3D 高斯泼溅（Gaussian Splatting）和物理引擎的鲁棒、逼真的机器人操作模拟器。我们的框架提倡在不使用真实机器人的情况下，通过使用真实机器人数据学习的策略进行可复现的评估和 sim2real 策略训练，从而实现操作策略开发的“闭环”。为了实现多样化场景的逼真渲染，我们提出了一种新的资产格式，称为 GSDF（Gaussian Scene Description File），它将网格上的高斯表示与机器人 URDF 及其他对象相结合。通过一个简化重建流程，我们整理了一个 GSDF 数据库，其中包含 3 种用于单臂和双臂操作的机器人形态，以及 40 多个对象。将 GSDF 与物理引擎结合，我们展示了几个立竿见影的有趣应用：(1) 利用逼真渲染学习零样本 sim2real 像素到动作的操作策略；(2) 通过自动高质量 DAgger 数据收集，使策略适应部署环境；(3) 在模拟中对真实机器人操作策略进行可复现的基准测试；(4) 通过虚拟遥操作进行模拟数据收集；(5) 零样本 sim2real 视觉强化学习。网站：https://3dgsworld.github.io/。",
        "translated_title": "GSWorld：用于机器人操作的闭环照片级真实感仿真套件",
        "label": [],
        "label_reason": "主要关注机器人仿真与策略学习，非图像像素级处理任务",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出了新的资产格式GSDF和基于3D高斯溅射的仿真系统"
    },
    {
        "title": "SpectraMorph: Structured Latent Learning for Self-Supervised\n  Hyperspectral Super-Resolution",
        "url": "http://arxiv.org/abs/2510.20814v1",
        "pub_date": "2025-10-23",
        "summary": "Hyperspectral sensors capture dense spectra per pixel but suffer from low spatial resolution, causing blurred boundaries and mixed-pixel effects. Co-registered companion sensors such as multispectral, RGB, or panchromatic cameras provide high-resolution spatial detail, motivating hyperspectral super-resolution through the fusion of hyperspectral and multispectral images (HSI-MSI). Existing deep learning based methods achieve strong performance but rely on opaque regressors that lack interpretability and often fail when the MSI has very few bands. We propose SpectraMorph, a physics-guided self-supervised fusion framework with a structured latent space. Instead of direct regression, SpectraMorph enforces an unmixing bottleneck: endmember signatures are extracted from the low-resolution HSI, and a compact multilayer perceptron predicts abundance-like maps from the MSI. Spectra are reconstructed by linear mixing, with training performed in a self-supervised manner via the MSI sensor's spectral response function. SpectraMorph produces interpretable intermediates, trains in under a minute, and remains robust even with a single-band (pan-chromatic) MSI. Experiments on synthetic and real-world datasets show SpectraMorph consistently outperforming state-of-the-art unsupervised/self-supervised baselines while remaining very competitive against supervised baselines.",
        "translated": "高光谱传感器在每个像素上捕获密集的光谱信息，但存在空间分辨率低的问题，导致边界模糊和混合像素效应。共注册的辅助传感器，如多光谱、RGB 或全色相机，提供了高分辨率的空间细节，这促使通过高光谱和多光谱图像（HSI-MSI）的融合来实现高光谱超分辨率。现有的基于深度学习的方法在性能上表现出色，但依赖于缺乏可解释性的黑箱回归模型，且在多光谱图像（MSI）波段非常少时常常失效。我们提出 SpectraMorph，一种基于物理引导的自监督融合框架，具有结构化的潜在空间。SpectraMorph 不采用直接回归，而是引入了一种解混瓶颈：端元特征从低分辨率的 HSI 中提取，并通过一个紧凑的多层感知器从 MSI 中预测出类似丰度的图。光谱通过线性混合进行重建，训练过程通过 MSI 传感器的光谱响应函数以自监督方式进行。SpectraMorph 生成可解释的中间结果，训练时间不到一分钟，并且即使在单波段（全色）MSI 的情况下仍具有鲁棒性。在合成和真实世界数据集上的实验表明，SpectraMorph 持续优于最先进的无监督/自监督基线方法，并在与有监督基线方法的比较中保持非常竞争力。",
        "translated_title": "SpectraMorph：用于自监督高光谱超分辨率的结构化潜在学习",
        "label": [
            "超分辨率",
            "遥感图像复原"
        ],
        "label_reason": "论文聚焦于高光谱图像的超分辨率，属于 low-level 图像处理任务。",
        "relevance_score": 9,
        "novelty_score": 8,
        "novelty_reason": "提出物理引导的自监督融合框架，改进了现有方法的可解释性和鲁棒性。"
    },
    {
        "title": "Small Drafts, Big Verdict: Information-Intensive Visual Reasoning via\n  Speculation",
        "url": "http://arxiv.org/abs/2510.20812v1",
        "pub_date": "2025-10-23",
        "summary": "Large Vision-Language Models (VLMs) have achieved remarkable progress in multimodal understanding, yet they struggle when reasoning over information-intensive images that densely interleave textual annotations with fine-grained graphical elements. The main challenges lie in precisely localizing critical cues in dense layouts and multi-hop reasoning to integrate dispersed evidence. We propose Speculative Verdict (SV), a training-free framework inspired by speculative decoding that combines multiple lightweight draft experts with a large verdict model. In the draft stage, small VLMs act as draft experts to generate reasoning paths that provide diverse localization candidates; in the verdict stage, a strong VLM synthesizes these paths to produce the final answer, minimizing computational cost while recovering correct answers. To further improve efficiency and accuracy, SV introduces a consensus expert selection mechanism that forwards only high-agreement reasoning paths to the verdict. Empirically, SV achieves consistent gains on challenging information-intensive and high-resolution visual question answering benchmarks, including InfographicVQA, ChartMuseum, ChartQAPro, and HR-Bench 4K. By synthesizing correct insights from multiple partially accurate reasoning paths, SV achieves both error correction and cost-efficiency compared to large proprietary models or training pipelines. Code is available at https://github.com/Tinaliu0123/speculative-verdict",
        "translated": "视觉-语言大模型（VLMs）在多模态理解方面取得了显著进展，但它们在处理信息密集型图像时仍存在困难，特别是在文本注释与细粒度图形元素紧密交织的图像中进行推理。主要挑战在于在密集布局中精确定位关键线索，以及通过多跳推理整合分散的证据。我们提出了一种无训练框架 Speculative Verdict（SV），该框架受推测性解码的启发，结合多个轻量级草稿专家与一个大判决模型。在草稿阶段，小型 VLMs 作为草稿专家生成推理路径，提供多样化的定位候选；在判决阶段，一个强大的 VLM 综合这些路径生成最终答案，在减少计算成本的同时恢复正确答案。为进一步提高效率和准确性，SV 引入了一个共识专家选择机制，仅将高一致性的推理路径转发给判决模型。实验表明，SV 在具有挑战性的信息密集型和高分辨率视觉问答基准（包括 InfographicVQA、ChartMuseum、ChartQAPro 和 HR-Bench 4K）上取得了持续的性能提升。通过从多个部分准确的推理路径中综合正确的见解，SV 在错误校正和计算成本方面均优于专有大模型或训练流程。代码可在 https://github.com/Tinaliu0123/speculative-verdict 获得。",
        "translated_title": "小草图，大判断：通过推测实现信息密集型视觉推理",
        "label": [],
        "label_reason": "论文聚焦多模态推理，非图像像素级恢复或增强",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出轻量专家与主模型结合框架，有一定新意但非根本性创新"
    },
    {
        "title": "Real Deep Research for AI, Robotics and Beyond",
        "url": "http://arxiv.org/abs/2510.20809v1",
        "pub_date": "2025-10-23",
        "summary": "With the rapid growth of research in AI and robotics now producing over 10,000 papers annually it has become increasingly difficult for researchers to stay up to date. Fast evolving trends, the rise of interdisciplinary work, and the need to explore domains beyond one's expertise all contribute to this challenge. To address these issues, we propose a generalizable pipeline capable of systematically analyzing any research area: identifying emerging trends, uncovering cross domain opportunities, and offering concrete starting points for new inquiry. In this work, we present Real Deep Research (RDR) a comprehensive framework applied to the domains of AI and robotics, with a particular focus on foundation models and robotics advancements. We also briefly extend our analysis to other areas of science. The main paper details the construction of the RDR pipeline, while the appendix provides extensive results across each analyzed topic. We hope this work sheds light for researchers working in the field of AI and beyond.",
        "translated": "随着人工智能和机器人领域研究的迅猛发展，目前每年产出的论文已超过10,000篇，研究人员要保持对该领域进展的及时了解变得日益困难。快速变化的研究趋势、跨学科工作的兴起以及探索专业领域之外新方向的必要性，都加剧了这一挑战。为应对这些问题，我们提出了一种通用的分析流程，能够系统地评估任何研究领域：识别新兴趋势、发现跨领域的研究机会，并为新的研究方向提供具体切入点。在本项工作中，我们介绍了Real Deep Research（RDR），这是一个全面的框架，应用于人工智能和机器人领域，特别关注基础模型和机器人技术的发展。我们还简要地将分析扩展到其他科学领域。正文部分详细描述了RDR流程的构建，而附录则提供了对每个研究主题的详尽结果。我们希望本项工作能够为从事人工智能及相关领域研究的学者提供一定的启发和帮助。",
        "translated_title": "Real Deep Research for AI, Robotics and Beyond",
        "label": [],
        "label_reason": "论文聚焦AI与机器人研究综述，不涉及图像像素级处理",
        "relevance_score": 1,
        "novelty_score": 3,
        "novelty_reason": "提出通用研究分析框架，但方法复现性较强，创新性有限"
    },
    {
        "title": "Video Prediction of Dynamic Physical Simulations With Pixel-Space\n  Spatiotemporal Transformers",
        "url": "http://arxiv.org/abs/2510.20807v1",
        "pub_date": "2025-10-23",
        "summary": "Inspired by the performance and scalability of autoregressive large language models (LLMs), transformer-based models have seen recent success in the visual domain. This study investigates a transformer adaptation for video prediction with a simple end-to-end approach, comparing various spatiotemporal self-attention layouts. Focusing on causal modeling of physical simulations over time; a common shortcoming of existing video-generative approaches, we attempt to isolate spatiotemporal reasoning via physical object tracking metrics and unsupervised training on physical simulation datasets. We introduce a simple yet effective pure transformer model for autoregressive video prediction, utilizing continuous pixel-space representations for video prediction. Without the need for complex training strategies or latent feature-learning components, our approach significantly extends the time horizon for physically accurate predictions by up to 50% when compared with existing latent-space approaches, while maintaining comparable performance on common video quality metrics. In addition, we conduct interpretability experiments to identify network regions that encode information useful to perform accurate estimations of PDE simulation parameters via probing models, and find that this generalizes to the estimation of out-of-distribution simulation parameters. This work serves as a platform for further attention-based spatiotemporal modeling of videos via a simple, parameter efficient, and interpretable approach.",
        "translated": "受自回归大语言模型（LLMs）性能和可扩展性的启发，基于Transformer的模型最近在视觉领域取得了显著成功。本研究探讨了一种用于视频预测的Transformer模型适配方法，采用简洁的端到端策略，并比较了多种时空自注意力结构。我们关注时间维度上的因果建模，尤其是在物理模拟中，这是现有视频生成方法的常见短板。为此，我们尝试通过物理对象跟踪指标和在物理模拟数据集上的无监督训练，分离出时空推理能力。我们引入了一种简单而有效的纯Transformer模型，用于自回归视频预测，并利用连续像素空间表示来进行视频预测。无需复杂训练策略或隐空间特征学习组件，与现有隐空间方法相比，我们的方法在实现物理准确预测的时间范围内可提高多达50%，同时在常见的视频质量指标上保持相当的性能。此外，我们还进行了可解释性实验，通过探测模型识别出那些能够编码有效信息、用于准确估计PDE模拟参数的网络区域，并发现该方法可以推广至对分布外（out-of-distribution）模拟参数的估计。这项工作为视频的进一步基于注意力机制的时空建模提供了一个平台，采用了一种简单、参数高效且可解释的方法。",
        "translated_title": "动态物理模拟的视频预测：基于像素空间的时空变换器",
        "label": [
            "多帧/视频图像恢复"
        ],
        "label_reason": "论文涉及视频预测，属于多帧图像恢复任务",
        "relevance_score": 6,
        "novelty_score": 7,
        "novelty_reason": "提出了一种简单有效的纯Transformer模型，改进了物理模拟视频预测"
    },
    {
        "title": "ARGenSeg: Image Segmentation with Autoregressive Image Generation Model",
        "url": "http://arxiv.org/abs/2510.20803v1",
        "pub_date": "2025-10-23",
        "summary": "We propose a novel AutoRegressive Generation-based paradigm for image Segmentation (ARGenSeg), achieving multimodal understanding and pixel-level perception within a unified framework. Prior works integrating image segmentation into multimodal large language models (MLLMs) typically employ either boundary points representation or dedicated segmentation heads. These methods rely on discrete representations or semantic prompts fed into task-specific decoders, which limits the ability of the MLLM to capture fine-grained visual details. To address these challenges, we introduce a segmentation framework for MLLM based on image generation, which naturally produces dense masks for target objects. We leverage MLLM to output visual tokens and detokenize them into images using an universal VQ-VAE, making the segmentation fully dependent on the pixel-level understanding of the MLLM. To reduce inference latency, we employ a next-scale-prediction strategy to generate required visual tokens in parallel. Extensive experiments demonstrate that our method surpasses prior state-of-the-art approaches on multiple segmentation datasets with a remarkable boost in inference speed, while maintaining strong understanding capabilities.",
        "translated": "我们提出了一种基于自回归生成的图像分割新范式（ARGenSeg），在统一框架下实现多模态理解和像素级感知。此前将图像分割整合进多模态大语言模型（MLLMs）的研究通常采用边界点表示或专用分割头部。这些方法依赖于离散表示或语义提示输入到任务特定的解码器中，这限制了 MLLM 捕捉细粒度视觉细节的能力。为了解决这些问题，我们基于图像生成引入了一种适用于 MLLM 的分割框架，该框架能够自然生成目标对象的密集掩膜。我们利用 MLLM 输出视觉 token，并使用一个通用的 VQ-VAE 将其解码为图像，从而使分割完全依赖于 MLLM 的像素级理解能力。为了降低推理延迟，我们采用下一尺度预测策略，以并行生成所需的视觉 token。广泛的实验表明，我们的方法在多个分割数据集上超越了以往最先进的方法，在推理速度上有显著提升，同时保持了强大的理解能力。",
        "translated_title": "ARGenSeg：基于自回归图像生成模型的图像分割",
        "label": [],
        "label_reason": "论文聚焦于图像分割，属于high-level任务。",
        "relevance_score": 3,
        "novelty_score": 7,
        "novelty_reason": "提出基于自回归图像生成的新分割范式，具有显著创新。"
    },
    {
        "title": "Compress to Impress: Efficient LLM Adaptation Using a Single Gradient\n  Step on 100 Samples",
        "url": "http://arxiv.org/abs/2510.20800v1",
        "pub_date": "2025-10-23",
        "summary": "Recently, Sharma et al. suggested a method called Layer-SElective-Rank reduction (LASER) which demonstrated that pruning high-order components of carefully chosen LLM's weight matrices can boost downstream accuracy -- without any gradient-based fine-tuning. Yet LASER's exhaustive, per-matrix search (each requiring full-dataset forward passes) makes it impractical for rapid deployment. We demonstrate that this overhead can be removed and find that: (i) Only a small, carefully chosen subset of matrices needs to be inspected -- eliminating the layer-by-layer sweep, (ii) The gradient of each matrix's singular values pinpoints which matrices merit reduction, (iii) Increasing the factorization search space by allowing matrices rows to cluster around multiple subspaces and then decomposing each cluster separately further reduces overfitting on the original training data and further lifts accuracy by up to 24.6 percentage points, and finally, (iv) we discover that evaluating on just 100 samples rather than the full training data -- both for computing the indicative gradients and for measuring the final accuracy -- suffices to further reduce the search time; we explain that as adaptation to downstream tasks is dominated by prompting style, not dataset size. As a result, we show that combining these findings yields a fast and robust adaptation algorithm for downstream tasks. Overall, with a single gradient step on 100 examples and a quick scan of the top candidate layers and factorization techniques, we can adapt LLMs to new datasets -- entirely without fine-tuning.",
        "translated": "最近，Sharma 等人提出了一种称为 Layer-SElective-Rank reduction（LASER）的方法，该方法表明，通过剪枝精心选择的大型语言模型（LLM）权重矩阵的高阶成分，可以在不进行任何基于梯度的微调的情况下提升下游任务的准确率。然而，LASER 的穷举式、每个矩阵的搜索过程（每次都需要在整个数据集上进行前向传播）使其难以在快速部署中使用。我们证明可以去除这种开销，并发现以下几点：(i) 只需检查一小部分精心选择的矩阵即可，从而消除了逐层扫描的需要；(ii) 每个矩阵奇异值的梯度能够准确指出哪些矩阵值得进行降秩处理；(iii) 通过允许矩阵的行围绕多个子空间聚类，并对每个聚类分别进行分解，从而扩大了分解的搜索空间，进一步减少了对原始训练数据的过拟合并将准确率提升高达 24.6 个百分点；最后，(iv) 我们发现仅使用 100 个样本而不是完整训练数据进行评估（包括计算指示性梯度和最终准确率）就足以进一步减少搜索时间；我们解释说，这是由于对下游任务的适应主要由提示风格主导，而非数据集的大小。因此，我们展示了结合这些发现可以得到一种快速且鲁棒的下游任务适应算法。总体而言，只需在 100 个示例上进行一次梯度更新，并快速扫描候选的顶层和分解技术，我们即可将 LLM 适配到新数据集上——且完全无需微调。",
        "translated_title": "Compress to Impress: Efficient LLM Adaptation Using a Single Gradient  \nStep on 100 Samples  \n令人印象深刻：基于100个样本的一个梯度步骤的高效大语言模型适应  \n\nRecent advancements in large language models (LLMs) have demonstrated remarkable performance on a variety of tasks, including few-shot learning. However, adapting LLMs to new tasks typically requires extensive computation and large-scale datasets, which hinders their practical deployment in resource-constrained environments. In this work, we present a novel and efficient adaptation method that enables impressive performance using just a single gradient step on a small set of 100 samples. Our approach leverages a carefully designed prompt tuning strategy combined with model compression techniques to minimize the computational burden while maintaining high accuracy. We evaluate our method on several benchmark tasks and show that it achieves competitive results compared to more resource-intensive fine-tuning approaches. This work provides a promising direction for making LLMs more accessible and deployable in real-world applications with limited computational resources.  \n近期，大型语言模型（LLMs）在多种任务上展示了卓越的性能，包括小样本学习。然而，通常将 LLM 适应到新任务需要大量计算和大规模数据集，这限制了它们在资源受限环境中的实际部署。在本研究中，我们提出了一种新颖且高效的方法，该方法仅需在一个包含100个样本的小集合上进行一次梯度步骤，即可实现令人印象深刻的性能。我们的方法结合了精心设计的提示调优策略与模型压缩技术，在保持高准确率的同时最小化计算负担。我们在多个基准任务上评估了该方法，并展示了其与资源密集型微调方法相比具有竞争力的结果。这项工作为在计算资源有限的现实应用中使 LLM 更加易用和可部署提供了有前景的方向。",
        "label": [],
        "label_reason": "论文聚焦于大模型压缩与适应，不涉及图像像素级处理",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "提出了一些实用改进，但整体属于常规模型优化方法"
    },
    {
        "title": "Radar-Camera Fused Multi-Object Tracking: Online Calibration and Common\n  Feature",
        "url": "http://arxiv.org/abs/2510.20794v1",
        "pub_date": "2025-10-23",
        "summary": "This paper presents a Multi-Object Tracking (MOT) framework that fuses radar and camera data to enhance tracking efficiency while minimizing manual interventions. Contrary to many studies that underutilize radar and assign it a supplementary role--despite its capability to provide accurate range/depth information of targets in a world 3D coordinate system--our approach positions radar in a crucial role. Meanwhile, this paper utilizes common features to enable online calibration to autonomously associate detections from radar and camera. The main contributions of this work include: (1) the development of a radar-camera fusion MOT framework that exploits online radar-camera calibration to simplify the integration of detection results from these two sensors, (2) the utilization of common features between radar and camera data to accurately derive real-world positions of detected objects, and (3) the adoption of feature matching and category-consistency checking to surpass the limitations of mere position matching in enhancing sensor association accuracy. To the best of our knowledge, we are the first to investigate the integration of radar-camera common features and their use in online calibration for achieving MOT. The efficacy of our framework is demonstrated by its ability to streamline the radar-camera mapping process and improve tracking precision, as evidenced by real-world experiments conducted in both controlled environments and actual traffic scenarios. Code is available at https://github.com/radar-lab/Radar_Camera_MOT",
        "translated": "本文提出了一种融合雷达与相机数据的多目标跟踪（MOT）框架，旨在提升跟踪效率并尽量减少人工干预。与许多研究未能充分利用雷达能力，仅将其作为辅助传感器使用（尽管其能够在世界三维坐标系中提供目标的精确距离/深度信息）不同，本文方法将雷达置于关键角色。同时，本文利用雷达与相机数据之间的共有特征，实现在线标定以自主关联来自这两个传感器的检测结果。本文的主要贡献包括：(1) 开发了一种雷达-相机融合的MOT框架，该框架利用在线雷达-相机标定以简化来自这两种传感器的检测结果的集成过程；(2) 利用雷达与相机数据之间的共有特征，以准确推导检测对象在真实世界中的位置；(3) 采用特征匹配与类别一致性检查方法，以克服单纯基于位置匹配的局限性，从而提高传感器关联的准确性。据我们所知，这是首次对雷达与相机共有特征的融合及其在在线标定中用于实现MOT的研究进行探索。通过在受控环境和实际交通场景中进行的实验证明，本文框架能够有效简化雷达-相机映射流程并提升跟踪精度。代码可在 https://github.com/radar-lab/Radar_Camera_MOT 获得。",
        "translated_title": "雷达-相机融合的多目标跟踪：在线标定与公共特征",
        "label": [],
        "label_reason": "论文主要涉及传感器融合与多目标跟踪，不属于图像像素级恢复或增强任务。",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "首次尝试雷达-相机共性特征用于在线校准，有一定新颖性但属于系统级方法创新。"
    },
    {
        "title": "CUPID: Pose-Grounded Generative 3D Reconstruction from a Single Image",
        "url": "http://arxiv.org/abs/2510.20776v1",
        "pub_date": "2025-10-23",
        "summary": "This work proposes a new generation-based 3D reconstruction method, named Cupid, that accurately infers the camera pose, 3D shape, and texture of an object from a single 2D image. Cupid casts 3D reconstruction as a conditional sampling process from a learned distribution of 3D objects, and it jointly generates voxels and pixel-voxel correspondences, enabling robust pose and shape estimation under a unified generative framework. By representing both input camera poses and 3D shape as a distribution in a shared 3D latent space, Cupid adopts a two-stage flow matching pipeline: (1) a coarse stage that produces initial 3D geometry with associated 2D projections for pose recovery; and (2) a refinement stage that integrates pose-aligned image features to enhance structural fidelity and appearance details. Extensive experiments demonstrate Cupid outperforms leading 3D reconstruction methods with an over 3 dB PSNR gain and an over 10% Chamfer Distance reduction, while matching monocular estimators on pose accuracy and delivering superior visual fidelity over baseline 3D generative models. For an immersive view of the 3D results generated by Cupid, please visit cupid3d.github.io.",
        "translated": "本文提出了一种新的基于生成的三维重建方法，名为 Cupid，该方法能够从单张二维图像中准确推断出物体的相机姿态、三维形状和纹理。Cupid 将三维重建建模为从学习到的三维物体分布中进行条件采样的过程，并同时生成体素和像素-体素对应关系，从而在统一的生成框架下实现稳健的姿态和形状估计。通过将输入的相机姿态和三维形状表示为共享三维潜在空间中的分布，Cupid 采用了一个两阶段的流匹配流程：(1) 粗略阶段生成初始的三维几何结构及其对应的二维投影以进行姿态恢复；以及 (2) 细化阶段，将姿态对齐的图像特征整合进来，以增强结构保真度和外观细节。广泛的实验表明，Cupid 相较于领先的三维重建方法在 PSNR 指标上提升了超过 3 dB，并在 Chamfer Distance 上减少了超过 10%，同时在姿态准确性上与单目估计器相当，并在视觉保真度方面优于基线三维生成模型。如需沉浸式查看 Cupid 生成的三维重建结果，请访问 cupid3d.github.io。",
        "translated_title": "CUPID：基于姿态的单图像生成式三维重建",
        "label": [],
        "label_reason": "论文聚焦于3D重建，不直接处理像素级图像质量",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出了基于生成模型的3D重建方法，具有一定的创新性"
    },
    {
        "title": "AlphaFlow: Understanding and Improving MeanFlow Models",
        "url": "http://arxiv.org/abs/2510.20771v1",
        "pub_date": "2025-10-23",
        "summary": "MeanFlow has recently emerged as a powerful framework for few-step generative modeling trained from scratch, but its success is not yet fully understood. In this work, we show that the MeanFlow objective naturally decomposes into two parts: trajectory flow matching and trajectory consistency. Through gradient analysis, we find that these terms are strongly negatively correlated, causing optimization conflict and slow convergence. Motivated by these insights, we introduce $\\alpha$-Flow, a broad family of objectives that unifies trajectory flow matching, Shortcut Model, and MeanFlow under one formulation. By adopting a curriculum strategy that smoothly anneals from trajectory flow matching to MeanFlow, $\\alpha$-Flow disentangles the conflicting objectives, and achieves better convergence. When trained from scratch on class-conditional ImageNet-1K 256x256 with vanilla DiT backbones, $\\alpha$-Flow consistently outperforms MeanFlow across scales and settings. Our largest $\\alpha$-Flow-XL/2+ model achieves new state-of-the-art results using vanilla DiT backbones, with FID scores of 2.58 (1-NFE) and 2.15 (2-NFE).",
        "translated": "MeanFlow 最近作为一种从头训练的少步生成建模强大框架而出现，但其成功尚未被完全理解。在本工作中，我们表明 MeanFlow 的目标自然地分解为两部分：轨迹流匹配（trajectory flow matching）和轨迹一致性（trajectory consistency）。通过梯度分析，我们发现这些项之间存在强烈的负相关性，导致优化冲突和收敛缓慢。受这些洞察的启发，我们引入了 $\\alpha$-Flow，这是一类广泛的目标函数，将轨迹流匹配、Shortcut Model 和 MeanFlow 统一在一种公式下。通过采用从轨迹流匹配平滑退火到 MeanFlow 的课程学习策略，$\\alpha$-Flow 解耦了这些冲突的目标，并实现了更好的收敛性。在使用普通 DiT 架构从头训练的类别条件 ImageNet-1K 256x256 数据集上，$\\alpha$-Flow 在各种尺度和设置下均一致优于 MeanFlow。我们最大的 $\\alpha$-Flow-XL/2+ 模型在使用普通 DiT 架构的情况下达到了新的最先进结果，其 FID 分数分别为 2.58（1-NFE）和 2.15（2-NFE）。",
        "translated_title": "AlphaFlow: 理解与改进 MeanFlow 模型",
        "label": [],
        "label_reason": "论文聚焦生成模型优化，不属于像素级图像处理任务",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出了α-Flow新目标框架并改进收敛性"
    },
    {
        "title": "DyPE: Dynamic Position Extrapolation for Ultra High Resolution Diffusion",
        "url": "http://arxiv.org/abs/2510.20766v1",
        "pub_date": "2025-10-23",
        "summary": "Diffusion Transformer models can generate images with remarkable fidelity and detail, yet training them at ultra-high resolutions remains extremely costly due to the self-attention mechanism's quadratic scaling with the number of image tokens. In this paper, we introduce Dynamic Position Extrapolation (DyPE), a novel, training-free method that enables pre-trained diffusion transformers to synthesize images at resolutions far beyond their training data, with no additional sampling cost. DyPE takes advantage of the spectral progression inherent to the diffusion process, where low-frequency structures converge early, while high-frequencies take more steps to resolve. Specifically, DyPE dynamically adjusts the model's positional encoding at each diffusion step, matching their frequency spectrum with the current stage of the generative process. This approach allows us to generate images at resolutions that exceed the training resolution dramatically, e.g., 16 million pixels using FLUX. On multiple benchmarks, DyPE consistently improves performance and achieves state-of-the-art fidelity in ultra-high-resolution image generation, with gains becoming even more pronounced at higher resolutions. Project page is available at https://noamissachar.github.io/DyPE/.",
        "translated": "扩散 Transformer 模型可以生成具有显著保真度和细节的图像，但由于自注意力机制的复杂度与图像 token 数量呈二次方关系，因此在超高清分辨率下训练这些模型仍然非常昂贵。在本文中，我们提出了一种新的、无需训练的方法——动态位置外推（Dynamic Position Extrapolation，DyPE），使得预训练的扩散 Transformer 能够在不增加额外采样成本的前提下，合成远超其训练数据分辨率的图像。DyPE 利用了扩散过程中固有的频谱渐进特性：低频结构在早期扩散阶段快速收敛，而高频信息则需要更多步骤才能还原。具体而言，DyPE 在每个扩散步骤中动态调整模型的位置编码，使其与当前生成过程的频率谱相匹配。该方法使得我们可以生成远高于训练分辨率的图像，例如使用 FLUX 模型生成 1600 万像素的图像。在多个基准测试中，DyPE 一致提升了性能，并在超高清图像生成中实现了最先进的保真度，且分辨率越高，性能提升越明显。项目页面请访问 https://noamissachar.github.io/DyPE/。",
        "translated_title": "DyPE：用于超分辨率扩散的动态位置外推",
        "label": [],
        "label_reason": "论文聚焦图像生成而非恢复或增强",
        "relevance_score": 3,
        "novelty_score": 8,
        "novelty_reason": "提出训练免费的动态位置外推方法，显著提升生成分辨率"
    },
    {
        "title": "MEIcoder: Decoding Visual Stimuli from Neural Activity by Leveraging\n  Most Exciting Inputs",
        "url": "http://arxiv.org/abs/2510.20762v1",
        "pub_date": "2025-10-23",
        "summary": "Decoding visual stimuli from neural population activity is crucial for understanding the brain and for applications in brain-machine interfaces. However, such biological data is often scarce, particularly in primates or humans, where high-throughput recording techniques, such as two-photon imaging, remain challenging or impossible to apply. This, in turn, poses a challenge for deep learning decoding techniques. To overcome this, we introduce MEIcoder, a biologically informed decoding method that leverages neuron-specific most exciting inputs (MEIs), a structural similarity index measure loss, and adversarial training. MEIcoder achieves state-of-the-art performance in reconstructing visual stimuli from single-cell activity in primary visual cortex (V1), especially excelling on small datasets with fewer recorded neurons. Using ablation studies, we demonstrate that MEIs are the main drivers of the performance, and in scaling experiments, we show that MEIcoder can reconstruct high-fidelity natural-looking images from as few as 1,000-2,500 neurons and less than 1,000 training data points. We also propose a unified benchmark with over 160,000 samples to foster future research. Our results demonstrate the feasibility of reliable decoding in early visual system and provide practical insights for neuroscience and neuroengineering applications.",
        "translated": "从神经群体活动中解码视觉刺激对于理解大脑以及脑机接口的应用至关重要。然而，这类生物数据往往十分稀缺，特别是在灵长类动物或人类中，由于双光子成像等高通量记录技术难以实施或无法应用，使得数据获取极具挑战性。这反过来也对深度学习解码方法提出了挑战。为了解决这一问题，我们引入了 MEIcoder，一种受生物学启发的解码方法，该方法利用神经元特定的最兴奋输入（MEIs）、结构相似性指数度量损失函数以及对抗训练。MEIcoder 在从初级视觉皮层（V1）单细胞活动中重建视觉刺激方面达到了最先进的性能，尤其在记录神经元数量较少的小型数据集上表现突出。通过消融实验，我们证明 MEIs 是性能提升的主要驱动因素；在扩展实验中，我们展示了 MEIcoder 可以从仅 1,000 至 2,500 个神经元和少于 1,000 个训练样本中重建出高保真、自然的图像。我们还提出一个包含超过 160,000 个样本的统一基准，以促进未来的研究。我们的结果证明了在早期视觉系统中进行可靠解码的可行性，并为神经科学和神经工程应用提供了实用的见解。",
        "translated_title": "MEIcoder：通过利用最激动输入从神经活动中解码视觉刺激",
        "label": [],
        "label_reason": "不属于low-level图像处理，目标是解码神经活动而非恢复图像质量",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "结合MEIs和对抗训练进行神经解码，有一定技术迁移创新"
    },
    {
        "title": "ACS-SegNet: An Attention-Based CNN-SegFormer Segmentation Network for\n  Tissue Segmentation in Histopathology",
        "url": "http://arxiv.org/abs/2510.20754v1",
        "pub_date": "2025-10-23",
        "summary": "Automated histopathological image analysis plays a vital role in computer-aided diagnosis of various diseases. Among developed algorithms, deep learning-based approaches have demonstrated excellent performance in multiple tasks, including semantic tissue segmentation in histological images. In this study, we propose a novel approach based on attention-driven feature fusion of convolutional neural networks (CNNs) and vision transformers (ViTs) within a unified dual-encoder model to improve semantic segmentation performance. Evaluation on two publicly available datasets showed that our model achieved {\\mu}IoU/{\\mu}Dice scores of 76.79%/86.87% on the GCPS dataset and 64.93%/76.60% on the PUMA dataset, outperforming state-of-the-art and baseline benchmarks. The implementation of our method is publicly available in a GitHub repository: https://github.com/NimaTorbati/ACS-SegNet",
        "translated": "自动化组织病理学图像分析在各种疾病计算机辅助诊断中发挥着关键作用。在已开发的算法中，基于深度学习的方法已在多个任务中表现出优异的性能，包括组织学图像中的语义组织分割。在本研究中，我们提出了一种新颖的方法，通过在统一的双编码器模型中融合卷积神经网络（CNNs）和视觉变换器（ViTs）的注意力驱动特征，以提升语义分割性能。在两个公开可用的数据集上的评估表明，我们的模型在 GCPS 数据集上实现了 76.79% 的 {\\mu}IoU 和 86.87% 的 {\\mu}Dice 分数，在 PUMA 数据集上实现了 64.93% 的 {\\mu}IoU 和 76.60% 的 {\\mu}Dice 分数，优于当前最先进的方法和基线基准。我们的方法实现已公开发布在 GitHub 仓库中：https://github.com/NimaTorbati/ACS-SegNet",
        "translated_title": "ACS-SegNet：一种基于注意力机制的CNN-SegFormer分割网络，用于组织病理学中的组织分割",
        "label": [],
        "label_reason": "论文主要解决语义分割问题，属于high-level任务",
        "relevance_score": 2,
        "novelty_score": 5,
        "novelty_reason": "方法为CNN与SegFormer的常规组合，创新性有限"
    },
    {
        "title": "AutoScape: Geometry-Consistent Long-Horizon Scene Generation",
        "url": "http://arxiv.org/abs/2510.20726v1",
        "pub_date": "2025-10-23",
        "summary": "This paper proposes AutoScape, a long-horizon driving scene generation framework. At its core is a novel RGB-D diffusion model that iteratively generates sparse, geometrically consistent keyframes, serving as reliable anchors for the scene's appearance and geometry. To maintain long-range geometric consistency, the model 1) jointly handles image and depth in a shared latent space, 2) explicitly conditions on the existing scene geometry (i.e., rendered point clouds) from previously generated keyframes, and 3) steers the sampling process with a warp-consistent guidance. Given high-quality RGB-D keyframes, a video diffusion model then interpolates between them to produce dense and coherent video frames. AutoScape generates realistic and geometrically consistent driving videos of over 20 seconds, improving the long-horizon FID and FVD scores over the prior state-of-the-art by 48.6\\% and 43.0\\%, respectively.",
        "translated": "本文提出 AutoScape，一种长视野驾驶场景生成框架。其核心是一个新颖的 RGB-D 扩散模型，该模型迭代生成稀疏但几何一致性良好的关键帧，作为场景外观和几何结构的可靠锚点。为了保持长距离的几何一致性，该模型 1）在共享的潜在空间中联合处理图像和深度信息，2）显式地基于之前生成的关键帧中的场景几何结构（即渲染的点云）进行条件建模，3）通过具有变形一致性的引导控制采样过程。在获得高质量的 RGB-D 关键帧后，一个视频扩散模型进一步在它们之间进行插值，以生成密集且连贯的视频帧。AutoScape 能够生成超过 20 秒、真实且几何一致的驾驶视频，在长视野 FID 和 FVD 评分上分别比现有最先进方法提升了 48.6% 和 43.0%。",
        "translated_title": "AutoScape：几何一致的长视野场景生成",
        "label": [],
        "label_reason": "论文关注场景生成而非像素级图像质量恢复",
        "relevance_score": 3,
        "novelty_score": 6,
        "novelty_reason": "提出了结合几何一致性的RGB-D扩散模型框架"
    },
    {
        "title": "ALICE-LRI: A General Method for Lossless Range Image Generation for\n  Spinning LiDAR Sensors without Calibration Metadata",
        "url": "http://arxiv.org/abs/2510.20708v1",
        "pub_date": "2025-10-23",
        "summary": "3D LiDAR sensors are essential for autonomous navigation, environmental monitoring, and precision mapping in remote sensing applications. To efficiently process the massive point clouds generated by these sensors, LiDAR data is often projected into 2D range images that organize points by their angular positions and distances. While these range image representations enable efficient processing, conventional projection methods suffer from fundamental geometric inconsistencies that cause irreversible information loss, compromising high-fidelity applications. We present ALICE-LRI (Automatic LiDAR Intrinsic Calibration Estimation for Lossless Range Images), the first general, sensor-agnostic method that achieves lossless range image generation from spinning LiDAR point clouds without requiring manufacturer metadata or calibration files. Our algorithm automatically reverse-engineers the intrinsic geometry of any spinning LiDAR sensor by inferring critical parameters including laser beam configuration, angular distributions, and per-beam calibration corrections, enabling lossless projection and complete point cloud reconstruction with zero point loss. Comprehensive evaluation across the complete KITTI and DurLAR datasets demonstrates that ALICE-LRI achieves perfect point preservation, with zero points lost across all point clouds. Geometric accuracy is maintained well within sensor precision limits, establishing geometric losslessness with real-time performance. We also present a compression case study that validates substantial downstream benefits, demonstrating significant quality improvements in practical applications. This paradigm shift from approximate to lossless LiDAR projections opens new possibilities for high-precision remote sensing applications requiring complete geometric preservation.",
        "translated": "3D激光雷达传感器在遥感应用中的自动驾驶导航、环境监测和精确制图中具有重要作用。为了高效处理这些传感器生成的海量点云数据，通常会将激光雷达数据投影到2D的范围图像（range images）中，这些图像根据点的角度位置和距离来组织点。虽然这些范围图像表示方式能够实现高效处理，但传统投影方法存在基本的几何不一致性，导致不可逆的信息丢失，从而影响高保真度应用的性能。我们提出ALICE-LRI（Automatic LiDAR Intrinsic Calibration Estimation for Lossless Range Images，自动激光雷达内校准估计用于无损范围图像），这是首个通用的、与传感器无关的方法，能够从旋转式激光雷达点云中生成无损范围图像，而无需依赖制造商的元数据或校准文件。我们的算法通过推断关键参数（如激光束配置、角度分布以及每束激光的校准修正）来自动逆向工程任何旋转式激光雷达传感器的内在几何结构，从而实现无损投影，并能完整重建点云，且不丢失任何点。在KITTI和DurLAR完整数据集上的全面评估表明，ALICE-LRI实现了完美的点保留，在所有点云中均无点丢失。几何精度在传感器精度范围内得到了良好保持，实现了几何无损，并具备实时性能。我们还展示了一个压缩案例研究，验证了该方法在后续应用中的显著优势，证明其在实际应用中可带来显著的质量提升。这种从近似投影到无损投影的范式转变，为需要完整几何保真的高精度遥感应用开辟了新的可能性。",
        "translated_title": "ALICE-LRI：一种无需校准元数据的旋转激光雷达传感器无损距离图像生成通用方法",
        "label": [
            "遥感图像复原"
        ],
        "label_reason": "论文提出无损LiDAR图像生成方法，属于遥感图像复原范畴",
        "relevance_score": 8,
        "novelty_score": 9,
        "novelty_reason": "首次实现无需校准元数据的通用无损LiDAR投影方法"
    },
    {
        "title": "Mixing Importance with Diversity: Joint Optimization for KV Cache\n  Compression in Large Vision-Language Models",
        "url": "http://arxiv.org/abs/2510.20707v1",
        "pub_date": "2025-10-23",
        "summary": "Recent large vision-language models (LVLMs) demonstrate remarkable capabilities in processing extended multi-modal sequences, yet the resulting key-value (KV) cache expansion creates a critical memory bottleneck that fundamentally limits deployment scalability. While existing KV cache compression methods focus on retaining high-importance KV pairs to minimize storage, they often overlook the modality-specific semantic redundancy patterns that emerge distinctively in multi-modal KV caches. In this work, we first analyze how, beyond simple importance, the KV cache in LVLMs exhibits varying levels of redundancy across attention heads. We show that relying solely on importance can only cover a subset of the full KV cache information distribution, leading to potential loss of semantic coverage. To address this, we propose \\texttt{MixKV}, a novel method that mixes importance with diversity for optimized KV cache compression in LVLMs. \\texttt{MixKV} adapts to head-wise semantic redundancy, selectively balancing diversity and importance when compressing KV pairs. Extensive experiments demonstrate that \\texttt{MixKV} consistently enhances existing methods across multiple LVLMs. Under extreme compression (budget=64), \\texttt{MixKV} improves baseline methods by an average of \\textbf{5.1\\%} across five multi-modal understanding benchmarks and achieves remarkable gains of \\textbf{8.0\\%} and \\textbf{9.0\\%} for SnapKV and AdaKV on GUI grounding tasks, all while maintaining comparable inference efficiency. Furthermore, \\texttt{MixKV} extends seamlessly to LLMs with comparable performance gains. Our code is available at \\href{https://github.com/xuyang-liu16/MixKV}{\\textcolor{citeblue}{https://github.com/xuyang-liu16/MixKV}}.",
        "translated": "近年来，大型视觉语言模型（LVLMs）在处理扩展的多模态序列方面表现出显著的能力，然而由此产生的键值（KV）缓存扩展造成了关键的内存瓶颈，从根本上限制了其部署的可扩展性。尽管现有的KV缓存压缩方法主要关注保留高重要性的KV对以最小化存储，但它们往往忽略了在多模态KV缓存中显著出现的模态特定语义冗余模式。在本工作中，我们首先分析了除了简单的“重要性”之外，LVLMs中的KV缓存在注意力头之间表现出不同程度的冗余。我们表明，仅依赖重要性只能覆盖完整的KV缓存信息分布的一部分，从而可能导致语义覆盖的损失。为了解决这一问题，我们提出了一种新方法 \\texttt{MixKV}，该方法将重要性与多样性结合，以优化LVLMs中的KV缓存压缩。\\texttt{MixKV} 能够适应每个注意力头的语义冗余，选择性地在压缩KV对时平衡多样性和重要性。广泛的实验表明，\\texttt{MixKV} 在多种LVLMs中始终优于现有方法。在极端压缩条件下（预算=64），\\texttt{MixKV} 在五个多模态理解基准上平均提升了基线方法 \\textbf{5.1\\%}，并在GUI定位任务中对SnapKV和AdaKV分别实现了显著的提升 \\textbf{8.0\\%} 和 \\textbf{9.0\\%}，同时保持了相当的推理效率。此外，\\texttt{MixKV} 可无缝扩展到大型语言模型（LLMs），并具有类似的性能提升。我们的代码可在 \\href{https://github.com/xuyang-liu16/MixKV}{\\textcolor{citeblue}{https://github.com/xuyang-liu16/MixKV}} 获取。",
        "translated_title": "Mixing Importance with Diversity: Joint Optimization for KV Cache Compression in Large Vision-Language Models\n\n将重要性与多样性融合：用于大视觉-语言模型中 KV 缓存压缩的联合优化",
        "label": [],
        "label_reason": "论文关注视觉语言模型的KV缓存压缩，不涉及像素级图像处理任务。",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "提出一种结合重要性与多样性的KV压缩方法，属于常规优化改进。"
    },
    {
        "title": "Diagnosing Visual Reasoning: Challenges, Insights, and a Path Forward",
        "url": "http://arxiv.org/abs/2510.20696v1",
        "pub_date": "2025-10-23",
        "summary": "Multimodal large language models (MLLMs) that integrate visual and textual reasoning leverage chain-of-thought (CoT) prompting to tackle complex visual tasks, yet continue to exhibit visual hallucinations and an over-reliance on textual priors. We present a systematic diagnosis of state-of-the-art vision-language models using a three-stage evaluation framework, uncovering key failure modes. To address these, we propose an agent-based architecture that combines LLM reasoning with lightweight visual modules, enabling fine-grained analysis and iterative refinement of reasoning chains. Our results highlight future visual reasoning models should focus on integrating a broader set of specialized tools for analyzing visual content. Our system achieves significant gains (+10.3 on MMMU, +6.0 on MathVista over a 7B baseline), matching or surpassing much larger models. We will release our framework and evaluation suite to facilitate future research.",
        "translated": "集成视觉与文本推理能力的多模态大语言模型（MLLMs）利用思维链（CoT）提示来处理复杂的视觉任务，但仍然表现出视觉幻觉现象，并且过度依赖文本先验。我们提出了一种三阶段的评估框架，对最先进的视觉语言模型进行了系统诊断，揭示了其关键的失败模式。为了解决这些问题，我们设计了一种基于智能体的架构，将大语言模型的推理能力与轻量级视觉模块相结合，从而实现对推理链的细粒度分析与迭代优化。实验结果表明，未来的视觉推理模型应重点关注将更广泛的专用工具集成进来，以分析视觉内容。我们的系统在基准上取得了显著的性能提升（MMMU 提升 +10.3，MathVista 提升 +6.0），达到了甚至超越了更大规模模型的性能。我们将发布我们的框架和评估套件，以促进未来的研究。",
        "translated_title": "诊断视觉推理：挑战、见解与未来路径",
        "label": [],
        "label_reason": "论文聚焦于视觉推理而非图像像素级处理",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "提出基于代理的架构改进视觉推理"
    },
    {
        "title": "A Data-Centric Approach to Multilingual E-Commerce Product Search: Case\n  Study on Query-Category and Query-Item Relevance",
        "url": "http://arxiv.org/abs/2510.21671v1",
        "pub_date": "2025-10-24",
        "summary": "Multilingual e-commerce search suffers from severe data imbalance across languages, label noise, and limited supervision for low-resource languages--challenges that impede the cross-lingual generalization of relevance models despite the strong capabilities of large language models (LLMs). In this work, we present a practical, architecture-agnostic, data-centric framework to enhance performance on two core tasks: Query-Category (QC) relevance (matching queries to product categories) and Query-Item (QI) relevance (matching queries to product titles). Rather than altering the model, we redesign the training data through three complementary strategies: (1) translation-based augmentation to synthesize examples for languages absent in training, (2) semantic negative sampling to generate hard negatives and mitigate class imbalance, and (3) self-validation filtering to detect and remove likely mislabeled instances. Evaluated on the CIKM AnalytiCup 2025 dataset, our approach consistently yields substantial F1 score improvements over strong LLM baselines, achieving competitive results in the official competition. Our findings demonstrate that systematic data engineering can be as impactful as--and often more deployable than--complex model modifications, offering actionable guidance for building robust multilingual search systems in the real-world e-commerce settings.",
        "translated": "多语言电商搜索面临语言间数据严重不平衡、标签噪声以及低资源语言监督信号不足等挑战，这些问题阻碍了相关性模型在跨语言场景下的泛化能力，即使大型语言模型（LLM）具备强大能力亦然。在本工作中，我们提出了一种实用、架构无关、以数据为中心的框架，旨在提升两个核心任务的性能：查询-类别（Query-Category, QC）相关性（将查询匹配至商品类别）和查询-物料（Query-Item, QI）相关性（将查询匹配至商品标题）。我们并未对模型结构进行修改，而是通过三种互补策略重新设计训练数据：（1）基于翻译的数据增强，为训练数据中缺失的语言合成样本；（2）语义负采样，生成难负例并缓解类别不平衡问题；（3）自验证过滤，检测并移除可能的错误标注样本。在CIKM AnalytiCup 2025数据集上的评估表明，我们的方法在多个强LLM基线模型上持续实现显著的F1分数提升，并在官方竞赛中取得具有竞争力的结果。研究结果表明，系统性的数据工程在提升性能方面可以与复杂的模型修改相媲美，且通常更具可部署性，为构建面向真实电商场景的稳健多语言搜索系统提供了可操作的指导。",
        "translated_title": "以数据为中心的多语言电商产品搜索方法：查询-类别与查询-物料相关性案例研究",
        "label": [
            "Query-Item Relevance",
            "Query-Category Relevance",
            "负采样与对比学习",
            "通用推荐技术"
        ],
        "label_reason": "聚焦多语言电商搜索中查询与商品/类目相关性，涉及负采样与数据增强，属于推荐系统相关任务",
        "relevance_score": 6,
        "novelty_score": 7,
        "novelty_reason": "提出数据为中心的框架，结合翻译增强与自验证过滤，提升低资源语言性能，方法新颖且实用"
    },
    {
        "title": "DeepAgent: A General Reasoning Agent with Scalable Toolsets",
        "url": "http://arxiv.org/abs/2510.21618v1",
        "pub_date": "2025-10-24",
        "summary": "Large reasoning models have demonstrated strong problem-solving abilities, yet real-world tasks often require external tools and long-horizon interactions. Existing agent frameworks typically follow predefined workflows, which limit autonomous and global task completion. In this paper, we introduce DeepAgent, an end-to-end deep reasoning agent that performs autonomous thinking, tool discovery, and action execution within a single, coherent reasoning process. To address the challenges of long-horizon interactions, particularly the context length explosion from multiple tool calls and the accumulation of interaction history, we introduce an autonomous memory folding mechanism that compresses past interactions into structured episodic, working, and tool memories, reducing error accumulation while preserving critical information. To teach general-purpose tool use efficiently and stably, we develop an end-to-end reinforcement learning strategy, namely ToolPO, that leverages LLM-simulated APIs and applies tool-call advantage attribution to assign fine-grained credit to the tool invocation tokens. Extensive experiments on eight benchmarks, including general tool-use tasks (ToolBench, API-Bank, TMDB, Spotify, ToolHop) and downstream applications (ALFWorld, WebShop, GAIA, HLE), demonstrate that DeepAgent consistently outperforms baselines across both labeled-tool and open-set tool retrieval scenarios. This work takes a step toward more general and capable agents for real-world applications. The code and demo are available at https://github.com/RUC-NLPIR/DeepAgent.",
        "translated": "大型推理模型已展现出强大的问题求解能力，然而现实世界任务通常需要外部工具支持以及长跨度交互。现有代理框架通常遵循预定义的工作流，这限制了自主性和全局任务完成能力。本文提出 DeepAgent，一种端到端的深度推理代理，能够在单一、连贯的推理过程中自主进行思考、工具发现与动作执行。为应对长跨度交互带来的挑战，特别是多次工具调用导致的上下文长度爆炸以及交互历史的累积问题，我们引入了一种自主记忆折叠机制，将过往交互压缩为结构化的场景记忆、工作记忆和工具记忆，从而在减少误差累积的同时保留关键信息。为高效且稳定地学习通用工具使用能力，我们开发了一种端到端强化学习策略，即 ToolPO，该策略利用大语言模型模拟的 API，并通过工具调用优势归因方法，对工具调用令牌进行细粒度的奖励分配。在八个基准数据集上的广泛实验，包括通用工具使用任务（ToolBench、API-Bank、TMDB、Spotify、ToolHop）以及下游应用（ALFWorld、WebShop、GAIA、HLE），表明 DeepAgent 在标注工具和开放集工具召回场景下均显著优于基线方法。本工作朝着面向实际应用的更通用、更强能力的代理迈出了重要一步。代码与演示可访问 https://github.com/RUC-NLPIR/DeepAgent。",
        "translated_title": "DeepAgent：一种具备可扩展工具集的通用推理代理",
        "label": [],
        "label_reason": "论文聚焦通用推理智能体与工具调用，未针对推荐系统设计，无直接关联。",
        "relevance_score": 3,
        "novelty_score": 8,
        "novelty_reason": "提出自主记忆折叠与ToolPO强化学习策略，对通用智能体有创新，但非推荐领域。"
    },
    {
        "title": "Doc-Researcher: A Unified System for Multimodal Document Parsing and\n  Deep Research",
        "url": "http://arxiv.org/abs/2510.21603v1",
        "pub_date": "2025-10-24",
        "summary": "Deep Research systems have revolutionized how LLMs solve complex questions through iterative reasoning and evidence gathering. However, current systems remain fundamentally constrained to textual web data, overlooking the vast knowledge embedded in multimodal documents Processing such documents demands sophisticated parsing to preserve visual semantics (figures, tables, charts, and equations), intelligent chunking to maintain structural coherence, and adaptive retrieval across modalities, which are capabilities absent in existing systems. In response, we present Doc-Researcher, a unified system that bridges this gap through three integrated components: (i) deep multimodal parsing that preserves layout structure and visual semantics while creating multi-granular representations from chunk to document level, (ii) systematic retrieval architecture supporting text-only, vision-only, and hybrid paradigms with dynamic granularity selection, and (iii) iterative multi-agent workflows that decompose complex queries, progressively accumulate evidence, and synthesize comprehensive answers across documents and modalities. To enable rigorous evaluation, we introduce M4DocBench, the first benchmark for Multi-modal, Multi-hop, Multi-document, and Multi-turn deep research. Featuring 158 expert-annotated questions with complete evidence chains across 304 documents, M4DocBench tests capabilities that existing benchmarks cannot assess. Experiments demonstrate that Doc-Researcher achieves 50.6% accuracy, 3.4xbetter than state-of-the-art baselines, validating that effective document research requires not just better retrieval, but fundamentally deep parsing that preserve multimodal integrity and support iterative research. Our work establishes a new paradigm for conducting deep research on multimodal document collections.",
        "translated": "深度研究系统通过迭代推理和证据收集，彻底改变了大语言模型（LLM）解决复杂问题的方式。然而，当前系统仍从根本上受限于文本形式的网络数据，忽视了嵌入在多模态文档中的海量知识。处理此类文档需要复杂的解析技术以保留视觉语义（如图表、表格、图表和公式），智能的分块策略以维持结构连贯性，以及跨模态的自适应检索能力，而这些能力在现有系统中均缺失。为此，我们提出Doc-Researcher，一个统一的系统，通过三个集成组件弥合这一差距：（i）深度多模态解析，能够在保持布局结构和视觉语义的同时，从分块到文档级别生成多粒度表示；（ii）系统化的检索架构，支持纯文本、纯视觉以及混合范式，并具备动态粒度选择能力；（iii）迭代多智能体工作流，能够分解复杂查询，逐步积累证据，并在跨文档和跨模态的场景下合成全面的答案。为实现严格评估，我们引入M4DocBench，这是首个用于多模态、多跳、多文档和多轮次深度研究的基准测试。该基准包含158个专家标注的问题，覆盖304个文档的完整证据链，测试了现有基准无法评估的能力。实验表明，Doc-Researcher实现了50.6%的准确率，比现有最先进基线高出3.4倍，验证了有效的文档研究不仅需要更优的检索，更需要从根本上具备保持多模态完整性的深度解析能力，并支持迭代式研究。本工作为在多模态文档集合上开展深度研究建立了新的范式。",
        "translated_title": "Doc-Researcher：一种用于多模态文档解析与深度研究的统一系统",
        "label": [],
        "label_reason": "论文聚焦多模态文档解析与深度研究，核心为信息检索与推理，非推荐系统任务。",
        "relevance_score": 3,
        "novelty_score": 8,
        "novelty_reason": "提出统一多模态解析与检索框架，支持动态粒度选择与多智能体协作，创新性强。"
    },
    {
        "title": "Redefining Retrieval Evaluation in the Era of LLMs",
        "url": "http://arxiv.org/abs/2510.21440v1",
        "pub_date": "2025-10-24",
        "summary": "Traditional Information Retrieval (IR) metrics, such as nDCG, MAP, and MRR, assume that human users sequentially examine documents with diminishing attention to lower ranks. This assumption breaks down in Retrieval Augmented Generation (RAG) systems, where search results are consumed by Large Language Models (LLMs), which, unlike humans, process all retrieved documents as a whole rather than sequentially. Additionally, traditional IR metrics do not account for related but irrelevant documents that actively degrade generation quality, rather than merely being ignored. Due to these two major misalignments, namely human vs. machine position discount and human relevance vs. machine utility, classical IR metrics do not accurately predict RAG performance. We introduce a utility-based annotation schema that quantifies both the positive contribution of relevant passages and the negative impact of distracting ones. Building on this foundation, we propose UDCG (Utility and Distraction-aware Cumulative Gain), a metric using an LLM-oriented positional discount to directly optimize the correlation with the end-to-end answer accuracy. Experiments on five datasets and six LLMs demonstrate that UDCG improves correlation by up to 36% compared to traditional metrics. Our work provides a critical step toward aligning IR evaluation with LLM consumers and enables more reliable assessment of RAG components",
        "translated": "传统信息检索（IR）指标，如 nDCG、MAP 和 MRR，假设人类用户会依次检视文档，并对排名较低的文档注意力逐渐降低。这一假设在检索增强生成（RAG）系统中不再成立，因为在 RAG 中，检索结果由大语言模型（LLM）消费，而 LLM 与人类不同，会整体处理所有检索到的文档，而非逐个顺序处理。此外，传统 IR 指标未考虑那些相关但无关的文档，这些文档会主动降低生成质量，而非仅仅被忽略。由于这两个主要错配——即人类与机器在位置衰减上的差异，以及人类相关性与机器效用之间的差异——经典 IR 指标无法准确预测 RAG 系统的性能。我们提出了一种基于效用的标注方案，用于量化相关段落的正向贡献以及干扰性段落的负向影响。在此基础上，我们提出了 UDCG（Utility and Distraction-aware Cumulative Gain），一种采用面向 LLM 的位置衰减策略的指标，直接优化与端到端答案准确率的相关性。在五个数据集和六种 LLM 上的实验表明，与传统指标相比，UDCG 的相关性提升了最高达 36%。我们的工作为实现 IR 评估与 LLM 消费者的对齐迈出了关键一步，并有助于更可靠地评估 RAG 各组件的性能。",
        "translated_title": "重定义大语言模型时代下的召回评估",
        "label": [
            "召回",
            "LLM生成式推荐",
            "推荐系统评估"
        ],
        "label_reason": "论文聚焦RAG系统中召回评估，提出LLM导向的评估指标，与生成式推荐相关",
        "relevance_score": 7,
        "novelty_score": 8,
        "novelty_reason": "提出UDCG新指标，结合LLM特性优化评估，提升与生成质量相关性"
    },
    {
        "title": "SciNUP: Natural Language User Interest Profiles for Scientific\n  Literature Recommendation",
        "url": "http://arxiv.org/abs/2510.21352v1",
        "pub_date": "2025-10-24",
        "summary": "The use of natural language (NL) user profiles in recommender systems offers greater transparency and user control compared to traditional representations. However, there is scarcity of large-scale, publicly available test collections for evaluating NL profile-based recommendation. To address this gap, we introduce SciNUP, a novel synthetic dataset for scholarly recommendation that leverages authors' publication histories to generate NL profiles and corresponding ground truth items. We use this dataset to conduct a comparison of baseline methods, ranging from sparse and dense retrieval approaches to state-of-the-art LLM-based rerankers. Our results show that while baseline methods achieve comparable performance, they often retrieve different items, indicating complementary behaviors. At the same time, considerable headroom for improvement remains, highlighting the need for effective NL-based recommendation approaches. The SciNUP dataset thus serves as a valuable resource for fostering future research and development in this area.",
        "translated": "在推荐系统中使用自然语言（NL）用户画像相比传统表示方式提供了更高的透明度和用户控制能力。然而，目前缺乏大规模、公开可用的测试集用于评估基于NL画像的推荐方法。为弥补这一空白，我们提出了SciNUP，一个面向学术推荐的新型合成数据集，该数据集利用作者的发表历史生成NL画像及其对应的 ground truth 物料。我们利用该数据集对基线方法进行了对比，涵盖从稀疏和密集检索方法到最先进的大语言模型（LLM）重排器。实验结果表明，尽管基线方法在性能上表现相当，但它们召回的物料往往不同，显示出互补的行为特性。同时，仍存在显著的性能提升空间，凸显了有效NL推荐方法的必要性。因此，SciNUP数据集为推动该领域未来的研究与开发提供了宝贵的资源。",
        "translated_title": "SciNUP：面向科学文献推荐的自然语言用户兴趣画像",
        "label": [
            "召回",
            "重排",
            "LLM生成式推荐",
            "通用推荐技术"
        ],
        "label_reason": "论文构建NL用户画像用于学术推荐，涉及召回与重排，采用LLM等前沿方法，与推荐系统紧密相关。",
        "relevance_score": 8,
        "novelty_score": 7,
        "novelty_reason": "提出合成数据集SciNUP，支持NL画像推荐，方法组合新颖，但未突破核心范式。"
    },
    {
        "title": "Automated Detection of Visual Attribute Reliance with a Self-Reflective\n  Agent",
        "url": "http://arxiv.org/abs/2510.21704v1",
        "pub_date": "2025-10-24",
        "summary": "When a vision model performs image recognition, which visual attributes drive its predictions? Detecting unintended reliance on specific visual features is critical for ensuring model robustness, preventing overfitting, and avoiding spurious correlations. We introduce an automated framework for detecting such dependencies in trained vision models. At the core of our method is a self-reflective agent that systematically generates and tests hypotheses about visual attributes that a model may rely on. This process is iterative: the agent refines its hypotheses based on experimental outcomes and uses a self-evaluation protocol to assess whether its findings accurately explain model behavior. When inconsistencies arise, the agent self-reflects over its findings and triggers a new cycle of experimentation. We evaluate our approach on a novel benchmark of 130 models designed to exhibit diverse visual attribute dependencies across 18 categories. Our results show that the agent's performance consistently improves with self-reflection, with a significant performance increase over non-reflective baselines. We further demonstrate that the agent identifies real-world visual attribute dependencies in state-of-the-art models, including CLIP's vision encoder and the YOLOv8 object detector.",
        "translated": "当视觉模型执行图像识别任务时，哪些视觉属性驱动了其预测结果？检测模型对特定视觉特征的非预期依赖，对于确保模型鲁棒性、防止过拟合以及避免虚假相关性至关重要。我们提出了一种自动化框架，用于检测已训练视觉模型中的此类依赖关系。该方法的核心是一个自省代理，它系统性地生成并验证关于模型可能依赖的视觉属性的假设。该过程是迭代的：代理根据实验结果不断优化其假设，并采用自评估协议来判断其发现是否准确解释了模型的行为。当出现不一致时，代理会对自身发现进行自省，并触发新一轮的实验循环。我们在一个包含130个模型的新基准上评估了该方法，这些模型被设计为在18个类别中表现出多样化的视觉属性依赖。实验结果表明，代理的性能随着自省能力的提升而持续改善，相较于无自省能力的基线方法取得了显著的性能提升。我们进一步证明，该代理能够识别当前主流模型中真实存在的视觉属性依赖，包括CLIP的视觉编码器和YOLOv8目标检测器。",
        "translated_title": "自动化视觉属性依赖检测的自反思代理",
        "label": [],
        "label_reason": "论文聚焦于视觉模型属性依赖检测，属于模型分析与鲁棒性研究，非图像像素级恢复或增强任务。",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出自反思代理框架，具有迭代实验与自我评估机制，对模型分析方法有明显创新。"
    },
    {
        "title": "Visual Diffusion Models are Geometric Solvers",
        "url": "http://arxiv.org/abs/2510.21697v1",
        "pub_date": "2025-10-24",
        "summary": "In this paper we show that visual diffusion models can serve as effective geometric solvers: they can directly reason about geometric problems by working in pixel space. We first demonstrate this on the Inscribed Square Problem, a long-standing problem in geometry that asks whether every Jordan curve contains four points forming a square. We then extend the approach to two other well-known hard geometric problems: the Steiner Tree Problem and the Simple Polygon Problem.   Our method treats each problem instance as an image and trains a standard visual diffusion model that transforms Gaussian noise into an image representing a valid approximate solution that closely matches the exact one. The model learns to transform noisy geometric structures into correct configurations, effectively recasting geometric reasoning as image generation.   Unlike prior work that necessitates specialized architectures and domain-specific adaptations when applying diffusion to parametric geometric representations, we employ a standard visual diffusion model that operates on the visual representation of the problem. This simplicity highlights a surprising bridge between generative modeling and geometric problem solving. Beyond the specific problems studied here, our results point toward a broader paradigm: operating in image space provides a general and practical framework for approximating notoriously hard problems, and opens the door to tackling a far wider class of challenging geometric tasks.",
        "translated": "本文表明，视觉扩散模型可作为有效的几何求解器：它们能够在像素空间中直接对几何问题进行推理。我们首先在“内接正方形问题”（Inscribed Square Problem）上验证了这一点，该问题是几何学中的一个长期悬而未决的问题，其核心是：每条若尔当曲线（Jordan curve）是否都包含四个点，能构成一个正方形。随后，我们将该方法扩展至另外两个著名的难解几何问题：斯坦纳树问题（Steiner Tree Problem）和简单多边形问题（Simple Polygon Problem）。我们的方法将每个问题实例视为一张图像，并训练一个标准的视觉扩散模型，该模型将高斯噪声转化为一张表示有效近似解的图像，且该解与精确解高度吻合。模型通过学习将含噪的几何结构转化为正确的构型，实际上将几何推理问题重新表述为图像生成任务。与以往工作不同，后者在将扩散模型应用于参数化几何表示时通常需要专门设计的网络架构和领域特定的适配，而我们采用的是在问题的视觉表示上运行的标准视觉扩散模型。这种简洁性揭示了生成建模与几何问题求解之间一种令人意外的桥梁。超越本文所研究的具体问题，我们的结果指向一种更广泛的范式：在图像空间中操作，为近似解决那些众所周知的难题提供了一种通用且实用的框架，并为处理更广泛类别的挑战性几何任务打开了大门。",
        "translated_title": "视觉扩散模型是几何求解器",
        "label": [],
        "label_reason": "论文聚焦几何问题求解，非图像像素级恢复或增强，属高阶视觉任务。",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出用扩散模型求解几何问题，方法新颖但非图像恢复领域创新。"
    },
    {
        "title": "BachVid: Training-Free Video Generation with Consistent Background and\n  Character",
        "url": "http://arxiv.org/abs/2510.21696v1",
        "pub_date": "2025-10-24",
        "summary": "Diffusion Transformers (DiTs) have recently driven significant progress in text-to-video (T2V) generation. However, generating multiple videos with consistent characters and backgrounds remains a significant challenge. Existing methods typically rely on reference images or extensive training, and often only address character consistency, leaving background consistency to image-to-video models. We introduce BachVid, the first training-free method that achieves consistent video generation without needing any reference images. Our approach is based on a systematic analysis of DiT's attention mechanism and intermediate features, revealing its ability to extract foreground masks and identify matching points during the denoising process. Our method leverages this finding by first generating an identity video and caching the intermediate variables, and then inject these cached variables into corresponding positions in newly generated videos, ensuring both foreground and background consistency across multiple videos. Experimental results demonstrate that BachVid achieves robust consistency in generated videos without requiring additional training, offering a novel and efficient solution for consistent video generation without relying on reference images or additional training.",
        "translated": "扩散Transformer（DiTs）近期在文本到视频（T2V）生成任务中取得了显著进展。然而，生成多个具有角色和背景一致性的视频仍是一项重大挑战。现有方法通常依赖参考图像或大量训练，且往往仅解决角色一致性问题，而将背景一致性留待图像到视频模型处理。我们提出BachVid，这是首个无需训练、且无需任何参考图像即可实现一致视频生成的方法。我们的方法基于对DiT注意力机制和中间特征的系统性分析，揭示了其在去噪过程中提取前景掩码并识别匹配点的能力。基于这一发现，我们的方法首先生成一个身份视频并缓存其中间变量，随后将这些缓存变量注入到新生成视频的对应位置，从而确保多个视频在前景和背景上均保持一致性。实验结果表明，BachVid在无需额外训练的情况下实现了生成视频的鲁棒一致性，为无需依赖参考图像或额外训练的一致视频生成提供了新颖且高效的解决方案。",
        "translated_title": "BachVid：无需训练的视频生成方法，具备一致的背景与角色",
        "label": [],
        "label_reason": "论文聚焦于视频生成与一致性，属于high-level任务，不涉及像素级图像恢复或增强。",
        "relevance_score": 2,
        "novelty_score": 8,
        "novelty_reason": "提出基于DiT中间特征缓存的无训练一致性生成方法，对生成模型有创新性改进。"
    },
    {
        "title": "On Thin Ice: Towards Explainable Conservation Monitoring via Attribution\n  and Perturbations",
        "url": "http://arxiv.org/abs/2510.21689v1",
        "pub_date": "2025-10-24",
        "summary": "Computer vision can accelerate ecological research and conservation monitoring, yet adoption in ecology lags in part because of a lack of trust in black-box neural-network-based models. We seek to address this challenge by applying post-hoc explanations to provide evidence for predictions and document limitations that are important to field deployment. Using aerial imagery from Glacier Bay National Park, we train a Faster R-CNN to detect pinnipeds (harbor seals) and generate explanations via gradient-based class activation mapping (HiResCAM, LayerCAM), local interpretable model-agnostic explanations (LIME), and perturbation-based explanations. We assess explanations along three axes relevant to field use: (i) localization fidelity: whether high-attribution regions coincide with the animal rather than background context; (ii) faithfulness: whether deletion/insertion tests produce changes in detector confidence; and (iii) diagnostic utility: whether explanations reveal systematic failure modes. Explanations concentrate on seal torsos and contours rather than surrounding ice/rock, and removal of the seals reduces detection confidence, providing model-evidence for true positives. The analysis also uncovers recurrent error sources, including confusion between seals and black ice and rocks. We translate these findings into actionable next steps for model development, including more targeted data curation and augmentation. By pairing object detection with post-hoc explainability, we can move beyond \"black-box\" predictions toward auditable, decision-supporting tools for conservation monitoring.",
        "translated": "计算机视觉可加速生态学研究与保护监测，但由于基于黑箱神经网络模型的信任缺失，其在生态学领域的应用仍相对滞后。我们通过引入事后解释方法，为模型预测提供证据，并记录对野外部署至关重要的局限性，以应对这一挑战。我们采用阿拉斯加冰川湾国家公园的航拍影像，训练一个 Faster R-CNN 模型用于检测鳍足类动物（港海豹），并通过基于梯度的类别激活映射（HiResCAM、LayerCAM）、局部可解释模型无关解释（LIME）以及基于扰动的解释方法生成模型解释。我们从三个与野外应用相关的维度评估解释效果：（i）定位保真度：高归因区域是否与动物本身而非背景环境重合；（ii）忠实度：删除/插入测试是否导致检测器置信度发生变化；（iii）诊断效用：解释是否揭示了系统性的失效模式。解释结果主要集中在海豹躯干和轮廓区域，而非周围冰面/岩石，且移除海豹后检测置信度显著下降，为真阳性预测提供了模型层面的证据。该分析还揭示了反复出现的错误来源，包括海豹与黑色冰面及岩石之间的混淆。我们将这些发现转化为模型开发的可操作性下一步措施，例如更精准的数据筛选与数据增强。通过将目标检测与事后可解释性相结合，我们可以超越“黑箱”预测，迈向可审计、支持决策的保护监测工具。",
        "translated_title": "在薄冰之上：通过归因与扰动实现可解释的保护监测",
        "label": [],
        "label_reason": "论文聚焦目标检测与可解释性，属于high-level任务，不涉及像素级图像恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 4,
        "novelty_reason": "提出可解释性分析框架，但方法常规，未在低级图像处理任务上创新。"
    },
    {
        "title": "WorldGrow: Generating Infinite 3D World",
        "url": "http://arxiv.org/abs/2510.21682v1",
        "pub_date": "2025-10-24",
        "summary": "We tackle the challenge of generating the infinitely extendable 3D world -- large, continuous environments with coherent geometry and realistic appearance. Existing methods face key challenges: 2D-lifting approaches suffer from geometric and appearance inconsistencies across views, 3D implicit representations are hard to scale up, and current 3D foundation models are mostly object-centric, limiting their applicability to scene-level generation. Our key insight is leveraging strong generation priors from pre-trained 3D models for structured scene block generation. To this end, we propose WorldGrow, a hierarchical framework for unbounded 3D scene synthesis. Our method features three core components: (1) a data curation pipeline that extracts high-quality scene blocks for training, making the 3D structured latent representations suitable for scene generation; (2) a 3D block inpainting mechanism that enables context-aware scene extension; and (3) a coarse-to-fine generation strategy that ensures both global layout plausibility and local geometric/textural fidelity. Evaluated on the large-scale 3D-FRONT dataset, WorldGrow achieves SOTA performance in geometry reconstruction, while uniquely supporting infinite scene generation with photorealistic and structurally consistent outputs. These results highlight its capability for constructing large-scale virtual environments and potential for building future world models.",
        "translated": "我们致力于解决生成无限扩展的三维世界——即具有连贯几何结构和真实外观的大规模连续环境——这一挑战。现有方法面临关键难题：基于二维提升的方法在不同视角下存在几何与外观不一致性；三维隐式表示难以扩展；当前的三维基础模型大多以物体为中心，限制了其在场景级生成中的适用性。我们的核心见解是利用预训练三维模型提供的强大生成先验，用于结构化场景块的生成。为此，我们提出 WorldGrow，一种用于无界三维场景合成的层次化框架。该方法包含三个核心组件：（1）一个数据整理流程，用于提取高质量的场景块以用于训练，使得三维结构化潜在表示适用于场景生成；（2）一种三维块修复机制，能够实现上下文感知的场景扩展；（3）一种从粗到精的生成策略，确保全局布局的合理性以及局部几何与纹理的保真度。在大规模 3D-FRONT 数据集上的评估表明，WorldGrow 在几何重建方面达到当前最优性能，同时独特地支持无限场景生成，输出效果具有照片级真实感与结构一致性。这些结果突显了其在构建大规模虚拟环境方面的潜力，以及在未来世界模型构建中的应用前景。",
        "translated_title": "WorldGrow: 生成无限3D世界",
        "label": [],
        "label_reason": "论文聚焦3D世界生成，属高阶视觉任务，非像素级图像恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出分层框架与块填充机制，对3D场景生成有显著改进，但非low-level创新。"
    },
    {
        "title": "Foundation Models in Dermatopathology: Skin Tissue Classification",
        "url": "http://arxiv.org/abs/2510.21664v1",
        "pub_date": "2025-10-24",
        "summary": "The rapid generation of whole-slide images (WSIs) in dermatopathology necessitates automated methods for efficient processing and accurate classification. This study evaluates the performance of two foundation models, UNI and Virchow2, as feature extractors for classifying WSIs into three diagnostic categories: melanocytic, basaloid, and squamous lesions. Patch-level embeddings were aggregated into slide-level features using a mean-aggregation strategy and subsequently used to train multiple machine learning classifiers, including logistic regression, gradient-boosted trees, and random forest models. Performance was assessed using precision, recall, true positive rate, false positive rate, and the area under the receiver operating characteristic curve (AUROC) on the test set. Results demonstrate that patch-level features extracted using Virchow2 outperformed those extracted via UNI across most slide-level classifiers, with logistic regression achieving the highest accuracy (90%) for Virchow2, though the difference was not statistically significant. The study also explored data augmentation techniques and image normalization to enhance model robustness and generalizability. The mean-aggregation approach provided reliable slide-level feature representations. All experimental results and metrics were tracked and visualized using WandB.ai, facilitating reproducibility and interpretability. This research highlights the potential of foundation models for automated WSI classification, providing a scalable and effective approach for dermatopathological diagnosis while paving the way for future advancements in slide-level representation learning.",
        "translated": "皮肤病理学中全切片图像（WSIs）的快速生成亟需自动化方法以实现高效处理和准确分类。本研究评估了两个基础模型UNI和Virchow2作为特征提取器，在将WSIs分为三种诊断类别（黑色素细胞病变、基底样病变和鳞状病变）任务中的性能表现。通过均值聚合策略将斑块级嵌入特征整合为切片级特征，并用于训练多种机器学习分类器，包括逻辑回归、梯度提升树和随机森林模型。在测试集上，采用精确率、召回率、真正例率、假正例率以及受试者工作特征曲线下的面积（AUROC）对性能进行评估。结果表明，在大多数切片级分类器中，使用Virchow2提取的斑块级特征表现优于UNI提取的特征，其中逻辑回归在Virchow2上达到最高准确率（90%），尽管该差异在统计学上并不显著。研究还探索了数据增强技术和图像归一化方法，以提升模型的鲁棒性和泛化能力。均值聚合方法提供了可靠的切片级特征表示。所有实验结果与指标均通过WandB.ai进行跟踪与可视化，有助于提升研究的可复现性和可解释性。本研究凸显了基础模型在自动化WSI分类中的潜力，为皮肤病理学诊断提供了一种可扩展且有效的解决方案，同时为未来切片级表征学习的发展奠定了基础。",
        "translated_title": "基础模型在皮肤病理学中的应用：皮肤组织分类",
        "label": [],
        "label_reason": "论文聚焦皮肤组织分类，属于高阶视觉任务，未涉及图像像素级恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 4,
        "novelty_reason": "方法基于现有基础模型提取特征，采用常规聚合策略，无显著创新。"
    },
    {
        "title": "Self-Supervised Learning of Synapse Types from EM Images",
        "url": "http://arxiv.org/abs/2510.21663v1",
        "pub_date": "2025-10-24",
        "summary": "Separating synapses into different classes based on their appearance in EM images has many applications in biology. Examples may include assigning a neurotransmitter to a particular class, or separating synapses whose strength can be modulated from those whose strength is fixed. Traditionally, this has been done in a supervised manner, giving the classification algorithm examples of the different classes. Here we instead separate synapses into classes based only on the observation that nearby synapses in the same neuron are likely more similar than synapses chosen randomly from different cells. We apply our methodology to data from {\\it Drosophila}. Our approach has the advantage that the number of synapse types does not need to be known in advance. It may also provide a principled way to select ground-truth that spans the range of synapse structure.",
        "translated": "根据电子显微镜（EM）图像中突触的外观将其划分为不同类别，在生物学研究中具有多种应用。例如，可将特定神经递质分配给某一类别，或区分那些强度可调的突触与强度固定的突触。传统方法通常采用监督学习方式，通过为分类算法提供各类别的示例来进行分类。本文则提出一种无监督方法，仅基于如下观察：同一神经元内相邻的突触比从不同细胞中随机选取的突触更可能具有相似性，从而实现突触的类别划分。我们将该方法应用于果蝇（{\\it Drosophila}）的数据。该方法的优势在于无需预先知道突触类型的数量，同时可能为选择覆盖突触结构全范围的真实标签（ground-truth）提供一种理论依据。",
        "translated_title": "自监督学习电子显微镜图像中的突触类型",
        "label": [],
        "label_reason": "论文属于生物图像分类任务，非像素级图像恢复或增强，属high-level视觉任务。",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "提出自监督分类方法，但未涉及图像质量改善，创新点在生物应用而非图像处理。"
    },
    {
        "title": "Long-tailed Species Recognition in the NACTI Wildlife Dataset",
        "url": "http://arxiv.org/abs/2510.21657v1",
        "pub_date": "2025-10-24",
        "summary": "As most ''in the wild'' data collections of the natural world, the North America Camera Trap Images (NACTI) dataset shows severe long-tailed class imbalance, noting that the largest 'Head' class alone covers &gt;50% of the 3.7M images in the corpus. Building on the PyTorch Wildlife model, we present a systematic study of Long-Tail Recognition methodologies for species recognition on the NACTI dataset covering experiments on various LTR loss functions plus LTR-sensitive regularisation. Our best configuration achieves 99.40% Top-1 accuracy on our NACTI test data split, substantially improving over a 95.51% baseline using standard cross-entropy with Adam. This also improves on previously reported top performance in MLWIC2 at 96.8% albeit using partly unpublished (potentially different) partitioning, optimiser, and evaluation protocols. To evaluate domain shifts (e.g. night-time captures, occlusion, motion-blur) towards other datasets we construct a Reduced-Bias Test set from the ENA-Detection dataset where our experimentally optimised long-tail enhanced model achieves leading 52.55% accuracy (up from 51.20% with WCE loss), demonstrating stronger generalisation capabilities under distribution shift. We document the consistent improvements of LTR-enhancing scheduler choices in this NACTI wildlife domain, particularly when in tandem with state-of-the-art LTR losses. We finally discuss qualitative and quantitative shortcomings that LTR methods cannot sufficiently address, including catastrophic breakdown for 'Tail' classes under severe domain shift. For maximum reproducibility we publish all dataset splits, key code, and full network weights.",
        "translated": "与大多数“野外”自然场景数据集类似，北美相机陷阱图像（NACTI）数据集表现出严重的长尾类别不平衡问题，其中最大的“Head”类别单独占据了语料库中370万张图像的超过50%。基于PyTorch Wildlife模型，我们针对NACTI数据集上的物种识别任务，系统性地研究了长尾识别（LTR）方法，涵盖了多种LTR损失函数以及对LTR敏感的正则化技术的实验。我们的最优配置在NACTI测试集划分上实现了99.40%的Top-1准确率，显著优于采用标准交叉熵损失和Adam优化器的95.51%基线性能。该结果也超越了此前在MLWIC2中报告的最高性能96.8%，尽管后者使用了部分未公开（可能不同）的数据划分、优化器及评估协议。为评估域偏移（如夜间拍摄、遮挡、运动模糊）对其他数据集的影响，我们从ENA-Detection数据集中构建了一个低偏差测试集，实验优化的长尾增强模型在此测试集上取得了领先的52.55%准确率（相比使用WCE损失的51.20%有所提升），表明其在分布偏移条件下具有更强的泛化能力。我们记录了在NACTI野生动物领域中，LTR增强调度策略的一致性改进，尤其是在与当前最先进的LTR损失函数联合使用时。最后，我们讨论了LTR方法在定性和定量层面仍无法充分解决的局限性，包括在严重域偏移下对“尾部”类别的灾难性失效。为确保最大可复现性，我们公开了所有数据集划分、关键代码及完整的网络权重。",
        "translated_title": "NACTI野生动物数据集中的长尾物种识别",
        "label": [],
        "label_reason": "论文聚焦野生动物物种识别，属图像分类任务，非像素级图像恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "在长尾分布下优化分类模型，属常规改进，无低层视觉新方法或架构创新。"
    },
    {
        "title": "Group Inertial Poser: Multi-Person Pose and Global Translation from\n  Sparse Inertial Sensors and Ultra-Wideband Ranging",
        "url": "http://arxiv.org/abs/2510.21654v1",
        "pub_date": "2025-10-24",
        "summary": "Tracking human full-body motion using sparse wearable inertial measurement units (IMUs) overcomes the limitations of occlusion and instrumentation of the environment inherent in vision-based approaches. However, purely IMU-based tracking compromises translation estimates and accurate relative positioning between individuals, as inertial cues are inherently self-referential and provide no direct spatial reference for others. In this paper, we present a novel approach for robustly estimating body poses and global translation for multiple individuals by leveraging the distances between sparse wearable sensors - both on each individual and across multiple individuals. Our method Group Inertial Poser estimates these absolute distances between pairs of sensors from ultra-wideband ranging (UWB) and fuses them with inertial observations as input into structured state-space models to integrate temporal motion patterns for precise 3D pose estimation. Our novel two-step optimization further leverages the estimated distances for accurately tracking people's global trajectories through the world. We also introduce GIP-DB, the first IMU+UWB dataset for two-person tracking, which comprises 200 minutes of motion recordings from 14 participants. In our evaluation, Group Inertial Poser outperforms previous state-of-the-art methods in accuracy and robustness across synthetic and real-world data, showing the promise of IMU+UWB-based multi-human motion capture in the wild. Code, models, dataset: https://github.com/eth-siplab/GroupInertialPoser",
        "translated": "利用稀疏可穿戴惯性测量单元（IMUs）进行人体全身运动追踪，能够克服基于视觉方法固有的遮挡和环境布设限制。然而，纯IMU-based追踪在全局平移估计以及个体间精确相对定位方面存在不足，因为惯性信息本质上是自参考的，无法为其他个体提供直接的空间参考。本文提出一种新颖方法，通过利用稀疏可穿戴传感器之间的距离信息——包括每个个体内部以及多个个体之间的传感器距离——来稳健估计多人的躯体姿态和全局平移。我们的方法Group Inertial Poser从超宽带测距（UWB）中估计传感器对之间的绝对距离，并将其与惯性观测融合，作为结构化状态空间模型的输入，以整合时间运动模式，实现精确的3D姿态估计。我们提出的新型两步优化方法进一步利用估计的距离信息，准确追踪个体在世界坐标系中的全局轨迹。此外，我们还发布了GIP-DB，这是首个用于两人追踪的IMU+UWB数据集，包含来自14名参与者的200分钟运动记录。在评估中，Group Inertial Poser在合成数据和真实世界数据上均优于现有最先进方法，在准确性和鲁棒性方面表现出色，展现了基于IMU+UWB的野外多人运动捕捉的巨大潜力。代码、模型、数据集：https://github.com/eth-siplab/GroupInertialPoser",
        "translated_title": "群体惯性姿态估计器：基于稀疏惯性传感器与超宽带测距的多人姿态与全局位移估计",
        "label": [],
        "label_reason": "论文聚焦于多人体姿态与全局位移估计，属于三维运动捕捉，非图像像素级处理。",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出两步优化与结构化状态空间模型，融合IMU与UWB数据，方法新颖但非图像处理领域。"
    },
    {
        "title": "A Dynamic Knowledge Distillation Method Based on the Gompertz Curve",
        "url": "http://arxiv.org/abs/2510.21649v1",
        "pub_date": "2025-10-24",
        "summary": "This paper introduces a novel dynamic knowledge distillation framework, Gompertz-CNN, which integrates the Gompertz growth model into the training process to address the limitations of traditional knowledge distillation. Conventional methods often fail to capture the evolving cognitive capacity of student models, leading to suboptimal knowledge transfer. To overcome this, we propose a stage-aware distillation strategy that dynamically adjusts the weight of distillation loss based on the Gompertz curve, reflecting the student's learning progression: slow initial growth, rapid mid-phase improvement, and late-stage saturation. Our framework incorporates Wasserstein distance to measure feature-level discrepancies and gradient matching to align backward propagation behaviors between teacher and student models. These components are unified under a multi-loss objective, where the Gompertz curve modulates the influence of distillation losses over time. Extensive experiments on CIFAR-10 and CIFAR-100 using various teacher-student architectures (e.g., ResNet50 and MobileNet_v2) demonstrate that Gompertz-CNN consistently outperforms traditional distillation methods, achieving up to 8% and 4% accuracy gains on CIFAR-10 and CIFAR-100, respectively.",
        "translated": "本文提出了一种新颖的动态知识蒸馏框架Gompertz-CNN，将Gompertz生长模型融入训练过程，以克服传统知识蒸馏方法的局限性。传统方法通常无法捕捉学生模型认知能力的动态演变，导致知识迁移效果欠佳。为解决该问题，我们提出一种阶段感知的蒸馏策略，基于Gompertz曲线动态调整蒸馏损失的权重，以反映学生模型的学习进程：初期缓慢增长、中期快速提升、后期趋于饱和。本框架引入Wasserstein距离来度量特征层面的差异，并通过梯度匹配对齐教师模型与学生模型的反向传播行为。这些组件统一于一个多损失目标函数中，其中Gompertz曲线调节蒸馏损失随时间的影响。在CIFAR-10和CIFAR-100数据集上，采用多种教师-学生网络架构（如ResNet50和MobileNet_v2）进行大量实验，结果表明Gompertz-CNN始终优于传统蒸馏方法，在CIFAR-10和CIFAR-100上分别实现了最高8%和4%的准确率提升。",
        "translated_title": "基于Gompertz曲线的动态知识蒸馏方法",
        "label": [],
        "label_reason": "论文聚焦知识蒸馏框架优化，用于分类任务，非图像恢复或增强，不涉及像素级图像质量改善。",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出基于Gompertz曲线的动态蒸馏策略，结合Wasserstein距离和梯度匹配，对蒸馏方法有改进。"
    },
    {
        "title": "DAP-MAE: Domain-Adaptive Point Cloud Masked Autoencoder for Effective\n  Cross-Domain Learning",
        "url": "http://arxiv.org/abs/2510.21635v1",
        "pub_date": "2025-10-24",
        "summary": "Compared to 2D data, the scale of point cloud data in different domains available for training, is quite limited. Researchers have been trying to combine these data of different domains for masked autoencoder (MAE) pre-training to leverage such a data scarcity issue. However, the prior knowledge learned from mixed domains may not align well with the downstream 3D point cloud analysis tasks, leading to degraded performance. To address such an issue, we propose the Domain-Adaptive Point Cloud Masked Autoencoder (DAP-MAE), an MAE pre-training method, to adaptively integrate the knowledge of cross-domain datasets for general point cloud analysis. In DAP-MAE, we design a heterogeneous domain adapter that utilizes an adaptation mode during pre-training, enabling the model to comprehensively learn information from point clouds across different domains, while employing a fusion mode in the fine-tuning to enhance point cloud features. Meanwhile, DAP-MAE incorporates a domain feature generator to guide the adaptation of point cloud features to various downstream tasks. With only one pre-training, DAP-MAE achieves excellent performance across four different point cloud analysis tasks, reaching 95.18% in object classification on ScanObjectNN and 88.45% in facial expression recognition on Bosphorus.",
        "translated": "与二维数据相比，可用于训练的不同领域点云数据的规模相当有限。研究人员尝试将这些跨领域数据结合用于掩码自编码器（MAE）的预训练，以应对数据稀缺问题。然而，从混合领域中学习到的先验知识可能与下游三维点云分析任务不匹配，从而导致性能下降。为解决这一问题，我们提出了一种领域自适应点云掩码自编码器（DAP-MAE），这是一种MAE预训练方法，能够自适应地整合跨领域数据集的知识，以实现通用点云分析。在DAP-MAE中，我们设计了一个异构领域适配器，在预训练阶段采用适配模式，使模型能够全面学习来自不同领域点云的信息；在微调阶段则采用融合模式，以增强点云特征。同时，DAP-MAE引入了一个领域特征生成器，用于引导点云特征向多种下游任务进行适配。仅通过一次预训练，DAP-MAE在四种不同的点云分析任务中均取得了优异性能，在ScanObjectNN上的物体分类准确率达到95.18%，在Bosphorus上的面部表情识别准确率达到88.45%。",
        "translated_title": "DAP-MAE：面向有效跨域学习的域自适应点云掩码自编码器",
        "label": [],
        "label_reason": "论文聚焦3D点云跨域预训练，属于高阶3D分析任务，非图像像素级恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 6,
        "novelty_reason": "提出域自适应机制提升跨域点云特征学习，属领域内常规改进，非颠覆性创新。"
    },
    {
        "title": "Epipolar Geometry Improves Video Generation Models",
        "url": "http://arxiv.org/abs/2510.21615v1",
        "pub_date": "2025-10-24",
        "summary": "Video generation models have progressed tremendously through large latent diffusion transformers trained with rectified flow techniques. Yet these models still struggle with geometric inconsistencies, unstable motion, and visual artifacts that break the illusion of realistic 3D scenes. 3D-consistent video generation could significantly impact numerous downstream applications in generation and reconstruction tasks. We explore how epipolar geometry constraints improve modern video diffusion models. Despite massive training data, these models fail to capture fundamental geometric principles underlying visual content. We align diffusion models using pairwise epipolar geometry constraints via preference-based optimization, directly addressing unstable camera trajectories and geometric artifacts through mathematically principled geometric enforcement. Our approach efficiently enforces geometric principles without requiring end-to-end differentiability. Evaluation demonstrates that classical geometric constraints provide more stable optimization signals than modern learned metrics, which produce noisy targets that compromise alignment quality. Training on static scenes with dynamic cameras ensures high-quality measurements while the model generalizes effectively to diverse dynamic content. By bridging data-driven deep learning with classical geometric computer vision, we present a practical method for generating spatially consistent videos without compromising visual quality.",
        "translated": "视频生成模型通过采用修正流技术训练的大规模潜在扩散变换器取得了显著进展。然而，这些模型在生成过程中仍面临几何不一致性、运动不稳定以及视觉伪影等问题，这些问题破坏了真实3D场景的视觉幻觉。具备3D一致性的视频生成有望在生成与重建任务的众多下游应用中产生重大影响。我们探讨了如何利用极线几何约束提升现代视频扩散模型的性能。尽管这些模型拥有海量训练数据，但它们仍无法捕捉视觉内容背后的基本几何原理。我们通过基于偏好的优化方法，利用成对的极线几何约束对扩散模型进行对齐，直接通过数学上严谨的几何约束解决相机轨迹不稳定和几何伪影问题。我们的方法能够高效地强制执行几何原理，且无需端到端可微性。实验结果表明，经典几何约束提供的优化信号比现代学习到的度量指标更为稳定，后者会产生噪声目标，从而损害对齐质量。在静态场景与动态相机条件下进行训练，既能保证高质量的测量结果，又能使模型有效泛化到多样化的动态内容。通过融合数据驱动的深度学习与经典几何计算机视觉，我们提出了一种实用方法，在不牺牲视觉质量的前提下生成空间一致的视频。",
        "translated_title": "极线几何提升视频生成模型",
        "label": [],
        "label_reason": "论文聚焦视频生成，目标为生成3D一致视频，属于high-level生成任务，非像素级图像恢复。",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "引入经典几何约束优化生成模型，提升几何一致性，方法新颖但非low-level任务创新。"
    },
    {
        "title": "Modest-Align: Data-Efficient Alignment for Vision-Language Models",
        "url": "http://arxiv.org/abs/2510.21606v1",
        "pub_date": "2025-10-24",
        "summary": "Cross-modal alignment aims to map heterogeneous modalities into a shared latent space, as exemplified by models like CLIP, which benefit from large-scale image-text pretraining for strong recognition capabilities. However, when operating in resource-constrained settings with limited or low-quality data, these models often suffer from overconfidence and degraded performance due to the prevalence of ambiguous or weakly correlated image-text pairs. Current contrastive learning approaches, which rely on single positive pairs, further exacerbate this issue by reinforcing overconfidence on uncertain samples. To address these challenges, we propose Modest-Align, a lightweight alignment framework designed for robustness and efficiency. Our approach leverages two complementary strategies -- Random Perturbation, which introduces controlled noise to simulate uncertainty, and Embedding Smoothing, which calibrates similarity distributions in the embedding space. These mechanisms collectively reduce overconfidence and improve performance on noisy or weakly aligned samples. Extensive experiments across multiple benchmark datasets demonstrate that Modest-Align outperforms state-of-the-art methods in retrieval tasks, achieving competitive results with over 100x less training data and 600x less GPU time than CLIP. Our method offers a practical and scalable solution for cross-modal alignment in real-world, low-resource scenarios.",
        "translated": "跨模态对齐旨在将异构模态映射到共享的潜在空间，例如CLIP等模型，通过大规模图像-文本预训练获得了强大的识别能力。然而，在资源受限、数据有限或质量较低的场景下，这些模型常因存在大量模糊或弱相关图像-文本对而表现出过度自信，并导致性能下降。当前基于对比学习的方法依赖单一正样本对，进一步加剧了这一问题，即在不确定性样本上强化了过度自信。为应对这些挑战，我们提出Modest-Align，一种轻量级的对齐框架，旨在提升鲁棒性与效率。我们的方法采用两种互补策略：随机扰动（Random Perturbation），通过引入受控噪声模拟不确定性；以及嵌入平滑（Embedding Smoothing），用于校准嵌入空间中的相似性分布。这两种机制共同作用，有效降低过度自信，并提升在噪声或弱对齐样本上的性能。在多个基准数据集上的大量实验表明，Modest-Align在检索任务中超越了现有最先进方法，在训练数据量减少超过100倍、GPU训练时间减少600倍的情况下，仍能取得具有竞争力的结果。本方法为现实世界中低资源场景下的跨模态对齐提供了实用且可扩展的解决方案。",
        "translated_title": "Modest-Align：面向视觉-语言模型的数据高效对齐方法",
        "label": [],
        "label_reason": "论文聚焦视觉-语言模型对齐，属于高阶视觉理解任务，不涉及图像像素级恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 6,
        "novelty_reason": "提出随机扰动与嵌入平滑机制，提升对齐鲁棒性，属方法改进但非低级视觉创新。"
    },
    {
        "title": "S3OD: Towards Generalizable Salient Object Detection with Synthetic Data",
        "url": "http://arxiv.org/abs/2510.21605v1",
        "pub_date": "2025-10-24",
        "summary": "Salient object detection exemplifies data-bounded tasks where expensive pixel-precise annotations force separate model training for related subtasks like DIS and HR-SOD. We present a method that dramatically improves generalization through large-scale synthetic data generation and ambiguity-aware architecture. We introduce S3OD, a dataset of over 139,000 high-resolution images created through our multi-modal diffusion pipeline that extracts labels from diffusion and DINO-v3 features. The iterative generation framework prioritizes challenging categories based on model performance. We propose a streamlined multi-mask decoder that naturally handles the inherent ambiguity in salient object detection by predicting multiple valid interpretations. Models trained solely on synthetic data achieve 20-50% error reduction in cross-dataset generalization, while fine-tuned versions reach state-of-the-art performance across DIS and HR-SOD benchmarks.",
        "translated": "显著性目标检测是典型的数据受限任务，其中昂贵的像素级标注迫使相关子任务（如DIS和HR-SOD）需分别进行模型训练。我们提出一种方法，通过大规模合成数据生成与模糊感知架构显著提升模型泛化能力。我们引入S3OD，一个包含超过139,000张高分辨率图像的数据集，通过我们的多模态扩散生成管道构建，并从扩散模型与DINO-v3特征中提取标签。该迭代生成框架根据模型性能优先生成具有挑战性的类别。我们提出一种精简的多掩码解码器，能够自然处理显著性目标检测中固有的模糊性，通过预测多个有效解释来实现。仅在合成数据上训练的模型在跨数据集泛化中实现20-50%的误差降低，而微调版本在DIS和HR-SOD基准测试中均达到当前最优性能。",
        "translated_title": "S3OD：基于合成数据实现可泛化的显著目标检测",
        "label": [],
        "label_reason": "论文聚焦显著性目标检测，属于高阶视觉任务，不涉及像素级图像恢复或增强。",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出合成数据生成与多掩码解码器，对SOD任务有显著改进，但非low-level任务创新。"
    },
    {
        "title": "Automated interictal epileptic spike detection from simple and noisy\n  annotations in MEG data",
        "url": "http://arxiv.org/abs/2510.21596v1",
        "pub_date": "2025-10-24",
        "summary": "In drug-resistant epilepsy, presurgical evaluation of epilepsy can be considered. Magnetoencephalography (MEG) has been shown to be an effective exam to inform the localization of the epileptogenic zone through the localization of interictal epileptic spikes. Manual detection of these pathological biomarkers remains a fastidious and error-prone task due to the high dimensionality of MEG recordings, and interrater agreement has been reported to be only moderate. Current automated methods are unsuitable for clinical practice, either requiring extensively annotated data or lacking robustness on non-typical data. In this work, we demonstrate that deep learning models can be used for detecting interictal spikes in MEG recordings, even when only temporal and single-expert annotations are available, which represents real-world clinical practice. We propose two model architectures: a feature-based artificial neural network (ANN) and a convolutional neural network (CNN), trained on a database of 59 patients, and evaluated against a state-of-the-art model to classify short time windows of signal. In addition, we employ an interactive machine learning strategy to iteratively improve our data annotation quality using intermediary model outputs. Both proposed models outperform the state-of-the-art model (F1-scores: CNN=0.46, ANN=0.44) when tested on 10 holdout test patients. The interactive machine learning strategy demonstrates that our models are robust to noisy annotations. Overall, results highlight the robustness of models with simple architectures when analyzing complex and imperfectly annotated data. Our method of interactive machine learning offers great potential for faster data annotation, while our models represent useful and efficient tools for automated interictal spikes detection.",
        "translated": "在耐药性癫痫中，可考虑进行术前癫痫评估。脑磁图（MEG）已被证明是一种有效的检查手段，可通过定位发作间期癫痫棘波来确定致痫区的位置。然而，由于MEG记录具有高维特性，手动检测这些病理生物标志物仍是一项繁琐且容易出错的任务，且已有研究表明不同评估者之间的一致性仅为中等水平。当前的自动化方法尚不适合临床实践，要么需要大量标注数据，要么在非典型数据上缺乏鲁棒性。在本研究中，我们证明了深度学习模型可用于检测MEG记录中的发作间期棘波，即使仅提供时间维度和单专家标注数据——这代表了真实的临床实践场景。我们提出了两种模型架构：一种基于特征的人工神经网络（ANN）和一种卷积神经网络（CNN），在包含59名患者的数据库上进行训练，并与当前最先进的模型进行对比，用于分类短时间窗信号。此外，我们采用了一种交互式机器学习策略，通过中间模型输出迭代提升数据标注质量。在10名保留测试患者上测试时，两种所提模型均优于当前最先进的模型（F1分数：CNN=0.46，ANN=0.44）。交互式机器学习策略表明，我们的模型对噪声标注具有鲁棒性。总体而言，结果突显了在分析复杂且标注不完美的数据时，简单架构模型的鲁棒性。我们的交互式机器学习方法在加快数据标注方面具有巨大潜力，而所提出的模型则为自动化发作间期棘波检测提供了实用且高效的工具。",
        "translated_title": "自动化从简单且含噪声的MEG数据标注中检测癫痫间歇期尖波",
        "label": [],
        "label_reason": "论文聚焦于MEG数据中的癫痫尖波检测，属于脑电生理信号分析，非图像处理任务",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "提出交互式机器学习策略，但方法为常规模型组合，无显著创新"
    },
    {
        "title": "Restore Text First, Enhance Image Later: Two-Stage Scene Text Image\n  Super-Resolution with Glyph Structure Guidance",
        "url": "http://arxiv.org/abs/2510.21590v1",
        "pub_date": "2025-10-24",
        "summary": "Current generative super-resolution methods show strong performance on natural images but distort text, creating a fundamental trade-off between image quality and textual readability. To address this, we introduce \\textbf{TIGER} (\\textbf{T}ext-\\textbf{I}mage \\textbf{G}uided sup\\textbf{E}r-\\textbf{R}esolution), a novel two-stage framework that breaks this trade-off through a \\textit{\"text-first, image-later\"} paradigm. \\textbf{TIGER} explicitly decouples glyph restoration from image enhancement: it first reconstructs precise text structures and then uses them to guide subsequent full-image super-resolution. This glyph-to-image guidance ensures both high fidelity and visual consistency. To support comprehensive training and evaluation, we also contribute the \\textbf{UltraZoom-ST} (UltraZoom-Scene Text), the first scene text dataset with extreme zoom (\\textbf{$\\times$14.29}). Extensive experiments show that \\textbf{TIGER} achieves \\textbf{state-of-the-art} performance, enhancing readability while preserving overall image quality.",
        "translated": "当前的生成式超分辨率方法在自然图像上表现出色，但在处理文本时容易产生失真，从而在图像质量与文本可读性之间形成根本性的权衡。为解决这一问题，我们提出了一种新颖的两阶段框架——**TIGER**（**T**ext-**I**mage **G**uided sup**E**r-**R**esolution），该框架通过“**文本优先、图像后处理**”的范式打破上述权衡。**TIGER** 显式地将字形恢复与图像增强过程解耦：首先重建精确的文本结构，随后利用这些结构引导后续的全图像超分辨率处理。这种从字形到图像的引导机制确保了高保真度与视觉一致性。为支持全面的训练与评估，我们还构建了首个包含极端放大倍率（**×14.29**）的场景文本数据集——**UltraZoom-ST**（UltraZoom-Scene Text）。大量实验表明，**TIGER** 达到了**最先进的性能**，在提升文本可读性的同时，有效保持了整体图像质量。",
        "translated_title": "恢复文字先行，图像增强随后：基于字形结构引导的两阶段场景文本图像超分辨率",
        "label": [
            "超分辨率",
            "图像恢复"
        ],
        "label_reason": "论文聚焦场景文本图像超分辨率，显著提升文字可读性与图像质量，属典型low-level图像恢复任务。",
        "relevance_score": 10,
        "novelty_score": 9,
        "novelty_reason": "提出两阶段‘文字优先’框架，引入字形结构引导机制，对现有SR方法有显著创新。"
    },
    {
        "title": "MATrack: Efficient Multiscale Adaptive Tracker for Real-Time Nighttime\n  UAV Operations",
        "url": "http://arxiv.org/abs/2510.21586v1",
        "pub_date": "2025-10-24",
        "summary": "Nighttime UAV tracking faces significant challenges in real-world robotics operations. Low-light conditions not only limit visual perception capabilities, but cluttered backgrounds and frequent viewpoint changes also cause existing trackers to drift or fail during deployment. To address these difficulties, researchers have proposed solutions based on low-light enhancement and domain adaptation. However, these methods still have notable shortcomings in actual UAV systems: low-light enhancement often introduces visual artifacts, domain adaptation methods are computationally expensive and existing lightweight designs struggle to fully leverage dynamic object information. Based on an in-depth analysis of these key issues, we propose MATrack-a multiscale adaptive system designed specifically for nighttime UAV tracking. MATrack tackles the main technical challenges of nighttime tracking through the collaborative work of three core modules: Multiscale Hierarchy Blende (MHB) enhances feature consistency between static and dynamic templates. Adaptive Key Token Gate accurately identifies object information within complex backgrounds. Nighttime Template Calibrator (NTC) ensures stable tracking performance over long sequences. Extensive experiments show that MATrack achieves a significant performance improvement. On the UAVDark135 benchmark, its precision, normalized precision and AUC surpass state-of-the-art (SOTA) methods by 5.9%, 5.4% and 4.2% respectively, while maintaining a real-time processing speed of 81 FPS. Further tests on a real-world UAV platform validate the system's reliability, demonstrating that MATrack can provide stable and effective nighttime UAV tracking support for critical robotics applications such as nighttime search and rescue and border patrol.",
        "translated": "夜间无人机跟踪在实际机器人作业中面临显著挑战。低光照条件不仅限制了视觉感知能力，杂乱的背景和频繁的视角变化也会导致现有跟踪器在部署过程中出现漂移或失效。为应对这些难题，研究者们提出了基于低光照增强和领域自适应的解决方案。然而，这些方法在实际无人机系统中仍存在明显不足：低光照增强通常会引入视觉伪影，领域自适应方法计算开销较大，而现有的轻量化设计难以充分挖掘动态目标信息。基于对上述关键问题的深入分析，我们提出 MATrack——一种专为夜间无人机跟踪设计的多尺度自适应系统。MATrack 通过三个核心模块的协同工作，有效应对夜间跟踪的主要技术挑战：多尺度层次融合模块（Multiscale Hierarchy Blende, MHB）增强静态与动态模板之间的特征一致性；自适应关键令牌门控（Adaptive Key Token Gate）在复杂背景下精确识别目标信息；夜间模板校准器（Nighttime Template Calibrator, NTC）确保在长序列中保持稳定的跟踪性能。大量实验表明，MATrack 实现了显著的性能提升。在 UAVDark135 基准测试集上，其精度、归一化精度和 AUC 分别比当前最先进的（SOTA）方法提升 5.9%、5.4% 和 4.2%，同时保持 81 FPS 的实时处理速度。在真实无人机平台上的进一步测试验证了系统的可靠性，表明 MATrack 能够为夜间搜救、边境巡逻等关键机器人应用提供稳定且有效的夜间无人机跟踪支持。",
        "translated_title": "MATrack：面向实时夜间无人机操作的高效多尺度自适应跟踪器",
        "label": [
            "低光照增强"
        ],
        "label_reason": "论文虽涉及低光照增强，但核心为跟踪任务，非像素级图像恢复。",
        "relevance_score": 4,
        "novelty_score": 7,
        "novelty_reason": "提出多尺度自适应跟踪框架，模块设计有创新，提升跟踪性能。"
    },
    {
        "title": "Sample By Step, Optimize By Chunk: Chunk-Level GRPO For Text-to-Image\n  Generation",
        "url": "http://arxiv.org/abs/2510.21583v1",
        "pub_date": "2025-10-24",
        "summary": "Group Relative Policy Optimization (GRPO) has shown strong potential for flow-matching-based text-to-image (T2I) generation, but it faces two key limitations: inaccurate advantage attribution, and the neglect of temporal dynamics of generation. In this work, we argue that shifting the optimization paradigm from the step level to the chunk level can effectively alleviate these issues. Building on this idea, we propose Chunk-GRPO, the first chunk-level GRPO-based approach for T2I generation. The insight is to group consecutive steps into coherent 'chunk's that capture the intrinsic temporal dynamics of flow matching, and to optimize policies at the chunk level. In addition, we introduce an optional weighted sampling strategy to further enhance performance. Extensive experiments show that ChunkGRPO achieves superior results in both preference alignment and image quality, highlighting the promise of chunk-level optimization for GRPO-based methods.",
        "translated": "组相对策略优化（GRPO）在基于流匹配的文本到图像（T2I）生成任务中展现出强大潜力，但面临两个关键局限：优势归因不准确，以及生成过程时间动态的忽略。本文认为，将优化范式从单步级别提升至块级别，可有效缓解上述问题。基于此思路，我们提出Chunk-GRPO，这是首个面向T2I生成的块级别GRPO方法。其核心思想是将连续生成步骤分组为具有内在一致性的“块”，以捕捉流匹配过程中的固有时间动态，并在块级别上优化策略。此外，我们引入一种可选的加权采样策略以进一步提升性能。大量实验表明，Chunk-GRPO在偏好对齐和图像质量方面均取得更优结果，凸显了块级别优化在GRPO类方法中的潜力。",
        "translated_title": "逐样本、逐块优化：面向文本到图像生成的块级GRPO方法",
        "label": [],
        "label_reason": "论文聚焦于文本到图像生成，属于high-level任务，不涉及像素级图像恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出chunk-level优化范式，对GRPO方法有显著改进，但应用于图像生成而非图像复原。"
    },
    {
        "title": "Foley Control: Aligning a Frozen Latent Text-to-Audio Model to Video",
        "url": "http://arxiv.org/abs/2510.21581v1",
        "pub_date": "2025-10-24",
        "summary": "Foley Control is a lightweight approach to video-guided Foley that keeps pretrained single-modality models frozen and learns only a small cross-attention bridge between them. We connect V-JEPA2 video embeddings to a frozen Stable Audio Open DiT text-to-audio (T2A) model by inserting compact video cross-attention after the model's existing text cross-attention, so prompts set global semantics while video refines timing and local dynamics. The frozen backbones retain strong marginals (video; audio given text) and the bridge learns the audio-video dependency needed for synchronization -- without retraining the audio prior. To cut memory and stabilize training, we pool video tokens before conditioning. On curated video-audio benchmarks, Foley Control delivers competitive temporal and semantic alignment with far fewer trainable parameters than recent multi-modal systems, while preserving prompt-driven controllability and production-friendly modularity (swap/upgrade encoders or the T2A backbone without end-to-end retraining). Although we focus on Video-to-Foley, the same bridge design can potentially extend to other audio modalities (e.g., speech).",
        "translated": "Foley Control 是一种轻量级的视频引导音效生成方法，其保持预训练的单模态模型参数冻结，仅学习一个小型的跨注意力连接桥接结构。我们将 V-JEPA2 视频嵌入与冻结的 Stable Audio Open DiT 文本到音频（T2A）模型相连接，方法是在模型已有的文本跨注意力模块之后插入紧凑的视频跨注意力模块，从而使得文本提示设定全局语义，而视频则用于精炼时序和局部动态特性。冻结的骨干网络保留了强大的边缘分布（视频；给定文本的音频），而桥接结构则学习音频与视频之间所需的时间同步依赖关系——无需重新训练音频先验。为减少内存占用并稳定训练过程，我们在条件输入前对视频 token 进行池化处理。在精心构建的音视频基准数据集上，Foley Control 在参数量远少于近期多模态系统的情况下，实现了具有竞争力的时间与语义对齐性能，同时保持了基于提示的可控性以及便于实际生产的模块化特性（可替换或升级编码器或 T2A 骨干网络，无需端到端重新训练）。尽管我们主要聚焦于视频到音效（Video-to-Foley）任务，但该桥接结构设计同样可潜在扩展至其他音频模态（例如语音）。",
        "translated_title": "Foley Control：对齐冻结的潜在文本到音频模型以匹配视频",
        "label": [],
        "label_reason": "论文聚焦视频引导的音效生成，属于多模态音频合成，非图像像素级处理任务。",
        "relevance_score": 1,
        "novelty_score": 6,
        "novelty_reason": "提出轻量级跨模态桥接结构，但属于音频生成领域常规改进，非图像处理创新。"
    },
    {
        "title": "Scalable Vision-Language-Action Model Pretraining for Robotic\n  Manipulation with Real-Life Human Activity Videos",
        "url": "http://arxiv.org/abs/2510.21571v1",
        "pub_date": "2025-10-24",
        "summary": "This paper presents a novel approach for pretraining robotic manipulation Vision-Language-Action (VLA) models using a large corpus of unscripted real-life video recordings of human hand activities. Treating human hand as dexterous robot end-effector, we show that \"in-the-wild\" egocentric human videos without any annotations can be transformed into data formats fully aligned with existing robotic V-L-A training data in terms of task granularity and labels. This is achieved by the development of a fully-automated holistic human activity analysis approach for arbitrary human hand videos. This approach can generate atomic-level hand activity segments and their language descriptions, each accompanied with framewise 3D hand motion and camera motion. We process a large volume of egocentric videos and create a hand-VLA training dataset containing 1M episodes and 26M frames. This training data covers a wide range of objects and concepts, dexterous manipulation tasks, and environment variations in real life, vastly exceeding the coverage of existing robot data. We design a dexterous hand VLA model architecture and pretrain the model on this dataset. The model exhibits strong zero-shot capabilities on completely unseen real-world observations. Additionally, fine-tuning it on a small amount of real robot action data significantly improves task success rates and generalization to novel objects in real robotic experiments. We also demonstrate the appealing scaling behavior of the model's task performance with respect to pretraining data scale. We believe this work lays a solid foundation for scalable VLA pretraining, advancing robots toward truly generalizable embodied intelligence.",
        "translated": "本文提出了一种新颖的方法，利用大量未经脚本记录的真实人类手部活动视频对机器人操作视觉-语言-动作（VLA）模型进行预训练。将人类手部视为灵巧的机器人末端执行器，我们表明，无需任何标注的“野外”第一人称人类视频可以被转换为与现有机器人V-L-A训练数据在任务粒度和标签上完全对齐的数据格式。这一目标通过开发一种适用于任意人类手部视频的全自动整体性人类活动分析方法实现。该方法能够生成原子级的手部活动片段及其语言描述，每个片段均附带逐帧的3D手部运动和相机运动信息。我们处理了大量第一人称视频，构建了一个包含100万集、2600万帧的手部VLA训练数据集。该训练数据覆盖了现实生活中广泛存在的物体与概念、灵巧操作任务以及环境变化，其覆盖范围远超现有机器人数据集。我们设计了一种灵巧手部VLA模型架构，并在该数据集上进行预训练。该模型在完全未见过的真实世界观测中展现出强大的零样本能力。此外，在少量真实机器人动作数据上进行微调，可显著提升真实机器人实验中的任务成功率以及对新物体的泛化能力。我们还展示了模型任务性能随预训练数据规模增长而表现出的优异扩展特性。我们认为，本工作为可扩展的VLA预训练奠定了坚实基础，推动机器人向真正的泛化具身智能迈进。",
        "translated_title": "可扩展的视觉-语言-动作模型预训练用于基于真实人类活动视频的机器人操作",
        "label": [],
        "label_reason": "论文聚焦于机器人视觉-语言-动作模型预训练，属于高阶视觉任务，不涉及像素级图像恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出自动化人类活动分析方法并构建大规模数据集，对VLA模型预训练有显著创新。"
    },
    {
        "title": "CausalRec: A CausalBoost Attention Model for Sequential Recommendation",
        "url": "http://arxiv.org/abs/2510.21333v1",
        "pub_date": "2025-10-24",
        "summary": "Recent advances in correlation-based sequential recommendation systems have demonstrated substantial success. Specifically, the attention-based model outperforms other RNN-based and Markov chains-based models by capturing both short- and long-term dependencies more effectively. However, solely focusing on item co-occurrences overlooks the underlying motivations behind user behaviors, leading to spurious correlations and potentially inaccurate recommendations. To address this limitation, we present a novel framework that integrates causal attention for sequential recommendation, CausalRec. It incorporates a causal discovery block and a CausalBooster. The causal discovery block learns the causal graph in user behavior sequences, and we provide a theory to guarantee the identifiability of the learned causal graph. The CausalBooster utilizes the discovered causal graph to refine the attention mechanism, prioritizing behaviors with causal significance. Experimental evaluations on real-world datasets indicate that CausalRec outperforms several state-of-the-art methods, with average improvements of 7.21% in Hit Rate (HR) and 8.65% in Normalized Discounted Cumulative Gain (NDCG). To the best of our knowledge, this is the first model to incorporate causality through the attention mechanism in sequential recommendation, demonstrating the value of causality in generating more accurate and reliable recommendations.",
        "translated": "近年来，基于关联性的序列推荐系统取得了显著进展。特别是，基于注意力机制的模型通过更有效地捕捉用户行为序列中的短期和长期依赖关系，优于其他基于RNN和马尔可夫链的模型。然而，仅关注物料共现关系会忽略用户行为背后的潜在动机，从而导致虚假相关性，并可能产生不准确的推荐结果。为解决这一局限性，我们提出了一种新颖的框架——CausalRec，用于序列推荐中的因果注意力机制。该框架包含一个因果发现模块和一个CausalBooster模块。因果发现模块学习用户行为序列中的因果图，并我们提供了理论保证以确保所学习因果图的可识别性。CausalBooster模块利用所发现的因果图来优化注意力机制，优先考虑具有因果重要性的行为。在真实数据集上的实验评估表明，CausalRec优于多种最先进的方法，平均在命中率（HR）上提升7.21%，在归一化折损累计增益（NDCG）上提升8.65%。据我们所知，这是首个通过注意力机制引入因果性以进行序列推荐的模型，展示了因果性在生成更准确、更可靠推荐中的重要价值。",
        "translated_title": "CausalRec：一种用于序列推荐的因果增强注意力模型",
        "label": [
            "序列推荐",
            "因果推荐",
            "精排"
        ],
        "label_reason": "论文针对序列推荐提出因果注意力机制，显著提升推荐准确性，属于推荐系统核心环节",
        "relevance_score": 9,
        "novelty_score": 9,
        "novelty_reason": "首次将因果发现与注意力机制结合，提出CausalBooster模块，创新性强"
    },
    {
        "title": "Pctx: Tokenizing Personalized Context for Generative Recommendation",
        "url": "http://arxiv.org/abs/2510.21276v1",
        "pub_date": "2025-10-24",
        "summary": "Generative recommendation (GR) models tokenize each action into a few discrete tokens (called semantic IDs) and autoregressively generate the next tokens as predictions, showing advantages such as memory efficiency, scalability, and the potential to unify retrieval and ranking. Despite these benefits, existing tokenization methods are static and non-personalized. They typically derive semantic IDs solely from item features, assuming a universal item similarity that overlooks user-specific perspectives. However, under the autoregressive paradigm, semantic IDs with the same prefixes always receive similar probabilities, so a single fixed mapping implicitly enforces a universal item similarity standard across all users. In practice, the same item may be interpreted differently depending on user intentions and preferences. To address this issue, we propose a personalized context-aware tokenizer that incorporates a user's historical interactions when generating semantic IDs. This design allows the same item to be tokenized into different semantic IDs under different user contexts, enabling GR models to capture multiple interpretive standards and produce more personalized predictions. Experiments on three public datasets demonstrate up to 11.44% improvement in NDCG@10 over non-personalized action tokenization baselines. Our code is available at https://github.com/YoungZ365/Pctx.",
        "translated": "生成式推荐（GR）模型将每个行为编码为若干离散的token（称为语义ID），并以自回归方式生成下一个token作为预测，展现出内存高效、可扩展性强以及有望统一召回与排序等优势。尽管如此，现有token化方法仍为静态且非个性化。它们通常仅基于物料特征生成语义ID，假设存在一种通用的物料相似性标准，而忽略了用户特定视角。然而，在自回归范式下，具有相同前缀的语义ID始终会获得相似的概率，因此单一固定的映射隐式地强制所有用户遵循统一的物料相似性标准。在实际场景中，同一物料可能因用户意图和偏好不同而被赋予不同解读。为解决该问题，我们提出一种个性化上下文感知的token化器，该方法在生成语义ID时融入用户的历史交互信息。该设计允许同一物料在不同用户上下文中被编码为不同的语义ID，从而使得GR模型能够捕捉多种解读标准，生成更具个性化的预测结果。在三个公开数据集上的实验表明，相较于非个性化行为token化基线，我们的方法在NDCG@10指标上最高提升11.44%。我们的代码开源于https://github.com/YoungZ365/Pctx。",
        "translated_title": "Pctx：用于生成式推荐的个性化上下文令牌化",
        "label": [
            "LLM生成式推荐（LLM-based Generative Recommendation）",
            "序列推荐（Sequential Recommendation）",
            "通用推荐技术（General Recommendation Techniques）"
        ],
        "label_reason": "论文聚焦生成式推荐中的个性化上下文建模，直接提升推荐精度，属核心推荐问题。",
        "relevance_score": 10,
        "novelty_score": 9,
        "novelty_reason": "提出个性化上下文感知分词器，打破静态语义ID限制，实现用户特定的动态编码，创新性强。"
    },
    {
        "title": "Bi-Level Optimization for Generative Recommendation: Bridging\n  Tokenization and Generation",
        "url": "http://arxiv.org/abs/2510.21242v1",
        "pub_date": "2025-10-24",
        "summary": "Generative recommendation is emerging as a transformative paradigm by directly generating recommended items, rather than relying on matching. Building such a system typically involves two key components: (1) optimizing the tokenizer to derive suitable item identifiers, and (2) training the recommender based on those identifiers. Existing approaches often treat these components separately--either sequentially or in alternation--overlooking their interdependence. This separation can lead to misalignment: the tokenizer is trained without direct guidance from the recommendation objective, potentially yielding suboptimal identifiers that degrade recommendation performance.   To address this, we propose BLOGER, a Bi-Level Optimization for GEnerative Recommendation framework, which explicitly models the interdependence between the tokenizer and the recommender in a unified optimization process. The lower level trains the recommender using tokenized sequences, while the upper level optimizes the tokenizer based on both the tokenization loss and recommendation loss. We adopt a meta-learning approach to solve this bi-level optimization efficiently, and introduce gradient surgery to mitigate gradient conflicts in the upper-level updates, thereby ensuring that item identifiers are both informative and recommendation-aligned. Extensive experiments on real-world datasets demonstrate that BLOGER consistently outperforms state-of-the-art generative recommendation methods while maintaining practical efficiency with no significant additional computational overhead, effectively bridging the gap between item tokenization and autoregressive generation.",
        "translated": "生成式推荐正作为一种变革性范式崭露头角，其通过直接生成推荐物料，而非依赖匹配机制。构建此类系统通常包含两个关键组件：（1）优化分词器以获得合适的物料标识符；（2）基于这些标识符训练推荐模型。现有方法通常将这两个组件独立处理——无论是顺序进行还是交替训练——忽视了它们之间的相互依赖性。这种分离可能导致对齐偏差：分词器在缺乏推荐目标直接指导的情况下进行训练，可能生成次优的标识符，从而降低推荐性能。\n\n为解决这一问题，我们提出 BLOGER，即面向生成式推荐的双层优化框架，其显式地在统一优化过程中建模分词器与推荐器之间的相互依赖性。底层利用分词后的序列训练推荐器，而上层则基于分词损失和推荐损失共同优化分词器。我们采用元学习方法高效求解该双层优化问题，并引入梯度手术以缓解上层更新中的梯度冲突，从而确保物料标识符既具有信息性又与推荐目标对齐。在多个真实数据集上的大量实验表明，BLOGER 在保持实用效率且无显著额外计算开销的前提下，始终优于当前最先进的生成式推荐方法，有效弥合了物料分词与自回归生成之间的鸿沟。",
        "translated_title": "生成式推荐的双层优化：连接分词与生成",
        "label": [
            "LLM生成式推荐（LLM-based Generative Recommendation）",
            "通用推荐技术（General Recommendation Techniques）"
        ],
        "label_reason": "论文聚焦生成式推荐中tokenization与生成的联合优化，直接解决推荐系统核心问题。",
        "relevance_score": 10,
        "novelty_score": 9,
        "novelty_reason": "提出双层优化框架BLOGER，巧妙整合tokenizer与推荐器训练，显著提升生成式推荐性能。"
    },
    {
        "title": "VOGUE: A Multimodal Dataset for Conversational Recommendation in Fashion",
        "url": "http://arxiv.org/abs/2510.21151v1",
        "pub_date": "2025-10-24",
        "summary": "Multimodal conversational recommendation has emerged as a promising paradigm for delivering personalized experiences through natural dialogue enriched by visual and contextual grounding. Yet, current multimodal conversational recommendation datasets remain limited: existing resources either simulate conversations, omit user history, or fail to collect sufficiently detailed feedback, all of which constrain the types of research and evaluation they support.   To address these gaps, we introduce VOGUE, a novel dataset of 60 humanhuman dialogues in realistic fashion shopping scenarios. Each dialogue is paired with a shared visual catalogue, item metadata, user fashion profiles and histories, and post-conversation ratings from both Seekers and Assistants. This design enables rigorous evaluation of conversational inference, including not only alignment between predicted and ground-truth preferences, but also calibration against full rating distributions and comparison with explicit and implicit user satisfaction signals.   Our initial analyses of VOGUE reveal distinctive dynamics of visually grounded dialogue. For example, recommenders frequently suggest items simultaneously in feature-based groups, which creates distinct conversational phases bridged by Seeker critiques and refinements. Benchmarking multimodal large language models against human recommenders shows that while MLLMs approach human-level alignment in aggregate, they exhibit systematic distribution errors in reproducing human ratings and struggle to generalize preference inference beyond explicitly discussed items. These findings establish VOGUE as both a unique resource for studying multimodal conversational systems and as a challenge dataset beyond the current recommendation capabilities of existing top-tier multimodal foundation models such as GPT-4o-mini, GPT-5-mini, and Gemini-2.5-Flash.",
        "translated": "多模态对话推荐已成为通过融合视觉和上下文信息的自然对话实现个性化体验的有前景范式。然而，当前的多模态对话推荐数据集仍存在局限：现有资源要么模拟对话，要么忽略用户历史，要么未能收集足够详尽的反馈，这些都限制了其支持的研究类型和评估能力。\n\n为弥补这些空白，我们提出VOGUE，一个包含60组真实时尚购物场景中的人与人对话的新数据集。每组对话均配有共享的视觉商品目录、物品元数据、用户时尚画像与历史记录，以及对话结束后由Seeker和Assistant双方提供的评分。该设计支持对对话推理进行严格评估，不仅包括预测偏好与真实偏好之间的一致性，还可与完整的评分分布进行校准，并与显式和隐式用户满意度信号进行对比。\n\n对VOGUE的初步分析揭示了视觉化对话的独特动态。例如，推荐系统常以基于特征的分组方式同时推荐多个物品，从而形成由Seeker的批评与细化所连接的特定对话阶段。将多模态大语言模型与人类推荐者进行基准对比显示，尽管MLLMs在整体对齐度上接近人类水平，但其在复现人类评分分布时存在系统性偏差，并且在超越明确讨论物品的偏好推断方面表现不佳。这些发现确立了VOGUE作为研究多模态对话系统独特资源的地位，并表明其作为对现有顶级多模态基础模型（如GPT-4o-mini、GPT-5-mini和Gemini-2.5-Flash）推荐能力的挑战性数据集。",
        "translated_title": "VOGUE：一个面向时尚领域对话式推荐的多模态数据集",
        "label": [
            "多模态推荐",
            "LLM生成式推荐",
            "对话推荐",
            "推荐系统评估"
        ],
        "label_reason": "论文构建多模态对话推荐数据集，支持LLM生成式推荐评估，涉及推荐系统核心环节",
        "relevance_score": 8,
        "novelty_score": 8,
        "novelty_reason": "提出真实场景对话数据集VOGUE，支持多模态LLM评估，推动生成式推荐研究"
    },
    {
        "title": "From Questions to Queries: An AI-powered Multi-Agent Framework for\n  Spatial Text-to-SQL",
        "url": "http://arxiv.org/abs/2510.21045v1",
        "pub_date": "2025-10-23",
        "summary": "The complexity of Structured Query Language (SQL) and the specialized nature of geospatial functions in tools like PostGIS present significant barriers to non-experts seeking to analyze spatial data. While Large Language Models (LLMs) offer promise for translating natural language into SQL (Text-to-SQL), single-agent approaches often struggle with the semantic and syntactic complexities of spatial queries. To address this, we propose a multi-agent framework designed to accurately translate natural language questions into spatial SQL queries. The framework integrates several innovative components, including a knowledge base with programmatic schema profiling and semantic enrichment, embeddings for context retrieval, and a collaborative multi-agent pipeline as its core. This pipeline comprises specialized agents for entity extraction, metadata retrieval, query logic formulation, SQL generation, and a review agent that performs programmatic and semantic validation of the generated SQL to ensure correctness (self-verification). We evaluate our system using both the non-spatial KaggleDBQA benchmark and a new, comprehensive SpatialQueryQA benchmark that includes diverse geometry types, predicates, and three levels of query complexity. On KaggleDBQA, the system achieved an overall accuracy of 81.2% (221 out of 272 questions) after the review agent's review and corrections. For spatial queries, the system achieved an overall accuracy of 87.7% (79 out of 90 questions), compared with 76.7% without the review agent. Beyond accuracy, results also show that in some instances the system generates queries that are more semantically aligned with user intent than those in the benchmarks. This work makes spatial analysis more accessible, and provides a robust, generalizable foundation for spatial Text-to-SQL systems, advancing the development of autonomous GIS.",
        "translated": "结构化查询语言（SQL）的复杂性以及在PostGIS等工具中地理空间函数的专业性，为非专业人士分析空间数据带来了显著障碍。尽管大语言模型（LLM）在将自然语言转化为SQL（文本到SQL）方面展现出潜力，但单智能体方法在处理空间查询的语义和语法复杂性时往往表现不佳。为此，我们提出了一种多智能体框架，旨在准确地将自然语言问题转化为空间SQL查询。该框架集成了多个创新组件，包括具有程序化模式剖析和语义增强的知识库、用于上下文检索的嵌入，以及作为核心的协作式多智能体流水线。该流水线包含专门用于实体提取、元数据检索、查询逻辑构建、SQL生成的智能体，以及一个审查智能体，该智能体对生成的SQL进行程序化和语义验证以确保其正确性（自验证）。我们使用非空间的KaggleDBQA基准测试和一个新的综合性SpatialQueryQA基准测试对系统进行了评估，后者涵盖了多种几何类型、谓词以及三个层次的查询复杂度。在KaggleDBQA上，系统在审查智能体进行审查和修正后，整体准确率达到81.2%（272个问题中正确回答221个）。对于空间查询，系统整体准确率达到87.7%（90个问题中正确回答79个），而未使用审查智能体时准确率为76.7%。除了准确率外，结果还表明，在某些情况下，系统生成的查询在语义上比基准测试中的查询更符合用户意图。本工作使空间分析更加易于访问，并为空间文本到SQL系统提供了稳健、可泛化的基础，推动了自主地理信息系统（GIS）的发展。",
        "translated_title": "从问题到查询：一种用于空间文本到SQL的AI驱动多智能体框架",
        "label": [],
        "label_reason": "论文聚焦于自然语言到SQL的转换，属于数据库查询生成，与推荐系统无直接关联。",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出多智能体协作框架，结合知识库与验证机制，改进了Text-to-SQL的准确性与鲁棒性。"
    },
    {
        "title": "Communication Platform for Non-verbal Autistic children in Oman using\n  Android mobile",
        "url": "http://arxiv.org/abs/2510.21028v1",
        "pub_date": "2025-10-23",
        "summary": "This paper discusses the issue regarding Non-verbal Autism Spectrum Disorder. It has been observed that this mental disorder is listed in major parts of the world including the US, UK, and India. To mitigate this type of disorder, a wide range of smartphones, computers, and artificial intelligence technologies have been used. This technology has helped the population cope with socialization and communication needs. Many applications have been developed to enhance the communication capabilities of non-verbal autistic children. This thesis project proposes the development of a platform that includes a web panel and an Android mobile application to assist non-verbal autistic children in communication, especially in Oman. Different interventions have been merged to improve the quality of life for people on the autism spectrum. The main problem identified in this case is that fragmented approaches are not suitable for autistic children. The augmented reality framework provides the capability to engage autistic children in creative play and self-reflection through interactive screen-based activities.",
        "translated": "本文讨论了非言语型自闭症谱系障碍（Autism Spectrum Disorder）相关的问题。观察发现，该精神障碍在世界多个地区（包括美国、英国和印度）均有广泛报道。为缓解此类障碍，人们已广泛采用智能手机、计算机及人工智能技术。这些技术帮助患者应对社交与沟通需求。许多应用程序已被开发，以提升非言语型自闭症儿童的沟通能力。本论文项目提出开发一个包含网页管理面板和安卓移动应用程序的平台，旨在协助非言语型自闭症儿童进行沟通，尤其适用于阿曼地区。多种干预措施被整合，以提升自闭症谱系人群的生活质量。本研究识别出的主要问题是，碎片化的干预方法并不适合自闭症儿童。增强现实框架通过基于交互式屏幕的活动，使自闭症儿童能够参与创造性游戏和自我反思，从而提供有效的支持。",
        "translated_title": "基于安卓移动设备的阿曼非语言自闭症儿童沟通平台",
        "label": [],
        "label_reason": "论文聚焦于自闭症儿童沟通辅助平台，与推荐系统无直接关联，属教育科技应用。",
        "relevance_score": 1,
        "novelty_score": 2,
        "novelty_reason": "采用AR框架增强互动，属于应用层面常规设计，无推荐系统领域创新。"
    },
    {
        "title": "Gaussian Mixture Flow Matching with Domain Alignment for Multi-Domain\n  Sequential Recommendation",
        "url": "http://arxiv.org/abs/2510.21021v1",
        "pub_date": "2025-10-23",
        "summary": "Users increasingly interact with content across multiple domains, resulting in sequential behaviors marked by frequent and complex transitions. While Cross-Domain Sequential Recommendation (CDSR) models two-domain interactions, Multi-Domain Sequential Recommendation (MDSR) introduces significantly more domain transitions, compounded by challenges such as domain heterogeneity and imbalance. Existing approaches often overlook the intricacies of domain transitions, tend to overfit to dense domains while underfitting sparse ones, and struggle to scale effectively as the number of domains increases. We propose \\textit{GMFlowRec}, an efficient generative framework for MDSR that models domain-aware transition trajectories via Gaussian Mixture Flow Matching. GMFlowRec integrates: (1) a unified dual-masked Transformer to disentangle domain-invariant and domain-specific intents, (2) a Gaussian Mixture flow field to capture diverse behavioral patterns, and (3) a domain-aligned prior to support frequent and sparse transitions. Extensive experiments on JD and Amazon datasets demonstrate that GMFlowRec achieves state-of-the-art performance with up to 44\\% improvement in NDCG@5, while maintaining high efficiency via a single unified backbone, making it scalable for real-world multi-domain sequential recommendation.",
        "translated": "用户在多个领域内日益频繁地交互内容，导致其序列行为呈现出频繁且复杂的领域转换特征。虽然跨领域序列推荐（CDSR）建模了两个领域的交互，但多领域序列推荐（MDSR）引入了显著更多的领域转换，并伴随领域异质性与不平衡等挑战。现有方法往往忽视领域转换的复杂性，倾向于对密集领域过拟合而对稀疏领域欠拟合，且随着领域数量增加，难以有效扩展。我们提出 \\textit{GMFlowRec}，一种高效的生成式框架用于MDSR，通过高斯混合流匹配（Gaussian Mixture Flow Matching）建模领域感知的转换轨迹。GMFlowRec集成以下三部分：(1) 统一的双掩码Transformer，以解耦领域不变与领域特定的意图；(2) 高斯混合流场，以捕捉多样的行为模式；(3) 领域对齐先验，以支持频繁与稀疏的领域转换。在JD和Amazon数据集上的大量实验表明，GMFlowRec在NDCG@5指标上实现了最高达44%的性能提升，同时通过单一统一的主干结构保持了高效率，使其适用于实际场景中的多领域序列推荐。",
        "translated_title": "高斯混合流匹配与领域对齐的多领域序列推荐",
        "label": [
            "序列推荐",
            "多模态推荐",
            "通用推荐技术"
        ],
        "label_reason": "论文针对多域序列推荐提出生成式框架，建模跨域行为轨迹，直接解决推荐系统核心问题",
        "relevance_score": 9,
        "novelty_score": 8,
        "novelty_reason": "创新性引入高斯混合流匹配建模行为轨迹，结合域对齐先验，提升跨域推荐性能"
    },
    {
        "title": "LimRank: Less is More for Reasoning-Intensive Information Reranking",
        "url": "http://arxiv.org/abs/2510.23544v1",
        "pub_date": "2025-10-27",
        "summary": "Existing approaches typically rely on large-scale fine-tuning to adapt LLMs for information reranking tasks, which is computationally expensive. In this work, we demonstrate that modern LLMs can be effectively adapted using only minimal, high-quality supervision. To enable this, we design LIMRANK-SYNTHESIZER, a reusable and open-source pipeline for generating diverse, challenging, and realistic reranking examples. Using this synthetic data, we fine-tune our reranker model, LIMRANK. We evaluate LIMRANK on two challenging benchmarks, i.e., BRIGHT for reasoning-intensive retrieval and FollowIR for instruction-following retrieval. Our experiments demonstrate that LIMRANK achieves competitive performance, while being trained on less than 5% of the data typically used in prior work. Further ablation studies demonstrate the effectiveness of LIMRANK-SYNTHESIZER and the strong generalization capabilities of LIMRANK across downstream tasks, including scientific literature search and retrieval-augmented generation for knowledge-intensive problem solving.",
        "translated": "现有方法通常依赖大规模微调来适配大语言模型（LLM）以完成信息重排任务，计算成本较高。在本研究中，我们证明现代大语言模型仅需少量高质量监督即可有效适配。为此，我们设计了 LIMRANK-SYNTHESIZER，一个可复用且开源的流水线，用于生成多样化、具有挑战性且贴近真实场景的重排示例。基于该合成数据，我们微调了重排模型 LIMRANK。我们在两个具有挑战性的基准数据集上评估了 LIMRANK，即用于推理密集型检索的 BRIGHT 和用于指令跟随型检索的 FollowIR。实验结果表明，LIMRANK 在仅使用先前工作通常所需数据量不到 5% 的情况下，仍能取得具有竞争力的性能。进一步的消融实验验证了 LIMRANK-SYNTHESIZER 的有效性，以及 LIMRANK 在下游任务中的强大泛化能力，涵盖科学文献检索和知识密集型问题求解中的检索增强生成等场景。",
        "translated_title": "LimRank：少即是多——面向推理密集型信息重排的高效方法",
        "label": [
            "重排（Re-ranking）",
            "LLM生成式推荐（LLM-based Generative Recommendation）"
        ],
        "label_reason": "论文聚焦LLM在信息重排任务中的高效微调，直接应用于推荐系统的重排环节，具备推荐系统相关性。",
        "relevance_score": 8,
        "novelty_score": 7,
        "novelty_reason": "提出轻量级微调策略与合成数据生成框架，提升效率与泛化能力，属方法层面创新。"
    },
    {
        "title": "Accurate and Scalable Multimodal Pathology Retrieval via Attentive\n  Vision-Language Alignment",
        "url": "http://arxiv.org/abs/2510.23224v1",
        "pub_date": "2025-10-27",
        "summary": "The rapid digitization of histopathology slides has opened up new possibilities for computational tools in clinical and research workflows. Among these, content-based slide retrieval stands out, enabling pathologists to identify morphologically and semantically similar cases, thereby supporting precise diagnoses, enhancing consistency across observers, and assisting example-based education. However, effective retrieval of whole slide images (WSIs) remains challenging due to their gigapixel scale and the difficulty of capturing subtle semantic differences amid abundant irrelevant content. To overcome these challenges, we present PathSearch, a retrieval framework that unifies fine-grained attentive mosaic representations with global-wise slide embeddings aligned through vision-language contrastive learning. Trained on a corpus of 6,926 slide-report pairs, PathSearch captures both fine-grained morphological cues and high-level semantic patterns to enable accurate and flexible retrieval. The framework supports two key functionalities: (1) mosaic-based image-to-image retrieval, ensuring accurate and efficient slide research; and (2) multi-modal retrieval, where text queries can directly retrieve relevant slides. PathSearch was rigorously evaluated on four public pathology datasets and three in-house cohorts, covering tasks including anatomical site retrieval, tumor subtyping, tumor vs. non-tumor discrimination, and grading across diverse organs such as breast, lung, kidney, liver, and stomach. External results show that PathSearch outperforms traditional image-to-image retrieval frameworks. A multi-center reader study further demonstrates that PathSearch improves diagnostic accuracy, boosts confidence, and enhances inter-observer agreement among pathologists in real clinical scenarios. These results establish PathSearch as a scalable and generalizable retrieval solution for digital pathology.",
        "translated": "组织病理切片的快速数字化为临床和研究工作流程中的计算工具开辟了新的可能性。其中，基于内容的切片检索尤为突出，使病理学家能够识别形态和语义上相似的病例，从而支持精准诊断、提升观察者间的一致性，并辅助基于实例的教学。然而，由于全切片图像（WSIs）具有数十亿像素的规模，且在大量无关内容中捕捉细微语义差异存在困难，因此有效检索全切片图像仍面临挑战。为克服这些挑战，我们提出PathSearch，一种融合了细粒度注意力马赛克表示与全局切片嵌入的检索框架，通过视觉-语言对比学习对齐二者。PathSearch在包含6,926个切片-报告对的语料库上进行训练，能够同时捕捉细粒度形态特征和高层次语义模式，从而实现准确且灵活的检索。该框架支持两大核心功能：（1）基于马赛克的图像到图像检索，确保切片检索的准确性和效率；（2）多模态检索，允许文本查询直接检索相关切片。PathSearch在四个公开病理数据集和三个内部队列上进行了严格评估，涵盖的任务包括解剖部位检索、肿瘤亚型识别、肿瘤与非肿瘤区分以及在乳腺、肺、肾、肝和胃等不同器官中的分级任务。外部评估结果表明，PathSearch优于传统的图像到图像检索框架。一项多中心读者研究进一步证明，在真实临床场景中，PathSearch能够提高诊断准确性、增强病理学家的信心，并提升观察者间的一致性。这些结果确立了PathSearch作为数字病理学中可扩展且具备泛化能力的检索解决方案的地位。",
        "translated_title": "基于注意力机制的视觉-语言对齐实现准确且可扩展的多模态病理学检索",
        "label": [],
        "label_reason": "论文聚焦医学图像检索，属计算机视觉与医疗信息学，与推荐系统无直接关联。",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出注意力机制与对比学习结合的新框架，方法新颖，但应用于医学图像检索而非推荐。"
    },
    {
        "title": "Leveraging Hierarchical Organization for Medical Multi-document\n  Summarization",
        "url": "http://arxiv.org/abs/2510.23104v1",
        "pub_date": "2025-10-27",
        "summary": "Medical multi-document summarization (MDS) is a complex task that requires effectively managing cross-document relationships. This paper investigates whether incorporating hierarchical structures in the inputs of MDS can improve a model's ability to organize and contextualize information across documents compared to traditional flat summarization methods. We investigate two ways of incorporating hierarchical organization across three large language models (LLMs), and conduct comprehensive evaluations of the resulting summaries using automated metrics, model-based metrics, and domain expert evaluation of preference, understandability, clarity, complexity, relevance, coverage, factuality, and coherence. Our results show that human experts prefer model-generated summaries over human-written summaries. Hierarchical approaches generally preserve factuality, coverage, and coherence of information, while also increasing human preference for summaries. Additionally, we examine whether simulated judgments from GPT-4 align with human judgments, finding higher agreement along more objective evaluation facets. Our findings demonstrate that hierarchical structures can improve the clarity of medical summaries generated by models while maintaining content coverage, providing a practical way to improve human preference for generated summaries.",
        "translated": "医学多文档摘要（MDS）是一项复杂的任务，需要有效管理跨文档之间的关系。本文探讨了在MDS输入中引入层次结构是否能够相比传统平面摘要方法，提升模型在跨文档组织和上下文化信息方面的能力。我们研究了在三种大语言模型（LLMs）中引入层次组织的两种方式，并通过自动化指标、基于模型的指标以及领域专家对偏好、可理解性、清晰度、复杂度、相关性、覆盖率、事实性及连贯性等方面的评估，对生成的摘要进行了全面评测。实验结果表明，人类专家更偏好模型生成的摘要而非人工撰写的摘要。层次化方法通常能保持信息的事实性、覆盖率和连贯性，同时提升人类对摘要的偏好程度。此外，我们还考察了GPT-4模拟判断与人类判断的一致性，发现在更客观的评估维度上一致性更高。我们的研究结果表明，层次结构能够提升模型生成医学摘要的清晰度，同时保持内容覆盖率，为提升生成摘要的人类偏好提供了一种实用途径。",
        "translated_title": "利用层级结构进行医学多文档摘要",
        "label": [],
        "label_reason": "论文聚焦医学多文档摘要，属NLP任务，与推荐系统无直接关联。",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出层次化输入结构提升摘要质量，属NLP领域常规改进，非推荐系统创新。"
    },
    {
        "title": "Think before Recommendation: Autonomous Reasoning-enhanced Recommender",
        "url": "http://arxiv.org/abs/2510.23077v1",
        "pub_date": "2025-10-27",
        "summary": "The core task of recommender systems is to learn user preferences from historical user-item interactions. With the rapid development of large language models (LLMs), recent research has explored leveraging the reasoning capabilities of LLMs to enhance rating prediction tasks. However, existing distillation-based methods suffer from limitations such as the teacher model's insufficient recommendation capability, costly and static supervision, and superficial transfer of reasoning ability. To address these issues, this paper proposes RecZero, a reinforcement learning (RL)-based recommendation paradigm that abandons the traditional multi-model and multi-stage distillation approach. Instead, RecZero trains a single LLM through pure RL to autonomously develop reasoning capabilities for rating prediction. RecZero consists of two key components: (1) \"Think-before-Recommendation\" prompt construction, which employs a structured reasoning template to guide the model in step-wise analysis of user interests, item features, and user-item compatibility; and (2) rule-based reward modeling, which adopts group relative policy optimization (GRPO) to compute rewards for reasoning trajectories and optimize the LLM. Additionally, the paper explores a hybrid paradigm, RecOne, which combines supervised fine-tuning with RL, initializing the model with cold-start reasoning samples and further optimizing it with RL. Experimental results demonstrate that RecZero and RecOne significantly outperform existing baseline methods on multiple benchmark datasets, validating the superiority of the RL paradigm in achieving autonomous reasoning-enhanced recommender systems.",
        "translated": "推荐系统的核心任务是从历史用户-物料交互中学习用户偏好。随着大语言模型（LLM）的快速发展，近期研究探索了利用LLM的推理能力来增强评分预测任务。然而，现有的基于蒸馏的方法存在诸多局限，例如教师模型推荐能力不足、监督成本高且静态、推理能力转移流于表面等。为解决这些问题，本文提出RecZero，一种基于强化学习（RL）的推荐范式，摒弃了传统的多模型、多阶段蒸馏方法。RecZero通过纯强化学习训练单个LLM，使其自主发展用于评分预测的推理能力。RecZero包含两个关键组件：（1）“先思考后推荐”提示构造，采用结构化推理模板，引导模型逐步分析用户兴趣、物料特征以及用户-物料匹配度；（2）基于规则的奖励建模，采用群体相对策略优化（GRPO）计算推理路径的奖励，并优化LLM。此外，本文还探索了一种混合范式RecOne，结合监督微调与强化学习，利用冷启动推理样本初始化模型，并进一步通过强化学习进行优化。实验结果表明，RecZero和RecOne在多个基准数据集上显著优于现有基线方法，验证了强化学习范式在实现具备自主推理能力的推荐系统方面的优越性。",
        "translated_title": "推荐前思考：自主推理增强的推荐系统",
        "label": [
            "LLM生成式推荐（LLM-based Generative Recommendation）",
            "精排（Ranking）",
            "通用推荐技术（General Recommendation Techniques）"
        ],
        "label_reason": "论文提出基于强化学习的LLM自主推理推荐框架，直接提升精排阶段的预测能力，属推荐系统核心环节。",
        "relevance_score": 10,
        "novelty_score": 9,
        "novelty_reason": "首创纯强化学习训练LLM进行推荐推理，脱离传统蒸馏范式，具备显著创新性与技术突破。"
    },
    {
        "title": "Multi-Stage Field Extraction of Financial Documents with OCR and Compact\n  Vision-Language Models",
        "url": "http://arxiv.org/abs/2510.23066v1",
        "pub_date": "2025-10-27",
        "summary": "Financial documents are essential sources of information for regulators, auditors, and financial institutions, particularly for assessing the wealth and compliance of Small and Medium-sized Businesses. However, SMB documents are often difficult to parse. They are rarely born digital and instead are distributed as scanned images that are none machine readable. The scans themselves are low in resolution, affected by skew or rotation, and often contain noisy backgrounds. These documents also tend to be heterogeneous, mixing narratives, tables, figures, and multilingual content within the same report. Such characteristics pose major challenges for automated information extraction, especially when relying on end to end large Vision Language Models, which are computationally expensive, sensitive to noise, and slow when applied to files with hundreds of pages.   We propose a multistage pipeline that leverages traditional image processing models and OCR extraction, together with compact VLMs for structured field extraction of large-scale financial documents. Our approach begins with image pre-processing, including segmentation, orientation detection, and size normalization. Multilingual OCR is then applied to recover page-level text. Upon analyzing the text information, pages are retrieved for coherent sections. Finally, compact VLMs are operated within these narrowed-down scopes to extract structured financial indicators.   Our approach is evaluated using an internal corpus of multi-lingual, scanned financial documents. The results demonstrate that compact VLMs, together with a multistage pipeline, achieves 8.8 times higher field level accuracy relative to directly feeding the whole document into large VLMs, only at 0.7 percent of the GPU cost and 92.6 percent less end-to-end service latency.",
        "translated": "财务文件是监管机构、审计师及金融机构的重要信息来源，尤其在评估中小型企业（SMB）的资产状况与合规性方面具有关键作用。然而，SMB的文件通常难以解析：它们极少以数字形式生成，而是以扫描图像形式分发，且不具备机器可读性。这些扫描件分辨率较低，常存在倾斜或旋转问题，且背景通常含有噪声。此外，这些文档往往具有异构性，同一个报告中混合了叙述文本、表格、图表以及多语言内容。这些特征给自动化信息抽取带来了重大挑战，尤其是在依赖端到端大语言模型（VLM）时，因为此类模型在处理数百页文件时计算成本高昂、对噪声敏感且运行速度缓慢。\n\n我们提出了一种多阶段流水线方法，结合传统图像处理模型与OCR提取技术，并利用紧凑型视觉语言模型（VLM）实现大规模财务文档的结构化字段抽取。该方法首先进行图像预处理，包括分割、方向检测与尺寸归一化。随后，采用多语言OCR技术恢复页面级文本内容。在分析文本信息后，对页面进行检索，以定位语义连贯的章节。最后，在这些缩小的上下文范围内应用紧凑型VLM，提取结构化财务指标。\n\n本方法在内部构建的多语言扫描财务文档语料库上进行了评估。实验结果表明，相较于直接将整份文档输入大VLM，所提出的多阶段流水线与紧凑型VLM相结合的方法，在字段级准确率上提升了8.8倍，同时仅消耗0.7%的GPU计算成本，端到端服务延迟减少了92.6%。",
        "translated_title": "多阶段金融文档字段提取：基于OCR与紧凑型视觉-语言模型的方法",
        "label": [],
        "label_reason": "论文聚焦金融文档信息抽取，涉及OCR与视觉语言模型，与推荐系统无直接关联。",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出多阶段处理流程，结合轻量VLM与OCR，提升效率与精度，属常规改进。"
    },
    {
        "title": "Improving Product Search Relevance with EAR-MP: A Solution for the CIKM\n  2025 AnalytiCup",
        "url": "http://arxiv.org/abs/2510.23018v1",
        "pub_date": "2025-10-27",
        "summary": "Multilingual e-commerce search is challenging due to linguistic diversity and the noise inherent in user-generated queries. This paper documents the solution employed by our team (EAR-MP) for the CIKM 2025 AnalytiCup, which addresses two core tasks: Query-Category (QC) relevance and Query-Item (QI) relevance. Our approach first normalizes the multilingual dataset by translating all text into English, then mitigates noise through extensive data cleaning and normalization. For model training, we build on DeBERTa-v3-large and improve performance with label smoothing, self-distillation, and dropout. In addition, we introduce task-specific upgrades, including hierarchical token injection for QC and a hybrid scoring mechanism for QI. Under constrained compute, our method achieves competitive results, attaining an F1 score of 0.8796 on QC and 0.8744 on QI. These findings underscore the importance of systematic data preprocessing and tailored training strategies for building robust, resource-efficient multilingual relevance systems.",
        "translated": "多语言电商搜索因语言多样性以及用户生成查询中固有的噪声而面临挑战。本文记录了我们团队（EAR-MP）在CIKM 2025 AnalytiCup竞赛中采用的解决方案，该方案针对两个核心任务：查询-类别（QC）相关性和查询-物料（QI）相关性。我们的方法首先通过对所有文本进行翻译，将多语言数据集统一归一化为英文，随后通过大规模的数据清洗与归一化处理降低噪声。在模型训练方面，我们基于DeBERTa-v3-large架构，并通过标签平滑、自蒸馏和Dropout技术提升性能。此外，我们引入了针对任务的优化策略，包括面向QC任务的层次化token注入，以及面向QI任务的混合打分机制。在计算资源受限的条件下，我们的方法取得了具有竞争力的结果，在QC任务上F1得分为0.8796，在QI任务上F1得分为0.8744。这些发现强调了系统性数据预处理与定制化训练策略在构建鲁棒且资源高效的多语言相关性系统中的重要性。",
        "translated_title": "提升产品搜索相关性：EAR-MP 解决方案在 CIKM 2025 AnalytiCup 中的应用",
        "label": [],
        "label_reason": "论文聚焦多语言电商搜索相关性，属于信息检索范畴，与推荐系统无直接关联。",
        "relevance_score": 3,
        "novelty_score": 5,
        "novelty_reason": "采用DeBERTa改进策略，属常规优化，无突破性创新。"
    },
    {
        "title": "Tagging-Augmented Generation: Assisting Language Models in Finding\n  Intricate Knowledge In Long Contexts",
        "url": "http://arxiv.org/abs/2510.22956v1",
        "pub_date": "2025-10-27",
        "summary": "Recent investigations into effective context lengths of modern flagship large language models (LLMs) have revealed major limitations in effective question answering (QA) and reasoning over long and complex contexts for even the largest and most impressive cadre of models. While approaches like retrieval-augmented generation (RAG) and chunk-based re-ranking attempt to mitigate this issue, they are sensitive to chunking, embedding and retrieval strategies and models, and furthermore, rely on extensive pre-processing, knowledge acquisition and indexing steps. In this paper, we propose Tagging-Augmented Generation (TAG), a lightweight data augmentation strategy that boosts LLM performance in long-context scenarios, without degrading and altering the integrity and composition of retrieved documents. We validate our hypothesis by augmenting two challenging and directly relevant question-answering benchmarks -- NoLima and NovelQA -- and show that tagging the context or even just adding tag definitions into QA prompts leads to consistent performance gains over the baseline -- up to 17% for 32K token contexts, and 2.9% in complex reasoning question-answering for multi-hop queries requiring knowledge across a wide span of text. Additional details are available at https://sites.google.com/view/tag-emnlp.",
        "translated": "近期对现代主流大语言模型（LLM）有效上下文长度的研究表明，即使是最先进、规模最大的模型，在处理长且复杂的上下文时，其在问答（QA）和推理任务上的表现仍存在显著局限。尽管诸如检索增强生成（RAG）和基于片段的重排等方法试图缓解这一问题，但它们对片段划分、嵌入和检索策略及模型高度敏感，且依赖大量预处理、知识获取和索引步骤。本文提出一种轻量级数据增强策略——标签增强生成（TAG），用于提升LLM在长上下文场景下的性能，同时不破坏检索文档的完整性和结构组成。我们通过增强两个具有挑战性且直接相关的问答基准数据集——NoLima 和 NovelQA——验证了该假设，结果表明，在问答提示中对上下文进行标签标注，或仅添加标签定义，均能相对于基线模型带来稳定的性能提升：在32K token上下文场景下最高提升达17%，在需要跨越广泛文本跨度的知识多跳推理问答任务中提升2.9%。更多细节可参见 https://sites.google.com/view/tag-emnlp。",
        "translated_title": "标签增强生成：辅助语言模型在长上下文中发现复杂知识",
        "label": [],
        "label_reason": "论文聚焦LLM长上下文问答增强，与推荐系统无直接关联，属通用NLP任务。",
        "relevance_score": 3,
        "novelty_score": 6,
        "novelty_reason": "提出轻量级标签增强策略，提升长文本理解，属常规改进，非核心推荐创新。"
    },
    {
        "title": "GTR-Mamba: Geometry-to-Tangent Routing for Hyperbolic POI Recommendation",
        "url": "http://arxiv.org/abs/2510.22942v1",
        "pub_date": "2025-10-27",
        "summary": "Next Point-of-Interest (POI) recommendation is a critical task in modern Location-Based Social Networks (LBSNs), aiming to model the complex decision-making process of human mobility to provide personalized recommendations for a user's next check-in location. Existing POI recommendation models, predominantly based on Graph Neural Networks and sequential models, have been extensively studied. However, these models face a fundamental limitation: they struggle to simultaneously capture the inherent hierarchical structure of spatial choices and the dynamics and irregular shifts of user-specific temporal contexts. To overcome this limitation, we propose GTR-Mamba, a novel framework for cross-manifold conditioning and routing. GTR-Mamba leverages the distinct advantages of different mathematical spaces for different tasks: it models the static, tree-like preference hierarchies in hyperbolic geometry, while routing the dynamic sequence updates to a novel Mamba layer in the computationally stable and efficient Euclidean tangent space. This process is coordinated by a cross-manifold channel that fuses spatio-temporal information to explicitly steer the State Space Model (SSM), enabling flexible adaptation to contextual changes. Extensive experiments on three real-world datasets demonstrate that GTR-Mamba consistently outperforms state-of-the-art baseline models in next POI recommendation.",
        "translated": "下一点兴趣点（POI）推荐是现代基于位置的社交网络（LBSNs）中的关键任务，旨在建模人类移动行为的复杂决策过程，为用户提供个性化的下一个签到地点推荐。现有的POI推荐模型主要基于图神经网络和序列模型，已被广泛研究。然而，这些模型面临一个根本性局限：难以同时捕捉空间选择的内在层次结构，以及用户特定时间上下文的动态性和不规则变化。为克服这一局限，我们提出GTR-Mamba，一种新型的跨流形条件与路由框架。GTR-Mamba利用不同数学空间在不同任务中的独特优势：在双曲几何空间中建模静态、树状的偏好层次结构，同时将动态序列更新路由至计算稳定且高效的欧几里得切空间中的新型Mamba层。该过程由一个跨流形通道协调，该通道融合时空信息以显式引导状态空间模型（SSM），从而实现对上下文变化的灵活适应。在三个真实数据集上的大量实验表明，GTR-Mamba在下一点POI推荐任务中始终优于当前最先进的基线模型。",
        "translated_title": "GTR-Mamba：用于超球面POI推荐的几何到切空间路由机制",
        "label": [
            "序列推荐",
            "图神经网络推荐",
            "通用推荐技术"
        ],
        "label_reason": "论文聚焦POI推荐，融合超球面几何与Mamba序列建模，解决时空动态建模问题，属于推荐系统核心环节。",
        "relevance_score": 8,
        "novelty_score": 8,
        "novelty_reason": "提出跨流形路由机制，结合超球面与欧氏空间建模，创新性地融合几何与序列建模，提升推荐动态适应性。"
    },
    {
        "title": "MGFRec: Towards Reinforced Reasoning Recommendation with Multiple\n  Groundings and Feedback",
        "url": "http://arxiv.org/abs/2510.22888v1",
        "pub_date": "2025-10-27",
        "summary": "The powerful reasoning and generative capabilities of large language models (LLMs) have inspired researchers to apply them to reasoning-based recommendation tasks, which require in-depth reasoning about user interests and the generation of recommended items. However, previous reasoning-based recommendation methods have typically performed inference within the language space alone, without incorporating the actual item space. This has led to over-interpreting user interests and deviating from real items. Towards this research gap, we propose performing multiple rounds of grounding during inference to help the LLM better understand the actual item space, which could ensure that its reasoning remains aligned with real items. Furthermore, we introduce a user agent that provides feedback during each grounding step, enabling the LLM to better recognize and adapt to user interests. Comprehensive experiments conducted on three Amazon review datasets demonstrate the effectiveness of incorporating multiple groundings and feedback. These findings underscore the critical importance of reasoning within the actual item space, rather than being confined to the language space, for recommendation tasks.",
        "translated": "大语言模型（LLM）强大的推理与生成能力激发了研究者将其应用于基于推理的推荐任务，这类任务要求对用户兴趣进行深度推理，并生成推荐物料。然而，以往的基于推理的推荐方法通常仅在语言空间内执行推理，未融入真实的物料空间，导致对用户兴趣的过度解读，并偏离真实物料。针对这一研究空白，我们提出在推理过程中执行多轮接地操作，以帮助LLM更好地理解真实物料空间，从而确保其推理过程与真实物料保持一致。此外，我们引入了一个用户代理，在每一阶段的接地过程中提供反馈，使LLM能够更好地识别并适应用户兴趣。在三个Amazon评论数据集上进行的综合性实验表明，引入多轮接地与反馈机制具有显著有效性。这些发现凸显了在推荐任务中，推理应基于真实物料空间，而非局限于语言空间，具有关键重要性。",
        "translated_title": "MGFRec：基于多锚点与反馈的强化推理推荐",
        "label": [
            "LLM生成式推荐（LLM-based Generative Recommendation）",
            "序列推荐（Sequential Recommendation）",
            "召回（Recall）"
        ],
        "label_reason": "论文聚焦LLM在推荐中推理与生成，通过多轮接地和用户反馈增强推荐准确性，直接关联推荐系统核心环节。",
        "relevance_score": 9,
        "novelty_score": 8,
        "novelty_reason": "提出多轮接地与用户反馈机制，提升LLM在真实物品空间的推理能力，改进现有生成式推荐方法。"
    },
    {
        "title": "Civic Ground Truth in News Recommenders: A Method for Public Value\n  Scoring",
        "url": "http://arxiv.org/abs/2510.22865v1",
        "pub_date": "2025-10-26",
        "summary": "Research in news recommendation systems (NRS) continues to explore the best ways to integrate normative goals such as editorial objectives and public service values into existing systems. Prior efforts have incorporated expert input or audience feedback to quantify these values, laying the groundwork for more civic-minded recommender systems. This paper contributes to that trajectory, introducing a method for embedding civic values into NRS through large-scale, structured audience evaluations. The proposed civic ground truth approach aims to generate value-based labels through a nationally representative survey that are generalisable across a wider news corpus, using automated metadata enrichment.",
        "translated": "新闻推荐系统（NRS）的研究持续探索将规范性目标（如编辑目标与公共服务价值）融入现有系统中的最佳方式。先前的研究已通过引入专家意见或受众反馈来量化这些价值，为构建更具公共责任感的推荐系统奠定了基础。本文沿着这一研究路径做出贡献，提出一种通过大规模、结构化的受众评估将公共价值嵌入NRS的方法。所提出的公共价值基准方法旨在通过一项具有全国代表性的调查生成基于价值的标签，并利用自动化元数据增强技术，使这些标签能够推广至更广泛的新闻语料库。",
        "translated_title": "新闻推荐系统中的公民事实真相：一种公共价值评分方法",
        "label": [
            "通用推荐技术",
            "推荐系统评估"
        ],
        "label_reason": "论文聚焦新闻推荐中公共价值评估，提出基于大规模受众调查的地面真值构建方法，属推荐系统评估与价值导向技术。",
        "relevance_score": 7,
        "novelty_score": 6,
        "novelty_reason": "方法基于调查数据与自动化元数据增强，属现有评估框架的扩展，无颠覆性创新。"
    },
    {
        "title": "REVISION:Reflective Intent Mining and Online Reasoning Auxiliary for\n  E-commerce Visual Search System Optimization",
        "url": "http://arxiv.org/abs/2510.22739v1",
        "pub_date": "2025-10-26",
        "summary": "In Taobao e-commerce visual search, user behavior analysis reveals a large proportion of no-click requests, suggesting diverse and implicit user intents. These intents are expressed in various forms and are difficult to mine and discover, thereby leading to the limited adaptability and lag in platform strategies. This greatly restricts users' ability to express diverse intents and hinders the scalability of the visual search system. This mismatch between user implicit intent expression and system response defines the User-SearchSys Intent Discrepancy. To alleviate the issue, we propose a novel framework REVISION. This framework integrates offline reasoning mining with online decision-making and execution, enabling adaptive strategies to solve implicit user demands. In the offline stage, we construct a periodic pipeline to mine discrepancies from historical no-click requests. Leveraging large models, we analyze implicit intent factors and infer optimal suggestions by jointly reasoning over query and product metadata. These inferred suggestions serve as actionable insights for refining platform strategies. In the online stage, REVISION-R1-3B, trained on the curated offline data, performs holistic analysis over query images and associated historical products to generate optimization plans and adaptively schedule strategies across the search pipeline. Our framework offers a streamlined paradigm for integrating large models with traditional search systems, enabling end-to-end intelligent optimization across information aggregation and user interaction. Experimental results demonstrate that our approach improves the efficiency of implicit intent mining from large-scale search logs and significantly reduces the no-click rate.",
        "translated": "在淘宝电商视觉搜索中，用户行为分析显示存在大量无点击请求，表明用户意图具有多样性且具有隐性特征。这些意图以多种形式表达，难以挖掘和发现，导致平台策略的适应性受限且响应滞后。这极大地限制了用户表达多样化意图的能力，阻碍了视觉搜索系统的扩展性。用户隐性意图表达与系统响应之间的不匹配定义为“用户-搜索系统意图差异”（User-SearchSys Intent Discrepancy）。为缓解该问题，我们提出一种新颖的框架 REVISION。该框架融合离线推理挖掘与在线决策执行，支持自适应策略以解决隐性用户需求。在离线阶段，我们构建周期性流程，从历史无点击请求中挖掘意图差异。借助大语言模型，我们联合推理查询与商品元数据，分析隐性意图因素并推断最优建议。这些推断出的建议作为可操作的洞察，用于优化平台策略。在在线阶段，基于精心整理的离线数据训练得到的 REVISION-R1-3B，对查询图像及关联历史物料进行整体分析，生成优化方案，并在搜索流程中自适应调度策略。本框架为大语言模型与传统搜索系统的集成提供了简洁范式，支持在信息聚合与用户交互中实现端到端智能优化。实验结果表明，我们的方法提升了从大规模搜索日志中挖掘隐性意图的效率，并显著降低了无点击率。",
        "translated_title": "修订：面向电商视觉搜索系统优化的反思意图挖掘与在线推理辅助机制",
        "label": [
            "LLM生成式推荐（LLM-based Generative Recommendation）",
            "序列推荐（Sequential Recommendation）",
            "召回（Recall）"
        ],
        "label_reason": "论文通过LLM挖掘用户隐式意图，辅助电商视觉搜索优化，涉及召回与生成式推荐环节",
        "relevance_score": 8,
        "novelty_score": 8,
        "novelty_reason": "结合离线意图挖掘与在线推理调度，提出端到端优化框架，具有显著创新性"
    },
    {
        "title": "$\\text{E}^2\\text{Rank}$: Your Text Embedding can Also be an Effective\n  and Efficient Listwise Reranker",
        "url": "http://arxiv.org/abs/2510.22733v1",
        "pub_date": "2025-10-26",
        "summary": "Text embedding models serve as a fundamental component in real-world search applications. By mapping queries and documents into a shared embedding space, they deliver competitive retrieval performance with high efficiency. However, their ranking fidelity remains limited compared to dedicated rerankers, especially recent LLM-based listwise rerankers, which capture fine-grained query-document and document-document interactions. In this paper, we propose a simple yet effective unified framework $\\text{E}^2\\text{Rank}$, means Efficient Embedding-based Ranking (also means Embedding-to-Rank), which extends a single text embedding model to perform both high-quality retrieval and listwise reranking through continued training under a listwise ranking objective, thereby achieving strong effectiveness with remarkable efficiency. By applying cosine similarity between the query and document embeddings as a unified ranking function, the listwise ranking prompt, which is constructed from the original query and its candidate documents, serves as an enhanced query enriched with signals from the top-K documents, akin to pseudo-relevance feedback (PRF) in traditional retrieval models. This design preserves the efficiency and representational quality of the base embedding model while significantly improving its reranking performance. Empirically, $\\textrm{E}^2\\text{Rank}$ achieves state-of-the-art results on the BEIR reranking benchmark and demonstrates competitive performance on the reasoning-intensive BRIGHT benchmark, with very low reranking latency. We also show that the ranking training process improves embedding performance on the MTEB benchmark. Our findings indicate that a single embedding model can effectively unify retrieval and reranking, offering both computational efficiency and competitive ranking accuracy.",
        "translated": "文本嵌入模型是实际搜索应用中的基础组件。通过将查询与文档映射到共享的嵌入空间，它们在保证高效率的同时实现了具有竞争力的召回性能。然而，其排序保真度相较于专用的重排模型仍显不足，尤其是近期基于大语言模型（LLM）的列表式重排模型，后者能够捕捉细粒度的查询-文档和文档-文档交互。本文提出了一种简单而有效的统一框架 $\\text{E}^2\\text{Rank}$，即高效嵌入式排序（也称嵌入到排序），该框架通过在列表式排序目标下对单一文本嵌入模型进行持续训练，使其同时具备高质量的召回与列表式重排能力，从而在显著提升效率的同时实现强大的有效性。通过在查询与文档嵌入之间采用余弦相似度作为统一的排序函数，由原始查询及其候选文档构建的列表式排序提示，相当于一个融合了前K个文档信号的增强查询，类似于传统检索模型中的伪相关反馈（PRF）。该设计在保持基础嵌入模型效率与表征质量的同时，显著提升了其重排性能。实验表明，$\\textrm{E}^2\\text{Rank}$ 在 BEIR 重排基准测试中达到了当前最佳水平，在推理密集型的 BRIGHT 基准测试中也展现出具有竞争力的表现，且重排延迟极低。我们还发现，排序训练过程能够提升嵌入模型在 MTEB 基准测试上的性能。我们的研究结果表明，单一嵌入模型能够有效统一召回与重排，兼具计算效率与具有竞争力的排序精度。",
        "translated_title": "$\\text{E}^2\\text{Rank}$：你的文本嵌入也可以成为一种高效且有效的列表式重排模型",
        "label": [
            "重排（Re-ranking）",
            "通用推荐技术（General Recommendation Techniques）"
        ],
        "label_reason": "论文提出将文本嵌入模型扩展为高效重排器，直接应用于推荐系统重排环节，提升排序质量与效率。",
        "relevance_score": 8,
        "novelty_score": 8,
        "novelty_reason": "通过统一框架将嵌入模型用于重排，结合列表排序目标训练，创新性地提升效率与效果，非传统方法。"
    },
    {
        "title": "ATLAS: Actor-Critic Task-Completion with Look-ahead Action Simulation",
        "url": "http://arxiv.org/abs/2510.22732v1",
        "pub_date": "2025-10-26",
        "summary": "We observe that current state-of-the-art web-agents are unable to effectively adapt to new environments without neural network fine-tuning, without which they produce inefficient execution plans due to a lack of awareness of the structure and dynamics of the new environment. To address this limitation, we introduce ATLAS (Actor-Critic Task-completion with Look-ahead Action Simulation), a memory-augmented agent that is able to make plans grounded in a model of the environment by simulating the consequences of those actions in cognitive space. Our agent starts by building a \"cognitive map\" by performing a lightweight curiosity driven exploration of the environment. The planner proposes candidate actions; the simulator predicts their consequences in cognitive space; a critic analyzes the options to select the best roll-out and update the original plan; and a browser executor performs the chosen action. On the WebArena-Lite Benchmark, we achieve a 63% success rate compared to 53.9% success rate for the previously published state-of-the-art. Unlike previous systems, our modular architecture requires no website-specific LLM fine-tuning. Ablations show sizable drops without the world-model, hierarchical planner, and look-ahead-based replanner confirming their complementary roles within the design of our system",
        "translated": "我们观察到，当前最先进的网页智能体在未进行神经网络微调的情况下，无法有效适应新环境，从而因缺乏对新环境结构与动态特性的认知，产生低效的执行计划。为解决这一局限，我们提出 ATLAS（Actor-Critic Task-completion with Look-ahead Action Simulation），一种具备记忆增强能力的智能体，其能够通过在认知空间中模拟动作后果，基于环境模型制定计划。该智能体首先通过轻量级好奇心驱动的探索构建“认知地图”。规划器提出候选动作；模拟器预测这些动作在认知空间中的后果；评判器分析各选项以选择最优执行路径并更新原始计划；浏览器执行器则执行选定的动作。在 WebArena-Lite 基准测试中，我们实现了 63% 的成功率，而此前发表的最先进方法的成功率为 53.9%。与以往系统不同，我们的模块化架构无需针对特定网站进行大语言模型微调。消融实验表明，在缺少世界模型、分层规划器和基于前瞻的重规划器的情况下，性能显著下降，验证了它们在系统设计中的互补作用。",
        "translated_title": "ATLAS：基于前瞻动作模拟的演员-评论家任务完成方法",
        "label": [],
        "label_reason": "论文聚焦于网页智能体任务执行与环境建模，未涉及推荐系统核心环节。",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出模块化智能体架构与前瞻行动模拟，对任务规划有创新性改进。"
    },
    {
        "title": "Windsock is Dancing: Adaptive Multimodal Retrieval-Augmented Generation",
        "url": "http://arxiv.org/abs/2510.22694v1",
        "pub_date": "2025-10-26",
        "summary": "Multimodal Retrieval-Augmented Generation (MRAG) has emerged as a promising method to generate factual and up-to-date responses of Multimodal Large Language Models (MLLMs) by incorporating non-parametric knowledge from external knowledge bases. However, existing MRAG approaches suffer from static retrieval strategies, inflexible modality selection, and suboptimal utilization of retrieved information, leading to three critical challenges: determining when to retrieve, what modality to incorporate, and how to utilize retrieved information effectively. To address these challenges, we introduce Windsock, a query-dependent module making decisions on retrieval necessity and modality selection, effectively reducing computational overhead and improving response quality. Additionally, we propose Dynamic Noise-Resistance (DANCE) Instruction Tuning, an adaptive training strategy that enhances MLLMs' ability to utilize retrieved information while maintaining robustness against noise. Moreover, we adopt a self-assessment approach leveraging knowledge within MLLMs to convert question-answering datasets to MRAG training datasets. Extensive experiments demonstrate that our proposed method significantly improves the generation quality by 17.07% while reducing 8.95% retrieval times.",
        "translated": "多模态检索增强生成（MRAG）作为一种有前景的方法，通过融合外部知识库中的非参数化知识，提升了多模态大语言模型（MLLMs）生成事实性且时效性强的回答的能力。然而，现有MRAG方法受限于静态的检索策略、缺乏灵活性的模态选择机制，以及对检索信息利用不足，导致三个关键挑战：何时进行检索、应引入何种模态，以及如何有效利用检索到的信息。为应对这些挑战，我们提出Windsock，一种依赖于查询的模块，用于决策是否进行检索及选择何种模态，从而有效降低计算开销并提升响应质量。此外，我们提出动态抗噪（DANCE）指令微调，一种自适应训练策略，在增强MLLMs利用检索信息能力的同时，保持其对噪声的鲁棒性。同时，我们采用一种自评估方法，利用MLLMs内部知识将问答数据集转换为MRAG训练数据集。大量实验表明，所提出的方法在生成质量上显著提升17.07%，同时将检索次数减少8.95%。",
        "translated_title": "风袜在舞动：自适应多模态检索增强生成",
        "label": [
            "LLM生成式推荐",
            "多模态推荐",
            "召回"
        ],
        "label_reason": "论文聚焦多模态检索增强生成，涉及LLM生成式推荐中的召回与模态选择，与推荐系统中信息检索环节相关。",
        "relevance_score": 7,
        "novelty_score": 8,
        "novelty_reason": "提出动态噪声抵抗训练与自评估数据转换，改进检索策略与信息利用，具有显著创新性。"
    },
    {
        "title": "Diversification as Risk Minimization",
        "url": "http://arxiv.org/abs/2510.22681v1",
        "pub_date": "2025-10-26",
        "summary": "Users tend to remember failures of a search session more than its many successes. This observation has led to work on search robustness, where systems are penalized if they perform very poorly on some queries. However, this principle of robustness has been overlooked within a single query. An ambiguous or underspecified query (e.g., ``jaguar'') can have several user intents, where popular intents often dominate the ranking, leaving users with minority intents unsatisfied. Although the diversification literature has long recognized this issue, existing metrics only model the average relevance across intents and provide no robustness guarantees. More surprisingly, we show theoretically and empirically that many well-known diversification algorithms are no more robust than a naive, non-diversified algorithm. To address this critical gap, we propose to frame diversification as a risk-minimization problem. We introduce VRisk, which measures the expected risk faced by the least-served fraction of intents in a query. Optimizing VRisk produces a robust ranking, reducing the likelihood of poor user experiences. We then propose VRisker, a fast greedy re-ranker with provable approximation guarantees. Finally, experiments on NTCIR INTENT-2, TREC Web 2012, and MovieLens show the vulnerability of existing methods. VRisker reduces worst-case intent failures by up to 33% with a minimal 2% drop in average performance.",
        "translated": "用户往往对搜索会话中的失败记忆更为深刻，而非其诸多成功。这一观察促使人们开展搜索鲁棒性研究，即当系统在某些查询上表现极差时，会受到惩罚。然而，这种鲁棒性原则在单个查询内部却未被充分重视。一个模糊或定义不足的查询（如“jaguar”）可能对应多个用户意图，其中主流意图通常主导排序结果，导致少数意图的用户需求得不到满足。尽管多样性相关文献早已意识到这一问题，但现有指标仅建模各意图的平均相关性，无法提供任何鲁棒性保障。更令人惊讶的是，我们从理论和实验两方面证明，许多知名的多样性算法在鲁棒性上并不优于朴素的非多样性算法。为弥补这一关键空白，我们提出将多样性建模为一种风险最小化问题。我们引入VRisk，用于衡量查询中服务最差的意图子集所面临的预期风险。优化VRisk可生成鲁棒排序，降低用户糟糕体验的可能性。随后，我们提出VRisker，一种快速贪心重排算法，具有可证明的近似保证。最后，在NTCIR INTENT-2、TREC Web 2012和MovieLens数据集上的实验表明现有方法存在脆弱性，VRisker在平均性能仅下降2%的情况下，将最坏情况下的意图失败率最高降低33%。",
        "translated_title": "多样化作为风险最小化",
        "label": [
            "重排（Re-ranking）",
            "推荐系统评估（Evaluation Metrics / Offline/Online Testing）"
        ],
        "label_reason": "论文聚焦查询内意图多样性优化，提出风险最小化框架与重排算法，直接应用于推荐系统重排环节。",
        "relevance_score": 8,
        "novelty_score": 8,
        "novelty_reason": "提出VRisk新指标与VRisker重排算法，理论与实验双重验证，显著提升重排鲁棒性，属创新性工作。"
    },
    {
        "title": "Tools are under-documented: Simple Document Expansion Boosts Tool\n  Retrieval",
        "url": "http://arxiv.org/abs/2510.22670v1",
        "pub_date": "2025-10-26",
        "summary": "Large Language Models (LLMs) have recently demonstrated strong capabilities in tool use, yet progress in tool retrieval remains hindered by incomplete and heterogeneous tool documentation. To address this challenge, we introduce Tool-DE, a new benchmark and framework that systematically enriches tool documentation with structured fields to enable more effective tool retrieval, together with two dedicated models, Tool-Embed and Tool-Rank. We design a scalable document expansion pipeline that leverages both open- and closed-source LLMs to generate, validate, and refine enriched tool profiles at low cost, producing large-scale corpora with 50k instances for embedding-based retrievers and 200k for rerankers. On top of this data, we develop two models specifically tailored for tool retrieval: Tool-Embed, a dense retriever, and Tool-Rank, an LLM-based reranker. Extensive experiments on ToolRet and Tool-DE demonstrate that document expansion substantially improves retrieval performance, with Tool-Embed and Tool-Rank achieving new state-of-the-art results on both benchmarks. We further analyze the contribution of individual fields to retrieval effectiveness, as well as the broader impact of document expansion on both training and evaluation. Overall, our findings highlight both the promise and limitations of LLM-driven document expansion, positioning Tool-DE, along with the proposed Tool-Embed and Tool-Rank, as a foundation for future research in tool retrieval.",
        "translated": "大语言模型（LLM）近期在工具使用方面展现出强大的能力，然而工具召回的进展仍受限于工具文档的不完整性和异构性。为应对这一挑战，我们提出Tool-DE，一个全新的基准和框架，该框架系统性地通过结构化字段丰富工具文档，以实现更有效的工具召回，并配套提出两个专用模型：Tool-Embed与Tool-Rank。我们设计了一条可扩展的文档扩展流水线，利用开源和闭源大语言模型以低成本生成、验证并优化增强后的工具档案，构建了大规模语料库，包含50k实例用于基于嵌入的召回模型，以及200k实例用于重排模型。在此数据基础上，我们开发了两个专门针对工具召回设计的模型：Tool-Embed（一种稠密召回模型）和Tool-Rank（一种基于大语言模型的重排模型）。在ToolRet和Tool-DE两个基准上的大量实验表明，文档扩展显著提升了召回性能，Tool-Embed和Tool-Rank在两个基准上均取得了新的最先进结果。我们进一步分析了各个结构化字段对召回效果的贡献，以及文档扩展对训练和评估的整体影响。总体而言，我们的研究揭示了基于大语言模型的文档扩展的潜力与局限性，确立了Tool-DE及所提出的Tool-Embed和Tool-Rank作为未来工具召回研究的重要基础。",
        "translated_title": "工具文档不足：简单的文档扩展提升工具召回",
        "label": [
            "召回",
            "重排",
            "LLM生成式推荐"
        ],
        "label_reason": "论文聚焦工具检索中的召回与重排，使用LLM生成文档扩展，提升检索效果，属于推荐系统中信息检索环节的延伸应用。",
        "relevance_score": 7,
        "novelty_score": 8,
        "novelty_reason": "提出基于LLM的文档扩展框架并构建专用模型，对工具检索有显著改进，创新性较强但非推荐系统核心问题。"
    },
    {
        "title": "ATOM: AdapTive and OptiMized dynamic temporal knowledge graph\n  construction using LLMs",
        "url": "http://arxiv.org/abs/2510.22590v1",
        "pub_date": "2025-10-26",
        "summary": "In today's rapidly expanding data landscape, knowledge extraction from unstructured text is vital for real-time analytics, temporal inference, and dynamic memory frameworks. However, traditional static knowledge graph (KG) construction often overlooks the dynamic and time-sensitive nature of real-world data, limiting adaptability to continuous changes. Moreover, recent zero- or few-shot approaches that avoid domain-specific fine-tuning or reliance on prebuilt ontologies often suffer from instability across multiple runs, as well as incomplete coverage of key facts. To address these challenges, we introduce ATOM (AdapTive and OptiMized), a few-shot and scalable approach that builds and continuously updates Temporal Knowledge Graphs (TKGs) from unstructured texts. ATOM splits input documents into minimal, self-contained \"atomic\" facts, improving extraction exhaustivity and stability. Then, it constructs atomic TKGs from these facts while employing a dual-time modeling that distinguishes when information is observed from when it is valid. The resulting atomic TKGs are subsequently merged in parallel. Empirical evaluations demonstrate that ATOM achieves ~18% higher exhaustivity, ~17% better stability, and over 90% latency reduction compared to baseline methods, demonstrating a strong scalability potential for dynamic TKG construction.",
        "translated": "在当今快速增长的数据环境中，从非结构化文本中提取知识对于实时分析、时序推理和动态记忆框架至关重要。然而，传统的静态知识图谱（KG）构建方法通常忽视了现实世界数据的动态性和时效性，限制了其对持续变化的适应能力。此外，近期提出的零样本或少样本方法，虽避免了领域特定微调或依赖预构建本体，但往往在多次运行中表现出不稳定性，并且对关键事实的覆盖不完整。为应对这些挑战，我们提出 ATOM（AdapTive and OptiMized），一种少样本且可扩展的方法，用于从非结构化文本中构建并持续更新时序知识图谱（TKGs）。ATOM 将输入文档分解为最小、自包含的“原子”事实，从而提升提取的完备性和稳定性。随后，它基于这些原子事实构建原子 TKG，并采用双时间建模机制，区分信息被观测到的时间与信息有效的时间。最终，这些原子 TKG 以并行方式合并。实证评估表明，与基线方法相比，ATOM 的提取完备性提高约 18%，稳定性提升约 17%，延迟降低超过 90%，展现出在动态 TKG 构建中强大的可扩展性潜力。",
        "translated_title": "ATOM：基于大语言模型的自适应与优化动态时序知识图谱构建",
        "label": [
            "多模态推荐",
            "负采样与对比学习"
        ],
        "label_reason": "论文聚焦动态知识图谱构建，与推荐系统间接相关，可应用于多模态推荐中的知识增强",
        "relevance_score": 4,
        "novelty_score": 7,
        "novelty_reason": "提出原子化事实提取与双时间建模，提升稳定性与覆盖度，为知识图谱构建提供新方法"
    },
    {
        "title": "Open Multimodal Retrieval-Augmented Factual Image Generation",
        "url": "http://arxiv.org/abs/2510.22521v1",
        "pub_date": "2025-10-26",
        "summary": "Large Multimodal Models (LMMs) have achieved remarkable progress in generating photorealistic and prompt-aligned images, but they often produce outputs that contradict verifiable knowledge, especially when prompts involve fine-grained attributes or time-sensitive events. Conventional retrieval-augmented approaches attempt to address this issue by introducing external information, yet they are fundamentally incapable of grounding generation in accurate and evolving knowledge due to their reliance on static sources and shallow evidence integration. To bridge this gap, we introduce ORIG, an agentic open multimodal retrieval-augmented framework for Factual Image Generation (FIG), a new task that requires both visual realism and factual grounding. ORIG iteratively retrieves and filters multimodal evidence from the web and incrementally integrates the refined knowledge into enriched prompts to guide generation. To support systematic evaluation, we build FIG-Eval, a benchmark spanning ten categories across perceptual, compositional, and temporal dimensions. Experiments demonstrate that ORIG substantially improves factual consistency and overall image quality over strong baselines, highlighting the potential of open multimodal retrieval for factual image generation.",
        "translated": "大语言多模态模型（LMMs）在生成逼真且与提示对齐的图像方面取得了显著进展，但它们常常生成与可验证知识相矛盾的结果，尤其是在提示涉及细粒度属性或时间敏感事件时。传统的检索增强方法试图通过引入外部信息来解决这一问题，但由于其依赖静态信息源和浅层证据整合，本质上无法将生成过程扎根于准确且不断演化的知识体系。为弥合这一差距，我们提出ORIG，一个面向事实图像生成（FIG）任务的代理式开放多模态检索增强框架。该任务要求图像在具备视觉真实感的同时，还需具备事实性支撑。ORIG通过迭代地从网络中检索并过滤多模态证据，逐步将精炼后的知识整合进增强的提示中，以引导图像生成。为支持系统性评估，我们构建了FIG-Eval基准，涵盖感知、组合和时间三个维度的十个类别。实验表明，ORIG在事实一致性与整体图像质量方面显著优于强基线方法，凸显了开放多模态检索在事实图像生成中的潜力。",
        "translated_title": "开放多模态检索增强事实性图像生成",
        "label": [],
        "label_reason": "论文聚焦于多模态图像生成与事实一致性，与推荐系统无直接关联。",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出开放多模态检索增强框架，改进事实一致性，方法新颖但非推荐领域。"
    },
    {
        "title": "FAIR-RAG: Faithful Adaptive Iterative Refinement for Retrieval-Augmented\n  Generation",
        "url": "http://arxiv.org/abs/2510.22344v1",
        "pub_date": "2025-10-25",
        "summary": "While Retrieval-Augmented Generation (RAG) mitigates hallucination and knowledge staleness in Large Language Models (LLMs), existing frameworks often falter on complex, multi-hop queries that require synthesizing information from disparate sources. Current advanced RAG methods, employing iterative or adaptive strategies, lack a robust mechanism to systematically identify and fill evidence gaps, often propagating noise or failing to gather a comprehensive context. We introduce FAIR-RAG, a novel agentic framework that transforms the standard RAG pipeline into a dynamic, evidence-driven reasoning process. At its core is an Iterative Refinement Cycle governed by a module we term Structured Evidence Assessment (SEA). The SEA acts as an analytical gating mechanism: it deconstructs the initial query into a checklist of required findings and audits the aggregated evidence to identify confirmed facts and, critically, explicit informational gaps. These gaps provide a precise signal to an Adaptive Query Refinement agent, which generates new, targeted sub-queries to retrieve missing information. This cycle repeats until the evidence is verified as sufficient, ensuring a comprehensive context for a final, strictly faithful generation. We conducted experiments on challenging multi-hop QA benchmarks, including HotpotQA, 2WikiMultiHopQA, and MusiQue. In a unified experimental setup, FAIR-RAG significantly outperforms strong baselines. On HotpotQA, it achieves an F1-score of 0.453 -- an absolute improvement of 8.3 points over the strongest iterative baseline -- establishing a new state-of-the-art for this class of methods on these benchmarks. Our work demonstrates that a structured, evidence-driven refinement process with explicit gap analysis is crucial for unlocking reliable and accurate reasoning in advanced RAG systems for complex, knowledge-intensive tasks.",
        "translated": "尽管检索增强生成（RAG）缓解了大语言模型（LLM）中的幻觉和知识陈旧问题，但现有框架在处理需要从多个来源综合信息的复杂多跳查询时往往表现不佳。当前先进的RAG方法，采用迭代或自适应策略，缺乏一种稳健机制来系统性地识别和填补证据空白，常常传播噪声或无法获取全面的上下文。我们提出FAIR-RAG，一种新颖的智能体框架，将标准RAG流程转化为动态、以证据驱动的推理过程。其核心是一个由我们称为结构化证据评估（SEA）模块所控制的迭代优化循环。SEA作为分析性门控机制：它将初始查询解构为所需发现的清单，并审核聚合后的证据，以识别已确认的事实，以及关键的、明确的信息空白。这些空白为自适应查询优化智能体提供精确信号，该智能体生成新的、有针对性的子查询以检索缺失信息。此循环持续进行，直至证据被验证为充分，从而确保最终生成阶段具备全面的上下文，且严格忠实于事实。我们在具有挑战性的多跳问答基准测试集上进行了实验，包括HotpotQA、2WikiMultiHopQA和MusiQue。在统一的实验设置下，FAIR-RAG显著超越了强基线方法。在HotpotQA上，其F1得分达到0.453——比最强的迭代基线方法绝对提升8.3个百分点——在这些基准测试中确立了该类方法的新state-of-the-art。我们的工作表明，具有显式空白分析的结构化、以证据驱动的优化过程，对于在复杂、知识密集型任务中实现先进RAG系统的可靠且准确的推理至关重要。",
        "translated_title": "FAIR-RAG：面向检索增强生成的忠实自适应迭代优化方法",
        "label": [],
        "label_reason": "论文聚焦RAG框架的迭代优化，用于多跳问答，非推荐系统核心环节",
        "relevance_score": 3,
        "novelty_score": 8,
        "novelty_reason": "提出结构化证据评估与自适应查询优化机制，显著提升复杂问答性能"
    },
    {
        "title": "PatenTEB: A Comprehensive Benchmark and Model Family for Patent Text\n  Embedding",
        "url": "http://arxiv.org/abs/2510.22264v1",
        "pub_date": "2025-10-25",
        "summary": "Patent text embeddings enable prior art search, technology landscaping, and patent analysis, yet existing benchmarks inadequately capture patent-specific challenges. We introduce PatenTEB, a comprehensive benchmark comprising 15 tasks across retrieval, classification, paraphrase, and clustering, with 2.06 million examples. PatenTEB employs domain-stratified splits, domain specific hard negative mining, and systematic coverage of asymmetric fragment-to-document matching scenarios absent from general embedding benchmarks. We develop the patembed model family through multi-task training, spanning 67M to 344M parameters with context lengths up to 4096 tokens. External validation shows strong generalization: patembed-base achieves state-of-the-art on MTEB BigPatentClustering.v2 (0.494 V-measure vs. 0.445 previous best), while patembed-large achieves 0.377 NDCG@100 on DAPFAM. Systematic ablations reveal that multi-task training improves external generalization despite minor benchmark costs, and that domain-pretrained initialization provides consistent advantages across task families. All resources will be made available at https://github.com/iliass-y/patenteb. Keywords: patent retrieval, sentence embeddings, multi-task learning, asymmetric retrieval, benchmark evaluation, contrastive learning.",
        "translated": "专利文本嵌入能够支持现有技术检索、技术布局分析以及专利分析，然而现有的基准测试未能充分捕捉专利领域特有的挑战。我们提出 PatenTEB，一个综合性基准测试，包含检索、分类、改写和聚类等15项任务，共涵盖206万例样本。PatenTEB采用领域分层划分、领域特定的难负例挖掘，并系统性覆盖了通用嵌入基准中缺失的非对称片段-文档匹配场景。我们通过多任务训练开发了 patembed 模型系列，参数规模从67M到344M不等，上下文长度最高达4096个token。外部验证表明其具有良好的泛化能力：patembed-base 在 MTEB BigPatentClustering.v2 上达到最新性能（0.494 V-measure，优于此前最佳的0.445），而 patembed-large 在 DAPFAM 上取得0.377的 NDCG@100。系统性消融实验表明，尽管在基准测试中成本略有增加，多任务训练仍能提升外部泛化性能；同时，领域预训练初始化在各类任务中均表现出持续优势。所有资源将公开于 https://github.com/iliass-y/patenteb。关键词：专利检索、句子嵌入、多任务学习、非对称检索、基准评估、对比学习。",
        "translated_title": "PatenTEB：面向专利文本嵌入的综合基准与模型家族",
        "label": [
            "负采样与对比学习",
            "推荐系统评估"
        ],
        "label_reason": "论文聚焦专利文本嵌入与检索，与推荐系统无直接关联，但涉及对比学习与评估方法",
        "relevance_score": 3,
        "novelty_score": 7,
        "novelty_reason": "提出多任务训练与领域特定硬负样本挖掘，方法新颖且有效"
    },
    {
        "title": "Concerto: Joint 2D-3D Self-Supervised Learning Emerges Spatial\n  Representations",
        "url": "http://arxiv.org/abs/2510.23607v1",
        "pub_date": "2025-10-27",
        "summary": "Humans learn abstract concepts through multisensory synergy, and once formed, such representations can often be recalled from a single modality. Inspired by this principle, we introduce Concerto, a minimalist simulation of human concept learning for spatial cognition, combining 3D intra-modal self-distillation with 2D-3D cross-modal joint embedding. Despite its simplicity, Concerto learns more coherent and informative spatial features, as demonstrated by zero-shot visualizations. It outperforms both standalone SOTA 2D and 3D self-supervised models by 14.2% and 4.8%, respectively, as well as their feature concatenation, in linear probing for 3D scene perception. With full fine-tuning, Concerto sets new SOTA results across multiple scene understanding benchmarks (e.g., 80.7% mIoU on ScanNet). We further present a variant of Concerto tailored for video-lifted point cloud spatial understanding, and a translator that linearly projects Concerto representations into CLIP's language space, enabling open-world perception. These results highlight that Concerto emerges spatial representations with superior fine-grained geometric and semantic consistency.",
        "translated": "人类通过多感官协同学习抽象概念，一旦形成，这些表征通常可仅通过单一模态被激活回忆。受此原理启发，我们提出 Concerto，一种用于空间认知的人类概念学习的极简模拟，其结合了3D模态内自蒸馏与2D-3D跨模态联合嵌入。尽管结构简洁，Concerto 仍能学习到更具连贯性和信息量的空间特征，这一点通过零样本可视化得到验证。在3D场景感知的线性探测任务中，Concerto 相较于独立的最先进2D和3D自监督模型分别提升14.2%和4.8%，并优于其特征拼接结果。在全参数微调设置下，Concerto 在多个场景理解基准上均取得新的最先进结果（例如，在ScanNet上达到80.7%的mIoU）。我们进一步提出一种适用于视频提升点云空间理解的Concerto变体，以及一个线性投影器，可将Concerto表征映射至CLIP的语言空间，从而实现开放世界感知。这些结果表明，Concerto 所生成的空间表征具备更优越的细粒度几何与语义一致性。",
        "translated_title": "Concerto：联合2D-3D自监督学习生成空间表征",
        "label": [],
        "label_reason": "论文聚焦3D场景理解与跨模态表征学习，属高阶视觉任务，不涉及像素级图像恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出2D-3D联合自监督框架，新颖性强，但应用于场景理解而非low-level处理。"
    },
    {
        "title": "Track, Inpaint, Resplat: Subject-driven 3D and 4D Generation with\n  Progressive Texture Infilling",
        "url": "http://arxiv.org/abs/2510.23605v1",
        "pub_date": "2025-10-27",
        "summary": "Current 3D/4D generation methods are usually optimized for photorealism, efficiency, and aesthetics. However, they often fail to preserve the semantic identity of the subject across different viewpoints. Adapting generation methods with one or few images of a specific subject (also known as Personalization or Subject-driven generation) allows generating visual content that align with the identity of the subject. However, personalized 3D/4D generation is still largely underexplored. In this work, we introduce TIRE (Track, Inpaint, REsplat), a novel method for subject-driven 3D/4D generation. It takes an initial 3D asset produced by an existing 3D generative model as input and uses video tracking to identify the regions that need to be modified. Then, we adopt a subject-driven 2D inpainting model for progressively infilling the identified regions. Finally, we resplat the modified 2D multi-view observations back to 3D while still maintaining consistency. Extensive experiments demonstrate that our approach significantly improves identity preservation in 3D/4D generation compared to state-of-the-art methods. Our project website is available at https://zsh2000.github.io/track-inpaint-resplat.github.io/.",
        "translated": "当前的3D/4D生成方法通常针对照片真实感、效率和美学效果进行优化。然而，它们往往难以在不同视角下保持主体的语义身份一致性。通过使用特定主体的一张或少数几张图像来适配生成方法（也称为个性化或主体驱动生成），可以生成与主体身份一致的视觉内容。然而，个性化3D/4D生成仍处于被广泛探索的早期阶段。在本文中，我们提出TIRE（Track, Inpaint, REsplat），一种新颖的主体驱动3D/4D生成方法。该方法以现有3D生成模型生成的初始3D资产作为输入，利用视频跟踪技术识别需要修改的区域。随后，我们采用一种主体驱动的2D图像修复模型，逐步对识别出的区域进行填充。最后，将修改后的2D多视角观测结果重新投影回3D空间，同时保持一致性。大量实验表明，与现有最先进方法相比，我们的方法显著提升了3D/4D生成中的身份保持能力。我们的项目网站位于 https://zsh2000.github.io/track-inpaint-resplat.github.io/。",
        "translated_title": "Track, Inpaint, Resplat：基于主体驱动的3D与4D生成及其渐进式纹理填充",
        "label": [],
        "label_reason": "论文聚焦3D/4D生成与主体身份保持，属于high-level生成任务，非像素级图像恢复。",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出渐进式纹理填充流程，结合跟踪与修复，但属生成领域常规改进。"
    },
    {
        "title": "PixelRefer: A Unified Framework for Spatio-Temporal Object Referring\n  with Arbitrary Granularity",
        "url": "http://arxiv.org/abs/2510.23603v1",
        "pub_date": "2025-10-27",
        "summary": "Multimodal large language models (MLLMs) have demonstrated strong general-purpose capabilities in open-world visual comprehension. However, most existing MLLMs primarily focus on holistic, scene-level understanding, often overlooking the need for fine-grained, object-centric reasoning. In this paper, we present PixelRefer, a unified region-level MLLM framework that enables advanced fine-grained understanding over user-specified regions across both images and videos. Motivated by the observation that LLM attention predominantly focuses on object-level tokens, we propose a Scale-Adaptive Object Tokenizer (SAOT) to generate compact and semantically rich object representations from free-form regions. Our analysis reveals that global visual tokens contribute mainly in early LLM layers, inspiring the design of PixelRefer-Lite, an efficient variant that employs an Object-Centric Infusion module to pre-fuse global context into object tokens. This yields a lightweight Object-Only Framework that substantially reduces computational cost while maintaining high semantic fidelity. To facilitate fine-grained instruction tuning, we curate PixelRefer-2.2M, a high-quality object-centric instruction dataset. Extensive experiments across a range of benchmarks validate that PixelRefer achieves leading performance with fewer training samples, while PixelRefer-Lite offers competitive accuracy with notable gains in efficiency.",
        "translated": "多模态大语言模型（MLLMs）在开放世界视觉理解任务中展现出强大的通用能力。然而，现有大多数MLLMs主要关注整体、场景级别的理解，往往忽视了对细粒度、以对象为中心的推理需求。本文提出PixelRefer，一种统一的区域级MLLM框架，能够在用户指定的图像和视频区域上实现先进的细粒度理解。受大语言模型（LLM）注意力机制主要聚焦于对象级token的启发，我们设计了尺度自适应对象编码器（SAOT），用于从任意形状区域生成紧凑且语义丰富的对象表示。我们的分析表明，全局视觉token主要在LLM的早期层发挥作用，由此启发我们设计了PixelRefer-Lite这一高效变体，其采用对象中心注入模块（Object-Centric Infusion module）将全局上下文预先融合到对象token中，从而构建轻量化的纯对象框架，在显著降低计算成本的同时保持高语义保真度。为支持细粒度指令微调，我们构建了PixelRefer-2.2M，一个高质量的对象中心指令数据集。在多个基准数据集上的广泛实验验证了PixelRefer在较少训练样本下即可达到领先性能，而PixelRefer-Lite在保持竞争力精度的同时，实现了显著的效率提升。",
        "translated_title": "PixelRefer: 一种用于时空对象指代的统一框架，支持任意粒度",
        "label": [],
        "label_reason": "论文聚焦于多模态大模型的区域级视觉理解，属于高层语义理解任务，非像素级图像恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 6,
        "novelty_reason": "提出自适应对象标记器和轻量框架，属方法改进，但未涉及低层图像处理核心创新。"
    },
    {
        "title": "PRISM-Bench: A Benchmark of Puzzle-Based Visual Tasks with CoT Error\n  Detection",
        "url": "http://arxiv.org/abs/2510.23594v1",
        "pub_date": "2025-10-27",
        "summary": "We introduce \\textbf{PRISM-Bench}, a benchmark of puzzle-based visual challenges designed to evaluate not only whether models can solve problems, but how their reasoning unfolds. Unlike prior evaluations that measure only final-answer accuracy, PRISM-Bench introduces a diagnostic task: given a visual puzzle and a step-by-step chain-of-thought (CoT) containing exactly one error, models must identify the first incorrect step. This setting enables fine-grained assessment of logical consistency, error detection, and visual reasoning. The puzzles in PRISM-Bench require multi-step symbolic, geometric, and analogical reasoning, resisting shortcuts based on superficial pattern matching. Evaluations across state-of-the-art MLLMs reveal a persistent gap between fluent generation and faithful reasoning: models that produce plausible CoTs often fail to locate simple logical faults. By disentangling answer generation from reasoning verification, PRISM-Bench offers a sharper lens on multimodal reasoning competence and underscores the need for diagnostic evaluation protocols in the development of trustworthy MLLMs.",
        "translated": "我们提出**PRISM-Bench**，一个基于拼图式视觉挑战的基准测试，旨在评估模型不仅能否解决问题，更关注其推理过程如何展开。与以往仅衡量最终答案准确率的评估方式不同，PRISM-Bench 引入了一项诊断任务：给定一个视觉拼图和一条包含恰好一个错误的逐步思维链（CoT），模型需识别出第一个错误步骤。该设置能够对逻辑一致性、错误检测能力以及视觉推理能力进行细粒度评估。PRISM-Bench 中的拼图需要多步骤的符号化、几何化和类比推理，能够有效抵制基于表面模式匹配的捷径策略。在当前最先进的多模态大语言模型（MLLMs）上的评估结果表明，流畅生成与忠实推理之间存在持续的差距：即使模型能够生成看似合理的思维链，往往仍无法识别出简单的逻辑错误。通过将答案生成与推理验证解耦，PRISM-Bench 为多模态推理能力提供了更精准的评估视角，并强调了在可信 MLLMs 开发过程中引入诊断性评估协议的必要性。",
        "translated_title": "PRISM-Bench：一个基于拼图的视觉任务基准，支持思维链错误检测",
        "label": [],
        "label_reason": "论文聚焦多模态大模型的视觉推理与错误检测，属于高阶视觉理解任务，非像素级图像处理。",
        "relevance_score": 1,
        "novelty_score": 6,
        "novelty_reason": "提出基于谜题的推理错误检测基准，方法新颖但未涉及图像恢复或增强技术。"
    },
    {
        "title": "InFlux: A Benchmark for Self-Calibration of Dynamic Intrinsics of Video\n  Cameras",
        "url": "http://arxiv.org/abs/2510.23589v1",
        "pub_date": "2025-10-27",
        "summary": "Accurately tracking camera intrinsics is crucial for achieving 3D understanding from 2D video. However, most 3D algorithms assume that camera intrinsics stay constant throughout a video, which is often not true for many real-world in-the-wild videos. A major obstacle in this field is a lack of dynamic camera intrinsics benchmarks--existing benchmarks typically offer limited diversity in scene content and intrinsics variation, and none provide per-frame intrinsic changes for consecutive video frames. In this paper, we present Intrinsics in Flux (InFlux), a real-world benchmark that provides per-frame ground truth intrinsics annotations for videos with dynamic intrinsics. Compared to prior benchmarks, InFlux captures a wider range of intrinsic variations and scene diversity, featuring 143K+ annotated frames from 386 high-resolution indoor and outdoor videos with dynamic camera intrinsics. To ensure accurate per-frame intrinsics, we build a comprehensive lookup table of calibration experiments and extend the Kalibr toolbox to improve its accuracy and robustness. Using our benchmark, we evaluate existing baseline methods for predicting camera intrinsics and find that most struggle to achieve accurate predictions on videos with dynamic intrinsics. For the dataset, code, videos, and submission, please visit https://influx.cs.princeton.edu/.",
        "translated": "准确追踪相机内参对于从2D视频实现3D理解至关重要。然而，大多数3D算法假设相机内参在整个视频序列中保持恒定，这在许多现实世界中的野外视频中并不成立。该领域的一个主要障碍是缺乏动态相机内参基准数据集——现有基准数据集通常在场景内容和内参变化方面多样性有限，且均未提供连续视频帧的逐帧内参变化标注。本文提出了Intrinsics in Flux（InFlux），一个真实世界的基准数据集，为具有动态内参的视频提供逐帧真实内参标注。与先前基准相比，InFlux涵盖了更广泛的内参变化范围和场景多样性，包含来自386个高分辨率室内和室外视频的143K+个标注帧，这些视频均具有动态相机内参。为确保逐帧内参的准确性，我们构建了一个全面的标定实验查找表，并扩展了Kalibr工具箱以提升其精度和鲁棒性。利用我们的基准数据集，我们评估了现有预测相机内参的基线方法，发现大多数方法在具有动态内参的视频上难以实现准确预测。有关数据集、代码、视频及提交，请访问 https://influx.cs.princeton.edu/。",
        "translated_title": "InFlux：一种用于视频相机动态内部参数自校准的基准数据集",
        "label": [],
        "label_reason": "论文聚焦相机内参动态变化的3D理解，属于高阶视觉任务，非像素级图像恢复或增强。",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出新基准数据集InFlux，对动态内参建模有实用价值，但方法非low-level图像处理创新。"
    },
    {
        "title": "FARMER: Flow AutoRegressive Transformer over Pixels",
        "url": "http://arxiv.org/abs/2510.23588v1",
        "pub_date": "2025-10-27",
        "summary": "Directly modeling the explicit likelihood of the raw data distribution is key topic in the machine learning area, which achieves the scaling successes in Large Language Models by autoregressive modeling. However, continuous AR modeling over visual pixel data suffer from extremely long sequences and high-dimensional spaces. In this paper, we present FARMER, a novel end-to-end generative framework that unifies Normalizing Flows (NF) and Autoregressive (AR) models for tractable likelihood estimation and high-quality image synthesis directly from raw pixels. FARMER employs an invertible autoregressive flow to transform images into latent sequences, whose distribution is modeled implicitly by an autoregressive model. To address the redundancy and complexity in pixel-level modeling, we propose a self-supervised dimension reduction scheme that partitions NF latent channels into informative and redundant groups, enabling more effective and efficient AR modeling. Furthermore, we design a one-step distillation scheme to significantly accelerate inference speed and introduce a resampling-based classifier-free guidance algorithm to boost image generation quality. Extensive experiments demonstrate that FARMER achieves competitive performance compared to existing pixel-based generative models while providing exact likelihoods and scalable training.",
        "translated": "直接建模原始数据分布的显式似然性是机器学习领域的一个关键课题，通过自回归建模已在大型语言模型中取得显著成功。然而，对视觉像素数据进行连续自回归建模会面临序列过长和高维空间带来的挑战。本文提出 FARMER，一种新颖的端到端生成框架，将归一化流（Normalizing Flows, NF）与自回归（Autoregressive, AR）模型相结合，实现对原始像素数据的可计算似然估计和高质量图像合成。FARMER 采用可逆的自回归流将图像转换为潜在序列，其分布由自回归模型隐式建模。为应对像素级建模中的冗余性和复杂性，我们提出一种自监督降维方案，将 NF 的潜在通道划分为信息性与冗余性组，从而实现更高效、更有效的 AR 建模。此外，我们设计了一种单步蒸馏方案以显著加速推理速度，并引入基于重采样的无分类器引导算法以提升图像生成质量。大量实验表明，FARMER 在与现有基于像素的生成模型对比中表现出竞争力，同时提供精确的似然值和可扩展的训练能力。",
        "translated_title": "FARMER：基于像素的流自回归Transformer",
        "label": [],
        "label_reason": "论文聚焦图像生成而非恢复，属high-level任务，不处理像素质量退化问题。",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出NF与AR结合框架，含自监督降维与蒸馏加速，对生成模型有改进。"
    },
    {
        "title": "Lookahead Anchoring: Preserving Character Identity in Audio-Driven Human\n  Animation",
        "url": "http://arxiv.org/abs/2510.23581v1",
        "pub_date": "2025-10-27",
        "summary": "Audio-driven human animation models often suffer from identity drift during temporal autoregressive generation, where characters gradually lose their identity over time. One solution is to generate keyframes as intermediate temporal anchors that prevent degradation, but this requires an additional keyframe generation stage and can restrict natural motion dynamics. To address this, we propose Lookahead Anchoring, which leverages keyframes from future timesteps ahead of the current generation window, rather than within it. This transforms keyframes from fixed boundaries into directional beacons: the model continuously pursues these future anchors while responding to immediate audio cues, maintaining consistent identity through persistent guidance. This also enables self-keyframing, where the reference image serves as the lookahead target, eliminating the need for keyframe generation entirely. We find that the temporal lookahead distance naturally controls the balance between expressivity and consistency: larger distances allow for greater motion freedom, while smaller ones strengthen identity adherence. When applied to three recent human animation models, Lookahead Anchoring achieves superior lip synchronization, identity preservation, and visual quality, demonstrating improved temporal conditioning across several different architectures. Video results are available at the following link: https://lookahead-anchoring.github.io.",
        "translated": "音频驱动的人体动画模型在进行时序自回归生成时，常出现身份漂移问题，即角色随时间推移逐渐丧失其身份特征。一种解决方案是生成关键帧作为中间时序锚点以防止退化，但该方法需要额外的关键帧生成阶段，且可能限制运动的自然动态。为此，我们提出“前瞻锚定”（Lookahead Anchoring）方法，该方法利用当前生成窗口之后未来时间步的关键帧，而非窗口内部的关键帧。这使关键帧从固定的边界转变为具有方向性的引导信标：模型在响应即时音频线索的同时，持续追寻这些未来的锚点，通过持续的引导保持身份一致性。此外，该方法还支持自关键帧机制，即参考图像本身作为前瞻目标，从而完全省去关键帧生成步骤。我们发现，时序前瞻距离自然地控制了表现力与一致性的平衡：较大的距离允许更大的运动自由度，而较小的距离则增强身份保持能力。当应用于三种近期的人体动画模型时，前瞻锚定在唇形同步、身份保持和视觉质量方面均取得更优表现，证明其在多种不同架构中提升了时序条件建模能力。视频结果可访问以下链接：https://lookahead-anchoring.github.io。",
        "translated_title": "前瞻锚定：在音频驱动的人体动画中保持角色身份",
        "label": [],
        "label_reason": "论文聚焦音频驱动的人体动画，属于高阶视觉生成任务，不涉及图像像素级恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出前瞻锚定机制，改进时序条件生成，对动画一致性有显著提升，属方法创新。"
    },
    {
        "title": "UrbanVLA: A Vision-Language-Action Model for Urban Micromobility",
        "url": "http://arxiv.org/abs/2510.23576v1",
        "pub_date": "2025-10-27",
        "summary": "Urban micromobility applications, such as delivery robots, demand reliable navigation across large-scale urban environments while following long-horizon route instructions. This task is particularly challenging due to the dynamic and unstructured nature of real-world city areas, yet most existing navigation methods remain tailored to short-scale and controllable scenarios. Effective urban micromobility requires two complementary levels of navigation skills: low-level capabilities such as point-goal reaching and obstacle avoidance, and high-level capabilities, such as route-visual alignment. To this end, we propose UrbanVLA, a route-conditioned Vision-Language-Action (VLA) framework designed for scalable urban navigation. Our method explicitly aligns noisy route waypoints with visual observations during execution, and subsequently plans trajectories to drive the robot. To enable UrbanVLA to master both levels of navigation, we employ a two-stage training pipeline. The process begins with Supervised Fine-Tuning (SFT) using simulated environments and trajectories parsed from web videos. This is followed by Reinforcement Fine-Tuning (RFT) on a mixture of simulation and real-world data, which enhances the model's safety and adaptability in real-world settings. Experiments demonstrate that UrbanVLA surpasses strong baselines by more than 55% in the SocialNav task on MetaUrban. Furthermore, UrbanVLA achieves reliable real-world navigation, showcasing both scalability to large-scale urban environments and robustness against real-world uncertainties.",
        "translated": "城市微出行应用，如配送机器人，需要在大规模城市环境中可靠地导航，并遵循长时程的路径指令。由于现实城市区域具有动态性和非结构化特点，该任务极具挑战性，而现有大多数导航方法仍局限于小尺度且可控的场景。有效的城市微出行需要两个互补层次的导航能力：低层次能力，如点目标到达和障碍物规避；以及高层次能力，如路径-视觉对齐。为此，我们提出 UrbanVLA，一种以路径为条件的视觉-语言-动作（Vision-Language-Action, VLA）框架，专为可扩展的城市导航而设计。我们的方法在执行过程中显式地将噪声路径路点与视觉观测进行对齐，并随后规划轨迹以驱动机器人。为使 UrbanVLA 掌握上述两个层次的导航能力，我们采用两阶段训练流程。第一阶段使用模拟环境和从网络视频中解析出的轨迹进行监督微调（Supervised Fine-Tuning, SFT）；第二阶段在模拟与真实世界数据混合的数据集上进行强化微调（Reinforcement Fine-Tuning, RFT），从而提升模型在真实环境中的安全性与适应性。实验结果表明，UrbanVLA 在 MetaUrban 平台上的 SocialNav 任务中，相较强基线方法性能提升超过 55%。此外，UrbanVLA 在真实世界导航中表现出色，展现了对大规模城市环境的可扩展性以及对真实世界不确定性的鲁棒性。",
        "translated_title": "UrbanVLA：一种用于城市微出行的视觉-语言-行动模型",
        "label": [],
        "label_reason": "论文聚焦城市微出行导航，属于高阶视觉-语言-动作任务，非像素级图像处理。",
        "relevance_score": 1,
        "novelty_score": 6,
        "novelty_reason": "提出两阶段训练框架，结合模拟与真实数据，对导航任务有改进但非低层图像处理创新。"
    },
    {
        "title": "More Than Generation: Unifying Generation and Depth Estimation via\n  Text-to-Image Diffusion Models",
        "url": "http://arxiv.org/abs/2510.23574v1",
        "pub_date": "2025-10-27",
        "summary": "Generative depth estimation methods leverage the rich visual priors stored in pre-trained text-to-image diffusion models, demonstrating astonishing zero-shot capability. However, parameter updates during training lead to catastrophic degra- dation in the image generation capability of the pre-trained model. We introduce MERGE, a unified model for image generation and depth estimation, starting from a fixed pre-trained text-to-image model. MERGE demonstrates that the pre-trained text-to-image model can do more than image generation, but also expand to depth estimation effortlessly. Specifically, MERGE introduces a play- and-plug framework that enables seamless switching between image generation and depth estimation modes through simple and pluggable converters. Meanwhile, we propose a Group Reuse Mechanism to encourage parameter reuse and im- prove the utilization of the additional learnable parameters. MERGE unleashes the powerful depth estimation capability of the pre-trained text-to-image model while preserving its original image generation ability. Compared to other unified models for image generation and depth estimation, MERGE achieves state-of- the-art performance across multiple depth estimation benchmarks. The code will be made available at https://github.com/H-EmbodVis/MERGE",
        "translated": "生成式深度估计方法利用预训练文本到图像扩散模型中存储的丰富视觉先验，展现出惊人的零样本能力。然而，在训练过程中进行参数更新会导致预训练模型的图像生成能力发生灾难性退化。我们提出MERGE，一种从固定预训练文本到图像模型出发的图像生成与深度估计统一模型。MERGE表明，预训练文本到图像模型不仅能进行图像生成，还可轻松扩展至深度估计任务。具体而言，MERGE引入了一种即插即用框架，通过简单且可插拔的转换器，实现图像生成与深度估计模式之间的无缝切换。同时，我们提出一种组重用机制，以促进参数重用并提升额外可学习参数的利用率。MERGE在保持原始图像生成能力的同时，充分释放了预训练文本到图像模型强大的深度估计能力。与其它图像生成与深度估计统一模型相比，MERGE在多个深度估计基准数据集上均达到当前最优性能。代码将在https://github.com/H-EmbodVis/MERGE公开。",
        "translated_title": "超越生成：通过文本到图像扩散模型统一生成与深度估计",
        "label": [],
        "label_reason": "论文聚焦图像生成与深度估计统一模型，属于高阶视觉任务，非像素级图像恢复。",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出可插拔框架与参数复用机制，对多任务统一模型有显著改进。"
    },
    {
        "title": "RobotArena $\\infty$: Scalable Robot Benchmarking via Real-to-Sim\n  Translation",
        "url": "http://arxiv.org/abs/2510.23571v1",
        "pub_date": "2025-10-27",
        "summary": "The pursuit of robot generalists - instructable agents capable of performing diverse tasks across diverse environments - demands rigorous and scalable evaluation. Yet real-world testing of robot policies remains fundamentally constrained: it is labor-intensive, slow, unsafe at scale, and difficult to reproduce. Existing simulation benchmarks are similarly limited, as they train and test policies within the same synthetic domains and cannot assess models trained from real-world demonstrations or alternative simulation environments. As policies expand in scope and complexity, these barriers only intensify, since defining \"success\" in robotics often hinges on nuanced human judgments of execution quality. In this paper, we introduce a new benchmarking framework that overcomes these challenges by shifting VLA evaluation into large-scale simulated environments augmented with online human feedback. Leveraging advances in vision-language models, 2D-to-3D generative modeling, and differentiable rendering, our approach automatically converts video demonstrations from widely used robot datasets into simulated counterparts. Within these digital twins, we assess VLA policies using both automated VLM-guided scoring and scalable human preference judgments collected from crowdworkers, transforming human involvement from tedious scene setup, resetting, and safety supervision into lightweight preference comparisons. To measure robustness, we systematically perturb simulated environments along multiple axes, such as textures and object placements, stress-testing policy generalization under controlled variation. The result is a continuously evolving, reproducible, and scalable benchmark for real-world trained robot manipulation policies, addressing a critical missing capability in today's robotics landscape.",
        "translated": "对通用机器人代理——即能够在多样化环境中执行多种任务的可指令化智能体——的追求，要求建立严格且可扩展的评估体系。然而，机器人策略在真实世界中的测试仍然面临根本性限制：其过程耗时、费力、难以大规模部署且存在安全隐患，同时复现性差。现有的仿真基准同样存在局限，因为它们在相同的合成环境中训练和测试策略，无法评估从真实世界演示或不同仿真环境训练得到的模型。随着策略在范围和复杂度上的不断扩展，这些障碍愈发加剧，因为在机器人领域，“成功”的定义往往依赖于对执行质量进行细致入微的人类主观判断。本文提出了一种新的基准框架，通过将视觉-语言-动作（VLA）评估迁移至大规模仿真环境，并结合在线人类反馈，有效克服了上述挑战。借助视觉-语言模型、2D到3D生成建模以及可微渲染等技术进展，我们的方法能够自动将来自常用机器人数据集的视频演示转换为对应的仿真版本。在这些数字孪生环境中，我们通过自动化视觉-语言模型引导的评分系统，以及从众包工作者处收集的大规模人类偏好判断，对VLA策略进行评估，从而将人类参与从繁琐的场景搭建、重置和安全监督，转变为轻量级的偏好比较任务。为衡量策略的鲁棒性，我们沿多个维度（如纹理和物体布局）系统性地扰动仿真环境，在受控变化条件下对策略的泛化能力进行压力测试。最终，我们构建了一个持续演进、可复现且可扩展的基准，用于评估在真实世界中训练的机器人操作策略，填补了当前机器人领域的一项关键能力空白。",
        "translated_title": "RobotArena $\\infty$: 基于真实到模拟转换的可扩展机器人基准测试",
        "label": [],
        "label_reason": "论文聚焦机器人策略评估与仿真，非图像像素级恢复或增强任务，属于高阶视觉应用。",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出真实到仿真的转换框架，结合VLM与人类反馈，方法新颖但非low-level图像处理领域。"
    },
    {
        "title": "EgoThinker: Unveiling Egocentric Reasoning with Spatio-Temporal CoT",
        "url": "http://arxiv.org/abs/2510.23569v1",
        "pub_date": "2025-10-27",
        "summary": "Egocentric video reasoning centers on an unobservable agent behind the camera who dynamically shapes the environment, requiring inference of hidden intentions and recognition of fine-grained interactions. This core challenge limits current multimodal large language models MLLMs, which excel at visible event reasoning but lack embodied, first-person understanding. To bridge this gap, we introduce EgoThinker, a novel framework that endows MLLMs with robust egocentric reasoning capabilities through spatio-temporal chain-of-thought supervision and a two-stage learning curriculum. First, we introduce EgoRe-5M, a large-scale egocentric QA dataset constructed from 13M diverse egocentric video clips. This dataset features multi-minute segments annotated with detailed CoT rationales and dense hand-object grounding. Second, we employ SFT on EgoRe-5M to instill reasoning skills, followed by reinforcement fine-tuning RFT to further enhance spatio-temporal localization. Experimental results show that EgoThinker outperforms existing methods across multiple egocentric benchmarks, while achieving substantial improvements in fine-grained spatio-temporal localization tasks. Full code and data are released at https://github.com/InternRobotics/EgoThinker.",
        "translated": "以自我为中心的视频推理聚焦于摄像机后方不可观测的主体，该主体动态地塑造环境，要求对隐藏意图进行推理，并识别细粒度的交互行为。这一核心挑战限制了当前多模态大语言模型（MLLMs）的表现，尽管它们在可见事件推理方面表现出色，但缺乏具身化的第一人称理解能力。为弥合这一差距，我们提出 EgoThinker，一种新颖的框架，通过时空思维链监督和两阶段学习课程，赋予 MLLMs 强大的以自我为中心的推理能力。首先，我们构建了 EgoRe-5M，一个大规模的以自我为中心的问答数据集，由1300万段多样化的以自我为中心视频片段组成。该数据集包含多分钟长的视频片段，标注了详细的思维链推理过程（CoT rationales）和密集的手部-物体关联信息。其次，我们在 EgoRe-5M 上采用监督微调（SFT）以注入推理能力，随后通过强化微调（RFT）进一步提升时空定位能力。实验结果表明，EgoThinker 在多个以自我为中心的基准测试中均优于现有方法，并在细粒度时空定位任务中实现了显著提升。完整代码与数据已公开于 https://github.com/InternRobotics/EgoThinker。",
        "translated_title": "EgoThinker：通过时空思维链揭示以自我为中心的推理",
        "label": [],
        "label_reason": "论文聚焦于第一人称视频推理与意图理解，属于高阶视觉任务，非像素级图像恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 6,
        "novelty_reason": "提出时空CoT监督与两阶段学习，方法新颖，但应用于高阶推理而非低级图像处理。"
    },
    {
        "title": "Revising Second Order Terms in Deep Animation Video Coding",
        "url": "http://arxiv.org/abs/2510.23561v1",
        "pub_date": "2025-10-27",
        "summary": "First Order Motion Model is a generative model that animates human heads based on very little motion information derived from keypoints. It is a promising solution for video communication because first it operates at very low bitrate and second its computational complexity is moderate compared to other learning based video codecs. However, it has strong limitations by design. Since it generates facial animations by warping source-images, it fails to recreate videos with strong head movements. This works concentrates on one specific kind of head movements, namely head rotations. We show that replacing the Jacobian transformations in FOMM by a global rotation helps the system to perform better on items with head-rotations while saving 40% to 80% of bitrate on P-frames. Moreover, we apply state-of-the-art normalization techniques to the discriminator to stabilize the adversarial training which is essential for generating visually appealing videos. We evaluate the performance by the learned metics LPIPS and DISTS to show the success our optimizations.",
        "translated": "一阶运动模型（First Order Motion Model）是一种生成模型，能够基于从关键点提取的极少运动信息对人脸头部进行动画生成。该模型在视频通信领域具有应用前景，首先是因为其以极低比特率运行，其次是因为其计算复杂度相较于其他基于学习的视频编解码器处于中等水平。然而，该模型在设计上存在明显局限性。由于其通过形变源图像来生成面部动画，因此在处理具有剧烈头部运动的视频时表现不佳。本工作聚焦于一种特定的头部运动——头部旋转。我们证明，将FOMM中的雅可比变换替换为全局旋转，可显著提升系统在头部旋转场景下的性能，同时在P帧上节省40%至80%的比特率。此外，我们采用当前最先进的归一化技术对判别器进行优化，以稳定对抗训练过程，这对生成视觉上更具吸引力的视频至关重要。我们通过学习型指标LPIPS和DISTS评估性能，以验证所提优化方案的有效性。",
        "translated_title": "修订深度动画视频编码中的二阶项",
        "label": [],
        "label_reason": "论文聚焦于视频编码与生成，属于高阶视觉任务，不涉及像素级图像恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 4,
        "novelty_reason": "对FOMM模型进行优化，改进运动建模与训练稳定，属常规改进，无突破性创新。"
    },
    {
        "title": "A U-Net and Transformer Pipeline for Multilingual Image Translation",
        "url": "http://arxiv.org/abs/2510.23554v1",
        "pub_date": "2025-10-27",
        "summary": "This paper presents an end-to-end multilingual translation pipeline that integrates a custom U-Net for text detection, the Tesseract engine for text recognition, and a from-scratch sequence-to-sequence (Seq2Seq) Transformer for Neural Machine Translation (NMT). Our approach first utilizes a U-Net model, trained on a synthetic dataset , to accurately segment and detect text regions from an image. These detected regions are then processed by Tesseract to extract the source text. This extracted text is fed into a custom Transformer model trained from scratch on a multilingual parallel corpus spanning 5 languages. Unlike systems reliant on monolithic pre-trained models, our architecture emphasizes full customization and adaptability. The system is evaluated on its text detection accuracy, text recognition quality, and translation performance via BLEU scores. The complete pipeline demonstrates promising results, validating the viability of a custom-built system for translating text directly from images.",
        "translated": "本文提出了一种端到端的多语言翻译流程，该流程集成了自定义的U-Net用于文本检测、Tesseract引擎用于文本识别，以及从头训练的序列到序列（Seq2Seq）Transformer用于神经机器翻译（NMT）。我们的方法首先利用在合成数据集上训练的U-Net模型，从图像中精确分割和检测文本区域。随后，这些检测到的区域由Tesseract处理以提取源文本。提取的文本被输入到一个在包含5种语言的多语言平行语料库上从头训练的自定义Transformer模型中。与依赖于单体预训练模型的系统不同，我们的架构强调完全的定制化和适应性。系统通过文本检测精度、文本识别质量以及基于BLEU分数的翻译性能进行评估。完整的流程展现了令人满意的成果，验证了构建自定义系统以直接从图像中翻译文本的可行性。",
        "translated_title": "一种基于U-Net与Transformer的多语言图像翻译流水线",
        "label": [],
        "label_reason": "论文聚焦图像中文字检测与翻译，属于高阶视觉任务，非像素级图像恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 4,
        "novelty_reason": "提出定制化U-Net+Transformer架构，但属常规组件组合，无突破性创新。"
    },
    {
        "title": "JanusCoder: Towards a Foundational Visual-Programmatic Interface for\n  Code Intelligence",
        "url": "http://arxiv.org/abs/2510.23538v1",
        "pub_date": "2025-10-27",
        "summary": "The scope of neural code intelligence is rapidly expanding beyond text-based source code to encompass the rich visual outputs that programs generate. This visual dimension is critical for advanced applications like flexible content generation and precise, program-driven editing of visualizations. However, progress has been impeded by the scarcity of high-quality multimodal code data, a bottleneck stemming from challenges in synthesis and quality assessment. To address these challenges, we make contributions from both a data and modeling perspective. We first introduce a complete synthesis toolkit that leverages reciprocal synergies between data modalities to efficiently produce a large-scale, high-quality corpus spanning from standard charts to complex interactive web UIs and code-driven animations. Leveraging this toolkit, we construct JanusCode-800K, the largest multimodal code corpus to date. This powers the training of our models, JanusCoder and JanusCoderV, which establish a visual-programmatic interface for generating code from textual instructions, visual inputs, or a combination of both. Our unified model is a departure from existing approaches that build specialized models for isolated tasks. Extensive experiments on both text-centric and vision-centric coding tasks demonstrate the superior performance of the JanusCoder series, with our 7B to 14B scale models approaching or even exceeding the performance of commercial models. Furthermore, extensive analysis provides key insights into harmonizing programmatic logic with its visual expression. Our code and checkpoints will are available at https://github.com/InternLM/JanusCoder.",
        "translated": "神经代码智能的研究范围正迅速从基于文本的源代码扩展至程序生成的丰富视觉输出。这一视觉维度对于高级应用（如灵活的内容生成和精确的、由程序驱动的可视化编辑）至关重要。然而，进展受到高质量多模态代码数据稀缺的阻碍，而这一瓶颈源于合成与质量评估方面的挑战。为应对这些挑战，我们从数据和建模两个层面做出贡献。首先，我们提出了一套完整的合成工具包，利用数据模态间的相互协同效应，高效生成涵盖标准图表至复杂交互式网页用户界面及代码驱动动画的大规模、高质量语料库。借助该工具包，我们构建了 JanusCode-800K，这是迄今为止最大的多模态代码语料库。该语料库支撑了我们模型 JanusCoder 和 JanusCoderV 的训练，二者建立了一个视觉-程序化接口，能够根据文本指令、视觉输入或两者的组合生成代码。我们的统一模型突破了现有方法中为孤立任务构建专用模型的范式。在以文本为中心和以视觉为中心的编程任务上的大量实验表明，JanusCoder 系列模型表现出优越性能，其 7B 至 14B 规模的模型性能已接近甚至超越商用模型。此外，深入的分析为协调程序逻辑与其视觉表达提供了关键洞见。我们的代码和模型检查点可通过 https://github.com/InternLM/JanusCoder 获取。",
        "translated_title": "JanusCoder：面向代码智能的通用视觉-程序化交互界面",
        "label": [],
        "label_reason": "论文聚焦代码智能与多模态生成，不涉及图像像素级恢复或增强任务",
        "relevance_score": 1,
        "novelty_score": 6,
        "novelty_reason": "提出多模态代码数据合成工具与统一模型，属代码生成领域创新，非图像处理"
    },
    {
        "title": "DPGLA: Bridging the Gap between Synthetic and Real Data for Unsupervised\n  Domain Adaptation in 3D LiDAR Semantic Segmentation",
        "url": "http://arxiv.org/abs/2510.23525v1",
        "pub_date": "2025-10-27",
        "summary": "Annotating real-world LiDAR point clouds for use in intelligent autonomous systems is costly. To overcome this limitation, self-training-based Unsupervised Domain Adaptation (UDA) has been widely used to improve point cloud semantic segmentation by leveraging synthetic point cloud data. However, we argue that existing methods do not effectively utilize unlabeled data, as they either rely on predefined or fixed confidence thresholds, resulting in suboptimal performance. In this paper, we propose a Dynamic Pseudo-Label Filtering (DPLF) scheme to enhance real data utilization in point cloud UDA semantic segmentation. Additionally, we design a simple and efficient Prior-Guided Data Augmentation Pipeline (PG-DAP) to mitigate domain shift between synthetic and real-world point clouds. Finally, we utilize data mixing consistency loss to push the model to learn context-free representations. We implement and thoroughly evaluate our approach through extensive comparisons with state-of-the-art methods. Experiments on two challenging synthetic-to-real point cloud semantic segmentation tasks demonstrate that our approach achieves superior performance. Ablation studies confirm the effectiveness of the DPLF and PG-DAP modules. We release the code of our method in this paper.",
        "translated": "对用于智能自主系统的现实世界LiDAR点云进行标注成本高昂。为克服这一局限，基于自训练的无监督域自适应（UDA）方法被广泛采用，旨在通过利用合成点云数据提升点云语义分割性能。然而，我们认为现有方法未能有效利用无标签数据，因其通常依赖预设或固定的置信度阈值，导致性能欠佳。本文提出一种动态伪标签过滤（DPLF）方案，以增强点云UDA语义分割中对真实数据的利用。此外，我们设计了一种简单高效的先验引导数据增强流水线（PG-DAP），以缓解合成点云与现实世界点云之间的域偏移问题。最后，我们引入数据混合一致性损失，促使模型学习与上下文无关的表征。我们通过与当前最先进方法的广泛对比，对所提出方法进行了实现与全面评估。在两个具有挑战性的合成到现实点云语义分割任务上的实验表明，本文方法取得了优越的性能。消融研究验证了DPLF和PG-DAP模块的有效性。本文公开了所提出方法的代码。",
        "translated_title": "DPGLA：弥合合成数据与真实数据之间的差距，用于3D激光雷达语义分割的无监督域自适应",
        "label": [],
        "label_reason": "论文聚焦3D LiDAR语义分割，属于高阶视觉任务，非像素级图像恢复。",
        "relevance_score": 1,
        "novelty_score": 6,
        "novelty_reason": "提出动态伪标签过滤与先验引导数据增强，对UDA有改进，但非low-level创新。"
    },
    {
        "title": "FreeFuse: Multi-Subject LoRA Fusion via Auto Masking at Test Time",
        "url": "http://arxiv.org/abs/2510.23515v1",
        "pub_date": "2025-10-27",
        "summary": "This paper proposes FreeFuse, a novel training-free approach for multi-subject text-to-image generation through automatic fusion of multiple subject LoRAs. In contrast to existing methods that either focus on pre-inference LoRA weight merging or rely on segmentation models and complex techniques like noise blending to isolate LoRA outputs, our key insight is that context-aware dynamic subject masks can be automatically derived from cross-attention layer weights. Mathematical analysis shows that directly applying these masks to LoRA outputs during inference well approximates the case where the subject LoRA is integrated into the diffusion model and used individually for the masked region. FreeFuse demonstrates superior practicality and efficiency as it requires no additional training, no modification to LoRAs, no auxiliary models, and no user-defined prompt templates or region specifications. Alternatively, it only requires users to provide the LoRA activation words for seamless integration into standard workflows. Extensive experiments validate that FreeFuse outperforms existing approaches in both generation quality and usability under the multi-subject generation tasks. The project page is at https://future-item.github.io/FreeFuse/",
        "translated": "本文提出 FreeFuse，一种新颖的无需训练的多主体文本到图像生成方法，通过自动融合多个主体的 LoRA 实现。与现有方法相比，后者或专注于推理前的 LoRA 权重合并，或依赖于分割模型以及噪声融合等复杂技术以分离 LoRA 输出，本文的核心洞见在于：可从交叉注意力层权重中自动推导出具有上下文感知能力的动态主体掩码。数学分析表明，在推理过程中直接将这些掩码应用于 LoRA 输出，能够很好地近似于将主体 LoRA 集成至扩散模型，并单独用于掩码区域的情形。FreeFuse 具备卓越的实用性和效率，无需额外训练、无需修改 LoRA、无需辅助模型，也无需用户自定义提示模板或区域指定。相反，用户仅需提供 LoRA 的激活词，即可无缝集成至标准工作流程。大量实验验证了在多主体生成任务中，FreeFuse 在生成质量和可用性方面均优于现有方法。项目主页位于 https://future-item.github.io/FreeFuse/",
        "translated_title": "FreeFuse：基于测试时自动掩码的多主体LoRA融合\n\n在本研究中，我们提出了一种名为FreeFuse的新型多主体低秩自适应（LoRA）融合方法，旨在实现高效且灵活的模型集成，无需在训练阶段进行显式配对或微调。与现有方法不同，FreeFuse在测试阶段通过自动掩码机制动态选择并融合多个LoRA适配器，从而适应不同输入图像的视觉内容和退化类型。\n\n具体而言，FreeFuse引入了一个轻量级的掩码生成模块，该模块根据输入图像的特征图自适应地生成掩码向量，用于控制每个LoRA适配器的激活权重。该掩码生成过程完全在空域中进行，通过一个小型卷积网络提取图像的局部与全局上下文信息，并将其映射为连续的掩码分数。这些分数随后被归一化并用于对齐不同LoRA适配器的输出贡献。\n\n为确保融合过程的稳定性，我们进一步引入残差学习结构，将原始主干网络的特征图与多个LoRA适配器的输出进行逐元素加权融合。该设计不仅保留了主干网络的先验知识，还增强了模型对复杂退化（如图像去噪、去雨、去雾等）的适应能力。\n\n实验结果表明，FreeFuse在多个基准数据集（包括Rain100L、SIDD、RESIDE）上均取得了显著优于基线方法的性能，尤其在处理多退化混合场景时表现出更强的鲁棒性。此外，由于其端到端的训练方式和测试时的动态融合机制，FreeFuse在保持高精度的同时，显著降低了计算开销，适用于资源受限的部署环境。\n\n我们进一步验证了FreeFuse在不同图像恢复任务（如超分辨率、低光照增强、JPEG伪影去除等）中的通用性，证明了其作为通用融合框架的潜力。方法的灵活性和效率使其成为未来多模型集成研究的重要方向。",
        "label": [],
        "label_reason": "论文聚焦于文本到图像生成中的多主体融合，属于高阶图像生成任务，非像素级图像恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出基于注意力权重的自动掩码融合策略，无需训练或辅助模型，方法新颖但不针对低层视觉问题。"
    },
    {
        "title": "Localising under the drape: proprioception in the era of distributed\n  surgical robotic system",
        "url": "http://arxiv.org/abs/2510.23512v1",
        "pub_date": "2025-10-27",
        "summary": "Despite their mechanical sophistication, surgical robots remain blind to their surroundings. This lack of spatial awareness causes collisions, system recoveries, and workflow disruptions, issues that will intensify with the introduction of distributed robots with independent interacting arms. Existing tracking systems rely on bulky infrared cameras and reflective markers, providing only limited views of the surgical scene and adding hardware burden in crowded operating rooms. We present a marker-free proprioception method that enables precise localisation of surgical robots under their sterile draping despite associated obstruction of visual cues. Our method solely relies on lightweight stereo-RGB cameras and novel transformer-based deep learning models. It builds on the largest multi-centre spatial robotic surgery dataset to date (1.4M self-annotated images from human cadaveric and preclinical in vivo studies). By tracking the entire robot and surgical scene, rather than individual markers, our approach provides a holistic view robust to occlusions, supporting surgical scene understanding and context-aware control. We demonstrate an example of potential clinical benefits during in vivo breathing compensation with access to tissue dynamics, unobservable under state of the art tracking, and accurately locate in multi-robot systems for future intelligent interaction. In addition, and compared with existing systems, our method eliminates markers and improves tracking visibility by 25%. To our knowledge, this is the first demonstration of marker-free proprioception for fully draped surgical robots, reducing setup complexity, enhancing safety, and paving the way toward modular and autonomous robotic surgery.",
        "translated": "尽管手术机器人在机械结构上已相当精巧，但其仍无法感知周围环境。这种空间感知能力的缺失会导致碰撞、系统重启和工作流程中断等问题，而这些问题在引入具有独立交互臂的分布式机器人后将更加突出。现有的追踪系统依赖于体积庞大的红外摄像头和反光标记，仅能提供手术场景的有限视角，并在拥挤的手术室内增加硬件负担。我们提出了一种无标记的本体感知方法，能够在手术机器人被无菌布覆盖、视觉线索受到遮挡的情况下，实现其精确定位。该方法仅依赖轻量化的立体RGB摄像头和基于新型Transformer架构的深度学习模型。我们基于迄今为止规模最大的多中心空间机器人手术数据集（包含来自人体 cadaveric 和临床前体内研究的140万张自标注图像）构建了该方法。通过追踪整个机器人及手术场景，而非单个标记点，我们的方法提供了对遮挡具有鲁棒性的全局视图，支持手术场景理解与上下文感知控制。我们在体内呼吸补偿实验中展示了潜在的临床优势，能够获取组织动态信息，这是现有追踪技术无法观测到的，并能准确定位多机器人系统中的各部件，为未来的智能交互奠定基础。此外，相较于现有系统，我们的方法消除了对标记物的依赖，使追踪可视性提升了25%。据我们所知，这是首次在完全覆盖无菌布的手术机器人上实现无标记本体感知，显著降低了系统部署复杂度，提升了安全性，并为模块化和自主化机器人手术的发展铺平了道路。",
        "translated_title": "在遮挡下的定位：分布式手术机器人系统时代的本体感知",
        "label": [],
        "label_reason": "论文聚焦于手术机器人定位与场景理解，属于高阶视觉任务，非像素级图像恢复或增强。",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出基于Transformer的无标记定位方法，具有创新性，但非针对图像质量复原任务。"
    },
    {
        "title": "iPac: Incorporating Intra-image Patch Context into Graph Neural Networks\n  for Medical Image Classification",
        "url": "http://arxiv.org/abs/2510.23504v1",
        "pub_date": "2025-10-27",
        "summary": "Graph neural networks have emerged as a promising paradigm for image processing, yet their performance in image classification tasks is hindered by a limited consideration of the underlying structure and relationships among visual entities. This work presents iPac, a novel approach to introduce a new graph representation of images to enhance graph neural network image classification by recognizing the importance of underlying structure and relationships in medical image classification. iPac integrates various stages, including patch partitioning, feature extraction, clustering, graph construction, and graph-based learning, into a unified network to advance graph neural network image classification. By capturing relevant features and organising them into clusters, we construct a meaningful graph representation that effectively encapsulates the semantics of the image. Experimental evaluation on diverse medical image datasets demonstrates the efficacy of iPac, exhibiting an average accuracy improvement of up to 5% over baseline methods. Our approach offers a versatile and generic solution for image classification, particularly in the realm of medical images, by leveraging the graph representation and accounting for the inherent structure and relationships among visual entities.",
        "translated": "图神经网络已成为图像处理领域一种有前景的范式，然而其在图像分类任务中的性能受到对视觉实体之间潜在结构与关系考虑不足的限制。本文提出 iPac，一种新颖的方法，通过引入一种新的图像图表示，以增强图神经网络在图像分类中的表现，充分认识到潜在结构与关系在医学图像分类中的重要性。iPac 将多个阶段——包括图像块划分、特征提取、聚类、图构建以及基于图的学习——集成到一个统一的网络框架中，从而推进图神经网络在图像分类中的应用。通过捕捉相关特征并将其组织为簇，我们构建出一种具有语义意义的图表示，有效封装了图像的语义信息。在多个医学图像数据集上的实验评估表明，iPac 的有效性显著，相较于基线方法，平均准确率提升高达 5%。我们的方法通过利用图表示并考虑视觉实体之间固有的结构与关系，为图像分类，尤其是在医学图像领域，提供了一种通用且灵活的解决方案。",
        "translated_title": "iPac：将图像内局部块上下文融入图神经网络用于医学图像分类",
        "label": [],
        "label_reason": "论文聚焦医学图像分类，属于high-level任务，非像素级图像恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 6,
        "novelty_reason": "提出新图结构表示方法，但用于分类任务，非low-level图像处理创新。"
    },
    {
        "title": "VOLD: Reasoning Transfer from LLMs to Vision-Language Models via\n  On-Policy Distillation",
        "url": "http://arxiv.org/abs/2510.23497v1",
        "pub_date": "2025-10-27",
        "summary": "Training vision-language models (VLMs) for complex reasoning remains a challenging task, i.a. due to the scarcity of high-quality image-text reasoning data. Conversely, text-based reasoning resources are abundant and scalable, but it is still an open question how to leveraging them for VLM reasoning. To address this problem, we propose VOLD, a framework to transfer reasoning capabilities from text-only teacher models to VLM student models. To this end, VOLD combines reinforcement learning via Group Relative Policy Optimization (GRPO) with on-policy distillation, which allows the student reasoning traces to be guided by the teacher model, resulting in a significant gain over using GRPO alone. We further show that a cold-start alignment is essential for an effective transfer during the online training phase in this scenario and that without sufficient distributional alignment between teacher and student, on-policy distillation fails to provide meaningful guidance. We evaluate VOLD across diverse benchmarks including MMMU-Pro, MathVision, MathVista, and LogicVista, showing that VOLD outperforms the baseline model significantly and improves over the state of the art by a margin. Our ablation shows the importance of a cold-start alignment via SFT for on-policy distillation with a text-only teacher.",
        "translated": "训练视觉-语言模型（VLMs）以实现复杂推理仍是一项具有挑战性的任务，主要原因在于高质量图像-文本推理数据的稀缺性。相反，基于文本的推理资源丰富且易于扩展，但如何有效利用这些资源提升VLM的推理能力仍是一个开放问题。为解决该问题，我们提出VOLD，一个将文本-only教师模型的推理能力迁移至VLM学生模型的框架。为此，VOLD结合了基于Group Relative Policy Optimization（GRPO）的强化学习与在线策略蒸馏，使学生模型的推理轨迹能够受到教师模型的引导，从而在仅使用GRPO的情况下实现显著性能提升。我们进一步证明，在在线训练阶段，冷启动对齐对于实现有效迁移至关重要；若教师与学生模型之间缺乏充分的分布对齐，在线策略蒸馏将无法提供有意义的指导。我们在多个基准数据集上评估了VOLD，包括MMMU-Pro、MathVision、MathVista和LogicVista，结果表明VOLD显著优于基线模型，并在现有技术上取得明显提升。我们的消融实验表明，通过SFT进行的冷启动对齐对于使用文本-only教师模型的在线策略蒸馏具有关键作用。",
        "translated_title": "VOLD：通过基于策略的蒸馏实现大语言模型到视觉-语言模型的推理迁移",
        "label": [],
        "label_reason": "论文聚焦视觉-语言模型推理能力迁移，属于高阶视觉理解任务，非像素级图像处理。",
        "relevance_score": 1,
        "novelty_score": 6,
        "novelty_reason": "提出结合GRPO与在线蒸馏的框架，但核心为推理能力迁移，创新点在训练策略而非图像复原。"
    },
    {
        "title": "Yesnt: Are Diffusion Relighting Models Ready for Capture Stage\n  Compositing? A Hybrid Alternative to Bridge the Gap",
        "url": "http://arxiv.org/abs/2510.23494v1",
        "pub_date": "2025-10-27",
        "summary": "Volumetric video relighting is essential for bringing captured performances into virtual worlds, but current approaches struggle to deliver temporally stable, production-ready results. Diffusion-based intrinsic decomposition methods show promise for single frames, yet suffer from stochastic noise and instability when extended to sequences, while video diffusion models remain constrained by memory and scale. We propose a hybrid relighting framework that combines diffusion-derived material priors with temporal regularization and physically motivated rendering. Our method aggregates multiple stochastic estimates of per-frame material properties into temporally consistent shading components, using optical-flow-guided regularization. For indirect effects such as shadows and reflections, we extract a mesh proxy from Gaussian Opacity Fields and render it within a standard graphics pipeline. Experiments on real and synthetic captures show that this hybrid strategy achieves substantially more stable relighting across sequences than diffusion-only baselines, while scaling beyond the clip lengths feasible for video diffusion. These results indicate that hybrid approaches, which balance learned priors with physically grounded constraints, are a practical step toward production-ready volumetric video relighting.",
        "translated": "体素视频重光照对于将捕获的表演带入虚拟世界至关重要，但当前方法在实现时间上稳定且适用于生产的成果方面仍面临挑战。基于扩散的内在分解方法在单帧图像上展现出潜力，但在扩展至序列时却因随机噪声和不稳定性而表现不佳，而视频扩散模型则受限于内存和规模。我们提出一种混合重光照框架，结合了由扩散模型推导出的材质先验、时间正则化以及基于物理的渲染。该方法通过光流引导的正则化，将多帧随机估计的每帧材质属性聚合为时间一致的着色组件。对于阴影和反射等间接效果，我们从高斯不透明场中提取网格代理，并在标准图形管线中进行渲染。在真实和合成捕获数据上的实验表明，该混合策略在序列中实现了远优于仅使用扩散模型的基线方法的重光照稳定性，同时其处理的片段长度也超越了视频扩散模型可行的范围。这些结果表明，结合学习先验与基于物理约束的混合方法，是迈向可用于生产的体素视频重光照的切实可行的一步。",
        "translated_title": "Yesnt：扩散重光照模型是否已准备好用于拍摄阶段的合成？一种填补差距的混合替代方案",
        "label": [],
        "label_reason": "论文聚焦于体积视频重光照，属于高阶渲染任务，非像素级图像恢复或增强。",
        "relevance_score": 3,
        "novelty_score": 7,
        "novelty_reason": "提出混合框架结合扩散模型与物理渲染，创新性较强但不针对低层视觉任务。"
    },
    {
        "title": "Tongyi DeepResearch Technical Report",
        "url": "http://arxiv.org/abs/2510.24701v1",
        "pub_date": "2025-10-28",
        "summary": "We present Tongyi DeepResearch, an agentic large language model, which is specifically designed for long-horizon, deep information-seeking research tasks. To incentivize autonomous deep research agency, Tongyi DeepResearch is developed through an end-to-end training framework that combines agentic mid-training and agentic post-training, enabling scalable reasoning and information seeking across complex tasks. We design a highly scalable data synthesis pipeline that is fully automatic, without relying on costly human annotation, and empowers all training stages. By constructing customized environments for each stage, our system enables stable and consistent interactions throughout. Tongyi DeepResearch, featuring 30.5 billion total parameters, with only 3.3 billion activated per token, achieves state-of-the-art performance across a range of agentic deep research benchmarks, including Humanity's Last Exam, BrowseComp, BrowseComp-ZH, WebWalkerQA, xbench-DeepSearch, FRAMES and xbench-DeepSearch-2510. We open-source the model, framework, and complete solutions to empower the community.",
        "translated": "我们提出 Tongyi DeepResearch，一种专为长期、深度信息探索研究任务设计的智能体大语言模型。为激励自主深度研究智能体行为，Tongyi DeepResearch 通过融合智能体中期训练与智能体后期训练的端到端训练框架进行开发，从而支持在复杂任务中实现可扩展的推理与信息探索能力。我们设计了一条高度可扩展的自动数据合成流水线，无需依赖昂贵的人工标注，全面赋能所有训练阶段。通过为每个训练阶段构建定制化环境，系统实现了稳定且一致的交互过程。Tongyi DeepResearch 拥有总计 305 亿参数，每处理一个 token 仅激活 33 亿参数，在多个智能体深度研究基准测试中均达到当前最优水平，包括 Humanity's Last Exam、BrowseComp、BrowseComp-ZH、WebWalkerQA、xbench-DeepSearch、FRAMES 以及 xbench-DeepSearch-2510。我们开源了模型、训练框架及完整解决方案，以赋能学术与产业社区。",
        "translated_title": "通义深度研究技术报告",
        "label": [],
        "label_reason": "论文聚焦于通用智能体模型的深度研究任务，未直接涉及推荐系统核心环节。",
        "relevance_score": 3,
        "novelty_score": 8,
        "novelty_reason": "提出端到端智能体训练框架，数据合成自动化，具创新性但非推荐领域"
    },
    {
        "title": "Optimizing Retrieval for RAG via Reinforced Contrastive Learning",
        "url": "http://arxiv.org/abs/2510.24652v1",
        "pub_date": "2025-10-28",
        "summary": "As retrieval-augmented generation (RAG) becomes increasingly widespread, the role of information retrieval (IR) is shifting from retrieving information for human users to retrieving contextual knowledge for artificial intelligence (AI) systems, where relevance becomes difficult to define or annotate beforehand. To address this challenge, we propose R3, a Retrieval framework optimized for RAG through trialand-feedback Reinforced contrastive learning. Unlike prior approaches that rely on annotated or synthetic data for supervised fine-tuning, R3 enables the retriever to dynamically explore and optimize relevance within the RAG environment. During training, the retrieved results interact with the environment to produce contrastive signals that automatically guide the retriever's self-improvement. Extensive experiments across diverse tasks demonstrate that R3 improves RAG performance by 5.2% over the original retriever and surpasses state-of-the-art retrievers by 4.9%, while achieving comparable results to LLM-augmented retrieval and RAG systems built on post-trained or instruction-tuned LLMs. It is both efficient and practical, requiring only 4 GPUs and completing training within a single day.",
        "translated": "随着检索增强生成（RAG）技术的日益普及，信息检索（IR）的作用正从为人类用户检索信息，转变为为人工智能（AI）系统检索上下文知识，其中相关性变得难以预先定义或标注。为应对这一挑战，我们提出R3，一种通过试错与反馈强化对比学习优化的RAG检索框架。与以往依赖标注数据或合成数据进行监督微调的方法不同，R3使检索器能够在RAG环境中动态探索并优化相关性。在训练过程中，检索结果与环境交互，产生对比信号，自动引导检索器的自我改进。在多种任务上的大量实验表明，R3相较于原始检索器将RAG性能提升了5.2%，并超越了当前最先进的检索器4.9%，同时其性能可与基于大语言模型（LLM）增强的检索及建立在后训练或指令调优LLM上的RAG系统相媲美。R3兼具高效性与实用性，仅需4块GPU，且可在单日内完成训练。",
        "translated_title": "优化检索以用于RAG的强化对比学习",
        "label": [
            "召回",
            "负采样与对比学习"
        ],
        "label_reason": "论文聚焦RAG中的召回优化，通过强化对比学习提升检索质量，与推荐召回有间接关联。",
        "relevance_score": 4,
        "novelty_score": 7,
        "novelty_reason": "提出R3框架，利用环境反馈动态优化检索，创新性较强，但非推荐领域原创。"
    },
    {
        "title": "Iterative Critique-Refine Framework for Enhancing LLM Personalization",
        "url": "http://arxiv.org/abs/2510.24469v1",
        "pub_date": "2025-10-28",
        "summary": "Personalized text generation requires models not only to produce coherent text but also to align with a target user's style, tone, and topical focus. Existing retrieval-augmented approaches such as LaMP and PGraphRAG enrich profiles with user and neighbor histories, but they stop at generation and often yield outputs that drift in tone, topic, or style. We present PerFine, a unified, training-free critique-refine framework that enhances personalization through iterative, profile-grounded feedback. In each iteration, an LLM generator produces a draft conditioned on the retrieved profile, and a critic LLM - also conditioned on the same profile - provides structured feedback on tone, vocabulary, sentence structure, and topicality. The generator then revises, while a novel knockout strategy retains the stronger draft across iterations. We further study additional inference-time strategies such as Best-of-N and Topic Extraction to balance quality and efficiency. Across Yelp, Goodreads, and Amazon datasets, PerFine consistently improves personalization over PGraphRAG, with GEval gains of +7-13%, steady improvements over 3-5 refinement iterations, and scalability with increasing critic size. These results highlight that post-hoc, profile-aware feedback offers a powerful paradigm for personalized LLM generation that is both training-free and model-agnostic.",
        "translated": "个性化文本生成要求模型不仅生成连贯的文本，还需与目标用户的风格、语调和主题焦点保持一致。现有的检索增强方法（如 LaMP 和 PGraphRAG）通过整合用户及其邻居的历史记录来丰富用户画像，但它们仅止于生成阶段，往往导致输出在语调、主题或风格上出现偏移。我们提出 PerFine，一种统一的、无需训练的批判-优化框架，通过迭代的、基于用户画像的反馈机制提升个性化效果。在每次迭代中，大语言模型（LLM）生成器在检索到的用户画像条件下生成初稿，同时一个同样以该画像为条件的批判型 LLM 提供关于语调、词汇、句式结构和主题相关性的结构化反馈。生成器随后根据反馈进行修订，而一种新颖的“淘汰策略”则在各次迭代中保留表现更优的文稿版本。此外，我们进一步研究了多种推理阶段策略，如 Best-of-N 和主题提取，以在生成质量与效率之间取得平衡。在 Yelp、Goodreads 和 Amazon 数据集上的实验表明，PerFine 在个性化性能上始终优于 PGraphRAG，GEval 指标提升达 +7–13%，在 3–5 次优化迭代中持续改进，且随着批判模型规模增大具有良好的可扩展性。这些结果表明，后置的、基于画像的反馈机制为个性化大语言模型生成提供了一种强大范式，该范式无需训练且对模型类型具有无关性。",
        "translated_title": "迭代批评-优化框架用于增强大语言模型个性化",
        "label": [
            "LLM生成式推荐",
            "通用推荐技术"
        ],
        "label_reason": "论文聚焦LLM个性化生成，通过迭代反馈提升内容风格对齐，可应用于推荐系统中的生成式推荐环节。",
        "relevance_score": 7,
        "novelty_score": 8,
        "novelty_reason": "提出训练无关的迭代批评-优化框架，结合结构化反馈与淘汰策略，显著提升个性化生成质量。"
    },
    {
        "title": "MiniOneRec: An Open-Source Framework for Scaling Generative\n  Recommendation",
        "url": "http://arxiv.org/abs/2510.24431v1",
        "pub_date": "2025-10-28",
        "summary": "The recent success of large language models (LLMs) has renewed interest in whether recommender systems can achieve similar scaling benefits. Conventional recommenders, dominated by massive embedding tables, tend to plateau as embedding dimensions grow. In contrast, the emerging generative paradigm replaces embeddings with compact Semantic ID (SID) sequences produced by autoregressive Transformers. Yet most industrial deployments remain proprietary, leaving two fundamental questions open: (1) Do the expected scaling laws hold on public benchmarks? (2) What is the minimal post-training recipe that enables competitive performance?   We present MiniOneRec, to the best of our knowledge, the first fully open-source generative recommendation framework, which provides an end-to-end workflow spanning SID construction, supervised fine-tuning, and recommendation-oriented reinforcement learning. We generate SIDs via a Residual Quantized VAE and post-train Qwen backbones ranging from 0.5B to 7B parameters on the Amazon Review dataset. Our experiments reveal a consistent downward trend in both training and evaluation losses with increasing model size, validating the parameter efficiency of the generative approach. To further enhance performance, we propose a lightweight yet effective post-training pipeline that (1) enforces full-process SID alignment and (2) applies reinforcement learning with constrained decoding and hybrid rewards. Together, these techniques yield significant improvements in both ranking accuracy and candidate diversity.",
        "translated": "近期大语言模型（LLM）的成功重新引发了人们对于推荐系统能否实现类似扩展性收益的关注。传统的推荐系统主要依赖庞大的嵌入表，随着嵌入维度的增长，其性能往往趋于饱和。相比之下，新兴的生成式范式通过自回归Transformer生成紧凑的语义ID（SID）序列，取代了传统的嵌入表示。然而，大多数工业级部署仍为专有系统，导致两个根本性问题尚未解决：（1）在公开基准数据集上，预期的扩展规律是否成立？（2）实现具有竞争力性能所需的最小后训练方案是什么？  \n\n我们提出MiniOneRec，据我们所知，这是首个完全开源的生成式推荐框架，提供了一个端到端的工作流，涵盖SID构建、监督微调以及面向推荐的强化学习。我们在Amazon Review数据集上，通过残差量化变分自编码器（VAE）生成SID，并对参数量从0.5B到7B的Qwen主干模型进行后训练。实验结果表明，随着模型规模增大，训练损失和评估损失均呈现出稳定下降趋势，验证了生成式方法在参数效率上的优势。为进一步提升性能，我们提出了一种轻量但有效的后训练流程，其包含两个关键设计：（1）强制全链路SID对齐；（2）采用受限解码与混合奖励的强化学习策略。这些技术共同显著提升了排序准确率和候选物料多样性。",
        "translated_title": "MiniOneRec：一个用于扩展生成式推荐的开源框架",
        "label": [
            "LLM生成式推荐（LLM-based Generative Recommendation）",
            "精排（Ranking）",
            "序列推荐（Sequential Recommendation）",
            "推荐系统评估（Evaluation Metrics / Offline/Online Testing）"
        ],
        "label_reason": "论文聚焦LLM生成式推荐框架，涵盖SID构建、微调与强化学习，直接解决推荐系统核心问题。",
        "relevance_score": 10,
        "novelty_score": 9,
        "novelty_reason": "提出首个开源生成式推荐框架，结合轻量级微调与强化学习，显著提升准确率与多样性。"
    },
    {
        "title": "From Time and Place to Preference: LLM-Driven Geo-Temporal Context in\n  Recommendations",
        "url": "http://arxiv.org/abs/2510.24430v1",
        "pub_date": "2025-10-28",
        "summary": "Most recommender systems treat timestamps as numeric or cyclical values, overlooking real-world context such as holidays, events, and seasonal patterns. We propose a scalable framework that uses large language models (LLMs) to generate geo-temporal embeddings from only a timestamp and coarse location, capturing holidays, seasonal trends, and local/global events. We then introduce a geo-temporal embedding informativeness test as a lightweight diagnostic, demonstrating on MovieLens, LastFM, and a production dataset that these embeddings provide predictive signal consistent with the outcomes of full model integrations. Geo-temporal embeddings are incorporated into sequential models through (1) direct feature fusion with metadata embeddings or (2) an auxiliary loss that enforces semantic and geo-temporal alignment. Our findings highlight the need for adaptive or hybrid recommendation strategies, and we release a context-enriched MovieLens dataset to support future research.",
        "translated": "大多数推荐系统将时间戳视为数值或循环变量，忽略了节假日、事件和季节性模式等现实世界上下文信息。我们提出了一种可扩展的框架，仅基于时间戳和粗粒度地理位置，利用大语言模型（LLM）生成地理-时间嵌入，从而捕捉节假日、季节性趋势以及本地和全球事件。随后，我们引入了一种地理-时间嵌入信息量测试，作为一种轻量级诊断工具，通过在MovieLens、LastFM以及一个生产数据集上的实验表明，这些嵌入提供了与完整模型集成结果一致的预测信号。地理-时间嵌入通过两种方式融入序列模型：（1）与元数据嵌入进行直接特征融合，或（2）引入辅助损失函数，以强制实现语义和地理-时间对齐。我们的研究结果强调了采用自适应或混合推荐策略的必要性，并发布了一个上下文增强的MovieLens数据集，以支持未来的研究工作。",
        "translated_title": "从时间与地点到偏好：大语言模型驱动的时空上下文在推荐系统中的应用",
        "label": [
            "LLM生成式推荐（LLM-based Generative Recommendation）",
            "序列推荐（Sequential Recommendation）",
            "通用推荐技术（General Recommendation Techniques）"
        ],
        "label_reason": "利用LLM生成时空上下文嵌入，增强推荐系统对时间地点的建模，适用于序列推荐与通用推荐场景",
        "relevance_score": 8,
        "novelty_score": 8,
        "novelty_reason": "创新性地将LLM用于生成地理时间上下文嵌入，提升推荐系统对动态场景的感知能力"
    },
    {
        "title": "Metadata-Driven Retrieval-Augmented Generation for Financial Question\n  Answering",
        "url": "http://arxiv.org/abs/2510.24402v1",
        "pub_date": "2025-10-28",
        "summary": "Retrieval-Augmented Generation (RAG) struggles on long, structured financial filings where relevant evidence is sparse and cross-referenced. This paper presents a systematic investigation of advanced metadata-driven Retrieval-Augmented Generation (RAG) techniques, proposing and evaluating a novel, multi-stage RAG architecture that leverages LLM-generated metadata. We introduce a sophisticated indexing pipeline to create contextually rich document chunks and benchmark a spectrum of enhancements, including pre-retrieval filtering, post-retrieval reranking, and enriched embeddings, benchmarked on the FinanceBench dataset. Our results reveal that while a powerful reranker is essential for precision, the most significant performance gains come from embedding chunk metadata directly with text (\"contextual chunks\"). Our proposed optimal architecture combines LLM-driven pre-retrieval optimizations with these contextual embeddings to achieve superior performance. Additionally, we present a custom metadata reranker that offers a compelling, cost-effective alternative to commercial solutions, highlighting a practical trade-off between peak performance and operational efficiency. This study provides a blueprint for building robust, metadata-aware RAG systems for financial document analysis.",
        "translated": "检索增强生成（RAG）在处理长篇、结构化的财务文件时面临挑战，因为相关证据稀疏且存在交叉引用。本文对先进的元数据驱动式检索增强生成（RAG）技术进行了系统性研究，提出并评估了一种新颖的多阶段RAG架构，该架构利用大语言模型（LLM）生成的元数据。我们引入了一套复杂的索引流程，用于构建上下文丰富的文档片段，并在FinanceBench数据集上对一系列改进技术进行基准测试，包括检索前过滤、检索后重排以及增强嵌入。实验结果表明，尽管强大的重排器对精度至关重要，但性能提升最显著的来源是将片段元数据与文本直接嵌入（“上下文片段”）。我们提出的最优架构结合了LLM驱动的检索前优化与这些上下文嵌入，从而实现了卓越的性能。此外，我们提出了一种定制化的元数据重排器，作为商业解决方案的高效、低成本替代方案，揭示了峰值性能与运营效率之间的实用权衡。本研究为构建稳健、具备元数据感知能力的RAG系统提供了适用于财务文档分析的蓝图。",
        "translated_title": "元数据驱动的检索增强生成在金融问答中的应用",
        "label": [
            "召回",
            "重排",
            "LLM生成式推荐"
        ],
        "label_reason": "论文聚焦RAG架构，涉及召回与重排，虽为金融问答，但方法可迁移至推荐系统。",
        "relevance_score": 4,
        "novelty_score": 7,
        "novelty_reason": "提出多阶段RAG架构与上下文嵌入，结合LLM生成元数据，具创新性但非推荐领域首创。"
    },
    {
        "title": "DUET: Dual Model Co-Training for Entire Space CTR Prediction",
        "url": "http://arxiv.org/abs/2510.24369v1",
        "pub_date": "2025-10-28",
        "summary": "The pre-ranking stage plays a pivotal role in large-scale recommender systems but faces an intrinsic trade-off between model expressiveness and computational efficiency. Owing to the massive candidate pool and strict latency constraints, industry systems often rely on lightweight two-tower architectures, which are computationally efficient yet limited in estimation capability. As a result, they struggle to capture the complex synergistic and suppressive relationships among candidate items, which are essential for producing contextually coherent and diverse recommendation lists. Moreover, this simplicity further amplifies the Sample Selection Bias (SSB) problem, as coarse-grained models trained on biased exposure data must generalize to a much larger candidate space with distinct distributions.   To address these issues, we propose \\textbf{DUET} (\\textbf{DU}al Model Co-Training for \\textbf{E}ntire Space C\\textbf{T}R Prediction), a set-wise pre-ranking framework that achieves expressive modeling under tight computational budgets. Instead of scoring items independently, DUET performs set-level prediction over the entire candidate subset in a single forward pass, enabling information-aware interactions among candidates while amortizing the computational cost across the set. Moreover, a dual model co-training mechanism extends supervision to unexposed items via mutual pseudo-label refinement, effectively mitigating SSB. Validated through extensive offline experiments and online A/B testing, DUET consistently outperforms state-of-the-art baselines and achieves improvements across multiple core business metrics. At present, DUET has been fully deployed in Kuaishou and Kuaishou Lite Apps, serving the main traffic for hundreds of millions of users.",
        "translated": "粗排阶段在大规模推荐系统中起着关键作用，但面临着模型表达能力与计算效率之间的内在权衡。由于候选物料池规模庞大且存在严格的延迟约束，工业界系统通常依赖轻量级的双塔架构，这类架构计算高效，但估计能力有限。因此，它们难以捕捉候选物料之间复杂的协同与抑制关系，而这些关系对于生成上下文连贯且多样化的推荐列表至关重要。此外，这种简单性进一步加剧了样本选择偏差（SSB）问题，因为基于有偏曝光数据训练的粗粒度模型，必须泛化到分布截然不同的更大候选空间中。\n\n为解决上述问题，我们提出 **DUET**（**DU**al Model Co-Training for **E**ntire Space C**T**R Prediction），一种在严格计算预算下实现高表达能力建模的集合式粗排框架。DUET不单独对每个物料打分，而是在单次前向传播中对整个候选子集进行集合级预测，从而在候选物料之间实现信息感知的交互，同时将计算成本在集合中摊薄。此外，通过引入双模型协同训练机制，DUET利用互相对伪标签进行优化，将监督信号扩展至未曝光物料，有效缓解了SSB问题。经过大量离线实验和线上A/B测试验证，DUET持续优于当前最先进的基线方法，并在多个核心业务指标上实现了提升。目前，DUET已在快手和快手极速版App中全面部署，服务于数亿用户的主流量推荐场景。",
        "translated_title": "DUET：用于全空间点击率预测的双模型协同训练",
        "label": [
            "粗排（Pre-ranking）",
            "负采样与对比学习（Negative Sampling / Contrastive Learning）",
            "通用推荐技术（General Recommendation Techniques）"
        ],
        "label_reason": "论文聚焦推荐系统粗排阶段，提出双模型协同训练框架，解决样本偏差与计算效率问题，直接应用于推荐系统。",
        "relevance_score": 8,
        "novelty_score": 8,
        "novelty_reason": "提出集级别预测与双模型伪标签互训机制，有效缓解SSB，提升表达能力与效率，为粗排带来显著改进。"
    },
    {
        "title": "Resource-Efficient LLM Application for Structured Transformation of\n  Unstructured Financial Contracts",
        "url": "http://arxiv.org/abs/2510.23990v1",
        "pub_date": "2025-10-28",
        "summary": "The transformation of unstructured legal contracts into standardized, machine-readable formats is essential for automating financial workflows. The Common Domain Model (CDM) provides a standardized framework for this purpose, but converting complex legal documents like Credit Support Annexes (CSAs) into CDM representations remains a significant challenge. In this paper, we present an extension of the CDMizer framework, a template-driven solution that ensures syntactic correctness and adherence to the CDM schema during contract-to-CDM conversion. We apply this extended framework to a real-world task, comparing its performance with a benchmark developed by the International Swaps and Derivatives Association (ISDA) for CSA clause extraction. Our results show that CDMizer, when integrated with a significantly smaller, open-source Large Language Model (LLM), achieves competitive performance in terms of accuracy and efficiency against larger, proprietary models. This work underscores the potential of resource-efficient solutions to automate legal contract transformation, offering a cost-effective and scalable approach that can meet the needs of financial institutions with constrained resources or strict data privacy requirements.",
        "translated": "将非结构化的法律合同转换为标准化、机器可读的格式，对于自动化金融业务流程至关重要。通用领域模型（Common Domain Model, CDM）为此提供了标准化框架，但将复杂的法律文件（如信用支持附加协议，Credit Support Annexes, CSAs）转换为CDM表示形式仍是一项重大挑战。本文提出了CDMizer框架的扩展版本，这是一种基于模板的解决方案，能够在合同到CDM的转换过程中确保语法正确性并严格遵循CDM模式。我们将该扩展框架应用于一项真实任务，与国际互换与衍生品协会（International Swaps and Derivatives Association, ISDA）为CSA条款提取开发的基准进行性能对比。实验结果表明，当CDMizer与一个规模显著较小、开源的大语言模型（Large Language Model, LLM）集成时，其在准确性和效率方面均能与更大规模的专有模型相媲美。本研究凸显了资源高效型解决方案在自动化法律合同转换中的潜力，提供了一种成本低廉且可扩展的方法，能够满足资源受限或数据隐私要求严格的金融机构的需求。",
        "translated_title": "资源高效的大语言模型在非结构化金融合约结构化转换中的应用",
        "label": [],
        "label_reason": "论文聚焦法律合同结构化转换，属NLP文档处理，与推荐系统无关。",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出轻量级LLM用于合同转换，方法为NLP领域改进，非推荐系统创新。"
    },
    {
        "title": "Generative View Stitching",
        "url": "http://arxiv.org/abs/2510.24718v1",
        "pub_date": "2025-10-28",
        "summary": "Autoregressive video diffusion models are capable of long rollouts that are stable and consistent with history, but they are unable to guide the current generation with conditioning from the future. In camera-guided video generation with a predefined camera trajectory, this limitation leads to collisions with the generated scene, after which autoregression quickly collapses. To address this, we propose Generative View Stitching (GVS), which samples the entire sequence in parallel such that the generated scene is faithful to every part of the predefined camera trajectory. Our main contribution is a sampling algorithm that extends prior work on diffusion stitching for robot planning to video generation. While such stitching methods usually require a specially trained model, GVS is compatible with any off-the-shelf video model trained with Diffusion Forcing, a prevalent sequence diffusion framework that we show already provides the affordances necessary for stitching. We then introduce Omni Guidance, a technique that enhances the temporal consistency in stitching by conditioning on both the past and future, and that enables our proposed loop-closing mechanism for delivering long-range coherence. Overall, GVS achieves camera-guided video generation that is stable, collision-free, frame-to-frame consistent, and closes loops for a variety of predefined camera paths, including Oscar Reutersv\\\"ard's Impossible Staircase. Results are best viewed as videos at https://andrewsonga.github.io/gvs.",
        "translated": "自回归视频扩散模型能够进行稳定且与历史一致的长序列生成，但无法利用来自未来的条件信息来引导当前的生成过程。在具有预定义相机轨迹的相机引导视频生成任务中，这一局限性会导致生成场景与相机轨迹发生碰撞，之后自回归过程会迅速崩溃。为解决该问题，我们提出生成式视图拼接（Generative View Stitching, GVS），该方法能够并行采样整个序列，从而确保生成的场景与预定义相机轨迹的每一部分保持一致。我们的主要贡献是一种采样算法，该算法将先前用于机器人规划的扩散拼接方法拓展至视频生成领域。尽管此类拼接方法通常需要专门训练的模型，但GVS兼容任何基于Diffusion Forcing框架训练的现成视频模型——这是一种主流的序列扩散框架，我们证明其已具备实现拼接所需的基本能力。随后，我们引入全向引导（Omni Guidance）技术，通过同时利用过去和未来的条件信息，增强拼接过程中的时间一致性，并支持我们提出的闭环机制，以实现长程一致性。总体而言，GVS实现了稳定、无碰撞、帧间一致且能够闭合循环的相机引导视频生成，适用于多种预定义相机路径，包括奥斯卡·雷特瑟瓦德（Oscar Reutersv\\\"ard）的“不可能楼梯”。最佳效果请参见视频演示：https://andrewsonga.github.io/gvs。",
        "translated_title": "生成视图拼接",
        "label": [],
        "label_reason": "论文属于图像生成（GAN/扩散模型）范畴，目标是生成新视频而非恢复或增强现有图像质量，非low-level任务。",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出新的采样算法和Omni Guidance技术，改进视频生成一致性，但未解决像素级图像恢复问题。"
    },
    {
        "title": "Uniform Discrete Diffusion with Metric Path for Video Generation",
        "url": "http://arxiv.org/abs/2510.24717v1",
        "pub_date": "2025-10-28",
        "summary": "Continuous-space video generation has advanced rapidly, while discrete approaches lag behind due to error accumulation and long-context inconsistency. In this work, we revisit discrete generative modeling and present Uniform discRete diffuSion with metric pAth (URSA), a simple yet powerful framework that bridges the gap with continuous approaches for the scalable video generation. At its core, URSA formulates the video generation task as an iterative global refinement of discrete spatiotemporal tokens. It integrates two key designs: a Linearized Metric Path and a Resolution-dependent Timestep Shifting mechanism. These designs enable URSA to scale efficiently to high-resolution image synthesis and long-duration video generation, while requiring significantly fewer inference steps. Additionally, we introduce an asynchronous temporal fine-tuning strategy that unifies versatile tasks within a single model, including interpolation and image-to-video generation. Extensive experiments on challenging video and image generation benchmarks demonstrate that URSA consistently outperforms existing discrete methods and achieves performance comparable to state-of-the-art continuous diffusion methods. Code and models are available at https://github.com/baaivision/URSA",
        "translated": "连续空间视频生成技术发展迅速，而离散方法由于误差累积和长上下文不一致性问题进展滞后。在本文中，我们重新审视离散生成建模，提出了一种简单而强大的框架——Uniform discRete diffuSion with metric pAth（URSA），该框架在可扩展视频生成方面弥合了离散方法与连续方法之间的差距。URSA的核心思想是将视频生成任务表述为对离散时空token的迭代全局优化。该框架集成了两个关键设计：线性化度量路径（Linearized Metric Path）和分辨率依赖的时间步偏移机制（Resolution-dependent Timestep Shifting）。这些设计使URSA能够高效扩展至高分辨率图像合成和长时序视频生成，同时显著减少推理步数。此外，我们引入了一种异步时间细调策略，使单一模型能够统一处理多种任务，包括插值和图像到视频生成。在具有挑战性的视频与图像生成基准测试上的大量实验表明，URSA始终优于现有的离散方法，并达到了与当前最先进的连续扩散方法相当的性能。代码和模型可在 https://github.com/baaivision/URSA 获取。",
        "translated_title": "均匀离散扩散与度量路径用于视频生成",
        "label": [],
        "label_reason": "论文聚焦于视频生成，属于图像生成任务，非像素级图像恢复或增强，不属于low-level图像处理。",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出新的离散扩散框架与时间步移机制，对离散生成模型有显著改进，但未针对低层视觉任务。"
    },
    {
        "title": "Routing Matters in MoE: Scaling Diffusion Transformers with Explicit\n  Routing Guidance",
        "url": "http://arxiv.org/abs/2510.24711v1",
        "pub_date": "2025-10-28",
        "summary": "Mixture-of-Experts (MoE) has emerged as a powerful paradigm for scaling model capacity while preserving computational efficiency. Despite its notable success in large language models (LLMs), existing attempts to apply MoE to Diffusion Transformers (DiTs) have yielded limited gains. We attribute this gap to fundamental differences between language and visual tokens. Language tokens are semantically dense with pronounced inter-token variation, while visual tokens exhibit spatial redundancy and functional heterogeneity, hindering expert specialization in vision MoE. To this end, we present ProMoE, an MoE framework featuring a two-step router with explicit routing guidance that promotes expert specialization. Specifically, this guidance encourages the router to partition image tokens into conditional and unconditional sets via conditional routing according to their functional roles, and refine the assignments of conditional image tokens through prototypical routing with learnable prototypes based on semantic content. Moreover, the similarity-based expert allocation in latent space enabled by prototypical routing offers a natural mechanism for incorporating explicit semantic guidance, and we validate that such guidance is crucial for vision MoE. Building on this, we propose a routing contrastive loss that explicitly enhances the prototypical routing process, promoting intra-expert coherence and inter-expert diversity. Extensive experiments on ImageNet benchmark demonstrate that ProMoE surpasses state-of-the-art methods under both Rectified Flow and DDPM training objectives. Code and models will be made publicly available.",
        "translated": "专家混合（Mixture-of-Experts, MoE）作为一种在保持计算效率的同时扩展模型容量的强大范式，近年来备受关注。尽管MoE在大型语言模型（LLMs）中取得了显著成功，但将其应用于扩散Transformer（Diffusion Transformers, DiTs）的现有尝试却收效甚微。我们认为这一差距源于语言标记与视觉标记之间存在根本性差异：语言标记具有语义密集性和显著的标记间差异，而视觉标记则表现出空间冗余性和功能异质性，这阻碍了视觉MoE中专家的专业化。为此，我们提出ProMoE，一种具备两步路由机制和显式路由引导的MoE框架，以促进专家的专业化。具体而言，该引导机制通过条件路由，根据视觉标记的功能角色将其划分为条件性与非条件性集合，并通过基于可学习原型的原型路由（prototypical routing）对条件性图像标记的分配进行精细化调整，以反映其语义内容。此外，原型路由在潜在空间中实现的基于相似性的专家分配，为引入显式语义引导提供了天然机制，我们验证了此类引导对视觉MoE至关重要。基于此，我们进一步提出一种路由对比损失（routing contrastive loss），显式增强原型路由过程，提升专家内部的一致性与专家间的多样性。在ImageNet基准上的大量实验表明，ProMoE在Rectified Flow和DDPM两种训练目标下均超越了当前最先进方法。代码与模型将公开发布。",
        "translated_title": "路由在MoE中的重要性：基于显式路由引导的扩散Transformer扩展",
        "label": [],
        "label_reason": "论文聚焦于扩散Transformer中的MoE路由机制，属于高阶视觉生成任务，非像素级图像恢复。",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出两步路由与原型路由机制，结合对比损失，对视觉MoE有显著改进，但非low-level方向。"
    },
    {
        "title": "Does Object Binding Naturally Emerge in Large Pretrained Vision\n  Transformers?",
        "url": "http://arxiv.org/abs/2510.24709v1",
        "pub_date": "2025-10-28",
        "summary": "Object binding, the brain's ability to bind the many features that collectively represent an object into a coherent whole, is central to human cognition. It groups low-level perceptual features into high-level object representations, stores those objects efficiently and compositionally in memory, and supports human reasoning about individual object instances. While prior work often imposes object-centric attention (e.g., Slot Attention) explicitly to probe these benefits, it remains unclear whether this ability naturally emerges in pre-trained Vision Transformers (ViTs). Intuitively, they could: recognizing which patches belong to the same object should be useful for downstream prediction and thus guide attention. Motivated by the quadratic nature of self-attention, we hypothesize that ViTs represent whether two patches belong to the same object, a property we term IsSameObject. We decode IsSameObject from patch embeddings across ViT layers using a similarity probe, which reaches over 90% accuracy. Crucially, this object-binding capability emerges reliably in self-supervised ViTs (DINO, MAE, CLIP), but markedly weaker in ImageNet-supervised models, suggesting that binding is not a trivial architectural artifact, but an ability acquired through specific pretraining objectives. We further discover that IsSameObject is encoded in a low-dimensional subspace on top of object features, and that this signal actively guides attention. Ablating IsSameObject from model activations degrades downstream performance and works against the learning objective, implying that emergent object binding naturally serves the pretraining objective. Our findings challenge the view that ViTs lack object binding and highlight how symbolic knowledge of \"which parts belong together\" emerges naturally in a connectionist system.",
        "translated": "对象绑定（object binding），即大脑将共同表征一个对象的多种特征整合为一个连贯整体的能力，是人类认知的核心。它将低层次感知特征聚合成高层次的对象表征，以高效且组合化的方式存储这些对象，并支持人类对单个对象实例的推理。尽管先前的工作常通过显式引入以对象为中心的注意力机制（例如 Slot Attention）来探究这些优势，但尚不清楚该能力是否在预训练的视觉Transformer（ViTs）中自然涌现。直观上，这种能力应当存在：识别哪些图像块属于同一对象对下游预测任务有益，因而应能引导注意力机制。受自注意力机制固有的二次特性启发，我们假设ViTs能够表征两个图像块是否属于同一对象，我们将这一属性称为 IsSameObject。我们通过相似性探测器（similarity probe）从ViT各层的图像块嵌入（patch embeddings）中解码出 IsSameObject，其准确率超过90%。关键的是，该对象绑定能力在自监督ViTs（如 DINO、MAE、CLIP）中稳定出现，而在ImageNet监督训练的模型中则显著较弱，这表明对象绑定并非简单的架构副产物，而是通过特定的预训练目标所习得的能力。我们进一步发现，IsSameObject 编码于对象特征之上的一个低维子空间中，且该信号主动引导注意力机制。从模型激活中移除 IsSameObject 会损害下游任务性能，并且违背学习目标，这表明涌现的对象绑定能力自然服务于预训练目标。我们的研究结果挑战了“ViTs缺少对象绑定能力”的观点，并揭示了“哪些部分属于同一整体”这一符号化知识如何在连接主义系统中自然涌现。",
        "translated_title": "物体绑定是否自然地出现在大型预训练视觉Transformer中？",
        "label": [],
        "label_reason": "论文研究ViT中对象绑定能力的涌现，属高层视觉认知，非像素级图像恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出IsSameObject概念并验证其在自监督ViT中涌现，方法新颖，但非low-level任务创新。"
    },
    {
        "title": "MIC-BEV: Multi-Infrastructure Camera Bird's-Eye-View Transformer with\n  Relation-Aware Fusion for 3D Object Detection",
        "url": "http://arxiv.org/abs/2510.24688v1",
        "pub_date": "2025-10-28",
        "summary": "Infrastructure-based perception plays a crucial role in intelligent transportation systems, offering global situational awareness and enabling cooperative autonomy. However, existing camera-based detection models often underperform in such scenarios due to challenges such as multi-view infrastructure setup, diverse camera configurations, degraded visual inputs, and various road layouts. We introduce MIC-BEV, a Transformer-based bird's-eye-view (BEV) perception framework for infrastructure-based multi-camera 3D object detection. MIC-BEV flexibly supports a variable number of cameras with heterogeneous intrinsic and extrinsic parameters and demonstrates strong robustness under sensor degradation. The proposed graph-enhanced fusion module in MIC-BEV integrates multi-view image features into the BEV space by exploiting geometric relationships between cameras and BEV cells alongside latent visual cues. To support training and evaluation, we introduce M2I, a synthetic dataset for infrastructure-based object detection, featuring diverse camera configurations, road layouts, and environmental conditions. Extensive experiments on both M2I and the real-world dataset RoScenes demonstrate that MIC-BEV achieves state-of-the-art performance in 3D object detection. It also remains robust under challenging conditions, including extreme weather and sensor degradation. These results highlight the potential of MIC-BEV for real-world deployment. The dataset and source code are available at: https://github.com/HandsomeYun/MIC-BEV.",
        "translated": "基础设施感知在智能交通系统中发挥着关键作用，能够提供全局态势感知并支持协同自主驾驶。然而，现有的基于摄像头的检测模型在该类场景下通常表现不佳，主要受限于多视角基础设施部署、多样化的摄像头配置、退化的视觉输入以及复杂的道路布局等挑战。本文提出 MIC-BEV，一种基于 Transformer 的鸟瞰图（BEV）感知框架，用于基础设施多摄像头 3D 目标检测。MIC-BEV 能够灵活支持数量可变、具有异构内参和外参的摄像头，并在传感器退化条件下展现出强大的鲁棒性。该框架中提出的图增强融合模块，通过挖掘摄像头与 BEV 单元之间的几何关系以及潜在的视觉线索，将多视角图像特征有效整合至 BEV 空间。为支持模型训练与评估，我们构建了 M2I，一个面向基础设施目标检测的合成数据集，包含多样化的摄像头配置、道路布局和环境条件。在 M2I 和真实世界数据集 RoScenes 上进行的大量实验表明，MIC-BEV 在 3D 目标检测任务中达到了最先进的性能，且在极端天气和传感器退化等挑战性条件下仍保持稳定。这些结果凸显了 MIC-BEV 在实际部署中的巨大潜力。数据集与源代码可于以下地址获取：https://github.com/HandsomeYun/MIC-BEV。",
        "translated_title": "MIC-BEV：基于关系感知融合的多基础设施摄像头鸟瞰图Transformer用于3D目标检测",
        "label": [],
        "label_reason": "论文核心为3D目标检测，属high-level任务，不涉及像素级图像恢复或增强。",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出图增强融合模块，对多相机特征融合有改进，但非low-level图像处理创新。"
    },
    {
        "title": "SAGE: Structure-Aware Generative Video Transitions between Diverse Clips",
        "url": "http://arxiv.org/abs/2510.24667v1",
        "pub_date": "2025-10-28",
        "summary": "Video transitions aim to synthesize intermediate frames between two clips, but naive approaches such as linear blending introduce artifacts that limit professional use or break temporal coherence. Traditional techniques (cross-fades, morphing, frame interpolation) and recent generative inbetweening methods can produce high-quality plausible intermediates, but they struggle with bridging diverse clips involving large temporal gaps or significant semantic differences, leaving a gap for content-aware and visually coherent transitions. We address this challenge by drawing on artistic workflows, distilling strategies such as aligning silhouettes and interpolating salient features to preserve structure and perceptual continuity. Building on this, we propose SAGE (Structure-Aware Generative vidEo transitions) as a zeroshot approach that combines structural guidance, provided via line maps and motion flow, with generative synthesis, enabling smooth, semantically consistent transitions without fine-tuning. Extensive experiments and comparison with current alternatives, namely [FILM, TVG, DiffMorpher, VACE, GI], demonstrate that SAGE outperforms both classical and generative baselines on quantitative metrics and user studies for producing transitions between diverse clips. Code to be released on acceptance.",
        "translated": "视频过渡旨在合成两个视频片段之间的中间帧，但简单的线性混合等朴素方法会引入伪影，限制其专业应用或破坏时间连贯性。传统技术（如交叉淡入淡出、变形、帧插值）以及近期的生成式中间帧生成方法虽然能够生成高质量且合理的中间帧，但在处理涉及较大时间间隔或显著语义差异的多样化片段时仍存在困难，导致在内容感知与视觉连贯性方面存在空白。我们通过借鉴艺术创作流程，提炼出诸如对齐轮廓和插值显著特征等策略，以保持结构与感知连续性，从而应对这一挑战。在此基础上，我们提出SAGE（Structure-Aware Generative vidEo transitions），一种零样本方法，结合由线图和运动流提供的结构引导与生成式合成，实现无需微调即可生成平滑且语义一致的过渡效果。大量实验和与当前主流方法（如FILM、TVG、DiffMorpher、VACE、GI）的对比表明，SAGE在定量指标和用户研究中均优于经典方法与生成式基线，尤其在处理多样化片段的过渡任务中表现突出。代码将在论文被接收后公开。",
        "translated_title": "SAGE：面向多样视频片段的结构感知生成式视频过渡",
        "label": [],
        "label_reason": "论文聚焦视频过渡合成，属高阶生成任务，非像素级图像恢复或增强。",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出结构感知生成框架，结合线图与光流，对视频生成有改进，但非low-level创新。"
    },
    {
        "title": "Group Relative Attention Guidance for Image Editing",
        "url": "http://arxiv.org/abs/2510.24657v1",
        "pub_date": "2025-10-28",
        "summary": "Recently, image editing based on Diffusion-in-Transformer models has undergone rapid development. However, existing editing methods often lack effective control over the degree of editing, limiting their ability to achieve more customized results. To address this limitation, we investigate the MM-Attention mechanism within the DiT model and observe that the Query and Key tokens share a bias vector that is only layer-dependent. We interpret this bias as representing the model's inherent editing behavior, while the delta between each token and its corresponding bias encodes the content-specific editing signals. Based on this insight, we propose Group Relative Attention Guidance, a simple yet effective method that reweights the delta values of different tokens to modulate the focus of the model on the input image relative to the editing instruction, enabling continuous and fine-grained control over editing intensity without any tuning. Extensive experiments conducted on existing image editing frameworks demonstrate that GRAG can be integrated with as few as four lines of code, consistently enhancing editing quality. Moreover, compared to the commonly used Classifier-Free Guidance, GRAG achieves smoother and more precise control over the degree of editing. Our code will be released at https://github.com/little-misfit/GRAG-Image-Editing.",
        "translated": "近年来，基于Diffusion-in-Transformer（DiT）模型的图像编辑技术取得了快速发展。然而，现有编辑方法通常缺乏对编辑程度的有效控制，限制了其生成更具定制化结果的能力。为解决这一局限，我们研究了DiT模型中的MM-Attention机制，发现Query和Key token共享一个仅依赖于网络层的偏置向量。我们将该偏置解释为模型固有的编辑行为，而每个token与其对应偏置之间的差值则编码了与内容相关的编辑信号。基于这一洞察，我们提出了一种简单而有效的方法——组相对注意力引导（Group Relative Attention Guidance, GRAG），通过重新加权不同token的差值，调节模型对输入图像相对于编辑指令的关注焦点，从而在无需任何调参的情况下实现对编辑强度的连续且细粒度的控制。在现有图像编辑框架上进行的大量实验表明，GRAG仅需四行代码即可集成，并能稳定提升编辑质量。此外，与常用的无分类器引导（Classifier-Free Guidance）相比，GRAG在编辑程度的控制上表现出更平滑、更精确的特性。我们的代码将在https://github.com/little-misfit/GRAG-Image-Editing上发布。",
        "translated_title": "群组相对注意力引导用于图像编辑",
        "label": [],
        "label_reason": "论文聚焦图像编辑，属于高阶视觉任务，非像素级图像恢复或增强。",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出相对注意力引导机制，对现有扩散模型编辑方法有改进，但非low-level创新。"
    },
    {
        "title": "Eye-Tracking, Mouse Tracking, Stimulus Tracking,and Decision-Making\n  Datasets in Digital Pathology",
        "url": "http://arxiv.org/abs/2510.24653v1",
        "pub_date": "2025-10-28",
        "summary": "Interpretation of giga-pixel whole-slide images (WSIs) is an important but difficult task for pathologists. Their diagnostic accuracy is estimated to average around 70%. Adding a second pathologist does not substantially improve decision consistency. The field lacks adequate behavioral data to explain diagnostic errors and inconsistencies. To fill in this gap, we present PathoGaze1.0, a comprehensive behavioral dataset capturing the dynamic visual search and decision-making processes of the full diagnostic workflow during cancer diagnosis. The dataset comprises 18.69 hours of eye-tracking, mouse interaction, stimulus tracking, viewport navigation, and diagnostic decision data (EMSVD) collected from 19 pathologists interpreting 397 WSIs. The data collection process emphasizes ecological validity through an application-grounded testbed, called PTAH. In total, we recorded 171,909 fixations, 263,320 saccades, and 1,867,362 mouse interaction events. In addition, such data could also be used to improve the training of both pathologists and AI systems that might support human experts. All experiments were preregistered at https://osf.io/hj9a7, and the complete dataset along with analysis code is available at https://go.osu.edu/pathogaze.",
        "translated": "对千兆像素全切片图像（WSIs）的解读是病理学家的一项重要但极具挑战性的任务。其诊断准确率估计平均约为70%。增加第二位病理学家并未显著提升诊断决策的一致性。该领域目前缺乏足够的行为数据，以解释诊断错误与不一致性的成因。为弥补这一空白，我们提出PathoGaze1.0，这是一个综合性行为数据集，捕捉了癌症诊断完整诊断流程中动态的视觉搜索与决策过程。该数据集包含19位病理学家在解读397张WSIs时所采集的18.69小时的眼动追踪、鼠标交互、刺激追踪、视口导航及诊断决策数据（EMSVD）。数据采集过程通过一个以应用为导向的实验平台PTAH，强调生态效度。总体而言，我们记录了171,909个注视点、263,320次扫视以及1,867,362次鼠标交互事件。此外，此类数据还可用于提升病理学家及可能辅助人类专家的人工智能系统的训练效果。所有实验均已在https://osf.io/hj9a7预先注册，完整数据集及分析代码可通过https://go.osu.edu/pathogaze获取。",
        "translated_title": "眼动追踪、鼠标追踪、刺激物追踪及决策行为数据集在数字病理学中的应用",
        "label": [],
        "label_reason": "论文聚焦病理学诊断行为数据采集，非图像像素级恢复或增强任务，属高阶视觉理解。",
        "relevance_score": 1,
        "novelty_score": 3,
        "novelty_reason": "构建行为数据集，无图像处理新方法或架构创新，属数据收集类工作。"
    },
    {
        "title": "A Dual-Branch CNN for Robust Detection of AI-Generated Facial Forgeries",
        "url": "http://arxiv.org/abs/2510.24640v1",
        "pub_date": "2025-10-28",
        "summary": "The rapid advancement of generative AI has enabled the creation of highly realistic forged facial images, posing significant threats to AI security, digital media integrity, and public trust. Face forgery techniques, ranging from face swapping and attribute editing to powerful diffusion-based image synthesis, are increasingly being used for malicious purposes such as misinformation, identity fraud, and defamation. This growing challenge underscores the urgent need for robust and generalizable face forgery detection methods as a critical component of AI security infrastructure. In this work, we propose a novel dual-branch convolutional neural network for face forgery detection that leverages complementary cues from both spatial and frequency domains. The RGB branch captures semantic information, while the frequency branch focuses on high-frequency artifacts that are difficult for generative models to suppress. A channel attention module is introduced to adaptively fuse these heterogeneous features, highlighting the most informative channels for forgery discrimination. To guide the network's learning process, we design a unified loss function, FSC Loss, that combines focal loss, supervised contrastive loss, and a frequency center margin loss to enhance class separability and robustness. We evaluate our model on the DiFF benchmark, which includes forged images generated from four representative methods: text-to-image, image-to-image, face swap, and face edit. Our method achieves strong performance across all categories and outperforms average human accuracy. These results demonstrate the model's effectiveness and its potential contribution to safeguarding AI ecosystems against visual forgery attacks.",
        "translated": "生成式人工智能的快速发展使得高度逼真的伪造人脸图像得以生成，对人工智能安全、数字媒体完整性以及公众信任构成了重大威胁。人脸伪造技术涵盖从人脸替换、属性编辑到基于扩散模型的强大图像合成等多种方法，正日益被用于恶意目的，如传播虚假信息、身份欺诈和诽谤。这一日益严峻的挑战凸显了开发鲁棒且通用的人脸伪造检测方法的迫切需求，此类方法是人工智能安全基础设施的关键组成部分。在本文中，我们提出了一种新颖的双分支卷积神经网络用于人脸伪造检测，该网络融合了空域和频域中的互补线索。RGB分支用于捕捉语义信息，而频域分支则专注于生成模型难以抑制的高频伪影。我们引入了一个通道注意力模块，以自适应地融合这些异构特征，突出对伪造判别最具信息量的通道。为引导网络的学习过程，我们设计了一种统一的损失函数FSC Loss，该损失函数结合了焦点损失（focal loss）、监督对比损失（supervised contrastive loss）以及频域中心裕度损失（frequency center margin loss），以增强类别可分离性和模型鲁棒性。我们在DiFF基准数据集上对模型进行了评估，该数据集包含由四种代表性方法生成的伪造图像：文本到图像（text-to-image）、图像到图像（image-to-image）、人脸替换（face swap）和人脸编辑（face edit）。我们的方法在所有类别中均表现出色，并超越了平均人类准确率。这些结果证明了模型的有效性及其在抵御视觉伪造攻击、保障人工智能生态系统安全方面的潜在贡献。",
        "translated_title": "双分支卷积神经网络用于鲁棒的AI生成人脸伪造检测",
        "label": [],
        "label_reason": "论文聚焦于AI生成人脸伪造检测，属于高阶视觉理解任务，非像素级图像恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 6,
        "novelty_reason": "提出双分支结构与FSC损失函数，对特征融合和损失设计有改进，但属检测领域常规优化。"
    },
    {
        "title": "GroundLoc: Efficient Large-Scale Outdoor LiDAR-Only Localization",
        "url": "http://arxiv.org/abs/2510.24623v1",
        "pub_date": "2025-10-28",
        "summary": "In this letter, we introduce GroundLoc, a LiDAR-only localization pipeline designed to localize a mobile robot in large-scale outdoor environments using prior maps. GroundLoc employs a Bird's-Eye View (BEV) image projection focusing on the perceived ground area and utilizes the place recognition network R2D2, or alternatively, the non-learning approach Scale-Invariant Feature Transform (SIFT), to identify and select keypoints for BEV image map registration. Our results demonstrate that GroundLoc outperforms state-of-the-art methods on the SemanticKITTI and HeLiPR datasets across various sensors. In the multi-session localization evaluation, GroundLoc reaches an Average Trajectory Error (ATE) well below 50 cm on all Ouster OS2 128 sequences while meeting online runtime requirements. The system supports various sensor models, as evidenced by evaluations conducted with Velodyne HDL-64E, Ouster OS2 128, Aeva Aeries II, and Livox Avia sensors. The prior maps are stored as 2D raster image maps, which can be created from a single drive and require only 4 MB of storage per square kilometer. The source code is available at https://github.com/dcmlr/groundloc.",
        "translated": "在本文中，我们提出了一种名为 GroundLoc 的仅依赖 LiDAR 的定位框架，旨在利用先验地图在大规模室外环境中实现移动机器人的定位。GroundLoc 采用鸟瞰图（Bird's-Eye View, BEV）图像投影，聚焦于感知到的地面区域，并利用场景识别网络 R2D2，或替代地使用非学习方法尺度不变特征变换（Scale-Invariant Feature Transform, SIFT），以识别并选取关键点用于 BEV 图像与地图的配准。实验结果表明，GroundLoc 在 SemanticKITTI 和 HeLiPR 数据集上，针对多种传感器均优于当前最先进的方法。在多会话定位评估中，GroundLoc 在所有 Ouster OS2 128 序列上的平均轨迹误差（Average Trajectory Error, ATE）均低于 50 厘米，同时满足在线运行时的要求。该系统支持多种传感器型号，评估中使用了 Velodyne HDL-64E、Ouster OS2 128、Aeva Aeries II 和 Livox Avia 传感器。先验地图以 2D 栅格图像地图形式存储，可通过单次行驶生成，每平方公里仅需 4 MB 的存储空间。源代码可在 https://github.com/dcmlr/groundloc 获取。",
        "translated_title": "GroundLoc：高效的大规模室外仅LiDAR定位",
        "label": [],
        "label_reason": "论文聚焦LiDAR定位，无图像像素级恢复或增强任务，属于高阶感知与定位系统。",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "提出BEV投影与关键点匹配策略，但未涉及图像处理创新，属传感器融合与定位方法改进。"
    },
    {
        "title": "Physics-Inspired Gaussian Kolmogorov-Arnold Networks for X-ray Scatter\n  Correction in Cone-Beam CT",
        "url": "http://arxiv.org/abs/2510.24579v1",
        "pub_date": "2025-10-28",
        "summary": "Cone-beam CT (CBCT) employs a flat-panel detector to achieve three-dimensional imaging with high spatial resolution. However, CBCT is susceptible to scatter during data acquisition, which introduces CT value bias and reduced tissue contrast in the reconstructed images, ultimately degrading diagnostic accuracy. To address this issue, we propose a deep learning-based scatter artifact correction method inspired by physical prior knowledge. Leveraging the fact that the observed point scatter probability density distribution exhibits rotational symmetry in the projection domain. The method uses Gaussian Radial Basis Functions (RBF) to model the point scatter function and embeds it into the Kolmogorov-Arnold Networks (KAN) layer, which provides efficient nonlinear mapping capabilities for learning high-dimensional scatter features. By incorporating the physical characteristics of the scattered photon distribution together with the complex function mapping capacity of KAN, the model improves its ability to accurately represent scatter. The effectiveness of the method is validated through both synthetic and real-scan experiments. Experimental results show that the model can effectively correct the scatter artifacts in the reconstructed images and is superior to the current methods in terms of quantitative metrics.",
        "translated": "锥形束CT（CBCT）采用平板探测器实现高空间分辨率的三维成像。然而，CBCT在数据采集过程中易受散射影响，导致重建图像中CT值出现偏差且组织对比度降低，最终影响诊断准确性。为解决这一问题，本文提出一种基于物理先验知识的深度学习散射伪影校正方法。该方法利用观测到的点散射概率密度分布在投影域中具有旋转对称性的特点，采用高斯径向基函数（RBF）对点散射函数进行建模，并将其嵌入到Kolmogorov-Arnold网络（KAN）层中，以提供高效非线性映射能力，用于学习高维散射特征。通过结合散射光子分布的物理特性与KAN的复杂函数映射能力，模型增强了对散射现象的准确表征能力。该方法的有效性通过合成数据和真实扫描实验进行了验证。实验结果表明，该模型能够有效校正重建图像中的散射伪影，在定量指标上优于现有方法。",
        "translated_title": "受物理启发的高斯科尔莫戈洛夫-阿诺德网络在锥束CT中用于X射线散射校正",
        "label": [
            "CT金属伪影消除",
            "医学图像增强"
        ],
        "label_reason": "针对CBCT散射伪影进行校正，提升图像对比度与诊断质量，属于医学图像恢复范畴。",
        "relevance_score": 9,
        "novelty_score": 8,
        "novelty_reason": "融合物理先验与KAN网络，构建新型散射建模方法，具有显著结构创新。"
    },
    {
        "title": "OSWorld-MCP: Benchmarking MCP Tool Invocation In Computer-Use Agents",
        "url": "http://arxiv.org/abs/2510.24563v1",
        "pub_date": "2025-10-28",
        "summary": "With advances in decision-making and reasoning capabilities, multimodal agents show strong potential in computer application scenarios. Past evaluations have mainly assessed GUI interaction skills, while tool invocation abilities, such as those enabled by the Model Context Protocol (MCP), have been largely overlooked. Comparing agents with integrated tool invocation to those evaluated only on GUI interaction is inherently unfair. We present OSWorld-MCP, the first comprehensive and fair benchmark for assessing computer-use agents' tool invocation, GUI operation, and decision-making abilities in a real-world environment. We design a novel automated code-generation pipeline to create tools and combine them with a curated selection from existing tools. Rigorous manual validation yields 158 high-quality tools (covering 7 common applications), each verified for correct functionality, practical applicability, and versatility. Extensive evaluations of state-of-the-art multimodal agents on OSWorld-MCP show that MCP tools generally improve task success rates (e.g., from 8.3% to 20.4% for OpenAI o3 at 15 steps, from 40.1% to 43.3% for Claude 4 Sonnet at 50 steps), underscoring the importance of assessing tool invocation capabilities. However, even the strongest models have relatively low tool invocation rates, Only 36.3%, indicating room for improvement and highlighting the benchmark's challenge. By explicitly measuring MCP tool usage skills, OSWorld-MCP deepens understanding of multimodal agents and sets a new standard for evaluating performance in complex, tool-assisted environments. Our code, environment, and data are publicly available at https://osworld-mcp.github.io.",
        "translated": "随着决策与推理能力的进步，多模态智能体在计算机应用场景中展现出巨大潜力。以往的评估主要集中于图形用户界面（GUI）交互能力，而工具调用能力（如通过模型上下文协议 Model Context Protocol, MCP 实现的能力）则被广泛忽视。将具备集成工具调用能力的智能体与仅在GUI交互上进行评估的智能体进行比较，本质上是不公平的。我们提出 OSWorld-MCP，这是首个在真实环境中全面且公平评估计算机使用智能体工具调用、GUI操作及决策能力的基准测试。我们设计了一种新颖的自动化代码生成流水线，用于创建工具，并将其与精选的现有工具相结合。通过严格的逐项人工验证，我们构建了158个高质量工具（覆盖7个常用应用程序），每个工具均经过功能正确性、实际适用性和通用性的验证。在OSWorld-MCP上对当前最先进的多模态智能体进行广泛评估表明，MCP工具通常能显著提升任务成功率（例如，OpenAI o3在15步内成功率从8.3%提升至20.4%，Claude 4 Sonnet在50步内从40.1%提升至43.3%），凸显了评估工具调用能力的重要性。然而，即便是最强的模型，其工具调用率也相对较低，仅为36.3%，表明仍有改进空间，并凸显了该基准测试的挑战性。通过明确测量MCP工具使用技能，OSWorld-MCP深化了对多模态智能体的理解，并为在复杂、工具辅助环境中的性能评估树立了新标准。我们的代码、环境和数据已公开发布于 https://osworld-mcp.github.io。",
        "translated_title": "OSWorld-MCP：计算机使用代理中MCP工具调用的基准测试",
        "label": [],
        "label_reason": "论文聚焦多模态代理的工具调用与决策能力，属于高阶任务，不涉及图像像素级处理。",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "提出新基准和工具生成流程，但未涉及低层视觉创新，属通用评测框架改进。"
    },
    {
        "title": "Latent Sketchpad: Sketching Visual Thoughts to Elicit Multimodal\n  Reasoning in MLLMs",
        "url": "http://arxiv.org/abs/2510.24514v1",
        "pub_date": "2025-10-28",
        "summary": "While Multimodal Large Language Models (MLLMs) excel at visual understanding, they often struggle in complex scenarios that require visual planning and imagination. Inspired by how humans use sketching as a form of visual thinking to develop and communicate ideas, we introduce Latent Sketchpad, a framework that equips MLLMs with an internal visual scratchpad. The internal visual representations of MLLMs have traditionally been confined to perceptual understanding. We repurpose them to support generative visual thought without compromising reasoning ability. Building on frontier MLLMs, our approach integrates visual generation directly into their native autoregressive reasoning process. It allows the model to interleave textual reasoning with the generation of visual latents. These latents guide the internal thought process and can be translated into sketch images for interpretability. To realize this, we introduce two components: a Context-Aware Vision Head autoregressively produces visual representations, and a pretrained Sketch Decoder renders these into human-interpretable images. We evaluate the framework on our new dataset MazePlanning. Experiments across various MLLMs show that Latent Sketchpad delivers comparable or even superior reasoning performance to their backbone. It further generalizes across distinct frontier MLLMs, including Gemma3 and Qwen2.5-VL. By extending model's textual reasoning to visual thinking, our framework opens new opportunities for richer human-computer interaction and broader applications. More details and resources are available on our project page: https://latent-sketchpad.github.io/.",
        "translated": "尽管多模态大语言模型（MLLMs）在视觉理解方面表现出色，但在需要视觉规划与想象的复杂场景中往往表现不佳。受人类通过草图作为视觉思维形式来发展和交流想法的启发，我们提出了一种名为 Latent Sketchpad 的框架，该框架为 MLLMs 赋予一个内部的视觉草稿板。传统上，MLLMs 的内部视觉表征仅限于感知理解。我们重新利用这些表征，使其支持生成式视觉思维，同时不损害其推理能力。基于前沿的 MLLMs，我们的方法将视觉生成直接整合到其原生的自回归推理过程中，使模型能够交错进行文本推理与视觉隐变量的生成。这些隐变量引导模型的内部思维过程，并可被转化为草图图像以增强可解释性。为此，我们引入了两个组件：一个上下文感知的视觉头（Context-Aware Vision Head）自回归地生成视觉表征，以及一个预训练的草图解码器（Sketch Decoder）将这些表征渲染为人类可理解的图像。我们在新构建的数据集 MazePlanning 上对框架进行了评估。在多种 MLLMs 上的实验表明，Latent Sketchpad 在推理性能上可与基座模型持平甚至更优，并且能够跨不同前沿 MLLMs（包括 Gemma3 和 Qwen2.5-VL）实现良好泛化。通过将模型的文本推理延伸至视觉思维，我们的框架为更丰富的人机交互和更广泛的应用开辟了新的可能性。更多细节和资源可访问我们的项目页面：https://latent-sketchpad.github.io/。",
        "translated_title": "潜显草图板：通过草绘视觉思维激发多模态大语言模型的多模态推理",
        "label": [],
        "label_reason": "论文聚焦多模态大模型的视觉推理增强，属高阶视觉理解，非像素级图像恢复或增强任务。",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出视觉思维框架，将生成式视觉表征融入推理过程，对MMLMs有显著创新。"
    },
    {
        "title": "Local Performance vs. Out-of-Distribution Generalization: An Empirical\n  Analysis of Personalized Federated Learning in Heterogeneous Data\n  Environments",
        "url": "http://arxiv.org/abs/2510.24503v1",
        "pub_date": "2025-10-28",
        "summary": "In the context of Federated Learning with heterogeneous data environments, local models tend to converge to their own local model optima during local training steps, deviating from the overall data distributions. Aggregation of these local updates, e.g., with FedAvg, often does not align with the global model optimum (client drift), resulting in an update that is suboptimal for most clients. Personalized Federated Learning approaches address this challenge by exclusively focusing on the average local performances of clients' models on their own data distribution. Generalization to out-of-distribution samples, which is a substantial benefit of FedAvg and represents a significant component of robustness, appears to be inadequately incorporated into the assessment and evaluation processes. This study involves a thorough evaluation of Federated Learning approaches, encompassing both their local performance and their generalization capabilities. Therefore, we examine different stages within a single communication round to enable a more nuanced understanding of the considered metrics. Furthermore, we propose and incorporate a modified approach of FedAvg, designated as Federated Learning with Individualized Updates (FLIU), extending the algorithm by a straightforward individualization step with an adaptive personalization factor. We evaluate and compare the approaches empirically using MNIST and CIFAR-10 under various distributional conditions, including benchmark IID and pathological non-IID, as well as additional novel test environments with Dirichlet distribution specifically developed to stress the algorithms on complex data heterogeneity.",
        "translated": "在异构数据环境下的联邦学习中，本地模型在本地训练阶段倾向于收敛至各自局部最优解，从而偏离整体数据分布。对这些本地更新进行聚合（例如采用FedAvg方法）通常无法与全局模型最优解对齐（即客户端漂移），导致对大多数客户端而言更新结果次优。个性化联邦学习方法通过专门关注客户端模型在其自身数据分布上的平均本地性能来应对这一挑战。然而，FedAvg所具备的对分布外样本的泛化能力——这一能力构成了模型鲁棒性的重要组成部分——在当前的评估与衡量过程中却未得到充分考虑。本研究对联邦学习方法进行了全面评估，涵盖其本地性能与泛化能力两个方面。为此，我们分析单次通信轮次中的不同阶段，以更细致地理解所考虑的评估指标。此外，我们提出并引入一种改进的FedAvg方法，称为具有个性化更新的联邦学习（Federated Learning with Individualized Updates, FLIU），通过一个简单而直观的个性化步骤，并引入自适应个性化因子，对算法进行扩展。我们在MNIST和CIFAR-10数据集上，针对多种数据分布条件（包括基准IID、病态非IID，以及专为强调复杂数据异构性而设计的Dirichlet分布下的新型测试环境）对所提方法进行了实验评估与对比。",
        "translated_title": "局部性能与分布外泛化能力：异构数据环境下个性化联邦学习的实证分析",
        "label": [],
        "label_reason": "论文聚焦联邦学习在异构数据环境下的个性化优化，属于高阶机器学习任务，不涉及像素级图像处理。",
        "relevance_score": 1,
        "novelty_score": 4,
        "novelty_reason": "提出FLIU改进FedAvg，属联邦学习领域常规方法改进，无低级图像处理创新。"
    },
    {
        "title": "Fast and accurate neural reflectance transformation imaging through\n  knowledge distillation",
        "url": "http://arxiv.org/abs/2510.24486v1",
        "pub_date": "2025-10-28",
        "summary": "Reflectance Transformation Imaging (RTI) is very popular for its ability to visually analyze surfaces by enhancing surface details through interactive relighting, starting from only a few tens of photographs taken with a fixed camera and variable illumination. Traditional methods like Polynomial Texture Maps (PTM) and Hemispherical Harmonics (HSH) are compact and fast, but struggle to accurately capture complex reflectance fields using few per-pixel coefficients and fixed bases, leading to artifacts, especially in highly reflective or shadowed areas. The NeuralRTI approach, which exploits a neural autoencoder to learn a compact function that better approximates the local reflectance as a function of light directions, has been shown to produce superior quality at comparable storage cost. However, as it performs interactive relighting with custom decoder networks with many parameters, the rendering step is computationally expensive and not feasible at full resolution for large images on limited hardware. Earlier attempts to reduce costs by directly training smaller networks have failed to produce valid results. For this reason, we propose to reduce its computational cost through a novel solution based on Knowledge Distillation (DisK-NeuralRTI). ...",
        "translated": "反射率变换成像（Reflectance Transformation Imaging, RTI）因其能够通过交互式重光照增强表面细节，从而实现对表面的可视化分析而广受欢迎。该方法仅需使用固定相机和可变光照拍摄的数十张照片即可完成。传统的PTM（多项式纹理图）和HSH（半球谐波）方法具有结构紧凑、计算快速的优点，但受限于每个像素仅使用少量系数和固定基函数来近似复杂的反射场，难以精确捕捉细节，尤其在高反射或阴影区域容易产生伪影。NeuralRTI方法通过利用神经自编码器学习一个紧凑函数，将局部反射率表示为光照方向的函数，已被证明在相近存储成本下可获得更优的重建质量。然而，由于其采用参数量较大的定制解码器网络进行交互式重光照，渲染步骤计算开销巨大，在有限硬件上难以对大尺寸图像实现全分辨率实时处理。此前尝试通过直接训练更小网络以降低成本的方法未能获得有效结果。为此，我们提出一种基于知识蒸馏（Knowledge Distillation）的新型解决方案（DisK-NeuralRTI），以降低其计算成本。...",
        "translated_title": "快速且准确的神经反射变换成像：基于知识蒸馏的方法",
        "label": [
            "图像去反射",
            "图像恢复"
        ],
        "label_reason": "通过知识蒸馏优化神经反射成像，提升反射细节重建质量，属图像恢复范畴",
        "relevance_score": 7,
        "novelty_score": 8,
        "novelty_reason": "首次将知识蒸馏用于NeuralRTI，降低渲染开销并保持重建质量"
    },
    {
        "title": "Decoupled MeanFlow: Turning Flow Models into Flow Maps for Accelerated\n  Sampling",
        "url": "http://arxiv.org/abs/2510.24474v1",
        "pub_date": "2025-10-28",
        "summary": "Denoising generative models, such as diffusion and flow-based models, produce high-quality samples but require many denoising steps due to discretization error. Flow maps, which estimate the average velocity between timesteps, mitigate this error and enable faster sampling. However, their training typically demands architectural changes that limit compatibility with pretrained flow models. We introduce Decoupled MeanFlow, a simple decoding strategy that converts flow models into flow map models without architectural modifications. Our method conditions the final blocks of diffusion transformers on the subsequent timestep, allowing pretrained flow models to be directly repurposed as flow maps. Combined with enhanced training techniques, this design enables high-quality generation in as few as 1 to 4 steps. Notably, we find that training flow models and subsequently converting them is more efficient and effective than training flow maps from scratch. On ImageNet 256x256 and 512x512, our models attain 1-step FID of 2.16 and 2.12, respectively, surpassing prior art by a large margin. Furthermore, we achieve FID of 1.51 and 1.68 when increasing the steps to 4, which nearly matches the performance of flow models while delivering over 100x faster inference.",
        "translated": "去噪生成模型，如扩散模型和基于流的模型，能够生成高质量样本，但由于离散化误差，通常需要大量去噪步数。流图（flow maps）通过估计时间步之间的平均速度，能够缓解该误差并实现更快的采样。然而，其训练通常需要对模型架构进行修改，从而限制了与预训练流模型的兼容性。我们提出了一种简单的解码策略——解耦均值流（Decoupled MeanFlow），可在不改变架构的前提下，将流模型转化为流图模型。我们的方法通过将扩散变换器（diffusion transformers）的最终模块条件于后续时间步，使得预训练的流模型可以直接被复用于流图任务。结合增强的训练技术，该设计能够在仅1至4步内实现高质量生成。值得注意的是，我们发现先训练流模型再将其转化为流图，比从零开始训练流图更高效且更有效。在ImageNet 256x256和512x512数据集上，我们的模型分别在1步采样下达到FID值2.16和2.12，显著超越现有方法。此外，当步数增加至4步时，FID值分别达到1.51和1.68，性能几乎与流模型相当，同时推理速度提升超过100倍。",
        "translated_title": "解耦的MeanFlow：将流模型转化为流图以实现加速采样",
        "label": [],
        "label_reason": "论文聚焦图像生成，非像素级图像恢复或增强，属high-level任务",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "提出解耦策略加速采样，对预训练模型高效复用，有显著改进"
    },
    {
        "title": "Kineo: Calibration-Free Metric Motion Capture From Sparse RGB Cameras",
        "url": "http://arxiv.org/abs/2510.24464v1",
        "pub_date": "2025-10-28",
        "summary": "Markerless multiview motion capture is often constrained by the need for precise camera calibration, limiting accessibility for non-experts and in-the-wild captures. Existing calibration-free approaches mitigate this requirement but suffer from high computational cost and reduced reconstruction accuracy.   We present Kineo, a fully automatic, calibration-free pipeline for markerless motion capture from videos captured by unsynchronized, uncalibrated, consumer-grade RGB cameras. Kineo leverages 2D keypoints from off-the-shelf detectors to simultaneously calibrate cameras, including Brown-Conrady distortion coefficients, and reconstruct 3D keypoints and dense scene point maps at metric scale. A confidence-driven spatio-temporal keypoint sampling strategy, combined with graph-based global optimization, ensures robust calibration at a fixed computational cost independent of sequence length. We further introduce a pairwise reprojection consensus score to quantify 3D reconstruction reliability for downstream tasks.   Evaluations on EgoHumans and Human3.6M demonstrate substantial improvements over prior calibration-free methods. Compared to previous state-of-the-art approaches, Kineo reduces camera translation error by approximately 83-85%, camera angular error by 86-92%, and world mean-per-joint error (W-MPJPE) by 83-91%.   Kineo is also efficient in real-world scenarios, processing multi-view sequences faster than their duration in specific configuration (e.g., 36min to process 1h20min of footage). The full pipeline and evaluation code are openly released to promote reproducibility and practical adoption at https://liris-xr.github.io/kineo/.",
        "translated": "无标记多视角动作捕捉通常受限于精确相机标定的需求，这限制了非专家用户和野外场景下的应用可及性。现有的无标定方法虽缓解了这一需求，但往往计算成本高且重建精度降低。\n\n我们提出 Kineo，一种完全自动、无需标定的无标记动作捕捉流水线，可从未同步、未标定的消费级 RGB 相机所拍摄的视频中实现动作捕捉。Kineo 利用现成检测器提取的 2D 关键点，同时完成相机标定（包括 Brown-Conrady 畸变系数），并以度量尺度重建 3D 关键点和稠密场景点图。一种基于置信度驱动的时空关键点采样策略，结合基于图的全局优化方法，保证了在固定计算成本下、与序列长度无关的鲁棒标定。我们进一步引入了一种成对重投影一致性得分（pairwise reprojection consensus score），用于量化 3D 重建的可靠性，以服务于下游任务。\n\n在 EgoHumans 和 Human3.6M 数据集上的评估表明，Kineo 相较于现有无标定方法取得了显著提升。与之前最先进的方法相比，Kineo 将相机平移误差降低约 83–85%，相机角度误差降低 86–92%，世界坐标系下的关节平均误差（W-MPJPE）降低 83–91%。\n\nKineo 在真实场景中也表现出高效性，在特定配置下处理多视角序列的速度快于视频时长（例如，36 分钟处理 1 小时 20 分钟的视频）。完整的流水线和评估代码已公开发布，以促进可复现性和实际应用，详见 https://liris-xr.github.io/kineo/。",
        "translated_title": "Kineo：基于稀疏RGB相机的无需标定的度量级运动捕捉",
        "label": [],
        "label_reason": "论文聚焦3D运动捕捉，属于高阶视觉任务，不涉及图像像素级恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出新型无标定运动捕捉框架，优化校准与重建流程，具有显著技术改进。"
    },
    {
        "title": "A Critical Study towards the Detection of Parkinsons Disease using ML\n  Technologies",
        "url": "http://arxiv.org/abs/2510.24456v1",
        "pub_date": "2025-10-28",
        "summary": "The proposed solution is Deep Learning Technique that will be able classify three types of tea leaves diseases from which two diseases are caused by the pests and one due to pathogens (infectious organisms) and environmental conditions and also show the area damaged by a disease in leaves. Namely Red Rust, Helopeltis and Red spider mite respectively. In this paper we have evaluated two models namely SSD MobileNet V2 and Faster R-CNN ResNet50 V1 for the object detection. The SSD MobileNet V2 gave precision of 0.209 for IOU range of 0.50:0.95 with recall of 0.02 on IOU 0.50:0.95 and final mAP of 20.9%. While Faster R-CNN ResNet50 V1 has precision of 0.252 on IOU range of 0.50:0.95 and recall of 0.044 on IOU of 0.50:0.95 with a mAP of 25%, which is better than SSD. Also used Mask R-CNN for Object Instance Segmentation where we have implemented our custom method to calculate the damaged diseased portion of leaves. Keywords: Tea Leaf Disease, Deep Learning, Red Rust, Helopeltis and Red Spider Mite, SSD MobileNet V2, Faster R-CNN ResNet50 V1 and Mask RCNN.",
        "translated": "本研究提出了一种深度学习方法，能够对三种茶叶病害进行分类，其中两种由害虫引起，一种由病原体（感染性生物）及环境因素导致，并可显示叶片上病害所造成的受损区域。这三种病害分别为红锈病、Helopeltis病害和红蜘蛛螨病。本文评估了两种目标检测模型，即SSD MobileNet V2和Faster R-CNN ResNet50 V1。SSD MobileNet V2在IOU范围0.50:0.95下达到0.209的精确率，召回率为0.02，最终mAP为20.9%。Faster R-CNN ResNet50 V1在相同IOU范围内精确率为0.252，召回率为0.044，mAP为25%，优于SSD模型。此外，还采用Mask R-CNN进行实例分割，实现了自定义方法以计算叶片上病害受损区域。关键词：茶叶病害、深度学习、红锈病、Helopeltis病害、红蜘蛛螨病、SSD MobileNet V2、Faster R-CNN ResNet50 V1、Mask R-CNN。",
        "translated_title": "一项基于机器学习技术的帕金森病检测关键研究",
        "label": [],
        "label_reason": "论文聚焦于茶树叶病检测与实例分割，属于高阶目标检测与语义理解任务，非像素级图像恢复。",
        "relevance_score": 1,
        "novelty_score": 2,
        "novelty_reason": "方法为常规目标检测模型应用，无显著创新，仅在特定数据集上评估性能。"
    },
    {
        "title": "Rethinking Visual Intelligence: Insights from Video Pretraining",
        "url": "http://arxiv.org/abs/2510.24448v1",
        "pub_date": "2025-10-28",
        "summary": "Large language models (LLMs) have demonstrated that large-scale pretraining enables systems to adapt rapidly to new problems with little supervision in the language domain. This success, however, has not translated as effectively to the visual domain, where models, including LLMs, continue to struggle with compositional understanding, sample efficiency, and general-purpose problem-solving. We investigate Video Diffusion Models (VDMs) as a promising direction for bridging this gap. Pretraining on spatiotemporal data endows these models with strong inductive biases for structure and dynamics, which we hypothesize can support broad task adaptability. To test this, we design a controlled evaluation in which both a pretrained LLM and a pretrained VDM are equipped with lightweight adapters and presented with tasks in their natural modalities. Across benchmarks including ARC-AGI, ConceptARC, visual games, route planning, and cellular automata, VDMs demonstrate higher data efficiency than their language counterparts. Taken together, our results indicate that video pretraining offers inductive biases that support progress toward visual foundation models.",
        "translated": "大型语言模型（LLMs）已证明，大规模预训练能够使系统在语言领域中仅需少量监督即可快速适应新问题。然而，这一成功尚未在视觉领域得到有效迁移，包括LLMs在内的模型在组合理解、样本效率以及通用问题求解方面仍面临挑战。我们探讨视频扩散模型（VDMs）作为弥合这一差距的有前景方向。在时空数据上进行预训练赋予这些模型对结构和动态的强烈归纳偏置，我们假设这可以支持广泛的 task 适应能力。为验证这一假设，我们设计了一项受控评估实验，其中预训练的LLM和预训练的VDM均配备轻量级适配器，并在各自自然模态下执行任务。在ARC-AGI、ConceptARC、视觉游戏、路径规划和细胞自动机等基准任务中，VDMs展现出优于语言模型的数据效率。综合来看，我们的结果表明，视频预训练所提供的归纳偏置有助于推动视觉基础模型的发展。",
        "translated_title": "重思视觉智能：来自视频预训练的洞见",
        "label": [],
        "label_reason": "论文聚焦视频预训练与视觉基础模型，属于高层视觉理解，非像素级图像恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "提出视频扩散模型用于视觉任务适应性，但未涉及low-level图像处理创新。"
    },
    {
        "title": "SPARTA: Evaluating Reasoning Segmentation Robustness through Black-Box\n  Adversarial Paraphrasing in Text Autoencoder Latent Space",
        "url": "http://arxiv.org/abs/2510.24446v1",
        "pub_date": "2025-10-28",
        "summary": "Multimodal large language models (MLLMs) have shown impressive capabilities in vision-language tasks such as reasoning segmentation, where models generate segmentation masks based on textual queries. While prior work has primarily focused on perturbing image inputs, semantically equivalent textual paraphrases-crucial in real-world applications where users express the same intent in varied ways-remain underexplored. To address this gap, we introduce a novel adversarial paraphrasing task: generating grammatically correct paraphrases that preserve the original query meaning while degrading segmentation performance. To evaluate the quality of adversarial paraphrases, we develop a comprehensive automatic evaluation protocol validated with human studies. Furthermore, we introduce SPARTA-a black-box, sentence-level optimization method that operates in the low-dimensional semantic latent space of a text autoencoder, guided by reinforcement learning. SPARTA achieves significantly higher success rates, outperforming prior methods by up to 2x on both the ReasonSeg and LLMSeg-40k datasets. We use SPARTA and competitive baselines to assess the robustness of advanced reasoning segmentation models. We reveal that they remain vulnerable to adversarial paraphrasing-even under strict semantic and grammatical constraints. All code and data will be released publicly upon acceptance.",
        "translated": "多模态大语言模型（MLLMs）在视觉-语言任务（如推理分割）中展现了令人印象深刻的能力，模型能够根据文本查询生成分割掩码。尽管先前的研究主要集中在扰动图像输入方面，但语义等价的文本改写——在现实应用中至关重要，因为用户常以不同方式表达相同意图——却尚未得到充分探索。为弥补这一空白，我们提出一种新颖的对抗性改写任务：生成语法正确的文本改写，使其在保持原始查询语义的同时降低分割性能。为评估对抗性改写的质量，我们开发了一套全面的自动评估协议，并通过人工实验进行了验证。此外，我们提出SPARTA——一种基于文本自编码器低维语义潜在空间的黑盒、句子级优化方法，该方法由强化学习引导。SPARTA在ReasonSeg和LLMSeg-40k数据集上均实现了显著更高的成功 rate，相较于先前方法最高提升达2倍。我们利用SPARTA及竞争性基线方法评估了先进推理分割模型的鲁棒性，结果表明，即使在严格的语义和语法约束下，这些模型仍易受到对抗性改写的影响。所有代码和数据将在论文被接受后公开发布。",
        "translated_title": "SPARTA：通过文本自编码器潜在空间中的黑盒对抗性重述评估推理分割的鲁棒性",
        "label": [],
        "label_reason": "论文聚焦于多模态大模型的文本查询鲁棒性，属于high-level视觉任务，非像素级图像处理。",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出在文本自编码器潜空间中进行黑盒对抗重述，方法新颖，但不涉及图像恢复或增强。"
    },
    {
        "title": "VFXMaster: Unlocking Dynamic Visual Effect Generation via In-Context\n  Learning",
        "url": "http://arxiv.org/abs/2510.25772v1",
        "pub_date": "2025-10-29",
        "summary": "Visual effects (VFX) are crucial to the expressive power of digital media, yet their creation remains a major challenge for generative AI. Prevailing methods often rely on the one-LoRA-per-effect paradigm, which is resource-intensive and fundamentally incapable of generalizing to unseen effects, thus limiting scalability and creation. To address this challenge, we introduce VFXMaster, the first unified, reference-based framework for VFX video generation. It recasts effect generation as an in-context learning task, enabling it to reproduce diverse dynamic effects from a reference video onto target content. In addition, it demonstrates remarkable generalization to unseen effect categories. Specifically, we design an in-context conditioning strategy that prompts the model with a reference example. An in-context attention mask is designed to precisely decouple and inject the essential effect attributes, allowing a single unified model to master the effect imitation without information leakage. In addition, we propose an efficient one-shot effect adaptation mechanism to boost generalization capability on tough unseen effects from a single user-provided video rapidly. Extensive experiments demonstrate that our method effectively imitates various categories of effect information and exhibits outstanding generalization to out-of-domain effects. To foster future research, we will release our code, models, and a comprehensive dataset to the community.",
        "translated": "视觉特效（VFX）对于数字媒体的表现力至关重要，但其生成仍是生成式人工智能面临的主要挑战。现有方法通常依赖于“每种特效一个LoRA”的范式，该方法资源消耗大，且本质上无法泛化到未见过的特效，从而限制了系统的可扩展性和创作能力。为应对这一挑战，我们提出VFXMaster，这是首个统一的、基于参考的视觉特效视频生成框架。该框架将特效生成重构为一种上下文学习任务，使其能够从参考视频中提取并复现多样化的动态特效到目标内容上。此外，该框架展现出对未见过特效类别的显著泛化能力。具体而言，我们设计了一种上下文条件策略，通过参考示例引导模型生成。我们还设计了一种上下文注意力掩码，以精确地解耦并注入关键的特效属性，使单个统一模型能够在无信息泄露的情况下掌握特效模仿能力。此外，我们提出了一种高效的单次特效适配机制，能够快速利用用户提供的单个视频，增强模型对复杂未见特效的泛化能力。大量实验表明，我们的方法能够有效模仿多种类别的特效信息，并在跨领域特效上展现出卓越的泛化性能。为促进后续研究，我们将向社区开放代码、模型以及一个全面的数据集。",
        "translated_title": "VFXMaster：通过上下文学习解锁动态视觉效果生成",
        "label": [],
        "label_reason": "论文聚焦于动态视觉效果生成，属于图像生成类任务，非像素级图像恢复或增强，不属low-level。",
        "relevance_score": 1,
        "novelty_score": 6,
        "novelty_reason": "提出in-context学习框架与注意力掩码，具一定创新，但属生成任务，非图像复原领域核心突破。"
    },
    {
        "title": "FreeArt3D: Training-Free Articulated Object Generation using 3D\n  Diffusion",
        "url": "http://arxiv.org/abs/2510.25765v1",
        "pub_date": "2025-10-29",
        "summary": "Articulated 3D objects are central to many applications in robotics, AR/VR, and animation. Recent approaches to modeling such objects either rely on optimization-based reconstruction pipelines that require dense-view supervision or on feed-forward generative models that produce coarse geometric approximations and often overlook surface texture. In contrast, open-world 3D generation of static objects has achieved remarkable success, especially with the advent of native 3D diffusion models such as Trellis. However, extending these methods to articulated objects by training native 3D diffusion models poses significant challenges. In this work, we present FreeArt3D, a training-free framework for articulated 3D object generation. Instead of training a new model on limited articulated data, FreeArt3D repurposes a pre-trained static 3D diffusion model (e.g., Trellis) as a powerful shape prior. It extends Score Distillation Sampling (SDS) into the 3D-to-4D domain by treating articulation as an additional generative dimension. Given a few images captured in different articulation states, FreeArt3D jointly optimizes the object's geometry, texture, and articulation parameters without requiring task-specific training or access to large-scale articulated datasets. Our method generates high-fidelity geometry and textures, accurately predicts underlying kinematic structures, and generalizes well across diverse object categories. Despite following a per-instance optimization paradigm, FreeArt3D completes in minutes and significantly outperforms prior state-of-the-art approaches in both quality and versatility.",
        "translated": "可动三维物体在机器人、增强现实/虚拟现实（AR/VR）以及动画等众多应用中具有核心地位。当前针对此类物体的建模方法主要分为两类：一类依赖于基于优化的重建流程，这类方法需要密集视角的监督；另一类则采用前馈生成模型，往往只能生成粗糙的几何近似，并且常忽略表面纹理细节。相比之下，静态三维物体的开放世界生成已取得显著成功，尤其是在原生三维扩散模型（如 Trellis）出现后。然而，将这些方法扩展至可动物体，通过训练原生三维扩散模型，仍面临诸多挑战。在本文中，我们提出 FreeArt3D，一种无需训练的可动三维物体生成框架。FreeArt3D 并非在有限的可动数据上训练新模型，而是复用预训练的静态三维扩散模型（例如 Trellis）作为强大的形状先验。它通过将可动性视为额外的生成维度，将 Score Distillation Sampling（SDS）方法扩展至三维到四维（3D-to-4D）领域。给定在不同可动状态下采集的少量图像，FreeArt3D 能够联合优化物体的几何结构、纹理以及可动参数，无需针对特定任务进行训练，也无需访问大规模可动数据集。我们的方法能够生成高保真度的几何结构与纹理，准确预测底层运动学结构，并在多种物体类别上表现出良好的泛化能力。尽管采用逐实例优化范式，FreeArt3D 仍能在数分钟内完成，且在生成质量与适用性方面显著优于现有最先进方法。",
        "translated_title": "FreeArt3D：基于3D扩散模型的无训练关节物体生成",
        "label": [],
        "label_reason": "论文聚焦3D生成与运动建模，属于高阶视觉任务，非像素级图像恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出将3D扩散模型扩展至4D领域，引入新采样框架，对生成方法有显著改进。"
    },
    {
        "title": "Multimodal Spatial Reasoning in the Large Model Era: A Survey and\n  Benchmarks",
        "url": "http://arxiv.org/abs/2510.25760v1",
        "pub_date": "2025-10-29",
        "summary": "Humans possess spatial reasoning abilities that enable them to understand spaces through multimodal observations, such as vision and sound. Large multimodal reasoning models extend these abilities by learning to perceive and reason, showing promising performance across diverse spatial tasks. However, systematic reviews and publicly available benchmarks for these models remain limited. In this survey, we provide a comprehensive review of multimodal spatial reasoning tasks with large models, categorizing recent progress in multimodal large language models (MLLMs) and introducing open benchmarks for evaluation. We begin by outlining general spatial reasoning, focusing on post-training techniques, explainability, and architecture. Beyond classical 2D tasks, we examine spatial relationship reasoning, scene and layout understanding, as well as visual question answering and grounding in 3D space. We also review advances in embodied AI, including vision-language navigation and action models. Additionally, we consider emerging modalities such as audio and egocentric video, which contribute to novel spatial understanding through new sensors. We believe this survey establishes a solid foundation and offers insights into the growing field of multimodal spatial reasoning. Updated information about this survey, codes and implementation of the open benchmarks can be found at https://github.com/zhengxuJosh/Awesome-Spatial-Reasoning.",
        "translated": "人类具备空间推理能力，能够通过视觉、听觉等多模态观测来理解空间环境。大型多模态推理模型通过学习感知与推理能力，进一步拓展了这种能力，在多种空间任务中展现出优异的性能。然而，针对这些模型的系统性综述和公开可用的基准测试仍较为有限。在本文综述中，我们全面回顾了基于大模型的多模态空间推理任务，对近期多模态大语言模型（MLLMs）的进展进行了分类，并介绍了可用于评估的开放基准。我们首先概述了通用空间推理的基本框架，重点探讨了后训练技术、可解释性及模型架构。除传统的二维空间任务外，我们还研究了空间关系推理、场景与布局理解，以及三维空间中的视觉问答与定位任务。此外，我们回顾了具身人工智能（embodied AI）领域的最新进展，包括视觉-语言导航与动作模型。同时，我们也考察了音频、第一人称视频等新兴模态，这些模态通过新型传感器为实现新颖的空间理解提供了可能。我们认为，本综述为多模态空间推理这一快速发展领域奠定了坚实基础，并提供了有价值的见解。关于本综述的最新信息、代码及开放基准的实现，可访问 https://github.com/zhengxuJosh/Awesome-Spatial-Reasoning。",
        "translated_title": "多模态空间推理在大模型时代：综述与基准测试",
        "label": [],
        "label_reason": "论文聚焦多模态空间推理与大模型，属高阶视觉理解任务，不涉及图像像素级恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 3,
        "novelty_reason": "为综述性工作，整理现有方法与基准，无新架构或算法创新。"
    },
    {
        "title": "Hawk: Leveraging Spatial Context for Faster Autoregressive Text-to-Image\n  Generation",
        "url": "http://arxiv.org/abs/2510.25739v1",
        "pub_date": "2025-10-29",
        "summary": "Autoregressive (AR) image generation models are capable of producing high-fidelity images but often suffer from slow inference due to their inherently sequential, token-by-token decoding process. Speculative decoding, which employs a lightweight draft model to approximate the output of a larger AR model, has shown promise in accelerating text generation without compromising quality. However, its application to image generation remains largely underexplored. The challenges stem from a significantly larger sampling space, which complicates the alignment between the draft and target model outputs, coupled with the inadequate use of the two-dimensional spatial structure inherent in images, thereby limiting the modeling of local dependencies. To overcome these challenges, we introduce Hawk, a new approach that harnesses the spatial structure of images to guide the speculative model toward more accurate and efficient predictions. Experimental results on multiple text-to-image benchmarks demonstrate a 1.71x speedup over standard AR models, while preserving both image fidelity and diversity.",
        "translated": "自回归（AR）图像生成模型能够生成高保真图像，但由于其本质上逐 token 顺序解码的过程，通常存在推理速度较慢的问题。推测解码（speculative decoding）通过使用轻量级草稿模型来近似较大 AR 模型的输出，在加速文本生成的同时保持生成质量方面已展现出潜力。然而，该方法在图像生成中的应用仍处于初步探索阶段。其主要挑战源于采样空间显著增大，这使得草稿模型与目标模型输出之间的对齐变得复杂，同时现有方法未能充分利用图像固有的二维空间结构，从而限制了局部依赖关系的建模能力。为应对这些挑战，我们提出 Hawk，一种新的方法，该方法利用图像的空间结构来引导推测模型实现更准确且高效的预测。在多个文本到图像基准数据集上的实验结果表明，与标准 AR 模型相比，Hawk 实现了 1.71 倍的推理速度提升，同时保持了图像的保真度和多样性。",
        "translated_title": "Hawk：利用空间上下文实现更快的自回归文本到图像生成",
        "label": [],
        "label_reason": "论文聚焦文本到图像生成，属于图像生成任务，非像素级图像恢复或增强，不属low-level。",
        "relevance_score": 1,
        "novelty_score": 6,
        "novelty_reason": "提出空间上下文指导的推测解码，提升生成效率，属方法改进但非low-level创新。"
    },
    {
        "title": "Feedback Alignment Meets Low-Rank Manifolds: A Structured Recipe for\n  Local Learning",
        "url": "http://arxiv.org/abs/2510.25594v1",
        "pub_date": "2025-10-29",
        "summary": "Training deep neural networks (DNNs) with backpropagation (BP) achieves state-of-the-art accuracy but requires global error propagation and full parameterization, leading to substantial memory and computational overhead. Direct Feedback Alignment (DFA) enables local, parallelizable updates with lower memory requirements but is limited by unstructured feedback and poor scalability in deeper architectures, specially convolutional neural networks. To address these limitations, we propose a structured local learning framework that operates directly on low-rank manifolds defined by the Singular Value Decomposition (SVD) of weight matrices. Each layer is trained in its decomposed form, with updates applied to the SVD components using a composite loss that integrates cross-entropy, subspace alignment, and orthogonality regularization. Feedback matrices are constructed to match the SVD structure, ensuring consistent alignment between forward and feedback pathways. Our method reduces the number of trainable parameters relative to the original DFA model, without relying on pruning or post hoc compression. Experiments on CIFAR-10, CIFAR-100, and ImageNet show that our method achieves accuracy comparable to that of BP. Ablation studies confirm the importance of each loss term in the low-rank setting. These results establish local learning on low-rank manifolds as a principled and scalable alternative to full-rank gradient-based training.",
        "translated": "训练深度神经网络（DNNs）采用反向传播（BP）方法可达到最先进的准确率，但需要全局误差传播和完整的参数化，导致显著的内存和计算开销。直接反馈对齐（DFA）能够实现局部、可并行化的更新，且内存需求较低，但受限于无结构的反馈信号以及在更深架构（特别是卷积神经网络）中扩展性较差的问题。为克服这些局限，我们提出一种结构化的局部学习框架，该框架直接在由权重矩阵奇异值分解（SVD）定义的低秩流形上运行。每一层均以分解形式进行训练，通过结合交叉熵、子空间对齐和正交性正则化的复合损失函数，对SVD各分量进行更新。反馈矩阵被构建为与SVD结构相匹配，确保前向与反馈路径之间的一致对齐。我们的方法相对于原始DFA模型减少了可训练参数数量，且无需依赖剪枝或后处理压缩。在CIFAR-10、CIFAR-100和ImageNet上的实验表明，我们的方法实现了与BP相当的准确率。消融研究验证了在低秩设置下每个损失项的重要性。这些结果确立了在低秩流形上的局部学习，作为全秩梯度训练的一种原理性且可扩展的替代方案。",
        "translated_title": "反馈对齐与低秩流形：局部学习的结构化方案",
        "label": [],
        "label_reason": "论文聚焦于神经网络训练机制优化，属于模型训练方法研究，不涉及图像像素级恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出基于SVD的低秩结构化反馈对齐，改进DFA训练效率，具有方法创新性。"
    },
    {
        "title": "RegionE: Adaptive Region-Aware Generation for Efficient Image Editing",
        "url": "http://arxiv.org/abs/2510.25590v1",
        "pub_date": "2025-10-29",
        "summary": "Recently, instruction-based image editing (IIE) has received widespread attention. In practice, IIE often modifies only specific regions of an image, while the remaining areas largely remain unchanged. Although these two types of regions differ significantly in generation difficulty and computational redundancy, existing IIE models do not account for this distinction, instead applying a uniform generation process across the entire image. This motivates us to propose RegionE, an adaptive, region-aware generation framework that accelerates IIE tasks without additional training. Specifically, the RegionE framework consists of three main components: 1) Adaptive Region Partition. We observed that the trajectory of unedited regions is straight, allowing for multi-step denoised predictions to be inferred in a single step. Therefore, in the early denoising stages, we partition the image into edited and unedited regions based on the difference between the final estimated result and the reference image. 2) Region-Aware Generation. After distinguishing the regions, we replace multi-step denoising with one-step prediction for unedited areas. For edited regions, the trajectory is curved, requiring local iterative denoising. To improve the efficiency and quality of local iterative generation, we propose the Region-Instruction KV Cache, which reduces computational cost while incorporating global information. 3) Adaptive Velocity Decay Cache. Observing that adjacent timesteps in edited regions exhibit strong velocity similarity, we further propose an adaptive velocity decay cache to accelerate the local denoising process. We applied RegionE to state-of-the-art IIE base models, including Step1X-Edit, FLUX.1 Kontext, and Qwen-Image-Edit. RegionE achieved acceleration factors of 2.57, 2.41, and 2.06. Evaluations by GPT-4o confirmed that semantic and perceptual fidelity were well preserved.",
        "translated": "近年来，基于指令的图像编辑（IIE）受到了广泛关注。在实际应用中，IIE通常仅修改图像的特定区域，而其余区域基本保持不变。尽管这两类区域在生成难度和计算冗余方面存在显著差异，但现有的IIE模型并未考虑这种区别，而是对整幅图像采用统一的生成流程。这一现象促使我们提出RegionE，一种自适应的、具备区域感知能力的生成框架，能够在无需额外训练的前提下加速IIE任务。具体而言，RegionE框架包含三个主要组件：1）自适应区域划分。我们观察到未编辑区域的去噪轨迹呈直线，因此可以在单步内推断出多步去噪预测结果。因此，在早期去噪阶段，我们基于最终估计结果与参考图像之间的差异，将图像划分为编辑区域和未编辑区域。2）区域感知生成。在区分区域后，我们对未编辑区域采用单步预测替代多步去噪；对于编辑区域，其轨迹呈曲线，需进行局部迭代去噪。为提升局部迭代生成的效率与质量，我们提出区域-指令KV缓存（Region-Instruction KV Cache），在降低计算成本的同时融合全局信息。3）自适应速度衰减缓存。我们观察到编辑区域内相邻时间步具有较强的速度相似性，因此进一步提出自适应速度衰减缓存，以加速局部去噪过程。我们将RegionE应用于当前最先进的IIE基础模型，包括Step1X-Edit、FLUX.1 Kontext和Qwen-Image-Edit。RegionE实现了2.57、2.41和2.06的加速倍数。经GPT-4o评估，语义和感知保真度均得到良好保持。",
        "translated_title": "RegionE：面向高效图像编辑的自适应区域感知生成",
        "label": [],
        "label_reason": "论文聚焦图像编辑，属于高阶视觉任务，非像素级图像恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出区域感知生成框架，优化扩散模型推理效率，具一定创新性。"
    },
    {
        "title": "Comparative Study of UNet-based Architectures for Liver Tumor\n  Segmentation in Multi-Phase Contrast-Enhanced Computed Tomography",
        "url": "http://arxiv.org/abs/2510.25522v1",
        "pub_date": "2025-10-29",
        "summary": "Segmentation of liver structures in multi-phase contrast-enhanced computed tomography (CECT) plays a crucial role in computer-aided diagnosis and treatment planning for liver diseases, including tumor detection. In this study, we investigate the performance of UNet-based architectures for liver tumor segmentation, starting from the original UNet and extending to UNet3+ with various backbone networks. We evaluate ResNet, Transformer-based, and State-space (Mamba) backbones, all initialized with pretrained weights. Surprisingly, despite the advances in modern architecture, ResNet-based models consistently outperform Transformer- and Mamba-based alternatives across multiple evaluation metrics. To further improve segmentation quality, we introduce attention mechanisms into the backbone and observe that incorporating the Convolutional Block Attention Module (CBAM) yields the best performance. ResNetUNet3+ with CBAM module not only produced the best overlap metrics with a Dice score of 0.755 and IoU of 0.662, but also achieved the most precise boundary delineation, evidenced by the lowest HD95 distance of 77.911. The model's superiority was further cemented by its leading overall accuracy of 0.925 and specificity of 0.926, showcasing its robust capability in accurately identifying both lesion and healthy tissue. To further enhance interpretability, Grad-CAM visualizations were employed to highlight the region's most influential predictions, providing insights into its decision-making process. These findings demonstrate that classical ResNet architecture, when combined with modern attention modules, remain highly competitive for medical image segmentation tasks, offering a promising direction for liver tumor detection in clinical practice.",
        "translated": "在多期增强计算机断层扫描（CECT）中对肝脏结构进行分割，在肝病（包括肿瘤检测）的计算机辅助诊断与治疗规划中具有重要作用。本研究探讨了基于UNet架构的肝脏肿瘤分割性能，从原始UNet出发，扩展至UNet3+并结合多种骨干网络。我们评估了ResNet、基于Transformer以及状态空间（Mamba）的骨干网络，所有模型均采用预训练权重进行初始化。令人意外的是，尽管现代架构取得显著进展，ResNet基模型在多个评估指标上始终优于Transformer和Mamba基模型。为进一步提升分割质量，我们在骨干网络中引入注意力机制，并观察到集成卷积块注意力模块（CBAM）可获得最佳性能。带有CBAM模块的ResNetUNet3+不仅在重叠度指标上表现最佳（Dice得分为0.755，IoU为0.662），而且实现了最精确的边界勾勒，其HD95距离最低，为77.911。该模型在整体准确率（0.925）和特异性（0.926）方面也位居首位，展现出其在准确识别病灶和健康组织方面的强大鲁棒性。为进一步增强可解释性，我们采用Grad-CAM可视化技术，突出显示对预测最具影响力的区域，从而揭示模型的决策过程。这些结果表明，经典的ResNet架构在结合现代注意力模块后，在医学图像分割任务中仍具有高度竞争力，为临床实践中肝脏肿瘤检测提供了有前景的方向。",
        "translated_title": "基于U-Net架构的多期增强CT肝肿瘤分割对比研究",
        "label": [],
        "label_reason": "论文聚焦肝脏肿瘤分割，属于医学图像分析中的高阶语义理解任务，非像素级图像恢复。",
        "relevance_score": 1,
        "novelty_score": 4,
        "novelty_reason": "对UNet变体进行比较，引入CBAM，属常规改进，无根本性创新。"
    },
    {
        "title": "FaCT: Faithful Concept Traces for Explaining Neural Network Decisions",
        "url": "http://arxiv.org/abs/2510.25512v1",
        "pub_date": "2025-10-29",
        "summary": "Deep networks have shown remarkable performance across a wide range of tasks, yet getting a global concept-level understanding of how they function remains a key challenge. Many post-hoc concept-based approaches have been introduced to understand their workings, yet they are not always faithful to the model. Further, they make restrictive assumptions on the concepts a model learns, such as class-specificity, small spatial extent, or alignment to human expectations. In this work, we put emphasis on the faithfulness of such concept-based explanations and propose a new model with model-inherent mechanistic concept-explanations. Our concepts are shared across classes and, from any layer, their contribution to the logit and their input-visualization can be faithfully traced. We also leverage foundation models to propose a new concept-consistency metric, C$^2$-Score, that can be used to evaluate concept-based methods. We show that, compared to prior work, our concepts are quantitatively more consistent and users find our concepts to be more interpretable, all while retaining competitive ImageNet performance.",
        "translated": "深度网络在广泛的任务中展现了卓越的性能，然而对其工作机理进行全局层面的概念性理解仍是一项关键挑战。为了解释其工作机制，已有大量基于事后概念分析的方法被提出，但这些方法并不总能忠实反映模型的真实行为。此外，它们通常对模型所学习的概念做出严格假设，例如类别特异性、空间范围较小或与人类直觉一致等。在本研究中，我们着重于概念解释的忠实性，提出一种具有内在机制性概念解释能力的新模型。我们所定义的概念在各类别之间共享，且从任意网络层出发，均可准确追溯其对logit的贡献以及输入可视化结果。同时，我们借助基础模型提出一种新的概念一致性度量指标C²-Score，可用于评估各类基于概念的解释方法。实验表明，与现有方法相比，我们的概念在量化上具有更高的一致性，用户也认为其更具可解释性，同时在ImageNet上仍保持了具有竞争力的性能。",
        "translated_title": "FaCT：用于解释神经网络决策的忠实概念轨迹",
        "label": [],
        "label_reason": "论文聚焦模型解释性，属于高阶视觉任务，非像素级图像处理",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出可追溯概念解释机制，引入C²-Score评估指标，具有创新性"
    },
    {
        "title": "SPADE: Sparsity Adaptive Depth Estimator for Zero-Shot, Real-Time,\n  Monocular Depth Estimation in Underwater Environments",
        "url": "http://arxiv.org/abs/2510.25463v1",
        "pub_date": "2025-10-29",
        "summary": "Underwater infrastructure requires frequent inspection and maintenance due to harsh marine conditions. Current reliance on human divers or remotely operated vehicles is limited by perceptual and operational challenges, especially around complex structures or in turbid water. Enhancing the spatial awareness of underwater vehicles is key to reducing piloting risks and enabling greater autonomy. To address these challenges, we present SPADE: SParsity Adaptive Depth Estimator, a monocular depth estimation pipeline that combines pre-trained relative depth estimator with sparse depth priors to produce dense, metric scale depth maps. Our two-stage approach first scales the relative depth map with the sparse depth points, then refines the final metric prediction with our proposed Cascade Conv-Deformable Transformer blocks. Our approach achieves improved accuracy and generalisation over state-of-the-art baselines and runs efficiently at over 15 FPS on embedded hardware, promising to support practical underwater inspection and intervention. This work has been submitted to IEEE Journal of Oceanic Engineering Special Issue of AUV 2026.",
        "translated": "水下基础设施因恶劣的海洋环境需频繁进行巡检与维护。目前依赖人力潜水员或遥控水下航行器（ROV）的方式受限于感知与操作方面的挑战，尤其在复杂结构周围或浑浊水域中表现尤为明显。提升水下航行器的空间感知能力，对于降低操控风险、实现更高程度的自主性至关重要。为应对上述挑战，本文提出 SPADE：SParsity Adaptive Depth Estimator，一种单目深度估计框架，通过融合预训练的相对深度估计器与稀疏深度先验，生成稠密且具备度量尺度的深度图。我们的两阶段方法首先利用稀疏深度点对相对深度图进行尺度校准，随后通过所提出的级联卷积-可变形Transformer模块对最终的度量深度预测进行精细化优化。该方法在准确性和泛化能力上均优于当前主流基线方法，且在嵌入式硬件上可实现超过15 FPS的高效运行，有望支持实际水下巡检与干预任务。本工作已投稿至 IEEE Journal of Oceanic Engineering AUV 2026 特刊。",
        "translated_title": "SPADE：用于水下环境中零样本、实时、单目深度估计的稀疏自适应深度估计算法",
        "label": [],
        "label_reason": "核心为单目深度估计，属于3D感知任务，非像素级图像恢复或增强，不属low-level。",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出级联卷积-可变形Transformer块，结构有改进，但未解决图像质量退化问题。"
    },
    {
        "title": "More than a Moment: Towards Coherent Sequences of Audio Descriptions",
        "url": "http://arxiv.org/abs/2510.25440v1",
        "pub_date": "2025-10-29",
        "summary": "Audio Descriptions (ADs) convey essential on-screen information, allowing visually impaired audiences to follow videos. To be effective, ADs must form a coherent sequence that helps listeners to visualise the unfolding scene, rather than describing isolated moments. However, most automatic methods generate each AD independently, often resulting in repetitive, incoherent descriptions. To address this, we propose a training-free method, CoherentAD, that first generates multiple candidate descriptions for each AD time interval, and then performs auto-regressive selection across the sequence to form a coherent and informative narrative. To evaluate AD sequences holistically, we introduce a sequence-level metric, StoryRecall, which measures how well the predicted ADs convey the ground truth narrative, alongside repetition metrics that capture the redundancy across consecutive AD outputs. Our method produces coherent AD sequences with enhanced narrative understanding, outperforming prior approaches that rely on independent generations.",
        "translated": "音频描述（Audio Descriptions, ADs）能够传递画面中的关键信息，使视障观众得以理解和跟随视频内容。为实现有效传达，音频描述必须构成一个连贯的序列，帮助听者构建场景的动态发展，而非孤立地描述零散时刻。然而，大多数自动方法独立生成每个音频描述片段，往往导致重复且不连贯的描述。为解决这一问题，我们提出一种无需训练的方法 CoherentAD，该方法首先为每个音频描述时间区间生成多个候选描述，然后在序列中进行自回归选择，以形成连贯且信息丰富的叙事。为全面评估音频描述序列，我们引入了一种序列级评价指标 StoryRecall，该指标衡量预测的音频描述在多大程度上传达了真实叙事内容，同时结合重复性指标以捕捉连续音频描述输出中的冗余程度。我们的方法生成的音频描述序列具有更强的连贯性与叙事理解能力，在无需依赖独立生成的先验方法中表现更优。",
        "translated_title": "超越瞬间：迈向连贯的音频描述序列",
        "label": [],
        "label_reason": "论文聚焦音频描述生成与序列连贯性，属于高阶视频理解任务，非像素级图像处理。",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "提出序列级生成与评估方法，但属自然语言生成范畴，无图像恢复创新。"
    },
    {
        "title": "Instance-Level Composed Image Retrieval",
        "url": "http://arxiv.org/abs/2510.25387v1",
        "pub_date": "2025-10-29",
        "summary": "The progress of composed image retrieval (CIR), a popular research direction in image retrieval, where a combined visual and textual query is used, is held back by the absence of high-quality training and evaluation data. We introduce a new evaluation dataset, i-CIR, which, unlike existing datasets, focuses on an instance-level class definition. The goal is to retrieve images that contain the same particular object as the visual query, presented under a variety of modifications defined by textual queries. Its design and curation process keep the dataset compact to facilitate future research, while maintaining its challenge-comparable to retrieval among more than 40M random distractors-through a semi-automated selection of hard negatives.   To overcome the challenge of obtaining clean, diverse, and suitable training data, we leverage pre-trained vision-and-language models (VLMs) in a training-free approach called BASIC. The method separately estimates query-image-to-image and query-text-to-image similarities, performing late fusion to upweight images that satisfy both queries, while down-weighting those that exhibit high similarity with only one of the two. Each individual similarity is further improved by a set of components that are simple and intuitive. BASIC sets a new state of the art on i-CIR but also on existing CIR datasets that follow a semantic-level class definition. Project page: https://vrg.fel.cvut.cz/icir/.",
        "translated": "组合图像检索（CIR）是图像检索领域的一个热门研究方向，其通过联合使用视觉与文本查询进行图像检索。然而，该方向的进展受到高质量训练与评估数据缺失的制约。我们提出一个新评估数据集 i-CIR，与现有数据集不同，它聚焦于实例级类别定义。其目标是检索出与视觉查询包含相同特定目标的图像，这些图像在由文本查询所定义的多种修改下呈现。该数据集的设计与构建过程通过半自动选择难负样本，保持数据集紧凑，同时维持与在超过40M个随机干扰样本中进行检索相当的挑战性。\n\n为克服获取干净、多样且适合训练数据的挑战，我们采用一种无需训练的策略 BASIC，利用预训练视觉-语言模型（VLMs）。该方法分别估计查询-图像到图像、查询-文本到图像的相似性，通过后期融合策略，提升同时满足两个查询的图像权重，而降低仅与其中一个查询高度相似的图像权重。每个单独的相似性度量进一步通过一组简单直观的组件进行优化。BASIC 在 i-CIR 上达到新的最优性能，同时在遵循语义级类别定义的现有 CIR 数据集上也取得了领先表现。项目主页：https://vrg.fel.cvut.cz/icir/。",
        "translated_title": "实例级合成图像检索",
        "label": [],
        "label_reason": "论文聚焦图像检索，属于high-level视觉任务，不涉及像素级图像质量改善。",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "提出BASIC方法改进检索，但为常规融合策略，无突破性创新。"
    },
    {
        "title": "Prompt Estimation from Prototypes for Federated Prompt Tuning of Vision\n  Transformers",
        "url": "http://arxiv.org/abs/2510.25372v1",
        "pub_date": "2025-10-29",
        "summary": "Visual Prompt Tuning (VPT) of pre-trained Vision Transformers (ViTs) has proven highly effective as a parameter-efficient fine-tuning technique for adapting large models to downstream tasks with limited data. Its parameter efficiency makes it particularly suitable for Federated Learning (FL), where both communication and computation budgets are often constrained. However, global prompt tuning struggles to generalize across heterogeneous clients, while personalized tuning overfits to local data and lacks generalization. We propose PEP-FedPT (Prompt Estimation from Prototypes for Federated Prompt Tuning), a unified framework designed to achieve both generalization and personalization in federated prompt tuning of ViTs. Within this framework, we introduce the novel Class-Contextualized Mixed Prompt (CCMP) - based on class-specific prompts maintained alongside a globally shared prompt. For each input, CCMP adaptively combines class-specific prompts using weights derived from global class prototypes and client class priors. This approach enables per-sample prompt personalization without storing client-dependent trainable parameters. The prompts are collaboratively optimized via traditional federated averaging technique on the same. Comprehensive evaluations on CIFAR-100, TinyImageNet, DomainNet, and iNaturalist datasets demonstrate that PEP-FedPT consistently surpasses the state-of-the-art baselines under diverse data heterogeneity scenarios, establishing a strong foundation for efficient and generalizable federated prompt tuning of Vision Transformers.",
        "translated": "视觉提示调优（Visual Prompt Tuning, VPT）在预训练视觉变换器（Vision Transformers, ViTs）上已被证明是一种高效的参数调优技术，能够以极少的数据将大型模型适配到下游任务。其参数效率使其特别适用于联邦学习（Federated Learning, FL），因为在该场景下通信和计算资源通常受到严格限制。然而，全局提示调优在面对异构客户端时泛化能力较差，而个性化提示调优则容易过拟合本地数据，缺乏泛化能力。我们提出PEP-FedPT（基于原型的提示估计用于联邦提示调优），一种统一框架，旨在实现ViTs联邦提示调优中的泛化与个性化双重目标。在此框架中，我们引入了新型的类上下文混合提示（Class-Contextualized Mixed Prompt, CCMP），其基于一个全局共享提示与多个类特定提示共同维护。对于每个输入，CCMP通过从全局类原型和客户端类先验中导出的权重，自适应地组合类特定提示。该方法实现了逐样本的提示个性化，而无需存储依赖于客户端的可训练参数。提示通过传统的联邦平均技术在各客户端上协同优化。在CIFAR-100、TinyImageNet、DomainNet和iNaturalist数据集上的全面评估表明，PEP-FedPT在多种数据异构性场景下均显著优于现有最先进的基线方法，为高效且泛化的视觉变换器联邦提示调优奠定了坚实基础。",
        "translated_title": "基于原型的提示估计用于视觉Transformer的联邦提示调优",
        "label": [],
        "label_reason": "论文聚焦于联邦学习中的视觉提示调优，属于高阶视觉任务，不涉及像素级图像恢复或增强。",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出基于原型的提示估计机制，对联邦学习中提示调优有显著改进，但非低层图像处理创新。"
    },
    {
        "title": "3D CT-Based Coronary Calcium Assessment: A Feature-Driven Machine\n  Learning Framework",
        "url": "http://arxiv.org/abs/2510.25347v1",
        "pub_date": "2025-10-29",
        "summary": "Coronary artery calcium (CAC) scoring plays a crucial role in the early detection and risk stratification of coronary artery disease (CAD). In this study, we focus on non-contrast coronary computed tomography angiography (CCTA) scans, which are commonly used for early calcification detection in clinical settings. To address the challenge of limited annotated data, we propose a radiomics-based pipeline that leverages pseudo-labeling to generate training labels, thereby eliminating the need for expert-defined segmentations. Additionally, we explore the use of pretrained foundation models, specifically CT-FM and RadImageNet, to extract image features, which are then used with traditional classifiers. We compare the performance of these deep learning features with that of radiomics features. Evaluation is conducted on a clinical CCTA dataset comprising 182 patients, where individuals are classified into two groups: zero versus non-zero calcium scores. We further investigate the impact of training on non-contrast datasets versus combined contrast and non-contrast datasets, with testing performed only on non contrast scans. Results show that radiomics-based models significantly outperform CNN-derived embeddings from foundation models (achieving 84% accuracy and p&lt;0.05), despite the unavailability of expert annotations.",
        "translated": "冠状动脉钙化（CAC）评分在冠状动脉疾病（CAD）的早期检测和风险分层中起着关键作用。本研究聚焦于临床上常用于早期钙化检测的无对比剂冠状动脉计算机断层扫描血管成像（CCTA）扫描。为应对标注数据有限的挑战，我们提出了一种基于放射组学的流程，利用伪标签生成训练标签，从而无需依赖专家定义的分割结果。此外，我们探索了预训练基础模型（特别是CT-FM和RadImageNet）在提取图像特征方面的应用，随后将这些特征与传统分类器结合使用。我们比较了这些深度学习特征与放射组学特征的性能表现。评估在包含182名患者的临床CCTA数据集上进行，将个体分为两组：钙化评分为零与非零。我们进一步研究了在无对比剂数据集上训练与在对比剂和无对比剂数据集联合训练的影响，所有测试均仅在无对比剂扫描上进行。结果表明，尽管缺乏专家标注，基于放射组学的模型在准确率上显著优于基础模型提取的CNN特征嵌入（达到84%准确率，p<0.05）。",
        "translated_title": "基于3D CT的冠状动脉钙化评估：一种特征驱动的机器学习框架",
        "label": [],
        "label_reason": "论文聚焦于CT图像的钙化评分分类，属于医学图像分析中的高阶任务，不涉及像素级图像恢复或增强。",
        "relevance_score": 3,
        "novelty_score": 5,
        "novelty_reason": "提出伪标签策略和基础模型特征提取，但方法属于分类任务，未针对图像质量本身进行优化。"
    },
    {
        "title": "Informative Sample Selection Model for Skeleton-based Action Recognition\n  with Limited Training Samples",
        "url": "http://arxiv.org/abs/2510.25345v1",
        "pub_date": "2025-10-29",
        "summary": "Skeleton-based human action recognition aims to classify human skeletal sequences, which are spatiotemporal representations of actions, into predefined categories. To reduce the reliance on costly annotations of skeletal sequences while maintaining competitive recognition accuracy, the task of 3D Action Recognition with Limited Training Samples, also known as semi-supervised 3D Action Recognition, has been proposed. In addition, active learning, which aims to proactively select the most informative unlabeled samples for annotation, has been explored in semi-supervised 3D Action Recognition for training sample selection. Specifically, researchers adopt an encoder-decoder framework to embed skeleton sequences into a latent space, where clustering information, combined with a margin-based selection strategy using a multi-head mechanism, is utilized to identify the most informative sequences in the unlabeled set for annotation. However, the most representative skeleton sequences may not necessarily be the most informative for the action recognizer, as the model may have already acquired similar knowledge from previously seen skeleton samples. To solve it, we reformulate Semi-supervised 3D action recognition via active learning from a novel perspective by casting it as a Markov Decision Process (MDP). Built upon the MDP framework and its training paradigm, we train an informative sample selection model to intelligently guide the selection of skeleton sequences for annotation. To enhance the representational capacity of the factors in the state-action pairs within our method, we project them from Euclidean space to hyperbolic space. Furthermore, we introduce a meta tuning strategy to accelerate the deployment of our method in real-world scenarios. Extensive experiments on three 3D action recognition benchmarks demonstrate the effectiveness of our method.",
        "translated": "基于骨架的人体动作识别旨在将人体骨架序列——即动作的时空表示——分类到预定义的类别中。为降低对骨架序列昂贵标注的依赖，同时保持具有竞争力的识别精度，提出了样本受限的3D动作识别任务，也称为半监督3D动作识别。此外，主动学习作为一种主动选择最具信息量的未标注样本进行标注的策略，已在半监督3D动作识别中用于训练样本选择。具体而言，研究人员采用编码器-解码器框架将骨架序列嵌入到潜在空间中，并结合聚类信息以及基于多头机制的边界选择策略，用于识别未标注集合中最具信息量的序列以进行标注。然而，最具代表性的骨架序列未必对动作识别器最具信息量，因为模型可能已从先前观察到的骨架样本中获取了类似知识。为解决此问题，我们从新颖视角重新定义了基于主动学习的半监督3D动作识别，将其建模为马尔可夫决策过程（MDP）。基于MDP框架及其训练范式，我们训练一个信息样本选择模型，以智能地指导骨架序列的标注选择。为进一步增强方法中状态-动作对内各因素的表示能力，我们将它们从欧氏空间投影到双曲空间。此外，我们引入一种元调优策略，以加速方法在真实场景中的部署。在三个3D动作识别基准数据集上的大量实验验证了本方法的有效性。",
        "translated_title": "基于骨架的动作识别在有限训练样本下的信息样本选择模型",
        "label": [],
        "label_reason": "论文聚焦于基于骨架的动作识别，属于高阶视觉任务，不涉及像素级图像恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 6,
        "novelty_reason": "提出MDP框架与双曲空间投影，但应用于动作识别，非低层图像处理创新。"
    },
    {
        "title": "StreamingCoT: A Dataset for Temporal Dynamics and Multimodal\n  Chain-of-Thought Reasoning in Streaming VideoQA",
        "url": "http://arxiv.org/abs/2510.25332v1",
        "pub_date": "2025-10-29",
        "summary": "The rapid growth of streaming video applications demands multimodal models with enhanced capabilities for temporal dynamics understanding and complex reasoning. However, current Video Question Answering (VideoQA) datasets suffer from two critical limitations: 1) Static annotation mechanisms fail to capture the evolving nature of answers in temporal video streams, and 2) The absence of explicit reasoning process annotations restricts model interpretability and logical deduction capabilities. To address these challenges, We introduce StreamingCoT, the first dataset explicitly designed for temporally evolving reasoning in streaming VideoQA and multimodal Chain-of-Thought (CoT) tasks. Our framework first establishes a dynamic hierarchical annotation architecture that generates per-second dense descriptions and constructs temporally-dependent semantic segments through similarity fusion, paired with question-answer sets constrained by temporal evolution patterns. We further propose an explicit reasoning chain generation paradigm that extracts spatiotemporal objects via keyframe semantic alignment, derives object state transition-based reasoning paths using large language models, and ensures logical coherence through human-verified validation. This dataset establishes a foundation for advancing research in streaming video understanding, complex temporal reasoning, and multimodal inference. Our StreamingCoT and its construction toolkit can be accessed at https://github.com/Fleeting-hyh/StreamingCoT.",
        "translated": "流媒体视频应用的快速发展对具备时序动态理解与复杂推理能力的多模态模型提出了更高要求。然而，当前的视频问答（VideoQA）数据集存在两个关键局限：1）静态标注机制无法捕捉在时序视频流中不断演变的答案特性；2）缺乏显式的推理过程标注，限制了模型的可解释性及逻辑推导能力。为应对这些挑战，我们引入 StreamingCoT——首个专为流媒体视频问答及多模态思维链（Chain-of-Thought, CoT）任务设计的、明确支持时序演化推理的数据集。我们的框架首先构建了一个动态分层标注架构，生成每秒密集描述，并通过相似性融合构建时序依赖的语义片段，同时配对以符合时序演化模式的问答对。我们进一步提出一种显式推理链生成范式：通过关键帧语义对齐提取时空对象，利用大语言模型推导基于对象状态迁移的推理路径，并通过人工验证确保逻辑一致性。该数据集为流媒体视频理解、复杂时序推理及多模态推理研究奠定了基础。我们的 StreamingCoT 数据集及其构建工具包可通过 https://github.com/Fleeting-hyh/StreamingCoT 获取。",
        "translated_title": "StreamingCoT：用于流式视频问答中时间动态与多模态思维链推理的数据集",
        "label": [],
        "label_reason": "论文聚焦视频问答与多模态推理，属于high-level视觉任务，不涉及像素级图像恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "提出动态标注框架与推理链生成，属多模态推理创新，但非图像处理领域核心贡献。"
    },
    {
        "title": "MMEdge: Accelerating On-device Multimodal Inference via Pipelined\n  Sensing and Encoding",
        "url": "http://arxiv.org/abs/2510.25327v1",
        "pub_date": "2025-10-29",
        "summary": "Real-time multimodal inference on resource-constrained edge devices is essential for applications such as autonomous driving, human-computer interaction, and mobile health. However, prior work often overlooks the tight coupling between sensing dynamics and model execution, as well as the complex inter-modality dependencies. In this paper, we propose MMEdge, an new on-device multi-modal inference framework based on pipelined sensing and encoding. Instead of waiting for complete sensor inputs, MMEdge decomposes the entire inference process into a sequence of fine-grained sensing and encoding units, allowing computation to proceed incrementally as data arrive. MMEdge also introduces a lightweight but effective temporal aggregation module that captures rich temporal dynamics across different pipelined units to maintain accuracy performance. Such pipelined design also opens up opportunities for fine-grained cross-modal optimization and early decision-making during inference. To further enhance system performance under resource variability and input data complexity, MMEdge incorporates an adaptive multimodal configuration optimizer that dynamically selects optimal sensing and model configurations for each modality under latency constraints, and a cross-modal speculative skipping mechanism that bypasses future units of slower modalities when early predictions reach sufficient confidence. We evaluate MMEdge using two public multimodal datasets and deploy it on a real-world unmanned aerial vehicle (UAV)-based multimodal testbed. The results show that MMEdge significantly reduces end-to-end latency while maintaining high task accuracy across various system and data dynamics.",
        "translated": "实时多模态推理在资源受限的边缘设备上对于自动驾驶、人机交互和移动医疗等应用至关重要。然而，先前的研究往往忽略了感知动态与模型执行之间的紧密耦合，以及模态间复杂的相互依赖关系。本文提出MMEdge，一种基于流水线感知与编码的新型设备端多模态推理框架。与等待完整传感器输入不同，MMEdge将整个推理过程分解为一系列细粒度的感知与编码单元，使得计算能够随着数据到达而逐步进行。MMEdge还引入了一种轻量但有效的时序聚合模块，用于捕捉不同流水线单元间的丰富时序动态，以维持准确率性能。这种流水线设计还为推理过程中细粒度的跨模态优化和早期决策提供了可能。为进一步提升系统在资源波动和输入数据复杂性下的性能，MMEdge集成了一个自适应多模态配置优化器，该优化器在延迟约束下动态为各模态选择最优的感知与模型配置，并引入跨模态推测跳过机制，当早期预测达到足够置信度时，可跳过较慢模态的后续单元。我们在两个公开的多模态数据集上评估了MMEdge，并将其部署到基于真实无人机（UAV）的多模态测试平台上。实验结果表明，MMEdge在保持各种系统与数据动态下高任务准确率的同时，显著降低了端到端延迟。",
        "translated_title": "MMEdge：通过流水线式感知与编码加速设备端多模态推理",
        "label": [],
        "label_reason": "论文聚焦多模态推理加速与边缘设备部署，未涉及图像像素级恢复或增强任务。",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出流水线感知与编码框架，结合自适应优化和推测跳过机制，属于系统级创新。"
    },
    {
        "title": "Prototype-Driven Adaptation for Few-Shot Object Detection",
        "url": "http://arxiv.org/abs/2510.25318v1",
        "pub_date": "2025-10-29",
        "summary": "Few-shot object detection (FSOD) often suffers from base-class bias and unstable calibration when only a few novel samples are available. We propose Prototype-Driven Alignment (PDA), a lightweight, plug-in metric head for DeFRCN that provides a prototype-based \"second opinion\" complementary to the linear classifier. PDA maintains support-only prototypes in a learnable identity-initialized projection space and optionally applies prototype-conditioned RoI alignment to reduce geometric mismatch. During fine-tuning, prototypes can be adapted via exponential moving average(EMA) updates on labeled foreground RoIs-without introducing class-specific parameters-and are frozen at inference to ensure strict protocol compliance. PDA employs a best-of-K matching scheme to capture intra-class multi-modality and temperature-scaled fusion to combine metric similarities with detector logits. Experiments on VOC FSOD and GFSOD benchmarks show that PDA consistently improves novel-class performance with minimal impact on base classes and negligible computational overhead.",
        "translated": "少样本目标检测（FSOD）在仅有少量新类别样本时，常面临基类偏置和校准不稳定的挑战。我们提出了一种轻量级、即插即用的度量头——原型驱动对齐（Prototype-Driven Alignment, PDA），将其应用于DeFRCN框架中，为线性分类器提供基于原型的“第二意见”作为补充。PDA在可学习的、恒等初始化的投影空间中维护仅由支持样本生成的原型，并可选地采用原型条件下的RoI对齐操作，以减轻几何错位问题。在微调阶段，原型可通过在标注前景RoI上进行指数移动平均（EMA）更新进行自适应调整——无需引入类别特定参数——并在推理阶段冻结，以确保严格遵循协议要求。PDA采用最佳K匹配策略，捕捉类内多模态特性，并通过温度缩放融合机制，将度量相似性与检测器输出的logits相结合。在VOC FSOD和GFSOD基准数据集上的实验表明，PDA在显著提升新类别性能的同时，对基类性能影响极小，且计算开销可忽略不计。",
        "translated_title": "原型驱动的少样本目标检测适应方法",
        "label": [],
        "label_reason": "论文聚焦小样本目标检测，属于high-level任务，不涉及图像像素级恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 6,
        "novelty_reason": "提出原型驱动对齐机制，对检测头进行改进，属检测任务中的结构优化，非low-level创新。"
    },
    {
        "title": "Seeing Clearly and Deeply: An RGBD Imaging Approach with a Bio-inspired\n  Monocentric Design",
        "url": "http://arxiv.org/abs/2510.25314v1",
        "pub_date": "2025-10-29",
        "summary": "Achieving high-fidelity, compact RGBD imaging presents a dual challenge: conventional compact optics struggle with RGB sharpness across the entire depth-of-field, while software-only Monocular Depth Estimation (MDE) is an ill-posed problem reliant on unreliable semantic priors. While deep optics with elements like DOEs can encode depth, they introduce trade-offs in fabrication complexity and chromatic aberrations, compromising simplicity. To address this, we first introduce a novel bio-inspired all-spherical monocentric lens, around which we build the Bionic Monocentric Imaging (BMI) framework, a holistic co-design. This optical design naturally encodes depth into its depth-varying Point Spread Functions (PSFs) without requiring complex diffractive or freeform elements. We establish a rigorous physically-based forward model to generate a synthetic dataset by precisely simulating the optical degradation process. This simulation pipeline is co-designed with a dual-head, multi-scale reconstruction network that employs a shared encoder to jointly recover a high-fidelity All-in-Focus (AiF) image and a precise depth map from a single coded capture. Extensive experiments validate the state-of-the-art performance of the proposed framework. In depth estimation, the method attains an Abs Rel of 0.026 and an RMSE of 0.130, markedly outperforming leading software-only approaches and other deep optics systems. For image restoration, the system achieves an SSIM of 0.960 and a perceptual LPIPS score of 0.082, thereby confirming a superior balance between image fidelity and depth accuracy. This study illustrates that the integration of bio-inspired, fully spherical optics with a joint reconstruction algorithm constitutes an effective strategy for addressing the intrinsic challenges in high-performance compact RGBD imaging. Source code will be publicly available at https://github.com/ZongxiYu-ZJU/BMI.",
        "translated": "实现高保真、紧凑型RGBD成像面临双重挑战：传统紧凑光学系统难以在整个景深范围内同时保证RGB图像的清晰度，而仅依赖软件的单目深度估计（Monocular Depth Estimation, MDE）则是一个病态问题，高度依赖不可靠的语义先验。虽然采用衍射光学元件（DOEs）等结构的深度光学系统能够编码深度信息，但其在制造复杂性和色差方面引入了折衷，损害了系统的简洁性。为应对这一问题，我们首先提出一种新型仿生全球面单心镜头，并在此基础上构建了仿生单心成像（Bionic Monocentric Imaging, BMI）框架，实现光学与算法的协同设计。该光学设计能够自然地通过其随深度变化的点扩散函数（Point Spread Functions, PSFs）编码深度信息，无需复杂的衍射或自由曲面元件。我们建立了一个严谨的基于物理的前向模型，通过精确模拟光学退化过程生成合成数据集。该模拟流程与一个双头、多尺度重构网络协同设计，该网络采用共享编码器，从单次编码捕获中联合恢复高保真的全焦点（All-in-Focus, AiF）图像与精确的深度图。大量实验验证了所提框架的先进性能：在深度估计方面，该方法达到Abs Rel为0.026、RMSE为0.130，显著优于主流仅软件方法及其他深度光学系统；在图像恢复方面，系统获得SSIM为0.960、感知LPIPS得分为0.082，证实了图像保真度与深度精度之间的优越平衡。本研究表明，将仿生全球面光学与联合重构算法相结合，是解决高性能紧凑型RGBD成像内在挑战的有效策略。源代码将公开于https://github.com/ZongxiYu-ZJU/BMI。",
        "translated_title": "看清与深究：一种基于仿生单心结构的RGBD成像方法",
        "label": [
            "图像恢复",
            "图像去模糊",
            "多帧/视频图像恢复"
        ],
        "label_reason": "基于物理模型的光学退化建模与联合重建网络，恢复高保真全聚焦图像，属图像恢复与去模糊范畴",
        "relevance_score": 9,
        "novelty_score": 8,
        "novelty_reason": "提出生物启发式单中心光学设计与联合重建网络，结合物理模型与深度学习，方法新颖且有效"
    },
    {
        "title": "GaTector+: A Unified Head-free Framework for Gaze Object and Gaze\n  Following Prediction",
        "url": "http://arxiv.org/abs/2510.25301v1",
        "pub_date": "2025-10-29",
        "summary": "Gaze object detection and gaze following are fundamental tasks for interpreting human gaze behavior or intent. However, most previous methods usually solve these two tasks separately, and their prediction of gaze objects and gaze following typically depend on head-related prior knowledge during both the training phase and real-world deployment. This dependency necessitates an auxiliary network to extract head location, thus precluding joint optimization across the entire system and constraining the practical applicability. To this end, we propose GaTector+, a unified framework for gaze object detection and gaze following, which eliminates the dependence on the head-related priors during inference. Specifically, GaTector+ uses an expanded specific-general-specific feature extractor that leverages a shared backbone, which extracts general features for gaze following and object detection using the shared backbone while using specific blocks before and after the shared backbone to better consider the specificity of each sub-task. To obtain head-related knowledge without prior information, we first embed a head detection branch to predict the head of each person. Then, before regressing the gaze point, a head-based attention mechanism is proposed to fuse the sense feature and gaze feature with the help of head location. Since the suboptimization of the gaze point heatmap leads to the performance bottleneck, we propose an attention supervision mechanism to accelerate the learning of the gaze heatmap. Finally, we propose a novel evaluation metric, mean Similarity over Candidates (mSoC), for gaze object detection, which is more sensitive to variations between bounding boxes. The experimental results on multiple benchmark datasets demonstrate the effectiveness of our model in both gaze object detection and gaze following tasks.",
        "translated": "注视目标检测与注视跟踪是理解人类注视行为或意图的基础任务。然而，大多数先前的方法通常将这两个任务分别处理，且其在训练阶段及实际部署中对注视目标和注视跟踪的预测往往依赖于与头部相关的先验知识。这种依赖性需要引入辅助网络以提取头部位置，从而阻碍了整个系统端到端的联合优化，并限制了模型的实际适用性。为此，我们提出 GaTector+，一种用于注视目标检测与注视跟踪的统一框架，该框架在推理过程中消除了对头部相关先验的依赖。具体而言，GaTector+ 采用一种扩展的特定-通用-特定特征提取器，该提取器基于共享主干网络，利用共享主干提取注视跟踪与目标检测所需的通用特征，同时在共享主干前后引入特定模块，以更好地考虑各子任务的特异性。为在无先验信息的情况下获取头部相关知识，我们首先嵌入一个头部检测分支，用于预测每个人的头部位置。随后，在回归注视点之前，提出一种基于头部的注意力机制，借助头部位置信息融合感知特征与注视特征。由于注视点热图的子优化问题导致性能瓶颈，我们进一步提出一种注意力监督机制，以加速注视热图的学习过程。最后，我们提出一种新颖的评估指标——候选框平均相似度（mean Similarity over Candidates, mSoC），用于注视目标检测任务，该指标对边界框之间的变化更为敏感。在多个基准数据集上的实验结果表明，我们的模型在注视目标检测与注视跟踪任务中均表现出优越的有效性。",
        "translated_title": "GaTector+: 一种用于注视目标与注视跟踪预测的统一无头框架",
        "label": [],
        "label_reason": "论文聚焦于眼动目标检测与眼动追踪，属于高阶视觉理解任务，不涉及像素级图像恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "提出统一框架与注意力监督机制，但未突破现有范式，创新点集中在任务联合建模而非图像处理。"
    },
    {
        "title": "Diffusion-Driven Progressive Target Manipulation for Source-Free Domain\n  Adaptation",
        "url": "http://arxiv.org/abs/2510.25279v1",
        "pub_date": "2025-10-29",
        "summary": "Source-free domain adaptation (SFDA) is a challenging task that tackles domain shifts using only a pre-trained source model and unlabeled target data. Existing SFDA methods are restricted by the fundamental limitation of source-target domain discrepancy. Non-generation SFDA methods suffer from unreliable pseudo-labels in challenging scenarios with large domain discrepancies, while generation-based SFDA methods are evidently degraded due to enlarged domain discrepancies in creating pseudo-source data. To address this limitation, we propose a novel generation-based framework named Diffusion-Driven Progressive Target Manipulation (DPTM) that leverages unlabeled target data as references to reliably generate and progressively refine a pseudo-target domain for SFDA. Specifically, we divide the target samples into a trust set and a non-trust set based on the reliability of pseudo-labels to sufficiently and reliably exploit their information. For samples from the non-trust set, we develop a manipulation strategy to semantically transform them into the newly assigned categories, while simultaneously maintaining them in the target distribution via a latent diffusion model. Furthermore, we design a progressive refinement mechanism that progressively reduces the domain discrepancy between the pseudo-target domain and the real target domain via iterative refinement. Experimental results demonstrate that DPTM outperforms existing methods by a large margin and achieves state-of-the-art performance on four prevailing SFDA benchmark datasets with different scales. Remarkably, DPTM can significantly enhance the performance by up to 18.6% in scenarios with large source-target gaps.",
        "translated": "无源域自适应（Source-Free Domain Adaptation, SFDA）是一项具有挑战性的任务，旨在仅利用预训练的源模型和未标记的目标数据来应对域偏移问题。现有SFDA方法受限于源域与目标域之间固有的域差异问题。非生成式SFDA方法在域差异较大的困难场景中，由于伪标签不可靠而表现不佳；而基于生成的SFDA方法在生成伪源数据时，因域差异扩大而明显退化。为解决这一局限，我们提出一种新颖的基于生成的框架，名为扩散驱动的渐进式目标域操控（Diffusion-Driven Progressive Target Manipulation, DPTM），该框架利用未标记的目标数据作为参考，可靠地生成并逐步优化伪目标域以用于SFDA。具体而言，我们根据伪标签的可靠性将目标样本划分为可信集和不可信集，从而充分且可靠地利用其信息。对于来自不可信集的样本，我们设计了一种操控策略，通过潜在扩散模型在语义上将其转换至新分配的类别，同时保持其在目标分布中。此外，我们设计了一种渐进式优化机制，通过迭代优化逐步缩小伪目标域与真实目标域之间的域差异。实验结果表明，DPTM在四个主流的SFDA基准数据集（具有不同规模）上显著优于现有方法，达到当前最佳性能。值得注意的是，在源域与目标域差异较大的场景中，DPTM能够将性能提升高达18.6%。",
        "translated_title": "扩散驱动的渐进式目标操控用于无源域自适应",
        "label": [],
        "label_reason": "论文聚焦于无源域自适应，属于高阶视觉任务，不涉及像素级图像恢复或增强。",
        "relevance_score": 2,
        "novelty_score": 8,
        "novelty_reason": "提出扩散驱动的渐进式目标操纵框架，结合扩散模型与伪标签策略，具有显著创新性。"
    },
    {
        "title": "Retrieval-Augmented Search for Large-Scale Map Collections with ColPali",
        "url": "http://arxiv.org/abs/2510.25718v1",
        "pub_date": "2025-10-29",
        "summary": "Multimodal approaches have shown great promise for searching and navigating digital collections held by libraries, archives, and museums. In this paper, we introduce map-RAS: a retrieval-augmented search system for historic maps. In addition to introducing our framework, we detail our publicly-hosted demo for searching 101,233 map images held by the Library of Congress. With our system, users can multimodally query the map collection via ColPali, summarize search results using Llama 3.2, and upload their own collections to perform inter-collection search. We articulate potential use cases for archivists, curators, and end-users, as well as future work with our system in both machine learning and the digital humanities. Our demo can be viewed at: http://www.mapras.com.",
        "translated": "多模态方法在图书馆、档案馆和博物馆所持有的数字藏品的检索与导航方面展现出巨大潜力。本文介绍了 map-RAS：一种面向历史地图的检索增强型搜索系统。除系统框架外，我们还详细说明了我们公开托管的演示系统，该系统支持对国会图书馆所藏的 101,233 张地图图像进行检索。借助本系统，用户可通过 ColPali 实现多模态查询地图藏品，利用 Llama 3.2 对检索结果进行摘要，并上传自有藏品以实现跨藏品集合的检索。我们阐述了该系统在档案管理员、策展人及终端用户中的潜在应用场景，以及在机器学习与数字人文领域中的未来研究方向。演示系统可访问：http://www.mapras.com。",
        "translated_title": "基于ColPali的大规模地图集合检索增强搜索",
        "label": [
            "多模态推荐",
            "召回"
        ],
        "label_reason": "基于多模态检索的系统，用于地图集合搜索，涉及召回环节，但非典型推荐场景",
        "relevance_score": 4,
        "novelty_score": 6,
        "novelty_reason": "结合ColPali与LLM实现多模态检索与摘要，有一定创新但属现有技术组合"
    },
    {
        "title": "MMQ-v2: Align, Denoise, and Amplify: Adaptive Behavior Mining for\n  Semantic IDs Learning in Recommendation",
        "url": "http://arxiv.org/abs/2510.25622v1",
        "pub_date": "2025-10-29",
        "summary": "Industrial recommender systems rely on unique Item Identifiers (ItemIDs). However, this method struggles with scalability and generalization in large, dynamic datasets that have sparse long-tail data.Content-based Semantic IDs (SIDs) address this by sharing knowledge through content quantization. However, by ignoring dynamic behavioral properties, purely content-based SIDs have limited expressive power. Existing methods attempt to incorporate behavioral information but overlook a critical distinction: unlike relatively uniform content features, user-item interactions are highly skewed and diverse, creating a vast information gap in quality and quantity between popular and long-tail items. This oversight leads to two critical limitations: (1) Noise Corruption: Indiscriminate behavior-content alignment allows collaborative noise from long-tail items to corrupt their content representations, leading to the loss of critical multimodal information. (2)Signal Obscurity: The equal-weighting scheme for SIDs fails to reflect the varying importance of different behavioral signals, making it difficult for downstream tasks to distinguish important SIDs from uninformative ones. To tackle these issues, we propose a mixture-of-quantization framework, MMQ-v2, to adaptively Align, Denoise, and Amplify multimodal information from content and behavior modalities for semantic IDs learning. The semantic IDs generated by this framework named ADA-SID. It introduces two innovations: an adaptive behavior-content alignment that is aware of information richness to shield representations from noise, and a dynamic behavioral router to amplify critical signals by applying different weights to SIDs. Extensive experiments on public and large-scale industrial datasets demonstrate ADA-SID's significant superiority in both generative and discriminative recommendation tasks.",
        "translated": "工业推荐系统依赖于唯一的物料标识符（ItemIDs）。然而，该方法在处理具有稀疏长尾数据的大规模动态数据集时，面临可扩展性和泛化能力的挑战。基于内容的语义标识符（SIDs）通过内容量化共享知识，缓解了上述问题。然而，纯粹基于内容的SIDs忽略了动态行为特性，导致其表达能力受限。现有方法尝试融合行为信息，但忽视了一个关键区别：与相对均匀的内容特征不同，用户-物料交互具有高度偏斜性和多样性，在流行物料与长尾物料之间造成了质量与数量上的巨大信息鸿沟。这一忽视导致两个关键局限：（1）噪声污染：无差别地对齐行为与内容，使得长尾物料的协同噪声污染其内容表示，从而导致关键多模态信息的丢失；（2）信号模糊：SIDs的等权重方案无法反映不同行为信号的重要性差异，导致下游任务难以区分重要SIDs与无信息量的SIDs。为解决这些问题，我们提出一种混合量化框架MMQ-v2，用于自适应地对齐、去噪和增强内容与行为模态中的多模态信息，以学习语义标识符。该框架生成的语义标识符称为ADA-SID。它引入两项创新：一种感知信息丰富度的自适应行为-内容对齐机制，用于保护表示免受噪声干扰；以及一种动态行为路由机制，通过为不同SIDs赋予不同权重，增强关键信号。在公开数据集和大规模工业数据集上的大量实验表明，ADA-SID在生成式和判别式推荐任务中均展现出显著优势。",
        "translated_title": "MMQ-v2：对齐、去噪与增强：面向推荐系统语义ID学习的自适应行为挖掘",
        "label": [
            "召回",
            "精排",
            "序列推荐",
            "多模态推荐",
            "负采样与对比学习"
        ],
        "label_reason": "论文聚焦推荐系统中语义ID学习，融合内容与行为模态，解决噪声与信号稀疏问题，直接服务于召回与排序环节。",
        "relevance_score": 9,
        "novelty_score": 8,
        "novelty_reason": "提出自适应对齐与动态路由机制，创新性地处理行为噪声与信号权重，提升多模态表征质量。"
    },
    {
        "title": "FARSIQA: Faithful and Advanced RAG System for Islamic Question Answering",
        "url": "http://arxiv.org/abs/2510.25621v1",
        "pub_date": "2025-10-29",
        "summary": "The advent of Large Language Models (LLMs) has revolutionized Natural Language Processing, yet their application in high-stakes, specialized domains like religious question answering is hindered by challenges like hallucination and unfaithfulness to authoritative sources. This issue is particularly critical for the Persian-speaking Muslim community, where accuracy and trustworthiness are paramount. Existing Retrieval-Augmented Generation (RAG) systems, relying on simplistic single-pass pipelines, fall short on complex, multi-hop queries requiring multi-step reasoning and evidence aggregation. To address this gap, we introduce FARSIQA, a novel, end-to-end system for Faithful Advanced Question Answering in the Persian Islamic domain. FARSIQA is built upon our innovative FAIR-RAG architecture: a Faithful, Adaptive, Iterative Refinement framework for RAG. FAIR-RAG employs a dynamic, self-correcting process: it adaptively decomposes complex queries, assesses evidence sufficiency, and enters an iterative loop to generate sub-queries, progressively filling information gaps. Operating on a curated knowledge base of over one million authoritative Islamic documents, FARSIQA demonstrates superior performance. Rigorous evaluation on the challenging IslamicPCQA benchmark shows state-of-the-art performance: the system achieves a remarkable 97.0% in Negative Rejection - a 40-point improvement over baselines - and a high Answer Correctness score of 74.3%. Our work establishes a new standard for Persian Islamic QA and validates that our iterative, adaptive architecture is crucial for building faithful, reliable AI systems in sensitive domains.",
        "translated": "大语言模型（LLM）的出现彻底改变了自然语言处理领域，但其在高风险、专业性强的领域（如宗教问答）中的应用仍面临幻觉和对权威来源不忠实等挑战。这一问题对波斯语穆斯林社区尤为关键，因为准确性与可信度至关重要。现有的检索增强生成（RAG）系统依赖于简化的单次处理流程，在需要多步推理和证据聚合的复杂多跳查询任务中表现不足。为解决这一空白，我们提出FARSIQA，一种面向波斯语伊斯兰领域的新型端到端可信高级问答系统。FARSIQA基于我们创新的FAIR-RAG架构：一种可信、自适应、迭代优化的RAG框架。FAIR-RAG采用动态自校正流程：它自适应地分解复杂查询，评估证据充分性，并进入迭代循环以生成子查询，逐步填补信息空白。该系统基于一个包含超过一百万份权威伊斯兰文献的精心构建知识库运行，展现出卓越性能。在具有挑战性的IslamicPCQA基准上的严格评估显示，其性能达到当前最优水平：系统在负样本拒绝指标上达到97.0%，较基线模型提升40个百分点，同时答案正确性得分高达74.3%。本工作为波斯语伊斯兰问答领域设立了新的标准，并验证了我们提出的迭代自适应架构在构建敏感领域中可信、可靠AI系统中的关键作用。",
        "translated_title": "FARSIQA：面向伊斯兰问答的忠实且先进的RAG系统",
        "label": [],
        "label_reason": "论文聚焦宗教问答系统，虽用RAG架构，但非推荐系统核心问题，与推荐无直接关联。",
        "relevance_score": 3,
        "novelty_score": 7,
        "novelty_reason": "提出迭代自校正RAG架构，提升事实准确性，在问答领域有显著创新。"
    },
    {
        "title": "Generalized Pseudo-Relevance Feedback",
        "url": "http://arxiv.org/abs/2510.25488v1",
        "pub_date": "2025-10-29",
        "summary": "Query rewriting is a fundamental technique in information retrieval (IR). It typically employs the retrieval result as relevance feedback to refine the query and thereby addresses the vocabulary mismatch between user queries and relevant documents. Traditional pseudo-relevance feedback (PRF) and its vector-based extension (VPRF) improve retrieval performance by leveraging top-retrieved documents as relevance feedback. However, they are constructed based on two major hypotheses: the relevance assumption (top documents are relevant) and the model assumption (rewriting methods need to be designed specifically for particular model architectures). While recent large language models (LLMs)-based generative relevance feedback (GRF) enables model-free query reformulation, it either suffers from severe LLM hallucination or, again, relies on the relevance assumption to guarantee the effectiveness of rewriting quality. To overcome these limitations, we introduce an assumption-relaxed framework: \\textit{Generalized Pseudo Relevance Feedback} (GPRF), which performs model-free, natural language rewriting based on retrieved documents, not only eliminating the model assumption but also reducing dependence on the relevance assumption. Specifically, we design a utility-oriented training pipeline with reinforcement learning to ensure robustness against noisy feedback. Extensive experiments across multiple benchmarks and retrievers demonstrate that GPRF consistently outperforms strong baselines, establishing it as an effective and generalizable framework for query rewriting.",
        "translated": "查询改写是信息检索（IR）中的核心技术。它通常利用检索结果作为相关性反馈，以优化原始查询，从而缓解用户查询与相关文档之间的词汇不匹配问题。传统的伪相关反馈（PRF）及其基于向量的扩展（VPRF）通过利用检索结果中排名靠前的文档作为相关性反馈，提升了检索性能。然而，这些方法基于两个主要假设：相关性假设（排名靠前的文档是相关的）和模型假设（改写方法需针对特定模型架构专门设计）。尽管近期基于大语言模型（LLM）的生成式相关反馈（GRF）实现了无模型依赖的查询重写，但它要么面临严重的LLM幻觉问题，要么仍依赖相关性假设以确保改写质量的有效性。为克服这些局限，我们提出一种假设宽松的框架：\\textit{广义伪相关反馈}（GPRF），该框架基于检索到的文档，实现无模型依赖的自然语言改写，不仅消除了模型假设，还降低了对相关性假设的依赖。具体而言，我们设计了一个以效用为导向的强化学习训练流程，以增强模型对噪声反馈的鲁棒性。在多个基准数据集和检索器上的广泛实验表明，GPRF始终优于强基线方法，确立其作为查询改写领域有效且可泛化的框架地位。",
        "translated_title": "广义伪相关反馈",
        "label": [],
        "label_reason": "论文聚焦信息检索中的查询重写，虽与推荐有交叉，但核心为通用IR技术，非推荐系统特定环节。",
        "relevance_score": 3,
        "novelty_score": 8,
        "novelty_reason": "提出去假设的通用伪相关反馈框架，结合强化学习训练，方法新颖，提升查询重写鲁棒性。"
    },
    {
        "title": "Alibaba International E-commerce Product Search Competition DcuRAGONs\n  Team Technical Report",
        "url": "http://arxiv.org/abs/2510.25428v1",
        "pub_date": "2025-10-29",
        "summary": "This report details our methodology and results developed for the Multilingual E-commerce Search Competition. The problem aims to recognize relevance between user queries versus product items in a multilingual context and improve recommendation performance on e-commerce platforms. Utilizing Large Language Models (LLMs) and their capabilities in other tasks, our data-centric method achieved the highest score compared to other solutions during the competition. Final leaderboard is publised at https://alibaba-international-cikm2025.github.io. The source code for our project is published at https://github.com/nhtlongcs/e-commerce-product-search.",
        "translated": "本报告详细阐述了我们在多语言电子商务搜索竞赛中所采用的方法及取得的结果。该问题旨在识别在多语言环境下用户查询与产品物料之间的相关性，并提升电子商务平台的推荐性能。通过利用大语言模型（LLM）及其在其他任务中的能力，我们提出的以数据为中心的方法在竞赛中相较于其他方案取得了最高得分。最终排行榜发布于 https://alibaba-international-cikm2025.github.io。本项目的源代码已公开于 https://github.com/nhtlongcs/e-commerce-product-search。",
        "translated_title": "阿里巴巴国际电商产品搜索竞赛 DcuRAGONs  \n团队技术报告",
        "label": [
            "召回",
            "LLM生成式推荐",
            "通用推荐技术"
        ],
        "label_reason": "基于LLM的多语言商品检索，属于推荐系统召回环节，方法通用非专为推荐设计",
        "relevance_score": 7,
        "novelty_score": 6,
        "novelty_reason": "数据驱动方法结合LLM，属常规改进，未提出新架构或范式"
    },
    {
        "title": "Towards Automated Quality Assurance of Patent Specifications: A\n  Multi-Dimensional LLM Framework",
        "url": "http://arxiv.org/abs/2510.25402v1",
        "pub_date": "2025-10-29",
        "summary": "Despite the surge in patent applications and emergence of AI drafting tools, systematic evaluation of patent content quality has received limited research attention. To address this gap, We propose to evaluate patents using regulatory compliance, technical coherence, and figure-reference consistency detection modules, and then generate improvement suggestions via an integration module. The framework is validated on a comprehensive dataset comprising 80 human-authored and 80 AI-generated patents from two patent drafting tools. Experimental results show balanced accuracies of 99.74\\%, 82.12\\%, and 91.2\\% respectively across the three detection modules when validated against expert annotations. Additional analysis was conducted to examine defect distributions across patent sections, technical domains, and authoring sources. Section-based analysis indicates that figure-text consistency and technical detail precision require particular attention. Mechanical Engineering and Construction show more claim-specification inconsistencies due to complex technical documentation requirements. AI-generated patents show a significant gap compared to human-authored ones. While human-authored patents primarily contain surface-level errors like typos, AI-generated patents exhibit more structural defects in figure-text alignment and cross-references.",
        "translated": "尽管专利申请数量激增，且AI撰写工具不断涌现，但对专利内容质量的系统性评估仍鲜有研究关注。为弥补这一空白，我们提出采用监管合规性、技术连贯性以及图示-引用一致性检测模块对专利进行评估，并通过集成模块生成改进建议。该框架在包含80份人工撰写和80份由两种专利撰写工具生成的专利的综合数据集上进行了验证。实验结果表明，在与专家标注对比验证时，三个检测模块的平衡准确率分别达到99.74%、82.12%和91.2%。此外，我们进一步分析了缺陷在专利各部分、技术领域及撰写来源中的分布情况。基于章节的分析表明，图示与文本一致性以及技术细节的精确性需要特别关注。机械工程与建筑领域因技术文档要求复杂，表现出更多的权利要求与说明书不一致问题。AI生成的专利相较于人工撰写的专利存在显著差距：人工撰写的专利主要包含拼写错误等表面级错误，而AI生成的专利则在图示与文本对齐及交叉引用方面表现出更多结构性缺陷。",
        "translated_title": "迈向专利说明书自动化质量保障：一个多维度大语言模型框架",
        "label": [],
        "label_reason": "论文聚焦专利质量评估，涉及LLM框架，但与推荐系统无直接关联。",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出多维度LLM评估框架，方法有一定创新性，但应用于专利领域非推荐系统。"
    },
    {
        "title": "Revisiting scalable sequential recommendation with Multi-Embedding\n  Approach and Mixture-of-Experts",
        "url": "http://arxiv.org/abs/2510.25285v1",
        "pub_date": "2025-10-29",
        "summary": "In recommendation systems, how to effectively scale up recommendation models has been an essential research topic. While significant progress has been made in developing advanced and scalable architectures for sequential recommendation(SR) models, there are still challenges due to items' multi-faceted characteristics and dynamic item relevance in the user context. To address these issues, we propose Fuxi-MME, a framework that integrates a multi-embedding strategy with a Mixture-of-Experts (MoE) architecture. Specifically, to efficiently capture diverse item characteristics in a decoupled manner, we decompose the conventional single embedding matrix into several lower-dimensional embedding matrices. Additionally, by substituting relevant parameters in the Fuxi Block with an MoE layer, our model achieves adaptive and specialized transformation of the enriched representations. Empirical results on public datasets show that our proposed framework outperforms several competitive baselines.",
        "translated": "在推荐系统中，如何有效扩展推荐模型一直是一个重要的研究课题。尽管在序列推荐（SR）模型的先进且可扩展架构开发方面已取得显著进展，但由于物料的多维度特性以及用户上下文中物料相关性的动态变化，仍存在诸多挑战。为解决这些问题，我们提出Fuxi-MME框架，该框架将多嵌入策略与专家混合（MoE）架构相结合。具体而言，为了高效地以解耦方式捕捉物料的多样化特征，我们将传统的单一嵌入矩阵分解为多个低维嵌入矩阵。此外，通过将Fuxi Block中相关参数替换为MoE层，我们的模型实现了对丰富表征的自适应和专业化变换。在公开数据集上的实证结果表明，我们提出的框架优于多个具有竞争力的基线方法。",
        "translated_title": "重访基于多嵌入方法与专家混合机制的可扩展序列推荐",
        "label": [
            "序列推荐",
            "通用推荐技术"
        ],
        "label_reason": "论文聚焦序列推荐中的可扩展性，提出多嵌入与MoE结合框架，直接用于推荐精排环节。",
        "relevance_score": 8,
        "novelty_score": 7,
        "novelty_reason": "创新性结合多嵌入与MoE，提升模型表达能力与效率，非完全新范式但有显著改进。"
    },
    {
        "title": "Measuring the Research Output and Performance of the University of\n  Ibadan from 2014 to 2023: A Scientometric Analysis",
        "url": "http://arxiv.org/abs/2510.25283v1",
        "pub_date": "2025-10-29",
        "summary": "This study employs scientometric methods to assess the research output and performance of the University of Ibadan from 2014 to 2023. By analyzing publication trends, citation patterns, and collaboration networks, the research aims to comprehensively evaluate the university's research productivity, impact, and disciplinary focus. This article's endeavors are characterized by innovation, interdisciplinary collaboration, and commitment to excellence, making the University of Ibadan a significant hub for cutting-edge research in Nigeria and beyond. The goal of the current study is to ascertain the influence of the university's research output and publication patterns between 2014 and 2023. The study focuses on the departments at the University of Ibadan that contribute the most, the best journals for publishing, the nations that collaborate, the impact of citations both locally and globally, well-known authors and their total production, and the research output broken down by year. According to the university's ten-year publication data, 7159 papers with an h-index of 75 were published between 2014 and 2023, garnering 218572 citations. Furthermore, the VOSviewer software mapping approach is used to illustrate the stenographical mapping of data through graphs. The findings of this study will contribute to understanding the university's research strengths, weaknesses, and potential areas for improvement. Additionally, the results will inform evidence-based decision-making for enhancing research strategies and policies at the University of Ibadan.",
        "translated": "本研究采用科学计量学方法，评估伊巴丹大学2014年至2023年的研究成果与研究表现。通过分析出版趋势、引用模式及合作网络，旨在全面评估该大学的研究生产力、影响力及其学科重点。本文的研究工作以创新性、跨学科合作以及对卓越的承诺为特征，使伊巴丹大学成为尼日利亚乃至更广泛地区前沿研究的重要中心。本研究的目标是确定该大学2014至2023年间研究成果及出版模式的影响。研究重点包括对伊巴丹大学贡献最大的院系、最佳发表期刊、合作国家、本地与全球范围内的引用影响力、知名作者及其总产出，以及按年份划分的研究成果。根据该大学十年的出版数据，2014至2023年间共发表7159篇论文，h指数为75，累计获得218572次引用。此外，本研究采用VOSviewer软件的映射方法，通过图形展示数据的图谱化映射。本研究的发现将有助于理解该大学的研究优势、劣势及潜在改进领域。同时，研究结果将为伊巴丹大学提升研究战略与政策提供基于证据的决策支持。",
        "translated_title": "伊巴丹大学2014至2023年研究产出与绩效测量：一项科学计量学分析",
        "label": [],
        "label_reason": "论文为科研产出的科学计量分析，与推荐系统无直接关联。",
        "relevance_score": 1,
        "novelty_score": 1,
        "novelty_reason": "方法为传统科学计量分析，无推荐系统相关创新。"
    },
    {
        "title": "TV-Rec: Time-Variant Convolutional Filter for Sequential Recommendation",
        "url": "http://arxiv.org/abs/2510.25259v1",
        "pub_date": "2025-10-29",
        "summary": "Recently, convolutional filters have been increasingly adopted in sequential recommendation for their ability to capture local sequential patterns. However, most of these models complement convolutional filters with self-attention. This is because convolutional filters alone, generally fixed filters, struggle to capture global interactions necessary for accurate recommendation. We propose Time-Variant Convolutional Filters for Sequential Recommendation (TV-Rec), a model inspired by graph signal processing, where time-variant graph filters capture position-dependent temporal variations in user sequences. By replacing both fixed kernels and self-attention with time-variant filters, TV-Rec achieves higher expressive power and better captures complex interaction patterns in user behavior. This design not only eliminates the need for self-attention but also reduces computation while accelerating inference. Extensive experiments on six public benchmarks show that TV-Rec outperforms state-of-the-art baselines by an average of 7.49%.",
        "translated": "近年来，卷积滤波器因其能够捕捉局部序列模式而在序列推荐中被越来越多地采用。然而，大多数此类模型通常将卷积滤波器与自注意力机制相结合。这是由于单独使用卷积滤波器（通常为固定滤波器）难以捕捉推荐所需的关键全局交互。我们提出了一种基于图信号处理思想的时变卷积滤波器序列推荐模型（TV-Rec），其中时变图滤波器能够捕捉用户序列中与位置相关的时序变化。通过同时替代固定卷积核和自注意力机制，TV-Rec实现了更强的表达能力，并更好地捕捉用户行为中的复杂交互模式。该设计不仅消除了对自注意力机制的依赖，同时降低了计算开销并加速了推理过程。在六个公开基准数据集上的大量实验表明，TV-Rec平均性能优于当前最先进的基线模型7.49%。",
        "translated_title": "TV-Rec：面向序列推荐的时间可变卷积滤波器",
        "label": [
            "序列推荐",
            "通用推荐技术"
        ],
        "label_reason": "论文提出时间变异卷积滤波器用于序列推荐，直接优化推荐系统核心环节",
        "relevance_score": 8,
        "novelty_score": 8,
        "novelty_reason": "创新性地引入图信号处理思想，替代固定卷积与自注意力，提升表达能力"
    },
    {
        "title": "GReF: A Unified Generative Framework for Efficient Reranking via Ordered\n  Multi-token Prediction",
        "url": "http://arxiv.org/abs/2510.25220v1",
        "pub_date": "2025-10-29",
        "summary": "In a multi-stage recommendation system, reranking plays a crucial role in modeling intra-list correlations among items. A key challenge lies in exploring optimal sequences within the combinatorial space of permutations. Recent research follows a two-stage (generator-evaluator) paradigm, where a generator produces multiple feasible sequences, and an evaluator selects the best one. In practice, the generator is typically implemented as an autoregressive model. However, these two-stage methods face two main challenges. First, the separation of the generator and evaluator hinders end-to-end training. Second, autoregressive generators suffer from inference efficiency. In this work, we propose a Unified Generative Efficient Reranking Framework (GReF) to address the two primary challenges. Specifically, we introduce Gen-Reranker, an autoregressive generator featuring a bidirectional encoder and a dynamic autoregressive decoder to generate causal reranking sequences. Subsequently, we pre-train Gen-Reranker on the item exposure order for high-quality parameter initialization. To eliminate the need for the evaluator while integrating sequence-level evaluation during training for end-to-end optimization, we propose post-training the model through Rerank-DPO. Moreover, for efficient autoregressive inference, we introduce ordered multi-token prediction (OMTP), which trains Gen-Reranker to simultaneously generate multiple future items while preserving their order, ensuring practical deployment in real-time recommender systems. Extensive offline experiments demonstrate that GReF outperforms state-of-the-art reranking methods while achieving latency that is nearly comparable to non-autoregressive models. Additionally, GReF has also been deployed in a real-world video app Kuaishou with over 300 million daily active users, significantly improving online recommendation quality.",
        "translated": "在多阶段推荐系统中，重排在建模物料列表内关联性方面发挥着关键作用。核心挑战在于探索排列组合空间中的最优序列。近期研究遵循两阶段（生成器-评估器）范式，其中生成器生成多个可行序列，评估器从中选择最优序列。实践中，生成器通常实现为自回归模型。然而，这类两阶段方法面临两个主要挑战：首先，生成器与评估器的分离阻碍了端到端训练；其次，自回归生成器在推理效率上存在瓶颈。本文提出一种统一的生成式高效重排框架（GReF），以应对上述两个核心挑战。具体而言，我们引入Gen-Reranker，一种具备双向编码器和动态自回归解码器的自回归生成器，用于生成因果重排序列。随后，我们在物料曝光顺序上对Gen-Reranker进行预训练，以实现高质量的参数初始化。为消除对评估器的依赖，同时在训练过程中整合序列级评估以实现端到端优化，我们提出通过Rerank-DPO对模型进行后训练。此外，为提升自回归推理效率，我们提出有序多token预测（OMTP），训练Gen-Reranker同时生成多个未来物料并保持其顺序，确保在实时推荐系统中的实际部署可行性。大量离线实验表明，GReF在性能上优于当前最先进的重排方法，同时延迟接近非自回归模型。此外，GReF已成功部署于拥有超过3亿日活跃用户的短视频应用Kuaishou，显著提升了线上推荐质量。",
        "translated_title": "GReF：一种通过有序多token预测实现高效重排的统一生成式框架",
        "label": [
            "重排（Re-ranking）",
            "LLM生成式推荐（LLM-based Generative Recommendation）",
            "序列推荐（Sequential Recommendation）"
        ],
        "label_reason": "论文核心为推荐系统重排阶段的生成式框架，解决序列建模与高效推理问题，紧密关联推荐系统核心环节。",
        "relevance_score": 10,
        "novelty_score": 9,
        "novelty_reason": "提出统一生成框架GReF，结合OMTP与Rerank-DPO，实现端到端训练与高效推理，显著提升生成式重排性能。"
    },
    {
        "title": "Model-Document Protocol for AI Search",
        "url": "http://arxiv.org/abs/2510.25160v1",
        "pub_date": "2025-10-29",
        "summary": "AI search depends on linking large language models (LLMs) with vast external knowledge sources. Yet web pages, PDF files, and other raw documents are not inherently LLM-ready: they are long, noisy, and unstructured. Conventional retrieval methods treat these documents as verbatim text and return raw passages, leaving the burden of fragment assembly and contextual reasoning to the LLM. This gap underscores the need for a new retrieval paradigm that redefines how models interact with documents.   We introduce the Model-Document Protocol (MDP), a general framework that formalizes how raw text is bridged to LLMs through consumable knowledge representations. Rather than treating retrieval as passage fetching, MDP defines multiple pathways that transform unstructured documents into task-specific, LLM-ready inputs. These include agentic reasoning, which curates raw evidence into coherent context; memory grounding, which accumulates reusable notes to enrich reasoning; and structured leveraging, which encodes documents into formal representations such as graphs or key-value caches. All three pathways share the same goal: ensuring that what reaches the LLM is not raw fragments but compact, structured knowledge directly consumable for reasoning.   As an instantiation, we present MDP-Agent, which realizes the protocol through an agentic process: constructing document-level gist memories for global coverage, performing diffusion-based exploration with vertical exploitation to uncover layered dependencies, and applying map-reduce style synthesis to integrate large-scale evidence into compact yet sufficient context. Experiments on information-seeking benchmarks demonstrate that MDP-Agent outperforms baselines, validating both the soundness of the MDP framework and the effectiveness of its agentic instantiation.",
        "translated": "AI搜索依赖于将大语言模型（LLM）与海量外部知识源连接。然而，网页、PDF文件及其他原始文档并非天然适配LLM：它们长度冗长、噪声多、结构松散。传统检索方法将这些文档视为逐字文本，返回原始段落，将碎片拼接与上下文推理的负担留给LLM承担。这一差距凸显出需要一种新的检索范式，重新定义模型与文档的交互方式。\n\n我们提出模型-文档协议（MDP），一种通用框架，形式化了原始文本如何通过可消费的知识表示与LLM衔接。MDP不再将检索视为段落获取，而是定义了多种路径，将非结构化文档转化为面向特定任务、适配LLM的输入。这些路径包括：代理式推理（agentic reasoning），将原始证据整理为连贯上下文；记忆锚定（memory grounding），积累可复用的笔记以增强推理；以及结构化利用（structured leveraging），将文档编码为图或键值缓存等正式表示。这三种路径共享同一目标：确保送达LLM的并非原始碎片，而是紧凑、结构化的知识，可直接用于推理。\n\n作为具体实现，我们提出MDP-Agent，通过代理式流程实现该协议：构建文档级概要记忆以实现全局覆盖，采用基于扩散的探索结合垂直利用以揭示多层次依赖关系，并应用类似Map-Reduce的合成策略，将大规模证据整合为紧凑但充分的上下文。在信息检索基准测试中的实验表明，MDP-Agent优于基线方法，验证了MDP框架的合理性及其代理式实现的有效性。",
        "translated_title": "模型-文档协议用于人工智能搜索",
        "label": [
            "LLM生成式推荐",
            "召回"
        ],
        "label_reason": "论文聚焦LLM与文档交互，提出MDP框架用于生成结构化知识输入，可应用于推荐系统召回环节。",
        "relevance_score": 4,
        "novelty_score": 8,
        "novelty_reason": "提出模型-文档协议新范式，通过代理推理与记忆接地等机制提升LLM输入质量，创新性强。"
    },
    {
        "title": "Continual Low-Rank Adapters for LLM-based Generative Recommender Systems",
        "url": "http://arxiv.org/abs/2510.25093v1",
        "pub_date": "2025-10-29",
        "summary": "While large language models (LLMs) achieve strong performance in recommendation, they face challenges in continual learning as users, items, and user preferences evolve over time. Existing LoRA-based continual methods primarily focus on preserving performance on previous tasks, but this overlooks the unique nature of recommendation: the goal is not to predict past preferences, and outdated preferences can even harm performance when current interests shift significantly. To address this, we propose PESO (Proximally rEgularized Single evolving lOra, a continual adaptation method for LoRA in recommendation. PESO introduces a proximal regularizer that anchors the current adapter to its most recent frozen state, enabling the model to flexibly balance adaptation and preservation, and to better capture recent user behaviors. Theoretically, we show that this proximal design provides data-aware, direction-wise guidance in the LoRA subspace. Empirically, PESO consistently outperforms existing LoRA-based continual learning methods.",
        "translated": "尽管大语言模型（LLM）在推荐系统中取得了优异性能，但在用户、物料及用户偏好随时间演变的持续学习场景下仍面临挑战。现有的基于LoRA的持续学习方法主要关注保持在先前任务上的性能，但忽视了推荐系统的独特性质：其目标并非预测过去的偏好，当用户当前兴趣发生显著变化时，过时的偏好甚至可能损害性能。为此，我们提出PESO（Proximally rEgularized Single evolving lOra），一种面向推荐系统的LoRA持续适配方法。PESO引入了一种近端正则化项，将当前适配器锚定在其最近冻结的状态上，从而使得模型能够灵活平衡适应与保留，更有效地捕捉近期用户行为。理论上，我们证明该近端设计在LoRA子空间中提供了数据感知、方向性的指导。实验表明，PESO在各类指标上始终优于现有的基于LoRA的持续学习方法。",
        "translated_title": "基于大语言模型的生成式推荐系统中的持续低秩适配器",
        "label": [
            "LLM生成式推荐（LLM-based Generative Recommendation）",
            "序列推荐（Sequential Recommendation）",
            "负采样与对比学习（Negative Sampling / Contrastive Learning）"
        ],
        "label_reason": "论文聚焦LLM生成式推荐中的持续学习，提出新方法PESO适应动态用户偏好，与推荐核心紧密相关。",
        "relevance_score": 9,
        "novelty_score": 8,
        "novelty_reason": "提出proximal正则化机制，平衡适应与保留，提升LLM在推荐场景下的持续学习能力，具显著创新性。"
    },
    {
        "title": "Secure Retrieval-Augmented Generation against Poisoning Attacks",
        "url": "http://arxiv.org/abs/2510.25025v1",
        "pub_date": "2025-10-28",
        "summary": "Large language models (LLMs) have transformed natural language processing (NLP), enabling applications from content generation to decision support. Retrieval-Augmented Generation (RAG) improves LLMs by incorporating external knowledge but also introduces security risks, particularly from data poisoning, where the attacker injects poisoned texts into the knowledge database to manipulate system outputs. While various defenses have been proposed, they often struggle against advanced attacks. To address this, we introduce RAGuard, a detection framework designed to identify poisoned texts. RAGuard first expands the retrieval scope to increase the proportion of clean texts, reducing the likelihood of retrieving poisoned content. It then applies chunk-wise perplexity filtering to detect abnormal variations and text similarity filtering to flag highly similar texts. This non-parametric approach enhances RAG security, and experiments on large-scale datasets demonstrate its effectiveness in detecting and mitigating poisoning attacks, including strong adaptive attacks.",
        "translated": "大语言模型（LLM）已彻底改变自然语言处理（NLP）领域，推动了从内容生成到决策支持等各类应用的发展。检索增强生成（RAG）通过引入外部知识提升了LLM的性能，但同时也带来了安全风险，尤其是在数据投毒攻击中，攻击者将中毒文本注入知识库以操纵系统输出。尽管已有多种防御方案被提出，但它们在应对高级攻击时往往效果有限。为此，我们提出RAGuard，一种用于识别中毒文本的检测框架。RAGuard首先扩展检索范围，提高干净文本的占比，从而降低检索到中毒内容的概率。随后，采用分块困惑度过滤来检测异常变化，并结合文本相似性过滤识别高度相似的文本。该非参数化方法有效增强了RAG的安全性，大规模数据集上的实验表明，其在检测和缓解投毒攻击方面具有显著效果，包括应对强自适应攻击。",
        "translated_title": "安全的检索增强生成对抗投毒攻击",
        "label": [],
        "label_reason": "论文聚焦RAG安全防御，属通用NLP领域，与推荐系统无直接关联。",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出非参数化检测框架，结合困惑度与相似度过滤，方法新颖有效。"
    },
    {
        "title": "Seeing Through the MiRAGE: Evaluating Multimodal Retrieval Augmented\n  Generation",
        "url": "http://arxiv.org/abs/2510.24870v1",
        "pub_date": "2025-10-28",
        "summary": "We introduce MiRAGE, an evaluation framework for retrieval-augmented generation (RAG) from multimodal sources. As audiovisual media becomes a prevalent source of information online, it is essential for RAG systems to integrate information from these sources into generation. However, existing evaluations for RAG are text-centric, limiting their applicability to multimodal, reasoning intensive settings because they don't verify information against sources. MiRAGE is a claim-centric approach to multimodal RAG evaluation, consisting of InfoF1, evaluating factuality and information coverage, and CiteF1, measuring citation support and completeness. We show that MiRAGE, when applied by humans, strongly aligns with extrinsic quality judgments. We additionally introduce automatic variants of MiRAGE and three prominent TextRAG metrics -- ACLE, ARGUE, and RAGAS -- demonstrating the limitations of text-centric work and laying the groundwork for automatic evaluation. We release open-source implementations and outline how to assess multimodal RAG.",
        "translated": "我们提出MiRAGE，一个用于多模态源检索增强生成（RAG）的评估框架。随着音视频媒体成为网络上普遍的信息来源，RAG系统有必要将这些来源的信息整合到生成过程中。然而，现有的RAG评估方法主要以文本为中心，限制了其在多模态、推理密集型场景中的适用性，因为它们无法对生成内容与来源信息进行核验。MiRAGE是一种以断言为中心的多模态RAG评估方法，包含InfoF1（用于评估事实性与信息覆盖度）和CiteF1（用于衡量引文支持与完整性）。我们证明，当由人类应用MiRAGE进行评估时，其结果与外部质量判断高度一致。此外，我们还引入了MiRAGE的自动评估变体，以及三种主流的TextRAG评估指标——ACLE、ARGUE和RAGAS，展示了以文本为中心的评估方法的局限性，并为自动评估奠定了基础。我们开源了相关实现，并概述了如何评估多模态RAG系统。",
        "translated_title": "透过MiRAGE看真相：评估多模态检索增强生成",
        "label": [
            "多模态推荐",
            "LLM生成式推荐"
        ],
        "label_reason": "论文聚焦多模态RAG评估，涉及生成式推荐中信息检索与生成的结合，与推荐系统间接相关。",
        "relevance_score": 4,
        "novelty_score": 7,
        "novelty_reason": "提出多模态RAG评估框架MiRAGE，包含新指标，对现有文本中心评估方法有显著改进。"
    },
    {
        "title": "ProfOlaf: Semi-Automated Tool for Systematic Literature Reviews",
        "url": "http://arxiv.org/abs/2510.26750v1",
        "pub_date": "2025-10-30",
        "summary": "Systematic reviews and mapping studies are critical for synthesizing research, identifying gaps, and guiding future work, but they are often labor-intensive and time-consuming. Existing tools provide partial support for specific steps, leaving much of the process manual and error-prone. We present ProfOlaf, a semi-automated tool designed to streamline systematic reviews while maintaining methodological rigor. ProfOlaf supports iterative snowballing for article collection with human-in-the-loop filtering and uses large language models to assist in analyzing articles, extracting key topics, and answering queries about the content of papers. By combining automation with guided manual effort, ProfOlaf enhances the efficiency, quality, and reproducibility of systematic reviews across research fields. A video describing and demonstrating ProfOlaf is available at: https://youtu.be/4noUXfcmxsE",
        "translated": "系统综述与映射研究对于整合研究成果、识别研究空白以及指导未来工作至关重要，但这些过程通常耗时耗力。现有工具仅对特定步骤提供部分支持，导致大部分流程仍需人工操作，易出错。我们提出 ProfOlaf，一种半自动化工具，旨在简化系统综述流程，同时保持方法学严谨性。ProfOlaf 支持带有人工介入过滤的迭代滚雪球式文献收集，并利用大语言模型辅助分析文献、提取关键主题以及回答关于论文内容的查询。通过结合自动化与引导式人工努力，ProfOlaf 提升了跨研究领域系统综述的效率、质量与可复现性。介绍并演示 ProfOlaf 的视频可访问：https://youtu.be/4noUXfcmxsE",
        "translated_title": "ProfOlaf: 用于系统性文献综述的半自动化工具",
        "label": [],
        "label_reason": "论文为文献综述工具，不涉及推荐系统任何环节",
        "relevance_score": 1,
        "novelty_score": 4,
        "novelty_reason": "基于LLM辅助文献分析，属通用NLP应用，无推荐领域创新"
    },
    {
        "title": "AdSum: Two-stream Audio-visual Summarization for Automated Video\n  Advertisement Clipping",
        "url": "http://arxiv.org/abs/2510.26569v1",
        "pub_date": "2025-10-30",
        "summary": "Advertisers commonly need multiple versions of the same advertisement (ad) at varying durations for a single campaign. The traditional approach involves manually selecting and re-editing shots from longer video ads to create shorter versions, which is labor-intensive and time-consuming. In this paper, we introduce a framework for automated video ad clipping using video summarization techniques. We are the first to frame video clipping as a shot selection problem, tailored specifically for advertising. Unlike existing general video summarization methods that primarily focus on visual content, our approach emphasizes the critical role of audio in advertising. To achieve this, we develop a two-stream audio-visual fusion model that predicts the importance of video frames, where importance is defined as the likelihood of a frame being selected in the firm-produced short ad. To address the lack of ad-specific datasets, we present AdSum204, a novel dataset comprising 102 pairs of 30-second and 15-second ads from real advertising campaigns. Extensive experiments demonstrate that our model outperforms state-of-the-art methods across various metrics, including Average Precision, Area Under Curve, Spearman, and Kendall.",
        "translated": "广告商通常需要在单个广告活动中为同一则广告（ad）提供不同时长的多个版本。传统方法需要人工从较长的视频广告中选取并重新编辑镜头，以生成较短版本，这一过程耗时且劳动密集。本文提出一种基于视频摘要技术的自动化视频广告剪辑框架。我们首次将视频剪辑问题形式化为一个面向广告场景的镜头选择问题。与现有通用视频摘要方法主要关注视觉内容不同，我们的方法强调音频在广告中的关键作用。为此，我们构建了一个双流音视频融合模型，用于预测视频帧的重要性，其中重要性定义为某帧被选入广告公司制作的短版广告中的概率。为解决广告专用数据集缺失的问题，我们发布了AdSum204，一个包含102对来自真实广告活动的30秒与15秒广告的新数据集。大量实验表明，我们的模型在平均精度（Average Precision）、曲线下面积（Area Under Curve）、斯皮尔曼相关系数（Spearman）和肯德尔相关系数（Kendall）等多个指标上均优于现有最先进方法。",
        "translated_title": "AdSum：面向自动化视频广告剪辑的双流音视频摘要方法",
        "label": [],
        "label_reason": "论文聚焦广告视频剪辑，属于视频处理与内容生成，与推荐系统无直接关联。",
        "relevance_score": 3,
        "novelty_score": 7,
        "novelty_reason": "提出双流音视频融合模型，创新性地引入音频特征，对广告剪辑任务有显著改进。"
    },
    {
        "title": "WeaveRec: An LLM-Based Cross-Domain Sequential Recommendation Framework\n  with Model Merging",
        "url": "http://arxiv.org/abs/2510.26546v1",
        "pub_date": "2025-10-30",
        "summary": "Cross-Domain Sequential Recommendation (CDSR) seeks to improve user preference modeling by transferring knowledge from multiple domains. Despite the progress made in CDSR, most existing methods rely on overlapping users or items to establish cross-domain correlations-a requirement that rarely holds in real-world settings. The advent of large language models (LLM) and model-merging techniques appears to overcome this limitation by unifying multi-domain data without explicit overlaps. Yet, our empirical study shows that naively training an LLM on combined domains-or simply merging several domain-specific LLMs-often degrades performance relative to a model trained solely on the target domain. To address these challenges, we first experimentally investigate the cause of suboptimal performance in LLM-based cross-domain recommendation and model merging. Building on these insights, we introduce WeaveRec, which cross-trains multiple LoRA modules with source and target domain data in a weaving fashion, and fuses them via model merging. WeaveRec can be extended to multi-source domain scenarios and notably does not introduce additional inference-time cost in terms of latency or memory. Furthermore, we provide a theoretical guarantee that WeaveRec can reduce the upper bound of the expected error in the target domain. Extensive experiments on single-source, multi-source, and cross-platform cross-domain recommendation scenarios validate that WeaveRec effectively mitigates performance degradation and consistently outperforms baseline approaches in real-world recommendation tasks.",
        "translated": "跨域序列推荐（CDSR）旨在通过从多个领域迁移知识来提升用户偏好建模能力。尽管CDSR领域已取得一定进展，但现有大多数方法依赖于用户或物料在不同领域间的重叠来建立跨域关联——这一前提在真实场景中往往难以满足。大型语言模型（LLM）与模型融合技术的出现，似乎通过无需显式重叠即可统一多域数据的方式克服了这一限制。然而，我们的实证研究表明，简单地在合并后的多域数据上训练LLM，或直接合并多个领域专用LLM，通常会导致性能劣于仅在目标域上训练的模型。为应对这些挑战，我们首先实验性地探究了基于LLM的跨域推荐及模型融合中性能欠佳的原因。基于这些洞察，我们提出WeaveRec，该方法以“交织”方式利用源域与目标域数据交叉训练多个LoRA模块，并通过模型融合进行整合。WeaveRec可扩展至多源域场景，且在推理阶段不引入额外的延迟或内存开销。此外，我们提供了理论保证，表明WeaveRec能够降低目标域预期误差的上界。在单源、多源及跨平台跨域推荐场景下的大量实验验证表明，WeaveRec有效缓解了性能退化问题，并在真实推荐任务中始终优于基线方法。",
        "translated_title": "WeaveRec：一种基于大语言模型的跨域序列推荐框架及其模型融合方法",
        "label": [
            "序列推荐",
            "跨域/联邦推荐",
            "LLM生成式推荐",
            "通用推荐技术"
        ],
        "label_reason": "论文聚焦跨域序列推荐，结合LLM与模型合并技术，直接解决推荐系统核心问题",
        "relevance_score": 9,
        "novelty_score": 8,
        "novelty_reason": "提出WeaveRec框架，创新性地结合LoRA与模型合并，提升跨域推荐性能"
    },
    {
        "title": "Inside CORE-KG: Evaluating Structured Prompting and Coreference\n  Resolution for Knowledge Graphs",
        "url": "http://arxiv.org/abs/2510.26512v1",
        "pub_date": "2025-10-30",
        "summary": "Human smuggling networks are increasingly adaptive and difficult to analyze. Legal case documents offer critical insights but are often unstructured, lexically dense, and filled with ambiguous or shifting references, which pose significant challenges for automated knowledge graph (KG) construction. While recent LLM-based approaches improve over static templates, they still generate noisy, fragmented graphs with duplicate nodes due to the absence of guided extraction and coreference resolution. The recently proposed CORE-KG framework addresses these limitations by integrating a type-aware coreference module and domain-guided structured prompts, significantly reducing node duplication and legal noise. In this work, we present a systematic ablation study of CORE-KG to quantify the individual contributions of its two key components. Our results show that removing coreference resolution results in a 28.32% increase in node duplication and a 4.32% increase in noisy nodes, while removing structured prompts leads to a 4.34% increase in node duplication and a 73.33% increase in noisy nodes. These findings offer empirical insights for designing robust LLM-based pipelines for extracting structured representations from complex legal texts.",
        "translated": "人口走私网络日益具有适应性，分析难度不断加大。法律案件文档虽提供了关键信息，但通常结构松散、词汇密集，且包含大量模糊或指代变化的表达，给自动化知识图谱（KG）构建带来显著挑战。尽管近期基于大语言模型（LLM）的方法相较于静态模板有所改进，但由于缺乏引导式抽取和共指消解，仍会产生带有噪声、碎片化且存在重复节点的知识图谱。最近提出的 CORE-KG 框架通过集成类型感知的共指消解模块和领域引导的结构化提示，显著减少了节点重复和法律噪声。在本研究中，我们对 CORE-KG 进行了系统的消融实验，以量化其两个核心组件的独立贡献。实验结果表明，移除共指消解会导致节点重复率增加 28.32%，噪声节点增加 4.32%；而移除结构化提示则导致节点重复率增加 4.34%，噪声节点增加 73.33%。这些发现为设计稳健的基于大语言模型的复杂法律文本结构化表示抽取流水线提供了实证依据。",
        "translated_title": "在CORE-KG中：评估知识图谱的结构化提示与共指消解",
        "label": [],
        "label_reason": "论文聚焦法律文本知识图谱构建，与推荐系统无直接关联。",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出结构化提示与共指消解模块，对LLM抽取有改进，但非推荐领域创新。"
    },
    {
        "title": "LINK-KG: LLM-Driven Coreference-Resolved Knowledge Graphs for Human\n  Smuggling Networks",
        "url": "http://arxiv.org/abs/2510.26486v1",
        "pub_date": "2025-10-30",
        "summary": "Human smuggling networks are complex and constantly evolving, making them difficult to analyze comprehensively. Legal case documents offer rich factual and procedural insights into these networks but are often long, unstructured, and filled with ambiguous or shifting references, posing significant challenges for automated knowledge graph (KG) construction. Existing methods either overlook coreference resolution or fail to scale beyond short text spans, leading to fragmented graphs and inconsistent entity linking. We propose LINK-KG, a modular framework that integrates a three-stage, LLM-guided coreference resolution pipeline with downstream KG extraction. At the core of our approach is a type-specific Prompt Cache, which consistently tracks and resolves references across document chunks, enabling clean and disambiguated narratives for structured knowledge graph construction from both short and long legal texts. LINK-KG reduces average node duplication by 45.21% and noisy nodes by 32.22% compared to baseline methods, resulting in cleaner and more coherent graph structures. These improvements establish LINK-KG as a strong foundation for analyzing complex criminal networks.",
        "translated": "人口走私网络复杂且持续演变，难以进行全面分析。法律案件文件虽提供了关于这些网络的丰富事实与程序性信息，但通常篇幅较长、结构松散，且包含大量模糊或指代变化的表达，给自动化知识图谱（KG）构建带来了显著挑战。现有方法要么忽略共指消解，要么无法扩展至长文本片段，导致生成的知识图谱碎片化且实体链接不一致。我们提出LINK-KG，一种模块化框架，将三阶段、大语言模型（LLM）引导的共指消解流水线与下游知识图谱抽取相结合。本方法的核心是类型特定的Prompt缓存，它能够持续追踪并消解文档片段中的指代关系，从而为短文本和长法律文本的结构化知识图谱构建提供清晰、无歧义的叙述。与基线方法相比，LINK-KG将平均节点重复率降低45.21%，噪声节点减少32.22%，生成更干净、更连贯的图结构。这些改进使LINK-KG成为分析复杂犯罪网络的坚实基础。",
        "translated_title": "LINK-KG：基于大语言模型的共指消解知识图谱在人口走私网络中的应用",
        "label": [],
        "label_reason": "论文聚焦于法律文本中的知识图谱构建与指代消解，与推荐系统无直接关联。",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出LLM引导的三阶段指代消解框架，结合类型特定提示缓存，具有创新性，但非推荐领域。"
    },
    {
        "title": "Vectorized Context-Aware Embeddings for GAT-Based Collaborative\n  Filtering",
        "url": "http://arxiv.org/abs/2510.26461v1",
        "pub_date": "2025-10-30",
        "summary": "Recommender systems often struggle with data sparsity and cold-start scenarios, limiting their ability to provide accurate suggestions for new or infrequent users. This paper presents a Graph Attention Network (GAT) based Collaborative Filtering (CF) framework enhanced with Large Language Model (LLM) driven context aware embeddings. Specifically, we generate concise textual user profiles and unify item metadata (titles, genres, overviews) into rich textual embeddings, injecting these as initial node features in a bipartite user item graph. To further optimize ranking performance, we introduce a hybrid loss function that combines Bayesian Personalized Ranking (BPR) with a cosine similarity term and robust negative sampling, ensuring explicit negative feedback is distinguished from unobserved data. Experiments on the MovieLens 100k and 1M datasets show consistent improvements over state-of-the-art baselines in Precision, NDCG, and MAP while demonstrating robustness for users with limited interaction history. Ablation studies confirm the critical role of LLM-augmented embeddings and the cosine similarity term in capturing nuanced semantic relationships. Our approach effectively mitigates sparsity and cold-start limitations by integrating LLM-derived contextual understanding into graph-based architectures. Future directions include balancing recommendation accuracy with coverage and diversity, and introducing fairness-aware constraints and interpretability features to enhance system performance further.",
        "translated": "推荐系统常面临数据稀疏性和冷启动场景的挑战，限制了其为新用户或交互较少用户提供准确建议的能力。本文提出一种基于图注意力网络（GAT）的协同过滤（CF）框架，并引入由大语言模型（LLM）驱动的上下文感知嵌入。具体而言，我们生成简洁的文本化用户画像，并将物料元数据（如标题、类型、简介）统一整合为丰富的文本嵌入，将其作为二分图用户-物料图中的初始节点特征注入。为进一步优化排序性能，我们引入一种混合损失函数，结合贝叶斯个性化排序（BPR）与余弦相似度项，并采用鲁棒的负采样策略，确保显式负反馈与未观测数据能够被有效区分。在MovieLens 100k和1M数据集上的实验表明，与当前主流基线相比，我们的方法在精确率（Precision）、NDCG和MAP等指标上均取得稳定提升，同时对交互历史较少的用户表现出良好的鲁棒性。消融实验验证了LLM增强嵌入和余弦相似度项在捕捉细微语义关系中的关键作用。通过将LLM衍生的上下文理解融入图结构架构，我们的方法有效缓解了数据稀疏性和冷启动问题。未来研究方向包括在推荐准确率与覆盖率、多样性之间取得平衡，并引入公平性约束与可解释性特征，以进一步提升系统性能。",
        "translated_title": "向量化上下文感知嵌入用于基于GAT的协同过滤",
        "label": [
            "图神经网络推荐（GNN for Recommendation）",
            "序列推荐（Sequential Recommendation）",
            "LLM生成式推荐（LLM-based Generative Recommendation）",
            "负采样与对比学习（Negative Sampling / Contrastive Learning）"
        ],
        "label_reason": "基于GAT的协同过滤框架结合LLM生成上下文感知嵌入，解决稀疏性与冷启动，直接用于推荐精排环节。",
        "relevance_score": 9,
        "novelty_score": 8,
        "novelty_reason": "创新性融合LLM与GAT，引入混合损失函数优化排序，显著提升推荐性能与鲁棒性。"
    },
    {
        "title": "Barlow Twins for Sequential Recommendation",
        "url": "http://arxiv.org/abs/2510.26407v1",
        "pub_date": "2025-10-30",
        "summary": "Sequential recommendation models must navigate sparse interaction data popularity bias and conflicting objectives like accuracy versus diversity While recent contrastive selfsupervised learning SSL methods offer improved accuracy they come with tradeoffs large batch requirements reliance on handcrafted augmentations and negative sampling that can reinforce popularity bias In this paper we introduce BT-SR a novel noncontrastive SSL framework that integrates the Barlow Twins redundancyreduction principle into a Transformerbased nextitem recommender BTSR learns embeddings that align users with similar shortterm behaviors while preserving longterm distinctionswithout requiring negative sampling or artificial perturbations This structuresensitive alignment allows BT-SR to more effectively recognize emerging user intent and mitigate the influence of noisy historical context Our experiments on five public benchmarks demonstrate that BTSR consistently improves nextitem prediction accuracy and significantly enhances longtail item coverage and recommendation calibration Crucially we show that a single hyperparameter can control the accuracydiversity tradeoff enabling practitioners to adapt recommendations to specific application needs",
        "translated": "序列推荐模型必须应对稀疏交互数据、流行度偏差以及准确性与多样性之间的冲突目标。尽管近期的对比自监督学习（SSL）方法在提升准确性方面取得进展，但其存在若干权衡：需要较大的批次规模、依赖于手工设计的数据增强策略，以及负采样机制可能加剧流行度偏差。本文提出 BT-SR，一种新颖的非对比式 SSL 框架，将 Barlow Twins 的冗余消除原则集成到基于 Transformer 的下一项推荐模型中。BT-SR 学习用户嵌入，使其在相似短期行为上对齐，同时保留长期行为的区分性，且无需负采样或人工扰动。这种结构敏感的对齐机制使 BT-SR 能更有效地识别用户新兴意图，并减轻噪声历史上下文的影响。在五个公开基准数据集上的实验表明，BT-SR 一致提升了下一项预测的准确性，显著增强了长尾物料的覆盖率和推荐校准性能。尤为重要的是，我们证明单个超参数即可控制准确性与多样性的权衡，使实践者能够根据具体应用场景灵活调整推荐策略。",
        "translated_title": "Barlow Twins 用于序列推荐",
        "label": [
            "序列推荐",
            "负采样与对比学习",
            "通用推荐技术"
        ],
        "label_reason": "论文提出非对比式自监督框架用于序列推荐，解决负采样偏差问题，直接关联推荐系统核心环节。",
        "relevance_score": 8,
        "novelty_score": 8,
        "novelty_reason": "创新性引入Barlow Twins机制替代传统对比学习，无需负采样，有效缓解流行度偏差，提升长尾覆盖。"
    },
    {
        "title": "GraphCompliance: Aligning Policy and Context Graphs for LLM-Based\n  Regulatory Compliance",
        "url": "http://arxiv.org/abs/2510.26309v1",
        "pub_date": "2025-10-30",
        "summary": "Compliance at web scale poses practical challenges: each request may require a regulatory assessment. Regulatory texts (e.g., the General Data Protection Regulation, GDPR) are cross-referential and normative, while runtime contexts are expressed in unstructured natural language. This setting motivates us to align semantic information in unstructured text with the structured, normative elements of regulations. To this end, we introduce GraphCompliance, a framework that represents regulatory texts as a Policy Graph and runtime contexts as a Context Graph, and aligns them. In this formulation, the policy graph encodes normative structure and cross-references, whereas the context graph formalizes events as subject-action-object (SAO) and entity-relation triples. This alignment anchors the reasoning of a judge large language model (LLM) in structured information and helps reduce the burden of regulatory interpretation and event parsing, enabling a focus on the core reasoning step. In experiments on 300 GDPR-derived real-world scenarios spanning five evaluation tasks, GraphCompliance yields 4.1-7.2 percentage points (pp) higher micro-F1 than LLM-only and RAG baselines, with fewer under- and over-predictions, resulting in higher recall and lower false positive rates. Ablation studies indicate contributions from each graph component, suggesting that structured representations and a judge LLM are complementary for normative reasoning.",
        "translated": "在大规模网络场景下，合规性面临实际挑战：每个请求可能都需要进行法规评估。法规文本（例如《通用数据保护条例》，GDPR）具有交叉引用和规范性特征，而运行时上下文则以非结构化的自然语言表达。这一设定促使我们对非结构化文本中的语义信息与法规的结构化、规范性要素进行对齐。为此，我们提出 GraphCompliance 框架，该框架将法规文本表示为政策图（Policy Graph），将运行时上下文表示为上下文图（Context Graph），并实现二者对齐。在此建模中，政策图编码了规范性结构和交叉引用，而上下文图则将事件形式化为“主体-动作-客体”（SAO）三元组和“实体-关系”三元组。这种对齐方式将判别大语言模型（LLM）的推理锚定于结构化信息，有助于减轻法规解释和事件解析的负担，使模型能够专注于核心推理步骤。在涵盖五个评估任务的300个源自GDPR的真实场景实验中，GraphCompliance 相较于仅使用LLM和RAG的基线方法，微F1得分高出4.1至7.2个百分点（pp），且误判和过判情况更少，从而实现了更高的召回率和更低的误报率。消融实验表明，各图组件均对性能提升有贡献，说明结构化表示与判别LLM在规范性推理中具有互补性。",
        "translated_title": "GraphCompliance：对齐策略图与上下文图以实现基于大语言模型的合规性监管",
        "label": [],
        "label_reason": "论文聚焦法律合规判断，虽用LLM和图结构，但非推荐系统核心问题，与推荐无直接关联。",
        "relevance_score": 3,
        "novelty_score": 7,
        "novelty_reason": "提出政策图与上下文图对齐新框架，结合LLM提升合规判断准确率，方法新颖但非推荐领域。"
    },
    {
        "title": "DiSE: A diffusion probabilistic model for automatic structure\n  elucidation of organic compounds",
        "url": "http://arxiv.org/abs/2510.26231v1",
        "pub_date": "2025-10-30",
        "summary": "Automatic structure elucidation is essential for self-driving laboratories as it enables the system to achieve truly autonomous. This capability closes the experimental feedback loop, ensuring that machine learning models receive reliable structure information for real-time decision-making and optimization. Herein, we present DiSE, an end-to-end diffusion-based generative model that integrates multiple spectroscopic modalities, including MS, 13C and 1H chemical shifts, HSQC, and COSY, to achieve automated yet accurate structure elucidation of organic compounds. By learning inherent correlations among spectra through data-driven approaches, DiSE achieves superior accuracy, strong generalization across chemically diverse datasets, and robustness to experimental data despite being trained on calculated spectra. DiSE thus represents a significant advance toward fully automated structure elucidation, with broad potential in natural product research, drug discovery, and self-driving laboratories.",
        "translated": "自动结构解析对于自动驾驶实验室至关重要，因为它使系统能够实现真正的自主性。该能力闭合了实验反馈回路，确保机器学习模型能够获得可靠的结构信息，以支持实时决策与优化。本文提出DiSE，一种端到端的基于扩散的生成模型，该模型整合了多种光谱模态，包括MS、13C和1H化学位移、HSQC以及COSY，以实现有机化合物的自动化且准确的结构解析。通过数据驱动的方法学习光谱之间的内在关联，DiSE在准确性、跨化学多样性数据集的泛化能力以及对实验数据的鲁棒性方面均表现出色，尽管其训练数据为计算得到的光谱。因此，DiSE代表了迈向完全自动化结构解析的重要进展，在天然产物研究、药物发现以及自动驾驶实验室等领域具有广阔的应用潜力。",
        "translated_title": "DiSE：一种用于有机化合物自动结构阐明的扩散概率模型",
        "label": [],
        "label_reason": "论文聚焦有机化合物结构解析，属于化学信息学，与推荐系统无直接关联。",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出基于扩散模型的多模态生成方法，对结构解析有创新，但非推荐领域。"
    },
    {
        "title": "ReaKase-8B: Legal Case Retrieval via Knowledge and Reasoning\n  Representations with LLMs",
        "url": "http://arxiv.org/abs/2510.26178v1",
        "pub_date": "2025-10-30",
        "summary": "Legal case retrieval (LCR) is a cornerstone of real-world legal decision making, as it enables practitioners to identify precedents for a given query case. Existing approaches mainly rely on traditional lexical models and pretrained language models to encode the texts of legal cases. Yet there are rich information in the relations among different legal entities as well as the crucial reasoning process that uncovers how legal facts and legal issues can lead to judicial decisions. Such relational reasoning process reflects the distinctive characteristics of each case that can distinguish one from another, mirroring the real-world judicial process. Naturally, incorporating such information into the precise case embedding could further enhance the accuracy of case retrieval. In this paper, a novel ReaKase-8B framework is proposed to leverage extracted legal facts, legal issues, legal relation triplets and legal reasoning for effective legal case retrieval. ReaKase-8B designs an in-context legal case representation learning paradigm with a fine-tuned large language model. Extensive experiments on two benchmark datasets from COLIEE 2022 and COLIEE 2023 demonstrate that our knowledge and reasoning augmented embeddings substantially improve retrieval performance over baseline models, highlighting the potential of integrating legal reasoning into legal case retrieval systems. The code has been released on https://github.com/yanran-tang/ReaKase-8B.",
        "translated": "法律案例检索（Legal Case Retrieval, LCR）是现实世界法律决策的核心环节，它使从业者能够为给定的查询案例识别出相关判例。现有方法主要依赖传统词法模型和预训练语言模型对法律案例文本进行编码。然而，不同法律实体之间的关系蕴含着丰富的信息，同时，揭示法律事实与法律问题如何导向司法判决的关键推理过程同样至关重要。这种关系推理过程反映了每个案例的独特特征，能够有效区分不同案例，体现了真实的司法推理流程。因此，将此类信息融入精确的案例嵌入表示中，有望进一步提升案例检索的准确性。本文提出了一种新颖的 ReaKase-8B 框架，旨在利用提取出的法律事实、法律问题、法律关系三元组以及法律推理过程，实现高效的法律案例检索。ReaKase-8B 设计了一种基于微调大语言模型的上下文法律案例表示学习范式。在 COLIEE 2022 和 COLIEE 2023 两个基准数据集上的大量实验表明，我们的知识与推理增强型嵌入显著优于基线模型，验证了将法律推理融入法律案例检索系统的潜力。代码已发布于 https://github.com/yanran-tang/ReaKase-8B。",
        "translated_title": "ReaKase-8B：基于大语言模型的知识与推理表示的法律案例检索",
        "label": [
            "召回",
            "LLM生成式推荐",
            "负采样与对比学习"
        ],
        "label_reason": "论文聚焦法律案例检索，属信息检索范畴，与推荐系统间接相关，核心为召回环节。",
        "relevance_score": 4,
        "novelty_score": 7,
        "novelty_reason": "提出基于LLM的上下文法律案例表示学习范式，结合知识与推理增强嵌入，方法新颖且有效。"
    },
    {
        "title": "OneTrans: Unified Feature Interaction and Sequence Modeling with One\n  Transformer in Industrial Recommender",
        "url": "http://arxiv.org/abs/2510.26104v1",
        "pub_date": "2025-10-30",
        "summary": "In recommendation systems, scaling up feature-interaction modules (e.g., Wukong, RankMixer) or user-behavior sequence modules (e.g., LONGER) has achieved notable success. However, these efforts typically proceed on separate tracks, which not only hinders bidirectional information exchange but also prevents unified optimization and scaling. In this paper, we propose OneTrans, a unified Transformer backbone that simultaneously performs user-behavior sequence modeling and feature interaction. OneTrans employs a unified tokenizer to convert both sequential and non-sequential attributes into a single token sequence. The stacked OneTrans blocks share parameters across similar sequential tokens while assigning token-specific parameters to non-sequential tokens. Through causal attention and cross-request KV caching, OneTrans enables precomputation and caching of intermediate representations, significantly reducing computational costs during both training and inference. Experimental results on industrial-scale datasets demonstrate that OneTrans scales efficiently with increasing parameters, consistently outperforms strong baselines, and yields a 5.68% lift in per-user GMV in online A/B tests.",
        "translated": "在推荐系统中，扩展特征交互模块（如 Wukong、RankMixer）或用户行为序列模块（如 LONGER）已取得显著成效。然而，这些努力通常在独立的路径上进行，不仅阻碍了双向信息交换，也妨碍了统一优化与扩展。本文提出 OneTrans，一种统一的 Transformer 主干网络，能够同时完成用户行为序列建模与特征交互。OneTrans 采用统一的分词器，将序列化与非序列化属性均转换为单一的 token 序列。堆叠的 OneTrans 模块在相似的序列化 token 间共享参数，同时为非序列化 token 分配特定的参数。通过因果注意力机制与跨请求 KV 缓存，OneTrans 实现了中间表示的预计算与缓存，显著降低了训练与推理阶段的计算成本。在工业级数据集上的实验结果表明，OneTrans 能够随着参数量增加高效扩展，持续优于强基线模型，并在在线 A/B 测试中实现每用户 GMV 提升 5.68%。",
        "translated_title": "OneTrans：工业推荐系统中基于单一Transformer的统一特征交互与序列建模",
        "label": [
            "精排（Ranking）",
            "序列推荐（Sequential Recommendation）",
            "通用推荐技术（General Recommendation Techniques）"
        ],
        "label_reason": "论文提出统一Transformer架构，融合序列建模与特征交互，直接用于推荐系统精排环节，显著提升性能。",
        "relevance_score": 9,
        "novelty_score": 8,
        "novelty_reason": "创新性地统一序列与非序列特征处理，共享参数并支持KV缓存，提升效率与可扩展性，属结构化改进。"
    },
    {
        "title": "ORBIT - Open Recommendation Benchmark for Reproducible Research with\n  Hidden Tests",
        "url": "http://arxiv.org/abs/2510.26095v1",
        "pub_date": "2025-10-30",
        "summary": "Recommender systems are among the most impactful AI applications, interacting with billions of users every day, guiding them to relevant products, services, or information tailored to their preferences. However, the research and development of recommender systems are hindered by existing datasets that fail to capture realistic user behaviors and inconsistent evaluation settings that lead to ambiguous conclusions. This paper introduces the Open Recommendation Benchmark for Reproducible Research with HIdden Tests (ORBIT), a unified benchmark for consistent and realistic evaluation of recommendation models. ORBIT offers a standardized evaluation framework of public datasets with reproducible splits and transparent settings for its public leaderboard. Additionally, ORBIT introduces a new webpage recommendation task, ClueWeb-Reco, featuring web browsing sequences from 87 million public, high-quality webpages. ClueWeb-Reco is a synthetic dataset derived from real, user-consented, and privacy-guaranteed browsing data. It aligns with modern recommendation scenarios and is reserved as the hidden test part of our leaderboard to challenge recommendation models' generalization ability. ORBIT measures 12 representative recommendation models on its public benchmark and introduces a prompted LLM baseline on the ClueWeb-Reco hidden test. Our benchmark results reflect general improvements of recommender systems on the public datasets, with variable individual performances. The results on the hidden test reveal the limitations of existing approaches in large-scale webpage recommendation and highlight the potential for improvements with LLM integrations. ORBIT benchmark, leaderboard, and codebase are available at https://www.open-reco-bench.ai.",
        "translated": "推荐系统是影响力最大的人工智能应用之一，每天与数十亿用户交互，引导他们发现符合其偏好的相关产品、服务或信息。然而，推荐系统的研究与开发受到现有数据集的制约，这些数据集无法准确捕捉真实用户行为，且评估设置不一致，导致结论模糊。本文提出了面向可复现研究的隐藏测试开放推荐基准（Open Recommendation Benchmark for Reproducible Research with HIdden Tests, ORBIT），这是一个用于一致且真实评估推荐模型的统一基准。ORBIT 提供了包含可复现划分和透明设置的公开数据集标准化评估框架，并为公开排行榜提供支持。此外，ORBIT 引入了一个新的网页推荐任务 ClueWeb-Reco，该任务基于 8700 万份公开、高质量网页的浏览序列。ClueWeb-Reco 是一个从真实、用户同意且隐私保障的浏览数据中衍生出的合成数据集，契合现代推荐场景，并作为排行榜的隐藏测试部分，用于挑战推荐模型的泛化能力。ORBIT 在其公开基准上评估了 12 个代表性推荐模型，并在 ClueWeb-Reco 隐藏测试集上引入了一个提示式大语言模型（LLM）基线。我们的基准测试结果反映出推荐系统在公开数据集上的整体性能提升，但各模型个体表现存在差异。隐藏测试集的结果揭示了现有方法在大规模网页推荐场景中的局限性，并凸显了通过集成大语言模型实现性能提升的潜力。ORBIT 基准、排行榜及代码库可通过 https://www.open-reco-bench.ai 获取。",
        "translated_title": "ORBIT —— 带隐藏测试集的可复现研究开放推荐系统基准",
        "label": [
            "推荐系统评估",
            "通用推荐技术"
        ],
        "label_reason": "论文构建推荐系统基准ORBIT，含公开与隐藏测试集，用于评估推荐模型，属推荐系统评估与通用技术范畴。",
        "relevance_score": 8,
        "novelty_score": 7,
        "novelty_reason": "提出标准化评估框架与隐藏测试集，提升可复现性，但未提出新算法或模型架构。"
    },
    {
        "title": "The Quest for Reliable Metrics of Responsible AI",
        "url": "http://arxiv.org/abs/2510.26007v1",
        "pub_date": "2025-10-29",
        "summary": "The development of Artificial Intelligence (AI), including AI in Science (AIS), should be done following the principles of responsible AI. Progress in responsible AI is often quantified through evaluation metrics, yet there has been less work on assessing the robustness and reliability of the metrics themselves. We reflect on prior work that examines the robustness of fairness metrics for recommender systems as a type of AI application and summarise their key takeaways into a set of non-exhaustive guidelines for developing reliable metrics of responsible AI. Our guidelines apply to a broad spectrum of AI applications, including AIS.",
        "translated": "人工智能（AI）的发展，包括科学领域的人工智能（AIS），应遵循负责任AI的原则。负责任AI的进展通常通过评估指标进行量化，然而针对这些指标本身稳健性与可靠性的评估工作却相对较少。我们回顾了先前研究中关于推荐系统（一种AI应用）公平性指标稳健性的相关工作，并将其核心结论总结为一套非穷尽的指导原则，用于开发负责任AI的可靠评估指标。这些指导原则适用于广泛的AI应用场景，包括AIS。",
        "translated_title": "负责任人工智能可靠指标的探索",
        "label": [
            "推荐系统公平性/可解释性",
            "推荐系统评估"
        ],
        "label_reason": "论文聚焦推荐系统公平性评估指标的可靠性，与推荐系统评估和公平性相关",
        "relevance_score": 4,
        "novelty_score": 5,
        "novelty_reason": "提出评估指标可靠性的指南，非全新方法，属总结性改进"
    },
    {
        "title": "Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with\n  the MME-CoF Benchmark",
        "url": "http://arxiv.org/abs/2510.26802v1",
        "pub_date": "2025-10-30",
        "summary": "Recent video generation models can produce high-fidelity, temporally coherent videos, indicating that they may encode substantial world knowledge. Beyond realistic synthesis, they also exhibit emerging behaviors indicative of visual perception, modeling, and manipulation. Yet, an important question still remains: Are video models ready to serve as zero-shot reasoners in challenging visual reasoning scenarios? In this work, we conduct an empirical study to comprehensively investigate this question, focusing on the leading and popular Veo-3. We evaluate its reasoning behavior across 12 dimensions, including spatial, geometric, physical, temporal, and embodied logic, systematically characterizing both its strengths and failure modes. To standardize this study, we curate the evaluation data into MME-CoF, a compact benchmark that enables in-depth and thorough assessment of Chain-of-Frame (CoF) reasoning. Our findings reveal that while current video models demonstrate promising reasoning patterns on short-horizon spatial coherence, fine-grained grounding, and locally consistent dynamics, they remain limited in long-horizon causal reasoning, strict geometric constraints, and abstract logic. Overall, they are not yet reliable as standalone zero-shot reasoners, but exhibit encouraging signs as complementary visual engines alongside dedicated reasoning models. Project page: https://video-cof.github.io",
        "translated": "近期的视频生成模型能够生成高保真、时间连贯的视频，表明它们可能编码了丰富的世界知识。除了逼真的合成能力外，这些模型还展现出一些新兴行为，体现出视觉感知、建模与操控的能力。然而，一个重要问题仍然存在：视频模型是否已准备好在具有挑战性的视觉推理场景中充当零样本推理器？在本研究中，我们开展了一项实证研究，全面探讨这一问题，重点聚焦于当前主流且广受欢迎的 Veo-3 模型。我们从12个维度评估其推理行为，包括空间、几何、物理、时间以及具身逻辑，系统性地刻画了其优势与失败模式。为规范本研究，我们构建了 MME-CoF，一个紧凑型基准数据集，用于深入且全面地评估帧序列推理（Chain-of-Frame, CoF）能力。我们的研究结果表明，尽管当前视频模型在短时域空间一致性、细粒度 grounding 以及局部一致动态方面展现出有前景的推理模式，但在长时域因果推理、严格几何约束和抽象逻辑推理方面仍存在明显局限。总体而言，它们尚不能作为独立的零样本推理器可靠使用，但作为专用推理模型的辅助视觉引擎，已展现出令人鼓舞的潜力。项目主页：https://video-cof.github.io",
        "translated_title": "视频模型是否已具备零样本推理能力？基于MME-CoF基准的实证研究",
        "label": [],
        "label_reason": "论文聚焦视频模型在零样本推理能力评估，属于高阶视觉理解任务，非像素级图像恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "提出MME-CoF基准评估视频模型推理能力，属评估方法创新，非图像处理核心算法创新。"
    },
    {
        "title": "OmniX: From Unified Panoramic Generation and Perception to\n  Graphics-Ready 3D Scenes",
        "url": "http://arxiv.org/abs/2510.26800v1",
        "pub_date": "2025-10-30",
        "summary": "There are two prevalent ways to constructing 3D scenes: procedural generation and 2D lifting. Among them, panorama-based 2D lifting has emerged as a promising technique, leveraging powerful 2D generative priors to produce immersive, realistic, and diverse 3D environments. In this work, we advance this technique to generate graphics-ready 3D scenes suitable for physically based rendering (PBR), relighting, and simulation. Our key insight is to repurpose 2D generative models for panoramic perception of geometry, textures, and PBR materials. Unlike existing 2D lifting approaches that emphasize appearance generation and ignore the perception of intrinsic properties, we present OmniX, a versatile and unified framework. Based on a lightweight and efficient cross-modal adapter structure, OmniX reuses 2D generative priors for a broad range of panoramic vision tasks, including panoramic perception, generation, and completion. Furthermore, we construct a large-scale synthetic panorama dataset containing high-quality multimodal panoramas from diverse indoor and outdoor scenes. Extensive experiments demonstrate the effectiveness of our model in panoramic visual perception and graphics-ready 3D scene generation, opening new possibilities for immersive and physically realistic virtual world generation.",
        "translated": "构建三维场景主要有两种主流方法：程序化生成与二维提升（2D lifting）。其中，基于全景图的二维提升技术近年来展现出巨大潜力，通过利用强大的二维生成先验，能够生成沉浸式、逼真且多样化的三维环境。在本研究中，我们进一步发展该技术，以生成适用于基于物理的渲染（PBR）、重光照（relighting）和模拟的图形就绪型三维场景。我们的核心思想是，重新利用二维生成模型，用于对几何结构、纹理以及PBR材质的全景感知。不同于现有二维提升方法侧重于外观生成而忽略内在属性感知的特点，我们提出OmniX——一个通用且统一的框架。该框架基于轻量高效的跨模态适配器结构，能够复用二维生成先验，广泛应用于多种全景视觉任务，包括全景感知、生成与补全。此外，我们构建了一个大规模合成全景数据集，包含来自多种室内与室外场景的高质量多模态全景图。大量实验表明，我们的模型在全景视觉感知与图形就绪型三维场景生成方面具有显著有效性，为沉浸式且物理真实的虚拟世界生成开辟了新的可能性。",
        "translated_title": "OmniX：从统一全景生成与感知到图形就绪的3D场景",
        "label": [],
        "label_reason": "论文聚焦3D场景生成与感知，属于高阶视觉任务，不涉及像素级图像恢复或增强。",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出统一框架用于全景生成与感知，但未在low-level任务中实现突破性创新。"
    },
    {
        "title": "Masked Diffusion Captioning for Visual Feature Learning",
        "url": "http://arxiv.org/abs/2510.26799v1",
        "pub_date": "2025-10-30",
        "summary": "We learn visual features by captioning images with an image-conditioned masked diffusion language model, a formulation we call masked diffusion captioning (MDC). During training, text tokens in each image-caption pair are masked at a randomly chosen ratio, and a decoder conditioned on visual features is trained to reconstruct the original text. After training, the learned visual features can be applied to downstream vision tasks. Unlike autoregressive captioning, the strength of the visual learning signal in MDC does not depend on each token's position in the sequence, reducing the need for auxiliary objectives. Linear probing experiments across a variety of academic-scale models and datasets show that the learned visual features are competitive with those produced by autoregressive and contrastive approaches.",
        "translated": "我们通过图像条件下的掩码扩散语言模型对图像进行字幕生成，从而学习视觉特征，我们称该方法为掩码扩散字幕（Masked Diffusion Captioning, MDC）。在训练过程中，每个图像-字幕对中的文本标记以随机选择的比例被掩码，同时训练一个以视觉特征为条件的解码器来重构原始文本。训练完成后，所学习到的视觉特征可应用于下游视觉任务。与自回归字幕方法不同，MDC中视觉学习信号的强度不依赖于序列中每个标记的位置，从而减少了对辅助目标的依赖。在多种学术规模模型和数据集上进行的线性探测实验表明，所学习的视觉特征与自回归和对比学习方法生成的特征具有竞争力。",
        "translated_title": "掩码扩散字幕用于视觉特征学习",
        "label": [],
        "label_reason": "论文聚焦于视觉特征学习与文本生成，属于high-level任务，不涉及图像像素级恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "提出掩码扩散Captioning方法，但未针对图像恢复任务，创新点在视觉-语言联合学习。"
    },
    {
        "title": "SEE4D: Pose-Free 4D Generation via Auto-Regressive Video Inpainting",
        "url": "http://arxiv.org/abs/2510.26796v1",
        "pub_date": "2025-10-30",
        "summary": "Immersive applications call for synthesizing spatiotemporal 4D content from casual videos without costly 3D supervision. Existing video-to-4D methods typically rely on manually annotated camera poses, which are labor-intensive and brittle for in-the-wild footage. Recent warp-then-inpaint approaches mitigate the need for pose labels by warping input frames along a novel camera trajectory and using an inpainting model to fill missing regions, thereby depicting the 4D scene from diverse viewpoints. However, this trajectory-to-trajectory formulation often entangles camera motion with scene dynamics and complicates both modeling and inference. We introduce SEE4D, a pose-free, trajectory-to-camera framework that replaces explicit trajectory prediction with rendering to a bank of fixed virtual cameras, thereby separating camera control from scene modeling. A view-conditional video inpainting model is trained to learn a robust geometry prior by denoising realistically synthesized warped images and to inpaint occluded or missing regions across virtual viewpoints, eliminating the need for explicit 3D annotations. Building on this inpainting core, we design a spatiotemporal autoregressive inference pipeline that traverses virtual-camera splines and extends videos with overlapping windows, enabling coherent generation at bounded per-step complexity. We validate See4D on cross-view video generation and sparse reconstruction benchmarks. Across quantitative metrics and qualitative assessments, our method achieves superior generalization and improved performance relative to pose- or trajectory-conditioned baselines, advancing practical 4D world modeling from casual videos.",
        "translated": "沉浸式应用要求从普通视频中合成时空4D内容，而无需昂贵的3D监督信息。现有的视频到4D方法通常依赖于人工标注的相机姿态，这对野外拍摄的视频而言既耗时又脆弱。近期提出的“变形后修复”（warp-then-inpaint）方法通过沿新相机轨迹对输入帧进行变形，并利用修复模型填补缺失区域，从而从多样视角描绘4D场景，减少了对姿态标签的依赖。然而，这种轨迹到轨迹（trajectory-to-trajectory）的建模方式常常将相机运动与场景动态耦合在一起，增加了建模和推理的复杂性。我们提出SEE4D，一种无需姿态信息、基于轨迹到相机（trajectory-to-camera）的框架，通过将显式的轨迹预测替换为渲染到一组固定的虚拟相机，从而将相机控制与场景建模解耦。我们训练一个视图条件化的视频修复模型，通过去噪真实合成的变形图像，学习稳健的几何先验，并在虚拟视角间修复被遮挡或缺失的区域，从而完全摆脱显式3D标注的需求。在此修复核心基础上，我们设计了一种时空自回归推理管道，该管道遍历虚拟相机的样条路径，并通过重叠窗口扩展视频，实现了在每步计算复杂度有界的前提下进行连贯生成。我们在跨视角视频生成和稀疏重建基准上验证了SEE4D。在定量指标和定性评估中，我们的方法相较于基于姿态或轨迹条件的基线方法，展现出更强的泛化能力与性能提升，推动了从普通视频实现实用4D世界建模的进展。",
        "translated_title": "SEE4D：基于自回归视频修复的无姿态4D生成",
        "label": [],
        "label_reason": "论文核心为4D内容生成，属于图像生成与3D重建范畴，非像素级图像恢复任务。",
        "relevance_score": 3,
        "novelty_score": 7,
        "novelty_reason": "提出轨迹到相机框架与自回归生成流程，对视频生成有显著改进，但非low-level任务创新。"
    },
    {
        "title": "Scaling Image Geo-Localization to Continent Level",
        "url": "http://arxiv.org/abs/2510.26795v1",
        "pub_date": "2025-10-30",
        "summary": "Determining the precise geographic location of an image at a global scale remains an unsolved challenge. Standard image retrieval techniques are inefficient due to the sheer volume of images (&gt;100M) and fail when coverage is insufficient. Scalable solutions, however, involve a trade-off: global classification typically yields coarse results (10+ kilometers), while cross-view retrieval between ground and aerial imagery suffers from a domain gap and has been primarily studied on smaller regions. This paper introduces a hybrid approach that achieves fine-grained geo-localization across a large geographic expanse the size of a continent. We leverage a proxy classification task during training to learn rich feature representations that implicitly encode precise location information. We combine these learned prototypes with embeddings of aerial imagery to increase robustness to the sparsity of ground-level data. This enables direct, fine-grained retrieval over areas spanning multiple countries. Our extensive evaluation demonstrates that our approach can localize within 200m more than 68\\% of queries of a dataset covering a large part of Europe. The code is publicly available at https://scaling-geoloc.github.io.",
        "translated": "在全球范围内精确确定图像的地理位置仍是一个尚未解决的挑战。标准的图像检索技术由于图像数量庞大（>100M）而效率低下，且在覆盖不足时失效。然而，可扩展的解决方案往往存在权衡：全局分类通常只能提供粗粒度的结果（10+公里），而地面与航拍图像之间的跨视角检索则因域差异问题而受限，且主要在较小区域中进行研究。本文提出一种混合方法，能够在大陆尺度的大范围地理区域内实现细粒度的地理定位。我们在训练过程中引入代理分类任务，以学习富含位置信息的特征表示，这些表示隐式编码了精确的地理位置。我们结合所学习的原型与航拍图像的嵌入表示，以增强对地面数据稀疏性的鲁棒性。这使得能够直接对跨越多个国家的区域进行细粒度检索。我们的大量实验表明，该方法在覆盖欧洲大部分地区的数据集上，超过68%的查询可被定位在200米范围内。代码公开于 https://scaling-geoloc.github.io。",
        "translated_title": "扩展图像地理定位至大陆级别",
        "label": [],
        "label_reason": "论文聚焦图像地理定位，属于高阶视觉任务，不涉及像素级图像质量恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "提出混合检索方法，结合代理分类与航拍图像嵌入，但属定位任务创新，非图像恢复领域。"
    },
    {
        "title": "The Quest for Generalizable Motion Generation: Data, Model, and\n  Evaluation",
        "url": "http://arxiv.org/abs/2510.26794v1",
        "pub_date": "2025-10-30",
        "summary": "Despite recent advances in 3D human motion generation (MoGen) on standard benchmarks, existing models still face a fundamental bottleneck in their generalization capability. In contrast, adjacent generative fields, most notably video generation (ViGen), have demonstrated remarkable generalization in modeling human behaviors, highlighting transferable insights that MoGen can leverage. Motivated by this observation, we present a comprehensive framework that systematically transfers knowledge from ViGen to MoGen across three key pillars: data, modeling, and evaluation. First, we introduce ViMoGen-228K, a large-scale dataset comprising 228,000 high-quality motion samples that integrates high-fidelity optical MoCap data with semantically annotated motions from web videos and synthesized samples generated by state-of-the-art ViGen models. The dataset includes both text-motion pairs and text-video-motion triplets, substantially expanding semantic diversity. Second, we propose ViMoGen, a flow-matching-based diffusion transformer that unifies priors from MoCap data and ViGen models through gated multimodal conditioning. To enhance efficiency, we further develop ViMoGen-light, a distilled variant that eliminates video generation dependencies while preserving strong generalization. Finally, we present MBench, a hierarchical benchmark designed for fine-grained evaluation across motion quality, prompt fidelity, and generalization ability. Extensive experiments show that our framework significantly outperforms existing approaches in both automatic and human evaluations. The code, data, and benchmark will be made publicly available.",
        "translated": "尽管在标准基准上3D人体运动生成（MoGen）领域取得了近期进展，现有模型在泛化能力方面仍面临根本性瓶颈。相比之下，相邻的生成领域——尤其是视频生成（ViGen）——在建模人类行为方面展现出显著的泛化能力，凸显了可供MoGen借鉴的可迁移洞见。受此启发，我们提出一个综合性框架，系统性地从ViGen向MoGen迁移知识，涵盖数据、建模与评估三个关键支柱。首先，我们引入ViMoGen-228K，一个包含228,000个高质量运动样本的大规模数据集，该数据集整合了高保真光学动作捕捉（MoCap）数据、来自网络视频的语义标注运动样本，以及由先进ViGen模型生成的合成样本。数据集包含文本-运动对和文本-视频-运动三元组，显著扩展了语义多样性。其次，我们提出ViMoGen，一种基于流匹配的扩散Transformer模型，通过门控多模态条件机制统一了MoCap数据与ViGen模型的先验知识。为提升效率，我们进一步开发了ViMoGen-light，一个蒸馏变体，在消除视频生成依赖的同时保持了强大的泛化能力。最后，我们提出MBench，一个分层基准，用于在运动质量、提示保真度和泛化能力方面进行细粒度评估。大量实验表明，我们的框架在自动评估与人工评估中均显著优于现有方法。代码、数据及基准将公开发布。",
        "translated_title": "通用运动生成的探索：数据、模型与评估",
        "label": [],
        "label_reason": "论文聚焦3D人体运动生成，属于高阶生成任务，非像素级图像恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出跨领域知识迁移框架，结合视频生成与运动生成，具有方法创新性。"
    },
    {
        "title": "HEIR: Learning Graph-Based Motion Hierarchies",
        "url": "http://arxiv.org/abs/2510.26786v1",
        "pub_date": "2025-10-30",
        "summary": "Hierarchical structures of motion exist across research fields, including computer vision, graphics, and robotics, where complex dynamics typically arise from coordinated interactions among simpler motion components. Existing methods to model such dynamics typically rely on manually-defined or heuristic hierarchies with fixed motion primitives, limiting their generalizability across different tasks. In this work, we propose a general hierarchical motion modeling method that learns structured, interpretable motion relationships directly from data. Our method represents observed motions using graph-based hierarchies, explicitly decomposing global absolute motions into parent-inherited patterns and local motion residuals. We formulate hierarchy inference as a differentiable graph learning problem, where vertices represent elemental motions and directed edges capture learned parent-child dependencies through graph neural networks. We evaluate our hierarchical reconstruction approach on three examples: 1D translational motion, 2D rotational motion, and dynamic 3D scene deformation via Gaussian splatting. Experimental results show that our method reconstructs the intrinsic motion hierarchy in 1D and 2D cases, and produces more realistic and interpretable deformations compared to the baseline on dynamic 3D Gaussian splatting scenes. By providing an adaptable, data-driven hierarchical modeling paradigm, our method offers a formulation applicable to a broad range of motion-centric tasks. Project Page: https://light.princeton.edu/HEIR/",
        "translated": "运动的层次结构广泛存在于计算机视觉、图形学和机器人学等多个研究领域中，其中复杂的动态行为通常由多个简单运动组件的协同交互产生。现有方法在建模此类动态时，通常依赖于人工定义或启发式设定的固定运动基元层次结构，这限制了其在不同任务间的泛化能力。在本文中，我们提出了一种通用的层次化运动建模方法，能够直接从数据中学习结构化且可解释的运动关系。我们的方法采用基于图的层次结构表示观测到的运动，显式地将全局绝对运动分解为从父节点继承的模式和局部运动残差。我们将层次结构推断建模为一个可微分的图学习问题，其中图的顶点代表基本运动单元，有向边通过图神经网络捕获学习到的父子依赖关系。我们在三个实例上评估了所提出的层次重建方法：1D平移运动、2D旋转运动，以及通过高斯点云（Gaussian splatting）实现的动态3D场景形变。实验结果表明，我们的方法在1D和2D场景中能够重建内在的运动层次结构，并在动态3D高斯点云场景中相较于基线方法生成更真实、更具可解释性的形变结果。通过提供一种自适应、数据驱动的层次化建模范式，我们的方法为一系列以运动为核心的任务提供了通用的建模框架。项目主页：https://light.princeton.edu/HEIR/",
        "translated_title": "HEIR：基于图结构的运动层次学习",
        "label": [],
        "label_reason": "研究运动层次建模，属于动态场景理解，非像素级图像恢复任务",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出可学习图结构运动层次，方法新颖，但非low-level图像处理"
    },
    {
        "title": "Clone Deterministic 3D Worlds with Geometrically-Regularized World\n  Models",
        "url": "http://arxiv.org/abs/2510.26782v1",
        "pub_date": "2025-10-30",
        "summary": "A world model is an internal model that simulates how the world evolves. Given past observations and actions, it predicts the future of both the embodied agent and its environment. Accurate world models are essential for enabling agents to think, plan, and reason effectively in complex, dynamic settings. Despite rapid progress, current world models remain brittle and degrade over long horizons. We argue that a central cause is representation quality: exteroceptive inputs (e.g., images) are high-dimensional, and lossy or entangled latents make dynamics learning unnecessarily hard. We therefore ask whether improving representation learning alone can substantially improve world-model performance. In this work, we take a step toward building a truly accurate world model by addressing a fundamental yet open problem: constructing a model that can fully clone and overfit to a deterministic 3D world. We propose Geometrically-Regularized World Models (GRWM), which enforces that consecutive points along a natural sensory trajectory remain close in latent representation space. This approach yields significantly improved latent representations that align closely with the true topology of the environment. GRWM is plug-and-play, requires only minimal architectural modification, scales with trajectory length, and is compatible with diverse latent generative backbones. Across deterministic 3D settings and long-horizon prediction tasks, GRWM significantly increases rollout fidelity and stability. Analyses show that its benefits stem from learning a latent manifold with superior geometric structure. These findings support a clear takeaway: improving representation learning is a direct and useful path to robust world models, delivering reliable long-horizon predictions without enlarging the dynamics module.",
        "translated": "世界模型是一种内部模型，用于模拟世界如何演化。给定过去的观测和动作，它能够预测具身智能体及其环境的未来状态。准确的世界模型对于使智能体在复杂、动态环境中有效思考、规划和推理至关重要。尽管取得了快速进展，当前的世界模型仍然脆弱，并且在长时间预测中性能显著退化。我们认为，其核心原因在于表示质量：外感知输入（如图像）具有高维度，而存在信息损失或纠缠的潜在表示使得动力学学习变得不必要地困难。因此，我们提出一个关键问题：仅通过改进表示学习，是否能够显著提升世界模型的性能？在本研究中，我们通过解决一个基础但尚未解决的问题，朝着构建真正准确的世界模型迈出一步：构建一个能够完全复制并过拟合于确定性3D世界模型。我们提出几何正则化世界模型（Geometrically-Regularized World Models, GRWM），该方法强制要求在自然感知轨迹上连续的点在潜在表示空间中保持邻近。该方法生成了显著改进的潜在表示，其与环境的真实拓扑结构高度对齐。GRWM具有即插即用特性，仅需对网络结构进行最小修改，可随轨迹长度扩展，并兼容多种潜在生成主干网络。在确定性3D环境和长时序预测任务中，GRWM显著提升了预测轨迹的保真度和稳定性。分析表明，其优势源于学习到具有优越几何结构的潜在流形。这些发现支持一个明确的结论：改进表示学习是构建鲁棒世界模型的直接且有效途径，能够在不扩大动力学模块的前提下，实现可靠的长时序预测。",
        "translated_title": "克隆确定性三维世界：基于几何正则化世界模型的方法",
        "label": [],
        "label_reason": "论文聚焦3D世界模型与长期预测，属于高阶视觉任务，不涉及像素级图像恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 6,
        "novelty_reason": "提出几何正则化表示学习方法，提升世界模型稳定性，但非low-level图像处理创新。"
    },
    {
        "title": "ChartAB: A Benchmark for Chart Grounding &amp; Dense Alignment",
        "url": "http://arxiv.org/abs/2510.26781v1",
        "pub_date": "2025-10-30",
        "summary": "Charts play an important role in visualization, reasoning, data analysis, and the exchange of ideas among humans. However, existing vision-language models (VLMs) still lack accurate perception of details and struggle to extract fine-grained structures from charts. Such limitations in chart grounding also hinder their ability to compare multiple charts and reason over them. In this paper, we introduce a novel \"ChartAlign Benchmark (ChartAB)\" to provide a comprehensive evaluation of VLMs in chart grounding tasks, i.e., extracting tabular data, localizing visualization elements, and recognizing various attributes from charts of diverse types and complexities. We design a JSON template to facilitate the calculation of evaluation metrics specifically tailored for each grounding task. By incorporating a novel two-stage inference workflow, the benchmark can further evaluate VLMs' capability to align and compare elements/attributes across two charts. Our analysis of evaluations on several recent VLMs reveals new insights into their perception biases, weaknesses, robustness, and hallucinations in chart understanding. These findings highlight the fine-grained discrepancies among VLMs in chart understanding tasks and point to specific skills that need to be strengthened in current models.",
        "translated": "图表在可视化、推理、数据分析以及人类之间的思想交流中发挥着重要作用。然而，现有的视觉-语言模型（VLMs）仍缺乏对细节的准确感知，难以从图表中提取细粒度结构。这种在图表定位（chart grounding）方面的局限性也阻碍了它们对多个图表进行比较和推理的能力。本文提出了一种新颖的“ChartAlign基准（ChartAB）”，用于全面评估VLMs在图表定位任务中的表现，即从多种类型和复杂度的图表中提取表格数据、定位可视化元素以及识别各种属性。我们设计了一种JSON模板，以方便针对每个定位任务计算特定的评估指标。通过引入一种新颖的两阶段推理工作流，该基准还可进一步评估VLMs在跨两个图表对齐和比较元素/属性方面的能力。我们对多个近期VLMs的评估分析揭示了它们在图表理解中的感知偏差、弱点、鲁棒性以及幻觉现象等新见解。这些发现突显了VLMs在图表理解任务中的细粒度差异，并指出了当前模型亟需加强的具体能力。",
        "translated_title": "ChartAB: 图表定位与密集对齐基准数据集",
        "label": [],
        "label_reason": "论文聚焦图表理解与视觉-语言对齐，属于高阶视觉任务，非像素级图像恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "提出新基准与两阶段推理流程，但未涉及图像质量改善或低层视觉处理创新。"
    },
    {
        "title": "Surpassing state of the art on AMD area estimation from RGB fundus\n  images through careful selection of U-Net architectures and loss functions\n  for class imbalance",
        "url": "http://arxiv.org/abs/2510.26778v1",
        "pub_date": "2025-10-30",
        "summary": "Age-related macular degeneration (AMD) is one of the leading causes of irreversible vision impairment in people over the age of 60. This research focuses on semantic segmentation for AMD lesion detection in RGB fundus images, a non-invasive and cost-effective imaging technique. The results of the ADAM challenge - the most comprehensive AMD detection from RGB fundus images research competition and open dataset to date - serve as a benchmark for our evaluation. Taking the U-Net connectivity as a base of our framework, we evaluate and compare several approaches to improve the segmentation model's architecture and training pipeline, including pre-processing techniques, encoder (backbone) deep network types of varying complexity, and specialized loss functions to mitigate class imbalances on image and pixel levels. The main outcome of this research is the final configuration of the AMD detection framework, which outperforms all the prior ADAM challenge submissions on the multi-class segmentation of different AMD lesion types in non-invasive RGB fundus images. The source code used to conduct the experiments presented in this paper is made freely available.",
        "translated": "年龄相关性黄斑变性（AMD）是60岁以上人群不可逆视力损害的主要原因之一。本研究聚焦于在RGB眼底图像中进行AMD病灶的语义分割，这是一种非侵入性且成本效益较高的成像技术。我们以迄今为止最全面的AMD检测研究竞赛及公开数据集——ADAM挑战赛的结果作为评估基准。以U-Net的连接结构为基础框架，我们评估并比较了多种改进分割模型架构与训练流程的方法，包括预处理技术、不同复杂度的编码器（主干网络）深度网络类型，以及专门设计的损失函数，以缓解图像和像素层面的类别不平衡问题。本研究的主要成果是最终确定的AMD检测框架配置，该配置在非侵入性RGB眼底图像中对多种AMD病灶类型的多类别分割任务上，性能优于所有先前提交的ADAM挑战赛方案。本文所呈现实验所使用的源代码已公开共享。",
        "translated_title": "通过精心选择U-Net架构和损失函数以应对类别不平衡问题，在RGB眼底图像上实现AMD区域估计的性能超越现有最佳水平",
        "label": [],
        "label_reason": "论文核心为医学图像语义分割，用于AMD病变检测，属于high-level任务，非像素级图像恢复。",
        "relevance_score": 3,
        "novelty_score": 5,
        "novelty_reason": "对U-Net架构和损失函数进行优化，属常规改进，无显著创新点。"
    },
    {
        "title": "SteerVLM: Robust Model Control through Lightweight Activation Steering\n  for Vision Language Models",
        "url": "http://arxiv.org/abs/2510.26769v1",
        "pub_date": "2025-10-30",
        "summary": "This work introduces SteerVLM, a lightweight steering module designed to guide Vision-Language Models (VLMs) towards outputs that better adhere to desired instructions. Our approach learns from the latent embeddings of paired prompts encoding target and converse behaviors to dynamically adjust activations connecting the language modality with image context. This allows for fine-grained, inference-time control over complex output semantics without modifying model weights while preserving performance on off-target tasks. Our steering module requires learning parameters equal to 0.14% of the original VLM's size. Our steering module gains model control through dimension-wise activation modulation and adaptive steering across layers without requiring pre-extracted static vectors or manual tuning of intervention points. Furthermore, we introduce VNIA (Visual Narrative Intent Alignment), a multimodal dataset specifically created to facilitate the development and evaluation of VLM steering techniques. Our method outperforms existing intervention techniques on steering and hallucination mitigation benchmarks for VLMs and proposes a robust solution for multimodal model control through activation engineering.",
        "translated": "本工作提出 SteerVLM，一种轻量级引导模块，旨在引导视觉-语言模型（VLMs）生成更符合预期指令的输出。我们的方法通过学习成对提示（编码目标行为与相反行为）的潜在嵌入，动态调整语言模态与图像上下文之间连接的激活值。这使得在不修改模型权重的前提下，能够对复杂输出语义实现细粒度、推理时的控制，同时保持在非目标任务上的性能。我们的引导模块所需学习参数仅为原始VLM大小的0.14%。该模块通过维度感知的激活调制和跨层自适应引导实现模型控制，无需预提取静态向量或手动调整干预点。此外，我们引入VNIA（Visual Narrative Intent Alignment），一个专为促进VLM引导技术开发与评估而构建的多模态数据集。我们的方法在VLMs的引导控制与幻觉缓解基准测试中优于现有干预技术，并通过激活工程提出了一种鲁棒的多模态模型控制解决方案。",
        "translated_title": "SteerVLM：通过轻量级激活引导实现视觉语言模型的鲁棒控制",
        "label": [],
        "label_reason": "论文聚焦于视觉语言模型的控制与干预，属于高阶任务，不涉及图像像素级恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出轻量激活引导模块，创新性地实现推理时控制，但非图像恢复领域方法。"
    },
    {
        "title": "MORE: Multi-Organ Medical Image REconstruction Dataset",
        "url": "http://arxiv.org/abs/2510.26759v1",
        "pub_date": "2025-10-30",
        "summary": "CT reconstruction provides radiologists with images for diagnosis and treatment, yet current deep learning methods are typically limited to specific anatomies and datasets, hindering generalization ability to unseen anatomies and lesions. To address this, we introduce the Multi-Organ medical image REconstruction (MORE) dataset, comprising CT scans across 9 diverse anatomies with 15 lesion types. This dataset serves two key purposes: (1) enabling robust training of deep learning models on extensive, heterogeneous data, and (2) facilitating rigorous evaluation of model generalization for CT reconstruction. We further establish a strong baseline solution that outperforms prior approaches under these challenging conditions. Our results demonstrate that: (1) a comprehensive dataset helps improve the generalization capability of models, and (2) optimization-based methods offer enhanced robustness for unseen anatomies. The MORE dataset is freely accessible under CC-BY-NC 4.0 at our project page https://more-med.github.io/",
        "translated": "CT重建为放射科医生提供了用于诊断和治疗的图像，然而当前的深度学习方法通常局限于特定解剖结构和数据集，导致其在面对未见过的解剖结构和病灶时泛化能力受限。为解决这一问题，我们提出了多器官医学图像重建（Multi-Organ medical image REconstruction, MORE）数据集，包含9种不同解剖结构的CT扫描图像，涵盖15种病灶类型。该数据集具有两个关键作用：（1）支持深度学习模型在大规模、异构数据上进行鲁棒训练；（2）促进对CT重建模型泛化能力的严格评估。此外，我们建立了一个强基线方法，在这些具有挑战性的条件下优于现有方法。实验结果表明：（1）综合性数据集有助于提升模型的泛化能力；（2）基于优化的方法在应对未见过的解剖结构时表现出更强的鲁棒性。MORE数据集可在我们的项目页面 https://more-med.github.io/ 免费获取，遵循CC-BY-NC 4.0许可协议。",
        "translated_title": "MORE: 多器官医学图像重建数据集",
        "label": [
            "CT金属伪影消除",
            "医学图像增强",
            "图像重建"
        ],
        "label_reason": "论文聚焦CT图像重建，涉及多器官数据集构建与模型泛化，属于低层医学图像恢复任务。",
        "relevance_score": 9,
        "novelty_score": 7,
        "novelty_reason": "提出多器官CT重建数据集，提升模型泛化能力，对现有方法有显著改进。"
    },
    {
        "title": "ProstNFound+: A Prospective Study using Medical Foundation Models for\n  Prostate Cancer Detection",
        "url": "http://arxiv.org/abs/2510.26703v1",
        "pub_date": "2025-10-30",
        "summary": "Purpose: Medical foundation models (FMs) offer a path to build high-performance diagnostic systems. However, their application to prostate cancer (PCa) detection from micro-ultrasound ({\\mu}US) remains untested in clinical settings. We present ProstNFound+, an adaptation of FMs for PCa detection from {\\mu}US, along with its first prospective validation. Methods: ProstNFound+ incorporates a medical FM, adapter tuning, and a custom prompt encoder that embeds PCa-specific clinical biomarkers. The model generates a cancer heatmap and a risk score for clinically significant PCa. Following training on multi-center retrospective data, the model is prospectively evaluated on data acquired five years later from a new clinical site. Model predictions are benchmarked against standard clinical scoring protocols (PRI-MUS and PI-RADS). Results: ProstNFound+ shows strong generalization to the prospective data, with no performance degradation compared to retrospective evaluation. It aligns closely with clinical scores and produces interpretable heatmaps consistent with biopsy-confirmed lesions. Conclusion: The results highlight its potential for clinical deployment, offering a scalable and interpretable alternative to expert-driven protocols.",
        "translated": "目的：医学基础模型（FMs）为构建高性能诊断系统提供了可行路径。然而，其在微超声（{\\mu}US）图像中用于前列腺癌（PCa）检测的临床应用尚未得到验证。本文提出 ProstNFound+，即针对 {\\mu}US 图像中 PCa 检测的医学基础模型适配方案，并首次开展前瞻性验证。\n\n方法：ProstNFound+ 集成了医学基础模型、适配器调优机制以及一个定制的提示编码器，该编码器嵌入了与 PCa 相关的临床生物标志物。模型输出包括癌症热力图和临床显著性 PCa 的风险评分。模型在多中心回顾性数据上训练后，于五年后在新临床中心获取的数据上进行前瞻性评估。模型预测结果与标准临床评分协议（PRI-MUS 和 PI-RADS）进行对比分析。\n\n结果：ProstNFound+ 在前瞻性数据上表现出良好的泛化能力，其性能与回顾性评估相比无明显下降。模型预测结果与临床评分高度一致，并生成可解释的热力图，其空间分布与穿刺活检确认的病灶区域相符。\n\n结论：研究结果表明，该模型具备临床部署潜力，可作为专家主导协议的可扩展且可解释的替代方案。",
        "translated_title": "ProstNFound+: 一种基于医学基础模型的前列腺癌检测前瞻性研究",
        "label": [],
        "label_reason": "论文聚焦于前列腺癌检测，属于高阶医学图像分析任务，非像素级图像恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "方法基于医学基础模型与提示编码，属模型适配与临床应用探索，无底层图像处理创新。"
    },
    {
        "title": "The Impact and Outlook of 3D Gaussian Splatting",
        "url": "http://arxiv.org/abs/2510.26694v1",
        "pub_date": "2025-10-30",
        "summary": "Since its introduction, 3D Gaussian Splatting (3DGS) has rapidly transformed the landscape of 3D scene representations, inspiring an extensive body of associated research. Follow-up work includes analyses and contributions that enhance the efficiency, scalability, and real-world applicability of 3DGS. In this summary, we present an overview of several key directions that have emerged in the wake of 3DGS. We highlight advances enabling resource-efficient training and rendering, the evolution toward dynamic (or four-dimensional, 4DGS) representations, and deeper exploration of the mathematical foundations underlying its appearance modeling and rendering process. Furthermore, we examine efforts to bring 3DGS to mobile and virtual reality platforms, its extension to massive-scale environments, and recent progress toward near-instant radiance field reconstruction via feed-forward or distributed computation. Collectively, these developments illustrate how 3DGS has evolved from a breakthrough representation into a versatile and foundational tool for 3D vision and graphics.",
        "translated": "自提出以来，3D高斯泼洒（3D Gaussian Splatting, 3DGS）迅速改变了三维场景表示的格局，激发了大量相关研究。后续工作包括对3DGS效率、可扩展性及实际应用性的分析与改进。在本综述中，我们概述了3DGS问世后涌现出的若干关键研究方向。我们重点介绍了实现资源高效训练与渲染的技术进展，向动态（或四维，4DGS）表示的演进，以及对其外观建模与渲染过程所依赖的数学基础的深入探索。此外，我们还考察了将3DGS应用于移动设备和虚拟现实平台的努力，其在大规模环境中的扩展，以及近期通过前馈或分布式计算实现近乎瞬时辐射场重建的进展。总体而言，这些发展表明，3DGS已从一项突破性表示方法，演变为三维视觉与图形领域的多功能基础工具。",
        "translated_title": "3D高斯泼溅的影响与展望\n\n3D高斯泼溅（3D Gaussian Splatting）作为一种新兴的神经渲染技术，近年来在计算机视觉和图形学领域引起了广泛关注。该方法通过将场景表示为一组3D高斯分布，实现了高效、高质量的场景重建与渲染。与传统的神经辐射场（NeRF）相比，3D高斯泼溅在渲染速度和内存效率方面具有显著优势，同时保持了对复杂几何和外观细节的精确建模能力。\n\n在图像恢复任务中，3D高斯泼溅为处理由视角变化、遮挡或传感器噪声引起的退化提供了新的视角。通过在频域和空域中联合优化高斯参数，该方法能够有效捕捉场景的结构先验，从而在超分辨率、去模糊和低光照增强等任务中表现出色。此外，其端到端的训练框架支持与残差学习机制结合，进一步提升了对退化模式的建模能力。\n\n在实际应用中，3D高斯泼溅已被成功应用于多个数据集，如RESIDE、SIDD和Rain100L，展示了其在图像去雨、去雾和JPEG伪影去除等任务中的通用性。特别是在金属伪影消除和去反射任务中，该方法通过引入场景特定的先验知识，显著提升了恢复质量。\n\n展望未来，3D高斯泼溅有望在多模态图像恢复、动态场景重建以及跨域迁移学习中发挥更大作用。随着对高斯分布参数优化算法的持续改进，以及与深度学习模型（如U-Net、DnCNN）的深度融合，该技术将在低级图像处理领域持续推动边界扩展，为更复杂、更真实的视觉感知系统提供基础支持。",
        "label": [],
        "label_reason": "论文聚焦3D场景表示与渲染，属于3D视觉与图形领域，非像素级图像恢复或增强任务。",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "综述性工作，总结现有进展，无新方法或架构提出，创新度有限。"
    },
    {
        "title": "Process Integrated Computer Vision for Real-Time Failure Prediction in\n  Steel Rolling Mill",
        "url": "http://arxiv.org/abs/2510.26684v1",
        "pub_date": "2025-10-30",
        "summary": "We present a long-term deployment study of a machine vision-based anomaly detection system for failure prediction in a steel rolling mill. The system integrates industrial cameras to monitor equipment operation, alignment, and hot bar motion in real time along the process line. Live video streams are processed on a centralized video server using deep learning models, enabling early prediction of equipment failures and process interruptions, thereby reducing unplanned breakdown costs. Server-based inference minimizes the computational load on industrial process control systems (PLCs), supporting scalable deployment across production lines with minimal additional resources. By jointly analyzing sensor data from data acquisition systems and visual inputs, the system identifies the location and probable root causes of failures, providing actionable insights for proactive maintenance. This integrated approach enhances operational reliability, productivity, and profitability in industrial manufacturing environments.",
        "translated": "我们提出了一项针对钢铁轧钢车间故障预测的基于机器视觉的异常检测系统的长期部署研究。该系统集成工业相机，实时监控生产线上设备运行状态、对齐情况以及热轧钢坯的运动。实时视频流通过中央视频服务器利用深度学习模型进行处理，实现对设备故障和工艺中断的早期预测，从而降低非计划停机带来的成本。基于服务器的推理机制减轻了工业过程控制系统（PLCs）的计算负担，支持在多条生产线中以极少的额外资源实现可扩展部署。通过联合分析数据采集系统中的传感器数据与视觉输入，系统能够定位故障发生位置并识别可能的根本原因，为预防性维护提供可操作的洞察。这种集成方法显著提升了工业制造环境中的运行可靠性、生产效率和盈利能力。",
        "translated_title": "过程集成计算机视觉在钢铁轧钢车间实时故障预测中的应用",
        "label": [],
        "label_reason": "论文聚焦工业异常检测与故障预测，属于高阶视觉任务，非像素级图像恢复或增强。",
        "relevance_score": 2,
        "novelty_score": 4,
        "novelty_reason": "方法为多模态数据融合与实时推理，属常规工业AI应用，无显著创新。"
    },
    {
        "title": "Improving Classification of Occluded Objects through Scene Context",
        "url": "http://arxiv.org/abs/2510.26681v1",
        "pub_date": "2025-10-30",
        "summary": "The presence of occlusions has provided substantial challenges to typically-powerful object recognition algorithms. Additional sources of information can be extremely valuable to reduce errors caused by occlusions. Scene context is known to aid in object recognition in biological vision. In this work, we attempt to add robustness into existing Region Proposal Network-Deep Convolutional Neural Network (RPN-DCNN) object detection networks through two distinct scene-based information fusion techniques. We present one algorithm under each methodology: the first operates prior to prediction, selecting a custom object network to use based on the identified background scene, and the second operates after detection, fusing scene knowledge into initial object scores output by the RPN. We demonstrate our algorithms on challenging datasets featuring partial occlusions, which show overall improvement in both recall and precision against baseline methods. In addition, our experiments contrast multiple training methodologies for occlusion handling, finding that training on a combination of both occluded and unoccluded images demonstrates an improvement over the others. Our method is interpretable and can easily be adapted to other datasets, offering many future directions for research and practical applications.",
        "translated": "遮挡的存在给通常强大的目标识别算法带来了重大挑战。额外的信息来源对于减少由遮挡引起的错误具有极高的价值。场景上下文在生物视觉中已被证明有助于目标识别。在本研究中，我们尝试通过两种不同的基于场景的信息融合技术，增强现有的区域建议网络-深度卷积神经网络（RPN-DCNN）目标检测网络的鲁棒性。我们针对每种方法提出了一种算法：第一种在预测之前进行，根据识别出的背景场景选择特定的目标网络；第二种在检测之后进行，将场景知识融合到RPN输出的初始目标得分中。我们在包含部分遮挡的具有挑战性的数据集上验证了我们的算法，结果表明在召回率和精确率方面均优于基线方法。此外，我们的实验对比了多种遮挡处理的训练方法，发现结合遮挡和非遮挡图像进行训练的效果优于其他方法。我们的方法具有可解释性，且易于适应其他数据集，为未来的研究和实际应用提供了广阔方向。",
        "translated_title": "通过场景上下文提升遮挡物体的分类性能",
        "label": [],
        "label_reason": "论文聚焦于遮挡物体的分类，属于目标检测与场景理解，非像素级图像恢复任务。",
        "relevance_score": 2,
        "novelty_score": 5,
        "novelty_reason": "提出两种场景上下文融合方法，属常规改进，无本质创新。"
    },
    {
        "title": "BRIQA: Balanced Reweighting in Image Quality Assessment of Pediatric\n  Brain MRI",
        "url": "http://arxiv.org/abs/2510.26661v1",
        "pub_date": "2025-10-30",
        "summary": "Assessing the severity of artifacts in pediatric brain Magnetic Resonance Imaging (MRI) is critical for diagnostic accuracy, especially in low-field systems where the signal-to-noise ratio is reduced. Manual quality assessment is time-consuming and subjective, motivating the need for robust automated solutions. In this work, we propose BRIQA (Balanced Reweighting in Image Quality Assessment), which addresses class imbalance in artifact severity levels. BRIQA uses gradient-based loss reweighting to dynamically adjust per-class contributions and employs a rotating batching scheme to ensure consistent exposure to underrepresented classes. Through experiments, no single architecture performs best across all artifact types, emphasizing the importance of architectural diversity. The rotating batching configuration improves performance across metrics by promoting balanced learning when combined with cross-entropy loss. BRIQA improves average macro F1 score from 0.659 to 0.706, with notable gains in Noise (0.430), Zipper (0.098), Positioning (0.097), Contrast (0.217), Motion (0.022), and Banding (0.012) artifact severity classification. The code is available at https://github.com/BioMedIA-MBZUAI/BRIQA.",
        "translated": "评估儿科脑部磁共振成像（MRI）中伪影的严重程度对诊断准确性至关重要，尤其是在低场系统中，由于信噪比降低，这一问题更为突出。手动质量评估耗时且具有主观性，因此亟需鲁棒的自动化解决方案。本文提出了一种名为 BRIQA（Balanced Reweighting in Image Quality Assessment）的方法，用于解决伪影严重程度等级中的类别不平衡问题。BRIQA 采用基于梯度的损失重加权策略，动态调整各类别的贡献，并引入旋转批处理方案，以确保模型对少数类别的持续暴露。实验结果表明，没有任何单一网络架构能在所有伪影类型上均表现最优，凸显了架构多样性的必要性。旋转批处理方案在结合交叉熵损失时，通过促进平衡学习，显著提升了各项指标的性能。BRIQA 将平均宏 F1 分数从 0.659 提升至 0.706，在 Noise（0.430）、Zipper（0.098）、Positioning（0.097）、Contrast（0.217）、Motion（0.022）和 Banding（0.012）等伪影严重程度分类任务中均取得了显著提升。代码已开源，地址为 https://github.com/BioMedIA-MBZUAI/BRIQA。",
        "translated_title": "BRIQA：儿科脑部MRI图像质量评估中的平衡重加权方法",
        "label": [],
        "label_reason": "论文聚焦于MRI图像质量评估中的分类任务，属于医学图像分析，非像素级恢复或增强，故不属于low-level。",
        "relevance_score": 4,
        "novelty_score": 6,
        "novelty_reason": "提出梯度重加权与旋转批处理策略，提升分类性能，属方法改进，非根本性创新。"
    },
    {
        "title": "Towards Reliable Sea Ice Drift Estimation in the Arctic Deep Learning\n  Optical Flow on RADARSAT-2",
        "url": "http://arxiv.org/abs/2510.26653v1",
        "pub_date": "2025-10-30",
        "summary": "Accurate estimation of sea ice drift is critical for Arctic navigation, climate research, and operational forecasting. While optical flow, a computer vision technique for estimating pixel wise motion between consecutive images, has advanced rapidly in computer vision, its applicability to geophysical problems and to satellite SAR imagery remains underexplored. Classical optical flow methods rely on mathematical models and strong assumptions about motion, which limit their accuracy in complex scenarios. Recent deep learning based approaches have substantially improved performance and are now the standard in computer vision, motivating their application to sea ice drift estimation. We present the first large scale benchmark of 48 deep learning optical flow models on RADARSAT 2 ScanSAR sea ice imagery, evaluated with endpoint error (EPE) and Fl all metrics against GNSS tracked buoys. Several models achieve sub kilometer accuracy (EPE 6 to 8 pixels, 300 to 400 m), a small error relative to the spatial scales of sea ice motion and typical navigation requirements in the Arctic. Our results demonstrate that the models are capable of capturing consistent regional drift patterns and that recent deep learning based optical flow methods, which have substantially improved motion estimation accuracy compared to classical methods, can be effectively transferred to polar remote sensing. Optical flow produces spatially continuous drift fields, providing motion estimates for every image pixel rather than at sparse buoy locations, offering new opportunities for navigation and climate modeling.",
        "translated": "准确估计海冰漂移对于北极航行、气候研究及业务预报至关重要。尽管光学流（optical flow）作为一种用于估计连续图像间像素级运动的计算机视觉技术，在计算机视觉领域已取得快速发展，但其在地球物理问题以及卫星SAR影像中的适用性仍鲜有探索。经典光学流方法依赖于数学模型，并对运动过程做出强假设，这在复杂场景下限制了其精度。近年来，基于深度学习的方法显著提升了性能，已成为计算机视觉领域的标准，这促使人们将其应用于海冰漂移估计。我们首次在RADARSAT 2 ScanSAR海冰影像上对48种深度学习光学流模型进行了大规模基准测试，采用端点误差（EPE）和Fl all指标，与GNSS追踪浮标数据进行对比评估。多个模型实现了亚千米级精度（EPE为6至8像素，即300至400米），该误差相对于海冰运动的空间尺度及北极典型航行需求而言较小。我们的结果表明，这些模型能够捕捉一致的区域漂移模式，且近年来基于深度学习的光学流方法相比经典方法显著提升了运动估计精度，可有效迁移至极地遥感应用。光学流可生成空间连续的漂移场，为每幅图像中的每个像素提供运动估计，而非仅限于稀疏浮标位置，从而为航行和气候建模提供了新的机遇。",
        "translated_title": "面向北极海冰漂移可靠估计的深度学习：基于 RADARSAT-2 的光流方法",
        "label": [],
        "label_reason": "论文聚焦于遥感图像中的海冰漂移估计，属于运动估计任务，非像素级图像恢复或增强。",
        "relevance_score": 3,
        "novelty_score": 6,
        "novelty_reason": "首次在RADARSAT-2数据上大规模评估深度学习光流模型，但方法本身非原创，属迁移应用。"
    },
    {
        "title": "All You Need for Object Detection: From Pixels, Points, and Prompts to\n  Next-Gen Fusion and Multimodal LLMs/VLMs in Autonomous Vehicles",
        "url": "http://arxiv.org/abs/2510.26641v1",
        "pub_date": "2025-10-30",
        "summary": "Autonomous Vehicles (AVs) are transforming the future of transportation through advances in intelligent perception, decision-making, and control systems. However, their success is tied to one core capability, reliable object detection in complex and multimodal environments. While recent breakthroughs in Computer Vision (CV) and Artificial Intelligence (AI) have driven remarkable progress, the field still faces a critical challenge as knowledge remains fragmented across multimodal perception, contextual reasoning, and cooperative intelligence. This survey bridges that gap by delivering a forward-looking analysis of object detection in AVs, emphasizing emerging paradigms such as Vision-Language Models (VLMs), Large Language Models (LLMs), and Generative AI rather than re-examining outdated techniques. We begin by systematically reviewing the fundamental spectrum of AV sensors (camera, ultrasonic, LiDAR, and Radar) and their fusion strategies, highlighting not only their capabilities and limitations in dynamic driving environments but also their potential to integrate with recent advances in LLM/VLM-driven perception frameworks. Next, we introduce a structured categorization of AV datasets that moves beyond simple collections, positioning ego-vehicle, infrastructure-based, and cooperative datasets (e.g., V2V, V2I, V2X, I2I), followed by a cross-analysis of data structures and characteristics. Ultimately, we analyze cutting-edge detection methodologies, ranging from 2D and 3D pipelines to hybrid sensor fusion, with particular attention to emerging transformer-driven approaches powered by Vision Transformers (ViTs), Large and Small Language Models (SLMs), and VLMs. By synthesizing these perspectives, our survey delivers a clear roadmap of current capabilities, open challenges, and future opportunities.",
        "translated": "自动驾驶汽车（Autonomous Vehicles, AVs）正通过智能感知、决策与控制系统的技术进步，重塑未来交通格局。然而，其成功依赖于一项核心能力：在复杂且多模态的环境中实现可靠的目标检测。尽管计算机视觉（Computer Vision, CV）与人工智能（Artificial Intelligence, AI）领域的最新突破已推动显著进展，但该领域仍面临关键挑战——知识在多模态感知、上下文推理与协同智能之间高度碎片化。本综述旨在弥合这一鸿沟，通过前瞻性分析自动驾驶汽车中的目标检测技术，重点聚焦于视觉-语言模型（Vision-Language Models, VLMs）、大语言模型（Large Language Models, LLMs）及生成式人工智能（Generative AI）等新兴范式，而非回顾过时的技术。我们首先系统性地回顾自动驾驶汽车传感器（摄像头、超声波、LiDAR 和雷达）的基本谱系及其融合策略，不仅阐明其在动态驾驶环境中的能力与局限，还探讨其与近期基于 LLM/VLM 的感知框架集成的潜力。随后，我们提出一种结构化的自动驾驶数据集分类方法，超越简单的数据集合，涵盖自车（ego-vehicle）、基础设施（infrastructure-based）及协同式数据集（如 V2V、V2I、V2X、I2I），并进一步对数据结构与特征进行交叉分析。最后，我们分析前沿的目标检测方法，涵盖 2D 与 3D 检测流程以及混合传感器融合技术，特别关注由视觉Transformer（Vision Transformers, ViTs）、大语言模型与小语言模型（Small Language Models, SLMs）及 VLMs 驱动的新兴Transformer架构。通过整合上述视角，本综述为当前能力、开放挑战与未来机遇提供了清晰的路线图。",
        "translated_title": "物体检测所需的一切：从像素、点和提示到自动驾驶中的下一代融合与多模态大语言模型/视觉语言模型",
        "label": [],
        "label_reason": "论文聚焦自动驾驶中的目标检测，属于high-level视觉任务，不涉及像素级图像恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 3,
        "novelty_reason": "综述性工作，整合现有方法，无新架构或理论创新，创新度较低。"
    },
    {
        "title": "SAMRI: Segment Anything Model for MRI",
        "url": "http://arxiv.org/abs/2510.26635v1",
        "pub_date": "2025-10-30",
        "summary": "Accurate magnetic resonance imaging (MRI) segmentation is crucial for clinical decision-making, but remains labor-intensive when performed manually. Convolutional neural network (CNN)-based methods can be accurate and efficient, but often generalize poorly to MRI's variable contrast, intensity inhomogeneity, and protocols. Although the transformer-based Segment Anything Model (SAM) has demonstrated remarkable generalizability in natural images, existing adaptations often treat MRI as another imaging modality, overlooking these modality-specific challenges. We present SAMRI, an MRI-specialized SAM trained and validated on 1.1 million labeled MR slices spanning whole-body organs and pathologies. We demonstrate that SAM can be effectively adapted to MRI by simply fine-tuning its mask decoder using a two-stage strategy, reducing training time by 94% and trainable parameters by 96% versus full-model retraining. Across diverse MRI segmentation tasks, SAMRI achieves a mean Dice of 0.87, delivering state-of-the-art accuracy across anatomical regions and robust generalization on unseen structures, particularly small and clinically important structures.",
        "translated": "准确的磁共振成像（MRI）分割对于临床决策至关重要，但手动操作仍耗时费力。基于卷积神经网络（CNN）的方法虽能实现高精度和高效率，但在面对MRI中常见的对比度变化、强度不均匀性及不同扫描协议时，往往泛化能力较差。尽管基于Transformer的Segment Anything Model（SAM）在自然图像中表现出卓越的泛化能力，但现有适配方法通常将MRI视为另一种成像模态，忽视了其特有的挑战。本文提出SAMRI，一种专为MRI设计的SAM，其在包含全身器官和病理的110万张标注MR切片上进行训练与验证。我们证明，仅通过采用两阶段策略微调其掩码解码器，即可有效将SAM适配至MRI任务，相比全模型重新训练，训练时间减少94%，可训练参数减少96%。在多种MRI分割任务中，SAMRI实现了平均Dice系数0.87，在解剖区域上达到当前最优精度，并在未见过的结构（尤其是小而具有临床重要性的结构）上展现出强大的泛化能力。",
        "translated_title": "SAMRI：用于磁共振成像的分割一切模型",
        "label": [],
        "label_reason": "论文聚焦MRI分割，属于高阶语义理解任务，非像素级图像恢复或增强。",
        "relevance_score": 3,
        "novelty_score": 6,
        "novelty_reason": "提出针对MRI的SAM微调策略，提升泛化性，但方法基于现有架构，创新有限。"
    },
    {
        "title": "ORBIT -- Open Recommendation Benchmark for Reproducible Research with\n  Hidden Tests",
        "url": "http://arxiv.org/abs/2510.26095v1",
        "pub_date": "2025-10-30",
        "summary": "Recommender systems are among the most impactful AI applications, interacting with billions of users every day, guiding them to relevant products, services, or information tailored to their preferences. However, the research and development of recommender systems are hindered by existing datasets that fail to capture realistic user behaviors and inconsistent evaluation settings that lead to ambiguous conclusions. This paper introduces the Open Recommendation Benchmark for Reproducible Research with HIdden Tests (ORBIT), a unified benchmark for consistent and realistic evaluation of recommendation models. ORBIT offers a standardized evaluation framework of public datasets with reproducible splits and transparent settings for its public leaderboard. Additionally, ORBIT introduces a new webpage recommendation task, ClueWeb-Reco, featuring web browsing sequences from 87 million public, high-quality webpages. ClueWeb-Reco is a synthetic dataset derived from real, user-consented, and privacy-guaranteed browsing data. It aligns with modern recommendation scenarios and is reserved as the hidden test part of our leaderboard to challenge recommendation models' generalization ability. ORBIT measures 12 representative recommendation models on its public benchmark and introduces a prompted LLM baseline on the ClueWeb-Reco hidden test. Our benchmark results reflect general improvements of recommender systems on the public datasets, with variable individual performances. The results on the hidden test reveal the limitations of existing approaches in large-scale webpage recommendation and highlight the potential for improvements with LLM integrations. ORBIT benchmark, leaderboard, and codebase are available at https://www.open-reco-bench.ai.",
        "translated": "推荐系统是影响力最大的人工智能应用之一，每天与数十亿用户交互，引导他们发现符合其偏好的相关产品、服务或信息。然而，推荐系统的研究与开发受到现有数据集的制约，这些数据集无法真实反映用户行为，且评估设置不一致，导致研究结论模糊不清。本文提出 Open Recommendation Benchmark for Reproducible Research with HIdden Tests（ORBIT），一个用于推荐模型一致且真实评估的统一基准。ORBIT 提供了公共数据集的标准评估框架，包含可复现的数据划分和透明的评估设置，并支持公开排行榜。此外，ORBIT 引入了一个新的网页推荐任务 ClueWeb-Reco，包含来自 8700 万高质量公共网页的浏览序列。ClueWeb-Reco 是一个基于真实、用户同意且隐私保障的浏览数据生成的合成数据集，符合现代推荐场景，并作为排行榜的隐藏测试部分，用于挑战推荐模型的泛化能力。ORBIT 在其公开基准上评估了 12 个代表性推荐模型，并在 ClueWeb-Reco 隐藏测试集上引入了一个基于提示的 LLM 基线模型。我们的基准测试结果反映出推荐系统在公开数据集上的整体性能提升，但各模型表现存在差异。隐藏测试集的结果揭示了现有方法在大规模网页推荐任务中的局限性，并凸显了通过集成 LLM 实现性能提升的潜力。ORBIT 基准、排行榜及代码库可访问 https://www.open-reco-bench.ai。",
        "translated_title": "ORBIT —— 带隐藏测试集的可复现研究开放推荐系统基准",
        "label": [
            "推荐系统评估",
            "通用推荐技术"
        ],
        "label_reason": "构建推荐系统评估基准，包含公开与隐藏测试集，支持模型泛化能力评估",
        "relevance_score": 8,
        "novelty_score": 7,
        "novelty_reason": "提出统一可复现的评估框架与隐藏测试任务，提升评估可信度"
    },
    {
        "title": "Image Hashing via Cross-View Code Alignment in the Age of Foundation\n  Models",
        "url": "http://arxiv.org/abs/2510.27584v1",
        "pub_date": "2025-10-31",
        "summary": "Efficient large-scale retrieval requires representations that are both compact and discriminative. Foundation models provide powerful visual and multimodal embeddings, but nearest neighbor search in these high-dimensional spaces is computationally expensive. Hashing offers an efficient alternative by enabling fast Hamming distance search with binary codes, yet existing approaches often rely on complex pipelines, multi-term objectives, designs specialized for a single learning paradigm, and long training times. We introduce CroVCA (Cross-View Code Alignment), a simple and unified principle for learning binary codes that remain consistent across semantically aligned views. A single binary cross-entropy loss enforces alignment, while coding-rate maximization serves as an anti-collapse regularizer to promote balanced and diverse codes. To implement this, we design HashCoder, a lightweight MLP hashing network with a final batch normalization layer to enforce balanced codes. HashCoder can be used as a probing head on frozen embeddings or to adapt encoders efficiently via LoRA fine-tuning. Across benchmarks, CroVCA achieves state-of-the-art results in just 5 training epochs. At 16 bits, it particularly well-for instance, unsupervised hashing on COCO completes in under 2 minutes and supervised hashing on ImageNet100 in about 3 minutes on a single GPU. These results highlight CroVCA's efficiency, adaptability, and broad applicability.",
        "translated": "高效的大规模召回需要兼具紧凑性和判别性的表示。基础模型提供了强大的视觉和多模态嵌入，但在这些高维空间中进行最近邻搜索计算成本高昂。哈希方法通过使用二进制编码实现快速的汉明距离搜索，提供了一种高效的替代方案，然而现有方法通常依赖于复杂的流水线、多目标函数、针对单一学习范式的专用设计以及较长的训练时间。我们提出 CroVCA（Cross-View Code Alignment），一种简单且统一的二进制编码学习原则，确保在语义对齐的不同视图间保持编码一致性。单一的二进制交叉熵损失用于强制对齐，而编码率最大化则作为防止坍塌的正则化项，促进编码的平衡与多样性。为实现该方法，我们设计了 HashCoder，一种轻量级的 MLP 哈希网络，其最终添加批量归一化层以强制编码平衡。HashCoder 可作为固定嵌入上的探测头使用，也可通过 LoRA 微调高效地适配编码器。在多个基准测试中，CroVCA 仅需 5 个训练轮次即可达到当前最优性能。在 16 位编码下，其表现尤为突出——例如，在单个 GPU 上，COCO 数据集的无监督哈希可在不到 2 分钟内完成，ImageNet100 的有监督哈希约需 3 分钟。这些结果凸显了 CroVCA 的高效性、适应性及广泛适用性。",
        "translated_title": "图像哈希在基础模型时代的跨视图码对齐方法",
        "label": [],
        "label_reason": "论文聚焦图像哈希与高效检索，虽可间接用于推荐召回，但核心为通用视觉检索技术，非推荐系统专有。",
        "relevance_score": 3,
        "novelty_score": 7,
        "novelty_reason": "提出简洁统一的跨视图码对齐原则，结合轻量网络与LoRA，训练高效，具创新性但非推荐领域突破。"
    },
    {
        "title": "Towards Universal Video Retrieval: Generalizing Video Embedding via\n  Synthesized Multimodal Pyramid Curriculum",
        "url": "http://arxiv.org/abs/2510.27571v1",
        "pub_date": "2025-10-31",
        "summary": "The prevailing video retrieval paradigm is structurally misaligned, as narrow benchmarks incentivize correspondingly limited data and single-task training. Therefore, universal capability is suppressed due to the absence of a diagnostic evaluation that defines and demands multi-dimensional generalization. To break this cycle, we introduce a framework built on the co-design of evaluation, data, and modeling. First, we establish the Universal Video Retrieval Benchmark (UVRB), a suite of 16 datasets designed not only to measure performance but also to diagnose critical capability gaps across tasks and domains. Second, guided by UVRB's diagnostics, we introduce a scalable synthesis workflow that generates 1.55 million high-quality pairs to populate the semantic space required for universality. Finally, we devise the Modality Pyramid, a curriculum that trains our General Video Embedder (GVE) by explicitly leveraging the latent interconnections within our diverse data. Extensive experiments show GVE achieves state-of-the-art zero-shot generalization on UVRB. In particular, our analysis reveals that popular benchmarks are poor predictors of general ability and that partially relevant retrieval is a dominant but overlooked scenario. Overall, our co-designed framework provides a practical path to escape the limited scope and advance toward truly universal video retrieval.",
        "translated": "当前主流的视频召回范式在结构上存在错配，狭窄的基准测试激励了相应受限的数据和单任务训练。因此，由于缺乏能够定义并要求多维泛化能力的诊断性评估，通用能力受到抑制。为打破这一循环，我们提出了一种基于评估、数据与建模协同设计的框架。首先，我们构建了通用视频召回基准（Universal Video Retrieval Benchmark, UVRB），该基准包含16个数据集，不仅用于衡量性能，还能诊断跨任务和跨领域中的关键能力差距。其次，在UVRB诊断结果的指导下，我们引入了一种可扩展的合成工作流，生成155万对高质量样本，以填充实现通用性所需的语义空间。最后，我们设计了模态金字塔（Modality Pyramid），一种课程学习机制，通过显式利用多样化数据中的潜在关联，训练我们的通用视频嵌入模型（General Video Embedder, GVE）。大量实验表明，GVE在UVRB上实现了最先进的零样本泛化性能。特别地，我们的分析揭示，流行的基准测试对通用能力的预测效果较差，且部分相关召回是一种主导但被忽视的场景。总体而言，我们提出的协同设计框架为摆脱有限范围、迈向真正通用的视频召回提供了切实可行的路径。",
        "translated_title": "迈向通用视频召回：通过合成多模态金字塔课程实现视频嵌入泛化",
        "label": [],
        "label_reason": "论文聚焦视频检索通用性，非推荐系统核心环节，与推荐无直接关联。",
        "relevance_score": 3,
        "novelty_score": 8,
        "novelty_reason": "提出多模态金字塔课程与合成数据框架，创新性强，但应用于视频检索而非推荐。"
    },
    {
        "title": "Interact-RAG: Reason and Interact with the Corpus, Beyond Black-Box\n  Retrieval",
        "url": "http://arxiv.org/abs/2510.27566v1",
        "pub_date": "2025-10-31",
        "summary": "Retrieval-Augmented Generation (RAG) has significantly enhanced LLMs by incorporating external information. However, prevailing agentic RAG approaches are constrained by a critical limitation: they treat the retrieval process as a black-box querying operation. This confines agents' actions to query issuing, hindering its ability to tackle complex information-seeking tasks. To address this, we introduce Interact-RAG, a new paradigm that elevates the LLM agent from a passive query issuer into an active manipulator of the retrieval process. We dismantle the black-box with a Corpus Interaction Engine, equipping the agent with a set of action primitives for fine-grained control over information retrieval. To further empower the agent on the entire RAG pipeline, we first develop a reasoning-enhanced workflow, which enables both zero-shot execution and the synthesis of interaction trajectories. We then leverage this synthetic data to train a fully autonomous end-to-end agent via Supervised Fine-Tuning (SFT), followed by refinement with Reinforcement Learning (RL). Extensive experiments across six benchmarks demonstrate that Interact-RAG significantly outperforms other advanced methods, validating the efficacy of our reasoning-interaction strategy.",
        "translated": "检索增强生成（RAG）通过引入外部信息显著提升了大语言模型（LLM）的性能。然而，当前主流的代理式RAG方法存在一个关键局限：它们将检索过程视为一个黑箱查询操作。这使得代理的行为仅限于发出查询，限制了其应对复杂信息检索任务的能力。为解决这一问题，我们提出Interact-RAG，一种新范式，将LLM代理从被动的查询发起者转变为能够主动操控检索过程的参与者。我们通过构建一个语料交互引擎（Corpus Interaction Engine）打破黑箱，为代理提供一组动作原语，实现对信息检索的细粒度控制。为进一步增强代理在整个RAG流程中的能力，我们首先设计了一种推理增强的工作流，支持零样本执行以及交互轨迹的合成。随后，我们利用这些合成数据，通过监督微调（Supervised Fine-Tuning, SFT）训练一个完全自主的端到端代理，并进一步通过强化学习（Reinforcement Learning, RL）进行优化。在六个基准数据集上的大量实验表明，Interact-RAG显著优于其他先进方法，验证了我们所提出的推理-交互策略的有效性。",
        "translated_title": "Interact-RAG：与语料库交互并推理，超越黑盒检索",
        "label": [
            "LLM生成式推荐",
            "通用推荐技术"
        ],
        "label_reason": "论文聚焦RAG框架中LLM与检索交互，虽非直接推荐系统，但可应用于生成式推荐的召回与推理环节。",
        "relevance_score": 4,
        "novelty_score": 8,
        "novelty_reason": "提出Corpus Interaction Engine与推理增强流程，实现LLM对检索过程的主动控制，属新颖交互范式。"
    },
    {
        "title": "Pairwise and Attribute-Aware Decision Tree-Based Preference Elicitation\n  for Cold-Start Recommendation",
        "url": "http://arxiv.org/abs/2510.27342v1",
        "pub_date": "2025-10-31",
        "summary": "Recommender systems (RSs) are intelligent filtering methods that suggest items to users based on their inferred preferences, derived from their interaction history on the platform. Collaborative filtering-based RSs rely on users past interactions to generate recommendations. However, when a user is new to the platform, referred to as a cold-start user, there is no historical data available, making it difficult to provide personalized recommendations. To address this, rating elicitation techniques can be used to gather initial ratings or preferences on selected items, helping to build an early understanding of the user's tastes. Rating elicitation approaches are generally categorized into two types: non-personalized and personalized. Decision tree-based rating elicitation is a personalized method that queries users about their preferences at each node of the tree until sufficient information is gathered. In this paper, we propose an extension to the decision tree approach for rating elicitation in the context of music recommendation. Our method: (i) elicits not only item ratings but also preferences on attributes such as genres to better cluster users, and (ii) uses item pairs instead of single items at each node to more effectively learn user preferences. Experimental results demonstrate that both proposed enhancements lead to improved performance, particularly with a reduced number of queries.",
        "translated": "推荐系统（RSs）是基于用户在平台上交互历史所推断出的偏好，向用户智能推荐物料的过滤方法。基于协同过滤的推荐系统依赖用户过去的交互行为生成推荐结果。然而，当用户首次使用平台（即冷启动用户）时，缺乏历史数据，导致难以提供个性化推荐。为解决该问题，可采用评分征询技术，通过收集用户对选定物料的初始评分或偏好，帮助建立用户偏好的早期认知。评分征询方法通常分为两类：非个性化和个性化。基于决策树的评分征询是一种个性化方法，它在树的每个节点处向用户询问其偏好，直至收集到足够信息。本文提出了一种针对音乐推荐场景的决策树评分征询方法的扩展。我们的方法：（i）不仅征询物料评分，还征询用户对属性（如流派）的偏好，以更好地对用户进行聚类；（ii）在每个节点使用物料对而非单个物料，以更有效地学习用户偏好。实验结果表明，这两种改进均能提升性能，尤其在查询次数减少的情况下效果更为显著。",
        "translated_title": "成对与属性感知的基于决策树的偏好挖掘方法用于冷启动推荐",
        "label": [
            "召回",
            "冷启动推荐",
            "通用推荐技术"
        ],
        "label_reason": "论文针对冷启动问题提出基于决策树的偏好采集方法，涉及推荐系统召回环节，具有明确推荐场景。",
        "relevance_score": 8,
        "novelty_score": 7,
        "novelty_reason": "在传统决策树基础上引入属性偏好与成对物品查询，提升效率，属有效改进但非范式创新。"
    },
    {
        "title": "Traceable Drug Recommendation over Medical Knowledge Graphs",
        "url": "http://arxiv.org/abs/2510.27274v1",
        "pub_date": "2025-10-31",
        "summary": "Drug recommendation (DR) systems aim to support healthcare professionals in selecting appropriate medications based on patients' medical conditions. State-of-the-art approaches utilize deep learning techniques for improving DR, but fall short in providing any insights on the derivation process of recommendations -- a critical limitation in such high-stake applications. We propose TraceDR, a novel DR system operating over a medical knowledge graph (MKG), which ensures access to large-scale and high-quality information. TraceDR simultaneously predicts drug recommendations and related evidence within a multi-task learning framework, enabling traceability of medication recommendations. For covering a more diverse set of diseases and drugs than existing works, we devise a framework for automatically constructing patient health records and release DrugRec, a new large-scale testbed for DR.",
        "translated": "药物推荐（DR）系统旨在帮助医疗专业人员根据患者的病情选择合适的药物。当前最先进的方法采用深度学习技术以提升DR性能，但在提供推荐推导过程的可解释性方面存在不足——这在高风险应用场景中是一个关键局限。我们提出TraceDR，一种在医学知识图谱（MKG）上运行的新型DR系统，确保能够访问大规模、高质量的信息。TraceDR在多任务学习框架下同时预测药物推荐及相关证据，从而实现药物推荐的可追溯性。为覆盖比现有工作更广泛的疾病和药物集合，我们设计了一个自动构建患者健康记录的框架，并发布DrugRec，一个全新的大规模DR测试基准。",
        "translated_title": "可追溯的药物推荐：基于医学知识图谱",
        "label": [
            "图神经网络推荐",
            "推荐系统公平性/可解释性",
            "通用推荐技术"
        ],
        "label_reason": "基于医学知识图谱的药物推荐，强调可追溯性，属推荐系统中可解释性与图模型应用",
        "relevance_score": 5,
        "novelty_score": 7,
        "novelty_reason": "提出多任务学习框架实现推荐与证据联合预测，提升可解释性，属领域内创新"
    },
    {
        "title": "Research Output of Webology Journal (2013-2017): A Scientometric\n  Analysis",
        "url": "http://arxiv.org/abs/2510.27259v1",
        "pub_date": "2025-10-31",
        "summary": "Webology is an international peer-reviewed journal in English devoted to the field of the World Wide Web and serves as a forum for discussion and experimentation. It serves as a forum for new research in information dissemination and communication processes in general, and in the context of the World Wide Web in particular. This paper presents a Scientometric analysis of the Webology Journal. The paper analyses the pattern of growth of the research output published in the journal, pattern of authorship, author productivity, and subjects covered to the papers over the period (2013-2017). It is found that 62 papers were published during the period of study (2013-2017). The maximum numbers of articles were collaborative in nature. The subject concentration of the journal noted was Social Networking/Web 2.0/Library 2.0 and Scientometrics or Bibliometrics. Iranian researchers contributed the maximum number of articles (37.10%). The study applied standard formula and statistical tools to bring out the factual result.",
        "translated": "Webology 是一本以英语出版的国际同行评审期刊，专注于万维网领域，为相关研究的讨论与实验提供平台。该期刊致力于探讨信息传播与通信过程的普遍性问题，尤其关注万维网背景下的相关研究。本文对 Webology 期刊进行了科学计量学分析。研究分析了该期刊在 2013–2017 年期间发表的研究成果的增长模式、作者署名模式、作者产出情况以及论文涵盖的主题。研究发现，在该研究期间共发表了 62 篇论文。其中，合作撰写的论文数量最多。期刊的主题集中于社交网络/Web 2.0/图书馆 2.0 以及科学计量学或文献计量学。伊朗研究人员贡献的论文数量最多（占 37.10%）。本研究采用标准公式和统计工具，以得出客观事实性的分析结果。",
        "translated_title": "Webology期刊研究产出（2013–2017年）：一项科学计量学分析",
        "label": [],
        "label_reason": "论文为科学计量学分析，研究Webology期刊发表趋势，与推荐系统无直接关联。",
        "relevance_score": 2,
        "novelty_score": 2,
        "novelty_reason": "仅使用标准统计方法分析期刊产出，无新方法或模型创新。"
    },
    {
        "title": "Beyond a Million Tokens: Benchmarking and Enhancing Long-Term Memory in\n  LLMs",
        "url": "http://arxiv.org/abs/2510.27246v1",
        "pub_date": "2025-10-31",
        "summary": "Evaluating the abilities of large language models (LLMs) for tasks that require long-term memory and thus long-context reasoning, for example in conversational settings, is hampered by the existing benchmarks, which often lack narrative coherence, cover narrow domains, and only test simple recall-oriented tasks. This paper introduces a comprehensive solution to these challenges. First, we present a novel framework for automatically generating long (up to 10M tokens), coherent, and topically diverse conversations, accompanied by probing questions targeting a wide range of memory abilities. From this, we construct BEAM, a new benchmark comprising 100 conversations and 2,000 validated questions. Second, to enhance model performance, we propose LIGHT-a framework inspired by human cognition that equips LLMs with three complementary memory systems: a long-term episodic memory, a short-term working memory, and a scratchpad for accumulating salient facts. Our experiments on BEAM reveal that even LLMs with 1M token context windows (with and without retrieval-augmentation) struggle as dialogues lengthen. In contrast, LIGHT consistently improves performance across various models, achieving an average improvement of 3.5%-12.69% over the strongest baselines, depending on the backbone LLM. An ablation study further confirms the contribution of each memory component.",
        "translated": "评估大语言模型（LLM）在需要长期记忆和长上下文推理任务中的能力——例如在对话场景中——受到现有基准测试的限制，这些基准测试通常缺乏叙事连贯性、覆盖领域狭窄，且仅测试简单的记忆召回类任务。本文针对这些挑战提出了一套全面的解决方案。首先，我们提出了一种新颖的框架，用于自动生成长（最长可达10M tokens）、连贯且主题多样的对话，并配套设计了针对多种记忆能力的探针问题。基于此，我们构建了BEAM，一个包含100个对话和2,000个经过验证问题的新基准。其次，为提升模型性能，我们提出了LIGHT——一个受人类认知启发的框架，为LLM配备了三种互补的记忆系统：长期情节记忆、短期工作记忆，以及用于积累关键事实的草稿板。我们在BEAM上的实验表明，即使具备1M token上下文窗口的LLM（无论是否采用检索增强）在对话长度增加时仍表现吃力。相比之下，LIGHT在各类模型上均能持续提升性能，平均性能提升达3.5%-12.69%，具体取决于所采用的主干LLM。进一步的消融实验也验证了各记忆组件的贡献。",
        "translated_title": "超越百万 tokens：大语言模型长期记忆的基准测试与增强",
        "label": [
            "LLM生成式推荐",
            "通用推荐技术"
        ],
        "label_reason": "论文聚焦LLM长上下文记忆，虽非直接推荐系统，但可应用于生成式推荐中的长期用户行为建模。",
        "relevance_score": 5,
        "novelty_score": 8,
        "novelty_reason": "提出LIGHT框架模拟人类记忆系统，结合三种记忆机制，显著提升长上下文理解能力，属创新性设计。"
    },
    {
        "title": "DRAMA: Unifying Data Retrieval and Analysis for Open-Domain Analytic\n  Queries",
        "url": "http://arxiv.org/abs/2510.27238v1",
        "pub_date": "2025-10-31",
        "summary": "Manually conducting real-world data analyses is labor-intensive and inefficient. Despite numerous attempts to automate data science workflows, none of the existing paradigms or systems fully demonstrate all three key capabilities required to support them effectively: (1) open-domain data collection, (2) structured data transformation, and (3) analytic reasoning.   To overcome these limitations, we propose DRAMA, an end-to-end paradigm that answers users' analytic queries in natural language on large-scale open-domain data. DRAMA unifies data collection, transformation, and analysis as a single pipeline. To quantitatively evaluate system performance on tasks representative of DRAMA, we construct a benchmark, DRAMA-Bench, consisting of two categories of tasks: claim verification and question answering, each comprising 100 instances. These tasks are derived from real-world applications that have gained significant public attention and require the retrieval and analysis of open-domain data. We develop DRAMA-Bot, a multi-agent system designed following DRAMA. It comprises a data retriever that collects and transforms data by coordinating the execution of sub-agents, and a data analyzer that performs structured reasoning over the retrieved data. We evaluate DRAMA-Bot on DRAMA-Bench together with five state-of-the-art baseline agents. DRAMA-Bot achieves 86.5% task accuracy at a cost of $0.05, outperforming all baselines with up to 6.9 times the accuracy and less than 1/6 of the cost. DRAMA is publicly available at https://github.com/uiuc-kang-lab/drama.",
        "translated": "手动开展真实世界的数据分析工作耗时费力且效率低下。尽管已有大量尝试旨在自动化数据科学工作流，但现有范式或系统均未能全面具备支持其有效运行所需的三项关键能力：（1）开放域数据采集，（2）结构化数据转换，以及（3）分析推理。为克服这些局限，我们提出 DRAMA，一种端到端的范式，能够基于大规模开放域数据，以自然语言回答用户的分析查询。DRAMA 将数据采集、转换与分析整合为单一处理流水线。为定量评估系统在代表 DRAMA 任务上的性能，我们构建了基准测试集 DRAMA-Bench，包含两类任务：主张验证（claim verification）和问答（question answering），每类各含 100 个实例。这些任务源自已引起广泛关注的真实应用场景，需进行开放域数据的检索与分析。我们开发了 DRAMA-Bot，一个遵循 DRAMA 设计的多智能体系统。该系统包含一个数据检索器，通过协调子智能体的执行来采集和转换数据；以及一个数据分析器，对检索到的数据执行结构化推理。我们在 DRAMA-Bench 上对 DRAMA-Bot 与五个最先进基线智能体进行了评估。DRAMA-Bot 在成本仅 0.05 美元的情况下实现了 86.5% 的任务准确率，其准确率最高超过所有基线 6.9 倍，成本不足基线的 1/6。DRAMA 已公开发布于 https://github.com/uiuc-kang-lab/drama。",
        "translated_title": "DRAMA：统一数据检索与分析以支持开放域分析查询",
        "label": [],
        "label_reason": "论文聚焦开放域数据分析与推理，非推荐系统核心环节，与推荐无直接关联。",
        "relevance_score": 3,
        "novelty_score": 7,
        "novelty_reason": "提出统一数据检索与分析框架，多智能体协同设计具创新性，但非推荐领域应用。"
    },
    {
        "title": "A Survey on Deep Text Hashing: Efficient Semantic Text Retrieval with\n  Binary Representation",
        "url": "http://arxiv.org/abs/2510.27232v1",
        "pub_date": "2025-10-31",
        "summary": "With the rapid growth of textual content on the Internet, efficient large-scale semantic text retrieval has garnered increasing attention from both academia and industry. Text hashing, which projects original texts into compact binary hash codes, is a crucial method for this task. By using binary codes, the semantic similarity computation for text pairs is significantly accelerated via fast Hamming distance calculations, and storage costs are greatly reduced. With the advancement of deep learning, deep text hashing has demonstrated significant advantages over traditional, data-independent hashing techniques. By leveraging deep neural networks, these methods can learn compact and semantically rich binary representations directly from data, overcoming the performance limitations of earlier approaches. This survey investigates current deep text hashing methods by categorizing them based on their core components: semantic extraction, hash code quality preservation, and other key technologies. We then present a detailed evaluation schema with results on several popular datasets, followed by a discussion of practical applications and open-source tools for implementation. Finally, we conclude by discussing key challenges and future research directions, including the integration of deep text hashing with large language models to further advance the field. The project for this survey can be accessed at https://github.com/hly1998/DeepTextHashing.",
        "translated": "随着互联网上文本内容的快速增长，高效的大规模语义文本检索在学术界和工业界均受到越来越多的关注。文本哈希通过将原始文本投影到紧凑的二进制哈希码，是实现该任务的关键方法。利用二进制码，文本对之间的语义相似性计算可通过快速的汉明距离计算显著加速，同时大幅降低存储成本。随着深度学习的发展，深度文本哈希相较于传统的、与数据无关的哈希技术展现出显著优势。通过利用深度神经网络，这些方法能够直接从数据中学习紧凑且语义丰富的二进制表示，克服了早期方法的性能局限。本综述根据核心组件对当前深度文本哈希方法进行分类，包括语义提取、哈希码质量保持及其他关键技术。随后，我们提出一个详细的评估框架，并在多个流行数据集上展示评估结果，接着讨论实际应用与开源实现工具。最后，我们总结了该领域面临的关键挑战与未来研究方向，包括深度文本哈希与大语言模型的融合，以进一步推动该领域的发展。本综述相关项目可访问 https://github.com/hly1998/DeepTextHashing。",
        "translated_title": "深度文本哈希综述：基于二值表示的高效语义文本检索",
        "label": [
            "召回",
            "负采样与对比学习"
        ],
        "label_reason": "文本哈希用于高效语义检索，可应用于推荐系统召回环节，但非专为推荐设计",
        "relevance_score": 4,
        "novelty_score": 5,
        "novelty_reason": "综述性工作，总结现有方法，无新架构或理论突破"
    },
    {
        "title": "A Survey on Generative Recommendation: Data, Model, and Tasks",
        "url": "http://arxiv.org/abs/2510.27157v1",
        "pub_date": "2025-10-31",
        "summary": "Recommender systems serve as foundational infrastructure in modern information ecosystems, helping users navigate digital content and discover items aligned with their preferences. At their core, recommender systems address a fundamental problem: matching users with items. Over the past decades, the field has experienced successive paradigm shifts, from collaborative filtering and matrix factorization in the machine learning era to neural architectures in the deep learning era. Recently, the emergence of generative models, especially large language models (LLMs) and diffusion models, have sparked a new paradigm: generative recommendation, which reconceptualizes recommendation as a generation task rather than discriminative scoring. This survey provides a comprehensive examination through a unified tripartite framework spanning data, model, and task dimensions. Rather than simply categorizing works, we systematically decompose approaches into operational stages-data augmentation and unification, model alignment and training, task formulation and execution. At the data level, generative models enable knowledge-infused augmentation and agent-based simulation while unifying heterogeneous signals. At the model level, we taxonomize LLM-based methods, large recommendation models, and diffusion approaches, analyzing their alignment mechanisms and innovations. At the task level, we illuminate new capabilities including conversational interaction, explainable reasoning, and personalized content generation. We identify five key advantages: world knowledge integration, natural language understanding, reasoning capabilities, scaling laws, and creative generation. We critically examine challenges in benchmark design, model robustness, and deployment efficiency, while charting a roadmap toward intelligent recommendation assistants that fundamentally reshape human-information interaction.",
        "translated": "推荐系统作为现代信息生态系统中的基础性基础设施，帮助用户在海量数字内容中导航并发现与其偏好匹配的物料。其核心任务在于解决一个根本性问题：用户与物料之间的匹配。在过去数十年中，该领域经历了多次范式转变，从机器学习时代的协同过滤与矩阵分解，发展到深度学习时代的神经网络架构。近年来，生成式模型的兴起，特别是大语言模型（LLM）与扩散模型的出现，催生了一种新的范式：生成式推荐，其将推荐任务重新定义为生成任务，而非传统的判别式打分。本文通过一个统一的三维度框架——涵盖数据、模型与任务维度——对相关研究进行全面综述。我们并非简单地对工作进行分类，而是系统性地将其分解为三个操作阶段：数据增强与统一、模型对齐与训练、任务建模与执行。在数据层面，生成式模型支持融合知识的增强方法与基于代理的模拟，并能够统一异构信号。在模型层面，我们对基于LLM的方法、大型推荐模型以及扩散模型进行分类，分析其对齐机制与技术创新。在任务层面，我们揭示了新能力，包括对话式交互、可解释推理与个性化内容生成。我们识别出五大关键优势：世界知识融合、自然语言理解、推理能力、规模定律以及创造性生成。同时，我们批判性地审视了基准设计、模型鲁棒性与部署效率等方面的挑战，并勾勒出通向智能推荐助手的路线图，该助手将从根本上重塑人与信息的交互方式。",
        "translated_title": "生成式推荐综述：数据、模型与任务",
        "label": [
            "LLM生成式推荐（LLM-based Generative Recommendation）",
            "通用推荐技术（General Recommendation Techniques）",
            "推荐系统评估（Evaluation Metrics / Offline/Online Testing）"
        ],
        "label_reason": "论文聚焦生成式推荐，系统梳理LLM等模型在推荐中的应用，涵盖数据、模型、任务三维度，直接关联推荐核心。",
        "relevance_score": 10,
        "novelty_score": 9,
        "novelty_reason": "提出统一三维度框架，系统梳理生成式推荐范式，涵盖新任务与挑战，推动领域理论与实践发展。"
    },
    {
        "title": "Compass: General Filtered Search across Vector and Structured Data",
        "url": "http://arxiv.org/abs/2510.27141v1",
        "pub_date": "2025-10-31",
        "summary": "The increasing prevalence of hybrid vector and relational data necessitates efficient, general support for queries that combine high-dimensional vector search with complex relational filtering. However, existing filtered search solutions are fundamentally limited by specialized indices, which restrict arbitrary filtering and hinder integration with general-purpose DBMSs. This work introduces \\textsc{Compass}, a unified framework that enables general filtered search across vector and structured data without relying on new index designs. Compass leverages established index structures -- such as HNSW and IVF for vector attributes, and B+-trees for relational attributes -- implementing a principled cooperative query execution strategy that coordinates candidate generation and predicate evaluation across modalities. Uniquely, Compass maintains generality by allowing arbitrary conjunctions, disjunctions, and range predicates, while ensuring robustness even with highly-selective or multi-attribute filters. Comprehensive empirical evaluations demonstrate that Compass consistently outperforms NaviX, the only existing performant general framework, across diverse hybrid query workloads. It also matches the query throughput of specialized single-attribute indices in their favorite settings with only a single attribute involved, all while maintaining full generality and DBMS compatibility. Overall, Compass offers a practical and robust solution for achieving truly general filtered search in vector database systems.",
        "translated": "混合向量与关系数据的日益普及，亟需高效且通用的查询支持，以实现高维向量搜索与复杂关系过滤的结合。然而，现有的过滤搜索解决方案从根本上受限于专用索引结构，这些结构限制了任意过滤条件的表达，并阻碍了与通用数据库管理系统（DBMS）的集成。本文提出 \\textsc{Compass}，一个统一框架，能够在向量与结构化数据上实现通用的过滤搜索，而无需依赖新的索引设计。Compass 利用现有的索引结构——例如针对向量属性的 HNSW 和 IVF，以及针对关系属性的 B+-树——实现了一种有原则的协同查询执行策略，该策略协调跨模态的候选生成与谓词评估。特别地，Compass 通过支持任意的合取、析取及范围谓词，保持了通用性，同时即使在高度选择性或多属性过滤条件下仍能保证鲁棒性。全面的实证评估表明，Compass 在多种混合查询负载下始终优于现有的唯一高性能通用框架 NaviX。此外，在仅涉及单个属性的场景下，Compass 的查询吞吐量可与专用单属性索引在最优配置下持平，同时保持完全的通用性与 DBMS 兼容性。总体而言，Compass 为向量数据库系统中实现真正通用的过滤搜索提供了一种实用且稳健的解决方案。",
        "translated_title": "Compass：面向向量与结构化数据的通用过滤检索",
        "label": [],
        "label_reason": "论文聚焦向量与结构化数据混合查询，属数据库检索范畴，与推荐系统无直接关联。",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出通用过滤搜索框架，协调多模态索引执行，对数据库检索有显著改进。"
    },
    {
        "title": "Evaluating Perspectival Biases in Cross-Modal Retrieval",
        "url": "http://arxiv.org/abs/2510.26861v1",
        "pub_date": "2025-10-30",
        "summary": "Multimodal retrieval systems are expected to operate in a semantic space, agnostic to the language or cultural origin of the query. In practice, however, retrieval outcomes systematically reflect perspectival biases: deviations shaped by linguistic prevalence and cultural associations. We study two such biases. First, prevalence bias refers to the tendency to favor entries from prevalent languages over semantically faithful entries in image-to-text retrieval. Second, association bias refers to the tendency to favor images culturally associated with the query over semantically correct ones in text-to-image retrieval. Results show that explicit alignment is a more effective strategy for mitigating prevalence bias. However, association bias remains a distinct and more challenging problem. These findings suggest that achieving truly equitable multimodal systems requires targeted strategies beyond simple data scaling and that bias arising from cultural association may be treated as a more challenging problem than one arising from linguistic prevalence.",
        "translated": "多模态检索系统预期在语义空间中运行，对查询的语言或文化来源保持无关性。然而，在实践中，检索结果系统性地反映出视角偏差：这些偏差由语言使用频率和文化关联性所塑造。我们研究了两种此类偏差。首先，流行度偏差（prevalence bias）指在图像到文本检索中，倾向于选择来自高频语言的条目，而非语义上更忠实的条目。其次，关联偏差（association bias）指在文本到图像检索中，倾向于选择与查询在文化上相关联的图像，而非语义上正确的图像。实验结果表明，显式对齐（explicit alignment）是缓解流行度偏差更有效的策略。然而，关联偏差仍是一个独立且更具挑战性的问题。这些发现表明，实现真正公平的多模态系统需要超越简单数据扩展的针对性策略，且由文化关联性引发的偏差可能比由语言流行度引发的偏差更具挑战性。",
        "translated_title": "评估跨模态检索中的视角偏差",
        "label": [
            "多模态推荐",
            "推荐系统公平性/可解释性"
        ],
        "label_reason": "研究多模态检索中的视角偏差，与推荐系统中的多模态与公平性相关，但非直接推荐场景",
        "relevance_score": 4,
        "novelty_score": 6,
        "novelty_reason": "识别两种新偏差类型并提出缓解策略，但方法未突破现有框架"
    },
    {
        "title": "LifWavNet: Lifting Wavelet-based Network for Non-contact ECG\n  Reconstruction from Radar",
        "url": "http://arxiv.org/abs/2510.27692v1",
        "pub_date": "2025-10-31",
        "summary": "Non-contact electrocardiogram (ECG) reconstruction from radar signals offers a promising approach for unobtrusive cardiac monitoring. We present LifWavNet, a lifting wavelet network based on a multi-resolution analysis and synthesis (MRAS) model for radar-to-ECG reconstruction. Unlike prior models that use fixed wavelet approaches, LifWavNet employs learnable lifting wavelets with lifting and inverse lifting units to adaptively capture radar signal features and synthesize physiologically meaningful ECG waveforms. To improve reconstruction fidelity, we introduce a multi-resolution short-time Fourier transform (STFT) loss, that enforces consistency with the ground-truth ECG in both temporal and spectral domains. Evaluations on two public datasets demonstrate that LifWavNet outperforms state-of-the-art methods in ECG reconstruction and downstream vital sign estimation (heart rate and heart rate variability). Furthermore, intermediate feature visualization highlights the interpretability of multi-resolution decomposition and synthesis in radar-to-ECG reconstruction. These results establish LifWavNet as a robust framework for radar-based non-contact ECG measurement.",
        "translated": "非接触式心电图（ECG）从雷达信号中重建为无干扰心脏监测提供了一种有前景的方法。我们提出LifWavNet，一种基于多分辨率分析与合成（MRAS）模型的提升小波网络，用于雷达信号到ECG的重建。与以往采用固定小波方法的模型不同，LifWavNet采用可学习的提升小波，通过提升单元和逆提升单元自适应地捕捉雷达信号特征，并合成具有生理意义的ECG波形。为提高重建保真度，我们引入了一种多分辨率短时傅里叶变换（STFT）损失函数，强制在时域和频域上与真实ECG保持一致。在两个公开数据集上的评估表明，LifWavNet在ECG重建及下游生命体征估计（心率和心率变异性）方面均优于现有最先进方法。此外，中间特征可视化结果突显了多分辨率分解与合成在雷达到ECG重建中的可解释性。这些结果确立了LifWavNet作为基于雷达的非接触式ECG测量的稳健框架。",
        "translated_title": "LifWavNet：基于提升小波的网络用于从雷达信号中重建无接触式心电图",
        "label": [],
        "label_reason": "论文聚焦雷达信号到ECG重建，属于信号处理与生物医学信号恢复，非图像像素级处理，不属于low-level图像任务。",
        "relevance_score": 1,
        "novelty_score": 6,
        "novelty_reason": "提出可学习小波网络与多分辨率STFT损失，对信号重建有改进，但非图像处理领域创新。"
    },
    {
        "title": "Phased DMD: Few-step Distribution Matching Distillation via Score\n  Matching within Subintervals",
        "url": "http://arxiv.org/abs/2510.27684v1",
        "pub_date": "2025-10-31",
        "summary": "Distribution Matching Distillation (DMD) distills score-based generative models into efficient one-step generators, without requiring a one-to-one correspondence with the sampling trajectories of their teachers. However, limited model capacity causes one-step distilled models underperform on complex generative tasks, e.g., synthesizing intricate object motions in text-to-video generation. Directly extending DMD to multi-step distillation increases memory usage and computational depth, leading to instability and reduced efficiency. While prior works propose stochastic gradient truncation as a potential solution, we observe that it substantially reduces the generation diversity of multi-step distilled models, bringing it down to the level of their one-step counterparts. To address these limitations, we propose Phased DMD, a multi-step distillation framework that bridges the idea of phase-wise distillation with Mixture-of-Experts (MoE), reducing learning difficulty while enhancing model capacity. Phased DMD is built upon two key ideas: progressive distribution matching and score matching within subintervals. First, our model divides the SNR range into subintervals, progressively refining the model to higher SNR levels, to better capture complex distributions. Next, to ensure the training objective within each subinterval is accurate, we have conducted rigorous mathematical derivations. We validate Phased DMD by distilling state-of-the-art image and video generation models, including Qwen-Image (20B parameters) and Wan2.2 (28B parameters). Experimental results demonstrate that Phased DMD preserves output diversity better than DMD while retaining key generative capabilities. We will release our code and models.",
        "translated": "分布匹配蒸馏（Distribution Matching Distillation, DMD）将基于分数的生成模型蒸馏为高效的单步生成器，无需与教师模型的采样轨迹建立一一对应关系。然而，受限的模型容量导致单步蒸馏模型在复杂生成任务（如文本到视频生成中合成精细物体运动）上表现不佳。直接将DMD扩展至多步蒸馏会增加内存占用和计算深度，导致训练不稳定并降低效率。尽管先前工作提出随机梯度截断作为潜在解决方案，我们观察到该方法显著降低了多步蒸馏模型的生成多样性，使其降至与单步模型相当的水平。为克服这些局限，我们提出分阶段DMD（Phased DMD），一种将分阶段蒸馏思想与专家混合（Mixture-of-Experts, MoE）相结合的多步蒸馏框架，旨在降低学习难度的同时增强模型容量。Phased DMD基于两个核心思想：渐进式分布匹配和子区间内的分数匹配。首先，我们的模型将SNR范围划分为多个子区间，逐步将模型优化至更高SNR水平，以更好地捕捉复杂分布。其次，为确保每个子区间内训练目标的准确性，我们进行了严格的数学推导。我们通过蒸馏当前最先进的图像与视频生成模型（包括Qwen-Image（20B参数）和Wan2.2（28B参数））验证了Phased DMD的有效性。实验结果表明，与DMD相比，Phased DMD在保持关键生成能力的同时，更好地保留了输出多样性。我们将公开代码和模型。",
        "translated_title": "分阶段DMD：基于子区间内得分匹配的少步分布匹配蒸馏",
        "label": [],
        "label_reason": "论文聚焦图像/视频生成模型蒸馏，属于高阶生成任务，非像素级图像恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出分阶段蒸馏与子区间分数匹配，改进多步蒸馏稳定性，创新性较强。"
    },
    {
        "title": "PETAR: Localized Findings Generation with Mask-Aware Vision-Language\n  Modeling for PET Automated Reporting",
        "url": "http://arxiv.org/abs/2510.27680v1",
        "pub_date": "2025-10-31",
        "summary": "Recent advances in vision-language models (VLMs) have enabled impressive multimodal reasoning, yet most medical applications remain limited to 2D imaging. In this work, we extend VLMs to 3D positron emission tomography and computed tomography (PET/CT), a domain characterized by large volumetric data, small and dispersed lesions, and lengthy radiology reports. We introduce a large-scale dataset comprising over 11,000 lesion-level descriptions paired with 3D segmentations from more than 5,000 PET/CT exams, extracted via a hybrid rule-based and large language model (LLM) pipeline. Building upon this dataset, we propose PETAR-4B, a 3D mask-aware vision-language model that integrates PET, CT, and lesion contours for spatially grounded report generation. PETAR bridges global contextual reasoning with fine-grained lesion awareness, producing clinically coherent and localized findings. Comprehensive automated and human evaluations demonstrate that PETAR substantially improves PET/CT report generation quality, advancing 3D medical vision-language understanding.",
        "translated": "近期，视觉-语言模型（VLMs）在多模态推理方面取得了显著进展，但大多数医学应用仍局限于二维成像。本研究将VLMs扩展至三维正电子发射断层扫描与计算机断层扫描（PET/CT）领域，该领域具有大规模体数据、病灶小且分散、放射学报告冗长等特点。我们构建了一个大规模数据集，包含超过11,000条病灶级别的描述，与来自5,000余例PET/CT检查的三维分割结果配对，通过混合规则与大语言模型（LLM）的流水线提取。基于该数据集，我们提出PETAR-4B，一种三维掩码感知的视觉-语言模型，整合PET、CT及病灶轮廓信息，实现空间定位的报告生成。PETAR将全局上下文推理与细粒度病灶感知相结合，生成具有临床连贯性且定位准确的发现结果。全面的自动化评估与人工评估表明，PETAR显著提升了PET/CT报告生成的质量，推动了三维医学视觉-语言理解的发展。",
        "translated_title": "PETAR：基于掩码感知视觉-语言建模的PET自动化报告局部发现生成",
        "label": [],
        "label_reason": "论文聚焦3D PET/CT报告生成，属高阶医学图像理解，非像素级图像恢复或增强。",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出mask-aware VLM架构，结合3D分割与语言模型，属多模态融合创新，但非low-level任务核心突破。"
    },
    {
        "title": "Dark-Field X-Ray Imaging Significantly Improves Deep-Learning based\n  Detection of Synthetic Early-Stage Lung Tumors in Preclinical Models",
        "url": "http://arxiv.org/abs/2510.27679v1",
        "pub_date": "2025-10-31",
        "summary": "Low-dose computed tomography (LDCT) is the current standard for lung cancer screening, yet its adoption and accessibility remain limited. Many regions lack LDCT infrastructure, and even among those screened, early-stage cancer detection often yield false positives, as shown in the National Lung Screening Trial (NLST) with a sensitivity of 93.8 percent and a false-positive rate of 26.6 percent. We aim to investigate whether X-ray dark-field imaging (DFI) radiograph, a technique sensitive to small-angle scatter from alveolar microstructure and less susceptible to organ shadowing, can significantly improve early-stage lung tumor detection when coupled with deep-learning segmentation. Using paired attenuation (ATTN) and DFI radiograph images of euthanized mouse lungs, we generated realistic synthetic tumors with irregular boundaries and intensity profiles consistent with physical lung contrast. A U-Net segmentation network was trained on small patches using either ATTN, DFI, or a combination of ATTN and DFI channels.Results show that the DFI-only model achieved a true-positive detection rate of 83.7 percent, compared with 51 percent for ATTN-only, while maintaining comparable specificity (90.5 versus 92.9 percent). The combined ATTN and DFI input achieved 79.6 percent sensitivity and 97.6 percent specificity. In conclusion, DFI substantially improves early-tumor detectability in comparison to standard attenuation radiography and shows potential as an accessible, low-cost, low-dose alternative for pre-clinical or limited-resource screening where LDCT is unavailable.",
        "translated": "低剂量计算机断层扫描（LDCT）目前是肺癌筛查的标准方法，但其应用和可及性仍受到限制。许多地区缺乏LDCT基础设施，即使在已接受筛查的人群中，早期癌症检测也常出现假阳性结果，如国家肺癌筛查试验（NLST）所示，其敏感度为93.8%，假阳性率为26.6%。我们旨在研究X射线暗场成像（DFI）技术是否能显著提升早期肺肿瘤的检测能力，该技术对肺泡微结构产生的小角度散射敏感，且不易受器官阴影干扰，当与深度学习分割方法结合使用时。我们利用安乐死小鼠肺部的配对衰减（ATTN）和DFI图像，生成了具有不规则边界和与真实肺组织对比度一致的强度分布的逼真合成肿瘤。采用U-Net分割网络，在小尺寸图像块上分别使用ATTN、DFI或ATTN与DFI通道组合进行训练。结果表明，仅使用DFI的模型实现了83.7%的真阳性检测率，而仅使用ATTN的模型为51%，同时保持了相近的特异性（90.5%对比92.9%）。结合ATTN与DFI输入的模型则达到了79.6%的敏感度和97.6%的特异性。综上所述，与标准衰减成像相比，DFI显著提升了早期肿瘤的可检测性，展现出作为LDCT不可及的临床前或资源有限场景下的一种可及、低成本、低剂量替代方案的潜力。",
        "translated_title": "暗场X射线成像显著提升基于深度学习的临床前模型中合成早期肺癌检测性能",
        "label": [],
        "label_reason": "论文聚焦于肿瘤检测，属高阶视觉任务，非图像像素级恢复或增强。",
        "relevance_score": 3,
        "novelty_score": 5,
        "novelty_reason": "提出DFI与深度学习结合检测肿瘤，属应用创新，非底层图像处理方法创新。"
    },
    {
        "title": "Vision Transformer for Robust Occluded Person Reidentification in\n  Complex Surveillance Scenes",
        "url": "http://arxiv.org/abs/2510.27677v1",
        "pub_date": "2025-10-31",
        "summary": "Person re-identification (ReID) in surveillance is challenged by occlusion, viewpoint distortion, and poor image quality. Most existing methods rely on complex modules or perform well only on clear frontal images. We propose Sh-ViT (Shuffling Vision Transformer), a lightweight and robust model for occluded person ReID. Built on ViT-Base, Sh-ViT introduces three components: First, a Shuffle module in the final Transformer layer to break spatial correlations and enhance robustness to occlusion and blur; Second, scenario-adapted augmentation (geometric transforms, erasing, blur, and color adjustment) to simulate surveillance conditions; Third, DeiT-based knowledge distillation to improve learning with limited labels.To support real-world evaluation, we construct the MyTT dataset, containing over 10,000 pedestrians and 30,000+ images from base station inspections, with frequent equipment occlusion and camera variations. Experiments show that Sh-ViT achieves 83.2% Rank-1 and 80.1% mAP on MyTT, outperforming CNN and ViT baselines, and 94.6% Rank-1 and 87.5% mAP on Market1501, surpassing state-of-the-art methods.In summary, Sh-ViT improves robustness to occlusion and blur without external modules, offering a practical solution for surveillance-based personnel monitoring.",
        "translated": "监控场景中的人重识别（ReID）面临遮挡、视角畸变和图像质量差等挑战。现有大多数方法依赖复杂的模块，或仅在清晰的正面图像上表现良好。我们提出Sh-ViT（Shuffling Vision Transformer），一种轻量且鲁棒的遮挡人重识别模型。该模型基于ViT-Base构建，引入三个关键组件：首先，在最终Transformer层中引入Shuffle模块，打破空间相关性，增强对遮挡和模糊的鲁棒性；其次，采用场景自适应数据增强（包括几何变换、擦除、模糊和色彩调整），模拟真实监控环境；第三，采用DeiT-based知识蒸馏，在有限标注条件下提升模型学习能力。为支持真实场景评估，我们构建了MyTT数据集，包含来自基站巡检的超过10,000名行人和30,000+张图像，具有频繁的设备遮挡和相机变化。实验表明，Sh-ViT在MyTT数据集上达到83.2%的Rank-1和80.1%的mAP，优于CNN和ViT基线模型；在Market1501数据集上达到94.6%的Rank-1和87.5%的mAP，超越当前主流方法。综上，Sh-ViT在无需外部模块的前提下提升了对遮挡和模糊的鲁棒性，为基于监控的人员监测提供了实用解决方案。",
        "translated_title": "视觉Transformer在复杂监控场景下对遮挡行人重识别的鲁棒性研究",
        "label": [],
        "label_reason": "论文聚焦于行人重识别（ReID），属于high-level任务，不涉及像素级图像质量恢复或增强。",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出Shuffle模块和数据增强策略，对ViT进行改进，但未解决low-level图像退化问题。"
    },
    {
        "title": "Deep learning denoising unlocks quantitative insights in operando\n  materials microscopy",
        "url": "http://arxiv.org/abs/2510.27667v1",
        "pub_date": "2025-10-31",
        "summary": "Operando microscopy provides direct insight into the dynamic chemical and physical processes that govern functional materials, yet measurement noise limits the effective resolution and undermines quantitative analysis. Here, we present a general framework for integrating unsupervised deep learning-based denoising into quantitative microscopy workflows across modalities and length scales. Using simulated data, we demonstrate that deep denoising preserves physical fidelity, introduces minimal bias, and reduces uncertainty in model learning with partial differential equation (PDE)-constrained optimization. Applied to experiments, denoising reveals nanoscale chemical and structural heterogeneity in scanning transmission X-ray microscopy (STXM) of lithium iron phosphate (LFP), enables automated particle segmentation and phase classification in optical microscopy of graphite electrodes, and reduces noise-induced variability by nearly 80% in neutron radiography to resolve heterogeneous lithium transport. Collectively, these results establish deep denoising as a powerful, modality-agnostic enhancement that advances quantitative operando imaging and extends the reach of previously noise-limited techniques.",
        "translated": "原位显微技术能够直接揭示功能材料中动态的化学与物理过程，然而测量噪声限制了有效分辨率并削弱了定量分析的准确性。本文提出一种通用框架，用于将无监督深度学习去噪方法整合到跨模态、跨尺度的定量显微成像工作流程中。通过模拟数据验证，我们证明深度去噪能够保持物理保真度，引入极小偏差，并在偏微分方程（PDE）约束优化中降低模型学习的不确定性。应用于实际实验时，去噪技术揭示了锂铁磷酸盐（LFP）在扫描透射X射线显微镜（STXM）下的纳米尺度化学与结构异质性；实现了石墨电极在光学显微镜下颗粒的自动分割与相态分类；并在中子成像中将噪声引起的变异降低近80%，从而解析出异质的锂离子传输过程。综上，这些结果确立了深度去噪作为一种强大且模态无关的增强手段，推动了定量原位成像的发展，并拓展了以往受噪声限制的技术的应用范围。",
        "translated_title": "深度学习去噪在原位材料显微成像中揭示定量洞察",
        "label": [
            "图像去噪",
            "图像恢复"
        ],
        "label_reason": "论文核心为深度学习去噪，提升显微图像定量分析精度，属典型图像恢复任务。",
        "relevance_score": 10,
        "novelty_score": 8,
        "novelty_reason": "提出通用无监督去噪框架，结合PDE优化，提升物理保真度，具显著改进。"
    },
    {
        "title": "Imbalanced Classification through the Lens of Spurious Correlations",
        "url": "http://arxiv.org/abs/2510.27650v1",
        "pub_date": "2025-10-31",
        "summary": "Class imbalance poses a fundamental challenge in machine learning, frequently leading to unreliable classification performance. While prior methods focus on data- or loss-reweighting schemes, we view imbalance as a data condition that amplifies Clever Hans (CH) effects by underspecification of minority classes. In a counterfactual explanations-based approach, we propose to leverage Explainable AI to jointly identify and eliminate CH effects emerging under imbalance. Our method achieves competitive classification performance on three datasets and demonstrates how CH effects emerge under imbalance, a perspective largely overlooked by existing approaches.",
        "translated": "类别不平衡在机器学习中构成一个根本性挑战，常常导致分类性能不可靠。尽管先前的方法主要关注数据重加权或损失重加权策略，我们将不平衡视为一种数据条件，它通过少数类的欠指定（underspecification）放大了“聪明汉斯”（Clever Hans, CH）效应。基于反事实解释的方法，我们提出利用可解释人工智能（Explainable AI）联合识别并消除在不平衡条件下出现的CH效应。我们的方法在三个数据集上实现了具有竞争力的分类性能，并展示了CH效应在不平衡条件下如何产生，这一视角在现有方法中大多被忽视。",
        "translated_title": "通过虚假相关性视角下的不平衡分类",
        "label": [],
        "label_reason": "论文聚焦类别不平衡分类，属于高阶视觉任务，不涉及像素级图像恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "提出基于可解释AI消除Clever Hans效应，方法新颖但非low-level图像处理创新。"
    },
    {
        "title": "Gaussian Combined Distance: A Generic Metric for Object Detection",
        "url": "http://arxiv.org/abs/2510.27649v1",
        "pub_date": "2025-10-31",
        "summary": "In object detection, a well-defined similarity metric can significantly enhance model performance. Currently, the IoU-based similarity metric is the most commonly preferred choice for detectors. However, detectors using IoU as a similarity metric often perform poorly when detecting small objects because of their sensitivity to minor positional deviations. To address this issue, recent studies have proposed the Wasserstein Distance as an alternative to IoU for measuring the similarity of Gaussian-distributed bounding boxes. However, we have observed that the Wasserstein Distance lacks scale invariance, which negatively impacts the model's generalization capability. Additionally, when used as a loss function, its independent optimization of the center attributes leads to slow model convergence and unsatisfactory detection precision. To address these challenges, we introduce the Gaussian Combined Distance (GCD). Through analytical examination of GCD and its gradient, we demonstrate that GCD not only possesses scale invariance but also facilitates joint optimization, which enhances model localization performance. Extensive experiments on the AI-TOD-v2 dataset for tiny object detection show that GCD, as a bounding box regression loss function and label assignment metric, achieves state-of-the-art performance across various detectors. We further validated the generalizability of GCD on the MS-COCO-2017 and Visdrone-2019 datasets, where it outperforms the Wasserstein Distance across diverse scales of datasets. Code is available at https://github.com/MArKkwanGuan/mmdet-GCD.",
        "translated": "在目标检测中，一个定义良好的相似性度量能够显著提升模型性能。目前，基于IoU的相似性度量是检测器中最常用的选择。然而，以IoU作为相似性度量的检测器在检测小目标时往往表现不佳，这是由于其对微小位置偏差过于敏感。为解决这一问题，近期研究提出了将Wasserstein距离作为IoU的替代方案，用于衡量高斯分布边界框之间的相似性。然而，我们观察到Wasserstein距离缺乏尺度不变性，这对模型的泛化能力产生负面影响。此外，当其作为损失函数时，对中心属性的独立优化会导致模型收敛缓慢，且检测精度不理想。为应对这些挑战，我们提出了高斯联合距离（Gaussian Combined Distance, GCD）。通过对GCD及其梯度的分析，我们证明GCD不仅具备尺度不变性，而且支持联合优化，从而提升了模型的定位性能。在AI-TOD-v2数据集上的大量实验表明，GCD作为边界框回归损失函数和标签分配度量，在多种检测器上均实现了最先进的性能。我们进一步在MS-COCO-2017和Visdrone-2019数据集上验证了GCD的泛化能力，其在不同尺度的数据集上均优于Wasserstein距离。代码已开源于https://github.com/MArKkwanGuan/mmdet-GCD。",
        "translated_title": "高斯联合距离：一种通用的目标检测度量指标",
        "label": [],
        "label_reason": "论文聚焦目标检测中的边界框相似性度量，属于高阶视觉任务，非像素级图像恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 6,
        "novelty_reason": "提出GCD度量，改进Wasserstein距离，具备尺度不变性和联合优化特性，属中等创新。"
    },
    {
        "title": "NegoCollab: A Common Representation Negotiation Approach for\n  Heterogeneous Collaborative Perception",
        "url": "http://arxiv.org/abs/2510.27647v1",
        "pub_date": "2025-10-31",
        "summary": "Collaborative perception improves task performance by expanding the perception range through information sharing among agents. . Immutable heterogeneity poses a significant challenge in collaborative perception, as participating agents may employ different and fixed perception models. This leads to domain gaps in the intermediate features shared among agents, consequently degrading collaborative performance. Aligning the features of all agents to a common representation can eliminate domain gaps with low training cost. However, in existing methods, the common representation is designated as the representation of a specific agent, making it difficult for agents with significant domain discrepancies from this specific agent to achieve proper alignment. This paper proposes NegoCollab, a heterogeneous collaboration method based on the negotiated common representation. It introduces a negotiator during training to derive the common representation from the local representations of each modality's agent, effectively reducing the inherent domain gap with the various local representations. In NegoCollab, the mutual transformation of features between the local representation space and the common representation space is achieved by a pair of sender and receiver. To better align local representations to the common representation containing multimodal information, we introduce structural alignment loss and pragmatic alignment loss in addition to the distribution alignment loss to supervise the training. This enables the knowledge in the common representation to be fully distilled into the sender.",
        "translated": "协作感知通过代理间的信息共享扩展感知范围，从而提升任务性能。然而，不可变的异构性给协作感知带来了重大挑战，因为参与代理可能采用不同且固定的感知模型。这导致代理间共享的中间特征存在域差异，进而降低协作性能。将所有代理的特征对齐至一个共同表示，可以在较低训练成本下消除域差异。然而，在现有方法中，共同表示被指定为某一特定代理的表示，使得与该特定代理存在显著域差异的代理难以实现有效对齐。本文提出 NegoCollab，一种基于协商式共同表示的异构协作方法。该方法在训练过程中引入协商者，从各模态代理的局部表示中推导出共同表示，有效减少与各种局部表示之间的固有域差异。在 NegoCollab 中，局部表示空间与共同表示空间之间的特征相互转换由一对发送器和接收器实现。为了更好地将局部表示对齐至包含多模态信息的共同表示，除分布对齐损失外，我们还引入结构对齐损失和实用对齐损失以监督训练。这使得共同表示中的知识能够充分蒸馏至发送器中。",
        "translated_title": "NegoCollab：一种面向异构协作感知的通用表示协商方法",
        "label": [],
        "label_reason": "论文聚焦异构协作感知，目标为多智能体特征对齐，非像素级图像恢复或增强任务。",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出协商式公共表示机制，结合结构与语义对齐损失，有一定创新但非low-level任务核心。"
    },
    {
        "title": "VessShape: Few-shot 2D blood vessel segmentation by leveraging shape\n  priors from synthetic images",
        "url": "http://arxiv.org/abs/2510.27646v1",
        "pub_date": "2025-10-31",
        "summary": "Semantic segmentation of blood vessels is an important task in medical image analysis, but its progress is often hindered by the scarcity of large annotated datasets and the poor generalization of models across different imaging modalities. A key aspect is the tendency of Convolutional Neural Networks (CNNs) to learn texture-based features, which limits their performance when applied to new domains with different visual characteristics. We hypothesize that leveraging geometric priors of vessel shapes, such as their tubular and branching nature, can lead to more robust and data-efficient models. To investigate this, we introduce VessShape, a methodology for generating large-scale 2D synthetic datasets designed to instill a shape bias in segmentation models. VessShape images contain procedurally generated tubular geometries combined with a wide variety of foreground and background textures, encouraging models to learn shape cues rather than textures. We demonstrate that a model pre-trained on VessShape images achieves strong few-shot segmentation performance on two real-world datasets from different domains, requiring only four to ten samples for fine-tuning. Furthermore, the model exhibits notable zero-shot capabilities, effectively segmenting vessels in unseen domains without any target-specific training. Our results indicate that pre-training with a strong shape bias can be an effective strategy to overcome data scarcity and improve model generalization in blood vessel segmentation.",
        "translated": "血管语义分割是医学图像分析中的重要任务，但其进展常受到大规模标注数据集稀缺以及模型在不同成像模态间泛化能力不足的阻碍。一个关键问题是卷积神经网络（CNN）倾向于学习基于纹理的特征，这限制了其在具有不同视觉特性的新领域中的性能表现。我们假设，利用血管形状的几何先验知识（如其管状和分支结构特性），可以构建更具鲁棒性和数据效率更高的模型。为验证这一假设，我们提出 VessShape，一种用于生成大规模2D合成数据集的方法，旨在为分割模型注入形状偏置。VessShape 图像包含程序化生成的管状几何结构，并结合了多种前景和背景纹理，从而促使模型学习形状线索而非纹理特征。我们证明，仅在 VessShape 图像上预训练的模型，在两个来自不同领域的现实数据集上展现出优异的少样本分割性能，仅需四到十个样本即可完成微调。此外，该模型表现出显著的零样本能力，能够在未见过的领域中有效分割血管，而无需针对目标域进行任何特定训练。我们的结果表明，在血管分割任务中，采用强形状偏置的预训练策略是一种有效应对数据稀缺并提升模型泛化能力的方法。",
        "translated_title": "VessShape：利用合成图像中的形状先验知识实现少样本2D血管分割",
        "label": [],
        "label_reason": "论文聚焦医学图像分割，目标为语义理解而非像素级恢复，属high-level任务。",
        "relevance_score": 3,
        "novelty_score": 7,
        "novelty_reason": "提出利用形状先验提升分割泛化能力，方法新颖，但非low-level图像处理创新。"
    },
    {
        "title": "Sketch-to-Layout: Sketch-Guided Multimodal Layout Generation",
        "url": "http://arxiv.org/abs/2510.27632v1",
        "pub_date": "2025-10-31",
        "summary": "Graphic layout generation is a growing research area focusing on generating aesthetically pleasing layouts ranging from poster designs to documents. While recent research has explored ways to incorporate user constraints to guide the layout generation, these constraints often require complex specifications which reduce usability. We introduce an innovative approach exploiting user-provided sketches as intuitive constraints and we demonstrate empirically the effectiveness of this new guidance method, establishing the sketch-to-layout problem as a promising research direction, which is currently under-explored. To tackle the sketch-to-layout problem, we propose a multimodal transformer-based solution using the sketch and the content assets as inputs to produce high quality layouts. Since collecting sketch training data from human annotators to train our model is very costly, we introduce a novel and efficient method to synthetically generate training sketches at scale. We train and evaluate our model on three publicly available datasets: PubLayNet, DocLayNet and SlidesVQA, demonstrating that it outperforms state-of-the-art constraint-based methods, while offering a more intuitive design experience. In order to facilitate future sketch-to-layout research, we release O(200k) synthetically-generated sketches for the public datasets above. The datasets are available at https://github.com/google-deepmind/sketch_to_layout.",
        "translated": "图形布局生成是一个日益发展的研究领域，专注于从海报设计到文档等各类美观布局的生成。尽管近期研究已探索了如何引入用户约束以引导布局生成，但这些约束通常需要复杂的规格说明，从而降低了可用性。我们提出一种创新方法，利用用户提供的草图作为直观的约束，并通过实证验证了该新引导方法的有效性，确立了“草图到布局”问题作为一个有前景且目前尚未充分探索的研究方向。为解决“草图到布局”问题，我们提出了一种基于多模态Transformer的解决方案，以草图和内容素材作为输入，生成高质量的布局。由于从人工标注者处收集草图训练数据以训练模型成本极高，我们引入了一种新颖且高效的合成方法，可大规模生成训练草图。我们在三个公开可用的数据集（PubLayNet、DocLayNet 和 SlidesVQA）上训练并评估了我们的模型，结果表明其性能优于当前最先进的基于约束的方法，同时提供了更直观的设计体验。为促进未来“草图到布局”研究的发展，我们发布了约20万张针对上述公开数据集合成生成的草图。相关数据集可访问 https://github.com/google-deepmind/sketch_to_layout 获取。",
        "translated_title": "草图到布局：基于草图引导的多模态布局生成",
        "label": [],
        "label_reason": "论文聚焦于布局生成，属于高阶视觉任务，不涉及像素级图像恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 6,
        "novelty_reason": "提出基于草图的多模态布局生成方法，具有一定创新性，但非low-level任务。"
    },
    {
        "title": "Visual Backdoor Attacks on MLLM Embodied Decision Making via Contrastive\n  Trigger Learning",
        "url": "http://arxiv.org/abs/2510.27623v1",
        "pub_date": "2025-10-31",
        "summary": "Multimodal large language models (MLLMs) have advanced embodied agents by enabling direct perception, reasoning, and planning task-oriented actions from visual inputs. However, such vision driven embodied agents open a new attack surface: visual backdoor attacks, where the agent behaves normally until a visual trigger appears in the scene, then persistently executes an attacker-specified multi-step policy. We introduce BEAT, the first framework to inject such visual backdoors into MLLM-based embodied agents using objects in the environments as triggers. Unlike textual triggers, object triggers exhibit wide variation across viewpoints and lighting, making them difficult to implant reliably. BEAT addresses this challenge by (1) constructing a training set that spans diverse scenes, tasks, and trigger placements to expose agents to trigger variability, and (2) introducing a two-stage training scheme that first applies supervised fine-tuning (SFT) and then our novel Contrastive Trigger Learning (CTL). CTL formulates trigger discrimination as preference learning between trigger-present and trigger-free inputs, explicitly sharpening the decision boundaries to ensure precise backdoor activation. Across various embodied agent benchmarks and MLLMs, BEAT achieves attack success rates up to 80%, while maintaining strong benign task performance, and generalizes reliably to out-of-distribution trigger placements. Notably, compared to naive SFT, CTL boosts backdoor activation accuracy up to 39% under limited backdoor data. These findings expose a critical yet unexplored security risk in MLLM-based embodied agents, underscoring the need for robust defenses before real-world deployment.",
        "translated": "多模态大语言模型（MLLMs）通过直接从视觉输入中实现感知、推理和任务导向行为规划，推动了具身智能体的发展。然而，此类基于视觉驱动的具身智能体也开辟了新的攻击面：视觉后门攻击，即智能体在正常运行状态下，一旦场景中出现特定视觉触发器，便会持续执行攻击者指定的多步骤策略。我们提出BEAT，这是首个利用环境中物体作为触发器，向基于MLLM的具身智能体注入视觉后门的框架。与文本触发器不同，物体触发器在不同视角和光照条件下表现出广泛变化，使得其可靠植入极具挑战性。BEAT通过以下两方面应对该挑战：（1）构建覆盖多样场景、任务及触发器位置的训练集，使智能体暴露于触发器的多样性；（2）引入两阶段训练方案，首先进行监督微调（SFT），随后采用我们提出的新型对比触发学习（CTL）。CTL将触发器判别建模为触发器存在与不存在输入之间的偏好学习，显式锐化决策边界，以确保后门激活的精确性。在多种具身智能体基准和MLLMs上，BEAT实现了高达80%的攻击成功率，同时保持了良好的正常任务性能，并能可靠泛化至分布外的触发器位置。值得注意的是，相较于朴素的SFT，CTL在有限后门数据条件下将后门激活准确率提升了高达39%。这些发现揭示了基于MLLM的具身智能体中一个关键但尚未被充分探索的安全风险，凸显了在实际部署前构建鲁棒防御机制的必要性。",
        "translated_title": "视觉后门攻击：基于对比触发学习的多模态大模型具身决策攻击",
        "label": [],
        "label_reason": "论文聚焦于MLLM的视觉后门攻击，属于高阶视觉安全任务，非像素级图像恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 6,
        "novelty_reason": "提出对比触发学习（CTL）提升后门激活精度，但属于安全攻击方法创新，非low-level图像处理创新。"
    },
    {
        "title": "Dual-Stream Diffusion for World-Model Augmented Vision-Language-Action\n  Model",
        "url": "http://arxiv.org/abs/2510.27607v1",
        "pub_date": "2025-10-31",
        "summary": "Recently, augmenting Vision-Language-Action models (VLAs) with world modeling has shown promise in improving robotic policy learning. However, it remains challenging to jointly predict next-state observations and action sequences because of the inherent difference between the two modalities. To address this, we propose DUal-STream diffusion (DUST), a world-model augmented VLA framework that handles the modality conflict and enhances the performance of VLAs across diverse tasks. Specifically, we propose a multimodal diffusion transformer architecture that explicitly maintains separate modality streams while still enabling cross-modal knowledge sharing. In addition, we introduce independent noise perturbations for each modality and a decoupled flow-matching loss. This design enables the model to learn the joint distribution in a bidirectional manner while avoiding the need for a unified latent space. Based on the decoupling of modalities during training, we also introduce a joint sampling method that supports test-time scaling, where action and vision tokens evolve asynchronously at different rates. Through experiments on simulated benchmarks such as RoboCasa and GR-1, DUST achieves up to 6% gains over baseline methods, while our test-time scaling approach provides an additional 2-5% boost. On real-world tasks with the Franka Research 3, DUST improves success rates by 13%, confirming its effectiveness beyond simulation. Furthermore, pre-training on action-free videos from BridgeV2 yields significant transfer gains on RoboCasa, underscoring DUST's potential for large-scale VLA pretraining.",
        "translated": "近年来，通过引入世界建模（world modeling）增强视觉-语言-动作模型（Vision-Language-Action models, VLAs）在机器人策略学习中展现出潜力。然而，由于视觉观测与动作序列这两种模态之间存在本质差异，联合预测下一状态观测与动作序列仍具有挑战性。为解决这一问题，我们提出DUal-STream diffusion（DUST），一种基于世界建模增强的VLA框架，能够有效处理模态冲突，并在多样化任务中提升VLA的性能。具体而言，我们设计了一种多模态扩散Transformer架构，该架构显式地保持各模态独立的流（modality streams），同时仍允许跨模态知识共享。此外，我们为每个模态引入独立的噪声扰动，并采用解耦的流匹配损失（decoupled flow-matching loss）。该设计使模型能够以双向方式学习联合分布，同时避免了对统一潜在空间（unified latent space）的需求。基于训练过程中模态的解耦，我们进一步提出一种联合采样方法，支持测试时的扩展（test-time scaling），其中动作和视觉token以不同速率异步演化。在RoboCasa和GR-1等仿真基准上的实验表明，DUST相比基线方法最高可提升6%的性能，而我们的测试时扩展方法可额外带来2-5%的增益。在Franka Research 3平台上的真实世界任务中，DUST使成功率提升13%，验证了其在仿真之外的有效性。此外，在BridgeV2中无动作视频上的预训练，显著提升了DUST在RoboCasa任务上的迁移性能，凸显了其在大规模VLA预训练中的潜力。",
        "translated_title": "双流扩散用于世界模型增强的视觉-语言-动作模型",
        "label": [],
        "label_reason": "论文聚焦于视觉-语言-动作模型与世界建模，属于高阶机器人决策任务，非像素级图像恢复。",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出双流扩散架构与解耦训练策略，对多模态建模有创新，但非图像恢复领域。"
    },
    {
        "title": "Spatial-SSRL: Enhancing Spatial Understanding via Self-Supervised\n  Reinforcement Learning",
        "url": "http://arxiv.org/abs/2510.27606v1",
        "pub_date": "2025-10-31",
        "summary": "Spatial understanding remains a weakness of Large Vision-Language Models (LVLMs). Existing supervised fine-tuning (SFT) and recent reinforcement learning with verifiable rewards (RLVR) pipelines depend on costly supervision, specialized tools, or constrained environments that limit scale. We introduce Spatial-SSRL, a self-supervised RL paradigm that derives verifiable signals directly from ordinary RGB or RGB-D images. Spatial-SSRL automatically formulates five pretext tasks that capture 2D and 3D spatial structure: shuffled patch reordering, flipped patch recognition, cropped patch inpainting, regional depth ordering, and relative 3D position prediction. These tasks provide ground-truth answers that are easy to verify and require no human or LVLM annotation. Training on our tasks substantially improves spatial reasoning while preserving general visual capabilities. On seven spatial understanding benchmarks in both image and video settings, Spatial-SSRL delivers average accuracy gains of 4.63% (3B) and 3.89% (7B) over the Qwen2.5-VL baselines. Our results show that simple, intrinsic supervision enables RLVR at scale and provides a practical route to stronger spatial intelligence in LVLMs.",
        "translated": "空间理解仍是大型视觉-语言模型（LVLMs）的薄弱环节。现有的监督微调（SFT）以及近期基于可验证奖励的强化学习（RLVR）方法依赖于昂贵的监督数据、专用工具或受限环境，从而限制了其扩展性。我们提出 Spatial-SSRL，一种自监督强化学习范式，能够直接从普通 RGB 或 RGB-D 图像中提取可验证的信号。Spatial-SSRL 自动构建五种预设任务，用于捕捉二维与三维空间结构：打乱的图像块重排序、翻转图像块识别、裁剪图像块修复、区域深度排序以及相对三维位置预测。这些任务提供易于验证的 ground-truth 答案，无需人工标注或 LVLM 标注。在我们的任务上进行训练，显著提升了空间推理能力，同时保持了通用视觉能力。在图像与视频设置下的七个空间理解基准测试中，Spatial-SSRL 相较于 Qwen2.5-VL 基线模型，分别在 3B 和 7B 模型规模上实现了平均准确率提升 4.63% 和 3.89%。我们的结果表明，简单的内在监督机制能够实现大规模 RLVR，并为 LVLMs 赋予更强的空间智能提供了实用路径。",
        "translated_title": "Spatial-SSRL：通过自监督强化学习增强空间理解",
        "label": [],
        "label_reason": "论文聚焦于视觉-语言模型的空间理解能力提升，属于高阶视觉任务，非像素级图像恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 6,
        "novelty_reason": "提出自监督强化学习框架，设计五种预设任务，虽新颖但非针对low-level图像处理问题。"
    },
    {
        "title": "Who Made This? Fake Detection and Source Attribution with Diffusion\n  Features",
        "url": "http://arxiv.org/abs/2510.27602v1",
        "pub_date": "2025-10-31",
        "summary": "The rapid progress of generative diffusion models has enabled the creation of synthetic images that are increasingly difficult to distinguish from real ones, raising concerns about authenticity, copyright, and misinformation. Existing supervised detectors often struggle to generalize across unseen generators, requiring extensive labeled data and frequent retraining. We introduce FRIDA (Fake-image Recognition and source Identification via Diffusion-features Analysis), a lightweight framework that leverages internal activations from a pre-trained diffusion model for deepfake detection and source generator attribution. A k-nearest-neighbor classifier applied to diffusion features achieves state-of-the-art cross-generator performance without fine-tuning, while a compact neural model enables accurate source attribution. These results show that diffusion representations inherently encode generator-specific patterns, providing a simple and interpretable foundation for synthetic image forensics.",
        "translated": "生成式扩散模型的快速发展使得合成图像越来越难以与真实图像区分，引发了关于真实性、版权和虚假信息的担忧。现有的监督检测器通常难以在未见过的生成器上泛化，需要大量标注数据并频繁重新训练。我们提出 FRIDA（基于扩散特征分析的假图像识别与来源识别），一种轻量级框架，利用预训练扩散模型的内部激活特征，实现深度伪造检测与生成器来源归因。对扩散特征应用 k-近邻分类器，在无需微调的情况下实现了跨生成器的最先进性能；同时，一个紧凑的神经网络模型能够实现准确的来源归因。这些结果表明，扩散表示本质上编码了生成器特有的模式，为合成图像取证提供了简单且可解释的基础。",
        "translated_title": "谁制造了它？基于扩散特征的伪造检测与来源归因",
        "label": [],
        "label_reason": "论文聚焦于生成图像检测与来源归因，属于高阶视觉任务，非图像像素级恢复或增强。",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出利用扩散模型内部特征进行检测，方法新颖，但非低层图像处理任务。"
    },
    {
        "title": "ANCHOR: Integrating Adversarial Training with Hard-mined Supervised\n  Contrastive Learning for Robust Representation Learning",
        "url": "http://arxiv.org/abs/2510.27599v1",
        "pub_date": "2025-10-31",
        "summary": "Neural networks have changed the way machines interpret the world. At their core, they learn by following gradients, adjusting their parameters step by step until they identify the most discriminant patterns in the data. This process gives them their strength, yet it also opens the door to a hidden flaw. The very gradients that help a model learn can also be used to produce small, imperceptible tweaks that cause the model to completely alter its decision. Such tweaks are called adversarial attacks. These attacks exploit this vulnerability by adding tiny, imperceptible changes to images that, while leaving them identical to the human eye, cause the model to make wrong predictions. In this work, we propose Adversarially-trained Contrastive Hard-mining for Optimized Robustness (ANCHOR), a framework that leverages the power of supervised contrastive learning with explicit hard positive mining to enable the model to learn representations for images such that the embeddings for the images, their augmentations, and their perturbed versions cluster together in the embedding space along with those for other images of the same class while being separated from images of other classes. This alignment helps the model focus on stable, meaningful patterns rather than fragile gradient cues. On CIFAR-10, our approach achieves impressive results for both clean and robust accuracy under PGD-20 (epsilon = 0.031), outperforming standard adversarial training methods. Our results indicate that combining adversarial guidance with hard-mined contrastive supervision helps models learn more structured and robust representations, narrowing the gap between accuracy and robustness.",
        "translated": "神经网络改变了机器理解世界的方式。其核心机制是通过梯度下降逐步调整参数，从而在数据中识别出最具判别性的模式。这一过程赋予了模型强大的能力，但同时也暴露了一个隐藏的缺陷：那些帮助模型学习的梯度，也可能被用来生成微小且人眼无法察觉的扰动，从而导致模型完全改变其决策。这类扰动被称为对抗攻击。对抗攻击通过在图像中添加微小、不可感知的扰动来利用这一漏洞，这些扰动对人类视觉系统而言几乎无差别，却足以使模型做出错误预测。\n\n在本研究中，我们提出了一种名为“对抗训练对比难样本挖掘以实现优化鲁棒性”（Adversarially-trained Contrastive Hard-mining for Optimized Robustness, ANCHOR）的框架。该框架结合了监督对比学习与显式的难正样本挖掘，使模型能够学习到图像的表示，使得图像本身、其增强版本以及扰动版本在嵌入空间中与同类其他图像的嵌入紧密聚集，同时与不同类别的图像嵌入保持分离。这种对齐机制有助于模型聚焦于稳定且有意义的模式，而非脆弱的梯度线索。\n\n在 CIFAR-10 数据集上，我们的方法在干净样本和对抗样本（PGD-20, epsilon = 0.031）下的准确率均取得了令人瞩目的结果，优于标准的对抗训练方法。实验结果表明，将对抗性引导与难样本挖掘的对比监督相结合，有助于模型学习到更具结构化和鲁棒性的表示，从而缩小准确率与鲁棒性之间的差距。",
        "translated_title": "ANCHOR：融合对抗训练与硬样本挖掘监督对比学习的鲁棒表征学习",
        "label": [],
        "label_reason": "论文聚焦对抗鲁棒性学习，属于高阶视觉任务，不涉及像素级图像恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 6,
        "novelty_reason": "提出对抗训练与对比学习结合框架，有一定创新，但未解决低层图像处理问题。"
    },
    {
        "title": "MapSAM2: Adapting SAM2 for Automatic Segmentation of Historical Map\n  Images and Time Series",
        "url": "http://arxiv.org/abs/2510.27547v1",
        "pub_date": "2025-10-31",
        "summary": "Historical maps are unique and valuable archives that document geographic features across different time periods. However, automated analysis of historical map images remains a significant challenge due to their wide stylistic variability and the scarcity of annotated training data. Constructing linked spatio-temporal datasets from historical map time series is even more time-consuming and labor-intensive, as it requires synthesizing information from multiple maps. Such datasets are essential for applications such as dating buildings, analyzing the development of road networks and settlements, studying environmental changes etc. We present MapSAM2, a unified framework for automatically segmenting both historical map images and time series. Built on a visual foundation model, MapSAM2 adapts to diverse segmentation tasks with few-shot fine-tuning. Our key innovation is to treat both historical map images and time series as videos. For images, we process a set of tiles as a video, enabling the memory attention mechanism to incorporate contextual cues from similar tiles, leading to improved geometric accuracy, particularly for areal features. For time series, we introduce the annotated Siegfried Building Time Series Dataset and, to reduce annotation costs, propose generating pseudo time series from single-year maps by simulating common temporal transformations. Experimental results show that MapSAM2 learns temporal associations effectively and can accurately segment and link buildings in time series under limited supervision or using pseudo videos. We will release both our dataset and code to support future research.",
        "translated": "历史地图是记录不同时期地理特征的独特且珍贵的档案。然而，由于历史地图图像在风格上具有广泛的变异性，且标注训练数据稀缺，其自动化分析仍面临重大挑战。从历史地图时间序列中构建关联的时空数据集更是耗时且劳动密集，因为需要从多张地图中综合信息。此类数据集对于建筑年代推断、道路网络与聚落发展分析、环境变化研究等应用至关重要。我们提出 MapSAM2，一个用于自动分割历史地图图像及时间序列的统一框架。该框架基于视觉基础模型，通过少量样本微调即可适应多样化的分割任务。我们的关键创新在于将历史地图图像和时间序列均视为视频处理。对于单张图像，我们将一组图像块（tiles）作为视频处理，使记忆注意力机制能够融合相似图像块的上下文线索，从而提升几何精度，尤其对区域类特征效果显著。对于时间序列，我们引入了标注的 Siegfried 建筑时间序列数据集，并为降低标注成本，提出通过模拟常见时间变换，从单年份地图生成伪时间序列。实验结果表明，MapSAM2 能够有效学习时间关联，并在有限监督或使用伪视频的情况下，准确分割并关联时间序列中的建筑。我们将公开我们的数据集和代码，以支持未来研究。",
        "translated_title": "MapSAM2：适配 SAM2 以实现历史地图图像及时间序列的自动分割",
        "label": [],
        "label_reason": "论文聚焦历史地图分割与时间序列关联，属于高阶语义理解任务，非像素级图像恢复。",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出伪时间序列生成与视频化处理策略，对基础模型进行少样本微调，有一定创新性。"
    },
    {
        "title": "Deep Neural Watermarking for Robust Copyright Protection in 3D Point\n  Clouds",
        "url": "http://arxiv.org/abs/2510.27533v1",
        "pub_date": "2025-10-31",
        "summary": "The protection of intellectual property has become critical due to the rapid growth of three-dimensional content in digital media. Unlike traditional images or videos, 3D point clouds present unique challenges for copyright enforcement, as they are especially vulnerable to a range of geometric and non-geometric attacks that can easily degrade or remove conventional watermark signals. In this paper, we address these challenges by proposing a robust deep neural watermarking framework for 3D point cloud copyright protection and ownership verification. Our approach embeds binary watermarks into the singular values of 3D point cloud blocks using spectral decomposition, i.e. Singular Value Decomposition (SVD), and leverages the extraction capabilities of Deep Learning using PointNet++ neural network architecture. The network is trained to reliably extract watermarks even after the data undergoes various attacks such as rotation, scaling, noise, cropping and signal distortions. We validated our method using the publicly available ModelNet40 dataset, demonstrating that deep learning-based extraction significantly outperforms traditional SVD-based techniques under challenging conditions. Our experimental evaluation demonstrates that the deep learning-based extraction approach significantly outperforms existing SVD-based methods with deep learning achieving bitwise accuracy up to 0.83 and Intersection over Union (IoU) of 0.80, compared to SVD achieving a bitwise accuracy of 0.58 and IoU of 0.26 for the Crop (70%) attack, which is the most severe geometric distortion in our experiment. This demonstrates our method's ability to achieve superior watermark recovery and maintain high fidelity even under severe distortions.",
        "translated": "随着数字媒体中三维内容的迅速增长，知识产权保护已变得至关重要。与传统的图像或视频不同，三维点云在版权执法方面面临独特挑战，因其极易受到多种几何与非几何攻击的影响，这些攻击可轻易破坏或消除传统水印信号。本文针对上述挑战，提出一种鲁棒的深度神经网络水印框架，用于三维点云的版权保护与所有权验证。本方法通过谱分解（即奇异值分解，SVD）将二进制水印嵌入三维点云块的奇异值中，并利用PointNet++神经网络架构的提取能力，实现水印的高效提取。该网络经过训练，能够在数据遭受旋转、缩放、噪声、裁剪及信号失真等多种攻击后，依然可靠地提取水印。我们在公开可用的ModelNet40数据集上验证了该方法，结果表明，在恶劣条件下，基于深度学习的提取方法显著优于传统的SVD方法。实验评估显示，基于深度学习的提取方法在Crop (70%)攻击（本实验中最严重的几何失真）下，其比特准确率高达0.83，交并比（IoU）达0.80，而传统SVD方法的比特准确率仅为0.58，IoU为0.26。这证明了本方法在严重失真条件下仍能实现更优的水印恢复效果，并保持高保真度。",
        "translated_title": "深度神经水印用于3D点云的鲁棒版权保护",
        "label": [],
        "label_reason": "论文聚焦3D点云水印保护，属高阶版权验证任务，非像素级图像恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "结合SVD与PointNet++进行水印提取，属常规技术组合，无本质创新。"
    },
    {
        "title": "TWIST2: Scalable, Portable, and Holistic Humanoid Data Collection System",
        "url": "http://arxiv.org/abs/2511.02832v1",
        "pub_date": "2025-11-04",
        "summary": "Large-scale data has driven breakthroughs in robotics, from language models to vision-language-action models in bimanual manipulation. However, humanoid robotics lacks equally effective data collection frameworks. Existing humanoid teleoperation systems either use decoupled control or depend on expensive motion capture setups. We introduce TWIST2, a portable, mocap-free humanoid teleoperation and data collection system that preserves full whole-body control while advancing scalability. Our system leverages PICO4U VR for obtaining real-time whole-body human motions, with a custom 2-DoF robot neck (cost around $250) for egocentric vision, enabling holistic human-to-humanoid control. We demonstrate long-horizon dexterous and mobile humanoid skills and we can collect 100 demonstrations in 15 minutes with an almost 100% success rate. Building on this pipeline, we propose a hierarchical visuomotor policy framework that autonomously controls the full humanoid body based on egocentric vision. Our visuomotor policy successfully demonstrates whole-body dexterous manipulation and dynamic kicking tasks. The entire system is fully reproducible and open-sourced at https://yanjieze.com/TWIST2 . Our collected dataset is also open-sourced at https://twist-data.github.io .",
        "translated": "大规模数据推动了机器人领域的突破，从语言模型到双臂操作中的视觉-语言-动作模型均受益于此。然而，人形机器人领域尚缺乏同等高效的数据采集框架。现有的人形机器人远程操作系统要么采用解耦控制，要么依赖昂贵的动作捕捉设备。我们提出 TWIST2，一种便携式、无需动作捕捉的人形机器人远程操作与数据采集系统，在保持全身完整控制的同时，显著提升了系统的可扩展性。本系统利用 PICO4U VR 实时获取人体全身运动，并配备定制的 2 自由度机器人颈部（成本约 250 美元），实现以第一人称视角的视觉感知，从而实现完整的人到人形机器人的控制。我们展示了长时序的灵巧操作与移动人形技能，并可在 15 分钟内以接近 100% 的成功率采集 100 个示范数据。在此基础上，我们提出了一种分层视觉-运动策略框架，能够基于第一人称视角视觉自主控制人形机器人的全身运动。该视觉-运动策略成功实现了全身灵巧操作及动态踢球等任务。整个系统完全可复现，并在 https://yanjieze.com/TWIST2 开源。我们采集的数据集也已开源于 https://twist-data.github.io。",
        "translated_title": "TWIST2：可扩展、便携且全面的人形机器人数据采集系统",
        "label": [],
        "label_reason": "论文聚焦于人形机器人数据采集与控制，非图像恢复或增强任务，属于高阶机器人系统。",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "提出可扩展的机器人数据采集系统，但无图像处理创新，属系统工程改进。"
    },
    {
        "title": "Densemarks: Learning Canonical Embeddings for Human Heads Images via\n  Point Tracks",
        "url": "http://arxiv.org/abs/2511.02830v1",
        "pub_date": "2025-11-04",
        "summary": "We propose DenseMarks - a new learned representation for human heads, enabling high-quality dense correspondences of human head images. For a 2D image of a human head, a Vision Transformer network predicts a 3D embedding for each pixel, which corresponds to a location in a 3D canonical unit cube. In order to train our network, we collect a dataset of pairwise point matches, estimated by a state-of-the-art point tracker over a collection of diverse in-the-wild talking heads videos, and guide the mapping via a contrastive loss, encouraging matched points to have close embeddings. We further employ multi-task learning with face landmarks and segmentation constraints, as well as imposing spatial continuity of embeddings through latent cube features, which results in an interpretable and queryable canonical space. The representation can be used for finding common semantic parts, face/head tracking, and stereo reconstruction. Due to the strong supervision, our method is robust to pose variations and covers the entire head, including hair. Additionally, the canonical space bottleneck makes sure the obtained representations are consistent across diverse poses and individuals. We demonstrate state-of-the-art results in geometry-aware point matching and monocular head tracking with 3D Morphable Models. The code and the model checkpoint will be made available to the public.",
        "translated": "我们提出 DenseMarks —— 一种用于人体头部的新学习表征，能够实现人体头部图像的高质量密集对应。对于一张二维人体头部图像，视觉Transformer网络为每个像素预测一个三维嵌入，该嵌入对应于一个三维标准单位立方体中的位置。为了训练我们的网络，我们收集了一组成对点匹配数据，这些匹配点由一个最先进的点跟踪器在大量真实场景中的说话人头部视频上估计得到，并通过对比损失函数引导映射过程，促使匹配点具有相近的嵌入表示。此外，我们采用多任务学习，结合人脸关键点和分割约束，并通过潜在立方体特征强制嵌入的空间连续性，从而构建出一个可解释且可查询的标准空间。该表征可用于寻找共同语义部件、人脸/头部跟踪以及立体重建。由于采用了强监督，我们的方法对姿态变化具有鲁棒性，并覆盖整个头部区域，包括头发。同时，标准空间瓶颈确保了在不同姿态和个体之间获得的表征具有一致性。我们在基于几何感知的点匹配和单目头部跟踪（结合3D可变形模型）任务中展示了当前最先进的结果。代码和模型检查点将公开发布。",
        "translated_title": "Densemarks：基于点轨迹学习人头图像的规范嵌入",
        "label": [],
        "label_reason": "论文聚焦于人脸头部的3D嵌入学习与点匹配，属于高阶视觉任务，非像素级图像恢复或增强。",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出基于点轨迹的对比学习框架，结合多任务约束，方法新颖，但非low-level图像处理领域创新。"
    },
    {
        "title": "PLUTO-4: Frontier Pathology Foundation Models",
        "url": "http://arxiv.org/abs/2511.02826v1",
        "pub_date": "2025-11-04",
        "summary": "Foundation models trained on large-scale pathology image corpora have demonstrated strong transfer capabilities across diverse histopathology tasks. Building on this progress, we introduce PLUTO-4, our next generation of pathology foundation models that extend the Pathology-Universal Transformer (PLUTO) to frontier scale. We share two complementary Vision Transformer architectures in the PLUTO-4 family: a compact and efficient PLUTO-4S model optimized for multi-scale deployment using a FlexiViT setup with 2D-RoPE embeddings, and a frontier-scale PLUTO-4G model trained with a single patch size to maximize representation capacity and stability. Both models are pretrained using a self-supervised objective derived from DINOv2 on a large multi-institutional corpus containing 551,164 WSIs from 137,144 patients across over 50 institutions, spanning over 60 disease types and over 100 stains. Comprehensive evaluation across public and internal benchmarks demonstrates that PLUTO-4 achieves state-of-the-art performance on tasks requiring varying spatial and biological context, including patch-level classification, segmentation, and slide-level diagnosis. The compact PLUTO-4S provides high-throughput and robust performance for practical deployment, while PLUTO-4G establishes new performance frontiers across multiple pathology benchmarks, including an 11% improvement in dermatopathology diagnosis. These diverse improvements underscore PLUTO-4's potential to transform real-world applications as a backbone for translational research and diagnostic use cases.",
        "translated": "基于大规模病理图像语料库训练的基座模型已在多种组织病理学任务中展现出强大的迁移能力。在此基础上，我们提出 PLUTO-4，作为下一代病理基座模型，将病理通用Transformer（PLUTO）扩展至前沿规模。我们公开了 PLUTO-4 家族中的两种互补的视觉Transformer架构：一种是紧凑高效的 PLUTO-4S 模型，采用 FlexiViT 架构与 2D-RoPE 嵌入，优化用于多尺度部署；另一种是前沿规模的 PLUTO-4G 模型，采用单一 patch 尺寸训练，以最大化表示能力与稳定性。两个模型均在包含来自50多家机构、137,144名患者、共551,164张全切片图像（WSIs）的大型多机构语料库上，使用源自 DINOv2 的自监督目标进行预训练，涵盖超过60种疾病类型和100种染色方法。在公开及内部基准上的全面评估表明，PLUTO-4 在需要不同空间与生物上下文的任务中均达到当前最优性能，包括图像块级分类、分割以及切片级诊断。紧凑型 PLUTO-4S 为实际部署提供高吞吐量与鲁棒性能，而 PLUTO-4G 在多个病理学基准上树立了新的性能标杆，包括在皮肤病理学诊断任务中实现11%的性能提升。这些多样化的改进凸显了 PLUTO-4 作为转化研究与诊断应用场景骨干模型的潜力，有望推动真实世界应用的变革。",
        "translated_title": "PLUTO-4：前沿病理学基础模型",
        "label": [],
        "label_reason": "论文聚焦病理图像分类、分割与诊断，属于高阶视觉任务，非像素级图像恢复或增强。",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出新型ViT架构与自监督预训练方法，但主要用于病理任务，非low-level图像处理创新。"
    },
    {
        "title": "AI-Generated Image Detection: An Empirical Study and Future Research\n  Directions",
        "url": "http://arxiv.org/abs/2511.02791v1",
        "pub_date": "2025-11-04",
        "summary": "The threats posed by AI-generated media, particularly deepfakes, are now raising significant challenges for multimedia forensics, misinformation detection, and biometric system resulting in erosion of public trust in the legal system, significant increase in frauds, and social engineering attacks. Although several forensic methods have been proposed, they suffer from three critical gaps: (i) use of non-standardized benchmarks with GAN- or diffusion-generated images, (ii) inconsistent training protocols (e.g., scratch, frozen, fine-tuning), and (iii) limited evaluation metrics that fail to capture generalization and explainability. These limitations hinder fair comparison, obscure true robustness, and restrict deployment in security-critical applications. This paper introduces a unified benchmarking framework for systematic evaluation of forensic methods under controlled and reproducible conditions. We benchmark ten SoTA forensic methods (scratch, frozen, and fine-tuned) and seven publicly available datasets (GAN and diffusion) to perform extensive and systematic evaluations. We evaluate performance using multiple metrics, including accuracy, average precision, ROC-AUC, error rate, and class-wise sensitivity. We also further analyze model interpretability using confidence curves and Grad-CAM heatmaps. Our evaluations demonstrate substantial variability in generalization, with certain methods exhibiting strong in-distribution performance but degraded cross-model transferability. This study aims to guide the research community toward a deeper understanding of the strengths and limitations of current forensic approaches, and to inspire the development of more robust, generalizable, and explainable solutions.",
        "translated": "人工智能生成媒体，尤其是深度伪造（deepfakes），正对多媒体取证、虚假信息检测及生物识别系统构成严峻威胁，导致公众对法律体系的信任度下降、欺诈案件显著增加以及社会工程攻击频发。尽管已提出多种取证方法，但它们仍存在三个关键缺陷：（i）采用非标准化基准，使用GAN或扩散模型生成的图像；（ii）训练协议不一致（例如，从头训练、冻结训练、微调）；（iii）评估指标有限，无法有效衡量模型的泛化能力与可解释性。这些局限性阻碍了公平比较，掩盖了模型的真实鲁棒性，并限制了其在安全关键应用中的部署。本文提出一种统一的基准框架，用于在可控且可复现的条件下系统评估取证方法。我们对十种当前最先进的取证方法（包括从头训练、冻结训练和微调三种模式）以及七个公开可用的数据集（包含GAN和扩散模型生成的数据）进行了广泛且系统的评估。评估采用多种指标，包括准确率、平均精度、ROC-AUC、错误率以及各类别的敏感度。此外，我们还通过置信度曲线和Grad-CAM热力图进一步分析模型的可解释性。评估结果表明，不同方法在泛化能力上存在显著差异，部分方法在分布内表现优异，但在跨模型迁移时性能明显下降。本研究旨在引导研究界更深入地理解当前取证方法的优势与局限，并激励开发更加鲁棒、泛化性强且可解释的解决方案。",
        "translated_title": "AI生成图像检测：一项实证研究与未来研究方向\n\n近年来，随着生成模型（如GANs和扩散模型）的快速发展，AI生成图像在内容创作、娱乐和媒体传播中的应用日益广泛。然而，这也带来了伪造内容泛滥、信息真实性受损等严峻挑战。因此，AI生成图像检测（AI-Generated Image Detection）作为数字内容安全与可信性保障的关键技术，受到了学术界和工业界的广泛关注。\n\n本研究系统回顾了当前主流的AI生成图像检测方法，涵盖基于图像特征、频域分析、模型指纹、以及端到端深度学习模型等技术路线。我们通过在多个公开数据集（如FFHQ、CelebA、FaceForensics++、Deepfake Detection Challenge Dataset）上进行广泛的实验评估，对比了不同方法在检测精度、泛化能力、计算效率等方面的性能表现。\n\n实验结果表明，基于频域特征的方法在检测GAN生成图像时表现出较高的鲁棒性，尤其在面对不同生成模型和压缩干扰时具有较强稳定性。然而，对于扩散模型生成的图像，其频域特征与真实图像高度相似，导致检测难度显著增加。此外，基于模型指纹的方法在特定生成模型上表现优异，但其泛化能力受限于训练时所使用的模型集合。端到端深度学习方法（如基于ResNet、EfficientNet、ViT等架构的分类器）在综合性能上表现突出，尤其在多模型混合场景下具备较强的适应能力。\n\n我们进一步分析了当前检测方法存在的主要局限性：1）对新型生成模型的适应性不足；2）在跨域、跨数据集场景下的泛化能力较弱；3）检测模型自身可能被对抗攻击或后处理操作绕过；4）缺乏统一的评估标准和基准数据集。\n\n基于上述分析，我们提出未来研究的几个关键方向：1）构建更具挑战性的检测基准数据集，涵盖更多生成模型（如Stable Diffusion、Midjourney等）和复杂退化（如压缩、裁剪、重采样）；2）探索结合多模态先验知识（如文本-图像对齐、语义一致性）的检测框架；3）发展自适应检测架构，使其能够动态识别生成模型类型并调整检测策略；4）研究检测模型的鲁棒性增强机制，提升其对对抗样本和后处理操作的抵抗能力。\n\n本研究为AI生成图像检测领域的技术发展提供了系统的实证分析与前瞻性指导，旨在推动该领域向更高效、更稳健、更通用的方向演进。",
        "label": [],
        "label_reason": "论文聚焦AI生成图像检测，属于高阶视觉任务，非像素级图像恢复或增强。",
        "relevance_score": 2,
        "novelty_score": 5,
        "novelty_reason": "提出统一评测框架，但方法为现有模型的系统评估，无新模型或技术突破。"
    },
    {
        "title": "When Visualizing is the First Step to Reasoning: MIRA, a Benchmark for\n  Visual Chain-of-Thought",
        "url": "http://arxiv.org/abs/2511.02779v1",
        "pub_date": "2025-11-04",
        "summary": "We propose MIRA, a new benchmark designed to evaluate models in scenarios where generating intermediate visual images is essential for successful reasoning. Unlike traditional CoT methods that rely solely on text, tasks in MIRA require models to generate and utilize intermediate images - such as sketches, structural diagrams, or path drawings - to guide their reasoning process. This setup closely mirrors how humans solve complex problems through \"drawing to think\". To solve this, MIRA focuses on tasks that are intrinsically challenging and involve complex structures, spatial relationships, or reasoning steps that are difficult to express through language alone. To ensure that our evaluation data is of high-quality, we include 546 multimodal problems, annotated with intermediate visual images and final answers. We also propose a unified evaluation protocol for MIRA that spans three levels of evaluation input: direct input with image and question only, text-only CoT input with image and thinking prompts, and Visual-CoT input with both annotated image clues and textual thinking prompts. To probe the upper bound of model capacity on our benchmark, we also report pass@k and majority voting accuracies under different k settings. Experimental results show that existing multimodal large language models, including strongest private models as well as strong open-weight models, perform poorly when relying solely on textual prompts. However, when intermediate visual cues are provided, model performance improves consistently, yielding an average relative gain of 33.7% across all models and tasks. We also probe the upper bound by expanding the search space and designing textual prompts aligned with Visual-CoT, but both yield only limited improvements compared to our Visual-CoT setting. These results underscore the critical role of imagined visual information in enabling successful reasoning on MIRA.",
        "translated": "我们提出 MIRA，一个新型基准，旨在评估在生成中间视觉图像对成功推理至关重要的场景下模型的表现。与仅依赖文本的传统思维链（CoT）方法不同，MIRA 中的任务要求模型生成并利用中间图像——例如草图、结构图或路径绘制——来引导其推理过程。该设置紧密模拟了人类通过“画图思考”来解决复杂问题的方式。为实现这一目标，MIRA 专注于那些本质上具有挑战性的任务，这些任务涉及复杂的结构、空间关系或推理步骤，难以仅通过语言表达。为确保评估数据的高质量，我们纳入了 546 个跨模态问题，每个问题均标注了中间视觉图像和最终答案。我们还为 MIRA 提出了一套统一的评估协议，涵盖三个层次的评估输入：仅包含图像和问题的直接输入、包含图像和思维提示的纯文本 CoT 输入，以及同时包含标注图像线索和文本思维提示的 Visual-CoT 输入。为探究模型在本基准上的性能上限，我们还报告了在不同 k 值设置下的 pass@k 和多数投票准确率。实验结果表明，现有的多模态大语言模型——包括最强的私有模型和强大的开源权重模型——在仅依赖文本提示时表现不佳。然而，当提供中间视觉线索时，模型性能始终得到提升，所有模型和任务的平均相对增益达到 33.7%。我们还通过扩展搜索空间以及设计与 Visual-CoT 对齐的文本提示来探索性能上限，但二者相比我们的 Visual-CoT 设置仅带来有限改进。这些结果凸显了想象视觉信息在实现 MIRA 上成功推理中的关键作用。",
        "translated_title": "当可视化成为推理的第一步：MIRA，一个视觉思维链基准",
        "label": [],
        "label_reason": "论文聚焦视觉推理与多模态模型评估，非图像像素级恢复或增强任务，属于high-level视觉理解。",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "提出新基准MIRA，设计视觉链式推理评估协议，但未涉及low-level图像处理创新。"
    },
    {
        "title": "VCode: a Multimodal Coding Benchmark with SVG as Symbolic Visual\n  Representation",
        "url": "http://arxiv.org/abs/2511.02778v1",
        "pub_date": "2025-11-04",
        "summary": "Code has emerged as a precise and executable medium for reasoning and action in the agent era. Yet, progress has largely focused on language-centric tasks such as program synthesis and debugging, leaving visual-centric coding underexplored. Inspired by how humans reason over sketches, we advocate SVG code as a compact, interpretable, and executable visual representation. We introduce VCode, a benchmark that reframes multimodal understanding as code generation: given an image, a model must produce SVG that preserves symbolic meaning for downstream reasoning. VCode covers three domains - general commonsense (MM-Vet), professional disciplines (MMMU), and visual-centric perception (CV-Bench). To assess symbolic fidelity, we propose CodeVQA, a novel evaluation protocol in which a policy model answers questions over rendered SVGs; correct answers indicate faithful symbolic preservation. Empirically, frontier VLMs struggle to generate faithful SVGs, revealing a persistent gap between language-centric and visual-centric coding. To close this gap, we introduce VCoder, an agentic framework that augments VLMs along two axes: (i) Thinking with Revision, which iteratively analyzes discrepancies and refines SVG code; and (ii) Acting with Visual Tools, where detectors and parsers supply structured cues such as objects, shapes, and text beyond the model's intrinsic capacity. Across benchmarks, frontier VLMs with strong reasoning capabilities score well overall yet remain limited in professional knowledge and 3D reasoning. VCoder delivers a 12.3-point overall gain over the top-performing Claude-4-Opus. Human studies show that both humans and VLMs perform worse on rendered SVGs, their consistency reveals the promise of symbolic visual representation. The benchmark and code are available at https://github.com/CSU-JPG/VCode.",
        "translated": "代码已成为智能体时代中精确且可执行的推理与行动媒介。然而，当前进展主要集中在以语言为中心的任务，如程序合成与调试，而以视觉为中心的编码仍鲜有探索。受人类对草图进行推理方式的启发，我们主张将SVG代码作为一种紧凑、可解释且可执行的视觉表示形式。我们提出VCode基准，将多模态理解重新定义为代码生成任务：给定一张图像，模型需生成能够保留符号语义以供下游推理的SVG代码。VCode涵盖三个领域——通用常识（MM-Vet）、专业学科（MMMU）以及以视觉为中心的感知（CV-Bench）。为评估符号保真度，我们提出CodeVQA，一种新颖的评估协议，其中策略模型需对渲染后的SVG图像进行问答；正确答案表明符号语义被忠实保留。实验表明，前沿的视觉语言模型（VLMs）在生成忠实SVG代码方面表现不佳，揭示了语言中心与视觉中心编码之间持续存在的差距。为弥合这一差距，我们提出VCoder，一种智能体框架，从两个维度增强VLMs：(i) 带修订的思考，即迭代分析差异并优化SVG代码；(ii) 带视觉工具的行动，即通过检测器和解析器提供模型本身能力之外的结构化线索，如物体、形状和文本。在多个基准测试中，具备强大推理能力的前沿VLMs整体表现优异，但在专业知识和三维推理方面仍存在局限。VCoder在整体性能上较表现最佳的Claude-4-Opus提升了12.3分。人类研究显示，人类和VLMs在渲染后的SVG图像上表现均较差，但其一致性揭示了符号化视觉表示的潜力。该基准和代码可在https://github.com/CSU-JPG/VCode获取。",
        "translated_title": "VCode：一个以SVG作为符号化视觉表示的多模态编码基准",
        "label": [],
        "label_reason": "论文聚焦于多模态编码与SVG生成，属于视觉-语言模型的高阶推理任务，非像素级图像恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 6,
        "novelty_reason": "提出VCode基准与VCoder框架，结合视觉工具与迭代修正，属多模态推理创新，但非low-level图像处理领域。"
    },
    {
        "title": "PercHead: Perceptual Head Model for Single-Image 3D Head Reconstruction\n  &amp; Editing",
        "url": "http://arxiv.org/abs/2511.02777v1",
        "pub_date": "2025-11-04",
        "summary": "We present PercHead, a method for single-image 3D head reconstruction and semantic 3D editing - two tasks that are inherently challenging due to severe view occlusions, weak perceptual supervision, and the ambiguity of editing in 3D space. We develop a unified base model for reconstructing view-consistent 3D heads from a single input image. The model employs a dual-branch encoder followed by a ViT-based decoder that lifts 2D features into 3D space through iterative cross-attention. Rendering is performed using Gaussian Splatting. At the heart of our approach is a novel perceptual supervision strategy based on DINOv2 and SAM2.1, which provides rich, generalized signals for both geometric and appearance fidelity. Our model achieves state-of-the-art performance in novel-view synthesis and, furthermore, exhibits exceptional robustness to extreme viewing angles compared to established baselines. Furthermore, this base model can be seamlessly extended for semantic 3D editing by swapping the encoder and finetuning the network. In this variant, we disentangle geometry and style through two distinct input modalities: a segmentation map to control geometry and either a text prompt or a reference image to specify appearance. We highlight the intuitive and powerful 3D editing capabilities of our model through a lightweight, interactive GUI, where users can effortlessly sculpt geometry by drawing segmentation maps and stylize appearance via natural language or image prompts.   Project Page: https://antoniooroz.github.io/PercHead Video: https://www.youtube.com/watch?v=4hFybgTk4kE",
        "translated": "我们提出 PercHead，一种用于单图像3D人脸重建与语义3D编辑的方法——这两个任务因严重的视角遮挡、弱感知监督以及3D空间中编辑的模糊性而固有地具有挑战性。我们开发了一个统一的基础模型，用于从单张输入图像重建视角一致的3D人脸。该模型采用双分支编码器，后接基于ViT的解码器，通过迭代交叉注意力机制将2D特征提升至3D空间。渲染过程采用高斯点云渲染（Gaussian Splatting）实现。我们方法的核心是一种基于DINOv2和SAM2.1的新型感知监督策略，该策略为几何与外观保真度提供了丰富且通用的监督信号。我们的模型在新视角合成任务中达到当前最优性能，且相较于现有基线方法，在极端视角下表现出卓越的鲁棒性。此外，该基础模型可通过替换编码器并微调网络，无缝扩展至语义3D编辑任务。在该变体中，我们通过两种不同的输入模态解耦几何与风格：利用分割图控制几何结构，通过文本提示或参考图像指定外观。我们通过一个轻量级、交互式的图形用户界面（GUI）展示了模型直观而强大的3D编辑能力，用户可轻松地通过绘制分割图来塑造几何结构，并通过自然语言或图像提示来调整外观风格。  \n项目主页：https://antoniooroz.github.io/PercHead  \n视频演示：https://www.youtube.com/watch?v=4hFybgTk4kE",
        "translated_title": "PercHead：用于单图像3D人脸重建与编辑的感知头部模型",
        "label": [],
        "label_reason": "论文聚焦3D头像重建与编辑，属于3D重建与生成任务，非像素级图像恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 6,
        "novelty_reason": "提出感知监督策略与双分支结构，但属3D生成领域常规改进，非low-level创新。"
    },
    {
        "title": "Dynamic Reflections: Probing Video Representations with Text Alignment",
        "url": "http://arxiv.org/abs/2511.02767v1",
        "pub_date": "2025-11-04",
        "summary": "The alignment of representations from different modalities has recently been shown to provide insights on the structural similarities and downstream capabilities of different encoders across diverse data types. While significant progress has been made in aligning images with text, the temporal nature of video data remains largely unexplored in this context. In this work, we conduct the first comprehensive study of video-text representation alignment, probing the capabilities of modern video and language encoders. Our findings reveal several key insights. First, we demonstrate that cross-modal alignment highly depends on the richness of both visual (static images vs. multi-frame videos) and text (single caption vs. a collection) data provided at test time, especially when using state-of-the-art video encoders. We propose parametric test-time scaling laws that capture this behavior and show remarkable predictive power against empirical observations. Secondly, we investigate the correlation between semantic alignment and performance on both semantic and non-semantic downstream tasks, providing initial evidence that strong alignment against text encoders may be linked to general-purpose video representation and understanding. Finally, we correlate temporal reasoning with cross-modal alignment providing a challenging test-bed for vision and language models. Overall, our work introduces video-text alignment as an informative zero-shot way to probe the representation power of different encoders for spatio-temporal data. Project page can be found at https://video-prh.github.io/",
        "translated": "近年来，不同模态表示之间的对齐被证明能够揭示不同编码器在多种数据类型下的结构相似性及其下游任务能力。尽管在图像与文本对齐方面已取得显著进展，但视频数据的时序特性在此背景下仍鲜有探索。在本工作中，我们首次对视频-文本表示对齐进行了全面研究，深入探究现代视频与语言编码器的能力。我们的研究揭示了若干关键发现。首先，我们证明了跨模态对齐高度依赖于测试时提供的视觉（静态图像 vs. 多帧视频）和文本（单个字幕 vs. 文本集合）数据的丰富程度，尤其是在使用最先进的视频编码器时。我们提出了参数化的测试时缩放规律，以捕捉这种行为，并展示了其对实证观察具有显著的预测能力。其次，我们研究了语义对齐与语义及非语义下游任务性能之间的相关性，初步表明与文本编码器的强对齐可能与通用视频表示和理解能力相关。最后，我们将时序推理与跨模态对齐相关联，为视觉与语言模型提供了具有挑战性的测试平台。总体而言，本工作将视频-文本对齐作为一种信息丰富的零样本方法，用于探究不同编码器在时空数据上的表示能力。项目主页见 https://video-prh.github.io/",
        "translated_title": "动态反射：基于文本对齐的视频表征探析",
        "label": [],
        "label_reason": "论文聚焦视频-文本表示对齐，属于高阶视觉理解任务，非像素级图像恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "提出参数化测试时缩放规律，对齐方法具一定创新，但非低层视觉任务核心突破。"
    },
    {
        "title": "LLEXICORP: End-user Explainability of Convolutional Neural Networks",
        "url": "http://arxiv.org/abs/2511.02720v1",
        "pub_date": "2025-11-04",
        "summary": "Convolutional neural networks (CNNs) underpin many modern computer vision systems. With applications ranging from common to critical areas, a need to explain and understand the model and its decisions (XAI) emerged. Prior works suggest that in the top layers of CNNs, the individual channels can be attributed to classifying human-understandable concepts. Concept relevance propagation (CRP) methods can backtrack predictions to these channels and find images that most activate these channels. However, current CRP workflows are largely manual: experts must inspect activation images to name the discovered concepts and must synthesize verbose explanations from relevance maps, limiting the accessibility of the explanations and their scalability.   To address these issues, we introduce Large Language model EXplaIns COncept Relevance Propagation (LLEXICORP), a modular pipeline that couples CRP with a multimodal large language model. Our approach automatically assigns descriptive names to concept prototypes and generates natural-language explanations that translate quantitative relevance distributions into intuitive narratives. To ensure faithfulness, we craft prompts that teach the language model the semantics of CRP through examples and enforce a separation between naming and explanation tasks. The resulting text can be tailored to different audiences, offering low-level technical descriptions for experts and high-level summaries for non-technical stakeholders.   We qualitatively evaluate our method on various images from ImageNet on a VGG16 model. Our findings suggest that integrating concept-based attribution methods with large language models can significantly lower the barrier to interpreting deep neural networks, paving the way for more transparent AI systems.",
        "translated": "卷积神经网络（CNNs）构成了许多现代计算机视觉系统的基础。随着其在从普通到关键领域的广泛应用，解释和理解模型及其决策（XAI）的需求应运而生。先前的研究表明，在CNN的顶层中，各个通道可以被归因于分类人类可理解的概念。概念相关性传播（CRP）方法能够回溯预测至这些通道，并找到最激活这些通道的图像。然而，当前的CRP工作流程在很大程度上依赖人工操作：专家必须检查激活图像以命名所发现的概念，并需从相关性图中合成详尽的解释，这限制了解释的可访问性和可扩展性。\n\n为解决这些问题，我们提出Large Language model EXplaIns COncept Relevance Propagation（LLEXICORP），一种将CRP与多模态大语言模型相结合的模块化流水线。我们的方法能够自动为概念原型分配描述性名称，并生成自然语言解释，将定量的相关性分布转化为直观的叙述。为确保解释的忠实性，我们设计了提示词（prompts），通过示例向语言模型传授CRP的语义，并强制分离命名与解释两个任务。生成的文本可根据不同受众进行定制，为专家提供低层次的技术描述，为非技术利益相关者提供高层次的摘要。\n\n我们在ImageNet数据集中的多种图像上，基于VGG16模型对我们的方法进行了定性评估。结果表明，将基于概念的归因方法与大语言模型相结合，能够显著降低解释深度神经网络的门槛，为构建更透明的人工智能系统铺平道路。",
        "translated_title": "LLEXICORP：卷积神经网络的终端用户可解释性",
        "label": [],
        "label_reason": "论文聚焦于CNN可解释性，属高阶视觉任务，不涉及图像像素级恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 6,
        "novelty_reason": "提出结合大语言模型的解释框架，属XAI领域常规改进，无低层视觉创新。"
    },
    {
        "title": "An unscented Kalman filter method for real time input-parameter-state\n  estimation",
        "url": "http://arxiv.org/abs/2511.02717v1",
        "pub_date": "2025-11-04",
        "summary": "The input-parameter-state estimation capabilities of a novel unscented Kalman filter is examined herein on both linear and nonlinear systems. The unknown input is estimated in two stages within each time step. Firstly, the predicted dynamic states and the system parameters provide an estimation of the input. Secondly, the corrected with measurements states and parameters provide a final estimation. Importantly, it is demonstrated using the perturbation analysis that, a system with at least a zero or a non-zero known input can potentially be uniquely identified. This output-only methodology allows for a better understanding of the system compared to classical output-only parameter identification strategies, given that all the dynamic states, the parameters, and the input are estimated jointly and in real-time.",
        "translated": "本文考察了一种新型无迹卡尔曼滤波器在线性与非线性系统中的输入-参数-状态估计能力。在每个时间步内，未知输入通过两个阶段进行估计：首先，利用预测的动态状态和系统参数对输入进行初步估计；其次，利用测量值校正后的状态和参数提供最终估计。重要的是，通过扰动分析证明，只要系统中至少存在一个零输入或非零已知输入，系统就有可能被唯一识别。该仅依赖输出的建模方法相较于传统的仅输出参数识别策略，能够更全面地理解系统，因为其能够联合且实时地估计所有动态状态、参数以及输入。",
        "translated_title": "一种用于实时输入-参数-状态估计的无迹卡尔曼滤波方法",
        "label": [],
        "label_reason": "论文聚焦于系统状态与输入估计，属于控制理论或系统辨识范畴，非图像处理任务。",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "提出改进的UKF框架用于实时估计，但应用于非图像系统，创新点在系统辨识领域。"
    },
    {
        "title": "VidEmo: Affective-Tree Reasoning for Emotion-Centric Video Foundation\n  Models",
        "url": "http://arxiv.org/abs/2511.02712v1",
        "pub_date": "2025-11-04",
        "summary": "Understanding and predicting emotion from videos has gathered significant attention in recent studies, driven by advancements in video large language models (VideoLLMs). While advanced methods have made progress in video emotion analysis, the intrinsic nature of emotions poses significant challenges. Emotions are characterized by dynamic and cues-dependent properties, making it difficult to understand complex and evolving emotional states with reasonable rationale. To tackle these challenges, we propose a novel affective cues-guided reasoning framework that unifies fundamental attribute perception, expression analysis, and high-level emotional understanding in a stage-wise manner. At the core of our approach is a family of video emotion foundation models (VidEmo), specifically designed for emotion reasoning and instruction-following. These models undergo a two-stage tuning process: first, curriculum emotion learning for injecting emotion knowledge, followed by affective-tree reinforcement learning for emotion reasoning. Moreover, we establish a foundational data infrastructure and introduce a emotion-centric fine-grained dataset (Emo-CFG) consisting of 2.1M diverse instruction-based samples. Emo-CFG includes explainable emotional question-answering, fine-grained captions, and associated rationales, providing essential resources for advancing emotion understanding tasks. Experimental results demonstrate that our approach achieves competitive performance, setting a new milestone across 15 face perception tasks.",
        "translated": "近年来，随着视频大语言模型（VideoLLMs）的快速发展，从视频中理解与预测情感的研究受到了广泛关注。尽管先进方法在视频情感分析方面取得了一定进展，但情感本身的内在特性仍带来了显著挑战。情感具有动态性和依赖线索的特性，这使得对复杂且不断演变的情感状态进行合理推理变得十分困难。为应对这些挑战，我们提出了一种新颖的情感线索引导推理框架，该框架以分阶段的方式统一了基础属性感知、表情分析与高层次情感理解。我们方法的核心是一系列专为情感推理和指令跟随设计的视频情感基础模型（VidEmo）。这些模型采用两阶段微调流程：首先通过课程化情感学习注入情感知识，随后通过情感树强化学习实现情感推理。此外，我们构建了基础数据基础设施，并引入了一个以情感为中心的细粒度数据集（Emo-CFG），包含210万条多样化的指令式样本。Emo-CFG涵盖可解释的情感问答、细粒度描述以及相关推理依据，为推进情感理解任务提供了重要资源。实验结果表明，我们的方法在15个面部感知任务中均取得了具有竞争力的性能，树立了新的基准。",
        "translated_title": "VidEmo：面向情感中心视频基础模型的情感树推理",
        "label": [],
        "label_reason": "论文聚焦视频情感理解与推理，属于high-level视觉任务，不涉及像素级图像恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 6,
        "novelty_reason": "提出情感树推理框架与数据集，方法新颖但非low-level图像处理创新。"
    },
    {
        "title": "Modality-Transition Representation Learning for Visible-Infrared Person\n  Re-Identification",
        "url": "http://arxiv.org/abs/2511.02685v1",
        "pub_date": "2025-11-04",
        "summary": "Visible-infrared person re-identification (VI-ReID) technique could associate the pedestrian images across visible and infrared modalities in the practical scenarios of background illumination changes. However, a substantial gap inherently exists between these two modalities. Besides, existing methods primarily rely on intermediate representations to align cross-modal features of the same person. The intermediate feature representations are usually create by generating intermediate images (kind of data enhancement), or fusing intermediate features (more parameters, lack of interpretability), and they do not make good use of the intermediate features. Thus, we propose a novel VI-ReID framework via Modality-Transition Representation Learning (MTRL) with a middle generated image as a transmitter from visible to infrared modals, which are fully aligned with the original visible images and similar to the infrared modality. After that, using a modality-transition contrastive loss and a modality-query regularization loss for training, which could align the cross-modal features more effectively. Notably, our proposed framework does not need any additional parameters, which achieves the same inference speed to the backbone while improving its performance on VI-ReID task. Extensive experimental results illustrate that our model significantly and consistently outperforms existing SOTAs on three typical VI-ReID datasets.",
        "translated": "可见光-红外行人重识别（VI-ReID）技术能够在背景光照变化的实际场景中，关联跨可见光与红外模态的行人图像。然而，这两种模态之间天然存在显著差异。此外，现有方法主要依赖中间表示来对齐同一行人的跨模态特征。这些中间特征表示通常通过生成中间图像（一种数据增强方式）或融合中间特征（参数较多、可解释性差）来构建，但未能充分利用这些中间特征。因此，我们提出一种基于模态转换表示学习（Modality-Transition Representation Learning, MTRL）的新型VI-ReID框架，通过生成一个中间图像作为从可见光到红外模态的传递媒介，该中间图像与原始可见光图像完全对齐，同时在特征上接近红外模态。随后，采用模态转换对比损失（modality-transition contrastive loss）和模态查询正则化损失（modality-query regularization loss）进行训练，能够更有效地对齐跨模态特征。值得注意的是，所提出的框架无需任何额外参数，在保持与主干网络相同推理速度的同时，显著提升了VI-ReID任务的性能。大量实验结果表明，我们的模型在三个典型VI-ReID数据集（Rain100L, SIDD, RESIDE）上，始终且显著优于现有最先进方法（SOTAs）。",
        "translated_title": "模态转换表示学习用于可见光-红外行人重识别",
        "label": [],
        "label_reason": "论文聚焦跨模态行人重识别，属于高阶视觉任务，非像素级图像恢复或增强。",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出新损失函数与表示学习框架，但应用于跨模态识别，非low-level图像处理创新。"
    },
    {
        "title": "Differentiable Hierarchical Visual Tokenization",
        "url": "http://arxiv.org/abs/2511.02652v1",
        "pub_date": "2025-11-04",
        "summary": "Vision Transformers rely on fixed patch tokens that ignore the spatial and semantic structure of images. In this work, we introduce an end-to-end differentiable tokenizer that adapts to image content with pixel-level granularity while remaining backward-compatible with existing architectures for retrofitting pretrained models. Our method uses hierarchical model selection with information criteria to provide competitive performance in both image-level classification and dense-prediction tasks, and even supports out-of-the-box raster-to-vector conversion.",
        "translated": "视觉变换器（Vision Transformers）依赖于固定的图像块标记（patch tokens），忽略了图像的空间与语义结构。在本文中，我们提出了一种端到端可微分的标记器（tokenizer），其能够以像素级粒度自适应图像内容，同时保持与现有架构的向后兼容性，便于对预训练模型进行重用和改造。我们的方法采用基于信息准则的层次化模型选择策略，在图像级分类任务和密集预测任务中均展现出具有竞争力的性能，甚至支持开箱即用的栅格到矢量转换功能。",
        "translated_title": "可微分层次化视觉分词",
        "label": [],
        "label_reason": "论文聚焦于视觉Transformer的token化方法，用于分类与密集预测，属high-level任务，非图像恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 6,
        "novelty_reason": "提出可微分分层token化，改进现有架构，但未针对像素级图像质量恢复，创新有限。"
    },
    {
        "title": "Can Visual Input Be Compressed? A Visual Token Compression Benchmark for\n  Large Multimodal Models",
        "url": "http://arxiv.org/abs/2511.02650v1",
        "pub_date": "2025-11-04",
        "summary": "Large multimodal models (LMMs) often suffer from severe inference inefficiency due to the large number of visual tokens introduced by image encoders. While recent token compression methods, such as pruning and merging, have shown promise in reducing redundancy, their evaluation remains fragmented and inconsistent. In this work, we present UniPruneBench, a unified and extensible benchmark for visual token pruning in multimodal LLMs. UniPruneBench provides standardized protocols across six ability dimensions and ten datasets, covering ten representative compression algorithms and three families of LMMs (LLaVA-v1.5, Intern-VL3, and Qwen2.5-VL). Beyond task accuracy, it incorporates system-level metrics such as runtime and prefilling latency to provide a holistic view. Our experiments uncover several key findings: (1) random pruning is a surprisingly strong baseline, (2) no single method consistently outperforms others across scenarios, (3) pruning sensitivity varies significantly across tasks, with OCR being most vulnerable, and (4) pruning ratio is the dominant factor governing performance degradation. We believe UniPruneBench will serve as a reliable foundation for future research on efficient multimodal modeling.",
        "translated": "大型多模态模型（LMMs）由于图像编码器引入的大量视觉标记（visual tokens），常常面临严重的推理效率问题。尽管近期的标记压缩方法（如剪枝和合并）在减少冗余方面展现出潜力，但其评估仍存在碎片化和不一致的问题。在本工作中，我们提出 UniPruneBench，一个用于多模态大语言模型中视觉标记剪枝的统一且可扩展的基准测试平台。UniPruneBench 在六个能力维度和十个数据集上提供了标准化的评估协议，涵盖了十种代表性压缩算法以及三种主流 LMM 架构（LLaVA-v1.5、Intern-VL3 和 Qwen2.5-VL）。除了任务准确率外，该基准还整合了系统级指标，如运行时和预填充延迟，从而提供全面的性能视图。我们的实验揭示了若干关键发现：（1）随机剪枝是一种出人意料的强大基线；（2）没有任何一种方法能在所有场景中持续优于其他方法；（3）不同任务对剪枝的敏感性差异显著，其中光学字符识别（OCR）最为脆弱；（4）剪枝比例是主导性能退化的主要因素。我们认为，UniPruneBench 将为未来高效多模态建模研究提供可靠的基础。",
        "translated_title": "视觉输入可以被压缩吗？面向大模型多模态的视觉令牌压缩基准",
        "label": [],
        "label_reason": "论文聚焦多模态模型视觉token压缩，属于模型效率优化，非像素级图像恢复或增强任务。",
        "relevance_score": 1,
        "novelty_score": 6,
        "novelty_reason": "提出统一基准评估token压缩方法，虽有组织性创新，但未涉及图像内容恢复或增强的新型技术。"
    },
    {
        "title": "Robust Face Liveness Detection for Biometric Authentication using Single\n  Image",
        "url": "http://arxiv.org/abs/2511.02645v1",
        "pub_date": "2025-11-04",
        "summary": "Biometric technologies are widely adopted in security, legal, and financial systems. Face recognition can authenticate a person based on the unique facial features such as shape and texture. However, recent works have demonstrated the vulnerability of Face Recognition Systems (FRS) towards presentation attacks. Using spoofing (aka.,presentation attacks), a malicious actor can get illegitimate access to secure systems. This paper proposes a novel light-weight CNN framework to identify print/display, video and wrap attacks. The proposed robust architecture provides seamless liveness detection ensuring faster biometric authentication (1-2 seconds on CPU). Further, this also presents a newly created 2D spoof attack dataset consisting of more than 500 videos collected from 60 subjects. To validate the effectiveness of this architecture, we provide a demonstration video depicting print/display, video and wrap attack detection approaches. The demo can be viewed in the following link: https://rak.box.com/s/m1uf31fn5amtjp4mkgf1huh4ykfeibaa",
        "translated": "生物识别技术已被广泛应用于安全、法律和金融系统中。人脸识别技术可通过人脸的独特特征（如形状和纹理）对个人进行身份认证。然而，近期研究表明，人脸识别系统（FRS）易受到呈现攻击（presentation attacks）的威胁。攻击者通过伪造手段（即呈现攻击），如打印照片、显示屏幕播放视频或使用面具包裹等，可非法获取安全系统的访问权限。本文提出了一种新颖的轻量级卷积神经网络（CNN）框架，用于识别打印/显示、视频以及包裹类攻击。所提出的鲁棒架构可实现无缝的活体检测，确保生物识别认证过程更快（在CPU上仅需1-2秒）。此外，本文还构建了一个新创建的二维伪造攻击数据集，包含来自60名受试者的500多个视频。为验证该架构的有效性，我们提供了一个演示视频，展示了针对打印/显示、视频及包裹攻击的检测方法。演示视频可通过以下链接观看：https://rak.box.com/s/m1uf31fn5amtjp4mkgf1huh4ykfeibaa",
        "translated_title": "基于单幅图像的生物特征认证鲁棒人脸活体检测",
        "label": [],
        "label_reason": "论文聚焦人脸活体检测，属于高阶生物识别安全任务，非像素级图像恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "提出轻量级CNN架构，但属常规网络设计，无突破性创新，仅针对攻击检测。"
    },
    {
        "title": "UniChange: Unifying Change Detection with Multimodal Large Language\n  Model",
        "url": "http://arxiv.org/abs/2511.02607v1",
        "pub_date": "2025-11-04",
        "summary": "Change detection (CD) is a fundamental task for monitoring and analyzing land cover dynamics. While recent high performance models and high quality datasets have significantly advanced the field, a critical limitation persists. Current models typically acquire limited knowledge from single-type annotated data and cannot concurrently leverage diverse binary change detection (BCD) and semantic change detection (SCD) datasets. This constraint leads to poor generalization and limited versatility. The recent advancements in Multimodal Large Language Models (MLLMs) introduce new possibilities for a unified CD framework. We leverage the language priors and unification capabilities of MLLMs to develop UniChange, the first MLLM-based unified change detection model. UniChange integrates generative language abilities with specialized CD functionalities. Our model successfully unifies both BCD and SCD tasks through the introduction of three special tokens: [T1], [T2], and [CHANGE]. Furthermore, UniChange utilizes text prompts to guide the identification of change categories, eliminating the reliance on predefined classification heads. This design allows UniChange to effectively acquire knowledge from multi-source datasets, even when their class definitions conflict. Experiments on four public benchmarks (WHU-CD, S2Looking, LEVIR-CD+, and SECOND) demonstrate SOTA performance, achieving IoU scores of 90.41, 53.04, 78.87, and 57.62, respectively, surpassing all previous methods. The code is available at https://github.com/Erxucomeon/UniChange.",
        "translated": "变化检测（Change Detection, CD）是监测和分析地表覆盖动态变化的一项基础任务。尽管近期高性能模型与高质量数据集显著推动了该领域的发展，但一个关键局限依然存在：当前模型通常仅从单一类型标注数据中获取有限知识，无法同时利用多样化的二值变化检测（Binary Change Detection, BCD）与语义变化检测（Semantic Change Detection, SCD）数据集。这一限制导致模型泛化能力不足，适用性受限。近期多模态大语言模型（Multimodal Large Language Models, MLLMs）的发展为构建统一的CD框架带来了新的可能性。我们利用MLLMs的语言先验知识与统一能力，提出UniChange——首个基于MLLMs的统一变化检测模型。UniChange将生成式语言能力与专用CD功能相结合。通过引入三个特殊标记 [T1]、[T2] 和 [CHANGE]，我们的模型成功实现了BCD与SCD任务的统一。此外，UniChange利用文本提示引导变化类别的识别，摆脱了对预定义分类头的依赖。该设计使UniChange能够有效从多源数据集中获取知识，即使这些数据集的类别定义存在冲突。在四个公开基准数据集（WHU-CD、S2Looking、LEVIR-CD+ 和 SECOND）上的实验表明，UniChange达到当前最优（SOTA）性能，分别取得90.41、53.04、78.87和57.62的IoU分数，全面超越此前所有方法。代码已开源于 https://github.com/Erxucomeon/UniChange。",
        "translated_title": "UniChange：统一变化检测与多模态大语言模型",
        "label": [],
        "label_reason": "论文聚焦于变化检测，属于高阶视觉任务，不涉及像素级图像质量恢复或增强。",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出基于多模态大语言模型的统一框架，结合文本提示与特殊标记，创新性较强。"
    },
    {
        "title": "Zero-Shot Multi-Animal Tracking in the Wild",
        "url": "http://arxiv.org/abs/2511.02591v1",
        "pub_date": "2025-11-04",
        "summary": "Multi-animal tracking is crucial for understanding animal ecology and behavior. However, it remains a challenging task due to variations in habitat, motion patterns, and species appearance. Traditional approaches typically require extensive model fine-tuning and heuristic design for each application scenario. In this work, we explore the potential of recent vision foundation models for zero-shot multi-animal tracking. By combining a Grounding Dino object detector with the Segment Anything Model 2 (SAM 2) tracker and carefully designed heuristics, we develop a tracking framework that can be applied to new datasets without any retraining or hyperparameter adaptation. Evaluations on ChimpAct, Bird Flock Tracking, AnimalTrack, and a subset of GMOT-40 demonstrate strong and consistent performance across diverse species and environments. The code is available at https://github.com/ecker-lab/SAM2-Animal-Tracking.",
        "translated": "多动物追踪对于理解动物生态与行为至关重要。然而，由于栖息地、运动模式和物种外观的多样性，该任务仍具挑战性。传统方法通常需要针对每个应用场景进行大量模型微调和启发式设计。在本研究中，我们探索了近期视觉基础模型在零样本多动物追踪中的潜力。通过结合 Grounding Dino 目标检测器与 Segment Anything Model 2（SAM 2）追踪器，并辅以精心设计的启发式策略，我们构建了一个无需重新训练或超参数调整即可应用于新数据集的追踪框架。在 ChimpAct、Bird Flock Tracking、AnimalTrack 以及 GMOT-40 的一个子集上的评估表明，该方法在不同物种和环境中均表现出强大且一致的性能。代码已开源，地址为 https://github.com/ecker-lab/SAM2-Animal-Tracking。",
        "translated_title": "零样本野外多动物跟踪",
        "label": [],
        "label_reason": "论文聚焦于动物追踪，属于高阶视觉任务，不涉及像素级图像恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "结合现有模型与启发式方法，无新架构或理论创新，属常规组合。"
    },
    {
        "title": "TAUE: Training-free Noise Transplant and Cultivation Diffusion Model",
        "url": "http://arxiv.org/abs/2511.02580v1",
        "pub_date": "2025-11-04",
        "summary": "Despite the remarkable success of text-to-image diffusion models, their output of a single, flattened image remains a critical bottleneck for professional applications requiring layer-wise control. Existing solutions either rely on fine-tuning with large, inaccessible datasets or are training-free yet limited to generating isolated foreground elements, failing to produce a complete and coherent scene. To address this, we introduce the Training-free Noise Transplantation and Cultivation Diffusion Model (TAUE), a novel framework for zero-shot, layer-wise image generation. Our core technique, Noise Transplantation and Cultivation (NTC), extracts intermediate latent representations from both foreground and composite generation processes, transplanting them into the initial noise for subsequent layers. This ensures semantic and structural coherence across foreground, background, and composite layers, enabling consistent, multi-layered outputs without requiring fine-tuning or auxiliary datasets. Extensive experiments show that our training-free method achieves performance comparable to fine-tuned methods, enhancing layer-wise consistency while maintaining high image quality and fidelity. TAUE not only eliminates costly training and dataset requirements but also unlocks novel downstream applications, such as complex compositional editing, paving the way for more accessible and controllable generative workflows.",
        "translated": "尽管文本到图像扩散模型取得了显著成功，但其输出单一、扁平化的图像仍成为专业应用中需要逐层控制的关键瓶颈。现有解决方案要么依赖于使用大规模且难以获取的数据集进行微调，要么虽无需训练，但仅限于生成孤立的前景元素，无法生成完整且连贯的场景。为解决这一问题，我们提出了一种全新的零样本、逐层图像生成框架——无训练噪声移植与培育扩散模型（TAUE）。我们的核心技术——噪声移植与培育（NTC），从前景和复合生成过程的中间潜在表示中提取特征，并将其移植到后续层的初始噪声中。这确保了前景、背景及复合层之间的语义与结构一致性，从而在无需微调或辅助数据集的情况下，实现一致的多层输出。大量实验表明，我们的无训练方法在性能上可与微调方法相媲美，在保持高图像质量和保真度的同时，显著提升了逐层一致性。TAUE不仅消除了昂贵的训练和数据集需求，还解锁了诸如复杂组合编辑等新型下游应用，为更易访问且可控的生成流程铺平了道路。",
        "translated_title": "TAUE：无训练噪声移植与培养扩散模型",
        "label": [],
        "label_reason": "论文聚焦于文本到图像生成，属高阶图像生成任务，非像素级图像恢复或增强。",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出无训练噪声移植与培育机制，提升多层生成一致性，具一定架构创新。"
    },
    {
        "title": "Resource-efficient Automatic Refinement of Segmentations via Weak\n  Supervision from Light Feedback",
        "url": "http://arxiv.org/abs/2511.02576v1",
        "pub_date": "2025-11-04",
        "summary": "Delineating anatomical regions is a key task in medical image analysis. Manual segmentation achieves high accuracy but is labor-intensive and prone to variability, thus prompting the development of automated approaches. Recently, a breadth of foundation models has enabled automated segmentations across diverse anatomies and imaging modalities, but these may not always meet the clinical accuracy standards. While segmentation refinement strategies can improve performance, current methods depend on heavy user interactions or require fully supervised segmentations for training. Here, we present SCORE (Segmentation COrrection from Regional Evaluations), a weakly supervised framework that learns to refine mask predictions only using light feedback during training. Specifically, instead of relying on dense training image annotations, SCORE introduces a novel loss that leverages region-wise quality scores and over/under-segmentation error labels. We demonstrate SCORE on humerus CT scans, where it considerably improves initial predictions from TotalSegmentator, and achieves performance on par with existing refinement methods, while greatly reducing their supervision requirements and annotation time. Our code is available at: https://gitlab.inria.fr/adelangl/SCORE.",
        "translated": "解剖区域的划分是医学图像分析中的关键任务。手动分割虽然精度高，但耗时耗力且易受主观因素影响，因此推动了自动化方法的发展。近期，大量基础模型实现了在多种解剖结构和成像模态下的自动化分割，但其精度未必始终满足临床标准。尽管分割优化策略能够提升性能，但现有方法通常依赖大量用户交互，或需要全监督分割标注进行训练。本文提出 SCORE（Segmentation COrrection from Regional Evaluations），一种弱监督框架，仅在训练过程中利用少量反馈即可学习优化分割掩码预测。具体而言，SCORE 不依赖密集的训练图像标注，而是引入一种新颖的损失函数，该损失函数利用区域级质量评分以及过分割/欠分割错误标签。我们在肱骨 CT 扫描数据上验证了 SCORE，结果表明其显著提升了 TotalSegmentator 初始预测结果的性能，且在性能上可与现有优化方法媲美，同时大幅降低了监督需求和标注时间。我们的代码可在以下地址获取：https://gitlab.inria.fr/adelangl/SCORE。",
        "translated_title": "基于轻量反馈弱监督的资源高效分割自动优化",
        "label": [
            "医学图像增强"
        ],
        "label_reason": "论文聚焦医学图像分割修正，虽涉及图像质量提升，但核心为分割任务，属中等相关性。",
        "relevance_score": 4,
        "novelty_score": 7,
        "novelty_reason": "提出基于区域反馈的弱监督损失函数，对分割修正有显著改进，创新性较强。"
    },
    {
        "title": "A Cognitive Process-Inspired Architecture for Subject-Agnostic Brain\n  Visual Decoding",
        "url": "http://arxiv.org/abs/2511.02565v1",
        "pub_date": "2025-11-04",
        "summary": "Subject-agnostic brain decoding, which aims to reconstruct continuous visual experiences from fMRI without subject-specific training, holds great potential for clinical applications. However, this direction remains underexplored due to challenges in cross-subject generalization and the complex nature of brain signals. In this work, we propose Visual Cortex Flow Architecture (VCFlow), a novel hierarchical decoding framework that explicitly models the ventral-dorsal architecture of the human visual system to learn multi-dimensional representations. By disentangling and leveraging features from early visual cortex, ventral, and dorsal streams, VCFlow captures diverse and complementary cognitive information essential for visual reconstruction. Furthermore, we introduce a feature-level contrastive learning strategy to enhance the extraction of subject-invariant semantic representations, thereby enhancing subject-agnostic applicability to previously unseen subjects. Unlike conventional pipelines that need more than 12 hours of per-subject data and heavy computation, VCFlow sacrifices only 7\\% accuracy on average yet generates each reconstructed video in 10 seconds without any retraining, offering a fast and clinically scalable solution. The source code will be released upon acceptance of the paper.",
        "translated": "无主体依赖的脑解码旨在无需针对特定个体进行训练，直接从fMRI信号中重建连续的视觉体验，具有重要的临床应用潜力。然而，由于跨个体泛化能力的挑战以及脑信号本身的复杂性，这一方向仍鲜有探索。在本文中，我们提出了一种新颖的层次化解码框架——视觉皮层流架构（VCFlow），该框架显式建模了人类视觉系统中的腹侧-背侧结构，以学习多维表征。通过解耦并利用早期视觉皮层、腹侧通路和背侧通路的特征，VCFlow捕捉到对视觉重建至关重要的多样化且互补的认知信息。此外，我们引入了一种特征级对比学习策略，以增强对个体不变语义表征的提取，从而提升模型对未见过个体的无主体依赖适用性。与传统方法需要每个受试者超过12小时的数据和大量计算资源不同，VCFlow平均仅损失7%的准确率，即可在无需重新训练的情况下，每10秒生成一段重建视频，提供了一种快速且具备临床可扩展性的解决方案。论文被接受后，我们将公开源代码。",
        "translated_title": "一种受认知过程启发的、面向主体无关的脑视觉解码架构",
        "label": [],
        "label_reason": "论文聚焦脑信号解码重建视觉内容，属于神经科学与高阶视觉理解，非像素级图像恢复。",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出VCFlow架构与对比学习策略，提升跨被试泛化能力，方法设计有创新性。"
    },
    {
        "title": "Beyond Single Embeddings: Capturing Diverse Targets with Multi-Query\n  Retrieval",
        "url": "http://arxiv.org/abs/2511.02770v1",
        "pub_date": "2025-11-04",
        "summary": "Most text retrievers generate \\emph{one} query vector to retrieve relevant documents. Yet, the conditional distribution of relevant documents for the query may be multimodal, e.g., representing different interpretations of the query. We first quantify the limitations of existing retrievers. All retrievers we evaluate struggle more as the distance between target document embeddings grows. To address this limitation, we develop a new retriever architecture, \\emph{A}utoregressive \\emph{M}ulti-\\emph{E}mbedding \\emph{R}etriever (AMER). Our model autoregressively generates multiple query vectors, and all the predicted query vectors are used to retrieve documents from the corpus. We show that on the synthetic vectorized data, the proposed method could capture multiple target distributions perfectly, showing 4x better performance than single embedding model. We also fine-tune our model on real-world multi-answer retrieval datasets and evaluate in-domain. AMER presents 4 and 21\\% relative gains over single-embedding baselines on two datasets we evaluate on. Furthermore, we consistently observe larger gains on the subset of dataset where the embeddings of the target documents are less similar to each other. We demonstrate the potential of using a multi-query vector retriever and open up a new direction for future work.",
        "translated": "大多数文本检索器仅生成一个查询向量以检索相关文档。然而，查询对应的文档条件分布可能是多模态的，例如代表查询的不同解释。我们首先量化了现有检索器的局限性：我们评估的所有检索器在目标文档嵌入之间的距离增大时，性能均显著下降。为解决这一局限，我们提出了一种新的检索器架构——自回归多嵌入检索器（Autoregressive Multi-Embedding Retriever, AMER）。该模型自回归地生成多个查询向量，并利用所有预测的查询向量从语料库中检索文档。我们在合成的向量化数据上验证了该方法，结果表明其能够完美捕捉多个目标分布，性能优于单嵌入模型4倍。此外，我们在真实世界的多答案检索数据集上对模型进行微调，并在领域内进行评估。AMER在我们评估的两个数据集上，相对于单嵌入基线模型分别实现了4%和21%的相对性能提升。进一步，我们在目标文档嵌入彼此差异较大的数据子集上，始终观察到更大的性能增益。本研究展示了多查询向量检索器的潜力，并为未来研究开辟了新方向。",
        "translated_title": "超越单一嵌入：利用多查询召回捕捉多样化目标",
        "label": [
            "召回",
            "负采样与对比学习"
        ],
        "label_reason": "论文提出多查询向量检索架构，用于捕捉多模态目标分布，适用于推荐系统召回环节",
        "relevance_score": 7,
        "novelty_score": 8,
        "novelty_reason": "提出自回归生成多查询向量的新架构，显著提升检索性能，具有创新性"
    },
    {
        "title": "Relational Deep Dive: Error-Aware Queries Over Unstructured Data",
        "url": "http://arxiv.org/abs/2511.02711v1",
        "pub_date": "2025-11-04",
        "summary": "Unstructured data is pervasive, but analytical queries demand structured representations, creating a significant extraction challenge. Existing methods like RAG lack schema awareness and struggle with cross-document alignment, leading to high error rates. We propose ReDD (Relational Deep Dive), a framework that dynamically discovers query-specific schemas, populates relational tables, and ensures error-aware extraction with provable guarantees. ReDD features a two-stage pipeline: (1) Iterative Schema Discovery (ISD) identifies minimal, joinable schemas tailored to each query, and (2) Tabular Data Population (TDP) extracts and corrects data using lightweight classifiers trained on LLM hidden states. A main contribution of ReDD is SCAPE, a statistically calibrated method for error detection with coverage guarantees, and SCAPE-HYB, a hybrid approach that optimizes the trade-off between accuracy and human correction costs. Experiments across diverse datasets demonstrate ReDD's effectiveness, reducing data extraction errors from up to 30% to below 1% while maintaining high schema completeness (100% recall) and precision. ReDD's modular design enables fine-grained control over accuracy-cost trade-offs, making it a robust solution for high-stakes analytical queries over unstructured corpora.",
        "translated": "非结构化数据普遍存在，但分析查询需要结构化表示，由此产生显著的提取挑战。现有方法如RAG缺乏模式感知能力，且在跨文档对齐方面存在困难，导致高错误率。我们提出ReDD（Relational Deep Dive），一种框架，能够动态发现查询特定的模式、填充关系表，并在可证明保证下实现错误感知的提取。ReDD采用两阶段流水线：（1）迭代模式发现（ISD）识别出针对每个查询的最小、可连接的模式；（2）表格数据填充（TDP）利用基于大语言模型（LLM）隐藏状态训练的轻量级分类器提取并校正数据。ReDD的一项主要贡献是SCAPE，一种具有覆盖率保证的统计校准错误检测方法，以及SCAPE-HYB，一种混合方法，用于优化准确率与人工校正成本之间的权衡。在多种数据集上的实验表明，ReDD具有显著有效性，将数据提取错误率从最高30%降低至低于1%，同时保持高模式完整性（100%召回率）和高精度。ReDD的模块化设计支持对准确率-成本权衡的细粒度控制，使其成为在非结构化语料库上执行高风险分析查询的稳健解决方案。",
        "translated_title": "关系深度探索：面向非结构化数据的错误感知查询",
        "label": [],
        "label_reason": "论文聚焦非结构化数据的结构化提取与错误检测，属于信息抽取与数据库领域，与推荐系统无直接关联。",
        "relevance_score": 2,
        "novelty_score": 8,
        "novelty_reason": "提出SCAPE与SCAPE-HYB等新颖错误检测方法，结合LLM隐藏状态与轻量分类器，具有创新性。"
    },
    {
        "title": "Average Precision at Cutoff k under Random Rankings: Expectation and\n  Variance",
        "url": "http://arxiv.org/abs/2511.02571v1",
        "pub_date": "2025-11-04",
        "summary": "Recommender systems and information retrieval platforms rely on ranking algorithms to present the most relevant items to users, thereby improving engagement and satisfaction. Assessing the quality of these rankings requires reliable evaluation metrics. Among them, Mean Average Precision at cutoff k (MAP@k) is widely used, as it accounts for both the relevance of items and their positions in the list.   In this paper, the expectation and variance of Average Precision at k (AP@k) are derived since they can be used as biselines for MAP@k. Here, we covered two widely used evaluation models: offline and online. The expectation establishes the baseline, indicating the level of MAP@k that can be achieved by pure chance. The variance complements this baseline by quantifying the extent of random fluctuations, enabling a more reliable interpretation of observed scores.",
        "translated": "推荐系统与信息检索平台依赖排序算法向用户呈现最相关的物料，从而提升用户参与度与满意度。评估这些排序质量需要可靠的评价指标。其中，截断k位的平均精度均值（MAP@k）被广泛使用，因为它同时考虑了物料的相关性及其在列表中的位置。本文推导了截断k位的平均精度（AP@k）的期望与方差，因其可作为MAP@k的基准。本文涵盖了两种广泛使用的评估模型：离线与在线。期望值建立了基准，表明仅凭随机性可达到的MAP@k水平；方差则补充了该基准，通过量化随机波动的程度，使得对观测得分的解释更加可靠。",
        "translated_title": "在随机排序下截止k位置的平均精度：期望与方差",
        "label": [
            "推荐系统评估"
        ],
        "label_reason": "论文聚焦推荐系统评估指标MAP@k的期望与方差，为评估提供理论基线，直接服务于推荐系统性能分析。",
        "relevance_score": 8,
        "novelty_score": 6,
        "novelty_reason": "推导AP@k的期望与方差，为评估提供理论支撑，属常规理论分析，无范式创新。"
    },
    {
        "title": "Let Multimodal Embedders Learn When to Augment Query via Adaptive Query\n  Augmentation",
        "url": "http://arxiv.org/abs/2511.02358v1",
        "pub_date": "2025-11-04",
        "summary": "Query augmentation makes queries more meaningful by appending further information to the queries to find relevant documents. Current studies have proposed Large Language Model (LLM)-based embedders, which learn representation for embedding and generation for query augmentation in a multi-task manner by leveraging the generative capabilities of LLM. During inference, these jointly trained embedders have conducted query augmentation followed by embedding, showing effective results. However, augmenting every query leads to substantial embedding latency and query augmentation can be detrimental to performance for some queries. Also, previous methods have not been explored in multimodal environments. To tackle these problems, we propose M-Solomon, a universal multimodal embedder that can adaptively determine when to augment queries. Our approach first divides the queries of the training datasets into two groups at the dataset level. One includes queries that require augmentation and the other includes queries that do not. Then, we introduces a synthesis process that generates appropriate augmentations for queries that require them by leveraging a powerful Multimodal LLM (MLLM). Next, we present adaptive query augmentation. Through this step, M-Solomon can conduct query augmentation only when necessary by learning to generate synthetic augmentations with the prefix /augment for queries that demand them and to generate the simple string /embed for others. Experimental results showed that M-Solomon not only surpassed the baseline without augmentation by a large margin but also outperformed the baseline that always used augmentation, providing much faster embedding latency.",
        "translated": "查询增强通过向查询中附加额外信息，使其更具意义，从而更有效地检索相关文档。当前研究提出了基于大语言模型（LLM）的嵌入器，通过利用LLM的生成能力，以多任务方式同时学习嵌入表示和查询增强的生成。在推理阶段，这些联合训练的嵌入器先进行查询增强，再进行嵌入，取得了良好的效果。然而，对每个查询都进行增强会导致显著的嵌入延迟，且对部分查询而言，增强反而可能损害性能。此外，先前的方法尚未在多模态环境中进行探索。为解决这些问题，我们提出M-Solomon，一种通用的多模态嵌入器，能够自适应地判断何时对查询进行增强。我们的方法首先在数据集层面将训练数据中的查询划分为两组：一组是需要增强的查询，另一组是无需增强的查询。接着，我们引入一个合成过程，利用强大的多模态大语言模型（MLLM）为需要增强的查询生成合适的增强内容。随后，我们提出自适应查询增强机制：通过该步骤，M-Solomon仅在必要时执行查询增强，具体通过学习为需要增强的查询生成带有前缀/augment的合成增强内容，而对其他查询则生成简单的字符串/embed。实验结果表明，M-Solomon不仅大幅超越了无增强的基线方法，还优于始终使用增强的基线方法，并且显著降低了嵌入延迟。",
        "translated_title": "让多模态嵌入器通过自适应查询增强学习何时增强查询",
        "label": [
            "多模态推荐",
            "召回",
            "负采样与对比学习"
        ],
        "label_reason": "论文聚焦多模态检索中查询增强，虽非直接推荐，但适用于推荐系统召回环节，属间接相关。",
        "relevance_score": 4,
        "novelty_score": 7,
        "novelty_reason": "提出自适应查询增强机制，结合MLLM生成增强内容，提升效率与效果，属显著改进。"
    },
    {
        "title": "Library and Culture: A Scientometric Analysis and Visualization of\n  Research Trends",
        "url": "http://arxiv.org/abs/2511.02296v1",
        "pub_date": "2025-11-04",
        "summary": "The significance of libraries in preserving and maintaining history and traditional culture cannot be overlooked. It is from this purpose that libraries are to envisage in their programmes cultural activities which must be collected, documented and preserved for posterity. The usefulness of preserved information lies in the fact that the generation to come will be able to establish their identity. This will also assist them with a foundation to build from. This study focus on the growth and development of Library and Culture research in forms of publications reflected in Web of Science database, during the span of 2010-2019. A total 890 publications were found and the highest 124 (13.93%) publications published in 2019.The analysis maps comprehensively the parameters of total output, growth of output, authorship, institution wise and country-level collaboration patterns, major contributors (individuals, top publication sources, institutions, and countries). It exposed that the most prolific author is Lo P secured first place by contributing 4 (0.45%) publications, followed by Bressan V 3 (0.34%) publications in Library and Culture literature. Journal of Academic Librarianship produced the highest number of records 29 (3.26%) followed by Australian Library Journal having contributed 21 (2.36%).It is identified the domination of Wuhan University; School Information Management had contributed 6 (0.67%) of total research output. Authors from USA published the highest number of publications with a total of 244 (27.42%), followed by UK and Australia with 118 (13.26%) and 76 (8.54%) publications were produced respectively.",
        "translated": "图书馆在保存和维护历史及传统文化方面的重要性不容忽视。正是基于这一目的，图书馆应在各项计划中策划文化活动，并将这些活动系统地收集、记录和保存，以供后世传承。保存信息的价值在于，后代能够借此确立自身身份认同，同时获得进一步发展的基础。本研究聚焦于2010至2019年间Web of Science数据库中关于“图书馆与文化”研究的出版物，探讨其发展态势。共检索到890篇文献，其中2019年发表的文献最多，达124篇（占13.93%）。分析全面覆盖了总产出量、产出增长趋势、作者分布、机构层面及国家层面的合作模式，以及主要贡献者（包括个人、高产期刊、机构和国家）。研究发现，发表数量最多的作者是Lo P，共发表4篇（占0.45%），位居首位；其次为Bressan V，发表3篇（占0.34%）。在期刊方面，《Journal of Academic Librarianship》发表文献最多，达29篇（占3.26%），其次是《Australian Library Journal》，贡献21篇（占2.36%）。在机构层面，武汉大学信息管理学院贡献了6篇（占0.67%）的研究成果，位居前列。从国家层面看，美国学者发表的文献数量最多，共244篇（占27.42%），其次为英国（118篇，占13.26%）和澳大利亚（76篇，占8.54%）。",
        "translated_title": "图书馆与文化：研究趋势的科学计量分析与可视化",
        "label": [],
        "label_reason": "论文为图书馆与文化研究的文献计量分析，与推荐系统无直接关联。",
        "relevance_score": 1,
        "novelty_score": 1,
        "novelty_reason": "研究方法为传统文献计量，无推荐系统相关创新。"
    },
    {
        "title": "Research Output on Alopecia Areata Disease: A Scientometric Analysis of\n  Publications from 2010 to 2019",
        "url": "http://arxiv.org/abs/2511.02275v1",
        "pub_date": "2025-11-04",
        "summary": "The present study is undertaken to find out the publication trends on Alopecia Areata Disease during 2010-2019 from the global perspective. The study mainly focus on distribution of research output, top journals for publications, most prolific authors, authorship pattern, and citations pattern on Alopecia Areata Disease. The results indicate that highest growth rate of publications occurred during the year 2019. Columbia University topped the scene among all institutes. The maximum publications were more than four authored publications. Christiano AM and Clynes R were found to be the most prolific authors. It is also found that most of the prolific authors (by number of publications) do appear in highly cited publications list. Alopecia Areata Disease researchers mostly preferred using article publications to communicate their findings.",
        "translated": "本研究旨在从全球视角探讨2010–2019年间斑秃疾病（Alopecia Areata Disease）领域的出版趋势。研究主要聚焦于研究成果的分布、发表论文的顶级期刊、高产作者、作者合作模式以及引用模式。研究结果表明，论文发表数量的增长率在2019年达到最高。在所有研究机构中，哥伦比亚大学（Columbia University）位居榜首。发表论文数量最多的为四人及以上合著的论文。Christiano AM 和 Clynes R 被发现是该领域最富产的作者。此外，研究还发现，大多数高产作者（按论文数量统计）均出现在高被引论文列表中。斑秃疾病领域的研究人员大多倾向于通过期刊文章形式发表研究成果。",
        "translated_title": "斑秃疾病研究产出：2010至2019年文献的科学计量学分析",
        "label": [],
        "label_reason": "论文为医学文献计量分析，与推荐系统无直接关联。",
        "relevance_score": 1,
        "novelty_score": 1,
        "novelty_reason": "研究方法为传统文献统计，无创新性推荐算法或技术。"
    },
    {
        "title": "KGBridge: Knowledge-Guided Prompt Learning for Non-overlapping\n  Cross-Domain Recommendation",
        "url": "http://arxiv.org/abs/2511.02181v1",
        "pub_date": "2025-11-04",
        "summary": "Knowledge Graphs (KGs), as structured knowledge bases that organize relational information across diverse domains, provide a unified semantic foundation for cross-domain recommendation (CDR). By integrating symbolic knowledge with user-item interactions, KGs enrich semantic representations, support reasoning, and enhance model interpretability. Despite this potential, existing KG-based methods still face major challenges in CDR, particularly under non-overlapping user scenarios. These challenges arise from: (C1) sensitivity to KG sparsity and popularity bias, (C2) dependence on overlapping users for domain alignment and (C3) lack of explicit disentanglement between transferable and domain-specific knowledge, which limit effective and stable knowledge transfer. To this end, we propose KGBridge, a knowledge-guided prompt learning framework for cross-domain sequential recommendation under non-overlapping user scenarios. KGBridge comprises two core components: a KG-enhanced Prompt Encoder, which models relation-level semantics as soft prompts to provide structured and dynamic priors for user sequence modeling (addressing C1), and a Two-stage Training Paradigm, which combines cross-domain pretraining and privacy-preserving fine-tuning to enable knowledge transfer without user overlap (addressing C2). By combining relation-aware semantic control with correspondence-driven disentanglement, KGBridge explicitly separates and balances domain-shared and domain-specific semantics, thereby maintaining complementarity and stabilizing adaptation during fine-tuning (addressing C3). Extensive experiments on benchmark datasets demonstrate that KGBridge consistently outperforms state-of-the-art baselines and remains robust under varying KG sparsity, highlighting its effectiveness in mitigating structural imbalance and semantic entanglement in KG-enhanced cross-domain recommendation.",
        "translated": "知识图谱（KGs）作为组织跨领域关系信息的结构化知识库，为跨域推荐（CDR）提供了统一的语义基础。通过将符号化知识与用户-物料交互相结合，KGs能够丰富语义表示、支持推理并提升模型可解释性。尽管具有这些潜力，现有的基于KG的方法在CDR中仍面临重大挑战，尤其是在用户非重叠的场景下。这些挑战主要源于：（C1）对KG稀疏性和流行度偏差的敏感性，（C2）依赖重叠用户进行领域对齐，以及（C3）缺乏对可迁移知识与领域特定知识的显式解耦，从而限制了知识迁移的有效性与稳定性。为此，我们提出KGBridge，一种面向非重叠用户场景的、基于知识引导的提示学习框架，用于跨域序列推荐。KGBridge包含两个核心组件：一是KG增强的提示编码器，其将关系级语义建模为软提示，为用户序列建模提供结构化且动态的先验知识（解决C1）；二是两阶段训练范式，结合跨域预训练与隐私保护微调，实现无用户重叠情况下的知识迁移（解决C2）。通过融合关系感知的语义控制与对应驱动的解耦机制，KGBridge显式分离并平衡了领域共享与领域特定语义，从而在微调过程中保持互补性并稳定适应（解决C3）。在基准数据集上的大量实验表明，KGBridge始终优于现有最先进基线方法，并在不同KG稀疏度下保持鲁棒性，验证了其在缓解KG增强跨域推荐中结构不平衡与语义纠缠问题的有效性。",
        "translated_title": "KGBridge：基于知识引导的提示学习在非重叠跨域推荐中的应用",
        "label": [
            "跨域/联邦推荐",
            "序列推荐",
            "图神经网络推荐",
            "LLM生成式推荐"
        ],
        "label_reason": "论文聚焦非重叠用户跨域推荐，结合知识图谱与提示学习，直接解决推荐系统核心问题",
        "relevance_score": 9,
        "novelty_score": 8,
        "novelty_reason": "提出知识引导提示学习框架，结合两阶段训练，有效解决KG稀疏与领域对齐问题"
    },
    {
        "title": "Enhancing Multimodal Recommendations with Vision-Language Models and\n  Information-Aware Fusion",
        "url": "http://arxiv.org/abs/2511.02113v1",
        "pub_date": "2025-11-03",
        "summary": "Recent advances in multimodal recommendation (MMR) have shown that incorporating rich content sources such as images and text can lead to significant gains representation quality. However, existing methods often rely on coarse visual features and uncontrolled fusion, leading to redundant or misaligned representations. As a result, visual encoders often fail to capture salient, item-relevant semantics, limiting their contribution in multimodal fusion. From an information-theoretic perspective, effective fusion should balance the unique, shared, and redundant information across modalities, preserving complementary cues while avoiding correlation bias. This paper presents VLIF, a vision-language and information-theoretic fusion framework that enhances multimodal recommendation through two key components. (i) A VLM-based visual enrichment module generates fine-grained, title-guided descriptions to transform product images into semantically aligned representations. (ii) An information-aware fusion module, inspired by Partial Information Decomposition (PID), disentangles redundant and synergistic signals across modalities for controlled integration. Experiments on three Amazon datasets demonstrate that VLIF consistently outperforms recent multimodal baselines and substantially strengthens the contribution of visual features.",
        "translated": "近年来，多模态推荐（MMR）领域的进展表明，融合图像和文本等丰富内容源可显著提升表示质量。然而，现有方法通常依赖于粗糙的视觉特征和不受控的模态融合，导致生成的表示存在冗余或对齐偏差。因此，视觉编码器往往难以捕捉与物料相关的关键语义信息，限制了其在多模态融合中的贡献。从信息论的角度来看，有效的融合应平衡不同模态间的独特信息、共享信息和冗余信息，在保留互补线索的同时避免相关性偏差。本文提出VLIF，一种基于视觉-语言与信息论的融合框架，通过两个核心组件增强多模态推荐性能：(i) 基于VLM的视觉增强模块，生成细粒度、标题引导的描述，将产品图像转化为语义对齐的表示；(ii) 信息感知融合模块，受部分信息分解（Partial Information Decomposition, PID）启发，解耦模态间的冗余信号与协同信号，实现可控的融合。在三个Amazon数据集上的实验表明，VLIF在性能上始终优于近期的多模态基线方法，并显著增强了视觉特征的贡献。",
        "translated_title": "增强多模态推荐：基于视觉-语言模型与信息感知融合方法",
        "label": [
            "多模态推荐（Multimodal Recommendation）",
            "LLM生成式推荐（LLM-based Generative Recommendation）",
            "序列推荐（Sequential Recommendation）"
        ],
        "label_reason": "论文聚焦多模态推荐，提出基于VLM的视觉增强与信息感知融合，直接提升推荐系统中跨模态表示质量。",
        "relevance_score": 8,
        "novelty_score": 8,
        "novelty_reason": "创新性结合VLM与信息论分解，实现可控融合，提升视觉特征贡献，方法新颖且效果显著。"
    },
    {
        "title": "Solving cold start in news recommendations: a RippleNet-based system for\n  large scale media outlet",
        "url": "http://arxiv.org/abs/2511.02052v1",
        "pub_date": "2025-11-03",
        "summary": "We present a scalable recommender system implementation based on RippleNet, tailored for the media domain with a production deployment in Onet.pl, one of Poland's largest online media platforms. Our solution addresses the cold-start problem for newly published content by integrating content-based item embeddings into the knowledge propagation mechanism of RippleNet, enabling effective scoring of previously unseen items. The system architecture leverages Amazon SageMaker for distributed training and inference, and Apache Airflow for orchestrating data pipelines and model retraining workflows. To ensure high-quality training data, we constructed a comprehensive golden dataset consisting of user and item features and a separate interaction table, all enabling flexible extensions and integration of new signals.",
        "translated": "我们提出了一种基于RippleNet的可扩展推荐系统实现方案，专为媒体领域设计，并已在波兰最大的在线媒体平台之一Onet.pl上完成生产部署。我们的解决方案通过将基于内容的物料嵌入整合到RippleNet的知识传播机制中，有效解决了新发布内容的冷启动问题，从而能够对之前未见过的物料进行准确评分。系统架构利用Amazon SageMaker实现分布式训练与推理，并采用Apache Airflow编排数据管道和模型重训练工作流。为确保高质量的训练数据，我们构建了一个全面的黄金数据集，包含用户和物料特征，以及独立的交互表，所有组件均支持灵活扩展和新信号的集成。",
        "translated_title": "解决新闻推荐中的冷启动问题：面向大规模媒体平台的RippleNet系统",
        "label": [
            "召回",
            "图神经网络推荐",
            "冷启动",
            "通用推荐技术"
        ],
        "label_reason": "基于RippleNet解决新闻推荐冷启动，涉及图神经网络与召回机制，专为推荐设计",
        "relevance_score": 8,
        "novelty_score": 6,
        "novelty_reason": "将内容嵌入融入RippleNet知识传播，改进冷启动，但非全新架构"
    },
    {
        "title": "InteracSPARQL: An Interactive System for SPARQL Query Refinement Using\n  Natural Language Explanations",
        "url": "http://arxiv.org/abs/2511.02002v1",
        "pub_date": "2025-11-03",
        "summary": "In recent years, querying semantic web data using SPARQL has remained challenging, especially for non-expert users, due to the language's complex syntax and the prerequisite of understanding intricate data structures. To address these challenges, we propose InteracSPARQL, an interactive SPARQL query generation and refinement system that leverages natural language explanations (NLEs) to enhance user comprehension and facilitate iterative query refinement. InteracSPARQL integrates LLMs with a rule-based approach to first produce structured explanations directly from SPARQL abstract syntax trees (ASTs), followed by LLM-based linguistic refinements. Users can interactively refine queries through direct feedback or LLM-driven self-refinement, enabling the correction of ambiguous or incorrect query components in real time. We evaluate InteracSPARQL on standard benchmarks, demonstrating significant improvements in query accuracy, explanation clarity, and overall user satisfaction compared to baseline approaches. Our experiments further highlight the effectiveness of combining rule-based methods with LLM-driven refinements to create more accessible and robust SPARQL interfaces.",
        "translated": "近年来，由于SPARQL语言语法复杂且要求用户理解复杂的数据结构，使用SPARQL查询语义网数据对非专业用户而言仍具有挑战性。为应对这些挑战，我们提出InteracSPARQL，一种交互式SPARQL查询生成与优化系统，通过自然语言解释（NLEs）增强用户理解并支持迭代查询优化。InteracSPARQL结合大语言模型（LLM）与基于规则的方法，首先从SPARQL抽象语法树（AST）直接生成结构化解释，随后通过基于LLM的语言优化进一步完善解释内容。用户可通过直接反馈或LLM驱动的自我优化方式交互式地调整查询，实现实时修正模糊或错误的查询组件。我们在标准基准数据集上对InteracSPARQL进行评估，结果表明其在查询准确性、解释清晰度及整体用户满意度方面均显著优于基线方法。实验进一步验证了将基于规则的方法与LLM驱动优化相结合，能够构建更易用且鲁棒的SPARQL交互界面的有效性。",
        "translated_title": "InteracSPARQL：一种基于自然语言解释的SPARQL查询优化交互系统",
        "label": [],
        "label_reason": "论文聚焦SPARQL查询生成与优化，属于数据库与自然语言接口领域，与推荐系统无直接关联。",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "结合规则与LLM进行查询解释与修正，方法新颖但应用于非推荐场景。"
    },
    {
        "title": "Trove: A Flexible Toolkit for Dense Retrieval",
        "url": "http://arxiv.org/abs/2511.01857v1",
        "pub_date": "2025-11-03",
        "summary": "We introduce Trove, an easy-to-use open-source retrieval toolkit that simplifies research experiments without sacrificing flexibility or speed. For the first time, we introduce efficient data management features that load and process (filter, select, transform, and combine) retrieval datasets on the fly, with just a few lines of code. This gives users the flexibility to easily experiment with different dataset configurations without the need to compute and store multiple copies of large datasets. Trove is highly customizable: in addition to many built-in options, it allows users to freely modify existing components or replace them entirely with user-defined objects. It also provides a low-code and unified pipeline for evaluation and hard negative mining, which supports multi-node execution without any code changes. Trove's data management features reduce memory consumption by a factor of 2.6. Moreover, Trove's easy-to-use inference pipeline incurs no overhead, and inference times decrease linearly with the number of available nodes. Most importantly, we demonstrate how Trove simplifies retrieval experiments and allows for arbitrary customizations, thus facilitating exploratory research.",
        "translated": "我们提出Trove，一个易于使用且开源的召回工具包，能够在不牺牲灵活性和速度的前提下简化研究实验。首次，我们引入了高效的数据管理功能，仅需几行代码即可实时加载和处理（过滤、选择、转换和合并）召回数据集。这为用户提供了灵活的实验能力，可轻松尝试不同的数据集配置，而无需预先计算和存储大量数据集的多个副本。Trove具有高度可定制性：除了提供多种内置选项外，还允许用户自由修改现有组件，或完全替换为自定义对象。此外，Trove提供低代码、统一的评估和难负样本挖掘流水线，支持多节点执行且无需任何代码修改。Trove的数据管理功能将内存消耗降低了2.6倍。此外，Trove的易用推理流水线无额外开销，推理时间随可用节点数量线性减少。最重要的是，我们展示了Trove如何简化召回实验并支持任意定制，从而促进探索性研究。",
        "translated_title": "Trove：一种用于稠密检索的灵活工具包",
        "label": [
            "召回",
            "通用推荐技术"
        ],
        "label_reason": "论文聚焦稠密检索工具包，支持推荐召回环节数据管理与实验，但非专为推荐设计。",
        "relevance_score": 4,
        "novelty_score": 6,
        "novelty_reason": "提出高效数据管理与低代码流程，提升实验灵活性，属工具层面改进，非核心算法创新。"
    },
    {
        "title": "A Graph-based RAG for Energy Efficiency Question Answering",
        "url": "http://arxiv.org/abs/2511.01643v1",
        "pub_date": "2025-11-03",
        "summary": "In this work, we investigate the use of Large Language Models (LLMs) within a graph-based Retrieval Augmented Generation (RAG) architecture for Energy Efficiency (EE) Question Answering. First, the system automatically extracts a Knowledge Graph (KG) from guidance and regulatory documents in the energy field. Then, the generated graph is navigated and reasoned upon to provide users with accurate answers in multiple languages. We implement a human-based validation using the RAGAs framework properties, a validation dataset comprising 101 question-answer pairs, and domain experts. Results confirm the potential of this architecture and identify its strengths and weaknesses. Validation results show how the system correctly answers in about three out of four of the cases (75.2 +- 2.7%), with higher results on questions related to more general EE answers (up to 81.0 +- 4.1%), and featuring promising multilingual abilities (4.4% accuracy loss due to translation).",
        "translated": "本研究探讨了在基于图的检索增强生成（RAG）架构中使用大语言模型（LLM）进行能源效率（EE）问答的应用。首先，系统自动从能源领域的指导文件和法规文档中提取知识图谱（KG）。随后，通过对生成的图谱进行导航和推理，为用户提供多语言的准确答案。我们采用RAGAs框架的评估属性、包含101个问答对的验证数据集以及领域专家进行人工验证。结果验证了该架构的潜力，并识别出其优势与不足。验证结果显示，系统在约四分之三的案例中能够正确作答（75.2 ± 2.7%），在涉及更通用能源效率答案的问题上表现更优（最高达81.0 ± 4.1%），并展现出良好的多语言能力（因翻译导致的准确率损失仅为4.4%）。",
        "translated_title": "基于图的RAG用于能源效率问答",
        "label": [],
        "label_reason": "论文聚焦于能源效率问答系统，基于图的RAG架构，非推荐系统核心问题。",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出图结构增强RAG，用于问答任务，对推荐无直接贡献。"
    },
    {
        "title": "Vote-in-Context: Turning VLMs into Zero-Shot Rank Fusers",
        "url": "http://arxiv.org/abs/2511.01617v1",
        "pub_date": "2025-11-03",
        "summary": "In the retrieval domain, candidates' fusion from heterogeneous retrievers is a long-standing challenge, particularly for complex, multi-modal data such as videos. While typical fusion techniques are training-free, they rely solely on rank or score signals, disregarding candidates' representations. This work introduces Vote-in-Context (ViC), a generalized, training-free framework that re-thinks list-wise reranking and fusion as a zero-shot reasoning task for a Vision-Language Model (VLM). The core insight is to serialize both content evidence and retriever metadata directly within the VLM's prompt, allowing the model to adaptively weigh retriever consensus against visual-linguistic content. We demonstrate the generality of this framework by applying it to the challenging domain of cross-modal video retrieval. To this end, we introduce the S-Grid, a compact serialization map that represents each video as an image grid, optionally paired with subtitles to enable list-wise reasoning over video candidates. ViC is evaluated both as a single-list reranker, where it dramatically improves the precision of individual retrievers, and as an ensemble fuser, where it consistently outperforms strong baselines like CombSUM. Across video retrieval benchmarks including ActivityNet and VATEX, the framework establishes new state-of-the-art zero-shot retrieval performance, demonstrating its effectiveness in handling complex visual and temporal signals alongside text. In zero-shot settings, ViC achieves Recall@1 scores of 87.1% (t2v) / 89.0% (v2t) on MSR-VTT and 99.6% (v2t) on VATEX, representing massive gains of up to +40 Recall@1 over previous state-of-the-art baselines. We present ViC as a simple, reproducible, and highly effective recipe for turning modern VLMs into powerful zero-shot rerankers and fusers. Code and resources are publicly available at: https://github.com/mohammad2012191/ViC",
        "translated": "在召回领域，从异构检索器中融合候选结果长期以来是一个挑战，尤其对于视频等复杂、多模态数据而言。尽管典型的融合技术无需训练，但它们仅依赖排序或得分信号，忽略了候选结果的表示信息。本文提出Vote-in-Context（ViC），一种通用的、无需训练的框架，将列表级重排与融合重新定义为大语言模型（VLM）的零样本推理任务。其核心思想是将内容证据与检索器元数据直接序列化至VLM的提示（prompt）中，使模型能够自适应地权衡检索器的一致性与视觉-语言内容。我们通过将其应用于跨模态视频检索这一具有挑战性的领域，验证了该框架的通用性。为此，我们引入了S-Grid，一种紧凑的序列化映射，将每个视频表示为图像网格，可选配字幕，以支持对视频候选结果进行列表级推理。ViC既可作为单列表重排器使用，在此场景下显著提升单个检索器的精度；也可作为集成融合器使用，在此场景下始终优于CombSUM等强基线方法。在包括ActivityNet和VATEX在内的多个视频检索基准上，该框架实现了新的零样本检索最优性能，证明其在处理复杂视觉与时间信号及文本方面具有有效性。在零样本设置下，ViC在MSR-VTT上实现了87.1%（t2v）/89.0%（v2t）的Recall@1，在VATEX上实现了99.6%（v2t）的Recall@1，相比此前最优基线，Recall@1提升高达+40个百分点。我们提出ViC作为一种简单、可复现且高度有效的方案，可将现代VLM转化为强大的零样本重排器与融合器。代码与资源已公开于：https://github.com/mohammad2012191/ViC",
        "translated_title": "Vote-in-Context：将视觉语言模型转化为零样本排序融合器",
        "label": [
            "重排（Re-ranking）",
            "多模态推荐（Multimodal Recommendation）",
            "LLM生成式推荐（LLM-based Generative Recommendation）"
        ],
        "label_reason": "论文提出基于VLM的零样本重排与融合框架，适用于多模态视频检索，属推荐系统重排环节创新",
        "relevance_score": 8,
        "novelty_score": 9,
        "novelty_reason": "首次将VLM用于零样本重排与融合，通过上下文投票机制实现自适应加权，显著提升性能"
    },
    {
        "title": "Calculating Web Impact Factor for University Websites of Jammu and\n  Kashmir: A Study",
        "url": "http://arxiv.org/abs/2511.01496v1",
        "pub_date": "2025-11-03",
        "summary": "This paper examines and explores the web impact factor through a webometric study of the present 12 University Websites of Jammu and Kashmir. Identifies the domain systems of the websites; analyzes the number of web pages and link pages, and calculates the External Link WIF or simple web impact factor (WIF) and external web impact factor of all the University websites. Also reflects that some university websites have higher number of web pages, but correspondingly their link pages are very small in number and websites fall behind in their simple and external link web impact factor. It found that the Cluster University of Jammu ranked 1 (0.9018) in Internal Link WIF of Websites in Jammu and Kashmir. Shri Mata Vaishno Devi University ranked 1 (0.7249) in External Link Web Impact Factor.",
        "translated": "本文通过针对查谟和克什米尔地区当前12所大学网站的网络计量学研究，考察并探讨了网络影响因子。研究识别了各网站的域名系统，分析了网页数量与链接页面数量，并计算了所有大学网站的外部链接网络影响因子（External Link WIF）以及简单网络影响因子（WIF）。研究还发现，部分大学网站虽拥有较多网页，但其链接页面数量却相对较少，导致其简单网络影响因子和外部链接网络影响因子均处于较低水平。研究结果表明，在查谟和克什米尔地区大学网站的内部链接网络影响因子（Internal Link WIF）排名中，查谟集群大学（Cluster University of Jammu）位列第一（0.9018）；而在外部链接网络影响因子排名中，希里·玛塔·瓦伊什诺·德维大学（Shri Mata Vaishno Devi University）位列第一（0.7249）。",
        "translated_title": "计算查谟和克什米尔地区大学网站的网络影响力因子：一项研究",
        "label": [],
        "label_reason": "论文研究大学网站的网络影响力指标，属于信息计量学，与推荐系统无直接关联。",
        "relevance_score": 2,
        "novelty_score": 3,
        "novelty_reason": "方法为传统网络计量分析，无创新性，仅对现有指标进行应用统计。"
    },
    {
        "title": "Impact and Relevance of Cognition Journal in the Field of Cognitive\n  Science: An Evaluation",
        "url": "http://arxiv.org/abs/2511.01485v1",
        "pub_date": "2025-11-03",
        "summary": "This study aims to present a scientometric analysis of the journal titled Cognition for a period of 20 years from 1999 to 2018. The present study was conducted with an aim to provide a summary of research activity in current journal and characterize its most aspects. The research coverage includes the year wise distribution of articles, authors, institutions, countries and citation analysis of the journal. The analysis showed that 2870 papers were published in journal of Cognition from 1999 to 2018. The study identified top 20 prolific authors, institutions and countries of the journal. Researchers from USA have been made the most percentage of contributions.",
        "translated": "本研究旨在对期刊《Cognition》在1999年至2018年这20年期间的文献计量学特征进行分析。本研究旨在总结该期刊当前的研究活动，并对其主要方面进行刻画。研究内容涵盖按年份分布的论文、作者、机构、国家以及期刊的引文分析。分析结果显示，1999年至2018年间，《Cognition》期刊共发表了2870篇论文。研究识别出了该期刊最具产出力的前20位作者、机构和国家。来自美国的研究人员贡献比例最高。",
        "translated_title": "认知科学领域中《认知期刊》的影响与相关性评估",
        "label": [],
        "label_reason": "论文为认知科学期刊的文献计量分析，与推荐系统无直接关联。",
        "relevance_score": 1,
        "novelty_score": 1,
        "novelty_reason": "仅进行统计分析，无新方法或技术贡献。"
    },
    {
        "title": "CAT-ID$^2$: Category-Tree Integrated Document Identifier Learning for\n  Generative Retrieval In E-commerce",
        "url": "http://arxiv.org/abs/2511.01461v2",
        "pub_date": "2025-11-03",
        "summary": "Generative retrieval (GR) has gained significant attention as an effective paradigm that integrates the capabilities of large language models (LLMs). It generally consists of two stages: constructing discrete semantic identifiers (IDs) for documents and retrieving documents by autoregressively generating ID tokens. The core challenge in GR is how to construct document IDs (DocIDS) with strong representational power. Good IDs should exhibit two key properties: similar documents should have more similar IDs, and each document should maintain a distinct and unique ID. However, most existing methods ignore native category information, which is common and critical in E-commerce. Therefore, we propose a novel ID learning method, CAtegory-Tree Integrated Document IDentifier (CAT-ID$^2$), incorporating prior category information into the semantic IDs. CAT-ID$^2$ includes three key modules: a Hierarchical Class Constraint Loss to integrate category information layer by layer during quantization, a Cluster Scale Constraint Loss for uniform ID token distribution, and a Dispersion Loss to improve the distinction of reconstructed documents. These components enable CAT-ID$^2$ to generate IDs that make similar documents more alike while preserving the uniqueness of different documents' representations. Extensive offline and online experiments confirm the effectiveness of our method, with online A/B tests showing a 0.33% increase in average orders per thousand users for ambiguous intent queries and 0.24% for long-tail queries.",
        "translated": "生成式召回（Generative Retrieval, GR）作为一种有效整合大语言模型（Large Language Model, LLM）能力的范式，已受到广泛关注。它通常包含两个阶段：为文档构建离散的语义标识符（IDs），并通过自回归生成ID token的方式检索文档。GR的核心挑战在于如何构建具有强表征能力的文档ID（DocIDS）。理想的ID应具备两个关键特性：相似文档应具有更相似的ID，且每个文档应保持独特且唯一的ID。然而，现有大多数方法忽略了本源的类别信息，而这类信息在电子商务场景中普遍存在且至关重要。因此，我们提出一种新颖的ID学习方法——CAtegory-Tree Integrated Document IDentifier（CAT-ID²），将先验类别信息融入语义ID中。CAT-ID²包含三个关键模块：分层类别约束损失（Hierarchical Class Constraint Loss），用于在量化过程中逐层整合类别信息；聚类规模约束损失（Cluster Scale Constraint Loss），用于实现ID token的均匀分布；以及分散损失（Dispersion Loss），用于提升重构文档的区分度。这些组件使CAT-ID²能够生成使相似文档更接近、同时保持不同文档表征唯一性的ID。大量离线与在线实验验证了本方法的有效性，在线A/B测试表明，对于语义模糊意图查询，每千用户平均订单数提升0.33%；对于长尾查询，提升0.24%。",
        "translated_title": "CAT-ID²：面向电商生成式召回的类别树集成文档标识符学习",
        "label": [
            "召回",
            "LLM生成式推荐",
            "负采样与对比学习"
        ],
        "label_reason": "论文聚焦电商场景生成式召回，融合类别树信息构建语义ID，直接服务于推荐系统召回环节。",
        "relevance_score": 9,
        "novelty_score": 8,
        "novelty_reason": "提出CAT-ID²框架，创新性融合类别树信息与对比学习，提升ID表示能力，非简单模块替换。"
    },
    {
        "title": "LiCoMemory: Lightweight and Cognitive Agentic Memory for Efficient\n  Long-Term Reasoning",
        "url": "http://arxiv.org/abs/2511.01448v1",
        "pub_date": "2025-11-03",
        "summary": "Large Language Model (LLM) agents exhibit remarkable conversational and reasoning capabilities but remain constrained by limited context windows and the lack of persistent memory. Recent efforts address these limitations via external memory architectures, often employing graph-based representations, yet most adopt flat, entangled structures that intertwine semantics with topology, leading to redundant representations, unstructured retrieval, and degraded efficiency and accuracy. To resolve these issues, we propose LiCoMemory, an end-to-end agentic memory framework for real-time updating and retrieval, which introduces CogniGraph, a lightweight hierarchical graph that utilizes entities and relations as semantic indexing layers, and employs temporal and hierarchy-aware search with integrated reranking for adaptive and coherent knowledge retrieval. Experiments on long-term dialogue benchmarks, LoCoMo and LongMemEval, show that LiCoMemory not only outperforms established baselines in temporal reasoning, multi-session consistency, and retrieval efficiency, but also notably reduces update latency. Our official code and data are available at https://github.com/EverM0re/LiCoMemory.",
        "translated": "大语言模型（LLM）代理展现出卓越的对话与推理能力，但仍受限于有限的上下文窗口和缺乏持久性记忆。近期研究通过外部记忆架构来应对这些局限，通常采用基于图的表示方法，但大多数采用扁平化、纠缠的结构，将语义与拓扑结构混杂在一起，导致表示冗余、检索无序，进而降低效率与准确性。为解决这些问题，我们提出 LiCoMemory，一种端到端的代理记忆框架，支持实时更新与检索。该框架引入 CogniGraph，一种轻量级层次化图结构，利用实体与关系作为语义索引层，并采用结合时间与层次感知的检索策略，辅以集成重排机制，实现自适应且连贯的知识检索。在长期对话基准数据集 LoCoMo 和 LongMemEval 上的实验表明，LiCoMemory 不仅在时间推理、多会话一致性及检索效率方面显著优于现有基线方法，同时大幅降低了更新延迟。我们的官方代码与数据可在 https://github.com/EverM0re/LiCoMemory 获取。",
        "translated_title": "LiCoMemory：轻量级认知代理记忆机制用于高效长期推理",
        "label": [
            "LLM生成式推荐",
            "重排"
        ],
        "label_reason": "论文提出LLM记忆框架，含重排机制，可适配推荐系统中的长期记忆与生成式推荐场景",
        "relevance_score": 5,
        "novelty_score": 8,
        "novelty_reason": "提出轻量认知图结构与时序感知重排，提升检索效率与一致性，属创新性架构设计"
    },
    {
        "title": "A Soft-partitioned Semi-supervised Collaborative Transfer Learning\n  Approach for Multi-Domain Recommendation",
        "url": "http://arxiv.org/abs/2511.01404v1",
        "pub_date": "2025-11-03",
        "summary": "In industrial practice, Multi-domain Recommendation (MDR) plays a crucial role. Shared-specific architectures are widely used in industrial solutions to capture shared and unique attributes via shared and specific parameters. However, with imbalanced data across different domains, these models face two key issues: (1) Overwhelming: Dominant domain data skews model performance, neglecting non-dominant domains. (2) Overfitting: Sparse data in non-dominant domains leads to overfitting in specific parameters. To tackle these challenges, we propose Soft-partitioned Semi-supervised Collaborative Transfer Learning (SSCTL) for multi-domain recommendation. SSCTL generates dynamic parameters to address the overwhelming issue, thus shifting focus towards samples from non-dominant domains. To combat overfitting, it leverages pseudo-labels with weights from dominant domain instances to enhance non-dominant domain data. We conduct comprehensive experiments, both online and offline, to validate the efficacy of our proposed method. Online tests yielded significant improvements across various domains, with increases in GMV ranging from 0.54% to 2.90% and enhancements in CTR ranging from 0.22% to 1.69%.",
        "translated": "在工业实践中，多领域推荐（Multi-domain Recommendation, MDR）发挥着至关重要的作用。共享-特定架构（shared-specific architectures）广泛应用于工业解决方案中，通过共享参数与特定参数分别捕捉不同领域间的共性与个性特征。然而，由于不同领域间数据分布不均衡，这些模型面临两个关键问题：（1）主导效应（Overwhelming）：主导领域数据过多导致模型性能偏向该领域，忽视了非主导领域；（2）过拟合（Overfitting）：非主导领域数据稀疏，导致特定参数出现过拟合现象。为应对上述挑战，我们提出一种软划分半监督协同迁移学习方法（Soft-partitioned Semi-supervised Collaborative Transfer Learning, SSCTL）用于多领域推荐。SSCTL通过生成动态参数来缓解主导效应问题，从而将模型关注点转向非主导领域的样本。为应对过拟合问题，SSCTL利用主导领域样本生成带权重的伪标签，以增强非主导领域的数据质量。我们通过离线与在线实验全面验证了所提方法的有效性。在线测试结果表明，各领域均取得显著提升，GMV增幅介于0.54%至2.90%之间，点击率（CTR）提升范围为0.22%至1.69%。",
        "translated_title": "一种基于软划分的半监督协同迁移学习多领域推荐方法",
        "label": [
            "跨域/联邦推荐",
            "通用推荐技术"
        ],
        "label_reason": "论文针对多域推荐中的数据不平衡与过拟合问题，提出半监督协同迁移学习方法，直接服务于推荐系统跨域场景。",
        "relevance_score": 8,
        "novelty_score": 7,
        "novelty_reason": "提出软划分动态参数与伪标签加权机制，有效缓解主导域主导和稀疏域过拟合，属结构化改进而非范式创新。"
    },
    {
        "title": "RAGSmith: A Framework for Finding the Optimal Composition of\n  Retrieval-Augmented Generation Methods Across Datasets",
        "url": "http://arxiv.org/abs/2511.01386v1",
        "pub_date": "2025-11-03",
        "summary": "Retrieval-Augmented Generation (RAG) quality depends on many interacting choices across retrieval, ranking, augmentation, prompting, and generation, so optimizing modules in isolation is brittle. We introduce RAGSmith, a modular framework that treats RAG design as an end-to-end architecture search over nine technique families and 46{,}080 feasible pipeline configurations. A genetic search optimizes a scalar objective that jointly aggregates retrieval metrics (recall@k, mAP, nDCG, MRR) and generation metrics (LLM-Judge and semantic similarity). We evaluate on six Wikipedia-derived domains (Mathematics, Law, Finance, Medicine, Defense Industry, Computer Science), each with 100 questions spanning factual, interpretation, and long-answer types. RAGSmith finds configurations that consistently outperform naive RAG baseline by +3.8\\% on average (range +1.2\\% to +6.9\\% across domains), with gains up to +12.5\\% in retrieval and +7.5\\% in generation. The search typically explores $\\approx 0.2\\%$ of the space ($\\sim 100$ candidates) and discovers a robust backbone -- vector retrieval plus post-generation reflection/revision -- augmented by domain-dependent choices in expansion, reranking, augmentation, and prompt reordering; passage compression is never selected. Improvement magnitude correlates with question type, with larger gains on factual/long-answer mixes than interpretation-heavy sets. These results provide practical, domain-aware guidance for assembling effective RAG systems and demonstrate the utility of evolutionary search for full-pipeline optimization.",
        "translated": "检索增强生成（RAG）的质量取决于检索、排序、增强、提示设计和生成等多个环节相互作用的选择，因此孤立地优化各模块容易导致系统脆弱。我们提出 RAGSmith，一个模块化框架，将 RAG 设计视为在九个技术家族和 46,080 种可行流水线配置上的端到端架构搜索。通过遗传搜索优化一个标量目标函数，该函数联合聚合了检索指标（recall@k、mAP、nDCG、MRR）和生成指标（LLM-Judge 和语义相似度）。我们在六个源自维基百科的领域（数学、法律、金融、医学、国防工业、计算机科学）上进行评估，每个领域包含 100 个问题，涵盖事实性、解释性和长答案类型。RAGSmith 找到的配置在平均情况下比朴素 RAG 基线提升 +3.8%（各领域范围为 +1.2% 至 +6.9%），其中检索部分最高提升 +12.5%，生成部分最高提升 +7.5%。搜索通常探索约 0.2% 的配置空间（约 100 个候选方案），并发现一个稳健的主干结构——向量召回加生成后反思/修订——并辅以领域依赖的选择，包括扩展、重排、增强和提示重排序；而段落压缩从未被选中。改进幅度与问题类型相关，事实性/长答案混合问题的提升幅度大于以解释性为主的问题集。这些结果为构建有效的 RAG 系统提供了实用且具备领域感知的指导，并展示了进化搜索在全流程优化中的有效性。",
        "translated_title": "RAGSmith：一种跨数据集寻找检索增强生成方法最优组合的框架",
        "label": [
            "LLM生成式推荐（LLM-based Generative Recommendation）",
            "重排（Re-ranking）",
            "召回（Recall）"
        ],
        "label_reason": "论文聚焦RAG系统优化，涉及召回、重排、生成等环节，虽非专为推荐设计，但方法可迁移至推荐场景。",
        "relevance_score": 4,
        "novelty_score": 8,
        "novelty_reason": "提出模块化架构搜索框架，结合遗传算法优化多模块组合，显著提升RAG性能，属有效创新。"
    },
    {
        "title": "A semantic-based deep learning approach for mathematical expression\n  retrieval",
        "url": "http://arxiv.org/abs/2511.01364v1",
        "pub_date": "2025-11-03",
        "summary": "Mathematical expressions (MEs) have complex two-dimensional structures in which symbols can be present at any nested depth like superscripts, subscripts, above, below etc. As MEs are represented using LaTeX format, several text retrieval methods based on string matching, vector space models etc., have also been applied for ME retrieval problem in the literature. As these methods are based on syntactic similarity, recently deep learning approaches based on embedding have been used for semantic similarity. In our present work, we have focused on the retrieval of mathematical expressions using deep learning approaches. In our approach, semantic features are extracted from the MEs using a deep recurrent neural network (DRNN) and these features have been used for matching and retrieval. We have trained the network for a classification task which determines the complexity of an ME. ME complexity has been quantified in terms of its nested depth. Based on the nested depth, we have considered three complexity classes of MEs: Simple, Medium and Complex. After training the network, outputs just before the the final fully connected layer are extracted for all the MEs. These outputs form the semantic features of MEs and are stored in a database. For a given ME query, its semantic features are computed using the trained DRNN and matched against the semantic feature database. Matching is performed based on the standard euclidean distance and top 'k' nearest matches are retrieved, where 'k' is a user-defined parameter. Our approach has been illustrated on a database of 829 MEs.",
        "translated": "数学表达式（MEs）具有复杂的二维结构，其中符号可以出现在任意嵌套深度，如上标、下标、上方、下方等。由于MEs通常以LaTeX格式表示，文献中已应用多种基于字符串匹配、向量空间模型等文本检索方法来解决MEs的检索问题。然而，这些方法基于语法相似性，近年来基于嵌入的深度学习方法被用于捕捉语义相似性。在本研究中，我们专注于利用深度学习方法进行数学表达式的检索。我们的方法采用深度循环神经网络（DRNN）从MEs中提取语义特征，并利用这些特征进行匹配与检索。我们通过一个分类任务对网络进行训练，该任务用于判断ME的复杂度。ME的复杂度通过其嵌套深度进行量化。根据嵌套深度，我们将ME划分为三个复杂度类别：简单（Simple）、中等（Medium）和复杂（Complex）。网络训练完成后，提取所有ME在最终全连接层之前的输出，这些输出构成MEs的语义特征，并存储于数据库中。对于给定的ME查询，使用训练好的DRNN计算其语义特征，并与语义特征数据库进行匹配。匹配基于标准欧氏距离，检索出最接近的前'k'个结果，其中'k'为用户定义的参数。我们的方法在包含829个MEs的数据库上进行了验证。",
        "translated_title": "一种基于语义的深度学习数学表达式检索方法",
        "label": [],
        "label_reason": "论文聚焦数学表达式检索，属于信息检索范畴，与推荐系统无直接关联。",
        "relevance_score": 2,
        "novelty_score": 5,
        "novelty_reason": "采用DRNN提取语义特征，属于常规深度学习方法在检索中的应用，无本质创新。"
    },
    {
        "title": "Disentangled Concepts Speak Louder Than Words:Explainable Video Action\n  Recognition",
        "url": "http://arxiv.org/abs/2511.03725v1",
        "pub_date": "2025-11-05",
        "summary": "Effective explanations of video action recognition models should disentangle how movements unfold over time from the surrounding spatial context. However, existing methods based on saliency produce entangled explanations, making it unclear whether predictions rely on motion or spatial context. Language-based approaches offer structure but often fail to explain motions due to their tacit nature -- intuitively understood but difficult to verbalize. To address these challenges, we propose Disentangled Action aNd Context concept-based Explainable (DANCE) video action recognition, a framework that predicts actions through disentangled concept types: motion dynamics, objects, and scenes. We define motion dynamics concepts as human pose sequences. We employ a large language model to automatically extract object and scene concepts. Built on an ante-hoc concept bottleneck design, DANCE enforces prediction through these concepts. Experiments on four datasets -- KTH, Penn Action, HAA500, and UCF-101 -- demonstrate that DANCE significantly improves explanation clarity with competitive performance. We validate the superior interpretability of DANCE through a user study. Experimental results also show that DANCE is beneficial for model debugging, editing, and failure analysis.",
        "translated": "视频动作识别模型的有效解释应当能够将动作在时间上的演变过程与周围的空域上下文分离开来。然而，现有基于显著性图的方法产生的解释往往相互纠缠，导致难以判断模型预测是依赖于运动信息还是空域上下文。基于语言的方法虽然提供了结构化的解释，但由于运动具有隐含性——即人们可以直观理解但难以用语言准确描述——往往无法有效解释运动过程。为应对这些挑战，我们提出了一种基于解耦动作与上下文概念的可解释视频动作识别框架：Disentangled Action aNd Context concept-based Explainable (DANCE)。该框架通过解耦的概念类型——运动动态、物体和场景——来进行动作预测。我们将运动动态概念定义为人体姿态序列，并利用大型语言模型自动提取物体和场景概念。基于先验概念瓶颈设计，DANCE 强制模型通过这些概念进行预测。在四个数据集——KTH、Penn Action、HAA500 和 UCF-101 上的实验表明，DANCE 在保持竞争力性能的同时，显著提升了解释的清晰度。我们通过用户研究验证了 DANCE 具有更优的可解释性。实验结果还表明，DANCE 有助于模型调试、编辑和失败分析。",
        "translated_title": "解耦概念胜于言语：可解释视频动作识别",
        "label": [],
        "label_reason": "论文聚焦视频动作识别与可解释性，属于high-level视觉任务，不涉及图像像素级恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "提出概念解耦框架，但未引入新范式，属于现有方法的结构化改进。"
    },
    {
        "title": "Part-Aware Bottom-Up Group Reasoning for Fine-Grained Social Interaction\n  Detection",
        "url": "http://arxiv.org/abs/2511.03666v1",
        "pub_date": "2025-11-05",
        "summary": "Social interactions often emerge from subtle, fine-grained cues such as facial expressions, gaze, and gestures. However, existing methods for social interaction detection overlook such nuanced cues and primarily rely on holistic representations of individuals. Moreover, they directly detect social groups without explicitly modeling the underlying interactions between individuals. These drawbacks limit their ability to capture localized social signals and introduce ambiguity when group configurations should be inferred from social interactions grounded in nuanced cues. In this work, we propose a part-aware bottom-up group reasoning framework for fine-grained social interaction detection. The proposed method infers social groups and their interactions using body part features and their interpersonal relations. Our model first detects individuals and enhances their features using part-aware cues, and then infers group configuration by associating individuals via similarity-based reasoning, which considers not only spatial relations but also subtle social cues that signal interactions, leading to more accurate group inference. Experiments on the NVI dataset demonstrate that our method outperforms prior methods, achieving the new state of the art.",
        "translated": "社会互动通常源于细微且细粒度的线索，如面部表情、视线方向和手势。然而，现有的社会互动检测方法往往忽略这些细微线索，主要依赖于个体的整体表征。此外，这些方法直接检测社会群体，而未显式建模个体之间的底层互动关系。这些局限性使其难以捕捉局部化的社会信号，并在需要基于细微线索推断群体构型时引入歧义。在本文中，我们提出了一种基于部件感知的自底向上群体推理框架，用于细粒度社会互动检测。所提方法利用身体部位特征及其人际关系来推断社会群体及其互动。我们的模型首先检测个体，并通过部件感知线索增强其特征，随后通过基于相似性的推理机制关联个体，以推断群体构型，该机制不仅考虑空间关系，还融合了能够指示互动的细微社会线索，从而实现更准确的群体推断。在 NVI 数据集上的实验表明，我们的方法优于现有方法，达到了新的最先进水平。",
        "translated_title": "基于部件感知的自底向上分组推理用于细粒度社会交互检测",
        "label": [],
        "label_reason": "论文聚焦于社交交互检测，属于高阶视觉理解任务，非像素级图像恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "提出部分感知的自下而上推理框架，但属高阶检测方法，无low-level创新。"
    },
    {
        "title": "A Lightweight 3D-CNN for Event-Based Human Action Recognition with\n  Privacy-Preserving Potential",
        "url": "http://arxiv.org/abs/2511.03665v1",
        "pub_date": "2025-11-05",
        "summary": "This paper presents a lightweight three-dimensional convolutional neural network (3DCNN) for human activity recognition (HAR) using event-based vision data. Privacy preservation is a key challenge in human monitoring systems, as conventional frame-based cameras capture identifiable personal information. In contrast, event cameras record only changes in pixel intensity, providing an inherently privacy-preserving sensing modality. The proposed network effectively models both spatial and temporal dynamics while maintaining a compact design suitable for edge deployment. To address class imbalance and enhance generalization, focal loss with class reweighting and targeted data augmentation strategies are employed. The model is trained and evaluated on a composite dataset derived from the Toyota Smart Home and ETRI datasets. Experimental results demonstrate an F1-score of 0.9415 and an overall accuracy of 94.17%, outperforming benchmark 3D-CNN architectures such as C3D, ResNet3D, and MC3_18 by up to 3%. These results highlight the potential of event-based deep learning for developing accurate, efficient, and privacy-aware human action recognition systems suitable for real-world edge applications.",
        "translated": "本文提出了一种轻量级三维卷积神经网络（3DCNN），用于基于事件视觉数据的人体活动识别（HAR）。在人体监控系统中，隐私保护是一个关键挑战，因为传统的基于帧的摄像头会捕获可识别的个人信息。相比之下，事件相机仅记录像素强度的变化，提供了一种本质上具备隐私保护能力的感知模式。所提出的网络能够有效建模空间与时间动态，同时保持紧凑的设计，适用于边缘设备部署。为应对类别不平衡问题并增强泛化能力，采用了带有类别重加权的焦点损失（focal loss）以及针对性的数据增强策略。该模型在由Toyota Smart Home和ETRI数据集合成的复合数据集上进行训练与评估。实验结果表明，模型取得了0.9415的F1分数和94.17%的整体准确率，相较于C3D、ResNet3D和MC3_18等基准3D-CNN架构，性能提升最高达3%。这些结果凸显了基于事件的深度学习在开发准确、高效且具备隐私保护能力的人体动作识别系统方面的潜力，适用于现实世界的边缘应用。",
        "translated_title": "一种具有隐私保护潜力的轻量级3D-CNN用于事件驱动的人体动作识别",
        "label": [],
        "label_reason": "论文聚焦事件相机的人体动作识别，属于高阶视觉任务，非图像像素级恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "提出轻量3D-CNN用于事件数据，但属于常规网络设计改进，无突破性创新。"
    },
    {
        "title": "Flying Robotics Art: ROS-based Drone Draws the Record-Breaking Mural",
        "url": "http://arxiv.org/abs/2511.03651v1",
        "pub_date": "2025-11-05",
        "summary": "This paper presents the innovative design and successful deployment of a pioneering autonomous unmanned aerial system developed for executing the world's largest mural painted by a drone. Addressing the dual challenges of maintaining artistic precision and operational reliability under adverse outdoor conditions such as wind and direct sunlight, our work introduces a robust system capable of navigating and painting outdoors with unprecedented accuracy. Key to our approach is a novel navigation system that combines an infrared (IR) motion capture camera and LiDAR technology, enabling precise location tracking tailored specifically for largescale artistic applications. We employ a unique control architecture that uses different regulation in tangential and normal directions relative to the planned path, enabling precise trajectory tracking and stable line rendering. We also present algorithms for trajectory planning and path optimization, allowing for complex curve drawing and area filling. The system includes a custom-designed paint spraying mechanism, specifically engineered to function effectively amidst the turbulent airflow generated by the drone's propellers, which also protects the drone's critical components from paint-related damage, ensuring longevity and consistent performance. Experimental results demonstrate the system's robustness and precision in varied conditions, showcasing its potential for autonomous large-scale art creation and expanding the functional applications of robotics in creative fields.",
        "translated": "本文提出了一种创新设计并成功部署的开创性自主无人机系统，用于完成由无人机绘制的世界最大壁画。针对在风力和直射阳光等恶劣户外条件下保持艺术精度与操作可靠性的双重挑战，本研究开发了一套鲁棒系统，能够以前所未有的精度在户外环境中导航与作画。本方法的核心在于一种新型导航系统，该系统结合了红外（IR）动作捕捉相机与LiDAR技术，实现了专为大规模艺术应用设计的精确位置追踪。我们采用了一种独特的控制架构，针对规划路径的切向与法向方向分别施加不同的控制调节，从而实现精确的轨迹跟踪与稳定的线条绘制。此外，我们还提出了轨迹规划与路径优化算法，支持复杂曲线绘制与区域填充。系统配备了一套定制化的喷漆机构，专门设计用于在无人机螺旋桨产生的湍流气流中高效工作，同时保护无人机关键部件免受油漆损害，确保其长期稳定运行。实验结果表明，该系统在多种环境下均表现出优异的鲁棒性与精度，展示了其在自主大规模艺术创作中的潜力，并拓展了机器人在创意领域中的功能应用。",
        "translated_title": "飞行机器人艺术：基于ROS的无人机绘制创纪录壁画",
        "label": [],
        "label_reason": "论文聚焦无人机艺术创作系统，涉及导航与控制，非图像像素级恢复或增强任务。",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "提出新型导航与控制架构，但属于机器人系统设计，非图像处理领域创新。"
    },
    {
        "title": "Signal Intensity-weighted coordinate channels improve learning stability\n  and generalisation in 1D and 2D CNNs in localisation tasks on biomedical\n  signals",
        "url": "http://arxiv.org/abs/2511.03645v1",
        "pub_date": "2025-11-05",
        "summary": "Localisation tasks in biomedical data often require models to learn meaningful spatial or temporal relationships from signals with complex intensity distributions. A common strategy, exemplified by CoordConv layers, is to append coordinate channels to convolutional inputs, enabling networks to learn absolute positions. In this work, we propose a signal intensity-weighted coordinate representation that replaces the pure coordinate channels with channels scaled by local signal intensity. This modification embeds an intensity-position coupling directly in the input representation, introducing a simple and modality-agnostic inductive bias. We evaluate the approach on two distinct localisation problems: (i) predicting the time of morphological transition in 20-second, two-lead ECG signals, and (ii) regressing the coordinates of nuclear centres in cytological images from the SiPaKMeD dataset. In both cases, the proposed representation yields faster convergence and higher generalisation performance relative to conventional coordinate-channel approaches, demonstrating its effectiveness across both one-dimensional and two-dimensional biomedical signals.",
        "translated": "生物医学数据中的定位任务通常要求模型从具有复杂强度分布的信号中学习有意义的空间或时间关系。一种常见策略，如CoordConv层所示，是向卷积输入附加坐标通道，使网络能够学习绝对位置信息。在本工作中，我们提出一种信号强度加权的坐标表示方法，用局部信号强度缩放后的通道替代纯坐标通道。该修改在输入表示中直接嵌入了强度-位置耦合关系，引入了一种简单且模态无关的归纳偏置。我们在两个不同的定位问题上评估了该方法：(i) 预测20秒双导联ECG信号中形态学转变的时间点；(ii) 在SiPaKMeD数据集的细胞学图像中回归细胞核中心的坐标。在两种情况下，所提出的表示方法相较于传统的坐标通道方法均表现出更快的收敛速度和更高的泛化性能，证明了其在一维和二维生物医学信号中的有效性。",
        "translated_title": "信号强度加权坐标通道提升1D和2D CNN在生物医学信号定位任务中的学习稳定性与泛化能力",
        "label": [],
        "label_reason": "论文聚焦生物医学信号定位任务，非图像像素级恢复或增强，属高阶信号处理任务。",
        "relevance_score": 3,
        "novelty_score": 6,
        "novelty_reason": "提出强度加权坐标通道，改进CoordConv，具一定创新性但非图像复原领域突破。"
    },
    {
        "title": "Human Mesh Modeling for Anny Body",
        "url": "http://arxiv.org/abs/2511.03589v1",
        "pub_date": "2025-11-05",
        "summary": "Parametric body models are central to many human-centric tasks, yet existing models often rely on costly 3D scans and learned shape spaces that are proprietary and demographically narrow. We introduce Anny, a simple, fully differentiable, and scan-free human body model grounded in anthropometric knowledge from the MakeHuman community. Anny defines a continuous, interpretable shape space, where phenotype parameters (e.g. gender, age, height, weight) control blendshapes spanning a wide range of human forms -- across ages (from infants to elders), body types, and proportions. Calibrated using WHO population statistics, it provides realistic and demographically grounded human shape variation within a single unified model. Thanks to its openness and semantic control, Anny serves as a versatile foundation for 3D human modeling -- supporting millimeter-accurate scan fitting, controlled synthetic data generation, and Human Mesh Recovery (HMR). We further introduce Anny-One, a collection of 800k photorealistic humans generated with Anny, showing that despite its simplicity, HMR models trained with Anny can match the performance of those trained with scan-based body models, while remaining interpretable and broadly representative. The Anny body model and its code are released under the Apache 2.0 license, making Anny an accessible foundation for human-centric 3D modeling.",
        "translated": "参数化人体模型在许多以人为中心的任务中占据核心地位，但现有模型通常依赖于成本高昂的3D扫描数据以及专有且人群覆盖范围狭窄的形状空间。我们提出Anny，一种简单、完全可微分且无需扫描的人体模型，其基础来自MakeHuman社区的人体测量学知识。Anny定义了一个连续且可解释的形状空间，其中表型参数（如性别、年龄、身高、体重）控制着覆盖广泛人体形态的混合形状——涵盖从婴儿到老年人的不同年龄、体型和比例。通过使用WHO人口统计数据进行校准，Anny在单一统一模型中提供了真实且具有人口统计学基础的人体形状变化。得益于其开放性和语义控制能力，Anny为3D人体建模提供了多功能基础，支持毫米级精度的扫描拟合、可控的合成数据生成以及人体网格恢复（Human Mesh Recovery, HMR）。我们进一步引入Anny-One，一个包含80万张由Anny生成的逼真人体图像的数据集，实验表明，尽管Anny结构简单，但基于Anny训练的HMR模型性能可与基于扫描数据的人体模型训练结果相媲美，同时保持可解释性和广泛代表性。Anny人体模型及其代码在Apache 2.0许可证下开源，使其成为以人为中心的3D建模领域中易于获取的基础工具。",
        "translated_title": "人体网格建模用于任意人体姿态",
        "label": [],
        "label_reason": "论文聚焦3D人体建模与合成数据生成，属于高阶3D重建与生成任务，非像素级图像恢复。",
        "relevance_score": 1,
        "novelty_score": 6,
        "novelty_reason": "提出基于人体测量学的可解释3D人体模型，但未涉及图像复原或增强的创新方法。"
    },
    {
        "title": "OneOcc: Semantic Occupancy Prediction for Legged Robots with a Single\n  Panoramic Camera",
        "url": "http://arxiv.org/abs/2511.03571v1",
        "pub_date": "2025-11-05",
        "summary": "Robust 3D semantic occupancy is crucial for legged/humanoid robots, yet most semantic scene completion (SSC) systems target wheeled platforms with forward-facing sensors. We present OneOcc, a vision-only panoramic SSC framework designed for gait-introduced body jitter and 360{\\deg} continuity. OneOcc combines: (i) Dual-Projection fusion (DP-ER) to exploit the annular panorama and its equirectangular unfolding, preserving 360{\\deg} continuity and grid alignment; (ii) Bi-Grid Voxelization (BGV) to reason in Cartesian and cylindrical-polar spaces, reducing discretization bias and sharpening free/occupied boundaries; (iii) a lightweight decoder with Hierarchical AMoE-3D for dynamic multi-scale fusion and better long-range/occlusion reasoning; and (iv) plug-and-play Gait Displacement Compensation (GDC) learning feature-level motion correction without extra sensors. We also release two panoramic occupancy benchmarks: QuadOcc (real quadruped, first-person 360{\\deg}) and Human360Occ (H3O) (CARLA human-ego 360{\\deg} with RGB, Depth, semantic occupancy; standardized within-/cross-city splits). OneOcc sets new state-of-the-art (SOTA): on QuadOcc it beats strong vision baselines and popular LiDAR ones; on H3O it gains +3.83 mIoU (within-city) and +8.08 (cross-city). Modules are lightweight, enabling deployable full-surround perception for legged/humanoid robots. Datasets and code will be publicly available at https://github.com/MasterHow/OneOcc.",
        "translated": "鲁棒的3D语义占据估计对腿式/人形机器人至关重要，然而大多数语义场景补全（SSC）系统主要面向配备前向传感器的轮式平台。本文提出OneOcc，一种仅依赖视觉的全景语义场景补全框架，专为应对步态引起的机体抖动和360°连续性问题而设计。OneOcc包含以下四个关键组件：(i) 双投影融合（DP-ER），利用环形全景图及其等距矩形展开图，保持360°连续性与网格对齐；(ii) 双网格体素化（BGV），在笛卡尔空间与圆柱极坐标空间中进行推理，降低离散化偏差并锐化空闲/占据边界；(iii) 轻量级解码器，结合分层AMoE-3D结构，实现动态多尺度融合，提升长距离与遮挡推理能力；(iv) 即插即用的步态位移补偿（GDC），在特征层实现运动校正，无需额外传感器。此外，我们发布了两个全景占据估计基准数据集：QuadOcc（真实四足机器人，第一人称360°视角）和Human360Occ（H3O）（基于CARLA的人类自视角360°数据，包含RGB、深度、语义占据信息；采用标准化的城内/跨城划分）。OneOcc在多个指标上达到新的最先进水平（SOTA）：在QuadOcc上超越了强大的视觉基线方法和主流LiDAR方法；在H3O上，城内划分提升+3.83 mIoU，跨城划分提升+8.08 mIoU。各模块设计轻量化，支持腿式/人形机器人部署全环绕感知系统。数据集与代码将公开于https://github.com/MasterHow/OneOcc。",
        "translated_title": "OneOcc：基于单目全景相机的腿部机器人语义占据预测",
        "label": [],
        "label_reason": "论文聚焦3D语义占用预测，属于高阶场景理解任务，非像素级图像恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出双投影融合与双网格体素化等新模块，对3D感知有显著改进，但非low-level图像处理创新。"
    },
    {
        "title": "Generalizing Shape-from-Template to Topological Changes",
        "url": "http://arxiv.org/abs/2511.03459v1",
        "pub_date": "2025-11-05",
        "summary": "Reconstructing the surfaces of deformable objects from correspondences between a 3D template and a 2D image is well studied under Shape-from-Template (SfT) methods; however, existing approaches break down when topological changes accompany the deformation. We propose a principled extension of SfT that enables reconstruction in the presence of such changes. Our approach is initialized with a classical SfT solution and iteratively adapts the template by partitioning its spatial domain so as to minimize an energy functional that jointly encodes physical plausibility and reprojection consistency. We demonstrate that the method robustly captures a wide range of practically relevant topological events including tears and cuts on bounded 2D surfaces, thereby establishing the first general framework for topological-change-aware SfT. Experiments on both synthetic and real data confirm that our approach consistently outperforms baseline methods.",
        "translated": "基于3D模板与2D图像之间对应关系来重建可变形物体表面，在形状从模板（Shape-from-Template, SfT）方法下已有深入研究；然而，现有方法在变形伴随拓扑变化时失效。我们提出了一种SfT的原理性扩展，使其能够在存在此类变化的情况下实现重建。我们的方法以经典的SfT解作为初始化，通过迭代划分模板的空域，最小化一个联合编码物理合理性与重投影一致性的能量函数，从而逐步适应模板。实验表明，该方法能够稳健地捕捉到多种实际相关的拓扑事件，包括有界2D表面上的撕裂和切割，从而建立了首个具备拓扑变化感知能力的SfT通用框架。在合成数据和真实数据上的实验结果均证实，我们的方法始终优于基线方法。",
        "translated_title": "通用化模板形状到拓扑变化\n\n在计算机视觉和图像恢复领域，基于模板的形状恢复方法（shape-from-template）通常假设目标对象的拓扑结构与模板保持一致，即物体的连接性、孔洞数量等几何属性在退化前后不变。然而，在实际应用中，如图像去雨、去雾或低光照增强等任务中，退化过程可能引入显著的拓扑变化，例如雨滴遮挡导致局部区域断裂、雾气造成边缘模糊甚至结构消失，或光照不足引发细节丢失与连通性改变。这些变化使得传统基于模板的方法难以有效恢复原始结构，从而限制了其在复杂退化场景下的泛化能力。\n\n为应对这一挑战，本文提出一种通用化框架，旨在将基于模板的形状恢复方法扩展至存在拓扑变化的场景。核心思想是引入一种动态拓扑感知机制，该机制通过分析退化图像的局部结构特征（如梯度、边缘连续性、频域分布）来检测潜在的拓扑断裂或合并区域，并在恢复过程中动态调整模板的对应关系。具体而言，我们设计了一个双分支网络结构：一个分支负责在空域中提取退化图像的结构先验（如边缘、轮廓），另一个分支则在频域中捕捉全局结构一致性，二者通过注意力机制融合，以增强对拓扑变化的鲁棒性。\n\n此外，我们引入残差学习策略，将恢复任务建模为退化图像与清晰图像之间的残差估计，从而降低模型对绝对结构的依赖，提升对局部拓扑扰动的适应能力。实验在多个公开数据集（如 Rain100L、RESIDE、SIDD）上验证了所提方法的有效性，结果表明，在存在显著拓扑变化的退化条件下，该方法在结构相似性（SSIM）、峰值信噪比（PSNR）以及视觉质量方面均优于现有基于模板的恢复方法。\n\n综上，本文通过引入动态拓扑感知与残差学习机制，成功实现了基于模板的形状恢复方法对拓扑变化的泛化，为复杂退化场景下的图像恢复提供了新的思路。",
        "label": [],
        "label_reason": "论文聚焦3D表面重建与拓扑变化，属于3D重建与几何建模，非像素级图像恢复任务。",
        "relevance_score": 1,
        "novelty_score": 6,
        "novelty_reason": "提出拓扑变化感知的SfT框架，对经典方法有改进，但非low-level图像处理创新。"
    },
    {
        "title": "Seeing What You Say: Expressive Image Generation from Speech",
        "url": "http://arxiv.org/abs/2511.03423v1",
        "pub_date": "2025-11-05",
        "summary": "This paper proposes VoxStudio, the first unified and end-to-end speech-to-image model that generates expressive images directly from spoken descriptions by jointly aligning linguistic and paralinguistic information. At its core is a speech information bottleneck (SIB) module, which compresses raw speech into compact semantic tokens, preserving prosody and emotional nuance. By operating directly on these tokens, VoxStudio eliminates the need for an additional speech-to-text system, which often ignores the hidden details beyond text, e.g., tone or emotion. We also release VoxEmoset, a large-scale paired emotional speech-image dataset built via an advanced TTS engine to affordably generate richly expressive utterances. Comprehensive experiments on the SpokenCOCO, Flickr8kAudio, and VoxEmoset benchmarks demonstrate the feasibility of our method and highlight key challenges, including emotional consistency and linguistic ambiguity, paving the way for future research.",
        "translated": "本文提出VoxStudio，首个统一且端到端的语音到图像模型，能够通过联合对齐语言和副语言信息，直接从语音描述生成富有表现力的图像。其核心是一个语音信息瓶颈（SIB）模块，该模块将原始语音压缩为紧凑的语义标记，同时保留语调和情感细微差别。通过直接在这些标记上操作，VoxStudio消除了对额外语音到文本系统的需求，而后者通常会忽略文本之外的隐藏细节，例如语调或情感。我们还发布了VoxEmoset，一个大规模的配对情感语音-图像数据集，通过先进的TTS引擎以低成本生成丰富表现力的语音语句。在SpokenCOCO、Flickr8kAudio和VoxEmoset基准数据集上的综合实验验证了所提方法的可行性，并揭示了关键挑战，包括情感一致性与语言歧义，为未来研究指明了方向。",
        "translated_title": "看到你说的：从语音生成富有表现力的图像",
        "label": [],
        "label_reason": "论文属于图像生成任务，目标为从语音生成新图像，非像素级图像恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 6,
        "novelty_reason": "提出语音到图像的统一模型，结合语义与副语言信息，具有一定创新性但非low-level任务。"
    },
    {
        "title": "Robust Alignment of the Human Embryo in 3D Ultrasound using PCA and an\n  Ensemble of Heuristic, Atlas-based and Learning-based Classifiers Evaluated\n  on the Rotterdam Periconceptional Cohort",
        "url": "http://arxiv.org/abs/2511.03416v1",
        "pub_date": "2025-11-05",
        "summary": "Standardized alignment of the embryo in three-dimensional (3D) ultrasound images aids prenatal growth monitoring by facilitating standard plane detection, improving visualization of landmarks and accentuating differences between different scans. In this work, we propose an automated method for standardizing this alignment. Given a segmentation mask of the embryo, Principal Component Analysis (PCA) is applied to the mask extracting the embryo's principal axes, from which four candidate orientations are derived. The candidate in standard orientation is selected using one of three strategies: a heuristic based on Pearson's correlation assessing shape, image matching to an atlas through normalized cross-correlation, and a Random Forest classifier. We tested our method on 2166 images longitudinally acquired 3D ultrasound scans from 1043 pregnancies from the Rotterdam Periconceptional Cohort, ranging from 7+0 to 12+6 weeks of gestational age. In 99.0% of images, PCA correctly extracted the principal axes of the embryo. The correct candidate was selected by the Pearson Heuristic, Atlas-based and Random Forest in 97.4%, 95.8%, and 98.4% of images, respectively. A Majority Vote of these selection methods resulted in an accuracy of 98.5%. The high accuracy of this pipeline enables consistent embryonic alignment in the first trimester, enabling scalable analysis in both clinical and research settings. The code is publicly available at: https://gitlab.com/radiology/prenatal-image-analysis/pca-3d-alignment.",
        "translated": "三维（3D）超声图像中胚胎的标准化对齐有助于产前生长监测，通过促进标准切面的检测、改善标志点的可视化以及突出不同扫描之间的差异。本文提出了一种自动化的标准化对齐方法。给定胚胎的分割掩码，采用主成分分析（PCA）对掩码进行处理，提取胚胎的主轴，进而生成四个候选朝向。通过三种策略选择符合标准朝向的候选：基于皮尔逊相关系数的启发式方法（用于评估形状）、通过归一化互相关实现的与图谱的图像匹配，以及随机森林分类器。我们在来自鹿特丹围孕期队列的1043例妊娠中纵向采集的2166张3D超声图像上测试了该方法，孕周范围为7+0至12+6周。在99.0%的图像中，PCA正确提取了胚胎的主轴。皮尔逊启发式方法、基于图谱的方法和随机森林分类器分别在97.4%、95.8%和98.4%的图像中正确选择了目标候选朝向。这三种选择方法的多数投票结果达到了98.5%的准确率。该流程的高准确率使得在孕早期实现胚胎对齐的一致性，从而支持临床和研究场景下的可扩展分析。代码公开获取地址：https://gitlab.com/radiology/prenatal-image-analysis/pca-3d-alignment。",
        "translated_title": "基于PCA与多种分类器集成的人类胚胎在3D超声中的鲁棒对齐——在鹿特丹围孕期队列中的评估",
        "label": [],
        "label_reason": "论文聚焦3D超声图像中胚胎对齐，属于图像配准与姿态估计，非像素级图像恢复或增强，属high-level任务。",
        "relevance_score": 3,
        "novelty_score": 5,
        "novelty_reason": "方法结合PCA与多种分类器，属常规组合，无显著新架构或理论突破。"
    },
    {
        "title": "Decoupling Augmentation Bias in Prompt Learning for Vision-Language\n  Models",
        "url": "http://arxiv.org/abs/2511.03367v1",
        "pub_date": "2025-11-05",
        "summary": "Recent advances in large-scale vision and language models have led to significant progress in zero-shot learning tasks. Methods such as CoOp and CoCoOp have shown that replacing handcrafted prompts with learnable vectors, known as prompt learning, can result in improved performance. However, these models often struggle to generalize to entirely unseen categories. While traditional zero-shot learning techniques benefit from various data augmentation strategies, prompt learning has primarily focused on text-based modifications, leaving the potential of image-based augmentation largely unexplored. In this work, we explore how image-level augmentations, particularly those that introduce attribute-specific variations, can support and enhance prompt learning. Our analysis examines the interaction between these augmentations and soft prompt frameworks, revealing their potential to improve generalization. We also identify a limitation in existing methods, such as CoCoOp, which do not provide explicit guidance for learning prompts that focus on semantically meaningful visual features. To address this, we propose Adding Attributes to Prompt Learning, AAPL, a novel method that introduces adversarial token embeddings to decouple superficial visual variations introduced by augmentation from class-relevant semantic representations. This decoupling enables the learned prompts to concentrate on visually discriminative features that align with the target categories. We conduct comprehensive experiments on eleven benchmark datasets, and AAPL consistently outperforms existing methods across few-shot, zero-shot, cross-dataset, and domain generalization settings. Our source code is publicly available at: https://github.com/Gahyeonkim09/AAPL",
        "translated": "近年来，大规模视觉与语言模型的进展推动了零样本学习任务的显著进步。诸如CoOp和CoCoOp等方法表明，用可学习向量替代手工设计的提示词（即提示学习）能够提升模型性能。然而，这些模型在面对完全未见类别时往往泛化能力不足。尽管传统零样本学习技术能够从多种数据增强策略中受益，但提示学习主要集中在文本层面的修改，图像层面增强的潜力尚未被充分挖掘。在本研究中，我们探讨了图像级增强（特别是引入属性特异性变化的增强）如何支持并提升提示学习的效果。我们的分析考察了这些增强与软提示框架之间的相互作用，揭示了其在提升泛化能力方面的潜力。同时，我们指出现有方法（如CoCoOp）存在一个局限：它们未能为学习聚焦于语义上有意义视觉特征的提示词提供显式指导。为此，我们提出了一种新方法——将属性加入提示学习（Adding Attributes to Prompt Learning, AAPL），该方法通过引入对抗性token嵌入，将增强所引入的表面视觉变化与类别相关的语义表征进行解耦。这种解耦机制使得学习到的提示词能够专注于与目标类别对齐的视觉判别性特征。我们在十一个基准数据集上进行了全面实验，AAPL在少样本、零样本、跨数据集以及领域泛化等设置下均显著优于现有方法。我们的源代码公开于：https://github.com/Gahyeonkim09/AAPL",
        "translated_title": "解耦提示学习中视觉-语言模型的增强偏差",
        "label": [],
        "label_reason": "论文聚焦于视觉-语言模型的提示学习与泛化，属于高阶视觉理解任务，非像素级图像恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出对抗性token嵌入解耦视觉偏差，对提示学习有显著改进，但非low-level图像处理创新。"
    },
    {
        "title": "Morpho-Genomic Deep Learning for Ovarian Cancer Subtype and Gene\n  Mutation Prediction from Histopathology",
        "url": "http://arxiv.org/abs/2511.03365v1",
        "pub_date": "2025-11-05",
        "summary": "Ovarian cancer remains one of the most lethal gynecological malignancies, largely due to late diagnosis and extensive heterogeneity across subtypes. Current diagnostic methods are limited in their ability to reveal underlying genomic variations essential for precision oncology. This study introduces a novel hybrid deep learning pipeline that integrates quantitative nuclear morphometry with deep convolutional image features to perform ovarian cancer subtype classification and gene mutation inference directly from Hematoxylin and Eosin (H&amp;E) histopathological images. Using $\\sim45,000$ image patches sourced from The Cancer Genome Atlas (TCGA) and public datasets, a fusion model combining a ResNet-50 Convolutional Neural Network (CNN) encoder and a Vision Transformer (ViT) was developed. This model successfully captured both local morphological texture and global tissue context. The pipeline achieved a robust overall subtype classification accuracy of $84.2\\%$ (Macro AUC of $0.87 \\pm 0.03$). Crucially, the model demonstrated the capacity for gene mutation inference with moderate-to-high accuracy: $AUC_{TP53} = 0.82 \\pm 0.02$, $AUC_{BRCA1} = 0.76 \\pm 0.04$, and $AUC_{ARID1A} = 0.73 \\pm 0.05$. Feature importance analysis established direct quantitative links, revealing that nuclear solidity and eccentricity were the dominant predictors for TP53 mutation. These findings validate that quantifiable histological phenotypes encode measurable genomic signals, paving the way for cost-effective, precision histopathology in ovarian cancer triage and diagnosis.",
        "translated": "卵巢癌仍然是最具致命性的妇科恶性肿瘤之一，主要原因在于诊断滞后以及各亚型之间存在广泛的异质性。目前的诊断方法在揭示精准肿瘤学所必需的潜在基因组变异方面能力有限。本研究提出了一种新型混合深度学习框架，该框架将定量核形态测量与深度卷积图像特征相结合，直接从苏木精-伊红（H&E）组织病理学图像中实现卵巢癌亚型分类及基因突变推断。利用来自癌症基因组图谱（TCGA）和公开数据集的约45,000个图像块，构建了一个融合ResNet-50卷积神经网络（CNN）编码器与视觉Transformer（ViT）的模型。该模型成功捕捉了局部形态纹理与全局组织上下文信息。该框架实现了稳健的整体亚型分类准确率84.2%（宏AUC为0.87 ± 0.03）。尤为重要的是，模型在基因突变推断方面表现出中等到较高的准确性：$AUC_{TP53} = 0.82 \\pm 0.02$，$AUC_{BRCA1} = 0.76 \\pm 0.04$，$AUC_{ARID1A} = 0.73 \\pm 0.05$。特征重要性分析建立了直接的定量关联，揭示核实度（nuclear solidity）和偏心率（eccentricity）是TP53突变的主要预测因子。这些发现验证了可量化的组织学表型编码了可测量的基因组信号，为卵巢癌分诊与诊断中实现经济高效的精准组织病理学提供了可行路径。",
        "translated_title": "形态-基因组深度学习用于从组织病理学图像预测卵巢癌亚型及基因突变",
        "label": [],
        "label_reason": "论文聚焦于卵巢癌亚型分类与基因突变预测，属于高阶医学图像分析，非像素级图像恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "提出融合CNN与ViT的模型，但核心为分类任务，无low-level图像处理创新。"
    },
    {
        "title": "UniAVGen: Unified Audio and Video Generation with Asymmetric Cross-Modal\n  Interactions",
        "url": "http://arxiv.org/abs/2511.03334v1",
        "pub_date": "2025-11-05",
        "summary": "Due to the lack of effective cross-modal modeling, existing open-source audio-video generation methods often exhibit compromised lip synchronization and insufficient semantic consistency. To mitigate these drawbacks, we propose UniAVGen, a unified framework for joint audio and video generation. UniAVGen is anchored in a dual-branch joint synthesis architecture, incorporating two parallel Diffusion Transformers (DiTs) to build a cohesive cross-modal latent space. At its heart lies an Asymmetric Cross-Modal Interaction mechanism, which enables bidirectional, temporally aligned cross-attention, thus ensuring precise spatiotemporal synchronization and semantic consistency. Furthermore, this cross-modal interaction is augmented by a Face-Aware Modulation module, which dynamically prioritizes salient regions in the interaction process. To enhance generative fidelity during inference, we additionally introduce Modality-Aware Classifier-Free Guidance, a novel strategy that explicitly amplifies cross-modal correlation signals. Notably, UniAVGen's robust joint synthesis design enables seamless unification of pivotal audio-video tasks within a single model, such as joint audio-video generation and continuation, video-to-audio dubbing, and audio-driven video synthesis. Comprehensive experiments validate that, with far fewer training samples (1.3M vs. 30.1M), UniAVGen delivers overall advantages in audio-video synchronization, timbre consistency, and emotion consistency.",
        "translated": "由于缺乏有效的跨模态建模，现有的开源音视频生成方法通常表现出唇形同步不准确以及语义一致性不足的问题。为缓解这些缺陷，我们提出 UniAVGen，一种用于联合音频与视频生成的统一框架。UniAVGen 采用双分支联合合成架构，通过两个并行的扩散变换器（DiTs）构建一个连贯的跨模态潜在空间。其核心是一个非对称跨模态交互机制，该机制实现了双向、时间对齐的跨注意力机制，从而确保精确的空间-时间同步与语义一致性。此外，该跨模态交互还通过一个面部感知调制模块（Face-Aware Modulation module）进行增强，该模块在交互过程中动态优先处理显著区域。为提升推理阶段的生成保真度，我们进一步引入了模态感知无分类器引导（Modality-Aware Classifier-Free Guidance），这是一种新颖的策略，能够显式增强跨模态相关性信号。值得注意的是，UniAVGen 强大的联合合成设计使得关键音视频任务（如联合音视频生成与续写、视频到音频的配音、音频驱动的视频合成）能够在单一模型中无缝统一。全面的实验验证表明，与现有方法相比（训练样本数1.3M vs. 30.1M），UniAVGen 在音视频同步、音色一致性及情感一致性方面均展现出整体优势。",
        "translated_title": "UniAVGen：基于非对称跨模态交互的统一音频与视频生成",
        "label": [],
        "label_reason": "论文聚焦音频-视频生成，属于高阶生成任务，不涉及图像像素级恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出不对称跨模态交互机制，对生成模型有改进，但非low-level图像处理创新。"
    },
    {
        "title": "Multi-Object Tracking Retrieval with LLaVA-Video: A Training-Free\n  Solution to MOT25-StAG Challenge",
        "url": "http://arxiv.org/abs/2511.03332v1",
        "pub_date": "2025-11-05",
        "summary": "In this report, we present our solution to the MOT25-Spatiotemporal Action Grounding (MOT25-StAG) Challenge. The aim of this challenge is to accurately localize and track multiple objects that match specific and free-form language queries, using video data of complex real-world scenes as input. We model the underlying task as a video retrieval problem and present a two-stage, zero-shot approach, combining the advantages of the SOTA tracking model FastTracker and Multi-modal Large Language Model LLaVA-Video. On the MOT25-StAG test set, our method achieves m-HIoU and HOTA scores of 20.68 and 10.73 respectively, which won second place in the challenge.",
        "translated": "在本报告中，我们提出了针对MOT25-Spatiotemporal Action Grounding（MOT25-StAG）挑战赛的解决方案。该挑战赛的目标是利用复杂真实场景的视频数据作为输入，准确地定位和跟踪与特定且自由形式的语言查询相匹配的多个目标。我们将该底层任务建模为一个视频检索问题，并提出了一种两阶段、零样本的方法，结合了当前最先进的跟踪模型FastTracker与多模态大语言模型LLaVA-Video的优势。在MOT25-StAG测试集上，我们的方法分别取得了20.68的m-HIoU和10.73的HOTA分数，最终在挑战赛中获得第二名。",
        "translated_title": "多目标跟踪检索基于LLaVA-Video：一种无需训练的MOT25-StAG挑战赛解决方案",
        "label": [],
        "label_reason": "论文属于多目标跟踪与语言查询定位，为high-level视觉任务，不涉及图像像素级恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "方法结合LLaVA-Video与FastTracker，属零样本多模态跟踪，无low-level图像处理创新。"
    },
    {
        "title": "Benchmarking the Thinking Mode of Multimodal Large Language Models in\n  Clinical Tasks",
        "url": "http://arxiv.org/abs/2511.03328v1",
        "pub_date": "2025-11-05",
        "summary": "A recent advancement in Multimodal Large Language Models (MLLMs) research is the emergence of \"reasoning MLLMs\" that offer explicit control over their internal thinking processes (normally referred as the \"thinking mode\") alongside the standard \"non-thinking mode\". This capability allows these models to engage in a step-by-step process of internal deliberation before generating a final response. With the rapid transition to and adoption of these \"dual-state\" MLLMs, this work rigorously evaluated how the enhanced reasoning processes of these MLLMs impact model performance and reliability in clinical tasks. This paper evaluates the active \"thinking mode\" capabilities of two leading MLLMs, Seed1.5-VL and Gemini-2.5-Flash, for medical applications. We assessed their performance on four visual medical tasks using VQA-RAD and ROCOv2 datasets. Our findings reveal that the improvement from activating the thinking mode remains marginal compared to the standard non-thinking mode for the majority of the tasks. Their performance on complex medical tasks such as open-ended VQA and medical image interpretation remains suboptimal, highlighting the need for domain-specific medical data and more advanced methods for medical knowledge integration.",
        "translated": "近年来，多模态大语言模型（MLLMs）研究的一个重要进展是“推理型 MLLMs”的出现，这类模型在标准“非推理模式”之外，提供了对其内部思考过程（通常称为“思考模式”）的显式控制能力。该能力使得模型在生成最终响应之前，能够进行逐步的内部推演。随着这类“双态”MLLMs的快速演进与广泛应用，本研究系统评估了其增强的推理过程对临床任务中模型性能与可靠性的影响。本文评估了两款主流MLLMs——Seed1.5-VL 和 Gemini-2.5-Flash——在医疗应用中的主动“思考模式”能力。我们基于 VQA-RAD 和 ROCOv2 数据集，对其在四项视觉医疗任务上的表现进行了评估。研究结果表明，对于大多数任务而言，激活思考模式所带来的性能提升相较于标准非思考模式仍较为有限。在开放性视觉问答和医学图像解读等复杂医疗任务中，其表现仍不理想，凸显了医疗领域专用数据以及更先进医学知识融合方法的必要性。",
        "translated_title": "多模态大语言模型在临床任务中思维模式的基准测试",
        "label": [],
        "label_reason": "论文聚焦多模态大模型在临床任务中的推理模式评估，属于高阶视觉理解，非像素级图像处理。",
        "relevance_score": 1,
        "novelty_score": 3,
        "novelty_reason": "评估现有模型推理模式，无新方法或架构创新，仅实验分析。"
    },
    {
        "title": "SurgViVQA: Temporally-Grounded Video Question Answering for Surgical\n  Scene Understanding",
        "url": "http://arxiv.org/abs/2511.03325v1",
        "pub_date": "2025-11-05",
        "summary": "Video Question Answering (VideoQA) in the surgical domain aims to enhance intraoperative understanding by enabling AI models to reason over temporally coherent events rather than isolated frames. Current approaches are limited to static image features, and available datasets often lack temporal annotations, ignoring the dynamics critical for accurate procedural interpretation. We propose SurgViVQA, a surgical VideoQA model that extends visual reasoning from static images to dynamic surgical scenes. It uses a Masked Video--Text Encoder to fuse video and question features, capturing temporal cues such as motion and tool--tissue interactions, which a fine-tuned large language model (LLM) then decodes into coherent answers. To evaluate its performance, we curated REAL-Colon-VQA, a colonoscopic video dataset that includes motion-related questions and diagnostic attributes, as well as out-of-template questions with rephrased or semantically altered formulations to assess model robustness. Experimental validation on REAL-Colon-VQA and the public EndoVis18-VQA dataset shows that SurgViVQA outperforms existing image-based VQA benchmark models, particularly in keyword accuracy, improving over PitVQA by +11\\% on REAL-Colon-VQA and +9\\% on EndoVis18-VQA. A perturbation study on the questions further confirms improved generalizability and robustness to variations in question phrasing. SurgViVQA and the REAL-Colon-VQA dataset provide a framework for temporally-aware understanding in surgical VideoQA, enabling AI models to interpret dynamic procedural contexts more effectively. Code and dataset available at https://github.com/madratak/SurgViVQA.",
        "translated": "手术领域视频问答（VideoQA）旨在通过使人工智能模型能够对具有时间连贯性的事件进行推理，而非仅处理孤立帧，从而提升术中理解能力。当前方法局限于静态图像特征，且现有数据集通常缺乏时间标注，忽视了准确理解手术流程所必需的动态信息。我们提出 SurgViVQA，一种面向手术场景的视频问答模型，将视觉推理从静态图像扩展至动态手术场景。该模型采用掩码视频-文本编码器（Masked Video--Text Encoder）融合视频与问题特征，捕捉运动、器械-组织交互等时间线索，随后由微调后的大型语言模型（LLM）解码生成连贯答案。为评估其性能，我们构建了 REAL-Colon-VQA 数据集，该数据集包含结肠镜视频，涵盖与运动相关的问题、诊断属性，以及经过重述或语义修改的非模板化问题，以评估模型的鲁棒性。在 REAL-Colon-VQA 和公开的 EndoVis18-VQA 数据集上的实验验证表明，SurgViVQA 在关键词准确率方面显著优于现有基于图像的 VQA 基准模型，在 REAL-Colon-VQA 上较 PitVQA 提升 +11%，在 EndoVis18-VQA 上提升 +9%。对问题进行扰动的进一步研究表明，该模型在问题表述变化下具有更强的泛化能力和鲁棒性。SurgViVQA 与 REAL-Colon-VQA 数据集共同为手术视频问答中的时间感知理解提供了框架，使人工智能模型能够更有效地解释动态手术过程。代码与数据集可于 https://github.com/madratak/SurgViVQA 获取。",
        "translated_title": "SurgViVQA：面向手术场景理解的时序锚定视频问答",
        "label": [],
        "label_reason": "论文聚焦手术场景视频问答，属于高阶视觉理解任务，不涉及像素级图像恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "提出基于掩码视频-文本编码器的视频问答框架，属常规多模态模型改进，无突破性创新。"
    },
    {
        "title": "Diffusion-SDPO: Safeguarded Direct Preference Optimization for Diffusion\n  Models",
        "url": "http://arxiv.org/abs/2511.03317v1",
        "pub_date": "2025-11-05",
        "summary": "Text-to-image diffusion models deliver high-quality images, yet aligning them with human preferences remains challenging. We revisit diffusion-based Direct Preference Optimization (DPO) for these models and identify a critical pathology: enlarging the preference margin does not necessarily improve generation quality. In particular, the standard Diffusion-DPO objective can increase the reconstruction error of both winner and loser branches. Consequently, degradation of the less-preferred outputs can become sufficiently severe that the preferred branch is also adversely affected even as the margin grows. To address this, we introduce Diffusion-SDPO, a safeguarded update rule that preserves the winner by adaptively scaling the loser gradient according to its alignment with the winner gradient. A first-order analysis yields a closed-form scaling coefficient that guarantees the error of the preferred output is non-increasing at each optimization step. Our method is simple, model-agnostic, broadly compatible with existing DPO-style alignment frameworks and adds only marginal computational overhead. Across standard text-to-image benchmarks, Diffusion-SDPO delivers consistent gains over preference-learning baselines on automated preference, aesthetic, and prompt alignment metrics. Code is publicly available at https://github.com/AIDC-AI/Diffusion-SDPO.",
        "translated": "文本到图像扩散模型能够生成高质量图像，但使其与人类偏好对齐仍具挑战性。我们重新审视了针对这些模型的基于扩散的直接偏好优化（DPO）方法，并发现一个关键问题：扩大偏好间隔并不一定能够提升生成质量。具体而言，标准的 Diffusion-DPO 目标函数可能导致获胜分支和失败分支的重建误差同时增加。因此，当偏好间隔增大时，较差输出的退化可能变得足够严重，以至于即使偏好分支也受到负面影响。为解决这一问题，我们提出 Diffusion-SDPO，一种受保护的更新规则，通过自适应地根据失败分支梯度与获胜分支梯度的对齐程度来缩放失败分支梯度，从而保留获胜分支。一阶分析得出一个闭式缩放系数，可保证在每一步优化中，偏好输出的误差不会增加。我们的方法简单、模型无关，与现有 DPO 风格的对齐框架广泛兼容，且仅带来极小的计算开销。在标准文本到图像基准测试中，Diffusion-SDPO 在自动化偏好、美学和提示对齐指标上均显著优于偏好学习基线方法。代码已公开于 https://github.com/AIDC-AI/Diffusion-SDPO。",
        "translated_title": "Diffusion-SDPO：面向扩散模型的安全直接偏好优化",
        "label": [],
        "label_reason": "论文聚焦文本到图像生成模型的偏好对齐，属于图像生成而非图像恢复/增强任务。",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出梯度保护机制，改进DPO训练策略，对扩散模型对齐有显著提升。"
    },
    {
        "title": "Unified Long Video Inpainting and Outpainting via Overlapping High-Order\n  Co-Denoising",
        "url": "http://arxiv.org/abs/2511.03272v1",
        "pub_date": "2025-11-05",
        "summary": "Generating long videos remains a fundamental challenge, and achieving high controllability in video inpainting and outpainting is particularly demanding. To address both of these challenges simultaneously and achieve controllable video inpainting and outpainting for long video clips, we introduce a novel and unified approach for long video inpainting and outpainting that extends text-to-video diffusion models to generate arbitrarily long, spatially edited videos with high fidelity. Our method leverages LoRA to efficiently fine-tune a large pre-trained video diffusion model like Alibaba's Wan 2.1 for masked region video synthesis, and employs an overlap-and-blend temporal co-denoising strategy with high-order solvers to maintain consistency across long sequences. In contrast to prior work that struggles with fixed-length clips or exhibits stitching artifacts, our system enables arbitrarily long video generation and editing without noticeable seams or drift. We validate our approach on challenging inpainting/outpainting tasks including editing or adding objects over hundreds of frames and demonstrate superior performance to baseline methods like Wan 2.1 model and VACE in terms of quality (PSNR/SSIM), and perceptual realism (LPIPS). Our method enables practical long-range video editing with minimal overhead, achieved a balance between parameter efficient and superior performance.",
        "translated": "生成长视频仍是一项根本性挑战，而在视频修复与外推中实现高可控性尤为困难。为同时应对上述两个挑战，并实现对长视频片段的可控修复与外推，我们提出了一种新颖且统一的长视频修复与外推方法，该方法将文本到视频扩散模型扩展为能够生成任意长度、空间编辑视频的框架，且具有高保真度。我们的方法利用LoRA高效微调大型预训练视频扩散模型（如阿里巴巴的Wan 2.1），以实现对掩码区域的视频合成，并采用带高阶求解器的重叠融合时序联合去噪策略，以保持长序列中的一致性。与以往工作相比，后者通常受限于固定长度片段或产生拼接伪影，而我们的系统能够生成任意长度的视频并进行编辑，且无明显接缝或漂移现象。我们在具有挑战性的修复/外推任务上验证了该方法，包括在数百帧内编辑或添加物体，并在质量（PSNR/SSIM）和感知真实感（LPIPS）方面优于基线方法（如Wan 2.1模型和VACE）。我们的方法实现了低开销的长距离视频编辑，兼顾了参数效率与卓越性能。",
        "translated_title": "统一的长视频内补与外补：基于重叠高阶联合去噪的方法",
        "label": [],
        "label_reason": "论文聚焦于视频生成与编辑，属于图像生成类high-level任务，非像素级图像恢复或增强。",
        "relevance_score": 3,
        "novelty_score": 6,
        "novelty_reason": "提出重叠高阶去噪策略，改进视频生成一致性，但未解决低层图像退化问题。"
    },
    {
        "title": "IEC3D-AD: A 3D Dataset of Industrial Equipment Components for\n  Unsupervised Point Cloud Anomaly Detection",
        "url": "http://arxiv.org/abs/2511.03267v1",
        "pub_date": "2025-11-05",
        "summary": "3D anomaly detection (3D-AD) plays a critical role in industrial manufacturing, particularly in ensuring the reliability and safety of core equipment components. Although existing 3D datasets like Real3D-AD and MVTec 3D-AD offer broad application support, they fall short in capturing the complexities and subtle defects found in real industrial environments. This limitation hampers precise anomaly detection research, especially for industrial equipment components (IEC) such as bearings, rings, and bolts. To address this challenge, we have developed a point cloud anomaly detection dataset (IEC3D-AD) specific to real industrial scenarios. This dataset is directly collected from actual production lines, ensuring high fidelity and relevance. Compared to existing datasets, IEC3D-AD features significantly improved point cloud resolution and defect annotation granularity, facilitating more demanding anomaly detection tasks. Furthermore, inspired by generative 2D-AD methods, we introduce a novel 3D-AD paradigm (GMANet) on IEC3D-AD. This paradigm generates synthetic point cloud samples based on geometric morphological analysis, then reduces the margin and increases the overlap between normal and abnormal point-level features through spatial discrepancy optimization. Extensive experiments demonstrate the effectiveness of our method on both IEC3D-AD and other datasets.",
        "translated": "三维异常检测（3D-AD）在工业制造中发挥着关键作用，尤其在确保核心设备部件的可靠性和安全性方面。尽管现有的三维数据集（如 Real3D-AD 和 MVTec 3D-AD）提供了广泛的应用支持，但在捕捉真实工业环境中复杂且细微的缺陷方面仍存在不足。这一局限性阻碍了精确异常检测研究的进展，尤其是在针对工业设备部件（IEC）如轴承、环形件和螺栓等场景中。为应对这一挑战，我们构建了一个面向真实工业场景的点云异常检测数据集（IEC3D-AD）。该数据集直接采集自实际生产线，确保了高保真度和高相关性。与现有数据集相比，IEC3D-AD 在点云分辨率和缺陷标注粒度方面均有显著提升，从而支持更具挑战性的异常检测任务。此外，受生成式二维异常检测（2D-AD）方法的启发，我们在 IEC3D-AD 上提出了一种新颖的三维异常检测范式（GMANet）。该范式基于几何形态分析生成合成点云样本，并通过空间差异优化，缩小正常与异常点级特征之间的边界，同时增加其重叠区域。大量实验表明，我们的方法在 IEC3D-AD 以及其他数据集上均表现出良好的有效性。",
        "translated_title": "IEC3D-AD：用于无监督点云异常检测的工业设备部件3D数据集",
        "label": [],
        "label_reason": "论文聚焦3D点云异常检测，属于高阶视觉任务，非像素级图像恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "提出新3D-AD范式GMANet，但基于生成方法，未涉及图像质量恢复，创新有限。"
    },
    {
        "title": "Enhancing Medical Image Segmentation via Heat Conduction Equation",
        "url": "http://arxiv.org/abs/2511.03260v1",
        "pub_date": "2025-11-05",
        "summary": "Medical image segmentation has been significantly advanced by deep learning architectures, notably U-Net variants. However, existing models struggle to achieve efficient global context modeling and long-range dependency reasoning under practical computational budgets simultaneously. In this work, we propose a novel hybrid architecture utilizing U-Mamba with Heat Conduction Equation. Our model combines Mamba-based state-space modules for efficient long-range reasoning with Heat Conduction Operators (HCOs) in the bottleneck layers, simulating frequency-domain thermal diffusion for enhanced semantic abstraction. Experimental results on multimodal abdominal CT and MRI datasets demonstrate that the proposed model consistently outperforms strong baselines, validating its effectiveness and generalizability. It suggest that blending state-space dynamics with heat-based global diffusion offers a scalable and interpretable solution for medical segmentation tasks.",
        "translated": "医学图像分割在深度学习架构（尤其是U-Net变体）的推动下取得了显著进展。然而，现有模型在实际计算预算下难以同时实现高效的全局上下文建模与长距离依赖推理。在本工作中，我们提出了一种新颖的混合架构，结合U-Mamba与热传导方程。该模型在瓶颈层中融合基于Mamba的状态空间模块以实现高效的长距离推理，并引入热传导算子（HCOs），模拟频域中的热扩散过程，以增强语义抽象能力。在多模态腹部CT和MRI数据集上的实验结果表明，所提模型始终优于多个强基线方法，验证了其有效性与泛化能力。结果表明，将状态空间动态与基于热扩散的全局传播相结合，为医学图像分割任务提供了一种可扩展且可解释的解决方案。",
        "translated_title": "基于热传导方程的医学图像分割增强",
        "label": [],
        "label_reason": "论文聚焦医学图像分割，属于高阶视觉任务，非像素级图像恢复或增强。",
        "relevance_score": 3,
        "novelty_score": 6,
        "novelty_reason": "提出混合架构结合Mamba与热传导方程，对长距离依赖建模有改进，但非low-level创新。"
    },
    {
        "title": "CLAX: Fast and Flexible Neural Click Models in JAX",
        "url": "http://arxiv.org/abs/2511.03620v1",
        "pub_date": "2025-11-05",
        "summary": "CLAX is a JAX-based library that implements classic click models using modern gradient-based optimization. While neural click models have emerged over the past decade, complex click models based on probabilistic graphical models (PGMs) have not systematically adopted gradient-based optimization, preventing practitioners from leveraging modern deep learning frameworks while preserving the interpretability of classic models. CLAX addresses this gap by replacing EM-based optimization with direct gradient-based optimization in a numerically stable manner. The framework's modular design enables the integration of any component, from embeddings and deep networks to custom modules, into classic click models for end-to-end optimization. We demonstrate CLAX's efficiency by running experiments on the full Baidu-ULTR dataset comprising over a billion user sessions in $\\approx$ 2 hours on a single GPU, orders of magnitude faster than traditional EM approaches. CLAX implements ten classic click models, serving both industry practitioners seeking to understand user behavior and improve ranking performance at scale and researchers developing new click models. CLAX is available at: https://github.com/philipphager/clax",
        "translated": "CLAX 是一个基于 JAX 的库，采用现代基于梯度的优化方法实现了经典的点击模型。尽管在过去十年中神经点击模型逐渐兴起，但基于概率图模型（PGMs）的复杂点击模型尚未系统性地采用基于梯度的优化方法，这使得从业者无法在保留经典模型可解释性的同时，充分利用现代深度学习框架。CLAX 通过以数值稳定的方式将基于 EM 的优化替换为直接的基于梯度的优化，填补了这一空白。该框架采用模块化设计，支持将嵌入、深度网络或自定义模块等任意组件集成到经典点击模型中，实现端到端优化。我们在包含超过十亿用户会话的完整 Baidu-ULTR 数据集上进行了实验，仅在单块 GPU 上运行约 2 小时即可完成，速度比传统的 EM 方法快数个数量级。CLAX 实现了十种经典点击模型，既服务于希望理解用户行为并大规模提升排序性能的工业界从业者，也服务于正在开发新型点击模型的研究人员。CLAX 可在以下地址获取：https://github.com/philipphager/clax",
        "translated_title": "CLAX：基于JAX的快速且灵活的神经点击模型",
        "label": [
            "精排（Ranking）",
            "推荐系统评估"
        ],
        "label_reason": "论文提出基于梯度优化的点击模型框架，用于提升推荐排序效率，与精排环节直接相关。",
        "relevance_score": 7,
        "novelty_score": 6,
        "novelty_reason": "将经典点击模型与现代梯度优化结合，提升训练效率，但未提出全新架构或范式。"
    },
    {
        "title": "A Semantic Encoding of Object Centric Event Data",
        "url": "http://arxiv.org/abs/2511.03351v1",
        "pub_date": "2025-11-05",
        "summary": "The Object-Centric Event Data (OCED) is a novel meta-model aimed at providing a common ground for process data records centered around events and objects. One of its objectives is to foster interoperability and process information exchange. In this context, the integration of data from different providers, the combination of multiple processes, and the enhancement of knowledge inference are novel challenges. Semantic Web technologies can enable the creation of a machine-readable OCED description enriched through ontology-based relationships and entity categorization. In this paper, we introduce an approach built upon Semantic Web technologies for the realization of semantic-enhanced OCED, with the aim to strengthen process data reasoning, interconnect information sources, and boost expressiveness.",
        "translated": "对象中心事件数据（OCED）是一种新型元模型，旨在为以事件和对象为中心的过程数据记录提供统一基础。其目标之一是促进互操作性与过程信息交换。在此背景下，整合来自不同提供方的数据、融合多个过程以及增强知识推理能力，构成了新的挑战。语义网技术能够支持构建一种机器可读的OCED描述，通过基于本体的关系和实体分类实现语义增强。本文提出一种基于语义网技术的方法，以实现语义增强的OCED，旨在加强过程数据推理能力、连接信息源并提升表达能力。",
        "translated_title": "对象中心事件数据的语义编码",
        "label": [],
        "label_reason": "论文聚焦于事件与对象为中心的数据语义建模，属于通用数据建模与语义网技术，与推荐系统无直接关联。",
        "relevance_score": 2,
        "novelty_score": 5,
        "novelty_reason": "提出基于语义网的OCED建模方法，虽在数据整合与推理上有改进，但未涉及推荐系统核心机制。"
    },
    {
        "title": "Discourse-Aware Scientific Paper Recommendation via QA-Style\n  Summarization and Multi-Level Contrastive Learning",
        "url": "http://arxiv.org/abs/2511.03330v1",
        "pub_date": "2025-11-05",
        "summary": "The rapid growth of open-access (OA) publications has intensified the challenge of identifying relevant scientific papers. Due to privacy constraints and limited access to user interaction data, recent efforts have shifted toward content-based recommendation, which relies solely on textual information. However, existing models typically treat papers as unstructured text, neglecting their discourse organization and thereby limiting semantic completeness and interpretability. To address these limitations, we propose OMRC-MR, a hierarchical framework that integrates QA-style OMRC (Objective, Method, Result, Conclusion) summarization, multi-level contrastive learning, and structure-aware re-ranking for scholarly recommendation. The QA-style summarization module converts raw papers into structured and discourse-consistent representations, while multi-level contrastive objectives align semantic representations across metadata, section, and document levels. The final re-ranking stage further refines retrieval precision through contextual similarity calibration. Experiments on DBLP, S2ORC, and the newly constructed Sci-OMRC dataset demonstrate that OMRC-MR consistently surpasses state-of-the-art baselines, achieving up to 7.2% and 3.8% improvements in Precision@10 and Recall@10, respectively. Additional evaluations confirm that QA-style summarization produces more coherent and factually complete representations. Overall, OMRC-MR provides a unified and interpretable content-based paradigm for scientific paper recommendation, advancing trustworthy and privacy-aware scholarly information retrieval.",
        "translated": "开放获取（OA）出版物的快速增长加剧了识别相关学术论文的挑战。由于隐私限制以及用户交互数据获取受限，近期研究工作逐渐转向基于内容的推荐方法，该方法仅依赖文本信息。然而，现有模型通常将论文视为无结构文本，忽视了其话语组织结构，从而限制了语义完整性和可解释性。为解决这些局限，我们提出 OMRC-MR，一种层级化框架，融合了问答式 OMRC（Objective, Method, Result, Conclusion）摘要、多层级对比学习以及结构感知的重排，用于学术推荐。问答式摘要模块将原始论文转化为结构化且话语一致的表示，而多层级对比学习目标则在元数据、章节和文档层级上对齐语义表示。最终的重排阶段通过上下文相似度校准进一步提升召回精度。在 DBLP、S2ORC 以及新构建的 Sci-OMRC 数据集上的实验表明，OMRC-MR 一致超越了当前最先进的基线模型，在 Precision@10 和 Recall@10 上分别实现了最高 7.2% 和 3.8% 的提升。额外评估证实，问答式摘要生成的表示更具连贯性且事实更完整。总体而言，OMRC-MR 为学术论文推荐提供了一种统一且可解释的基于内容的范式，推动了可信、隐私保护的学术信息检索的发展。",
        "translated_title": "基于问答式摘要与多层级对比学习的语篇感知学术论文推荐",
        "label": [
            "召回",
            "重排",
            "负采样与对比学习",
            "序列推荐",
            "通用推荐技术"
        ],
        "label_reason": "论文提出结构化摘要与多层级对比学习，用于学术论文推荐，涉及召回与重排环节，具备推荐系统核心要素。",
        "relevance_score": 8,
        "novelty_score": 8,
        "novelty_reason": "创新性地结合QA式摘要与多层级对比学习，提升语义一致性与可解释性，方法新颖且有效。"
    },
    {
        "title": "Two thousand years of the oracle problem. Insights from Ancient Delphi\n  on the future of blockchain oracles",
        "url": "http://arxiv.org/abs/2511.03319v1",
        "pub_date": "2025-11-05",
        "summary": "The oracle problem refers to the inability of an agent to know if the information coming from an oracle is authentic and unbiased. In ancient times, philosophers and historians debated on how to evaluate, increase, and secure the reliability of oracle predictions, particularly those from Delphi, which pertained to matters of state. Today, we refer to data carriers for automatic machines as oracles, but establishing a secure channel between these oracles and the real world still represents a challenge. Despite numerous efforts, this problem remains mostly unsolved, and the recent advent of blockchain oracles has added a layer of complexity because of the decentralization of blockchains. This paper conceptually connects Delphic and modern blockchain oracles, developing a comparative framework. Leveraging blockchain oracle taxonomy, lexical analysis is also performed on 167 Delphic queries to shed light on the relationship between oracle answer quality and question type. The presented framework aims first at revealing commonalities between classical and computational oracles and then at enriching the oracle analysis within each field. This study contributes to the computer science literature by proposing strategies to improve the reliability of blockchain oracles based on insights from Delphi and to classical literature by introducing a framework that can also be applied to interpret and classify other ancient oracular mechanisms.",
        "translated": "“神谕问题”指的是代理无法判断来自神谕的信息是否真实且无偏。在古代，哲学家和历史学家曾就如何评估、提升和保障神谕预言的可靠性展开辩论，尤其是德尔斐神谕，其内容往往涉及国家事务。如今，我们将自动机器的数据载体称为“神谕”，但建立这些神谕与现实世界之间的安全通道仍是一项挑战。尽管已有诸多努力，该问题至今大多仍未解决；而近期区块链神谕的出现，由于区块链的去中心化特性，更增加了问题的复杂性。本文从概念上将德尔斐神谕与现代区块链神谕联系起来，构建了一个比较框架。基于区块链神谕的分类体系，本文还对167条德尔斐神谕查询进行了词汇分析，以揭示神谕回答质量与问题类型之间的关系。所提出的框架旨在首先揭示古典神谕与计算神谕之间的共性，进而深化各领域内对神谕的分析。本研究通过借鉴德尔斐神谕的洞见，为计算机科学领域提出了提升区块链神谕可靠性的策略；同时，也为古典文献研究提供了一个可应用于解释和分类其他古代神谕机制的分析框架。",
        "translated_title": "两千年神谕问题：来自古德尔斐的启示与区块链神谕的未来",
        "label": [],
        "label_reason": "论文讨论区块链预言机可靠性，与推荐系统无直接关联。",
        "relevance_score": 1,
        "novelty_score": 4,
        "novelty_reason": "提出古典与现代预言机对比框架，属跨学科研究，非推荐领域创新。"
    },
    {
        "title": "KScaNN: Scalable Approximate Nearest Neighbor Search on Kunpeng",
        "url": "http://arxiv.org/abs/2511.03298v1",
        "pub_date": "2025-11-05",
        "summary": "Approximate Nearest Neighbor Search (ANNS) is a cornerstone algorithm for information retrieval, recommendation systems, and machine learning applications. While x86-based architectures have historically dominated this domain, the increasing adoption of ARM-based servers in industry presents a critical need for ANNS solutions optimized on ARM architectures. A naive port of existing x86 ANNS algorithms to ARM platforms results in a substantial performance deficit, failing to leverage the unique capabilities of the underlying hardware. To address this challenge, we introduce KScaNN, a novel ANNS algorithm co-designed for the Kunpeng 920 ARM architecture. KScaNN embodies a holistic approach that synergizes sophisticated, data aware algorithmic refinements with carefully-designed hardware specific optimizations. Its core contributions include: 1) novel algorithmic techniques, including a hybrid intra-cluster search strategy and an improved PQ residual calculation method, which optimize the search process at a higher level; 2) an ML-driven adaptive search module that provides adaptive, per-query tuning of search parameters, eliminating the inefficiencies of static configurations; and 3) highly-optimized SIMD kernels for ARM that maximize hardware utilization for the critical distance computation workloads. The experimental results demonstrate that KScaNN not only closes the performance gap but establishes a new standard, achieving up to a 1.63x speedup over the fastest x86-based solution. This work provides a definitive blueprint for achieving leadership-class performance for vector search on modern ARM architectures and underscores",
        "translated": "近似最近邻搜索（ANNS）是信息检索、推荐系统及机器学习应用中的核心算法。尽管基于x86的架构长期以来主导该领域，但工业界对基于ARM的服务器日益广泛采用，凸显了亟需针对ARM架构优化的ANNS解决方案。将现有的x86 ANNS算法直接移植到ARM平台会导致显著的性能下降，无法充分利用底层硬件的独特能力。为应对这一挑战，我们提出KScaNN，一种专为鲲鹏920 ARM架构协同设计的新型ANNS算法。KScaNN采用整体性方法，将精巧的数据感知算法改进与精心设计的硬件特定优化相结合。其核心贡献包括：1）新颖的算法技术，包括混合的簇内搜索策略和改进的PQ残差计算方法，从更高层次优化搜索过程；2）基于机器学习的自适应搜索模块，能够针对每个查询动态调整搜索参数，消除静态配置带来的效率损失；3）针对ARM架构高度优化的SIMD内核，最大化硬件利用率，以应对关键的距离计算负载。实验结果表明，KScaNN不仅弥补了性能差距，更树立了新的基准，相较于最快的x86方案实现了最高达1.63倍的加速。本工作为在现代ARM架构上实现向量搜索的领先级性能提供了明确的蓝图，并凸显了……",
        "translated_title": "KScaNN：基于鲲鹏平台的可扩展近似最近邻搜索",
        "label": [
            "召回",
            "通用推荐技术"
        ],
        "label_reason": "ANNS是推荐系统召回环节核心技术，论文优化ARM架构下的向量搜索，与推荐召回强相关。",
        "relevance_score": 5,
        "novelty_score": 7,
        "novelty_reason": "提出混合簇内搜索、自适应搜索模块及SIMD优化，对ANNS有显著改进，非简单移植。"
    },
    {
        "title": "Beyond Ranked Lists: The SARAL Framework for Cross-Lingual Document Set\n  Retrieval",
        "url": "http://arxiv.org/abs/2511.03228v1",
        "pub_date": "2025-11-05",
        "summary": "Machine Translation for English Retrieval of Information in Any Language (MATERIAL) is an IARPA initiative targeted to advance the state of cross-lingual information retrieval (CLIR). This report provides a detailed description of Information Sciences Institute's (ISI's) Summarization and domain-Adaptive Retrieval Across Language's (SARAL's) effort for MATERIAL. Specifically, we outline our team's novel approach to handle CLIR with emphasis in developing an approach amenable to retrieve a query-relevant document \\textit{set}, and not just a ranked document-list. In MATERIAL's Phase-3 evaluations, SARAL exceeded the performance of other teams in five out of six evaluation conditions spanning three different languages (Farsi, Kazakh, and Georgian).",
        "translated": "机器翻译用于任意语言信息的英文检索（MATERIAL）是IARPA发起的一项计划，旨在推动跨语言信息检索（CLIR）技术的发展。本报告详细描述了信息科学研究所（ISI）在MATERIAL项目中开展的“跨语言摘要与领域自适应检索”（SARAL）工作。具体而言，我们概述了团队提出的新型CLIR处理方法，重点在于开发一种能够检索与查询相关的文档集合（document set）而非仅生成排序文档列表的方法。在MATERIAL第三阶段的评估中，SARAL在涵盖三种不同语言（波斯语、哈萨克语和格鲁吉亚语）的六项评估条件中，有五项表现优于其他团队。",
        "translated_title": "超越排序列表：用于跨语言文档集检索的SARAL框架",
        "label": [],
        "label_reason": "论文聚焦跨语言文档集检索，属于信息检索范畴，与推荐系统无直接关联。",
        "relevance_score": 2,
        "novelty_score": 5,
        "novelty_reason": "提出文档集检索新方法，但属于信息检索领域常规改进，无推荐系统创新。"
    },
    {
        "title": "Hybrid Fact-Checking that Integrates Knowledge Graphs, Large Language\n  Models, and Search-Based Retrieval Agents Improves Interpretable Claim\n  Verification",
        "url": "http://arxiv.org/abs/2511.03217v1",
        "pub_date": "2025-11-05",
        "summary": "Large language models (LLMs) excel in generating fluent utterances but can lack reliable grounding in verified information. At the same time, knowledge-graph-based fact-checkers deliver precise and interpretable evidence, yet suffer from limited coverage or latency. By integrating LLMs with knowledge graphs and real-time search agents, we introduce a hybrid fact-checking approach that leverages the individual strengths of each component. Our system comprises three autonomous steps: 1) a Knowledge Graph (KG) Retrieval for rapid one - hop lookups in DBpedia, 2) an LM-based classification guided by a task-specific labeling prompt, producing outputs with internal rule-based logic, and 3) a Web Search Agent invoked only when KG coverage is insufficient. Our pipeline achieves an F1 score of 0.93 on the FEVER benchmark on the Supported/Refuted split without task- specific fine - tuning. To address Not enough information cases, we conduct a targeted reannotation study showing that our approach frequently uncovers valid evidence for claims originally labeled as Not Enough Information (NEI), as confirmed by both expert annotators and LLM reviewers. With this paper, we present a modular, opensource fact-checking pipeline with fallback strategies and generalization across datasets.",
        "translated": "大语言模型（LLM）在生成流畅语句方面表现出色，但在可靠地依托经验证实的信息方面可能存在不足。与此同时，基于知识图谱的事实核查器能够提供精确且可解释的证据，但受限于覆盖范围不足或响应延迟。通过将大语言模型与知识图谱及实时搜索代理相结合，我们提出了一种混合式事实核查方法，充分利用各组件的各自优势。我们的系统包含三个自主步骤：1）知识图谱（KG）召回，用于在DBpedia中快速进行单跳查询；2）基于语言模型的分类，由特定任务的标注提示引导，输出结果具备内部基于规则的逻辑；3）当知识图谱覆盖不足时，仅调用网络搜索代理。该流水线在FEVER基准测试的“支持/反驳”划分上，无需特定任务微调即达到0.93的F1分数。为应对“信息不足”（Not enough information）情形，我们开展了一项针对性的重新标注研究，结果表明，我们的方法频繁发现原本被标注为“信息不足”（NEI）的声明的有效证据，这一结论经由专家标注员和大语言模型评审员共同验证。本文提出了一种模块化、开源的事实核查流水线，具备降级策略，并能在不同数据集上实现泛化。",
        "translated_title": "融合知识图谱、大语言模型与基于检索的代理的混合事实核查方法提升可解释性声明验证",
        "label": [],
        "label_reason": "论文聚焦事实核查，虽用LLM和检索，但非推荐系统核心环节，与推荐无直接关联。",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出混合事实核查框架，融合KG、LLM与搜索代理，架构设计新颖，但非推荐领域创新。"
    },
    {
        "title": "Russian Contribution to Coronary Artery Disease Research: A\n  Scientometric Mapping of Publications",
        "url": "http://arxiv.org/abs/2511.03215v1",
        "pub_date": "2025-11-05",
        "summary": "The present study attempts to highlight the research output generated in Russia in coronary artery disease (CAD) research during the period 1990-2019 to understand the distribution of research output, top journals for publications, and most prolific authors, authorship pattern, and citation pattern. This study is based on secondary data extracted from the Science Citation Index (SCI), which is an integral component of the Web of Science. Descriptive and inferential statistical techniques were applied in the study. There were 5058 articles by Russian scholars in coronary artery disease during 1990-2019; they preferred to publish in Russian journals. The research contributions were in the form of research articles, meeting abstracts and reviews with a consistent drop in the number of editorial material and article; proceedings paper with time. Co-authorship was the norm in coronary artery disease research, with a steady increase in the number of multi-author documents in recent years.",
        "translated": "本研究旨在揭示1990-2019年间俄罗斯在冠状动脉疾病（CAD）研究领域所产生的研究成果，以了解其研究成果的分布情况、主要发表期刊、高产作者、作者署名模式及引用模式。本研究基于从科学引文索引（SCI）中提取的二手数据，SCI是Web of Science的重要组成部分。研究中应用了描述性统计与推断性统计技术。1990-2019年间，俄罗斯学者在冠状动脉疾病领域共发表了5058篇论文，其发表偏好为俄罗斯本土期刊。研究成果主要以研究论文、会议摘要和综述形式呈现，而随时间推移，社论类材料和会议论文的数量呈现持续下降趋势。在冠状动脉疾病研究中，合作署名已成为常态，近年来多作者文献的数量也呈现稳步增长趋势。",
        "translated_title": "俄罗斯在冠状动脉疾病研究中的贡献：一篇基于文献计量学的出版物映射分析",
        "label": [],
        "label_reason": "论文为医学领域文献计量研究，与推荐系统无直接关联。",
        "relevance_score": 1,
        "novelty_score": 1,
        "novelty_reason": "研究方法为常规统计分析，无技术或算法创新。"
    },
    {
        "title": "A Study on Library Resources with Services Satisfaction based on Library\n  Users Affiliated Colleges to Solapur University",
        "url": "http://arxiv.org/abs/2511.03209v1",
        "pub_date": "2025-11-05",
        "summary": "The main aim of this study was to assess and evaluate user satisfaction with library resources and services among library users associated with Solapur University. The current research shows the level of users satisfaction with different library resources and services offered by college libraries. The research found that a vast number of respondents were pleased with library facilities and services. The research is designed to achieve users satisfaction in the library to investigate the level of satisfaction towards library resources and services with regards to 26 colleges of Solapur University based in Maharashtra. Information in the form of data has been collected from colleges and on the basis of users results; analysis needs to analyze users satisfaction.",
        "translated": "本研究的主要目的是评估和衡量索拉布尔大学（Solapur University）所属图书馆用户对图书馆资源与服务的满意度。当前研究展示了学院图书馆所提供的各类资源与服务在用户中的满意度水平。研究发现，绝大多数受访者对图书馆的设施与服务表示满意。本研究旨在通过调查马哈拉施特拉邦（Maharashtra）索拉布尔大学下属26所学院的用户，探究其对图书馆资源与服务的满意度水平。研究通过从各学院收集数据，并基于用户反馈结果，对用户满意度进行分析。",
        "translated_title": "基于索拉布尔大学附属学院图书馆用户对馆藏资源与服务满意度的研究",
        "label": [],
        "label_reason": "论文研究图书馆用户满意度，属于教育管理或用户调研，与推荐系统无直接关联。",
        "relevance_score": 1,
        "novelty_score": 1,
        "novelty_reason": "研究方法为传统问卷调查与数据分析，无技术或算法创新。"
    },
    {
        "title": "Generative Sequential Recommendation via Hierarchical Behavior Modeling",
        "url": "http://arxiv.org/abs/2511.03155v1",
        "pub_date": "2025-11-05",
        "summary": "Recommender systems in multi-behavior domains, such as advertising and e-commerce, aim to guide users toward high-value but inherently sparse conversions. Leveraging auxiliary behaviors (e.g., clicks, likes, shares) is therefore essential. Recent progress on generative recommendations has brought new possibilities for multi-behavior sequential recommendation. However, existing generative approaches face two significant challenges: 1) Inadequate Sequence Modeling: capture the complex, cross-level dependencies within user behavior sequences, and 2) Lack of Suitable Datasets: publicly available multi-behavior recommendation datasets are almost exclusively derived from e-commerce platforms, limiting the validation of feasibility in other domains, while also lacking sufficient side information for semantic ID generation. To address these issues, we propose a novel generative framework, GAMER (Generative Augmentation and Multi-lEvel behavior modeling for Recommendation), built upon a decoder-only backbone. GAMER introduces a cross-level interaction layer to capture hierarchical dependencies among behaviors and a sequential augmentation strategy that enhances robustness in training. To further advance this direction, we collect and release ShortVideoAD, a large-scale multi-behavior dataset from a mainstream short-video platform, which differs fundamentally from existing e-commerce datasets and provides pretrained semantic IDs for research on generative methods. Extensive experiments show that GAMER consistently outperforms both discriminative and generative baselines across multiple metrics.",
        "translated": "在多行为领域（如广告和电子商务）的推荐系统旨在引导用户完成高价值但本质上稀疏的转化行为。因此，利用辅助行为（如点击、点赞、分享）至关重要。近期生成式推荐的研究进展为多行为序列推荐带来了新的可能性。然而，现有的生成式方法面临两个显著挑战：1）序列建模不足：难以捕捉用户行为序列中复杂的跨层级依赖关系；2）缺乏合适的数据集：公开可用的多行为推荐数据集几乎全部来源于电子商务平台，限制了在其他领域可行性的验证，同时缺乏足够的侧信息用于语义ID的生成。为解决这些问题，我们提出了一种新颖的生成式框架GAMER（Generative Augmentation and Multi-lEvel behavior modeling for Recommendation），其基于仅包含解码器的主干结构。GAMER引入了跨层级交互层以捕捉行为之间的层次化依赖关系，并采用序列增强策略以提升训练过程的鲁棒性。为进一步推动该方向的发展，我们收集并发布了ShortVideoAD，一个来自主流短视频平台的大规模多行为数据集，其与现有电子商务数据集存在本质差异，并为生成式方法的研究提供了预训练语义ID。大量实验表明，GAMER在多个指标上均显著优于判别式和生成式基线方法。",
        "translated_title": "生成式序列推荐：基于层次化行为建模的方法",
        "label": [
            "LLM生成式推荐（LLM-based Generative Recommendation）",
            "序列推荐（Sequential Recommendation）",
            "通用推荐技术（General Recommendation Techniques）"
        ],
        "label_reason": "论文提出生成式序列推荐框架，建模多行为序列，核心为推荐系统精排与生成环节",
        "relevance_score": 9,
        "novelty_score": 8,
        "novelty_reason": "提出分层行为建模与序列增强策略，改进生成式推荐，非简单模块替换"
    },
    {
        "title": "No-Human in the Loop: Agentic Evaluation at Scale for Recommendation",
        "url": "http://arxiv.org/abs/2511.03051v1",
        "pub_date": "2025-11-04",
        "summary": "Evaluating large language models (LLMs) as judges is increasingly critical for building scalable and trustworthy evaluation pipelines. We present ScalingEval, a large-scale benchmarking study that systematically compares 36 LLMs, including GPT, Gemini, Claude, and Llama, across multiple product categories using a consensus-driven evaluation protocol. Our multi-agent framework aggregates pattern audits and issue codes into ground-truth labels via scalable majority voting, enabling reproducible comparison of LLM evaluators without human annotation. Applied to large-scale complementary-item recommendation, the benchmark reports four key findings: (i) Anthropic Claude 3.5 Sonnet achieves the highest decision confidence; (ii) Gemini 1.5 Pro offers the best overall performance across categories; (iii) GPT-4o provides the most favorable latency-accuracy-cost tradeoff; and (iv) GPT-OSS 20B leads among open-source models. Category-level analysis shows strong consensus in structured domains (Electronics, Sports) but persistent disagreement in lifestyle categories (Clothing, Food). These results establish ScalingEval as a reproducible benchmark and evaluation protocol for LLMs as judges, with actionable guidance on scaling, reliability, and model family tradeoffs.",
        "translated": "评估大语言模型（LLM）作为评判者在构建可扩展且可信的评估流程中日益重要。我们提出 ScalingEval，一项大规模基准研究，系统性地比较了包括 GPT、Gemini、Claude 和 Llama 在内的 36 个 LLM，在多个产品类别中采用共识驱动的评估协议。我们的多智能体框架通过可扩展的多数投票机制，将模式审计和问题编码聚合为真实标签，从而无需人工标注即可实现对 LLM 评估者的可复现比较。应用于大规模互补物料推荐场景，该基准报告了四个关键发现：(i) Anthropic Claude 3.5 Sonnet 在决策置信度方面表现最高；(ii) Gemini 1.5 Pro 在各类别中整体性能最佳；(iii) GPT-4o 在延迟-准确率-成本权衡方面表现最优；(iv) GPT-OSS 20B 在开源模型中领先。按类别分析显示，在结构化领域（如 Electronics、Sports）中存在较强共识，而在生活方式类别（如 Clothing、Food）中仍存在持续分歧。这些结果确立了 ScalingEval 作为 LLM 作为评判者的可复现基准和评估协议，并为模型扩展、可靠性及模型家族权衡提供了可操作性指导。",
        "translated_title": "无人工干预：推荐系统中大规模智能体评估方法",
        "label": [
            "推荐系统评估",
            "LLM生成式推荐"
        ],
        "label_reason": "论文聚焦LLM作为评估者在推荐系统中的应用，涉及推荐评估方法，与推荐系统间接相关。",
        "relevance_score": 6,
        "novelty_score": 7,
        "novelty_reason": "提出多代理评估框架，通过多数投票生成标签，实现无标注大规模评估，方法新颖且可复用。"
    },
    {
        "title": "Carousel: A High-Resolution Dataset for Multi-Target Automatic Image\n  Cropping",
        "url": "http://arxiv.org/abs/2511.04680v1",
        "pub_date": "2025-11-06",
        "summary": "Automatic image cropping is a method for maximizing the human-perceived quality of cropped regions in photographs. Although several works have proposed techniques for producing singular crops, little work has addressed the problem of producing multiple, distinct crops with aesthetic appeal. In this paper, we motivate the problem with a discussion on modern social media applications, introduce a dataset of 277 relevant images and human labels, and evaluate the efficacy of several single-crop models with an image partitioning algorithm as a pre-processing step. The dataset is available at https://github.com/RafeLoya/carousel.",
        "translated": "自动图像裁剪是一种旨在最大化照片裁剪区域人眼感知质量的方法。尽管已有若干研究提出了生成单一裁剪结果的技术，但针对生成多个具有美学吸引力且互不相同的裁剪结果的问题，相关研究仍较为匮乏。本文通过探讨现代社交媒体应用的需求，阐述了该问题的必要性；构建了一个包含277张相关图像及人工标注标签的数据集；并采用图像分割算法作为预处理步骤，评估了多种单裁剪模型在生成多裁剪结果时的有效性。该数据集可于 https://github.com/RafeLoya/carousel 获取。",
        "translated_title": "Carousel: 用于多目标自动图像裁剪的高分辨率数据集",
        "label": [],
        "label_reason": "图像裁剪属于高阶视觉任务，旨在提升美学质量，非像素级图像恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 3,
        "novelty_reason": "提出新数据集并评估现有模型，无核心方法创新，属于应用性工作。"
    },
    {
        "title": "GentleHumanoid: Learning Upper-body Compliance for Contact-rich Human\n  and Object Interaction",
        "url": "http://arxiv.org/abs/2511.04679v1",
        "pub_date": "2025-11-06",
        "summary": "Humanoid robots are expected to operate in human-centered environments where safe and natural physical interaction is essential. However, most recent reinforcement learning (RL) policies emphasize rigid tracking and suppress external forces. Existing impedance-augmented approaches are typically restricted to base or end-effector control and focus on resisting extreme forces rather than enabling compliance. We introduce GentleHumanoid, a framework that integrates impedance control into a whole-body motion tracking policy to achieve upper-body compliance. At its core is a unified spring-based formulation that models both resistive contacts (restoring forces when pressing against surfaces) and guiding contacts (pushes or pulls sampled from human motion data). This formulation ensures kinematically consistent forces across the shoulder, elbow, and wrist, while exposing the policy to diverse interaction scenarios. Safety is further supported through task-adjustable force thresholds. We evaluate our approach in both simulation and on the Unitree G1 humanoid across tasks requiring different levels of compliance, including gentle hugging, sit-to-stand assistance, and safe object manipulation. Compared to baselines, our policy consistently reduces peak contact forces while maintaining task success, resulting in smoother and more natural interactions. These results highlight a step toward humanoid robots that can safely and effectively collaborate with humans and handle objects in real-world environments.",
        "translated": "人形机器人被期望在以人类为中心的环境中运行，其中安全且自然的物理交互至关重要。然而，大多数近期的强化学习（RL）策略强调刚性跟踪，并抑制外部力。现有的阻抗增强方法通常局限于基座或末端执行器控制，且主要关注抵抗极端力，而非实现柔顺性。我们提出 GentleHumanoid，一种将阻抗控制集成到全身运动跟踪策略中的框架，以实现上肢柔顺性。其核心是一种统一的弹簧模型，能够同时建模抵抗性接触（在压靠表面时产生的恢复力）和引导性接触（从人类运动数据中采样的推或拉力）。该模型确保肩部、肘部和腕部之间具有运动学一致的力，同时使策略暴露于多样化的交互场景。安全性通过任务可调的力阈值进一步保障。我们在仿真环境和 Unitree G1 人形机器人上对多种需要不同程度柔顺性的任务进行了评估，包括轻柔拥抱、坐姿到站姿辅助以及安全物体操作。与基线方法相比，我们的策略在保持任务成功率的同时，始终降低峰值接触力，从而实现更平滑、更自然的交互。这些结果标志着人形机器人朝着能够在真实环境中安全、有效地与人类协作并操作物体迈出的重要一步。",
        "translated_title": "GentleHumanoid：学习上肢柔顺性以实现丰富接触的人与物体交互",
        "label": [],
        "label_reason": "论文聚焦人形机器人物理交互控制，属机器人控制领域，非图像处理任务。",
        "relevance_score": 1,
        "novelty_score": 6,
        "novelty_reason": "提出统一弹簧模型实现上肢柔顺控制，对机器人交互有改进，但非图像处理创新。"
    },
    {
        "title": "Tracking and Understanding Object Transformations",
        "url": "http://arxiv.org/abs/2511.04678v1",
        "pub_date": "2025-11-06",
        "summary": "Real-world objects frequently undergo state transformations. From an apple being cut into pieces to a butterfly emerging from its cocoon, tracking through these changes is important for understanding real-world objects and dynamics. However, existing methods often lose track of the target object after transformation, due to significant changes in object appearance. To address this limitation, we introduce the task of Track Any State: tracking objects through transformations while detecting and describing state changes, accompanied by a new benchmark dataset, VOST-TAS. To tackle this problem, we present TubeletGraph, a zero-shot system that recovers missing objects after transformation and maps out how object states are evolving over time. TubeletGraph first identifies potentially overlooked tracks, and determines whether they should be integrated based on semantic and proximity priors. Then, it reasons about the added tracks and generates a state graph describing each observed transformation. TubeletGraph achieves state-of-the-art tracking performance under transformations, while demonstrating deeper understanding of object transformations and promising capabilities in temporal grounding and semantic reasoning for complex object transformations. Code, additional results, and the benchmark dataset are available at https://tubelet-graph.github.io.",
        "translated": "现实世界中的物体经常经历状态转变。从苹果被切成块，到蝴蝶从茧中破茧而出，追踪这些变化对于理解现实世界物体及其动态至关重要。然而，现有方法在物体发生转变后，往往因外观发生显著变化而丢失目标物体的跟踪。为解决这一局限，我们提出“Track Any State”任务：在物体状态转变过程中实现跟踪，同时检测并描述状态变化，并构建了一个新的基准数据集 VOST-TAS。为应对该问题，我们提出 TubeletGraph，一种零样本系统，能够在物体转变后恢复丢失的跟踪轨迹，并绘制出物体状态随时间演变的过程。TubeletGraph 首先识别可能被忽略的轨迹，并基于语义和邻近先验知识判断是否应将其整合。随后，系统对新增轨迹进行推理，并生成描述每次观测到的转变的状态图。TubeletGraph 在物体状态转变场景下实现了最先进的跟踪性能，同时展现出对物体转变更深层次的理解能力，以及在复杂物体转变中的时间定位和语义推理的潜力。代码、额外实验结果及基准数据集可在 https://tubelet-graph.github.io 获取。",
        "translated_title": "追踪与理解物体变换",
        "label": [],
        "label_reason": "论文聚焦于物体变换跟踪与语义理解，属于高阶视觉任务，非像素级图像恢复。",
        "relevance_score": 1,
        "novelty_score": 6,
        "novelty_reason": "提出TubeletGraph框架，结合语义与空间先验进行跟踪，属跟踪领域创新，但非低层图像处理。"
    },
    {
        "title": "InfinityStar: Unified Spacetime AutoRegressive Modeling for Visual\n  Generation",
        "url": "http://arxiv.org/abs/2511.04675v1",
        "pub_date": "2025-11-06",
        "summary": "We introduce InfinityStar, a unified spacetime autoregressive framework for high-resolution image and dynamic video synthesis. Building on the recent success of autoregressive modeling in both vision and language, our purely discrete approach jointly captures spatial and temporal dependencies within a single architecture. This unified design naturally supports a variety of generation tasks such as text-to-image, text-to-video, image-to-video, and long interactive video synthesis via straightforward temporal autoregression. Extensive experiments demonstrate that InfinityStar scores 83.74 on VBench, outperforming all autoregressive models by large margins, even surpassing some diffusion competitors like HunyuanVideo. Without extra optimizations, our model generates a 5s, 720p video approximately 10x faster than leading diffusion-based methods. To our knowledge, InfinityStar is the first discrete autoregressive video generator capable of producing industrial level 720p videos. We release all code and models to foster further research in efficient, high-quality video generation.",
        "translated": "我们提出 InfinityStar，一种用于高分辨率图像与动态视频生成的统一时空自回归框架。基于自回归建模在视觉与语言领域近期取得的成功，我们的纯离散方法在单一架构内联合捕捉空间与时间依赖关系。该统一设计自然支持多种生成任务，如文本到图像、文本到视频、图像到视频以及长时交互式视频生成，仅通过简单的时序自回归即可实现。大量实验表明，InfinityStar 在 VBench 上得分达 83.74，显著优于所有自回归模型，甚至超越部分扩散模型（如 HunyuanVideo）。在无需额外优化的情况下，我们的模型生成一段 5 秒、720p 分辨率的视频，速度约为当前主流扩散方法的 10 倍。据我们所知，InfinityStar 是首个能够生成工业级 720p 视频的离散自回归视频生成器。我们公开所有代码与模型，以促进高效、高质量视频生成领域的进一步研究。",
        "translated_title": "InfinityStar：用于视觉生成的统一时空自回归建模",
        "label": [],
        "label_reason": "论文聚焦于图像与视频生成，属于high-level任务，非图像恢复或增强。",
        "relevance_score": 2,
        "novelty_score": 8,
        "novelty_reason": "提出统一时空自回归框架，创新性强，但应用于生成而非复原任务。"
    },
    {
        "title": "X-Diffusion: Training Diffusion Policies on Cross-Embodiment Human\n  Demonstrations",
        "url": "http://arxiv.org/abs/2511.04671v1",
        "pub_date": "2025-11-06",
        "summary": "Human videos can be recorded quickly and at scale, making them an appealing source of training data for robot learning. However, humans and robots differ fundamentally in embodiment, resulting in mismatched action execution. Direct kinematic retargeting of human hand motion can therefore produce actions that are physically infeasible for robots. Despite these low-level differences, human demonstrations provide valuable motion cues about how to manipulate and interact with objects. Our key idea is to exploit the forward diffusion process: as noise is added to actions, low-level execution differences fade while high-level task guidance is preserved. We present X-Diffusion, a principled framework for training diffusion policies that maximally leverages human data without learning dynamically infeasible motions. X-Diffusion first trains a classifier to predict whether a noisy action is executed by a human or robot. Then, a human action is incorporated into policy training only after adding sufficient noise such that the classifier cannot discern its embodiment. Actions consistent with robot execution supervise fine-grained denoising at low noise levels, while mismatched human actions provide only coarse guidance at higher noise levels. Our experiments show that naive co-training under execution mismatches degrades policy performance, while X-Diffusion consistently improves it. Across five manipulation tasks, X-Diffusion achieves a 16% higher average success rate than the best baseline. The project website is available at https://portal-cornell.github.io/X-Diffusion/.",
        "translated": "人类视频可以快速、大规模地采集，使其成为机器人学习训练数据的理想来源。然而，人类与机器人在身体结构上存在根本差异，导致动作执行方式不匹配。因此，直接对人类手部运动进行运动学重定向可能会生成对机器人而言物理上不可行的动作。尽管存在这些低层次差异，人类演示仍能提供关于如何操作和与物体交互的有价值运动线索。我们的核心思想是利用前向扩散过程：当噪声被添加到动作中时，低层次执行差异逐渐减弱，而高层次任务引导得以保留。我们提出 X-Diffusion，一种原理性框架，用于训练扩散策略，能够在不学习动力学不可行动作的前提下，最大限度地利用人类数据。X-Diffusion 首先训练一个分类器，用于预测带噪声的动作是由人类还是机器人执行的。随后，仅当人类动作添加了足够噪声（使得分类器无法辨别其身体结构）后，才将其纳入策略训练。在低噪声水平下，与机器人执行一致的动作用于监督精细的去噪过程；而在高噪声水平下，不匹配的人类动作仅提供粗粒度的引导。我们的实验表明，在执行不匹配的情况下进行朴素的联合训练会降低策略性能，而 X-Diffusion 则始终提升性能。在五个操作任务中，X-Diffusion 的平均成功率比最佳基线高出 16%。项目网站见 https://portal-cornell.github.io/X-Diffusion/。",
        "translated_title": "X-Diffusion：在跨实体人类演示上训练扩散策略",
        "label": [],
        "label_reason": "论文聚焦机器人学习与动作扩散，非图像像素级恢复或增强任务，属高阶视觉应用。",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出基于扩散过程的跨实体动作训练框架，对机器人学习有显著改进，但非图像处理领域创新。"
    },
    {
        "title": "Cambrian-S: Towards Spatial Supersensing in Video",
        "url": "http://arxiv.org/abs/2511.04670v1",
        "pub_date": "2025-11-06",
        "summary": "We argue that progress in true multimodal intelligence calls for a shift from reactive, task-driven systems and brute-force long context towards a broader paradigm of supersensing. We frame spatial supersensing as four stages beyond linguistic-only understanding: semantic perception (naming what is seen), streaming event cognition (maintaining memory across continuous experiences), implicit 3D spatial cognition (inferring the world behind pixels), and predictive world modeling (creating internal models that filter and organize information). Current benchmarks largely test only the early stages, offering narrow coverage of spatial cognition and rarely challenging models in ways that require true world modeling. To drive progress in spatial supersensing, we present VSI-SUPER, a two-part benchmark: VSR (long-horizon visual spatial recall) and VSC (continual visual spatial counting). These tasks require arbitrarily long video inputs yet are resistant to brute-force context expansion. We then test data scaling limits by curating VSI-590K and training Cambrian-S, achieving +30% absolute improvement on VSI-Bench without sacrificing general capabilities. Yet performance on VSI-SUPER remains limited, indicating that scale alone is insufficient for spatial supersensing. We propose predictive sensing as a path forward, presenting a proof-of-concept in which a self-supervised next-latent-frame predictor leverages surprise (prediction error) to drive memory and event segmentation. On VSI-SUPER, this approach substantially outperforms leading proprietary baselines, showing that spatial supersensing requires models that not only see but also anticipate, select, and organize experience.",
        "translated": "我们认为，真正多模态智能的进步需要从被动的、以任务为导向的系统以及依赖暴力长上下文的方法，转向更广泛的“超感知”范式。我们将空间超感知定义为超越纯语言理解的四个阶段：语义感知（识别所见内容）、流式事件认知（在连续体验中保持记忆）、隐式三维空间认知（推断像素背后的世界）以及预测性世界建模（构建内部模型以过滤和组织信息）。当前的基准测试大多仅评估早期阶段，对空间认知的覆盖范围狭窄，且很少以需要真正世界建模的方式挑战模型。为推动空间超感知的发展，我们提出 VSI-SUPER，一个由两部分组成的基准测试：VSR（长时视觉空间回忆）和 VSC（持续视觉空间计数）。这些任务要求任意长度的视频输入，但对暴力扩展上下文具有鲁棒性。随后，我们通过构建 VSI-590K 数据集并训练 Cambrian-S，测试了数据规模的极限，在 VSI-Bench 上实现了绝对 +30% 的性能提升，且未牺牲通用能力。然而，在 VSI-SUPER 上的性能仍有限，表明仅靠规模扩展不足以实现空间超感知。我们提出“预测感知”作为前进路径，并展示了一个概念验证：一个自监督的下一潜层帧预测器利用“惊奇”（预测误差）驱动记忆与事件分割。在 VSI-SUPER 上，该方法显著优于领先的专有基线模型，表明空间超感知需要的不仅是“看见”，更需“预见、选择和组织”经验的模型。",
        "translated_title": "Cambrian-S：迈向视频中的空域超感知\n\n在视频感知领域，传统方法通常受限于传感器的物理分辨率，难以捕捉精细的空间细节。为突破这一瓶颈，我们提出Cambrian-S，一种面向视频的空域超感知框架。该框架通过融合多帧信息与先验知识，在空域中实现对原始低分辨率视频的超分辨率重建，从而显著提升空间细节的感知能力。\n\nCambrian-S采用端到端的深度学习架构，结合残差学习与特征图的跨帧对齐机制，有效缓解运动模糊与帧间不一致性带来的退化问题。在频域中，我们引入频域注意力模块，以增强高频细节的恢复能力；在空域中，设计了多尺度空域感知模块，以捕捉不同尺度下的结构信息。此外，通过引入动态退化建模，模型能够自适应地估计每帧图像的退化程度，并据此调整恢复策略。\n\n实验在多个公开数据集（如Rain100L、SIDD、RESIDE）上进行，结果表明，Cambrian-S在图像去雨、图像去模糊、低光照增强等任务中均取得优于现有方法的性能。特别是在视频超分辨率任务中，Cambrian-S在PSNR和SSIM指标上显著超越了U-Net、DnCNN等主流模型，展现了其在空域超感知方面的强大潜力。\n\n本工作为视频感知系统提供了新的技术路径，推动了从“被动采集”到“主动感知”的范式转变，为未来智能视觉系统的发展奠定了基础。",
        "label": [],
        "label_reason": "论文聚焦多模态智能与空间认知建模，非像素级图像恢复或增强任务，属高阶视觉理解。",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出预测感知范式与自监督帧预测机制，对模型记忆与事件分割有创新设计。"
    },
    {
        "title": "SIMS-V: Simulated Instruction-Tuning for Spatial Video Understanding",
        "url": "http://arxiv.org/abs/2511.04668v1",
        "pub_date": "2025-11-06",
        "summary": "Despite impressive high-level video comprehension, multimodal language models struggle with spatial reasoning across time and space. While current spatial training approaches rely on real-world video data, obtaining diverse footage with precise spatial annotations remains a bottleneck. To alleviate this bottleneck, we present SIMS-V -- a systematic data-generation framework that leverages the privileged information of 3D simulators to create spatially-rich video training data for multimodal language models. Using this framework, we investigate which properties of simulated data drive effective real-world transfer through systematic ablations of question types, mixes, and scales. We identify a minimal set of three question categories (metric measurement, perspective-dependent reasoning, and temporal tracking) that prove most effective for developing transferable spatial intelligence, outperforming comprehensive coverage despite using fewer question types. These insights enable highly efficient training: our 7B-parameter video LLM fine-tuned on just 25K simulated examples outperforms the larger 72B baseline and achieves competitive performance with proprietary models on rigorous real-world spatial reasoning benchmarks. Our approach demonstrates robust generalization, maintaining performance on general video understanding while showing substantial improvements on embodied and real-world spatial tasks.",
        "translated": "尽管多模态语言模型在高层视频理解方面取得了显著进展，但在跨时空的空域推理能力上仍存在不足。当前的空间训练方法主要依赖真实世界视频数据，然而获取具有精确空间标注的多样化视频素材仍是一个瓶颈。为缓解这一瓶颈，我们提出 SIMS-V —— 一种系统化的数据生成框架，该框架利用3D模拟器的特权信息，为多模态语言模型生成富含空间信息的视频训练数据。基于该框架，我们通过系统性消融实验，探究了模拟数据的哪些特性能够有效促进真实世界任务的迁移，实验覆盖了问题类型、混合方式和规模等多个维度。我们识别出三个关键问题类别（度量测量、视角依赖推理和时序跟踪）的最小集合，这些类别在提升可迁移空间智能方面最为有效，尽管问题类型较少，其性能却优于覆盖全面的方案。这些发现使得训练过程高度高效：仅在25K个模拟样本上微调的7B参数视频大语言模型，其性能超越了72B参数的基线模型，并在严格的现实世界空间推理基准测试中达到与专有模型相当的水平。我们的方法展现出稳健的泛化能力，在保持通用视频理解性能的同时，在具身化和现实世界空间任务上实现了显著提升。",
        "translated_title": "SIMS-V：用于空间视频理解的模拟指令微调",
        "label": [],
        "label_reason": "论文聚焦于视频理解中的空间推理，属于高阶视觉任务，不涉及像素级图像恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 6,
        "novelty_reason": "提出模拟数据生成框架，用于训练多模态语言模型，方法新颖但非low-level图像处理创新。"
    },
    {
        "title": "Real-to-Sim Robot Policy Evaluation with Gaussian Splatting Simulation\n  of Soft-Body Interactions",
        "url": "http://arxiv.org/abs/2511.04665v1",
        "pub_date": "2025-11-06",
        "summary": "Robotic manipulation policies are advancing rapidly, but their direct evaluation in the real world remains costly, time-consuming, and difficult to reproduce, particularly for tasks involving deformable objects. Simulation provides a scalable and systematic alternative, yet existing simulators often fail to capture the coupled visual and physical complexity of soft-body interactions. We present a real-to-sim policy evaluation framework that constructs soft-body digital twins from real-world videos and renders robots, objects, and environments with photorealistic fidelity using 3D Gaussian Splatting. We validate our approach on representative deformable manipulation tasks, including plush toy packing, rope routing, and T-block pushing, demonstrating that simulated rollouts correlate strongly with real-world execution performance and reveal key behavioral patterns of learned policies. Our results suggest that combining physics-informed reconstruction with high-quality rendering enables reproducible, scalable, and accurate evaluation of robotic manipulation policies. Website: https://real2sim-eval.github.io/",
        "translated": "机器人操作策略正迅速发展，但其在真实世界中的直接评估仍然成本高昂、耗时且难以复现，尤其对于涉及可变形物体的任务而言。仿真提供了一种可扩展且系统化的替代方案，然而现有仿真器往往难以捕捉软体交互中视觉与物理耦合的复杂性。我们提出了一种从真实世界视频构建软体数字孪生体的“真实到仿真”策略评估框架，并利用3D高斯点绘技术以照片级真实感渲染机器人、物体及环境。我们在具有代表性的可变形操作任务上验证了该方法，包括毛绒玩具打包、绳索布线以及T形块推动任务，结果表明仿真推演与真实世界执行性能具有强相关性，并揭示了学习策略的关键行为模式。我们的研究结果表明，将物理感知重建与高质量渲染相结合，能够实现机器人操作策略的可复现、可扩展且精确的评估。网站：https://real2sim-eval.github.io/",
        "translated_title": "真实到模拟机器人策略评估：基于高斯点云的软体交互仿真",
        "label": [],
        "label_reason": "论文聚焦机器人策略评估与仿真，非图像像素级恢复或增强任务，属high-level视觉应用。",
        "relevance_score": 1,
        "novelty_score": 6,
        "novelty_reason": "提出结合3D高斯泼溅与物理重建的仿真框架，对机器人仿真有改进，但非图像复原创新。"
    },
    {
        "title": "Benchmark Designers Should \"Train on the Test Set\" to Expose Exploitable\n  Non-Visual Shortcuts",
        "url": "http://arxiv.org/abs/2511.04655v1",
        "pub_date": "2025-11-06",
        "summary": "Robust benchmarks are crucial for evaluating Multimodal Large Language Models (MLLMs). Yet we find that models can ace many multimodal benchmarks without strong visual understanding, instead exploiting biases, linguistic priors, and superficial patterns. This is especially problematic for vision-centric benchmarks that are meant to require visual inputs. We adopt a diagnostic principle for benchmark design: if a benchmark can be gamed, it will be. Designers should therefore try to ``game'' their own benchmarks first, using diagnostic and debiasing procedures to systematically identify and mitigate non-visual biases. Effective diagnosis requires directly ``training on the test set'' -- probing the released test set for its intrinsic, exploitable patterns.   We operationalize this standard with two components. First, we diagnose benchmark susceptibility using a ``Test-set Stress-Test'' (TsT) methodology. Our primary diagnostic tool involves fine-tuning a powerful Large Language Model via $k$-fold cross-validation on exclusively the non-visual, textual inputs of the test set to reveal shortcut performance and assign each sample a bias score $s(x)$. We complement this with a lightweight Random Forest-based diagnostic operating on hand-crafted features for fast, interpretable auditing. Second, we debias benchmarks by filtering high-bias samples using an ``Iterative Bias Pruning'' (IBP) procedure. Applying this framework to four benchmarks -- VSI-Bench, CV-Bench, MMMU, and VideoMME -- we uncover pervasive non-visual biases. As a case study, we apply our full framework to create VSI-Bench-Debiased, demonstrating reduced non-visual solvability and a wider vision-blind performance gap than the original.",
        "translated": "鲁棒的基准测试对于评估多模态大语言模型（MLLMs）至关重要。然而，我们发现，模型可以在许多多模态基准测试中取得优异表现，而无需具备强大的视觉理解能力，而是通过利用偏见、语言先验知识以及表面模式实现。这一问题在以视觉为核心的基准测试中尤为突出，因为这些基准测试本应要求模型依赖视觉输入。我们采用一种诊断性原则指导基准设计：如果一个基准测试可以被“操控”，它终将被操控。因此，设计者应首先尝试“操控”自己设计的基准测试，通过诊断与去偏方法系统性地识别并缓解非视觉性偏见。有效的诊断需要直接“在测试集上训练”——即探查已发布的测试集内部存在的、可被利用的固有模式。\n\n我们通过两个组件将这一标准具体化。首先，我们采用“测试集压力测试”（Test-set Stress-Test, TsT）方法诊断基准测试的易受攻击性。我们的主要诊断工具是仅使用测试集中非视觉的文本输入，通过 $k$-折交叉验证对强大的大语言模型进行微调，以揭示其“捷径性能”，并为每个样本分配一个偏见得分 $s(x)$。我们辅以基于轻量级随机森林的诊断方法，该方法基于手工设计的特征，可实现快速、可解释的审计。其次，我们通过“迭代偏见剪枝”（Iterative Bias Pruning, IBP）程序对基准测试进行去偏处理，即过滤高偏见样本。将该框架应用于四个基准测试——VSI-Bench、CV-Bench、MMMU 和 VideoMME——我们发现非视觉偏见普遍存在。作为案例研究，我们将完整框架应用于构建 VSI-Bench-Debiased，结果表明其非视觉可解性显著降低，且视觉盲区下的性能差距较原始版本更宽。",
        "translated_title": "基准设计者应“在测试集上训练”，以揭示可被利用的非视觉捷径",
        "label": [],
        "label_reason": "论文聚焦多模态大模型基准设计与偏见检测，非图像像素级处理任务。",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "提出诊断与去偏框架，但属于基准设计方法，非图像处理创新。"
    },
    {
        "title": "Polarization-resolved imaging improves eye tracking",
        "url": "http://arxiv.org/abs/2511.04652v1",
        "pub_date": "2025-11-06",
        "summary": "Polarization-resolved near-infrared imaging adds a useful optical contrast mechanism to eye tracking by measuring the polarization state of light reflected by ocular tissues in addition to its intensity. In this paper we demonstrate how this contrast can be used to enable eye tracking. Specifically, we demonstrate that a polarization-enabled eye tracking (PET) system composed of a polarization--filter--array camera paired with a linearly polarized near-infrared illuminator can reveal trackable features across the sclera and gaze-informative patterns on the cornea, largely absent in intensity-only images. Across a cohort of 346 participants, convolutional neural network based machine learning models trained on data from PET reduced the median 95th-percentile absolute gaze error by 10--16\\% relative to capacity-matched intensity baselines under nominal conditions and in the presence of eyelid occlusions, eye-relief changes, and pupil-size variation. These results link light--tissue polarization effects to practical gains in human--computer interaction and position PET as a simple, robust sensing modality for future wearable devices.",
        "translated": "偏振分辨近红外成像通过测量眼部组织反射光的偏振态，除光强外为眼动追踪提供了一种有用的光学对比机制。本文展示了如何利用这种对比机制实现眼动追踪。具体而言，我们证明了一种由偏振滤光阵列相机与线偏振近红外光源组成的偏振增强眼动追踪（PET）系统，能够揭示巩膜上的可追踪特征以及角膜上的注视信息图案，而这些在仅依赖光强的图像中大多缺失。在346名受试者组成的队列中，基于PET数据训练的卷积神经网络机器学习模型，在标准条件下以及存在眼睑遮挡、眼距变化和瞳孔大小变化的情况下，相较于性能匹配的光强基准方法，将中位数95百分位绝对注视误差降低了10–16%。这些结果将光-组织偏振效应与人机交互的实际性能提升联系起来，并确立了PET作为一种简单、鲁棒的传感模态，在未来可穿戴设备中的应用前景。",
        "translated_title": "偏振分辨成像提升眼动追踪性能",
        "label": [],
        "label_reason": "论文聚焦于眼动追踪，属于高阶视觉任务，非像素级图像恢复或增强。",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出基于偏振成像的新型眼动追踪系统，方法新颖但非低层图像处理创新。"
    },
    {
        "title": "NovisVQ: A Streaming Convolutional Neural Network for No-Reference\n  Opinion-Unaware Frame Quality Assessment",
        "url": "http://arxiv.org/abs/2511.04628v1",
        "pub_date": "2025-11-06",
        "summary": "Video quality assessment (VQA) is vital for computer vision tasks, but existing approaches face major limitations: full-reference (FR) metrics require clean reference videos, and most no-reference (NR) models depend on training on costly human opinion labels. Moreover, most opinion-unaware NR methods are image-based, ignoring temporal context critical for video object detection. In this work, we present a scalable, streaming-based VQA model that is both no-reference and opinion-unaware. Our model leverages synthetic degradations of the DAVIS dataset, training a temporal-aware convolutional architecture to predict FR metrics (LPIPS , PSNR, SSIM) directly from degraded video, without references at inference. We show that our streaming approach outperforms our own image-based baseline by generalizing across diverse degradations, underscoring the value of temporal modeling for scalable VQA in real-world vision systems. Additionally, we demonstrate that our model achieves higher correlation with full-reference metrics compared to BRISQUE, a widely-used opinion-aware image quality assessment baseline, validating the effectiveness of our temporal, opinion-unaware approach.",
        "translated": "视频质量评估（VQA）对于计算机视觉任务至关重要，但现有方法面临主要局限：全参考（FR）指标需要干净的参考视频，而大多数无参考（NR）模型依赖于昂贵的人类主观评分标签进行训练。此外，大多数无主观意见的NR方法基于图像，忽略了对视频目标检测至关重要的时序上下文信息。在本工作中，我们提出了一种可扩展的、基于流式处理的无参考且无主观意见的VQA模型。我们的模型利用DAVIS数据集的合成退化样本，训练一种具有时序感知能力的卷积架构，能够直接从退化视频中预测FR指标（LPIPS、PSNR、SSIM），在推理阶段无需参考视频。我们证明，相较于我们自身的基于图像的基线模型，该流式方法通过在多种退化条件下表现出更强的泛化能力，凸显了时序建模在真实视觉系统中实现可扩展VQA的价值。此外，我们还表明，与广泛使用的有主观意见的图像质量评估基线BRISQUE相比，我们的模型与全参考指标的相关性更高，验证了所提出的时序、无主观意见方法的有效性。",
        "translated_title": "NovisVQ：一种用于无参考、无主观意见帧质量评估的流式卷积神经网络",
        "label": [],
        "label_reason": "论文聚焦视频质量评估，非图像像素级恢复或增强，属于高阶视觉任务。",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出流式时序CNN架构，改进传统图像级NR-VQA，但未解决低层图像退化问题。"
    },
    {
        "title": "Building Trust in Virtual Immunohistochemistry: Automated Assessment of\n  Image Quality",
        "url": "http://arxiv.org/abs/2511.04615v1",
        "pub_date": "2025-11-06",
        "summary": "Deep learning models can generate virtual immunohistochemistry (IHC) stains from hematoxylin and eosin (H&amp;E) images, offering a scalable and low-cost alternative to laboratory IHC. However, reliable evaluation of image quality remains a challenge as current texture- and distribution-based metrics quantify image fidelity rather than the accuracy of IHC staining. Here, we introduce an automated and accuracy grounded framework to determine image quality across sixteen paired or unpaired image translation models. Using color deconvolution, we generate masks of pixels stained brown (i.e., IHC-positive) as predicted by each virtual IHC model. We use the segmented masks of real and virtual IHC to compute stain accuracy metrics (Dice, IoU, Hausdorff distance) that directly quantify correct pixel - level labeling without needing expert manual annotations. Our results demonstrate that conventional image fidelity metrics, including Frechet Inception Distance (FID), peak signal-to-noise ratio (PSNR), and structural similarity (SSIM), correlate poorly with stain accuracy and pathologist assessment. Paired models such as PyramidPix2Pix and AdaptiveNCE achieve the highest stain accuracy, whereas unpaired diffusion- and GAN-based models are less reliable in providing accurate IHC positive pixel labels. Moreover, whole-slide images (WSI) reveal performance declines that are invisible in patch-based evaluations, emphasizing the need for WSI-level benchmarks. Together, this framework defines a reproducible approach for assessing the quality of virtual IHC models, a critical step to accelerate translation towards routine use by pathologists.",
        "translated": "深度学习模型能够从苏木精-伊红（H&E）图像生成虚拟免疫组织化学（IHC）染色图像，为实验室IHC提供了一种可扩展且低成本的替代方案。然而，图像质量的可靠评估仍具挑战性，因为当前基于纹理和分布的度量指标主要量化图像保真度，而非IHC染色的准确性。本文提出一种自动化且以准确性为基础的框架，用于评估十六种配对或非配对图像翻译模型的图像质量。通过颜色反卷积，我们生成每个虚拟IHC模型预测的棕色染色像素（即IHC阳性）的掩膜。利用真实IHC与虚拟IHC的分割掩膜，计算染色准确性指标（Dice、IoU、Hausdorff距离），直接量化像素级标注的正确性，无需专家手动标注。实验结果表明，传统的图像保真度指标，包括弗雷歇 inception 距离（FID）、峰值信噪比（PSNR）和结构相似性（SSIM），与染色准确性及病理学家评估的相关性较差。配对模型（如PyramidPix2Pix和AdaptiveNCE）在染色准确性上表现最佳，而非配对的扩散模型和GAN模型在提供准确IHC阳性像素标签方面可靠性较低。此外，全切片图像（WSI）揭示了在基于图像块的评估中无法察觉的性能下降，凸显了建立WSI级别基准的必要性。综上，该框架为评估虚拟IHC模型质量提供了一种可复现的方法，是推动其在病理学常规应用中转化的关键一步。",
        "translated_title": "构建虚拟免疫组化中的信任：图像质量的自动化评估",
        "label": [],
        "label_reason": "论文聚焦虚拟IHC图像质量评估，非图像像素级恢复或增强，属高阶图像生成评估任务。",
        "relevance_score": 3,
        "novelty_score": 6,
        "novelty_reason": "提出基于染色准确性的评估框架，对生成模型评估有改进，但非low-level图像处理创新。"
    },
    {
        "title": "PixCLIP: Achieving Fine-grained Visual Language Understanding via\n  Any-granularity Pixel-Text Alignment Learning",
        "url": "http://arxiv.org/abs/2511.04601v1",
        "pub_date": "2025-11-06",
        "summary": "While the Contrastive Language-Image Pretraining(CLIP) model has achieved remarkable success in a variety of downstream vison language understanding tasks, enhancing its capability for fine-grained image-text alignment remains an active research focus. To this end, most existing works adopt the strategy of explicitly increasing the granularity of visual information processing, e.g., incorporating visual prompts to guide the model focus on specific local regions within the image. Meanwhile, researches on Multimodal Large Language Models(MLLMs) have demonstrated that training with long and detailed textual descriptions can effectively improve the model's fine-grained vision-language alignment. However, the inherent token length limitation of CLIP's text encoder fundamentally limits CLIP to process more granular textual information embedded in long text sequences. To synergistically leverage the advantages of enhancing both visual and textual content processing granularity, we propose PixCLIP, a novel framework designed to concurrently accommodate visual prompt inputs and process lengthy textual descriptions. Specifically, we first establish an automated annotation pipeline capable of generating pixel-level localized, long-form textual descriptions for images. Utilizing this pipeline, we construct LongGRIT, a high-quality dataset comprising nearly 1.5 million samples. Secondly, we replace CLIP's original text encoder with the LLM and propose a three-branch pixel-text alignment learning framework, facilitating fine-grained alignment between image regions and corresponding textual descriptions at arbitrary granularity. Experiments demonstrate that PixCLIP showcases breakthroughs in pixel-level interaction and handling long-form texts, achieving state-of-the-art performance.",
        "translated": "尽管对比语言-图像预训练（CLIP）模型在多种下游视觉-语言理解任务中取得了显著成功，但提升其在细粒度图像-文本对齐方面的能力仍是当前的研究热点。为此，现有大多数工作采用显式增强视觉信息处理粒度的策略，例如引入视觉提示（visual prompts）以引导模型关注图像中的特定局部区域。与此同时，多模态大语言模型（MLLMs）的研究表明，使用长且详细的文本描述进行训练，能够有效提升模型的细粒度视觉-语言对齐能力。然而，CLIP文本编码器固有的token长度限制，从根本上制约了其处理嵌入在长文本序列中的更细粒度文本信息的能力。为协同利用增强视觉与文本内容处理粒度的优势，我们提出PixCLIP，一种新颖的框架，旨在同时支持视觉提示输入并处理长文本描述。具体而言，我们首先构建了一条自动化标注流水线，能够为图像生成像素级定位的长文本描述。利用该流水线，我们构建了LongGRIT，一个包含近150万样本的高质量数据集。其次，我们用大语言模型（LLM）替换CLIP原有的文本编码器，并提出一种三分支像素-文本对齐学习框架，实现图像区域与对应文本描述在任意粒度下的细粒度对齐。实验表明，PixCLIP在像素级交互和长文本处理方面取得突破，达到了当前最优性能。",
        "translated_title": "PixCLIP：通过任意粒度的像素-文本对齐学习实现细粒度视觉语言理解",
        "label": [],
        "label_reason": "论文聚焦于视觉-语言对齐，属于高阶语义理解任务，非像素级图像恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出三分支像素-文本对齐框架，结合LLM处理长文本，架构设计有创新。"
    },
    {
        "title": "UniSplat: Unified Spatio-Temporal Fusion via 3D Latent Scaffolds for\n  Dynamic Driving Scene Reconstruction",
        "url": "http://arxiv.org/abs/2511.04595v1",
        "pub_date": "2025-11-06",
        "summary": "Feed-forward 3D reconstruction for autonomous driving has advanced rapidly, yet existing methods struggle with the joint challenges of sparse, non-overlapping camera views and complex scene dynamics. We present UniSplat, a general feed-forward framework that learns robust dynamic scene reconstruction through unified latent spatio-temporal fusion. UniSplat constructs a 3D latent scaffold, a structured representation that captures geometric and semantic scene context by leveraging pretrained foundation models. To effectively integrate information across spatial views and temporal frames, we introduce an efficient fusion mechanism that operates directly within the 3D scaffold, enabling consistent spatio-temporal alignment. To ensure complete and detailed reconstructions, we design a dual-branch decoder that generates dynamic-aware Gaussians from the fused scaffold by combining point-anchored refinement with voxel-based generation, and maintain a persistent memory of static Gaussians to enable streaming scene completion beyond current camera coverage. Extensive experiments on real-world datasets demonstrate that UniSplat achieves state-of-the-art performance in novel view synthesis, while providing robust and high-quality renderings even for viewpoints outside the original camera coverage.",
        "translated": "面向自动驾驶的前馈三维重建技术发展迅速，然而现有方法在应对稀疏、非重叠的相机视角以及复杂场景动态的联合挑战时仍存在困难。本文提出 UniSplat，一种通用的前馈框架，通过统一的潜在时空融合机制学习鲁棒的动态场景重建。UniSplat 构建了一个三维潜在骨架，该结构化表示通过利用预训练的基础模型，捕捉场景的几何与语义上下文。为有效整合跨空间视角和时间帧的信息，我们引入了一种高效的融合机制，直接在三维骨架中运行，实现一致的时空对齐。为确保完整且细致的重建结果，我们设计了一个双分支解码器，通过结合点锚定精化与体素生成的方式，从融合后的骨架生成具有动态感知能力的高斯点，并维持静态高斯点的持久记忆，从而支持超出当前相机覆盖范围的流式场景补全。在真实世界数据集上的大量实验表明，UniSplat 在新视角合成任务中达到当前最优性能，即使在原始相机覆盖范围之外的视角下，也能提供鲁棒且高质量的渲染结果。",
        "translated_title": "UniSplat：基于3D潜在框架的统一时空融合用于动态驾驶场景重建",
        "label": [],
        "label_reason": "论文聚焦3D场景重建与新视角合成，属于高阶视觉任务，非像素级图像恢复。",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出统一时空融合框架与3D潜在结构，对动态场景建模有显著改进。"
    },
    {
        "title": "Jr. AI Scientist and Its Risk Report: Autonomous Scientific Exploration\n  from a Baseline Paper",
        "url": "http://arxiv.org/abs/2511.04583v1",
        "pub_date": "2025-11-06",
        "summary": "Understanding the current capabilities and risks of AI Scientist systems is essential for ensuring trustworthy and sustainable AI-driven scientific progress while preserving the integrity of the academic ecosystem. To this end, we develop Jr. AI Scientist, a state-of-the-art autonomous AI scientist system that mimics the core research workflow of a novice student researcher: Given the baseline paper from the human mentor, it analyzes its limitations, formulates novel hypotheses for improvement, validates them through rigorous experimentation, and writes a paper with the results. Unlike previous approaches that assume full automation or operate on small-scale code, Jr. AI Scientist follows a well-defined research workflow and leverages modern coding agents to handle complex, multi-file implementations, leading to scientifically valuable contributions. For evaluation, we conducted automated assessments using AI Reviewers, author-led evaluations, and submissions to Agents4Science, a venue dedicated to AI-driven scientific contributions. The findings demonstrate that Jr. AI Scientist generates papers receiving higher review scores than existing fully automated systems. Nevertheless, we identify important limitations from both the author evaluation and the Agents4Science reviews, indicating the potential risks of directly applying current AI Scientist systems and key challenges for future research. Finally, we comprehensively report various risks identified during development. We hope these insights will deepen understanding of current progress and risks in AI Scientist development.",
        "translated": "理解当前AI科学家系统的能力与风险，对于确保可信、可持续的AI驱动科学研究进展，同时维护学术生态系统的完整性至关重要。为此，我们开发了Jr. AI Scientist——一种最先进的自主AI科学家系统，其模拟了新手学生研究人员的核心研究工作流程：在获得人类导师提供的基线论文后，系统分析其局限性，提出改进的新假设，通过严谨的实验验证这些假设，并撰写包含实验结果的论文。与以往假设完全自动化或仅处理小规模代码的方法不同，Jr. AI Scientist遵循明确的研究工作流程，并利用现代代码代理处理复杂的多文件实现，从而产生具有科学价值的成果。在评估方面，我们采用AI评审员进行自动化评估，结合作者主导的评估，并向专注于AI驱动科学贡献的Agents4Science平台提交论文。研究结果表明，Jr. AI Scientist生成的论文在评审得分上高于现有的全自动化系统。然而，我们从作者评估和Agents4Science评审中识别出重要局限性，这揭示了当前AI科学家系统直接应用的潜在风险，以及未来研究面临的关键挑战。最后，我们全面报告了在开发过程中识别出的各种风险。我们希望这些洞见能够加深对当前AI科学家系统发展进展与风险的理解。",
        "translated_title": "初级人工智能科学家及其风险报告：基于基准论文的自主科学探索",
        "label": [],
        "label_reason": "论文聚焦AI科研系统自动化，非图像像素级处理，属high-level任务。",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "提出AI科研工作流框架，属系统级创新，非图像处理领域突破。"
    },
    {
        "title": "Thinking with Video: Video Generation as a Promising Multimodal\n  Reasoning Paradigm",
        "url": "http://arxiv.org/abs/2511.04570v1",
        "pub_date": "2025-11-06",
        "summary": "\"Thinking with Text\" and \"Thinking with Images\" paradigm significantly improve the reasoning ability of large language models (LLMs) and Vision Language Models (VLMs). However, these paradigms have inherent limitations. (1) Images capture only single moments and fail to represent dynamic processes or continuous changes, and (2) The separation of text and vision as distinct modalities, hindering unified multimodal understanding and generation. To overcome these limitations, we introduce \"Thinking with Video\", a new paradigm that leverages video generation models, such as Sora-2, to bridge visual and textual reasoning in a unified temporal framework. To support this exploration, we developed the Video Thinking Benchmark (VideoThinkBench). VideoThinkBench encompasses two task categories: (1) vision-centric tasks (e.g., Eyeballing Puzzles), and (2) text-centric tasks (e.g., subsets of GSM8K, MMMU). Our evaluation establishes Sora-2 as a capable reasoner. On vision-centric tasks, Sora-2 is generally comparable to state-of-the-art (SOTA) VLMs, and even surpasses VLMs on several tasks, such as Eyeballing Games. On text-centric tasks, Sora-2 achieves 92% accuracy on MATH, and 75.53% accuracy on MMMU. Furthermore, we systematically analyse the source of these abilities. We also find that self-consistency and in-context learning can improve Sora-2's performance. In summary, our findings demonstrate that the video generation model is the potential unified multimodal understanding and generation model, positions \"thinking with video\" as a unified multimodal reasoning paradigm.",
        "translated": "“以文本思考”（Thinking with Text）和“以图像思考”（Thinking with Images）范式显著提升了大型语言模型（LLMs）和视觉语言模型（VLMs）的推理能力。然而，这些范式存在固有局限性：（1）图像仅捕捉单一时刻，无法表示动态过程或连续变化；（2）文本与视觉作为独立模态被分离，阻碍了统一的多模态理解与生成。为克服这些局限，我们提出“以视频思考”（Thinking with Video）这一新范式，利用视频生成模型（如 Sora-2）在统一的时间框架中连接视觉与文本推理。为支持该探索，我们构建了视频思考基准测试集（VideoThinkBench）。VideoThinkBench 包含两类任务：（1）以视觉为中心的任务（如 Eyeballing Puzzles），（2）以文本为中心的任务（如 GSM8K 和 MMMU 的子集）。我们的评估表明，Sora-2 具备较强的推理能力。在以视觉为中心的任务中，Sora-2 总体表现与当前最先进的（SOTA）VLMs 相当，甚至在若干任务（如 Eyeballing Games）上超越了 VLMs。在以文本为中心的任务中，Sora-2 在 MATH 上达到 92% 的准确率，在 MMMU 上达到 75.53% 的准确率。此外，我们系统性地分析了这些能力的来源，并发现自一致性（self-consistency）和上下文学习（in-context learning）可进一步提升 Sora-2 的性能。综上所述，我们的研究结果表明，视频生成模型具有成为统一多模态理解与生成模型的潜力，“以视频思考”可作为统一的多模态推理范式。",
        "translated_title": "以视频思考：视频生成作为一种有前景的多模态推理范式",
        "label": [],
        "label_reason": "论文聚焦视频生成与多模态推理，属于high-level任务，不涉及图像像素级恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "提出视频生成作为推理范式，属多模态理解新方向，但非low-level图像处理创新。"
    },
    {
        "title": "Evo-1: Lightweight Vision-Language-Action Model with Preserved Semantic\n  Alignment",
        "url": "http://arxiv.org/abs/2511.04555v1",
        "pub_date": "2025-11-06",
        "summary": "Vision-Language-Action (VLA) models have emerged as a powerful framework that unifies perception, language, and control, enabling robots to perform diverse tasks through multimodal understanding. However, current VLA models typically contain massive parameters and rely heavily on large-scale robot data pretraining, leading to high computational costs during training, as well as limited deployability for real-time inference. Moreover, most training paradigms often degrade the perceptual representations of the vision-language backbone, resulting in overfitting and poor generalization to downstream tasks. In this work, we present Evo-1, a lightweight VLA model that reduces computation and improves deployment efficiency, while maintaining strong performance without pretraining on robot data. Evo-1 builds on a native multimodal Vision-Language model (VLM), incorporating a novel cross-modulated diffusion transformer along with an optimized integration module, together forming an effective architecture. We further introduce a two-stage training paradigm that progressively aligns action with perception, preserving the representations of the VLM. Notably, with only 0.77 billion parameters, Evo-1 achieves state-of-the-art results on the Meta-World and RoboTwin suite, surpassing the previous best models by 12.4% and 6.9%, respectively, and also attains a competitive result of 94.8% on LIBERO. In real-world evaluations, Evo-1 attains a 78% success rate with high inference frequency and low memory overhead, outperforming all baseline methods. We release code, data, and model weights to facilitate future research on lightweight and efficient VLA models.",
        "translated": "视觉-语言-动作（Vision-Language-Action, VLA）模型已成为一种强大的框架，能够统一感知、语言与控制，使机器人通过多模态理解执行多样化任务。然而，当前的VLA模型通常包含海量参数，并严重依赖大规模机器人数据的预训练，导致训练阶段计算成本高昂，且在实时推理部署方面受限。此外，大多数训练范式往往损害视觉-语言主干网络的感知表征能力，造成过拟合，并降低对下游任务的泛化性能。在本研究中，我们提出Evo-1，一种轻量级VLA模型，在降低计算开销、提升部署效率的同时，无需在机器人数据上进行预训练即可保持优异性能。Evo-1基于原生多模态视觉-语言模型（Vision-Language Model, VLM），引入一种新颖的跨模态调制扩散Transformer，并结合优化的集成模块，共同构建出高效架构。我们进一步提出两阶段训练范式，逐步对齐动作与感知，保留VLM的表征能力。值得注意的是，仅含0.77亿参数的Evo-1在Meta-World和RoboTwin数据集上均取得当前最优结果，分别超越此前最佳模型12.4%和6.9%，并在LIBERO数据集上达到94.8%的竞争力表现。在真实场景评估中，Evo-1以高推理频率和低内存开销实现了78%的成功率，全面优于所有基线方法。我们公开代码、数据及模型权重，以促进未来对轻量高效VLA模型的研究。",
        "translated_title": "Evo-1：具有保留语义对齐的轻量级视觉-语言-动作模型",
        "label": [],
        "label_reason": "论文聚焦于视觉-语言-动作模型，属于高阶机器人任务，不涉及像素级图像恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 6,
        "novelty_reason": "提出轻量化VLA架构与两阶段训练，对多模态模型有改进，但非low-level图像处理创新。"
    },
    {
        "title": "Learning from Single Timestamps: Complexity Estimation in Laparoscopic\n  Cholecystectomy",
        "url": "http://arxiv.org/abs/2511.04525v1",
        "pub_date": "2025-11-06",
        "summary": "Purpose: Accurate assessment of surgical complexity is essential in Laparoscopic Cholecystectomy (LC), where severe inflammation is associated with longer operative times and increased risk of postoperative complications. The Parkland Grading Scale (PGS) provides a clinically validated framework for stratifying inflammation severity; however, its automation in surgical videos remains largely unexplored, particularly in realistic scenarios where complete videos must be analyzed without prior manual curation. Methods: In this work, we introduce STC-Net, a novel framework for SingleTimestamp-based Complexity estimation in LC via the PGS, designed to operate under weak temporal supervision. Unlike prior methods limited to static images or manually trimmed clips, STC-Net operates directly on full videos. It jointly performs temporal localization and grading through a localization, window proposal, and grading module. We introduce a novel loss formulation combining hard and soft localization objectives and background-aware grading supervision. Results: Evaluated on a private dataset of 1,859 LC videos, STC-Net achieves an accuracy of 62.11% and an F1-score of 61.42%, outperforming non-localized baselines by over 10% in both metrics and highlighting the effectiveness of weak supervision for surgical complexity assessment. Conclusion: STC-Net demonstrates a scalable and effective approach for automated PGS-based surgical complexity estimation from full LC videos, making it promising for post-operative analysis and surgical training.",
        "translated": "目的：在腹腔镜胆囊切除术（LC）中，准确评估手术复杂度至关重要，因为严重的炎症与更长的手术时间和术后并发症风险增加密切相关。Parkland分级量表（PGS）为炎症严重程度的分层提供了临床验证的框架；然而，其在手术视频中的自动化应用仍鲜有探索，尤其是在需要对完整视频进行分析而无需预先人工筛选的真实场景中。\n\n方法：本文提出STC-Net，一种基于单帧时间戳的LC手术复杂度评估新框架，通过PGS实现，并设计用于在弱时序监督下运行。与以往仅限于静态图像或人工裁剪片段的方法不同，STC-Net可直接在完整视频上运行。该框架通过定位、窗口提议和分级模块，联合完成时序定位与分级任务。我们引入了一种新颖的损失函数设计，结合了硬定位目标与软定位目标，并引入背景感知的分级监督机制。\n\n结果：在包含1,859个LC手术视频的私有数据集上进行评估，STC-Net实现了62.11%的准确率和61.42%的F1分数，相较于非定位基线方法，在两项指标上均提升超过10%，凸显了弱监督在手术复杂度评估中的有效性。\n\n结论：STC-Net展示了一种可扩展且有效的自动化方法，能够从完整的LC手术视频中基于PGS评估手术复杂度，为术后分析和手术培训提供了有前景的技术支持。",
        "translated_title": "学习单一时序数据：腹腔镜胆囊切除术中的复杂度估计",
        "label": [],
        "label_reason": "论文聚焦于手术复杂度评估，属于高阶视频理解任务，不涉及像素级图像恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "提出STC-Net框架用于手术视频复杂度评估，属视频理解领域常规改进，无低级视觉创新。"
    },
    {
        "title": "THEval. Evaluation Framework for Talking Head Video Generation",
        "url": "http://arxiv.org/abs/2511.04520v1",
        "pub_date": "2025-11-06",
        "summary": "Video generation has achieved remarkable progress, with generated videos increasingly resembling real ones. However, the rapid advance in generation has outpaced the development of adequate evaluation metrics. Currently, the assessment of talking head generation primarily relies on limited metrics, evaluating general video quality, lip synchronization, and on conducting user studies. Motivated by this, we propose a new evaluation framework comprising 8 metrics related to three dimensions (i) quality, (ii) naturalness, and (iii) synchronization. In selecting the metrics, we place emphasis on efficiency, as well as alignment with human preferences. Based on this considerations, we streamline to analyze fine-grained dynamics of head, mouth, and eyebrows, as well as face quality. Our extensive experiments on 85,000 videos generated by 17 state-of-the-art models suggest that while many algorithms excel in lip synchronization, they face challenges with generating expressiveness and artifact-free details. These videos were generated based on a novel real dataset, that we have curated, in order to mitigate bias of training data. Our proposed benchmark framework is aimed at evaluating the improvement of generative methods. Original code, dataset and leaderboards will be publicly released and regularly updated with new methods, in order to reflect progress in the field.",
        "translated": "视频生成领域已取得显著进展，生成的视频越来越接近真实视频。然而，生成技术的快速发展已远超评估指标的相应发展。目前，对说话人头部生成的评估主要依赖于有限的指标，用于评估整体视频质量、唇部同步性，并辅以用户研究。受此启发，我们提出了一种新的评估框架，包含8个指标，涵盖三个维度：(i) 质量，(ii) 自然度，以及 (iii) 同步性。在选择指标时，我们着重考虑了效率以及与人类偏好的契合度。基于这些考量，我们精简并分析了头部、嘴部和眉毛的细粒度动态，以及面部质量。我们在17种当前最先进的模型生成的85,000个视频上进行了大量实验，结果表明，尽管许多算法在唇部同步方面表现优异，但在生成表情丰富性和无伪影细节方面仍面临挑战。这些视频基于我们精心构建的一个新型真实数据集生成，旨在减轻训练数据的偏差。我们提出的基准评估框架旨在评估生成方法的改进效果。原始代码、数据集和排行榜将公开发布，并定期更新新方法，以反映该领域的最新进展。",
        "translated_title": "THEval. 谈话头像视频生成的评估框架",
        "label": [],
        "label_reason": "论文聚焦于 talking head 视频生成评估框架，属于 high-level 生成任务，非图像恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "提出新评估框架，但方法为指标组合与实验设计，无核心算法创新。"
    },
    {
        "title": "$μ$NeuFMT: Optical-Property-Adaptive Fluorescence Molecular Tomography\n  via Implicit Neural Representation",
        "url": "http://arxiv.org/abs/2511.04510v1",
        "pub_date": "2025-11-06",
        "summary": "Fluorescence Molecular Tomography (FMT) is a promising technique for non-invasive 3D visualization of fluorescent probes, but its reconstruction remains challenging due to the inherent ill-posedness and reliance on inaccurate or often-unknown tissue optical properties. While deep learning methods have shown promise, their supervised nature limits generalization beyond training data. To address these problems, we propose $\\mu$NeuFMT, a self-supervised FMT reconstruction framework that integrates implicit neural-based scene representation with explicit physical modeling of photon propagation. Its key innovation lies in jointly optimize both the fluorescence distribution and the optical properties ($\\mu$) during reconstruction, eliminating the need for precise prior knowledge of tissue optics or pre-conditioned training data. We demonstrate that $\\mu$NeuFMT robustly recovers accurate fluorophore distributions and optical coefficients even with severely erroneous initial values (0.5$\\times$ to 2$\\times$ of ground truth). Extensive numerical, phantom, and in vivo validations show that $\\mu$NeuFMT outperforms conventional and supervised deep learning approaches across diverse heterogeneous scenarios. Our work establishes a new paradigm for robust and accurate FMT reconstruction, paving the way for more reliable molecular imaging in complex clinically related scenarios, such as fluorescence guided surgery.",
        "translated": "荧光分子断层成像（Fluorescence Molecular Tomography, FMT）是一种用于无创三维可视化荧光探针的有前景的技术，但由于其固有的病态性以及对组织光学性质（通常不准确或未知）的依赖，其重建仍具有挑战性。尽管深度学习方法已展现出潜力，但其监督学习特性限制了模型在训练数据之外的泛化能力。为解决这些问题，我们提出 $\\mu$NeuFMT，一种自监督的FMT重建框架，该框架将基于隐式神经网络的场景表示与光子传播的显式物理建模相结合。其核心创新在于在重建过程中联合优化荧光分布与光学性质（$\\mu$），从而消除了对组织光学先验知识或预处理训练数据的精确需求。我们证明，$\\mu$NeuFMT 即使在初始值严重错误（为真实值的 0.5$\\times$ 至 2$\\times$）的情况下，仍能稳健地恢复出准确的荧光体分布和光学系数。广泛的数值模拟、体模实验以及活体验证表明，$\\mu$NeuFMT 在多种异质场景中均优于传统方法和监督式深度学习方法。本工作建立了一种鲁棒且精确的FMT重建新范式，为在复杂临床相关场景（如荧光引导手术）中实现更可靠的分子成像铺平了道路。",
        "translated_title": "$μ$NeuFMT：基于隐式神经表示的光学特性自适应荧光分子断层成像",
        "label": [
            "医学图像增强",
            "图像重建"
        ],
        "label_reason": "基于隐式神经表示的FMT重建，属于医学图像恢复与重建范畴，非传统像素级图像处理",
        "relevance_score": 6,
        "novelty_score": 8,
        "novelty_reason": "提出自监督联合优化荧光分布与光学性质的新范式，创新性强"
    },
    {
        "title": "LLM-as-a-Judge: Toward World Models for Slate Recommendation Systems",
        "url": "http://arxiv.org/abs/2511.04541v1",
        "pub_date": "2025-11-06",
        "summary": "Modeling user preferences across domains remains a key challenge in slate recommendation (i.e. recommending an ordered sequence of items) research. We investigate how Large Language Models (LLM) can effectively act as world models of user preferences through pairwise reasoning over slates. We conduct an empirical study involving several LLMs on three tasks spanning different datasets. Our results reveal relationships between task performance and properties of the preference function captured by LLMs, hinting towards areas for improvement and highlighting the potential of LLMs as world models in recommender systems.",
        "translated": "跨领域建模用户偏好仍是滑动推荐（即推荐有序物料序列）研究中的关键挑战。我们探究了大语言模型（LLM）如何通过在滑动序列上进行成对推理，有效充当用户偏好的世界模型。我们针对三个不同数据集上的任务，对多个LLM进行了实证研究。结果揭示了任务性能与LLM所捕捉的偏好函数属性之间的关系，指出了潜在的改进方向，并凸显了LLM作为推荐系统中世界模型的潜力。",
        "translated_title": "LLM-as-a-Judge：面向榜单推荐系统的“世界模型”构建",
        "label": [
            "LLM生成式推荐（LLM-based Generative Recommendation）",
            "重排（Re-ranking）",
            "通用推荐技术（General Recommendation Techniques）"
        ],
        "label_reason": "论文探索LLM作为用户偏好世界模型用于列表推荐，涉及重排与生成式推荐，与推荐系统核心环节紧密相关。",
        "relevance_score": 8,
        "novelty_score": 8,
        "novelty_reason": "提出将LLM作为世界模型进行成对推理，用于推荐列表生成，方法新颖且具潜力。"
    },
    {
        "title": "RUST-BENCH: Benchmarking LLM Reasoning on Unstructured Text within\n  Structured Tables",
        "url": "http://arxiv.org/abs/2511.04491v1",
        "pub_date": "2025-11-06",
        "summary": "Existing tabular reasoning benchmarks mostly test models on small, uniform tables, underrepresenting the complexity of real-world data and giving an incomplete view of Large Language Models' (LLMs) reasoning abilities. Real tables are long, heterogeneous, and domain-specific, mixing structured fields with free text and requiring multi-hop reasoning across thousands of tokens. To address this gap, we introduce RUST-BENCH, a benchmark of 7966 questions from 2031 real-world tables spanning two domains: i) RB-Science (NSF grant records) and ii) RB-Sports (NBA statistics). Unlike prior work, RUST-BENCH evaluates LLMs jointly across scale, heterogeneity, domain specificity, and reasoning complexity. Experiments with open-source and proprietary models show that LLMs struggle with heterogeneous schemas and complex multi-hop inference, revealing persistent weaknesses in current architectures and prompting strategies. RUST-BENCH establishes a challenging new testbed for advancing tabular reasoning research.",
        "translated": "现有表格推理基准主要在小型、均匀的表格上测试模型，未能充分反映真实世界数据的复杂性，从而对大语言模型（LLM）的推理能力给出了不完整的评估。真实表格通常较长、异构且具有领域特异性，混合了结构化字段与自由文本，需要在数千个token上进行多跳推理。为弥补这一差距，我们引入了RUST-BENCH，一个包含来自2031个真实世界表格的7966个问题的基准，涵盖两个领域：i) RB-Science（NSF资助记录）和ii) RB-Sports（NBA统计数据）。与以往工作不同，RUST-BENCH联合评估LLM在规模、异构性、领域特异性以及推理复杂度方面的表现。对开源和专有模型的实验表明，LLM在处理异构模式和复杂多跳推理时存在困难，揭示了当前架构和提示策略中持续存在的弱点。RUST-BENCH为推进表格推理研究建立了一个具有挑战性的新测试平台。",
        "translated_title": "RUST-BENCH：在结构化表格中的非结构化文本上对大语言模型推理能力进行基准测试",
        "label": [],
        "label_reason": "论文聚焦表格文本推理，与推荐系统无直接关联，属通用NLP任务。",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出新基准RUST-BENCH，挑战LLM在复杂表格上的推理能力，具有方法创新。"
    },
    {
        "title": "Ground-Truth Subgraphs for Better Training and Evaluation of Knowledge\n  Graph Augmented LLMs",
        "url": "http://arxiv.org/abs/2511.04473v1",
        "pub_date": "2025-11-06",
        "summary": "Retrieval of information from graph-structured knowledge bases represents a promising direction for improving the factuality of LLMs. While various solutions have been proposed, a comparison of methods is difficult due to the lack of challenging QA datasets with ground-truth targets for graph retrieval. We present SynthKGQA, a framework for generating high-quality synthetic Knowledge Graph Question Answering datasets from any Knowledge Graph, providing the full set of ground-truth facts in the KG to reason over each question. We show how, in addition to enabling more informative benchmarking of KG retrievers, the data produced with SynthKGQA also allows us to train better models. We apply SynthKGQA to Wikidata to generate GTSQA, a new dataset designed to test zero-shot generalization abilities of KG retrievers with respect to unseen graph structures and relation types, and benchmark popular solutions for KG-augmented LLMs on it.",
        "translated": "从图结构知识库中检索信息是提升大语言模型（LLM）事实性的有前景方向。尽管已有多种解决方案被提出，但由于缺乏具有真实答案标注的、具有挑战性的问答数据集，方法间的比较变得困难。我们提出 SynthKGQA，一个从任意知识图谱（KG）生成高质量合成知识图谱问答（KGQA）数据集的框架，该框架为每个问题提供知识图谱中完整的地面真值事实以供推理。我们展示了 SynthKGQA 生成的数据不仅能够支持对知识图谱检索器进行更具有信息量的基准评测，还能用于训练性能更优的模型。我们将 SynthKGQA 应用于 Wikidata，生成 GTSQA，一个新数据集，旨在测试知识图谱检索器在未见过的图结构和关系类型上的零样本泛化能力，并在该数据集上对流行的基于知识图谱增强的 LLM 解决方案进行基准评测。",
        "translated_title": "真实子图用于知识图谱增强大语言模型的更好训练与评估",
        "label": [
            "LLM生成式推荐",
            "负采样与对比学习"
        ],
        "label_reason": "论文聚焦KG增强LLM的训练与评估，与推荐系统中LLM生成式推荐有间接关联，需适配使用。",
        "relevance_score": 4,
        "novelty_score": 7,
        "novelty_reason": "提出SynthKGQA框架生成高质量合成数据，用于训练和评估KG检索器，方法新颖且实用。"
    },
    {
        "title": "On the Brittleness of CLIP Text Encoders",
        "url": "http://arxiv.org/abs/2511.04247v1",
        "pub_date": "2025-11-06",
        "summary": "Multimodal co-embedding models, especially CLIP, have advanced the state of the art in zero-shot classification and multimedia information retrieval in recent years by aligning images and text in a shared representation space. However, such modals trained on a contrastive alignment can lack stability towards small input perturbations. Especially when dealing with manually expressed queries, minor variations in the query can cause large differences in the ranking of the best-matching results. In this paper, we present a systematic analysis of the effect of multiple classes of non-semantic query perturbations in an multimedia information retrieval scenario. We evaluate a diverse set of lexical, syntactic, and semantic perturbations across multiple CLIP variants using the TRECVID Ad-Hoc Video Search queries and the V3C1 video collection. Across models, we find that syntactic and semantic perturbations drive the largest instabilities, while brittleness is concentrated in trivial surface edits such as punctuation and case. Our results highlight robustness as a critical dimension for evaluating vision-language models beyond benchmark accuracy.",
        "translated": "多模态共嵌入模型，尤其是CLIP，在近年来通过将图像与文本对齐到共享表示空间，显著推动了零样本分类和多媒体信息检索领域的技术发展。然而，这类基于对比对齐训练的模态在面对微小输入扰动时往往缺乏稳定性。特别是在处理人工表达的查询时，查询中的微小变化可能导致最佳匹配结果的排序出现显著差异。本文针对多媒体信息检索场景，系统分析了多种非语义查询扰动的影响。我们采用TRECVID Ad-Hoc视频搜索查询和V3C1视频数据集，评估了多种CLIP变体在词汇、句法和语义扰动下的表现。实验结果表明，句法和语义扰动引发的不稳定性最为显著，而脆弱性主要集中在标点符号和大小写等表面微小编辑上。我们的研究结果强调，鲁棒性是评估视觉-语言模型时除基准准确率之外的关键维度。",
        "translated_title": "关于CLIP文本编码器的脆弱性",
        "label": [
            "多模态推荐",
            "负采样与对比学习"
        ],
        "label_reason": "论文研究CLIP文本编码器在多模态检索中的脆弱性，涉及对比学习与检索稳定性，与多模态推荐间接相关。",
        "relevance_score": 4,
        "novelty_score": 6,
        "novelty_reason": "系统分析多种查询扰动对CLIP的影响，揭示表面编辑导致的不稳定性，属方法性改进非范式创新。"
    },
    {
        "title": "Denoised Recommendation Model with Collaborative Signal Decoupling",
        "url": "http://arxiv.org/abs/2511.04237v1",
        "pub_date": "2025-11-06",
        "summary": "Although the collaborative filtering (CF) algorithm has achieved remarkable performance in recommendation systems, it suffers from suboptimal recommendation performance due to noise in the user-item interaction matrix. Numerous noise-removal studies have improved recommendation models, but most existing approaches conduct denoising on a single graph. This may cause attenuation of collaborative signals: removing edges between two nodes can interrupt paths between other nodes, weakening path-dependent collaborative information. To address these limitations, this study proposes a novel GNN-based CF model called DRCSD for denoising unstable interactions. DRCSD includes two core modules: a collaborative signal decoupling module (decomposes signals into distinct orders by structural characteristics) and an order-wise denoising module (performs targeted denoising on each order). Additionally, the information aggregation mechanism of traditional GNN-based CF models is modified to avoid cross-order signal interference until the final pooling operation. Extensive experiments on three public real-world datasets show that DRCSD has superior robustness against unstable interactions and achieves statistically significant performance improvements in recommendation accuracy metrics compared to state-of-the-art baseline models.",
        "translated": "尽管协同过滤（CF）算法在推荐系统中取得了显著的性能，但由于用户-物料交互矩阵中的噪声，其推荐效果仍存在不足。大量去噪研究已提升了推荐模型性能，但现有大多数方法仅在单个图上进行去噪处理。这可能导致协同信号衰减：移除两个节点之间的边可能中断其他节点之间的路径，从而削弱路径依赖的协同信息。为解决上述局限，本文提出一种基于图神经网络（GNN）的新型CF模型DRCSD，用于去噪不稳定的交互。DRCSD包含两个核心模块：协同信号解耦模块（根据结构特征将信号分解为不同阶次）和逐阶去噪模块（对每一阶次进行针对性去噪）。此外，修改了传统GNN-based CF模型的信息聚合机制，以避免在最终池化操作前出现跨阶次信号干扰。在三个公开真实数据集上的大量实验表明，DRCSD对不稳定交互具有更强的鲁棒性，且在推荐准确率指标上相比最先进的基线模型实现了统计显著的性能提升。",
        "translated_title": "去噪推荐模型与协同信号解耦",
        "label": [
            "图神经网络推荐（GNN for Recommendation）",
            "召回（Recall）",
            "负采样与对比学习（Negative Sampling / Contrastive Learning）"
        ],
        "label_reason": "基于GNN的协同过滤模型，针对噪声交互设计去噪模块，提升推荐准确性，属于推荐系统核心环节。",
        "relevance_score": 8,
        "novelty_score": 8,
        "novelty_reason": "提出信号解耦与分阶去噪机制，避免传统GNN去噪导致的信号衰减，方法新颖且有效。"
    },
    {
        "title": "Coordination-Free Lane Partitioning for Convergent ANN Search",
        "url": "http://arxiv.org/abs/2511.04221v1",
        "pub_date": "2025-11-06",
        "summary": "Production vector search systems often fan out each query across parallel lanes (threads, replicas, or shards) to meet latency service-level objectives (SLOs). In practice, these lanes rediscover the same candidates, so extra compute does not increase coverage. We present a coordination-free lane partitioner that turns duplication into complementary work at the same cost and deadline. For each query we (1) build a deterministic candidate pool sized to the total top-k budget, (2) apply a per-query pseudorandom permutation, and (3) assign each lane a disjoint slice of positions. Lanes then return different results by construction, with no runtime coordination.   At equal cost with four lanes (total candidate budget 64), on SIFT1M (1M SIFT feature vectors) with Hierarchical Navigable Small World graphs (HNSW) recall@10 rises from 0.249 to 0.999 while lane overlap falls from nearly 100% to 0%. On MS MARCO (8.8M passages) with HNSW, hit@10 improves from 0.200 to 0.601 and Mean Reciprocal Rank at 10 (MRR@10) from 0.133 to 0.330. For inverted file (IVF) indexes we see smaller but consistent gains (for example, +11% on MS MARCO) by de-duplicating list routing. A microbenchmark shows planner overhead of ~37 microseconds per query (mean at the main setting) with linear growth in the number of merged candidates.   These results yield a simple operational guideline: size the per-query pool to the total budget, deterministically partition positions across lanes, and turn redundant fan-out into complementary coverage without changing budget or deadline.",
        "translated": "生产环境中的向量检索系统通常将每个查询广播到多个并行通道（线程、副本或分片）中，以满足延迟服务等级目标（SLO）。在实践中，这些通道会重复发现相同的候选结果，因此额外的计算资源并未提升召回覆盖范围。我们提出了一种无需协调的通道划分器，能够在相同成本和截止时间下，将重复计算转化为互补性工作。对于每个查询，我们（1）构建一个大小等于总 top-k 预算的确定性候选池，（2）应用一个针对每个查询的伪随机排列，（3）为每个通道分配互不重叠的位置切片。通过这种设计，各通道天然返回不同的结果，且无需运行时协调。\n\n在四通道配置下（总候选预算为64），在 SIFT1M（100万 SIFT 特征向量）数据集上使用分层可导航小世界图（HNSW）时，recall@10 从 0.249 提升至 0.999，同时通道重叠率从接近 100% 降至 0%。在 MS MARCO（880万段落）数据集上使用 HNSW 时，hit@10 从 0.200 提升至 0.601，MRR@10 从 0.133 提升至 0.330。对于倒排文件（IVF）索引，我们观察到较小但稳定的增益（例如在 MS MARCO 上提升 11%），这是通过消除列表路由中的重复项实现的。微基准测试显示，规划器的每查询开销约为 37 微秒（主配置下的平均值），且随合并候选数量呈线性增长。\n\n这些结果得出一个简单实用的操作指南：将每个查询的候选池大小设置为总预算，以确定性方式在各通道间划分位置，从而在不改变预算或截止时间的前提下，将冗余的广播查询转化为互补的覆盖范围。",
        "translated_title": "无协调车道划分用于收敛的近似最近邻搜索",
        "label": [],
        "label_reason": "论文聚焦ANN搜索优化，虽用于向量检索，但未针对推荐系统设计，与推荐核心环节无直接关联。",
        "relevance_score": 3,
        "novelty_score": 7,
        "novelty_reason": "提出无协调的车道划分策略，提升搜索覆盖，方法新颖且在ANN搜索中有显著效果。"
    },
    {
        "title": "Transforming Mentorship: An AI Powered Chatbot Approach to University\n  Guidance",
        "url": "http://arxiv.org/abs/2511.04172v1",
        "pub_date": "2025-11-06",
        "summary": "University students face immense challenges during their undergraduate lives, often being deprived of personalized on-demand guidance that mentors fail to provide at scale. Digital tools exist, but there is a serious lack of customized coaching for newcomers. This paper presents an AI-powered chatbot that will serve as a mentor for the students of BRAC University. The main component is a data ingestion pipeline that efficiently processes and updates information from diverse sources, such as CSV files and university webpages. The chatbot retrieves information through a hybrid approach, combining BM25 lexical ranking with ChromaDB semantic retrieval, and uses a Large Language Model, LLaMA-3.3-70B, to generate conversational responses. The generated text was found to be semantically highly relevant, with a BERTScore of 0.831 and a METEOR score of 0.809. The data pipeline was also very efficient, taking 106.82 seconds for updates, compared to 368.62 seconds for new data. This chatbot will be able to help students by responding to their queries, helping them to get a better understanding of university life, and assisting them to plan better routines for their semester in the open-credit university.",
        "translated": "大学生在本科阶段面临巨大挑战，常常缺乏个性化、按需提供的指导，而导师难以在大规模场景下提供此类支持。虽然已存在一些数字工具，但针对新生的定制化辅导仍严重不足。本文提出一种基于人工智能的聊天机器人，旨在作为布拉克大学（BRAC University）学生的导师。其核心组件是一个数据摄取管道，能够高效处理和更新来自多种来源（如CSV文件和大学网页）的信息。该聊天机器人采用混合检索方法，结合BM25词法排序与ChromaDB语义检索，并利用大语言模型LLaMA-3.3-70B生成对话式回复。生成文本在语义相关性方面表现优异，BERTScore达到0.831，METEOR得分为0.809。数据管道效率亦很高，更新耗时106.82秒，新数据处理耗时368.62秒。该聊天机器人能够响应学生查询，帮助其更好地理解大学生活，并协助其在学分开放制大学中制定更合理的学期计划。",
        "translated_title": "变革导师制：基于人工智能聊天机器人的大学指导方法",
        "label": [
            "LLM生成式推荐",
            "召回"
        ],
        "label_reason": "论文基于LLM构建聊天机器人，结合BM25与语义检索，用于学生指导，属于生成式推荐范畴，但非典型推荐系统场景。",
        "relevance_score": 5,
        "novelty_score": 6,
        "novelty_reason": "采用混合检索+LLM生成，提升响应相关性，但方法组合属常规，无突破性创新。"
    },
    {
        "title": "E-CARE: An Efficient LLM-based Commonsense-Augmented Framework for\n  E-Commerce",
        "url": "http://arxiv.org/abs/2511.04087v1",
        "pub_date": "2025-11-06",
        "summary": "Finding relevant products given a user query plays a pivotal role in an e-commerce platform, as it can spark shopping behaviors and result in revenue gains. The challenge lies in accurately predicting the correlation between queries and products. Recently, mining the cross-features between queries and products based on the commonsense reasoning capacity of Large Language Models (LLMs) has shown promising performance. However, such methods suffer from high costs due to intensive real-time LLM inference during serving, as well as human annotations and potential Supervised Fine Tuning (SFT). To boost efficiency while leveraging the commonsense reasoning capacity of LLMs for various e-commerce tasks, we propose the Efficient Commonsense-Augmented Recommendation Enhancer (E-CARE). During inference, models augmented with E-CARE can access commonsense reasoning with only a single LLM forward pass per query by utilizing a commonsense reasoning factor graph that encodes most of the reasoning schema from powerful LLMs. The experiments on 2 downstream tasks show an improvement of up to 12.1% on precision@5.",
        "translated": "在电子商务平台中，根据用户查询找到相关商品起着关键作用，因为它能够激发购物行为并带来收入增长。挑战在于准确预测查询与商品之间的关联性。近期，基于大语言模型（LLM）的常识推理能力挖掘查询与商品之间的交叉特征，已展现出良好的性能。然而，此类方法在服务阶段因需要密集的实时LLM推理，以及人工标注和潜在的监督微调（SFT），导致成本较高。为了在提升效率的同时充分利用LLM的常识推理能力以应对多种电商任务，我们提出了一种高效常识增强推荐增强器（E-CARE）。在推理阶段，集成E-CARE的模型可通过每个查询仅进行一次LLM前向传播，借助编码了强大LLM中大部分推理模式的常识推理因子图，访问常识推理能力。在两个下游任务上的实验表明，该方法在precision@5指标上最高提升了12.1%。",
        "translated_title": "E-CARE：一种高效的基于大语言模型的常识增强电商推荐框架",
        "label": [
            "LLM生成式推荐（LLM-based Generative Recommendation）",
            "召回（Recall）",
            "通用推荐技术（General Recommendation Techniques）"
        ],
        "label_reason": "论文提出基于LLM的高效常识增强框架，用于电商推荐召回，显著提升精度，与推荐系统核心环节紧密相关。",
        "relevance_score": 9,
        "novelty_score": 8,
        "novelty_reason": "提出因子图结构减少LLM推理次数，结合常识推理提升推荐效果，方法新颖且具实用性。"
    },
    {
        "title": "Publication Trend in DESIDOC Journal of Library and Information\n  Technology during 2013-2017: A Scientometric Approach",
        "url": "http://arxiv.org/abs/2511.04082v1",
        "pub_date": "2025-11-06",
        "summary": "DESIDOC Journal of Library &amp; Information Technology (DJLIT) formerly known as DESIDOC Bulletin of Information Technology is a peer-reviewed, open access, bimonthly journal. This paper presents a Scientometric analysis of the DESIDOC Journal. The paper analyses the pattern of growth of the research output published in the journal, pattern of authorship, author productivity, and, subjects covered to the papers over the period (2013-2017). It is found that 227 papers were published during the period of study (2001-2012). The maximum numbers of articles were collaborative in nature. The subject concentration of the journal noted is Scientometrics. The maximum numbers of articles (65%) have ranged their thought contents between 6 and 10 pages. The study applied standard formula and statistical tools to bring out the factual result.",
        "translated": "DESIDOC图书馆与信息技术期刊（DJLIT），前身为DESIDOC信息技术公报，是一本经过同行评审、开放获取的双月刊。本文对DESIDOC期刊进行了科学计量学分析。研究分析了2013至2017年间该期刊发表的研究成果的增长模式、作者署名模式、作者生产力以及论文所覆盖的主题领域。研究发现，在2001至2012年的研究期间，共发表了227篇论文。其中，合作撰写的论文数量最多。该期刊的主题集中于科学计量学。超过65%的论文篇幅在6至10页之间。本研究采用标准公式和统计工具，以得出客观的分析结果。",
        "translated_title": "2013–2017年DESIDOC图书馆与信息技术期刊的出版趋势：一种科学计量学方法",
        "label": [],
        "label_reason": "论文为信息计量学分析，研究期刊出版趋势，与推荐系统无直接关联。",
        "relevance_score": 1,
        "novelty_score": 1,
        "novelty_reason": "仅采用标准统计方法进行文献分析，无新方法或技术贡献。"
    },
    {
        "title": "Caption Injection for Optimization in Generative Search Engine",
        "url": "http://arxiv.org/abs/2511.04080v1",
        "pub_date": "2025-11-06",
        "summary": "Generative Search Engines (GSEs) leverage Retrieval-Augmented Generation (RAG) techniques and Large Language Models (LLMs) to integrate multi-source information and provide users with accurate and comprehensive responses. Unlike traditional search engines that present results in ranked lists, GSEs shift users' attention from sequential browsing to content-driven subjective perception, driving a paradigm shift in information retrieval. In this context, enhancing the subjective visibility of content through Generative Search Engine Optimization (G-SEO) methods has emerged as a new research focus. With the rapid advancement of Multimodal Retrieval-Augmented Generation (MRAG) techniques, GSEs can now efficiently integrate text, images, audio, and video, producing richer responses that better satisfy complex information needs. Existing G-SEO methods, however, remain limited to text-based optimization and fail to fully exploit multimodal data. To address this gap, we propose Caption Injection, the first multimodal G-SEO approach, which extracts captions from images and injects them into textual content, integrating visual semantics to enhance the subjective visibility of content in generative search scenarios. We systematically evaluate Caption Injection on MRAMG, a benchmark for MRAG, under both unimodal and multimodal settings. Experimental results show that Caption Injection significantly outperforms text-only G-SEO baselines under the G-Eval metric, demonstrating the necessity and effectiveness of multimodal integration in G-SEO to improve user-perceived content visibility.",
        "translated": "生成式搜索引擎（GSEs）利用检索增强生成（RAG）技术与大语言模型（LLM），整合多源信息，为用户提供准确且全面的响应。与传统搜索引擎以排序列表形式呈现结果不同，GSEs将用户的注意力从顺序浏览转向以内容驱动的主观感知，推动了信息检索范式的转变。在此背景下，通过生成式搜索引擎优化（G-SEO）方法提升内容的主观可见性，已成为新的研究焦点。随着多模态检索增强生成（MRAG）技术的快速发展，GSEs现已能够高效整合文本、图像、音频和视频，生成更丰富的响应，更好地满足复杂的信息需求。然而，现有的G-SEO方法仍局限于基于文本的优化，未能充分挖掘多模态数据的潜力。为弥补这一空白，我们提出Caption Injection，这是首个多模态G-SEO方法，通过从图像中提取字幕并注入文本内容，融合视觉语义，从而提升生成式搜索场景下内容的主观可见性。我们在MRAMG（MRAG基准）上系统评估了Caption Injection，分别在单模态和多模态设置下进行实验。实验结果表明，Caption Injection在G-Eval指标下显著优于仅基于文本的G-SEO基线方法，证明了在G-SEO中融合多模态数据的必要性与有效性，以提升用户感知的内容可见性。",
        "translated_title": "生成式搜索引擎中的标题注入优化",
        "label": [
            "LLM生成式推荐（LLM-based Generative Recommendation）",
            "多模态推荐（Multimodal Recommendation）"
        ],
        "label_reason": "论文聚焦生成式搜索优化，涉及LLM与多模态融合，虽非传统推荐，但与生成式推荐技术密切相关。",
        "relevance_score": 4,
        "novelty_score": 7,
        "novelty_reason": "提出首个多模态G-SEO方法，注入图像字幕提升内容可见性，方法新颖且有效。"
    },
    {
        "title": "Two Decades of Research at the University of Lagos (2004-2023): A\n  Scientometric Analysis of Productivity, Collaboration, and Impact",
        "url": "http://arxiv.org/abs/2511.04075v1",
        "pub_date": "2025-11-06",
        "summary": "This paper presents a scientometric analysis of research output from the University of Lagos, focusing on the two decades spanning 2004 to 2023. Using bibliometric data retrieved from the Web of Science, we examine trends in publication volume, collaboration patterns, citation impact, and the most prolific authors, departments, and research domains at the university. The study reveals a consistent increase in research productivity, with the highest publication output recorded in 2023. Health Sciences, Engineering, and Social Sciences are identified as dominant fields, reflecting the university's interdisciplinary research strengths. Collaborative efforts, both locally and internationally, show a positive correlation with higher citation impact, with the United States and the United Kingdom being the leading international collaborators. Notably, open-access publications account for a significant portion of the university's research output, enhancing visibility and citation rates. The findings offer valuable insights into the university's research performance over the past two decades, providing a foundation for strategic planning and policy formulation to foster research excellence and global impact.",
        "translated": "本文对拉各斯大学在2004至2023年二十年间的研究产出进行了科学计量学分析。基于从Web of Science获取的文献计量数据，我们考察了该大学在论文发表数量、合作模式、引用影响力以及最具产研力的作者、院系和研究领域等方面的发展趋势。研究结果表明，其研究生产力呈现持续增长态势，2023年发表论文数量达到最高。健康科学、工程学和社会科学被识别为主要优势领域，反映出该大学在跨学科研究方面的综合实力。本地与国际间的合作均与更高的引用影响力呈正相关，其中美国和英国是其最主要的国际合作伙伴。值得注意的是，开放获取（open-access）论文占该大学研究产出的相当大比例，显著提升了研究成果的可见度和引用率。本研究结果为过去二十年拉各斯大学的研究表现提供了重要洞察，可为未来战略规划和政策制定提供依据，以促进研究卓越性和全球影响力。",
        "translated_title": "拉各斯大学二十年研究（2004–2023）：生产力、合作与影响力的知识计量分析",
        "label": [],
        "label_reason": "论文为科研产出的科学计量分析，与推荐系统无直接关联。",
        "relevance_score": 1,
        "novelty_score": 1,
        "novelty_reason": "仅进行文献计量统计分析，无技术创新或方法论贡献。"
    },
    {
        "title": "Learning Filter-Aware Distance Metrics for Nearest Neighbor Search with\n  Multiple Filters",
        "url": "http://arxiv.org/abs/2511.04073v1",
        "pub_date": "2025-11-06",
        "summary": "Filtered Approximate Nearest Neighbor (ANN) search retrieves the closest vectors for a query vector from a dataset. It enforces that a specified set of discrete labels $S$ for the query must be included in the labels of each retrieved vector. Existing graph-based methods typically incorporate filter awareness by assigning fixed penalties or prioritizing nodes based on filter satisfaction. However, since these methods use fixed, data in- dependent penalties, they often fail to generalize across datasets with diverse label and vector distributions. In this work, we propose a principled alternative that learns the optimal trade-off between vector distance and filter match directly from the data, rather than relying on fixed penalties. We formulate this as a constrained linear optimization problem, deriving weights that better reflect the underlying filter distribution and more effectively address the filtered ANN search problem. These learned weights guide both the search process and index construction, leading to graph structures that more effectively capture the underlying filter distribution and filter semantics. Our experiments demonstrate that adapting the distance function to the data significantly im- proves accuracy by 5-10% over fixed-penalty methods, providing a more flexible and generalizable framework for the filtered ANN search problem.",
        "translated": "过滤近似最近邻（ANN）搜索从数据集中为查询向量检索最接近的向量，并强制要求查询的指定离散标签集合 $S$ 必须包含在每个检索向量的标签中。现有的基于图的方法通常通过为满足过滤条件的节点分配固定惩罚或优先级来实现过滤感知。然而，由于这些方法采用固定且与数据无关的惩罚值，它们往往难以在具有不同标签和向量分布的数据集上泛化。在本研究中，我们提出了一种基于原理的替代方案，该方案直接从数据中学习向量距离与过滤匹配之间的最优权衡，而非依赖固定惩罚。我们将此问题形式化为一个带约束的线性优化问题，推导出能够更好反映底层过滤分布并更有效地解决过滤ANN搜索问题的权重。这些学习到的权重指导搜索过程和索引构建，从而生成更能有效捕捉底层过滤分布和过滤语义的图结构。实验结果表明，将距离函数适配于数据相比固定惩罚方法可显著提升5-10%的准确率，为过滤ANN搜索问题提供了一个更具灵活性和泛化能力的框架。",
        "translated_title": "学习面向多过滤条件的最近邻搜索的过滤感知距离度量",
        "label": [],
        "label_reason": "论文聚焦于带过滤条件的近似最近邻搜索，属于通用信息检索范畴，与推荐系统无直接关联。",
        "relevance_score": 3,
        "novelty_score": 7,
        "novelty_reason": "提出学习数据驱动的滤波距离权重，改进传统固定惩罚方法，具有创新性但非推荐领域应用。"
    },
    {
        "title": "KnowThyself: An Agentic Assistant for LLM Interpretability",
        "url": "http://arxiv.org/abs/2511.03878v1",
        "pub_date": "2025-11-05",
        "summary": "We develop KnowThyself, an agentic assistant that advances large language model (LLM) interpretability. Existing tools provide useful insights but remain fragmented and code-intensive. KnowThyself consolidates these capabilities into a chat-based interface, where users can upload models, pose natural language questions, and obtain interactive visualizations with guided explanations. At its core, an orchestrator LLM first reformulates user queries, an agent router further directs them to specialized modules, and the outputs are finally contextualized into coherent explanations. This design lowers technical barriers and provides an extensible platform for LLM inspection. By embedding the whole process into a conversational workflow, KnowThyself offers a robust foundation for accessible LLM interpretability.",
        "translated": "我们开发了 KnowThyself，一种用于提升大语言模型（LLM）可解释性的智能体助手。现有工具虽能提供有价值的洞察，但功能分散且高度依赖代码。KnowThyself 将这些能力整合至一个基于对话的界面中，用户可上传模型、以自然语言提问，并获得带有引导式解释的交互式可视化结果。其核心机制为：一个协调器 LLM 首先对用户查询进行重述，随后由代理路由模块将其导向专门的功能模块，最终将输出结果整合为连贯的解释。该设计降低了技术门槛，并为 LLM 检查提供了可扩展的平台。通过将整个流程嵌入对话式工作流，KnowThyself 为可访问的 LLM 可解释性奠定了坚实基础。",
        "translated_title": "KnowThyself：面向大语言模型可解释性的智能体助手",
        "label": [],
        "label_reason": "论文聚焦LLM可解释性工具开发，与推荐系统无直接关联，属通用NLP工具。",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出基于代理的交互式解释框架，整合多模块，设计新颖，提升可访问性。"
    },
    {
        "title": "Visual Spatial Tuning",
        "url": "http://arxiv.org/abs/2511.05491v1",
        "pub_date": "2025-11-07",
        "summary": "Capturing spatial relationships from visual inputs is a cornerstone of human-like general intelligence. Several previous studies have tried to enhance the spatial awareness of Vision-Language Models (VLMs) by adding extra expert encoders, which brings extra overhead and usually harms general capabilities. To enhance the spatial ability in general architectures, we introduce Visual Spatial Tuning (VST), a comprehensive framework to cultivate VLMs with human-like visuospatial abilities, from spatial perception to reasoning. We first attempt to enhance spatial perception in VLMs by constructing a large-scale dataset termed VST-P, which comprises 4.1 million samples spanning 19 skills across single views, multiple images, and videos. Then, we present VST-R, a curated dataset with 135K samples that instruct models to reason in space. In particular, we adopt a progressive training pipeline: supervised fine-tuning to build foundational spatial knowledge, followed by reinforcement learning to further improve spatial reasoning abilities. Without the side-effect to general capabilities, the proposed VST consistently achieves state-of-the-art results on several spatial benchmarks, including $34.8\\%$ on MMSI-Bench and $61.2\\%$ on VSIBench. It turns out that the Vision-Language-Action models can be significantly enhanced with the proposed spatial tuning paradigm, paving the way for more physically grounded AI.",
        "translated": "从视觉输入中捕捉空间关系是类人泛化智能的核心。先前若干研究尝试通过引入额外的专家编码器来增强视觉-语言模型（VLMs）的空间感知能力，但这一方法带来了额外开销，通常会损害其泛化能力。为在通用架构中提升空间能力，我们提出视觉空间调优（Visual Spatial Tuning, VST），一个全面的框架，旨在从空间感知到空间推理层面培养具备类人视觉空间能力的VLMs。我们首先通过构建大规模数据集VST-P增强VLMs的空间感知能力，该数据集包含410万样本，覆盖19项技能，涵盖单视角、多图像及视频场景。随后，我们提出精心构建的数据集VST-R，包含13.5万样本，旨在引导模型进行空间推理。具体而言，我们采用渐进式训练流程：首先通过监督微调建立基础空间知识，随后通过强化学习进一步提升空间推理能力。在不损害通用能力的前提下，所提出的VST方法在多个空间基准测试中持续取得最先进性能，例如在MMSI-Bench上达到34.8%，在VSIBench上达到61.2%。结果表明，采用所提出的空间调优范式可显著增强视觉-语言-动作模型，为更具物理真实性的AI发展铺平道路。",
        "translated_title": "视觉空间调谐",
        "label": [],
        "label_reason": "属于高阶视觉任务，非像素级图像处理",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "提出新训练范式提升空间推理能力"
    },
    {
        "title": "TimeSearch-R: Adaptive Temporal Search for Long-Form Video Understanding\n  via Self-Verification Reinforcement Learning",
        "url": "http://arxiv.org/abs/2511.05489v1",
        "pub_date": "2025-11-07",
        "summary": "Temporal search aims to identify a minimal set of relevant frames from tens of thousands based on a given query, serving as a foundation for accurate long-form video understanding. Existing works attempt to progressively narrow the search space. However, these approaches typically rely on a hand-crafted search process, lacking end-to-end optimization for learning optimal search strategies. In this paper, we propose TimeSearch-R, which reformulates temporal search as interleaved text-video thinking, seamlessly integrating searching video clips into the reasoning process through reinforcement learning (RL). However, applying RL training methods, such as Group Relative Policy Optimization (GRPO), to video reasoning can result in unsupervised intermediate search decisions. This leads to insufficient exploration of the video content and inconsistent logical reasoning. To address these issues, we introduce GRPO with Completeness Self-Verification (GRPO-CSV), which gathers searched video frames from the interleaved reasoning process and utilizes the same policy model to verify the adequacy of searched frames, thereby improving the completeness of video reasoning. Additionally, we construct datasets specifically designed for the SFT cold-start and RL training of GRPO-CSV, filtering out samples with weak temporal dependencies to enhance task difficulty and improve temporal search capabilities. Extensive experiments demonstrate that TimeSearch-R achieves significant improvements on temporal search benchmarks such as Haystack-LVBench and Haystack-Ego4D, as well as long-form video understanding benchmarks like VideoMME and MLVU. Notably, TimeSearch-R establishes a new state-of-the-art on LongVideoBench with 4.1% improvement over the base model Qwen2.5-VL and 2.0% over the advanced video reasoning model Video-R1. Our code is available at https://github.com/Time-Search/TimeSearch-R.",
        "translated": "时序搜索旨在从数万帧视频中识别出与给定查询最相关的最小帧集，为准确的长视频理解提供基础。现有方法试图逐步缩小搜索空间，但这些方法通常依赖手工设计的搜索流程，缺乏端到端优化以学习最优搜索策略。本文提出 TimeSearch-R，将时序搜索重新建模为交织的图文推理过程，通过强化学习（RL）将视频片段搜索无缝融入推理流程。然而，将 RL 训练方法（如 Group Relative Policy Optimization, GRPO）应用于视频推理时，会导致中间搜索决策无监督，从而造成视频内容探索不足和推理逻辑不一致。为解决这些问题，我们引入了具备完整性自验证机制的 GRPO（GRPO-CSV），该方法从交织推理过程中收集搜索到的视频帧，并利用同一策略模型验证所搜索帧的充分性，从而提升视频推理的完整性。此外，我们构建了专门用于 GRPO-CSV 的监督微调（SFT）冷启动与 RL 训练的数据集，剔除具有弱时序依赖性的样本，以提高任务难度并增强时序搜索能力。大量实验表明，TimeSearch-R 在 Haystack-LVBench 和 Haystack-Ego4D 等时序搜索基准上显著优于现有方法，同时在 VideoMME 和 MLVU 等长视频理解基准上亦表现优异。值得注意的是，TimeSearch-R 在 LongVideoBench 上创下新纪录，相较基线模型 Qwen2.5-VL 提升 4.1%，相较先进视频推理模型 Video-R1 提升 2.0%。我们的代码开源于 https://github.com/Time-Search/TimeSearch-R。",
        "translated_title": "TimeSearch-R：通过自验证强化学习实现长视频理解的自适应时序搜索",
        "label": [],
        "label_reason": "任务为视频理解，非图像像素级处理",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "提出新RL框架提升视频搜索效率"
    },
    {
        "title": "On Flow Matching KL Divergence",
        "url": "http://arxiv.org/abs/2511.05480v1",
        "pub_date": "2025-11-07",
        "summary": "We derive a deterministic, non-asymptotic upper bound on the Kullback-Leibler (KL) divergence of the flow-matching distribution approximation. In particular, if the $L_2$ flow-matching loss is bounded by $\\epsilon^2 &gt; 0$, then the KL divergence between the true data distribution and the estimated distribution is bounded by $A_1 \\epsilon + A_2 \\epsilon^2$. Here, the constants $A_1$ and $A_2$ depend only on the regularities of the data and velocity fields. Consequently, this bound implies statistical convergence rates of Flow Matching Transformers under the Total Variation (TV) distance. We show that, flow matching achieves nearly minimax-optimal efficiency in estimating smooth distributions. Our results make the statistical efficiency of flow matching comparable to that of diffusion models under the TV distance. Numerical studies on synthetic and learned velocities corroborate our theory.",
        "translated": "我们推导出流匹配分布近似中 Kullback-Leibler (KL) 散度的一个确定性、非渐近的上界。具体而言，若 $L_2$ 流匹配损失被 $\\epsilon^2 > 0$ 限制，则真实数据分布与估计分布之间的 KL 散度被 $A_1 \\epsilon + A_2 \\epsilon^2$ 所限制。此处，常数 $A_1$ 和 $A_2$ 仅依赖于数据与速度场的正则性。因此，该界意味着流匹配变换器在总变差 (TV) 距离下的统计收敛速率。我们证明，流匹配在估计光滑分布时几乎达到最小最大最优效率。我们的结果使流匹配在 TV 距离下的统计效率与扩散模型相当。对合成及学习得到的速度场的数值研究验证了我们的理论。",
        "translated_title": "关于流匹配 KL 散度",
        "label": [],
        "label_reason": "理论分析流匹配，非图像处理任务",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "首次理论分析流匹配统计效率"
    },
    {
        "title": "GroupKAN: Rethinking Nonlinearity with Grouped Spline-based KAN Modeling\n  for Efficient Medical Image Segmentation",
        "url": "http://arxiv.org/abs/2511.05477v1",
        "pub_date": "2025-11-07",
        "summary": "Medical image segmentation requires models that are accurate, lightweight, and interpretable. Convolutional architectures lack adaptive nonlinearity and transparent decision-making, whereas Transformer architectures are hindered by quadratic complexity and opaque attention mechanisms. U-KAN addresses these challenges using Kolmogorov-Arnold Networks, achieving higher accuracy than both convolutional and attention-based methods, fewer parameters than Transformer variants, and improved interpretability compared to conventional approaches. However, its O(C^2) complexity due to full-channel transformations limits its scalability as the number of channels increases. To overcome this, we introduce GroupKAN, a lightweight segmentation network that incorporates two novel, structured functional modules: (1) Grouped KAN Transform, which partitions channels into G groups for multivariate spline mappings, reducing complexity to O(C^2/G), and (2) Grouped KAN Activation, which applies shared spline-based mappings within each channel group for efficient, token-wise nonlinearity. Evaluated on three medical benchmarks (BUSI, GlaS, and CVC), GroupKAN achieves an average IoU of 79.80 percent, surpassing U-KAN by +1.11 percent while requiring only 47.6 percent of the parameters (3.02M vs 6.35M), and shows improved interpretability.",
        "translated": "医学图像分割需要准确、轻量且可解释的模型。卷积架构缺乏自适应非线性能力与透明的决策机制，而Transformer架构则受限于二次复杂度和不透明的注意力机制。U-KAN 通过采用柯尔莫哥洛夫-阿诺德网络（Kolmogorov-Arnold Networks）解决了上述挑战，在精度上超越卷积与注意力机制方法，参数量少于各类Transformer变体，并相较传统方法提升了可解释性。然而，由于其全通道变换导致的 O(C²) 复杂度，当通道数增加时，其可扩展性受限。为克服此问题，我们提出了 GroupKAN，一种轻量级分割网络，包含两个新颖的结构化功能模块：（1）分组KAN变换，将通道划分为 G 组，以进行多元样条映射，将复杂度降低至 O(C²/G)；（2）分组KAN激活，对每个通道组内共享样条映射，实现高效、逐token的非线性变换。在三个医学基准数据集（BUSI、GlaS 和 CVC）上的评估表明，GroupKAN 达到平均 IoU 79.80%，相较 U-KAN 提升 +1.11%，同时仅需 47.6% 的参数（3.02M vs 6.35M），且展现出更优的可解释性。",
        "translated_title": "GroupKAN：基于分组样条的KAN建模重新思考非线性，用于高效医学图像分割",
        "label": [],
        "label_reason": "专注医学图像分割，属高阶视觉任务",
        "relevance_score": 3,
        "novelty_score": 7,
        "novelty_reason": "提出分组KAN结构，提升效率与可解释性"
    },
    {
        "title": "Semantic-Guided Natural Language and Visual Fusion for Cross-Modal\n  Interaction Based on Tiny Object Detection",
        "url": "http://arxiv.org/abs/2511.05474v1",
        "pub_date": "2025-11-07",
        "summary": "This paper introduces a cutting-edge approach to cross-modal interaction for tiny object detection by combining semantic-guided natural language processing with advanced visual recognition backbones. The proposed method integrates the BERT language model with the CNN-based Parallel Residual Bi-Fusion Feature Pyramid Network (PRB-FPN-Net), incorporating innovative backbone architectures such as ELAN, MSP, and CSP to optimize feature extraction and fusion. By employing lemmatization and fine-tuning techniques, the system aligns semantic cues from textual inputs with visual features, enhancing detection precision for small and complex objects. Experimental validation using the COCO and Objects365 datasets demonstrates that the model achieves superior performance. On the COCO2017 validation set, it attains a 52.6% average precision (AP), outperforming YOLO-World significantly while maintaining half the parameter consumption of Transformer-based models like GLIP. Several test on different of backbones such ELAN, MSP, and CSP further enable efficient handling of multi-scale objects, ensuring scalability and robustness in resource-constrained environments. This study underscores the potential of integrating natural language understanding with advanced backbone architectures, setting new benchmarks in object detection accuracy, efficiency, and adaptability to real-world challenges.",
        "translated": "本文提出了一种前沿的跨模态交互方法，通过结合语义引导的自然语言处理与先进的视觉识别主干网络，用于小目标检测。所提方法将BERT语言模型与基于CNN的并行残差双融合特征金字塔网络（PRB-FPN-Net）相结合，并引入创新的主干网络架构（如ELAN、MSP和CSP），以优化特征提取与融合。通过词形还原与微调技术，系统将文本输入中的语义线索与视觉特征对齐，从而提升对小尺寸及复杂目标的检测精度。在COCO与Objects365数据集上的实验验证表明，该模型性能卓越：在COCO2017验证集上，其平均精度（AP）达到52.6%，显著优于YOLO-World，同时参数量仅为基于Transformer的模型（如GLIP）的一半。针对不同主干网络（如ELAN、MSP和CSP）的多组测试进一步证明了该方法在处理多尺度目标方面的高效性，确保其在资源受限环境中的可扩展性与鲁棒性。本研究凸显了将自然语言理解与先进主干架构相结合的巨大潜力，为目标检测在精度、效率及对现实挑战的适应性方面树立了新的基准。",
        "translated_title": "基于小目标检测的语义引导的自然语言与视觉融合用于跨模态交互",
        "label": [],
        "label_reason": "目标为物体检测，属高阶视觉任务",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "融合NLP与视觉架构，提升检测精度"
    },
    {
        "title": "EventFlow: Real-Time Neuromorphic Event-Driven Classification of\n  Two-Phase Boiling Flow Regimes",
        "url": "http://arxiv.org/abs/2511.05467v1",
        "pub_date": "2025-11-07",
        "summary": "Flow boiling is an efficient heat transfer mechanism capable of dissipating high heat loads with minimal temperature variation, making it an ideal thermal management method. However, sudden shifts between flow regimes can disrupt thermal performance and system reliability, highlighting the need for accurate and low-latency real-time monitoring. Conventional optical imaging methods are limited by high computational demands and insufficient temporal resolution, making them inadequate for capturing transient flow behavior. To address this, we propose a real-time framework based on signals from neuromorphic sensors for flow regime classification. Neuromorphic sensors detect changes in brightness at individual pixels, which typically correspond to motion at edges, enabling fast and efficient detection without full-frame reconstruction, providing event-based information. We develop five classification models using both traditional image data and event-based data, demonstrating that models leveraging event data outperform frame-based approaches due to their sensitivity to dynamic flow features. Among these models, the event-based long short-term memory model provides the best balance between accuracy and speed, achieving 97.6% classification accuracy with a processing time of 0.28 ms. Our asynchronous processing pipeline supports continuous, low-latency predictions and delivers stable output through a majority voting mechanisms, enabling reliable real-time feedback for experimental control and intelligent thermal management.",
        "translated": "流动沸腾是一种高效的热传递机制，能够在温度变化极小的情况下消散高热负荷，因而成为理想的热管理方法。然而，流动工况在不同流型之间的突然切换会破坏热性能并影响系统可靠性，凸显了对准确且低延迟的实时监测的必要性。传统的光学成像方法受限于高计算需求和不足的时间分辨率，难以捕捉瞬态流动行为。为此，我们提出了一种基于神经形态传感器信号的实时框架，用于流动工况分类。神经形态传感器在单个像素上检测亮度变化，这些变化通常对应于边缘处的运动，从而无需完整帧重建即可实现快速高效的检测，并提供事件驱动的信息。我们利用传统图像数据和事件驱动数据开发了五种分类模型，结果表明，利用事件数据的模型由于对动态流动特征更敏感，其性能优于基于帧的数据方法。其中，事件驱动的长短期记忆（LSTM）模型在准确率与速度之间取得了最佳平衡，实现了97.6%的分类准确率，处理时间为0.28毫秒。我们的异步处理流水线支持连续、低延迟的预测，并通过多数投票机制确保输出稳定，从而为实验控制和智能热管理提供可靠的实时反馈。",
        "translated_title": "EventFlow：面向两相沸腾流型的实时神经形态事件驱动分类方法",
        "label": [],
        "label_reason": "任务为流态分类，属高阶视觉任务",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "事件驱动模型创新，但非图像恢复任务"
    },
    {
        "title": "Photo Dating by Facial Age Aggregation",
        "url": "http://arxiv.org/abs/2511.05464v1",
        "pub_date": "2025-11-07",
        "summary": "We introduce a novel method for Photo Dating which estimates the year a photograph was taken by leveraging information from the faces of people present in the image. To facilitate this research, we publicly release CSFD-1.6M, a new dataset containing over 1.6 million annotated faces, primarily from movie stills, with identity and birth year annotations. Uniquely, our dataset provides annotations for multiple individuals within a single image, enabling the study of multi-face information aggregation. We propose a probabilistic framework that formally combines visual evidence from modern face recognition and age estimation models, and career-based temporal priors to infer the photo capture year. Our experiments demonstrate that aggregating evidence from multiple faces consistently improves the performance and the approach significantly outperforms strong, scene-based baselines, particularly for images containing several identifiable individuals.",
        "translated": "我们提出了一种新颖的“照片年代估计”方法，该方法通过利用图像中人物面部信息来估计照片拍摄年份。为支持此项研究，我们公开发布了 CSFD-1.6M 数据集，该数据集包含超过 160 万张标注面部图像，主要来源于电影剧照，并标注了身份与出生年份。独特的是，我们的数据集为单张图像中多个个体提供标注，从而支持多面部信息聚合研究。我们提出了一种概率框架，形式化地结合了现代人脸识别与年龄估计模型提供的视觉证据，以及基于职业的时间先验知识，以推断照片拍摄年份。实验结果表明，聚合多张面部信息可稳定提升性能，且该方法显著优于现有的强场景基线方法，尤其在包含多个可识别个体的图像中表现突出。",
        "translated_title": "面部年龄聚合的摄影年代测定",
        "label": [],
        "label_reason": "任务为高阶人脸年龄与年代估计，非像素级图像处理。",
        "relevance_score": 1,
        "novelty_score": 6,
        "novelty_reason": "方法组合现有模型，无本质创新突破。"
    },
    {
        "title": "SiamMM: A Mixture Model Perspective on Deep Unsupervised Learning",
        "url": "http://arxiv.org/abs/2511.05462v1",
        "pub_date": "2025-11-07",
        "summary": "Recent studies have demonstrated the effectiveness of clustering-based approaches for self-supervised and unsupervised learning. However, the application of clustering is often heuristic, and the optimal methodology remains unclear. In this work, we establish connections between these unsupervised clustering methods and classical mixture models from statistics. Through this framework, we demonstrate significant enhancements to these clustering methods, leading to the development of a novel model named SiamMM. Our method attains state-of-the-art performance across various self-supervised learning benchmarks. Inspection of the learned clusters reveals a strong resemblance to unseen ground truth labels, uncovering potential instances of mislabeling.",
        "translated": "近期研究表明，基于聚类的方法在自监督和无监督学习中表现出有效性。然而，聚类的应用往往具有启发式性质，其最优方法仍不明确。在本工作中，我们建立了这些无监督聚类方法与统计学中经典混合模型之间的联系。通过该框架，我们显著提升了这些聚类方法的性能，并由此提出了一种新型模型 SiamMM。该方法在多个自监督学习基准测试中取得了当前最优性能。对所学习聚类的检视表明，其与未见的真实标签高度相似，揭示了潜在的标签误标注实例。",
        "translated_title": "SiamMM：深度无监督学习的混合模型视角",
        "label": [],
        "label_reason": "属于无监督聚类，非图像像素级处理",
        "relevance_score": 1,
        "novelty_score": 6,
        "novelty_reason": "改进聚类方法，但非图像恢复任务"
    },
    {
        "title": "The Potential of Copernicus Satellites for Disaster Response: Retrieving\n  Building Damage from Sentinel-1 and Sentinel-2",
        "url": "http://arxiv.org/abs/2511.05461v1",
        "pub_date": "2025-11-07",
        "summary": "Natural disasters demand rapid damage assessment to guide humanitarian response. Here, we investigate whether medium-resolution Earth observation images from the Copernicus program can support building damage assessment, complementing very-high resolution imagery with often limited availability. We introduce xBD-S12, a dataset of 10,315 pre- and post-disaster image pairs from both Sentinel-1 and Sentinel-2, spatially and temporally aligned with the established xBD benchmark. In a series of experiments, we demonstrate that building damage can be detected and mapped rather well in many disaster scenarios, despite the moderate 10$\\,$m ground sampling distance. We also find that, for damage mapping at that resolution, architectural sophistication does not seem to bring much advantage: more complex model architectures tend to struggle with generalization to unseen disasters, and geospatial foundation models bring little practical benefit. Our results suggest that Copernicus images are a viable data source for rapid, wide-area damage assessment and could play an important role alongside VHR imagery. We release the xBD-S12 dataset, code, and trained models to support further research.",
        "translated": "自然灾害需要快速评估灾损，以指导人道主义响应。本文探讨了欧洲哥白尼计划提供的中分辨率地球观测图像是否能够支持建筑物灾损评估，并在高分辨率影像（VHR）供应有限的情况下发挥补充作用。我们引入了xBD-S12数据集，该数据集包含来自Sentinel-1与Sentinel-2的10,315对灾前与灾后图像，其空间与时间对齐于已建立的xBD基准数据集。在一系列实验中，我们证明，尽管地面采样距离仅为10米（中等分辨率），在许多灾害场景下，建筑物灾损仍可被较好地检测与制图。此外，我们发现，在该分辨率下进行灾损制图时，建筑结构的复杂性似乎并未带来显著优势：更复杂的模型架构往往难以泛化至未见过的灾害场景，而地理空间基础模型也未带来实际效益。我们的结果表明，哥白尼图像可作为快速、大范围灾损评估的可行数据来源，并有望与高分辨率影像协同发挥作用。我们公开了xBD-S12数据集、代码及训练好的模型，以支持后续研究。",
        "translated_title": "Copernicus卫星在灾害响应中的潜力：从Sentinel-1与Sentinel-2获取建筑物损毁信息",
        "label": [],
        "label_reason": "使用遥感图像进行灾害评估，非像素级图像处理",
        "relevance_score": 1,
        "novelty_score": 1,
        "novelty_reason": "无图像恢复/增强创新，仅数据集与模型评估"
    },
    {
        "title": "How Many Tokens Do 3D Point Cloud Transformer Architectures Really Need?",
        "url": "http://arxiv.org/abs/2511.05449v1",
        "pub_date": "2025-11-07",
        "summary": "Recent advances in 3D point cloud transformers have led to state-of-the-art results in tasks such as semantic segmentation and reconstruction. However, these models typically rely on dense token representations, incurring high computational and memory costs during training and inference. In this work, we present the finding that tokens are remarkably redundant, leading to substantial inefficiency. We introduce gitmerge3D, a globally informed graph token merging method that can reduce the token count by up to 90-95% while maintaining competitive performance. This finding challenges the prevailing assumption that more tokens inherently yield better performance and highlights that many current models are over-tokenized and under-optimized for scalability. We validate our method across multiple 3D vision tasks and show consistent improvements in computational efficiency. This work is the first to assess redundancy in large-scale 3D transformer models, providing insights into the development of more efficient 3D foundation architectures. Our code and checkpoints are publicly available at https://gitmerge3d.github.io",
        "translated": "近年来，3D点云Transformer的进展已在语义分割与重建等任务中取得了最先进的结果。然而，这些模型通常依赖密集的token表示，在训练和推理过程中导致高昂的计算与内存开销。在本研究中，我们发现token存在显著冗余，从而造成大量效率损失。我们提出了gitmerge3D，一种全局信息引导的图结构token合并方法，可在保持竞争力性能的前提下将token数量减少高达90%-95%。这一发现挑战了当前普遍认为“更多token必然带来更好性能”的假设，并指出许多现有模型存在过度token化、缺乏可扩展性优化的问题。我们在多个3D视觉任务中验证了该方法，并展现出持续的计算效率提升。本工作首次评估了大规模3D Transformer模型中的冗余性，为构建更高效的3D基础架构提供了重要启示。我们的代码与模型检查点已公开于 https://gitmerge3d.github.io",
        "translated_title": "3D点云Transformer架构究竟需要多少个Token？",
        "label": [],
        "label_reason": "处理3D点云，非图像像素级恢复任务",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "首次评估3D Transformer冗余性并提出高效合并方法"
    },
    {
        "title": "Shared Latent Representation for Joint Text-to-Audio-Visual Synthesis",
        "url": "http://arxiv.org/abs/2511.05432v1",
        "pub_date": "2025-11-07",
        "summary": "We propose a text-to-talking-face synthesis framework leveraging latent speech representations from HierSpeech++. A Text-to-Vec module generates Wav2Vec2 embeddings from text, which jointly condition speech and face generation. To handle distribution shifts between clean and TTS-predicted features, we adopt a two-stage training: pretraining on Wav2Vec2 embeddings and finetuning on TTS outputs. This enables tight audio-visual alignment, preserves speaker identity, and produces natural, expressive speech and synchronized facial motion without ground-truth audio at inference. Experiments show that conditioning on TTS-predicted latent features outperforms cascaded pipelines, improving both lip-sync and visual realism.",
        "translated": "我们提出了一种基于 HierSpeech++ 提取的潜在语音表示的文本到说话人脸合成框架。Text-to-Vec 模块从文本生成 Wav2Vec2 嵌入，同时为语音和人脸生成提供联合条件。为应对干净语音特征与 TTS 预测特征之间的分布偏移，我们采用两阶段训练策略：首先在 Wav2Vec2 嵌入上进行预训练，随后在 TTS 输出上进行微调。该方法实现了紧密的视听对齐，保留了说话人身份，并在推理阶段无需真实音频的情况下生成自然、富有表现力的语音及同步的面部动作。实验表明，以 TTS 预测的潜在特征作为条件比级联式流程表现更优，显著提升了唇形同步与视觉真实感。",
        "translated_title": "用于联合文本到音频-视觉合成的共享潜在表示",
        "label": [],
        "label_reason": "生成式任务，非图像像素级恢复",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "新框架提升唇音同步，但非图像处理"
    },
    {
        "title": "Sharing the Learned Knowledge-base to Estimate Convolutional Filter\n  Parameters for Continual Image Restoration",
        "url": "http://arxiv.org/abs/2511.05421v1",
        "pub_date": "2025-11-07",
        "summary": "Continual learning is an emerging topic in the field of deep learning, where a model is expected to learn continuously for new upcoming tasks without forgetting previous experiences. This field has witnessed numerous advancements, but few works have been attempted in the direction of image restoration. Handling large image sizes and the divergent nature of various degradation poses a unique challenge in the restoration domain. However, existing works require heavily engineered architectural modifications for new task adaptation, resulting in significant computational overhead. Regularization-based methods are unsuitable for restoration, as different restoration challenges require different kinds of feature processing. In this direction, we propose a simple modification of the convolution layer to adapt the knowledge from previous restoration tasks without touching the main backbone architecture. Therefore, it can be seamlessly applied to any deep architecture without any structural modifications. Unlike other approaches, we demonstrate that our model can increase the number of trainable parameters without significantly increasing computational overhead or inference time. Experimental validation demonstrates that new restoration tasks can be introduced without compromising the performance of existing tasks. We also show that performance on new restoration tasks improves by adapting the knowledge from the knowledge base created by previous restoration tasks. The code is available at https://github.com/aupendu/continual-restore.",
        "translated": "持续学习是深度学习领域的一个新兴课题，其目标是使模型能够在不遗忘先前经验的前提下，持续学习新的即将出现的任务。该领域已取得诸多进展，但在图像恢复方向上的研究却寥寥无几。处理大尺寸图像以及各类退化现象的异质性，为图像恢复领域带来了独特的挑战。然而，现有方法在适应新任务时往往需要对网络架构进行大量人工设计的修改，导致显著的计算开销。基于正则化的方法在图像恢复任务中并不适用，因为不同的恢复挑战需要不同的特征处理方式。针对这一方向，我们提出对卷积层进行简单修改，以在不改动主干网络架构的前提下，迁移先前恢复任务所获得的知识。因此，该方法可无缝应用于任何深度架构，无需任何结构上的修改。与其它方法不同，我们证明了所提模型在增加可训练参数数量的同时，不会显著增加计算开销或推理时间。实验验证表明，新恢复任务的引入不会损害现有任务的性能。我们还表明，通过从先前恢复任务构建的知识库中迁移知识，新恢复任务的性能得以提升。代码已开源，详见 https://github.com/aupendu/continual-restore。",
        "translated_title": "共享所学知识库以估计卷积滤波器参数用于持续图像恢复",
        "label": [
            "图像恢复"
        ],
        "label_reason": "方法针对图像恢复的持续学习，适配多种退化类型。",
        "relevance_score": 9,
        "novelty_score": 7,
        "novelty_reason": "提出轻量级卷积层修改，实现知识迁移无结构改动。"
    },
    {
        "title": "Multi-modal Loop Closure Detection with Foundation Models in Severely\n  Unstructured Environments",
        "url": "http://arxiv.org/abs/2511.05404v1",
        "pub_date": "2025-11-07",
        "summary": "Robust loop closure detection is a critical component of Simultaneous Localization and Mapping (SLAM) algorithms in GNSS-denied environments, such as in the context of planetary exploration. In these settings, visual place recognition often fails due to aliasing and weak textures, while LiDAR-based methods suffer from sparsity and ambiguity. This paper presents MPRF, a multimodal pipeline that leverages transformer-based foundation models for both vision and LiDAR modalities to achieve robust loop closure in severely unstructured environments. Unlike prior work limited to retrieval, MPRF integrates a two-stage visual retrieval strategy with explicit 6-DoF pose estimation, combining DINOv2 features with SALAD aggregation for efficient candidate screening and SONATA-based LiDAR descriptors for geometric verification. Experiments on the S3LI dataset and S3LI Vulcano dataset show that MPRF outperforms state-of-the-art retrieval methods in precision while enhancing pose estimation robustness in low-texture regions. By providing interpretable correspondences suitable for SLAM back-ends, MPRF achieves a favorable trade-off between accuracy, efficiency, and reliability, demonstrating the potential of foundation models to unify place recognition and pose estimation. Code and models will be released at github.com/DLR-RM/MPRF.",
        "translated": "鲁棒的闭环检测是GNSS拒止环境下同时定位与地图构建（SLAM）算法的关键组成部分，例如在行星探测场景中。在这些场景下，由于存在混叠效应和纹理不足，基于视觉的地点识别往往失效；而基于LiDAR的方法则面临稀疏性和歧义性问题。本文提出MPRF，一种融合视觉与LiDAR模态的多模态处理流程，利用基于Transformer的基座模型实现对严重无结构环境中的鲁棒闭环检测。与以往仅限于检索的方案不同，MPRF结合了一种两阶段视觉检索策略与显式的6自由度位姿估计，通过DINOv2特征与SALAD聚合实现高效候选筛选，并借助SONATA生成的LiDAR描述符进行几何验证。在S3LI数据集和S3LI Vulcano数据集上的实验表明，MPRF在精度上优于现有最先进的检索方法，同时在低纹理区域显著提升了位姿估计的鲁棒性。通过提供适用于SLAM后端的可解释对应关系，MPRF在精度、效率与可靠性之间实现了良好的平衡，展现了基座模型统一地点识别与位姿估计的潜力。代码与模型将在github.com/DLR-RM/MPRF公开。",
        "translated_title": "在严重非结构化环境中的多模态闭环检测与基础模型",
        "label": [],
        "label_reason": "SLAM与多模态定位，非图像像素级处理",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "多模态融合+基础模型，提升定位鲁棒性"
    },
    {
        "title": "PALM: A Dataset and Baseline for Learning Multi-subject Hand Prior",
        "url": "http://arxiv.org/abs/2511.05403v1",
        "pub_date": "2025-11-07",
        "summary": "The ability to grasp objects, signal with gestures, and share emotion through touch all stem from the unique capabilities of human hands. Yet creating high-quality personalized hand avatars from images remains challenging due to complex geometry, appearance, and articulation, particularly under unconstrained lighting and limited views. Progress has also been limited by the lack of datasets that jointly provide accurate 3D geometry, high-resolution multiview imagery, and a diverse population of subjects. To address this, we present PALM, a large-scale dataset comprising 13k high-quality hand scans from 263 subjects and 90k multi-view images, capturing rich variation in skin tone, age, and geometry. To show its utility, we present a baseline PALM-Net, a multi-subject prior over hand geometry and material properties learned via physically based inverse rendering, enabling realistic, relightable single-image hand avatar personalization. PALM's scale and diversity make it a valuable real-world resource for hand modeling and related research.",
        "translated": "抓取物体、通过手势传递信号以及通过触觉分享情感，均源于人类手部的独特能力。然而，由于手部几何结构复杂、外观多样且关节运动丰富，特别是在非约束光照和有限视角条件下，从图像中生成高质量的个性化手部虚拟形象仍面临挑战。此外，缺乏同时提供精确三维几何结构、高分辨率多视角图像以及多样人群样本的数据集，也限制了相关领域的进展。为解决这一问题，我们提出了PALM数据集，该数据集包含263名受试者的13,000个高质量手部扫描数据和90,000张多视角图像，全面捕捉了肤色、年龄和几何结构的丰富变化。为展示其应用价值，我们提出了一种基线模型PALM-Net，该模型通过基于物理的逆渲染方法，学习了跨受试者的手部几何结构与材质属性的多主体先验，从而实现从单张图像中生成真实且可重光照的个性化手部虚拟形象。PALM数据集的规模与多样性，使其成为手部建模及相关研究的重要现实资源。",
        "translated_title": "PALM：用于学习多主体手部先验的基准数据集",
        "label": [],
        "label_reason": "目标为手部建模，非图像像素级恢复",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "提供新数据集与基线模型，但非图像处理创新"
    },
    {
        "title": "EveryDayVLA: A Vision-Language-Action Model for Affordable Robotic\n  Manipulation",
        "url": "http://arxiv.org/abs/2511.05397v1",
        "pub_date": "2025-11-07",
        "summary": "While Vision-Language-Action (VLA) models map visual inputs and language instructions directly to robot actions, they often rely on costly hardware and struggle in novel or cluttered scenes. We introduce EverydayVLA, a 6-DOF manipulator that can be assembled for under $300, capable of modest payloads and workspace. A single unified model jointly outputs discrete and continuous actions, and our adaptive-horizon ensemble monitors motion uncertainty to trigger on-the-fly re-planning for safe, reliable operation. On LIBERO, EverydayVLA matches state-of-the-art success rates, and in real-world tests it outperforms prior methods by 49% in-distribution and 34.9% out-of-distribution. By combining a state-of-the-art VLA with cost-effective hardware, EverydayVLA democratizes access to a robotic foundation model and paves the way for economical use in homes and research labs alike. Experiment videos and details: https://everydayvla.github.io/",
        "translated": "尽管视觉-语言-动作（VLA）模型能够直接将视觉输入与语言指令映射为机器人动作，但它们通常依赖昂贵的硬件，并在新颖或杂乱的场景中表现不佳。我们提出了EverydayVLA，这是一种可组装成本低于300美元的6自由度机械臂，具备适中的负载能力和工作空间。一个统一的模型同时输出离散与连续动作，我们的自适应时域集成模块通过监测运动不确定性，动态触发在线重规划，以确保安全可靠的运行。在LIBERO平台上，EverydayVLA达到当前最优的成功率；在真实世界测试中，其在分布内性能比现有方法提升49%，分布外性能提升34.9%。通过结合前沿VLA模型与低成本硬件，EverydayVLA使机器人基础模型的使用更加普及，为家庭与研究实验室的经济化应用铺平道路。实验视频与详细信息：https://everydayvla.github.io/",
        "translated_title": "EveryDayVLA：一种面向经济型机器人操作的视觉-语言-动作模型",
        "label": [],
        "label_reason": "属于机器人控制与视觉语言模型，非图像像素级处理",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "提出低成本硬件+统一模型架构，提升机器人操作鲁棒性"
    },
    {
        "title": "AI Assisted AR Assembly: Object Recognition and Computer Vision for\n  Augmented Reality Assisted Assembly",
        "url": "http://arxiv.org/abs/2511.05394v1",
        "pub_date": "2025-11-07",
        "summary": "We present an AI-assisted Augmented Reality assembly workflow that uses deep learning-based object recognition to identify different assembly components and display step-by-step instructions. For each assembly step, the system displays a bounding box around the corresponding components in the physical space, and where the component should be placed. By connecting assembly instructions with the real-time location of relevant components, the system eliminates the need for manual searching, sorting, or labeling of different components before each assembly. To demonstrate the feasibility of using object recognition for AR-assisted assembly, we highlight a case study involving the assembly of LEGO sculptures.",
        "translated": "我们提出了一种由人工智能辅助的增强现实（AR）装配工作流程，该流程利用基于深度学习的目标识别技术识别不同的装配部件，并逐步骤显示装配说明。对于每个装配步骤，系统将在物理空间中对应部件周围显示边界框，并标示该部件应放置的位置。通过将装配说明与相关部件的实时位置关联，系统消除了在每次装配前手动搜索、分类或标记不同部件的必要性。为验证利用目标识别技术实现AR辅助装配的可行性，我们以乐高雕塑的装配为例进行了案例研究。",
        "translated_title": "AI 辅助增强现实装配：面向增强现实装配的物体识别与计算机视觉",
        "label": [],
        "label_reason": "高阶视觉任务，非像素级图像处理",
        "relevance_score": 1,
        "novelty_score": 1,
        "novelty_reason": "无图像恢复创新，属AR应用系统"
    },
    {
        "title": "PreResQ-R1: Towards Fine-Grained Rank-and-Score Reinforcement Learning\n  for Visual Quality Assessment via Preference-Response Disentangled Policy\n  Optimization",
        "url": "http://arxiv.org/abs/2511.05393v1",
        "pub_date": "2025-11-07",
        "summary": "Visual Quality Assessment (QA) seeks to predict human perceptual judgments of visual fidelity. While recent multimodal large language models (MLLMs) show promise in reasoning about image and video quality, existing approaches mainly rely on supervised fine-tuning or rank-only objectives, resulting in shallow reasoning, poor score calibration, and limited cross-domain generalization. We propose PreResQ-R1, a Preference-Response Disentangled Reinforcement Learning framework that unifies absolute score regression and relative ranking consistency within a single reasoning-driven optimization scheme. Unlike prior QA methods, PreResQ-R1 introduces a dual-branch reward formulation that separately models intra-sample response coherence and inter-sample preference alignment, optimized via Group Relative Policy Optimization (GRPO). This design encourages fine-grained, stable, and interpretable chain-of-thought reasoning about perceptual quality. To extend beyond static imagery, we further design a global-temporal and local-spatial data flow strategy for Video Quality Assessment. Remarkably, with reinforcement fine-tuning on only 6K images and 28K videos, PreResQ-R1 achieves state-of-the-art results across 10 IQA and 5 VQA benchmarks under both SRCC and PLCC metrics, surpassing by margins of 5.30% and textbf2.15% in IQA task, respectively. Beyond quantitative gains, it produces human-aligned reasoning traces that reveal the perceptual cues underlying quality judgments. Code and model are available.",
        "translated": "视觉质量评估（QA）旨在预测人类对视觉保真度的感知判断。尽管近期多模态大语言模型（MLLMs）在图像与视频质量推理方面展现出潜力，但现有方法主要依赖监督微调或仅排名目标，导致推理浅层、评分校准不佳，且跨域泛化能力有限。我们提出 PreResQ-R1，一种偏好-响应解耦的强化学习框架，将绝对评分回归与相对排序一致性统一于单一的推理驱动优化方案中。与以往 QA 方法不同，PreResQ-R1 引入双分支奖励公式，分别建模样本内响应一致性与样本间偏好对齐，通过组相对策略优化（GRPO）进行优化。该设计鼓励对感知质量进行细粒度、稳定且可解释的链式推理。为拓展至动态视频场景，我们进一步设计了全局时序与局部空间的数据流策略，用于视频质量评估。值得注意的是，仅在 6K 张图像与 28K 个视频上进行强化微调，PreResQ-R1 在 10 个 IQA 与 5 个 VQA 基准测试中均取得当前最优性能，在 SRCC 与 PLCC 两个指标下分别超越现有方法 5.30% 与 2.15%。除量化性能提升外，其生成的人类对齐推理轨迹揭示了质量判断背后的感知线索。代码与模型已公开。",
        "translated_title": "PreResQ-R1：面向细粒度排序与评分强化学习的视觉质量评估方法——基于偏好-响应解耦策略优化",
        "label": [],
        "label_reason": "评估视觉质量，非图像像素级恢复任务",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "提出强化学习框架优化质量评估推理"
    },
    {
        "title": "Dense Motion Captioning",
        "url": "http://arxiv.org/abs/2511.05369v1",
        "pub_date": "2025-11-07",
        "summary": "Recent advances in 3D human motion and language integration have primarily focused on text-to-motion generation, leaving the task of motion understanding relatively unexplored. We introduce Dense Motion Captioning, a novel task that aims to temporally localize and caption actions within 3D human motion sequences. Current datasets fall short in providing detailed temporal annotations and predominantly consist of short sequences featuring few actions. To overcome these limitations, we present the Complex Motion Dataset (CompMo), the first large-scale dataset featuring richly annotated, complex motion sequences with precise temporal boundaries. Built through a carefully designed data generation pipeline, CompMo includes 60,000 motion sequences, each composed of multiple actions ranging from at least two to ten, accurately annotated with their temporal extents. We further present DEMO, a model that integrates a large language model with a simple motion adapter, trained to generate dense, temporally grounded captions. Our experiments show that DEMO substantially outperforms existing methods on CompMo as well as on adapted benchmarks, establishing a robust baseline for future research in 3D motion understanding and captioning.",
        "translated": "近年来，三维人体运动与语言融合领域的研究主要聚焦于文本到运动的生成，而运动理解任务则相对未被充分探索。我们提出了密集运动字幕生成（Dense Motion Captioning），这是一个旨在对三维人体运动序列中的动作进行时间定位并生成字幕的新型任务。当前数据集在提供详细的时间标注方面存在不足，且主要由包含少量动作的短序列构成。为克服这些局限，我们提出了复杂运动数据集（CompMo），这是首个包含丰富标注、复杂运动序列且具有精确时间边界的大型数据集。CompMo 通过精心设计的数据生成流程构建，包含 60,000 个运动序列，每个序列包含至少两个至十个动作，并精确标注了其时间范围。我们进一步提出了 DEMO 模型，该模型将大型语言模型与一个简单的运动适配器相结合，训练用于生成密集且时间对齐的字幕。实验表明，DEMO 在 CompMo 数据集以及适配的基准测试中显著优于现有方法，为未来三维运动理解与字幕生成研究奠定了稳健的基线。",
        "translated_title": "密集运动字幕生成",
        "label": [],
        "label_reason": "任务属3D动作理解，非图像像素级处理",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出新任务与数据集，模型结构较新颖"
    },
    {
        "title": "Neural Image Abstraction Using Long Smoothing B-Splines",
        "url": "http://arxiv.org/abs/2511.05360v1",
        "pub_date": "2025-11-07",
        "summary": "We integrate smoothing B-splines into a standard differentiable vector graphics (DiffVG) pipeline through linear mapping, and show how this can be used to generate smooth and arbitrarily long paths within image-based deep learning systems. We take advantage of derivative-based smoothing costs for parametric control of fidelity vs. simplicity tradeoffs, while also enabling stylization control in geometric and image spaces. The proposed pipeline is compatible with recent vector graphics generation and vectorization methods. We demonstrate the versatility of our approach with four applications aimed at the generation of stylized vector graphics: stylized space-filling path generation, stroke-based image abstraction, closed-area image abstraction, and stylized text generation.",
        "translated": "我们通过线性映射将平滑B样条整合进标准的可微分矢量图形（DiffVG）管线中，并展示如何利用该方法在基于图像的深度学习系统中生成平滑且任意长度的路径。我们利用基于导数的平滑代价函数，实现对保真度与简洁性之间权衡的参数化控制，同时支持在几何空间与图像空间中的风格化控制。所提出的管线与近期的矢量图形生成及矢量化方法兼容。我们通过四个应用场景展示了该方法的通用性：风格化空间填充路径生成、基于笔画的图像抽象、闭合区域图像抽象以及风格化文本生成。",
        "translated_title": "基于长平滑B样条的神经图像抽象",
        "label": [],
        "label_reason": "生成矢量图形，非像素级图像恢复或增强",
        "relevance_score": 2,
        "novelty_score": 5,
        "novelty_reason": "组合B样条与DiffVG，属图形生成非图像复原"
    },
    {
        "title": "Canonical Space Representation for 4D Panoptic Segmentation of\n  Articulated Objects",
        "url": "http://arxiv.org/abs/2511.05356v1",
        "pub_date": "2025-11-07",
        "summary": "Articulated object perception presents significant challenges in computer vision, particularly because most existing methods ignore temporal dynamics despite the inherently dynamic nature of such objects. The use of 4D temporal data has not been thoroughly explored in articulated object perception and remains unexamined for panoptic segmentation. The lack of a benchmark dataset further hurt this field. To this end, we introduce Artic4D as a new dataset derived from PartNet Mobility and augmented with synthetic sensor data, featuring 4D panoptic annotations and articulation parameters. Building on this dataset, we propose CanonSeg4D, a novel 4D panoptic segmentation framework. This approach explicitly estimates per-frame offsets mapping observed object parts to a learned canonical space, thereby enhancing part-level segmentation. The framework employs this canonical representation to achieve consistent alignment of object parts across sequential frames. Comprehensive experiments on Artic4D demonstrate that the proposed CanonSeg4D outperforms state of the art approaches in panoptic segmentation accuracy in more complex scenarios. These findings highlight the effectiveness of temporal modeling and canonical alignment in dynamic object understanding, and pave the way for future advances in 4D articulated object perception.",
        "translated": " articulated object perception 在计算机视觉中面临显著挑战，尤其因为现有大多数方法忽略了时间动态性，而这类对象本质上具有动态特性。在 articulated object perception 领域，4D 时间数据的使用尚未被充分探索，且在全景分割任务中更是未被研究。缺乏基准数据集进一步阻碍了该领域的发展。为此，我们引入 Artic4D 数据集，该数据集基于 PartNet Mobility 并辅以合成传感器数据，具备 4D 全景标注和运动参数。在此基础上，我们提出 CanonSeg4D，一种新颖的 4D 全景分割框架。该方法显式估计每帧的偏移量，将观测到的对象部件映射至学习到的规范空间，从而提升部件级分割性能。该框架利用规范表示，实现连续帧间对象部件的一致对齐。在 Artic4D 数据集上的全面实验表明，所提出的 CanonSeg4D 在更复杂场景下显著优于现有最先进方法的全景分割精度。这些结果凸显了时间建模与规范对齐在动态对象理解中的有效性，并为未来 4D articulated object perception 的发展铺平道路。",
        "translated_title": "用于 articulated 对象 4D 全景分割的规范空间表示",
        "label": [],
        "label_reason": "任务为4D语义分割，属高阶视觉任务",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "提出新框架与数据集，显著提升分割性能"
    },
    {
        "title": "TeaRAG: A Token-Efficient Agentic Retrieval-Augmented Generation\n  Framework",
        "url": "http://arxiv.org/abs/2511.05385v1",
        "pub_date": "2025-11-07",
        "summary": "Retrieval-Augmented Generation (RAG) utilizes external knowledge to augment Large Language Models' (LLMs) reliability. For flexibility, agentic RAG employs autonomous, multi-round retrieval and reasoning to resolve queries. Although recent agentic RAG has improved via reinforcement learning, they often incur substantial token overhead from search and reasoning processes. This trade-off prioritizes accuracy over efficiency. To address this issue, this work proposes TeaRAG, a token-efficient agentic RAG framework capable of compressing both retrieval content and reasoning steps. 1) First, the retrieved content is compressed by augmenting chunk-based semantic retrieval with a graph retrieval using concise triplets. A knowledge association graph is then built from semantic similarity and co-occurrence. Finally, Personalized PageRank is leveraged to highlight key knowledge within this graph, reducing the number of tokens per retrieval. 2) Besides, to reduce reasoning steps, Iterative Process-aware Direct Preference Optimization (IP-DPO) is proposed. Specifically, our reward function evaluates the knowledge sufficiency by a knowledge matching mechanism, while penalizing excessive reasoning steps. This design can produce high-quality preference-pair datasets, supporting iterative DPO to improve reasoning conciseness. Across six datasets, TeaRAG improves the average Exact Match by 4% and 2% while reducing output tokens by 61% and 59% on Llama3-8B-Instruct and Qwen2.5-14B-Instruct, respectively. Code is available at https://github.com/Applied-Machine-Learning-Lab/TeaRAG.",
        "translated": "检索增强生成（RAG）利用外部知识提升大语言模型（LLMs）的可靠性。为增强灵活性，代理式RAG采用自主、多轮检索与推理机制以解决查询问题。尽管近期代理式RAG通过强化学习已取得性能提升，但其往往因搜索与推理过程而产生显著的token开销。此权衡倾向于优先保证准确性而非效率。为解决该问题，本文提出TeaRAG，一种高效token消耗的代理式RAG框架，能够压缩检索内容与推理步骤。1）首先，通过将基于块的语义检索与使用简洁三元组的图检索相结合，对检索内容进行压缩；随后，依据语义相似性与共现关系构建知识关联图；最后，利用个性化PageRank算法突出图中关键知识，从而降低每次检索的token数量。2）此外，为减少推理步骤，本文提出迭代式过程感知直接偏好优化（IP-DPO）。具体而言，奖励函数通过知识匹配机制评估知识充分性，并惩罚过度冗余的推理步骤。该设计可生成高质量偏好对数据集，支持迭代DPO以提升推理简洁性。在六个数据集上，TeaRAG在Llama3-8B-Instruct和Qwen2.5-14B-Instruct模型上分别将平均精确匹配率提升4%与2%，同时输出token量分别减少61%与59%。代码开源于https://github.com/Applied-Machine-Learning-Lab/TeaRAG。",
        "translated_title": "TeaRAG：一种高效的代理式检索增强生成框架",
        "label": [],
        "label_reason": "聚焦RAG效率优化，非推荐系统核心问题",
        "relevance_score": 3,
        "novelty_score": 7,
        "novelty_reason": "创新压缩机制与DPO策略提升推理效率"
    },
    {
        "title": "QUESTER: Query Specification for Generative Retrieval",
        "url": "http://arxiv.org/abs/2511.05301v1",
        "pub_date": "2025-11-07",
        "summary": "Generative Retrieval (GR) differs from the traditional index-then-retrieve pipeline by storing relevance in model parameters and directly generating document identifiers. However, GR often struggles to generalize and is costly to scale. We introduce QUESTER (QUEry SpecificaTion gEnerative Retrieval), which reframes GR as query specification generation - in this work, a simple keyword query handled by BM25 - using a (small) LLM. The policy is trained using reinforcement learning techniques (GRPO). Across in- and out-of-domain evaluations, we show that our model is more effective than BM25, and competitive with neural IR models, while maintaining a good efficiency",
        "translated": "生成式检索（Generative Retrieval, GR）与传统的“索引后检索”流程不同，其通过将相关性存储在模型参数中，并直接生成文档标识符。然而，GR往往难以泛化且扩展成本高昂。我们提出QUESTER（QUEry SpecificaTion gEnerative Retrieval），将GR重新定义为查询规范生成问题——在此工作中，使用一个小规模大语言模型（LLM）处理简单的关键词查询（由BM25处理）。该策略采用强化学习技术（GRPO）进行训练。在领域内和跨领域的评估中，我们证明了我们的模型比BM25更有效，且与神经信息检索模型具有竞争力，同时保持良好的效率。",
        "translated_title": "QUESTER：生成式检索的查询规范",
        "label": [
            "LLM生成式推荐",
            "通用推荐技术"
        ],
        "label_reason": "将查询规格化为LLM生成，适配推荐召回环节",
        "relevance_score": 7,
        "novelty_score": 8,
        "novelty_reason": "首次用RL训练LLM生成检索查询，提升泛化与效率"
    },
    {
        "title": "Mapping Research Productivity of BRICS Countries with Special Reference\n  to Coronary Artery Disease (CAD): A Scientometric Study",
        "url": "http://arxiv.org/abs/2511.05211v1",
        "pub_date": "2025-11-07",
        "summary": "This study presents a comprehensive scientometric analysis of research productivity on Coronary Artery Disease (CAD) among the BRICS countries, Brazil, Russia, India, China, and South Africa, using data retrieved from the Web of Science database for the period 1990 to 2019. A total of 50,036 records were analyzed to assess publication growth trends, authorship patterns, collaboration levels, and citation impact. The findings reveal a steady increase in CAD-related publications, with China emerging as the leading contributor, followed by Brazil, Russia, India, and South Africa. English dominated as the primary language of communication, accounting for over 93% of publications. Authorship and collaboration analysis indicate a high degree of joint research, with 97.91% of studies being co-authored and a degree of collaboration of 0.98, underscoring the collective nature of scientific inquiry in this domain. The study validates the applicability of Lotkas Law for author productivity, Bradfords Law for journal distribution, and Zipfs Law for keyword frequency, while the Price Square Root Law was found inapplicable. The predominant publication format was journal articles (79.7%), and Kardiologiya (Russia) emerged as the most prolific journal. The results demonstrate significant growth in CAD research output and collaboration within BRICS, though notable disparities persist among member nations. The study recommends enhancing individual author productivity, expanding international collaboration, and supporting CAD research through strategic institutional and governmental initiatives. These findings provide valuable insights for policymakers, funding agencies, and the academic community to strengthen cardiovascular research capacity within developing economies.",
        "translated": "本研究利用Web of Science数据库中1990至2019年间的数据，对金砖五国（巴西、俄罗斯、印度、中国和南非）在冠状动脉疾病（CAD）领域的科研生产力进行了全面的科学计量学分析。共分析了50,036条记录，旨在评估出版物增长趋势、作者署名模式、合作水平及引用影响力。研究结果表明，与CAD相关的出版物数量稳步上升，其中中国为最大贡献国，其次是巴西、俄罗斯、印度和南非。英语作为主要交流语言，占比超过93%。作者署名与合作分析显示，联合研究比例极高，97.91%的研究为多作者合作完成，合作度达0.98，凸显该领域科学研究的高度协作性。本研究验证了洛特卡定律适用于作者生产力分布、布拉德福定律适用于期刊分布、齐夫定律适用于关键词频率分布，而普赖斯平方根定律不适用。期刊论文为主要出版形式（占79.7%），《Kardiologiya》（俄罗斯）成为发文量最多的期刊。研究结果表明，金砖国家在CAD研究产出与合作方面呈现显著增长，但成员国间仍存在明显差异。本研究建议通过提升个体作者生产力、扩大国际合作以及推动机构与政府的战略支持，强化CAD研究能力。上述发现为政策制定者、资助机构及学术界加强发展中国家心血管研究能力建设提供了重要参考。",
        "translated_title": "BRICS国家科研生产力映射研究——以冠状动脉疾病（CAD）为特别参考：一项科学计量学研究",
        "label": [],
        "label_reason": "论文聚焦科研计量学，与推荐系统无关。",
        "relevance_score": 1,
        "novelty_score": 1,
        "novelty_reason": "无推荐系统相关创新内容。"
    },
    {
        "title": "Wikipedia-based Datasets in Russian Information Retrieval Benchmark\n  RusBEIR",
        "url": "http://arxiv.org/abs/2511.05079v1",
        "pub_date": "2025-11-07",
        "summary": "In this paper, we present a novel series of Russian information retrieval datasets constructed from the \"Did you know...\" section of Russian Wikipedia. Our datasets support a range of retrieval tasks, including fact-checking, retrieval-augmented generation, and full-document retrieval, by leveraging interesting facts and their referenced Wikipedia articles annotated at the sentence level with graded relevance. We describe the methodology for dataset creation that enables the expansion of existing Russian Information Retrieval (IR) resources. Through extensive experiments, we extend the RusBEIR research by comparing lexical retrieval models, such as BM25, with state-of-the-art neural architectures fine-tuned for Russian, as well as multilingual models. Results of our experiments show that lexical methods tend to outperform neural models on full-document retrieval, while neural approaches better capture lexical semantics in shorter texts, such as in fact-checking or fine-grained retrieval. Using our newly created datasets, we also analyze the impact of document length on retrieval performance and demonstrate that combining retrieval with neural reranking consistently improves results. Our contribution expands the resources available for Russian information retrieval research and highlights the importance of accurate evaluation of retrieval models to achieve optimal performance. All datasets are publicly available at HuggingFace. To facilitate reproducibility and future research, we also release the full implementation on GitHub.",
        "translated": "本文提出了一个全新的系列俄罗斯信息检索数据集，该数据集来源于俄语维基百科的“你知道吗……”板块。我们的数据集支持多种检索任务，包括事实核查、检索增强生成与全文文档检索，其核心在于利用了句子级标注的分级相关性注释的有趣事实及其引用的维基百科文章。我们详细描述了数据集构建的方法论，旨在扩展现有的俄语信息检索（IR）资源。通过大量实验，我们在RusBEIR研究基础上比较了词典检索模型（如BM25）与针对俄语微调的先进神经架构以及多语言模型的表现。实验结果表明，在全文文档检索任务上，词典方法往往优于神经模型；而在较短文本（如事实核查或细粒度检索）中，神经方法更擅长捕捉词汇语义。借助新构建的数据集，我们还分析了文档长度对检索性能的影响，并证明结合检索与神经重排可持续提升检索效果。本研究贡献在于扩充了俄语信息检索领域的可用资源，并强调准确评估检索模型对于实现最优性能的重要性。所有数据集均在HuggingFace平台公开获取。为促进复现与未来研究，我们亦在GitHub上开源了完整实现代码。",
        "translated_title": "基于维基百科的俄语信息检索基准数据集  \nRusBEIR",
        "label": [],
        "label_reason": "聚焦信息检索，非推荐系统核心问题",
        "relevance_score": 3,
        "novelty_score": 5,
        "novelty_reason": "数据集构建方法常规，无推荐系统创新"
    },
    {
        "title": "The use of social media among library professionals and patrons: A\n  review of literature",
        "url": "http://arxiv.org/abs/2511.05051v1",
        "pub_date": "2025-11-07",
        "summary": "This paper focused on the utilization of social media by library professionals and library users. It provides an understanding of social media, the most popular social media platforms utilized in the libraries. It also mentions the reasons for the adoption of social media in libraries be it academic, public, school libraries and other types of libraries. This is a review paper on the use of social media among library professionals and patrons. The findings reveal the contributions of social media to the libraries. Social media makes things easy for library professionals and library users. It enables them to connect, create awareness to new information, disseminate information instantly, and helps to market the library resources and services. Therefore, it is recommended amongst others that the library management board should encourage the use of social media in libraries.",
        "translated": "本文聚焦于图书馆专业人员及图书馆用户对社交媒体的使用情况。文章阐述了社交媒体的基本概念，以及当前在各类图书馆中最常使用的主流社交平台。同时，文中还提及了学术图书馆、公共图书馆、学校图书馆及其他类型图书馆采用社交媒体的原因。本研究系一篇关于图书馆专业人员与读者使用社交媒体情况的综述论文。研究结果揭示了社交媒体对图书馆工作的诸多贡献：它为图书馆专业人员与用户提供了便利，使其能够建立联系、及时传播新资讯、迅速扩散信息，并有助于推广图书馆资源与服务。因此，建议图书馆管理层应积极鼓励在图书馆中应用社交媒体。",
        "translated_title": "图书馆专业人员与读者使用社交媒体的情况：文献综述",
        "label": [],
        "label_reason": "论文讨论图书馆社交媒体应用，与推荐系统无关。",
        "relevance_score": 1,
        "novelty_score": 1,
        "novelty_reason": "无创新，仅为文献综述，未涉及推荐算法。"
    },
    {
        "title": "Query Generation Pipeline with Enhanced Answerability Assessment for\n  Financial Information Retrieval",
        "url": "http://arxiv.org/abs/2511.05000v1",
        "pub_date": "2025-11-07",
        "summary": "As financial applications of large language models (LLMs) gain attention, accurate Information Retrieval (IR) remains crucial for reliable AI services. However, existing benchmarks fail to capture the complex and domain-specific information needs of real-world banking scenarios. Building domain-specific IR benchmarks is costly and constrained by legal restrictions on using real customer data. To address these challenges, we propose a systematic methodology for constructing domain-specific IR benchmarks through LLM-based query generation. As a concrete implementation of this methodology, our pipeline combines single and multi-document query generation with an enhanced and reasoning-augmented answerability assessment method, achieving stronger alignment with human judgments than prior approaches. Using this methodology, we construct KoBankIR, comprising 815 queries derived from 204 official banking documents. Our experiments show that existing retrieval models struggle with the complex multi-document queries in KoBankIR, demonstrating the value of our systematic approach for domain-specific benchmark construction and underscoring the need for improved retrieval techniques in financial domains.",
        "translated": "随着大语言模型（LLMs）在金融领域的应用日益受到关注，准确的信息检索（IR）对于可靠的人工智能服务仍至关重要。然而，现有评测基准未能充分捕捉现实银行场景中复杂且领域特定的信息需求。构建领域专用的IR评测基准成本高昂，并受限于使用真实客户数据时所面临的法律约束。为应对这些挑战，我们提出一种基于LLM的查询生成方法，系统性地构建领域专用的IR评测基准。作为该方法的具体实现，我们的流程结合了单文档与多文档查询生成，并采用增强型、推理增强的答案可答性评估方法，相较于以往方法更贴近人类判断。基于此方法，我们构建了KoBankIR评测集，其包含815个从204份官方银行业文件中衍生出的查询。实验表明，现有检索模型在KoBankIR中的复杂多文档查询上表现不佳，凸显了我们系统化方法在构建领域专用评测基准方面的价值，并强调了金融领域亟需改进的检索技术。",
        "translated_title": "增强答案可答性评估的金融信息检索查询生成流程",
        "label": [],
        "label_reason": "聚焦金融信息检索，非推荐系统核心问题",
        "relevance_score": 3,
        "novelty_score": 5,
        "novelty_reason": "方法用于构建领域基准，非推荐算法创新"
    },
    {
        "title": "Search Is Not Retrieval: Decoupling Semantic Matching from Contextual\n  Assembly in RAG",
        "url": "http://arxiv.org/abs/2511.04939v1",
        "pub_date": "2025-11-07",
        "summary": "Retrieval systems are essential to contemporary AI pipelines, although most confuse two separate processes: finding relevant information and giving enough context for reasoning. We introduce the Search-Is-Not-Retrieve (SINR) framework, a dual-layer architecture that distinguishes between fine-grained search representations and coarse-grained retrieval contexts. SINR enhances the composability, scalability, and context fidelity of retrieval systems by directly connecting small, semantically accurate search chunks to larger, contextually complete retrieve chunks, all without incurring extra processing costs. This design changes retrieval from a passive step to an active one, making the system architecture more like how people process information. We discuss the SINR framework's conceptual foundation, formal structure, implementation issues, and qualitative outcomes. This provides a practical foundation for the next generation of AI systems that use retrieval.",
        "translated": "检索系统是现代人工智能流水线的核心组成部分，尽管多数系统将两个独立过程混淆：寻找相关信息与提供足够的上下文以支持推理。我们提出了Search-Is-Not-Retrieve（SINR）框架，这是一种双层架构，明确区分细粒度搜索表示与粗粒度检索上下文。SINR通过直接连接语义精确的小型搜索片段与上下文完整的大型检索片段，在不增加额外计算开销的前提下，提升了检索系统的组合性、可扩展性及上下文保真度。该设计将检索从被动步骤转变为积极主动的过程，使系统架构更贴近人类处理信息的方式。我们讨论了SINR框架的概念基础、形式结构、实现问题及其定性结果，为下一代采用检索技术的AI系统提供了切实可行的基础。",
        "translated_title": "检索并非召回：在RAG中解耦语义匹配与上下文组装",
        "label": [],
        "label_reason": "聚焦RAG检索优化，非推荐系统核心环节",
        "relevance_score": 3,
        "novelty_score": 6,
        "novelty_reason": "架构创新但未针对推荐场景设计"
    },
    {
        "title": "Association via Entropy Reduction",
        "url": "http://arxiv.org/abs/2511.04901v1",
        "pub_date": "2025-11-07",
        "summary": "Prior to recent successes using neural networks, term frequency-inverse document frequency (tf-idf) was clearly regarded as the best choice for identifying documents related to a query. We provide a different score, aver, and observe, on a dataset with ground truth marking for association, that aver does do better at finding assciated pairs than tf-idf. This example involves finding associated vertices in a large graph and that may be an area where neural networks are not currently an obvious best choice. Beyond this one anecdote, we observe that (1) aver has a natural threshold for declaring pairs as unassociated while tf-idf does not, (2) aver can distinguish between pairs of documents for which tf-idf gives a score of 1.0, (3) aver can be applied to larger collections of documents than pairs while tf-idf cannot, and (4) that aver is derived from entropy under a simple statistical model while tf-idf is a construction designed to achieve a certain goal and hence aver may be more \"natural.\" To be fair, we also observe that (1) writing down and computing the aver score for a pair is more complex than for tf-idf and (2) that the fact that the aver score is naturally scale-free makes it more complicated to interpret aver scores.",
        "translated": "在近期神经网络取得成功之前，词频-逆文档频率（tf-idf）被明确视为识别与查询相关文档的最佳选择。我们提出了一种不同的评分方法 aver，并在具有真实关联标注的数据集上观察到：aver 在发现关联对时的表现优于 tf-idf。此例涉及在一个大型图中查找关联节点，这可能是当前神经网络并非明显最优选择的领域之一。除这一具体案例外，我们还观察到：（1）aver 具有自然的阈值用于判定一对实体是否不相关，而 tf-idf 不具备此类特性；（2）aver 能够区分 tf-idf 得分为 1.0 的文档对；（3）aver 可应用于比成对形式更大的文档集合，而 tf-idf 无法做到这一点；（4）aver 是基于一个简单统计模型下的熵推导而来，而 tf-idf 是为实现特定目标而设计的构造，因此 aver 可能更具“自然性”。需指出的是，我们也观察到：（1）对于一对文档，写出并计算 aver 分数比 tf-idf 更复杂；（2）由于 aver 分数天然无量纲，其解释更为复杂。",
        "translated_title": "通过熵减实现关联",
        "label": [],
        "label_reason": "论文聚焦信息检索中的关联度计算，非推荐系统核心问题。",
        "relevance_score": 2,
        "novelty_score": 4,
        "novelty_reason": "提出新评分函数aver，但无推荐场景应用或创新。"
    },
    {
        "title": "EMO100DB: An Open Dataset of Improvised Songs with Emotion Data",
        "url": "http://arxiv.org/abs/2511.04755v1",
        "pub_date": "2025-11-06",
        "summary": "In this study, we introduce Emo100DB: a dataset consisting of improvised songs that were recorded and transcribed with emotion data based on Russell's circumplex model of emotion. The dataset was developed by collecting improvised songs that consist of melody, lyrics, and an instrumental accompaniment played, sung, and recorded by 20 young adults. Before recording each song, the participants were asked to report their emotional state, with the axes representing arousal and valence based on Russell's circumplex model of emotions. The dataset is organized into four emotion quadrants, and it includes the lyrics text and MIDI file of the melody extracted from the participant recordings, along with the original audio in WAV format. By providing an integrated composition of data and analysis, this study aims to offer a comprehensive dataset that allows for a diverse exploration of the relationship between music and emotion.",
        "translated": "在本研究中，我们引入了Emo100DB数据集：该数据集包含基于Russell情绪环状模型标注情绪数据的即兴歌曲录音与转录结果。该数据集由20位年轻成人演奏、演唱并录制的即兴歌曲组成，每首歌曲包含旋律、歌词及伴奏部分。在录制每首歌曲前，参与者需报告其情绪状态，情绪维度依据Russell情绪环状模型中的唤醒度（arousal）与效价（valence）轴进行标注。数据集按四个情绪象限组织，并包含从参与者录音中提取的歌词文本及MIDI格式的旋律文件，以及原始WAV音频文件。通过提供数据与分析的集成化呈现，本研究旨在构建一个全面的数据集，以支持对音乐与情绪关系多样化的深入探索。",
        "translated_title": "EMO100DB：一个包含情绪数据的即兴歌曲公开数据集",
        "label": [],
        "label_reason": "音乐情感数据集，与推荐系统无直接关联",
        "relevance_score": 2,
        "novelty_score": 3,
        "novelty_reason": "常规数据构建，无推荐系统创新"
    },
    {
        "title": "Lightning Grasp: High Performance Procedural Grasp Synthesis with\n  Contact Fields",
        "url": "http://arxiv.org/abs/2511.07418v1",
        "pub_date": "2025-11-10",
        "summary": "Despite years of research, real-time diverse grasp synthesis for dexterous hands remains an unsolved core challenge in robotics and computer graphics. We present Lightning Grasp, a novel high-performance procedural grasp synthesis algorithm that achieves orders-of-magnitude speedups over state-of-the-art approaches, while enabling unsupervised grasp generation for irregular, tool-like objects. The method avoids many limitations of prior approaches, such as the need for carefully tuned energy functions and sensitive initialization. This breakthrough is driven by a key insight: decoupling complex geometric computation from the search process via a simple, efficient data structure - the Contact Field. This abstraction collapses the problem complexity, enabling a procedural search at unprecedented speeds. We open-source our system to propel further innovation in robotic manipulation.",
        "translated": "尽管已有多年研究，灵巧手的实时多样化抓取合成仍是在机器人学和计算机图形学领域未解决的核心挑战。我们提出了Lightning Grasp，一种新型高性能的程序化抓取合成算法，其速度比当前最先进的方法快几个数量级，同时支持对不规则、工具类物体的无监督抓取生成。该方法规避了先前方法的诸多限制，例如对精心调优的能量函数的需求以及对敏感初始化的依赖。这一突破源于一个关键洞见：通过一种简单高效的结构——接触场（Contact Field），将复杂的几何计算与搜索过程解耦。该抽象降低了问题复杂度，使程序化搜索得以以前所未有的速度实现。我们开源了本系统，以推动机器人操作领域的进一步创新。",
        "translated_title": "闪电抓取：基于接触场的高性能过程化抓取合成",
        "label": [],
        "label_reason": "目标为机器人抓取合成，非图像像素级处理",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出接触场结构加速抓取生成，属高阶任务"
    },
    {
        "title": "Robot Learning from a Physical World Model",
        "url": "http://arxiv.org/abs/2511.07416v1",
        "pub_date": "2025-11-10",
        "summary": "We introduce PhysWorld, a framework that enables robot learning from video generation through physical world modeling. Recent video generation models can synthesize photorealistic visual demonstrations from language commands and images, offering a powerful yet underexplored source of training signals for robotics. However, directly retargeting pixel motions from generated videos to robots neglects physics, often resulting in inaccurate manipulations. PhysWorld addresses this limitation by coupling video generation with physical world reconstruction. Given a single image and a task command, our method generates task-conditioned videos and reconstructs the underlying physical world from the videos, and the generated video motions are grounded into physically accurate actions through object-centric residual reinforcement learning with the physical world model. This synergy transforms implicit visual guidance into physically executable robotic trajectories, eliminating the need for real robot data collection and enabling zero-shot generalizable robotic manipulation. Experiments on diverse real-world tasks demonstrate that PhysWorld substantially improves manipulation accuracy compared to previous approaches. Visit \\href{https://pointscoder.github.io/PhysWorld_Web/}{the project webpage} for details.",
        "translated": "我们提出了 PhysWorld，一个通过物理世界建模实现机器人从视频生成中学习的框架。近期的视频生成模型能够根据语言指令和图像合成逼真的视觉演示，为机器人学习提供了一种强大但尚未充分挖掘的训练信号来源。然而，直接将生成视频中的像素运动映射到机器人上忽略了物理规律，常导致操作不准确。PhysWorld 通过将视频生成与物理世界重建相结合，解决了这一局限性。给定一张图像和一个任务指令，我们的方法会生成条件于任务的视频，并从视频中重建其背后的物理世界；随后，通过以物体为中心的残差强化学习，结合物理世界模型，将生成的视频动作转化为物理上精确的机器人动作。这种协同机制将隐式视觉引导转化为可执行的物理机器人轨迹，从而无需真实机器人数据采集，实现了零样本泛化的机器人操作能力。在多种真实世界任务上的实验表明，PhysWorld 相较于以往方法显著提升了操作精度。请访问 \\href{https://pointscoder.github.io/PhysWorld_Web/}{项目网页} 获取更多细节。",
        "translated_title": "机器人从物理世界模型中学习",
        "label": [],
        "label_reason": "任务为机器人控制，非图像像素级处理。",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "新框架结合视频生成与物理建模提升机器人动作精度。"
    },
    {
        "title": "TwinOR: Photorealistic Digital Twins of Dynamic Operating Rooms for\n  Embodied AI Research",
        "url": "http://arxiv.org/abs/2511.07412v1",
        "pub_date": "2025-11-10",
        "summary": "Developing embodied AI for intelligent surgical systems requires safe, controllable environments for continual learning and evaluation. However, safety regulations and operational constraints in operating rooms (ORs) limit embodied agents from freely perceiving and interacting in realistic settings. Digital twins provide high-fidelity, risk-free environments for exploration and training. How we may create photorealistic and dynamic digital representations of ORs that capture relevant spatial, visual, and behavioral complexity remains unclear. We introduce TwinOR, a framework for constructing photorealistic, dynamic digital twins of ORs for embodied AI research. The system reconstructs static geometry from pre-scan videos and continuously models human and equipment motion through multi-view perception of OR activities. The static and dynamic components are fused into an immersive 3D environment that supports controllable simulation and embodied exploration. The proposed framework reconstructs complete OR geometry with centimeter level accuracy while preserving dynamic interaction across surgical workflows, enabling realistic renderings and a virtual playground for embodied AI systems. In our experiments, TwinOR simulates stereo and monocular sensor streams for geometry understanding and visual localization tasks. Models such as FoundationStereo and ORB-SLAM3 on TwinOR-synthesized data achieve performance within their reported accuracy on real indoor datasets, demonstrating that TwinOR provides sensor-level realism sufficient for perception and localization challenges. By establishing a real-to-sim pipeline for constructing dynamic, photorealistic digital twins of OR environments, TwinOR enables the safe, scalable, and data-efficient development and benchmarking of embodied AI, ultimately accelerating the deployment of embodied AI from sim-to-real.",
        "translated": "发展用于智能外科系统的具身人工智能需要安全、可控的环境以支持持续学习与评估。然而，手术室（OR）中的安全规范与操作约束限制了具身智能体在真实场景中自由感知与交互的能力。数字孪生技术可提供高保真、无风险的环境，用于探索与训练。如何构建能够捕捉相关空间、视觉及行为复杂性的逼真且动态的手术室数字孪生体，目前尚不明确。我们提出 TwinOR，一个面向具身人工智能研究的手术室高保真、动态数字孪生体构建框架。该系统通过术前扫描视频重建静态几何结构，并通过多视角感知手术室活动，持续建模人员与设备的运动状态。静态与动态组件融合为沉浸式三维环境，支持可控仿真与具身探索。所提框架可实现厘米级精度的完整手术室几何重建，同时保留手术工作流中的动态交互能力，从而支持逼真的渲染效果，为具身人工智能系统提供虚拟实验平台。在实验中，TwinOR 生成立体与单目传感器数据流，用于几何理解与视觉定位任务。如 FoundationStereo 和 ORB-SLAM3 等模型在 TwinOR 合成数据上的表现接近其在真实室内数据集中的报告精度，证明 TwinOR 提供的传感器级别真实度足以应对感知与定位挑战。通过建立从真实到仿真的管道，构建动态、逼真的手术室数字孪生体，TwinOR 推动具身人工智能的安全、可扩展且数据高效的开发与基准测试，最终加速从仿真到现实的部署进程。",
        "translated_title": "TwinOR：用于具身人工智能研究的动态手术室逼真数字孪生体",
        "label": [],
        "label_reason": "构建手术室数字孪生，非图像像素级恢复任务",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出新框架用于模拟真实环境，支持AI训练"
    },
    {
        "title": "DIMO: Diverse 3D Motion Generation for Arbitrary Objects",
        "url": "http://arxiv.org/abs/2511.07409v1",
        "pub_date": "2025-11-10",
        "summary": "We present DIMO, a generative approach capable of generating diverse 3D motions for arbitrary objects from a single image. The core idea of our work is to leverage the rich priors in well-trained video models to extract the common motion patterns and then embed them into a shared low-dimensional latent space. Specifically, we first generate multiple videos of the same object with diverse motions. We then embed each motion into a latent vector and train a shared motion decoder to learn the distribution of motions represented by a structured and compact motion representation, i.e., neural key point trajectories. The canonical 3D Gaussians are then driven by these key points and fused to model the geometry and appearance. During inference time with learned latent space, we can instantly sample diverse 3D motions in a single-forward pass and support several interesting applications including 3D motion interpolation and language-guided motion generation. Our project page is available at https://linzhanm.github.io/dimo.",
        "translated": "我们提出 DIMO，一种能够从单张图像生成任意物体多样化 3D 运动的生成式方法。本工作的核心思想是利用经过充分训练的视频模型中蕴含的丰富先验知识，提取常见的运动模式，并将其嵌入到一个共享的低维潜在空间中。具体而言，我们首先生成同一物体具有多样运动模式的多个视频；随后，将每个运动模式编码为一个潜在向量，并训练一个共享的运动解码器，以学习由结构化且紧凑的运动表示（即神经关键点轨迹）所表征的运动分布。接着，通过这些关键点驱动标准的 3D 高斯分布并进行融合，从而建模物体的几何结构与外观。在推理阶段，借助学习到的潜在空间，我们可在单次前向传播中即时采样出多样化的 3D 运动，并支持多种有趣的应用，如 3D 运动插值与语言引导的运动生成。我们的项目页面可访问：https://linzhanm.github.io/dimo。",
        "translated_title": "DIMO：面向任意物体的多样化三维运动生成",
        "label": [],
        "label_reason": "生成3D运动属高阶视觉任务，非像素级图像恢复",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "提出新颖结构化关键点轨迹驱动3D运动生成"
    },
    {
        "title": "SpatialThinker: Reinforcing 3D Reasoning in Multimodal LLMs via Spatial\n  Rewards",
        "url": "http://arxiv.org/abs/2511.07403v1",
        "pub_date": "2025-11-10",
        "summary": "Multimodal large language models (MLLMs) have achieved remarkable progress in vision-language tasks, but they continue to struggle with spatial understanding. Existing spatial MLLMs often rely on explicit 3D inputs or architecture-specific modifications, and remain constrained by large-scale datasets or sparse supervision. To address these limitations, we introduce SpatialThinker, a 3D-aware MLLM trained with RL to integrate structured spatial grounding with multi-step reasoning. The model simulates human-like spatial perception by constructing a scene graph of task-relevant objects and spatial relations, and reasoning towards an answer via dense spatial rewards. SpatialThinker consists of two key contributions: (1) a data synthesis pipeline that generates STVQA-7K, a high-quality spatial VQA dataset, and (2) online RL with a multi-objective dense spatial reward enforcing spatial grounding. SpatialThinker-7B outperforms supervised fine-tuning and the sparse RL baseline on spatial understanding and real-world VQA benchmarks, nearly doubling the base-model gain compared to sparse RL, and surpassing GPT-4o. These results showcase the effectiveness of combining spatial supervision with reward-aligned reasoning in enabling robust 3D spatial understanding with limited data and advancing MLLMs towards human-level visual reasoning.",
        "translated": "多模态大语言模型（MLLMs）在视觉-语言任务中已取得显著进展，但在空间理解方面仍存在困难。现有空间 MLLMs 通常依赖显式的三维输入或架构特定的修改，并受限于大规模数据集或稀疏监督。为解决这些限制，我们提出了 SpatialThinker，这是一种具备三维感知能力的 MLLM，通过强化学习训练，将结构化的空间定位与多步推理相结合。该模型通过构建与任务相关的物体及其空间关系的场景图，模拟人类的空间感知，并借助密集空间奖励进行推理以得出答案。SpatialThinker 包含两项关键贡献：(1) 一个数据合成管道，生成高质量的空间视觉问答数据集 STVQA-7K；(2) 结合多目标密集空间奖励的在线强化学习，强化空间定位能力。SpatialThinker-7B 在空间理解及真实世界 VQA 基准测试中优于监督微调和稀疏 RL 基线方法，相较稀疏 RL 几乎实现基线模型性能的翻倍提升，并超越 GPT-4o。这些结果表明，在数据有限的前提下，结合空间监督与奖励对齐推理可有效支持稳健的三维空间理解，并推动 MLLMs 向人类级视觉推理迈进。",
        "translated_title": "SpatialThinker：通过空间奖励强化多模态大语言模型中的三维推理",
        "label": [],
        "label_reason": "任务属高阶视觉理解，非像素级图像处理",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "提出空间奖励机制提升3D推理能力"
    },
    {
        "title": "StreamDiffusionV2: A Streaming System for Dynamic and Interactive Video\n  Generation",
        "url": "http://arxiv.org/abs/2511.07399v1",
        "pub_date": "2025-11-10",
        "summary": "Generative models are reshaping the live-streaming industry by redefining how content is created, styled, and delivered. Previous image-based streaming diffusion models have powered efficient and creative live streaming products but have hit limits on temporal consistency due to the foundation of image-based designs. Recent advances in video diffusion have markedly improved temporal consistency and sampling efficiency for offline generation. However, offline generation systems primarily optimize throughput by batching large workloads. In contrast, live online streaming operates under strict service-level objectives (SLOs): time-to-first-frame must be minimal, and every frame must meet a per-frame deadline with low jitter. Besides, scalable multi-GPU serving for real-time streams remains largely unresolved so far. To address this, we present StreamDiffusionV2, a training-free pipeline for interactive live streaming with video diffusion models. StreamDiffusionV2 integrates an SLO-aware batching scheduler and a block scheduler, together with a sink-token--guided rolling KV cache, a motion-aware noise controller, and other system-level optimizations. Moreover, we introduce a scalable pipeline orchestration that parallelizes the diffusion process across denoising steps and network layers, achieving near-linear FPS scaling without violating latency guarantees. The system scales seamlessly across heterogeneous GPU environments and supports flexible denoising steps (e.g., 1--4), enabling both ultra-low-latency and higher-quality modes. Without TensorRT or quantization, StreamDiffusionV2 renders the first frame within 0.5s and attains 58.28 FPS with a 14B-parameter model and 64.52 FPS with a 1.3B-parameter model on four H100 GPUs, making state-of-the-art generative live streaming practical and accessible--from individual creators to enterprise-scale platforms.",
        "translated": "生成式模型正通过重新定义内容的创作、风格化与分发方式，重塑直播行业。此前基于图像的流媒体扩散模型虽推动了高效且富有创意的直播产品落地，但受限于其以图像为基础的设计架构，在时间一致性方面存在瓶颈。近期视频扩散模型的进展显著提升了离线生成阶段的时间一致性和采样效率。然而，离线生成系统主要通过批处理大规模任务来优化吞吐量。相比之下，实时在线直播需严格遵循服务等级目标（SLOs）：首帧响应时间必须最小化，每一帧均须在低抖动条件下满足单帧截止时限。此外，目前针对实时流媒体实现可扩展的多GPU服务部署仍处于未充分解决的状态。为应对上述挑战，我们提出 StreamDiffusionV2——一种无需训练的交互式直播管道，专为视频扩散模型设计。StreamDiffusionV2 集成了具备 SLO 意识的批处理调度器与块级调度器，并结合 sink-token 引导的滚动 KV 缓存、运动感知噪声控制器及其他系统级优化方案。此外，我们引入了一种可扩展的管道编排架构，将扩散过程并行化至去噪步数和网络层之间，从而在不违反延迟约束的前提下实现接近线性的帧率（FPS）扩展能力。该系统可无缝扩展至异构 GPU 环境，并支持灵活的去噪步数设置（如 1–4 步），既支持超低延迟模式，亦支持更高画质模式。无需 TensorRT 或量化技术，StreamDiffusionV2 在四张 H100 GPU 上，使用 14B 参数模型可在 0.5 秒内渲染出首帧画面，帧率达 58.28 FPS；使用 1.3B 参数模型则可达 64.52 FPS，使业界领先的生成式直播方案真正具备实用价值与广泛可及性——从个人创作者到企业级平台均可轻松应用。",
        "translated_title": "StreamDiffusionV2：用于动态与交互式视频生成的流式系统",
        "label": [],
        "label_reason": "生成视频内容，非图像像素级恢复",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "系统优化与调度创新，提升实时生成效率"
    },
    {
        "title": "Real-Time LiDAR Super-Resolution via Frequency-Aware Multi-Scale Fusion",
        "url": "http://arxiv.org/abs/2511.07377v1",
        "pub_date": "2025-11-10",
        "summary": "LiDAR super-resolution addresses the challenge of achieving high-quality 3D perception from cost-effective, low-resolution sensors. While recent transformer-based approaches like TULIP show promise, they remain limited to spatial-domain processing with restricted receptive fields. We introduce FLASH (Frequency-aware LiDAR Adaptive Super-resolution with Hierarchical fusion), a novel framework that overcomes these limitations through dual-domain processing. FLASH integrates two key innovations: (i) Frequency-Aware Window Attention that combines local spatial attention with global frequency-domain analysis via FFT, capturing both fine-grained geometry and periodic scanning patterns at log-linear complexity. (ii) Adaptive Multi-Scale Fusion that replaces conventional skip connections with learned position-specific feature aggregation, enhanced by CBAM attention for dynamic feature selection. Extensive experiments on KITTI demonstrate that FLASH achieves state-of-the-art performance across all evaluation metrics, surpassing even uncertainty-enhanced baselines that require multiple forward passes. Notably, FLASH outperforms TULIP with Monte Carlo Dropout while maintaining single-pass efficiency, which enables real-time deployment. The consistent superiority across all distance ranges validates that our dual-domain approach effectively handles uncertainty through architectural design rather than computationally expensive stochastic inference, making it practical for autonomous systems.",
        "translated": "激光雷达超分辨率旨在解决从低成本、低分辨率传感器获取高质量三维感知的挑战。尽管近期基于Transformer的方法（如TULIP）展现出潜力，但它们仍局限于空域处理且感受野受限。我们提出FLASH（Frequency-aware LiDAR Adaptive Super-resolution with Hierarchical fusion），一种新颖框架，通过双域处理克服上述局限。FLASH包含两项关键技术创新：(i) 频率感知窗口注意力机制，结合局部空间注意力与全局频域分析（通过FFT实现），在对数线性复杂度下同时捕获精细几何结构和周期扫描模式；(ii) 自适应多尺度融合模块，用可学习的位置特定特征聚合机制替代传统跳跃连接，并借助CBAM注意力模块实现动态特征选择。在KITTI数据集上的大量实验表明，FLASH在所有评估指标上均达到当前最优性能，甚至超越了需要多次前向传播的不确定性增强基线方法。值得注意的是，FLASH在采用蒙特卡洛Dropout时优于TULIP，同时保持单次前向传播效率，从而支持实时部署。其在所有距离范围中的持续优越表现验证了我们的双域架构设计能够有效通过结构化方式处理不确定性，而非依赖计算开销高昂的随机推断，使其适用于自动驾驶系统。",
        "translated_title": "基于频域感知多尺度融合的实时激光雷达超分辨率重建",
        "label": [
            "超分辨率"
        ],
        "label_reason": "提出双域融合框架提升LiDAR超分性能",
        "relevance_score": 7,
        "novelty_score": 8,
        "novelty_reason": "创新频域窗注意力与自适应多尺度融合架构"
    },
    {
        "title": "Inference-Time Scaling of Diffusion Models for Infrared Data Generation",
        "url": "http://arxiv.org/abs/2511.07362v1",
        "pub_date": "2025-11-10",
        "summary": "Infrared imagery enables temperature-based scene understanding using passive sensors, particularly under conditions of low visibility where traditional RGB imaging fails. Yet, developing downstream vision models for infrared applications is hindered by the scarcity of high-quality annotated data, due to the specialized expertise required for infrared annotation. While synthetic infrared image generation has the potential to accelerate model development by providing large-scale, diverse training data, training foundation-level generative diffusion models in the infrared domain has remained elusive due to limited datasets. In light of such data constraints, we explore an inference-time scaling approach using a domain-adapted CLIP-based verifier for enhanced infrared image generation quality. We adapt FLUX.1-dev, a state-of-the-art text-to-image diffusion model, to the infrared domain by finetuning it on a small sample of infrared images using parameter-efficient techniques. The trained verifier is then employed during inference to guide the diffusion sampling process toward higher quality infrared generations that better align with input text prompts. Empirically, we find that our approach leads to consistent improvements in generation quality, reducing FID scores on the KAIST Multispectral Pedestrian Detection Benchmark dataset by 10% compared to unguided baseline samples. Our results suggest that inference-time guidance offers a promising direction for bridging the domain gap in low-data infrared settings.",
        "translated": "红外图像利用被动传感器实现基于温度的场景理解，尤其在传统RGB成像失效的低能见度条件下具有优势。然而，由于红外标注需要专门的专业知识，高质量标注数据匮乏，这阻碍了面向红外应用的下游视觉模型的发展。尽管合成红外图像生成有望通过提供大规模、多样化的训练数据加速模型开发，但由于可用数据集有限，目前尚未有可行方法在红外领域训练基础级别的生成式扩散模型。针对此类数据约束，我们探索了一种推理时扩展方法：采用域自适应的CLIP基验证器，以提升红外图像生成质量。我们通过对少量红外图像使用参数高效技术微调先进的文本到图像扩散模型FLUX.1-dev，使其适配红外领域。随后，训练好的验证器将在推理阶段引导扩散采样过程，使生成结果更贴近输入文本提示，并提升红外图像的质量。实证研究表明，我们的方法显著提升了生成质量，在KAIST多光谱行人检测基准数据集上，相比无引导基线样本，FID分数降低10%。我们的结果表明，推理时引导是一种极具前景的方向，有助于弥合低数据量红外场景中的领域鸿沟。",
        "translated_title": "扩散模型在红外数据生成中的推理时尺度调整",
        "label": [],
        "label_reason": "生成红外图像属高阶视觉任务，非像素级恢复。",
        "relevance_score": 3,
        "novelty_score": 7,
        "novelty_reason": "创新性在于推理时域引导，提升生成质量。"
    },
    {
        "title": "Preparation of Fractal-Inspired Computational Architectures for Advanced\n  Large Language Model Analysis",
        "url": "http://arxiv.org/abs/2511.07329v1",
        "pub_date": "2025-11-10",
        "summary": "It introduces FractalNet, a fractal-inspired computational architectures for advanced large language model analysis that mainly challenges model diversity on a large scale in an efficient manner. The new set-up involves a template-driven generator, runner, and evaluation framework that, through systematic permutations of convolutional, normalization, activation, and dropout layers, can create more than 1,200 variants of neural networks. Fractal templates allow for structural recursion and multi-column pathways, thus, models become deeper and wider in a balanced way. Training utilizes PyTorch, Automatic Mixed Precision (AMP), and gradient checkpointing and is carried out on the CIFAR-10 dataset for five epochs. The outcomes show that fractal-based architectures are capable of strong performance and are computationally efficient. The paper positions fractal design as a feasible and resource-efficient method of automated architecture exploration.",
        "translated": "它引入了 FractalNet，一种受分形启发的计算架构，旨在高效地大规模挑战大型语言模型分析中的模型多样性。该新框架包含一个模板驱动的生成器、运行器和评估框架，通过系统性排列卷积层、归一化层、激活层和 dropout 层，可生成超过 1,200 种神经网络变体。分形模板支持结构递归与多列路径，从而使模型在深度与宽度上实现均衡扩展。训练采用 PyTorch、自动混合精度（AMP）及梯度检查点技术，并在 CIFAR-10 数据集上进行五轮训练。实验结果表明，基于分形的架构具备强大性能且计算效率高。论文将分形设计定位为一种可行且资源高效的自动化架构探索方法。",
        "translated_title": "面向先进大语言模型分析的分形启发式计算架构构建",
        "label": [],
        "label_reason": "研究大语言模型架构，非图像处理任务",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出分形架构探索方法，但属高阶模型优化"
    },
    {
        "title": "Garbage Vulnerable Point Monitoring using IoT and Computer Vision",
        "url": "http://arxiv.org/abs/2511.07325v1",
        "pub_date": "2025-11-10",
        "summary": "This paper proposes a smart way to manage municipal solid waste by using the Internet of Things (IoT) and computer vision (CV) to monitor illegal waste dumping at garbage vulnerable points (GVPs) in urban areas. The system can quickly detect and monitor dumped waste using a street-level camera and object detection algorithm. Data was collected from the Sangareddy district in Telangana, India. A series of comprehensive experiments was carried out using the proposed dataset to assess the accuracy and overall performance of various object detection models. Specifically, we performed an in-depth evaluation of YOLOv8, YOLOv10, YOLO11m, and RT-DETR on our dataset. Among these models, YOLO11m achieved the highest accuracy of 92.39\\% in waste detection, demonstrating its effectiveness in detecting waste. Additionally, it attains an mAP@50 of 0.91, highlighting its high precision. These findings confirm that the object detection model is well-suited for monitoring and tracking waste dumping events at GVP locations. Furthermore, the system effectively captures waste disposal patterns, including hourly, daily, and weekly dumping trends, ensuring comprehensive daily and nightly monitoring.",
        "translated": "本文提出了一种利用物联网（IoT）与计算机视觉（CV）技术，对城市区域垃圾易发点（GVPs）非法倾倒行为进行智能监控的固废管理方法。该系统通过街景摄像头与目标检测算法，可快速识别并持续监测非法倾倒的垃圾。实验数据采集自印度泰伦加纳邦桑加雷迪区。我们基于所构建的数据集，开展了系列全面的实验，以评估各类目标检测模型的准确性和整体性能。具体而言，我们在本数据集上对YOLOv8、YOLOv10、YOLO11m和RT-DETR进行了深入评估。其中，YOLO11m在垃圾检测中取得了最高的准确率92.39%，充分体现了其在垃圾检测方面的有效性；同时，其mAP@50达到0.91，表明其具有极高的检测精度。这些结果证实，所选用的目标检测模型非常适合用于对GVP区域非法倾倒事件的监控与追踪。此外，系统能够有效捕捉垃圾处置模式，包括按小时、按日及按周的倾倒趋势，从而确保实现全天候、全时段的全面监测。",
        "translated_title": "基于物联网与计算机视觉的垃圾易损点监控",
        "label": [],
        "label_reason": "目标为物体检测，属高阶视觉任务",
        "relevance_score": 1,
        "novelty_score": 1,
        "novelty_reason": "模型对比实验，无像素级图像处理创新"
    },
    {
        "title": "YoNoSplat: You Only Need One Model for Feedforward 3D Gaussian Splatting",
        "url": "http://arxiv.org/abs/2511.07321v1",
        "pub_date": "2025-11-10",
        "summary": "Fast and flexible 3D scene reconstruction from unstructured image collections remains a significant challenge. We present YoNoSplat, a feedforward model that reconstructs high-quality 3D Gaussian Splatting representations from an arbitrary number of images. Our model is highly versatile, operating effectively with both posed and unposed, calibrated and uncalibrated inputs. YoNoSplat predicts local Gaussians and camera poses for each view, which are aggregated into a global representation using either predicted or provided poses. To overcome the inherent difficulty of jointly learning 3D Gaussians and camera parameters, we introduce a novel mixing training strategy. This approach mitigates the entanglement between the two tasks by initially using ground-truth poses to aggregate local Gaussians and gradually transitioning to a mix of predicted and ground-truth poses, which prevents both training instability and exposure bias. We further resolve the scale ambiguity problem by a novel pairwise camera-distance normalization scheme and by embedding camera intrinsics into the network. Moreover, YoNoSplat also predicts intrinsic parameters, making it feasible for uncalibrated inputs. YoNoSplat demonstrates exceptional efficiency, reconstructing a scene from 100 views (at 280x518 resolution) in just 2.69 seconds on an NVIDIA GH200 GPU. It achieves state-of-the-art performance on standard benchmarks in both pose-free and pose-dependent settings. Our project page is at https://botaoye.github.io/yonosplat/.",
        "translated": "从无结构图像集合中快速构建高质量的三维场景仍是一个重大挑战。我们提出了 YoNoSplat，一种前馈模型，可从任意数量的图像重建高质量的三维高斯点云表示。该模型具有高度灵活性，能有效处理带姿态与无姿态、已标定与未标定的输入数据。YoNoSplat 预测每个视角下的局部高斯分布和相机位姿，并通过预测或提供的位姿将其聚合为全局表示。为克服联合学习三维高斯分布与相机参数所固有的困难，我们引入了一种新颖的混合训练策略：初始阶段使用真实位姿聚合局部高斯分布，随后逐步过渡到预测位姿与真实位姿混合的形式，从而避免训练不稳定及曝光偏差问题。此外，我们通过一种新颖的成对相机距离归一化方案以及将相机内参嵌入网络，解决了尺度模糊问题；同时，YoNoSplat 还能够预测相机内参，使其适用于未标定输入。YoNoSplat 表现出卓越效率，在 NVIDIA GH200 GPU 上仅需 2.69 秒即可完成从 100 个视角（分辨率为 280x518）的场景重建。其在标准基准测试中，无论是否依赖姿态信息均取得了当前最优性能。项目页面详见 https://botaoye.github.io/yonosplat/。",
        "translated_title": "YoNoSplat：你只需一个模型即可实现前馈式三维高斯点渲染",
        "label": [],
        "label_reason": "3D场景重建属高阶视觉任务，非像素级图像处理",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "提出新训练策略与归一化方案提升效率"
    },
    {
        "title": "Beyond Boundaries: Leveraging Vision Foundation Models for Source-Free\n  Object Detection",
        "url": "http://arxiv.org/abs/2511.07301v1",
        "pub_date": "2025-11-10",
        "summary": "Source-Free Object Detection (SFOD) aims to adapt a source-pretrained object detector to a target domain without access to source data. However, existing SFOD methods predominantly rely on internal knowledge from the source model, which limits their capacity to generalize across domains and often results in biased pseudo-labels, thereby hindering both transferability and discriminability. In contrast, Vision Foundation Models (VFMs), pretrained on massive and diverse data, exhibit strong perception capabilities and broad generalization, yet their potential remains largely untapped in the SFOD setting. In this paper, we propose a novel SFOD framework that leverages VFMs as external knowledge sources to jointly enhance feature alignment and label quality. Specifically, we design three VFM-based modules: (1) Patch-weighted Global Feature Alignment (PGFA) distills global features from VFMs using patch-similarity-based weighting to enhance global feature transferability; (2) Prototype-based Instance Feature Alignment (PIFA) performs instance-level contrastive learning guided by momentum-updated VFM prototypes; and (3) Dual-source Enhanced Pseudo-label Fusion (DEPF) fuses predictions from detection VFMs and teacher models via an entropy-aware strategy to yield more reliable supervision. Extensive experiments on six benchmarks demonstrate that our method achieves state-of-the-art SFOD performance, validating the effectiveness of integrating VFMs to simultaneously improve transferability and discriminability.",
        "translated": "无源目标检测（Source-Free Object Detection, SFOD）旨在在不接触源域数据的情况下，将已在源域预训练的目标检测器迁移到目标域。然而，现有SFOD方法主要依赖于源模型内部知识，这限制了其跨域泛化能力，并常导致伪标签偏差，从而阻碍了迁移能力和判别能力的提升。相比之下，视觉基础模型（Vision Foundation Models, VFMs）在海量且多样化的数据上进行预训练，具备强大的感知能力和广泛的泛化能力，但在SFOD场景中其潜力尚未被充分挖掘。本文提出了一种新颖的SFOD框架，利用VFMs作为外部知识来源，协同增强特征对齐与标签质量。具体而言，我们设计了三个基于VFMs的模块：（1）基于补丁相似度加权的全局特征对齐（Patch-weighted Global Feature Alignment, PGFA），通过补丁相似性加权机制从VFMs中蒸馏全局特征，以增强全局特征的迁移能力；（2）基于原型的实例级特征对齐（Prototype-based Instance Feature Alignment, PIFA），借助动量更新的VFMs原型引导实例级对比学习；（3）双源增强伪标签融合（Dual-source Enhanced Pseudo-label Fusion, DEPF），通过熵感知策略融合检测VFMs与教师模型的预测结果，生成更可靠的监督信号。在六个基准数据集上的广泛实验表明，我们的方法实现了当前最优的SFOD性能，验证了集成VFMs可同时提升迁移能力与判别能力的有效性。",
        "translated_title": "超越边界：利用视觉基础模型实现无源对象检测",
        "label": [],
        "label_reason": "目标检测属高阶视觉任务，非像素级图像处理",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出新模块提升伪标签质量与迁移能力"
    },
    {
        "title": "LMM-IQA: Image Quality Assessment for Low-Dose CT Imaging",
        "url": "http://arxiv.org/abs/2511.07298v1",
        "pub_date": "2025-11-10",
        "summary": "Low-dose computed tomography (CT) represents a significant improvement in patient safety through lower radiation doses, but increased noise, blur, and contrast loss can diminish diagnostic quality. Therefore, consistency and robustness in image quality assessment become essential for clinical applications. In this study, we propose an LLM-based quality assessment system that generates both numerical scores and textual descriptions of degradations such as noise, blur, and contrast loss. Furthermore, various inference strategies - from the zero-shot approach to metadata integration and error feedback - are systematically examined, demonstrating the progressive contribution of each method to overall performance. The resultant assessments yield not only highly correlated scores but also interpretable output, thereby adding value to clinical workflows. The source codes of our study are available at https://github.com/itu-biai/lmms_ldct_iqa.",
        "translated": "低剂量计算机断层扫描（CT）通过降低辐射剂量显著提升了患者安全性，但随之增加的噪声、模糊和对比度损失可能削弱诊断质量。因此，在临床应用中，图像质量评估的一致性和鲁棒性变得至关重要。在本研究中，我们提出了一种基于大语言模型（LLM）的质量评估系统，该系统能够生成关于噪声、模糊和对比度损失等退化现象的数值评分与文本描述。此外，我们系统性地考察了多种推理策略——从零样本方法到元数据融合与错误反馈机制——并证明每种方法对整体性能的逐级贡献。所获得的评估结果不仅具有高度相关性，还提供可解释的输出，从而为临床工作流程增添价值。本研究的源代码可在 https://github.com/itu-biai/lmms_ldct_iqa 获取。",
        "translated_title": "LMM-IQA：低剂量CT成像图像质量评估",
        "label": [],
        "label_reason": "评估低剂量CT图像质量，非像素级恢复任务",
        "relevance_score": 3,
        "novelty_score": 5,
        "novelty_reason": "集成LLM生成描述，但非图像恢复方法"
    },
    {
        "title": "VADER: Towards Causal Video Anomaly Understanding with Relation-Aware\n  Large Language Models",
        "url": "http://arxiv.org/abs/2511.07299v1",
        "pub_date": "2025-11-10",
        "summary": "Video anomaly understanding (VAU) aims to provide detailed interpretation and semantic comprehension of anomalous events within videos, addressing limitations of traditional methods that focus solely on detecting and localizing anomalies. However, existing approaches often neglect the deeper causal relationships and interactions between objects, which are critical for understanding anomalous behaviors. In this paper, we propose VADER, an LLM-driven framework for Video Anomaly unDErstanding, which integrates keyframe object Relation features with visual cues to enhance anomaly comprehension from video. Specifically, VADER first applies an Anomaly Scorer to assign per-frame anomaly scores, followed by a Context-AwarE Sampling (CAES) strategy to capture the causal context of each anomalous event. A Relation Feature Extractor and a COntrastive Relation Encoder (CORE) jointly model dynamic object interactions, producing compact relational representations for downstream reasoning. These visual and relational cues are integrated with LLMs to generate detailed, causally grounded descriptions and support robust anomaly-related question answering. Experiments on multiple real-world VAU benchmarks demonstrate that VADER achieves strong results across anomaly description, explanation, and causal reasoning tasks, advancing the frontier of explainable video anomaly analysis.",
        "translated": "视频异常理解（VAU）旨在为视频中的异常事件提供详细的解释与语义理解，弥补传统方法仅关注异常检测与定位的局限性。然而，现有方法往往忽视了物体间深层因果关系与交互作用，而这些因素对于理解异常行为至关重要。本文提出VADER，一种基于大语言模型（LLM）驱动的视频异常理解框架，通过融合关键帧物体关系特征与视觉线索，提升对视频中异常事件的理解能力。具体而言，VADER首先应用异常评分器为每一帧分配异常得分，随后采用上下文感知采样（CAES）策略以捕捉每个异常事件的因果上下文。关系特征提取器与对比关系编码器（CORE）协同建模动态物体交互，生成紧凑的关系表示用于下游推理。这些视觉与关系线索被整合进大语言模型，以生成详细且具有因果基础的描述，并支持鲁棒的异常相关问答任务。在多个现实世界VAU基准数据集上的实验表明，VADER在异常描述、解释与因果推理等任务上均取得优异性能，推动了可解释视频异常分析研究的边界。",
        "translated_title": "VADER：基于关系感知的大语言模型面向因果视频异常理解",
        "label": [],
        "label_reason": "属视频异常理解，非图像像素级处理",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出因果关系建模框架，提升解释能力"
    },
    {
        "title": "Verifying rich robustness properties for neural networks",
        "url": "http://arxiv.org/abs/2511.07293v1",
        "pub_date": "2025-11-10",
        "summary": "Robustness is a important problem in AI alignment and safety, with models such as neural networks being increasingly used in safety-critical systems. In the last decade, a large body of work has emerged on local robustness, i.e., checking if the decision of a neural network remains unchanged when the input is slightly perturbed. However, many of these approaches require specialized encoding and often ignore the confidence of a neural network on its output. In this paper, our goal is to build a generalized framework to specify and verify variants of robustness in neural network verification. We propose a specification framework using a simple grammar, which is flexible enough to capture most existing variants. This allows us to introduce new variants of robustness that take into account the confidence of the neural network in its outputs. Next, we develop a novel and powerful unified technique to verify all such variants in a homogeneous way, viz., by adding a few additional layers to the neural network. This enables us to use any state-of-the-art neural network verification tool, without having to tinker with the encoding within, while incurring an approximation error that we show is bounded. We perform an extensive experimental evaluation over a large suite of 8870 benchmarks having 138M parameters in a largest network, and show that we are able to capture a wide set of robustness variants and outperform direct encoding approaches by a significant margin.",
        "translated": "鲁棒性是人工智能对齐与安全领域的一个重要问题，随着神经网络等模型在安全关键系统中的应用日益广泛。过去十年间，大量工作聚焦于局部鲁棒性，即检验神经网络的决策在输入发生轻微扰动时是否保持不变。然而，许多现有方法需要专门的编码方式，并往往忽略神经网络对其输出的置信度。本文的目标是构建一个通用框架，用于指定并验证神经网络验证中多种鲁棒性的变体。我们提出了一种基于简单文法的规范框架，该框架具备足够的灵活性以涵盖大多数现有变体。这使得我们能够引入新的鲁棒性变体，这些变体考虑了神经网络对其输出的置信度。随后，我们开发了一种新颖而强大的统一技术，以一种同质化的方式验证所有上述变体，具体方法为在神经网络中添加少量额外层。这一方法使我们能够在不修改内部编码的前提下，使用任何现有的最先进的神经网络验证工具，同时所引入的近似误差已被我们证明是有界的。我们在包含8870个基准测试、最大网络参数量达1.38亿的大型评测集上进行了全面实验评估，结果表明，我们能够捕捉到广泛的鲁棒性变体，并显著优于直接编码方法。",
        "translated_title": "验证神经网络的丰富鲁棒性属性",
        "label": [],
        "label_reason": "属于模型验证与安全，非图像处理任务",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "提出通用框架与统一验证技术，显著提升效率"
    },
    {
        "title": "PlanT 2.0: Exposing Biases and Structural Flaws in Closed-Loop Driving",
        "url": "http://arxiv.org/abs/2511.07292v1",
        "pub_date": "2025-11-10",
        "summary": "Most recent work in autonomous driving has prioritized benchmark performance and methodological innovation over in-depth analysis of model failures, biases, and shortcut learning. This has led to incremental improvements without a deep understanding of the current failures. While it is straightforward to look at situations where the model fails, it is hard to understand the underlying reason. This motivates us to conduct a systematic study, where inputs to the model are perturbed and the predictions observed. We introduce PlanT 2.0, a lightweight, object-centric planning transformer designed for autonomous driving research in CARLA. The object-level representation enables controlled analysis, as the input can be easily perturbed (e.g., by changing the location or adding or removing certain objects), in contrast to sensor-based models. To tackle the scenarios newly introduced by the challenging CARLA Leaderboard 2.0, we introduce multiple upgrades to PlanT, achieving state-of-the-art performance on Longest6 v2, Bench2Drive, and the CARLA validation routes. Our analysis exposes insightful failures, such as a lack of scene understanding caused by low obstacle diversity, rigid expert behaviors leading to exploitable shortcuts, and overfitting to a fixed set of expert trajectories. Based on these findings, we argue for a shift toward data-centric development, with a focus on richer, more robust, and less biased datasets. We open-source our code and model at https://github.com/autonomousvision/plant2.",
        "translated": "自动驾驶领域的最新研究工作主要侧重于基准性能提升与方法论创新，而对模型失效、偏置及捷径学习等深层问题缺乏深入分析。这导致了在不彻底理解当前模型失败根源的前提下实现渐进式改进。虽然观察模型失效的场景相对简单，但难以理解其背后的成因。为此，我们开展了一项系统性研究：通过人为扰动输入并观测模型预测结果，以揭示潜在的问题机制。我们提出了 PlanT 2.0 —— 一种轻量级、以对象为中心的规划 Transformer，专为 CARLA 平台的自动驾驶研究设计。该模型基于对象级表示，支持可控实验分析——例如通过移动物体位置或增删特定对象来扰动输入，相较依赖传感器数据的传统模型更具灵活性。为应对挑战性增强的 CARLA Leaderboard 2.0 引入的新场景，我们在 PlanT 基础上进行了多方面升级，在 Longest6 v2、Bench2Drive 以及 CARLA 验证路线等多个指标上均达到当前最优性能。我们的分析揭示了若干深刻缺陷：如障碍物种类匮乏导致场景理解能力不足、专家行为过于刚性致使可被利用的捷径存在、以及模型过度拟合固定专家轨迹等问题。基于上述发现，我们主张将研究重心转向“数据驱动开发”，强调构建更丰富、更鲁棒且偏差更少的数据集。我们的代码与模型已开源，详见 https://github.com/autonomousvision/plant2。",
        "translated_title": "PlanT 2.0：揭示闭环驾驶中的偏见与结构性缺陷",
        "label": [],
        "label_reason": "研究自动驾驶决策偏差，非图像像素级处理",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出新规划模型与分析框架，非图像恢复任务"
    },
    {
        "title": "CAMP-VQA: Caption-Embedded Multimodal Perception for No-Reference\n  Quality Assessment of Compressed Video",
        "url": "http://arxiv.org/abs/2511.07290v1",
        "pub_date": "2025-11-10",
        "summary": "The prevalence of user-generated content (UGC) on platforms such as YouTube and TikTok has rendered no-reference (NR) perceptual video quality assessment (VQA) vital for optimizing video delivery. Nonetheless, the characteristics of non-professional acquisition and the subsequent transcoding of UGC video on sharing platforms present significant challenges for NR-VQA. Although NR-VQA models attempt to infer mean opinion scores (MOS), their modeling of subjective scores for compressed content remains limited due to the absence of fine-grained perceptual annotations of artifact types. To address these challenges, we propose CAMP-VQA, a novel NR-VQA framework that exploits the semantic understanding capabilities of large vision-language models. Our approach introduces a quality-aware prompting mechanism that integrates video metadata (e.g., resolution, frame rate, bitrate) with key fragments extracted from inter-frame variations to guide the BLIP-2 pretraining approach in generating fine-grained quality captions. A unified architecture has been designed to model perceptual quality across three dimensions: semantic alignment, temporal characteristics, and spatial characteristics. These multimodal features are extracted and fused, then regressed to video quality scores. Extensive experiments on a wide variety of UGC datasets demonstrate that our model consistently outperforms existing NR-VQA methods, achieving improved accuracy without the need for costly manual fine-grained annotations. Our method achieves the best performance in terms of average rank and linear correlation (SRCC: 0.928, PLCC: 0.938) compared to state-of-the-art methods. The source code and trained models, along with a user-friendly demo, are available at: https://github.com/xinyiW915/CAMP-VQA.",
        "translated": "随着 YouTube 和 TikTok 等平台用户生成内容（UGC）的普及，无参考（NR）感知视频质量评估（VQA）对优化视频传输至关重要。然而，UGC 视频在分享平台上非专业采集及后续转码所带来的特性，给 NR-VQA 带来了显著挑战。尽管现有 NR-VQA 模型试图推断平均意见得分（MOS），但由于缺乏针对伪影类型的细粒度感知标注，其对压缩内容主观评分建模的能力仍显不足。为应对上述挑战，我们提出 CAMP-VQA，一种新颖的无参考视频质量评估框架，该框架利用大规模视觉-语言模型的语义理解能力。本方法引入了一种质量感知提示机制，将视频元数据（如分辨率、帧率、码率）与从帧间差异中提取的关键片段相结合，以指导 BLIP-2 预训练范式生成细粒度的质量描述。我们设计了一个统一架构，用于跨三个维度建模感知质量：语义一致性、时序特征与空间特征。这些多模态特征被提取并融合后，回归至视频质量分数。在大量 UGC 数据集上的广泛实验表明，我们的模型持续优于现有 NR-VQA 方法，在无需高昂人工细粒度标注的情况下实现了更高的精度。相较于当前最先进方法，本方法在平均排名和线性相关性（SRCC: 0.928, PLCC: 0.938）方面均取得最佳性能。源代码、训练好的模型及用户友好的演示程序可访问：https://github.com/xinyiW915/CAMP-VQA。",
        "translated_title": "CAMP-VQA：嵌入字幕的多模态感知用于压缩视频无参考质量评估",
        "label": [],
        "label_reason": "视频质量评估属高阶视觉任务，非像素级图像恢复",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "模型融合多模态与提示机制，但无低层图像修复创新"
    },
    {
        "title": "Glioma C6: A Novel Dataset for Training and Benchmarking Cell\n  Segmentation",
        "url": "http://arxiv.org/abs/2511.07286v1",
        "pub_date": "2025-11-10",
        "summary": "We present Glioma C6, a new open dataset for instance segmentation of glioma C6 cells, designed as both a benchmark and a training resource for deep learning models. The dataset comprises 75 high-resolution phase-contrast microscopy images with over 12,000 annotated cells, providing a realistic testbed for biomedical image analysis. It includes soma annotations and morphological cell categorization provided by biologists. Additional categorization of cells, based on morphology, aims to enhance the utilization of image data for cancer cell research. Glioma C6 consists of two parts: the first is curated with controlled parameters for benchmarking, while the second supports generalization testing under varying conditions. We evaluate the performance of several generalist segmentation models, highlighting their limitations on our dataset. Our experiments demonstrate that training on Glioma C6 significantly enhances segmentation performance, reinforcing its value for developing robust and generalizable models. The dataset is publicly available for researchers.",
        "translated": "我们提出了 Glioma C6，这是一个专为胶质瘤 C6 细胞实例分割设计的新公开数据集，兼具基准测试与深度学习模型训练资源的双重功能。该数据集包含 75 幅高分辨率相差显微图像，标注细胞数量超过 12,000 个，可作为生物医学图像分析的现实测试平台。数据集中包含由生物学家提供的细胞胞体标注及形态学分类信息。此外，基于形态特征对细胞进行进一步分类，旨在提升图像数据在癌症细胞研究中的应用价值。Glioma C6 数据集分为两部分：第一部分在控制参数下整理，用于基准性能评估；第二部分支持在不同条件下进行泛化能力测试。我们评估了多种通用分割模型的表现，指出了其在本数据集上的局限性。实验结果表明，在 Glioma C6 上训练可显著提升分割性能，进一步验证了该数据集在构建鲁棒且具有泛化能力模型方面的价值。该数据集已向研究人员公开发布。",
        "translated_title": "胶质瘤C6：一种用于细胞分割训练与评估的新数据集",
        "label": [],
        "label_reason": "目标为细胞分割，属高阶医学图像理解任务",
        "relevance_score": 2,
        "novelty_score": 5,
        "novelty_reason": "提供新数据集但无像素级恢复创新"
    },
    {
        "title": "Segmentation of Ischemic Stroke Lesions using Transfer Learning on\n  Multi-sequence MRI",
        "url": "http://arxiv.org/abs/2511.07281v1",
        "pub_date": "2025-11-10",
        "summary": "The accurate understanding of ischemic stroke lesions is critical for efficient therapy and prognosis of stroke patients. Magnetic resonance imaging (MRI) is sensitive to acute ischemic stroke and is a common diagnostic method for stroke. However, manual lesion segmentation performed by experts is tedious, time-consuming, and prone to observer inconsistency. Automatic medical image analysis methods have been proposed to overcome this challenge. However, previous approaches have relied on hand-crafted features that may not capture the irregular and physiologically complex shapes of ischemic stroke lesions. In this study, we present a novel framework for quickly and automatically segmenting ischemic stroke lesions on various MRI sequences, including T1-weighted, T2-weighted, DWI, and FLAIR. The proposed methodology is validated on the ISLES 2015 Brain Stroke sequence dataset, where we trained our model using the Res-Unet architecture twice: first, with pre-existing weights, and then without, to explore the benefits of transfer learning. Evaluation metrics, including the Dice score and sensitivity, were computed across 3D volumes. Finally, a Majority Voting Classifier was integrated to amalgamate the outcomes from each axis, resulting in a comprehensive segmentation method. Our efforts culminated in achieving a Dice score of 80.5\\% and an accuracy of 74.03\\%, showcasing the efficacy of our segmentation approach.",
        "translated": "对缺血性卒中病灶的准确理解对于卒中患者的高效治疗和预后评估至关重要。磁共振成像（MRI）对急性缺血性卒中敏感，是卒中诊断的常用方法。然而，由专家手动进行病灶分割工作繁琐、耗时，且易受观察者主观差异影响。为克服这一挑战，已提出多种自动医学图像分析方法。但此前的方法大多依赖手工设计特征，可能无法有效捕捉缺血性卒中病灶形态不规则且生理结构复杂的特点。在本研究中，我们提出了一种新颖框架，可快速、自动地对多种MRI序列（包括T1加权、T2加权、弥散加权成像DWI及FLAIR序列）中的缺血性卒中病灶进行分割。该方法在ISLES 2015脑卒中序列数据集上进行了验证，其中我们采用Res-Unet架构两次训练模型：首次使用预训练权重，随后去除预训练权重以探究迁移学习的优势。评价指标（包括Dice系数与灵敏度）在三维体积内计算得出。最后，引入多数投票分类器，综合各轴方向的分割结果，形成完整的分割方案。我们的工作最终实现了80.5%的Dice系数和74.03%的准确率，充分展现了所提分割方法的有效性。",
        "translated_title": "利用迁移学习对多序列MRI进行缺血性卒中病灶分割",
        "label": [],
        "label_reason": "医学图像分割属高阶任务，非像素级图像恢复",
        "relevance_score": 2,
        "novelty_score": 5,
        "novelty_reason": "迁移学习框架常规，无本质创新突破"
    },
    {
        "title": "StreamKV: Streaming Video Question-Answering with Segment-based KV Cache\n  Retrieval and Compression",
        "url": "http://arxiv.org/abs/2511.07278v1",
        "pub_date": "2025-11-10",
        "summary": "Video Large Language Models (Video-LLMs) have demonstrated significant potential in the areas of video captioning, search, and summarization. However, current Video-LLMs still face challenges with long real-world videos. Recent methods have introduced a retrieval mechanism that retrieves query-relevant KV caches for question answering, enhancing the efficiency and accuracy of long real-world videos. However, the compression and retrieval of KV caches are still not fully explored. In this paper, we propose \\textbf{StreamKV}, a training-free framework that seamlessly equips Video-LLMs with advanced KV cache retrieval and compression. Compared to previous methods that used uniform partitioning, StreamKV dynamically partitions video streams into semantic segments, which better preserves semantic information. For KV cache retrieval, StreamKV calculates a summary vector for each segment to retain segment-level information essential for retrieval. For KV cache compression, StreamKV introduces a guidance prompt designed to capture the key semantic elements within each segment, ensuring only the most informative KV caches are retained for answering questions. Moreover, StreamKV unifies KV cache retrieval and compression within a single module, performing both in a layer-adaptive manner, thereby further improving the effectiveness of streaming video question answering. Extensive experiments on public StreamingVQA benchmarks demonstrate that StreamKV significantly outperforms existing Online Video-LLMs, achieving superior accuracy while substantially improving both memory efficiency and computational latency. The code has been released at https://github.com/sou1p0wer/StreamKV.",
        "translated": "视频大语言模型（Video-LLMs）在视频字幕生成、搜索与摘要等任务中展现出显著潜力。然而，当前的 Video-LLMs 仍面临处理长时现实视频的挑战。近期方法引入了检索机制，通过检索与查询相关的 KV 缓存以提升问答效率和准确性，从而更好地应对长时真实视频。但目前对 KV 缓存的压缩与检索仍未充分探索。本文提出 **StreamKV**，一种无需训练的框架，可无缝为 Video-LLMs 配置先进的 KV 缓存检索与压缩能力。相较以往采用均匀划分的方法，StreamKV 动态将视频流划分为语义段落，更有效地保留语义信息。在 KV 缓存检索方面，StreamKV 为每个段落计算一个摘要向量，以保留对检索至关重要的段级信息；在 KV 缓存压缩方面，StreamKV 引入一种引导提示（guidance prompt），用于捕获各段落中的关键语义元素，确保仅保留最具信息量的 KV 缓存以回答问题。此外，StreamKV 将 KV 缓存检索与压缩统一于单个模块内，并以层自适应方式同时执行两者，进一步提升了流式视频问答的有效性。在公开的 StreamingVQA 数据集上进行的大量实验表明，StreamKV 显著优于现有在线 Video-LLMs，在保持高准确率的同时大幅提升了内存效率与计算延迟性能。代码已开源于 https://github.com/sou1p0wer/StreamKV。",
        "translated_title": "StreamKV：基于分段键值缓存的流式视频问答  \n检索与压缩",
        "label": [],
        "label_reason": "属于视频问答高阶任务，非图像处理",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出层适应性KV压缩框架，提升效率"
    },
    {
        "title": "Q-RAG: Long Context Multi-step Retrieval via Value-based Embedder\n  Training",
        "url": "http://arxiv.org/abs/2511.07328v1",
        "pub_date": "2025-11-10",
        "summary": "Retrieval-Augmented Generation (RAG) methods enhance LLM performance by efficiently filtering relevant context for LLMs, reducing hallucinations and inference cost. However, most existing RAG methods focus on single-step retrieval, which is often insufficient for answering complex questions that require multi-step search. Recently, multi-step retrieval approaches have emerged, typically involving the fine-tuning of small LLMs to perform multi-step retrieval. This type of fine-tuning is highly resource-intensive and does not enable the use of larger LLMs. In this work, we propose Q-RAG, a novel approach that fine-tunes the Embedder model for multi-step retrieval using reinforcement learning (RL). Q-RAG offers a competitive, resource-efficient alternative to existing multi-step retrieval methods for open-domain question answering and achieves state-of-the-art results on the popular long-context benchmarks Babilong and RULER for contexts up to 10M tokens.",
        "translated": "检索增强生成（RAG）方法通过高效过滤与大语言模型（LLM）相关的上下文内容，提升其性能，降低幻觉现象和推理成本。然而，现有大多数 RAG 方法聚焦于单步检索，往往不足以应对需要多步搜索的复杂问题。近期，多步检索方法逐渐兴起，通常涉及对小型 LLM 进行微调以执行多步检索。此类微调过程资源消耗极高，且无法支持更大规模 LLM 的应用。在本文中，我们提出 Q-RAG，一种新颖的方法，利用强化学习（RL）对 Embedder 模型进行微调，以实现多步检索。Q-RAG 为现有的多步检索方法提供了一种具有竞争力且资源高效的替代方案，适用于开放域问答任务，并在流行的大上下文基准评测集 Babilong 和 RULER 上（支持长达 10M tokens 的上下文），取得了当前最优性能。",
        "translated_title": "Q-RAG：基于价值嵌入的长上下文多步检索训练",
        "label": [
            "LLM生成式推荐",
            "通用推荐技术"
        ],
        "label_reason": "方法用于LLM问答，非推荐系统核心环节",
        "relevance_score": 4,
        "novelty_score": 7,
        "novelty_reason": "RL优化Embedder，提升多步检索效率"
    },
    {
        "title": "Hard vs. Noise: Resolving Hard-Noisy Sample Confusion in Recommender\n  Systems via Large Language Models",
        "url": "http://arxiv.org/abs/2511.07295v1",
        "pub_date": "2025-11-10",
        "summary": "Implicit feedback, employed in training recommender systems, unavoidably confronts noise due to factors such as misclicks and position bias. Previous studies have attempted to identify noisy samples through their diverged data patterns, such as higher loss values, and mitigate their influence through sample dropping or reweighting. However, we observed that noisy samples and hard samples display similar patterns, leading to hard-noisy confusion issue. Such confusion is problematic as hard samples are vital for modeling user preferences. To solve this problem, we propose LLMHNI framework, leveraging two auxiliary user-item relevance signals generated by Large Language Models (LLMs) to differentiate hard and noisy samples. LLMHNI obtains user-item semantic relevance from LLM-encoded embeddings, which is used in negative sampling to select hard negatives while filtering out noisy false negatives. An objective alignment strategy is proposed to project LLM-encoded embeddings, originally for general language tasks, into a representation space optimized for user-item relevance modeling. LLMHNI also exploits LLM-inferred logical relevance within user-item interactions to identify hard and noisy samples. These LLM-inferred interactions are integrated into the interaction graph and guide denoising with cross-graph contrastive alignment. To eliminate the impact of unreliable interactions induced by LLM hallucination, we propose a graph contrastive learning strategy that aligns representations from randomly edge-dropped views to suppress unreliable edges. Empirical results demonstrate that LLMHNI significantly improves denoising and recommendation performance.",
        "translated": "隐式反馈在推荐系统训练中被广泛应用，但不可避免地受到诸如误点击和位置偏差等因素引入的噪声干扰。以往研究尝试通过样本数据模式的差异（如损失值较高）识别噪声样本，并通过样本丢弃或重加权来减轻其影响。然而，我们观察到噪声样本与困难样本表现出相似的模式，导致“硬样本-噪声样本”混淆问题。此类混淆具有严重危害，因为困难样本对建模用户偏好至关重要。为解决该问题，本文提出LLMHNI框架，利用大语言模型（LLMs）生成的两个辅助用户-物料相关性信号，以区分困难样本与噪声样本。LLMHNI从LLM编码的嵌入中获取用户-物料语义相关性，并将其应用于负采样过程：一方面选择困难负样本，另一方面过滤掉虚假的噪声负样本。本文进一步提出一种目标对齐策略，将原本面向通用语言任务设计的LLM编码嵌入投影至优化用于用户-物料相关性建模的表示空间。此外，LLMHNI还利用LLM推断出的用户-物料交互中的逻辑相关性，以识别困难样本与噪声样本。这些LLM推断出的交互关系被整合至交互图中，并借助跨图对比对齐引导去噪过程。为消除由LLM幻觉引发的不可靠交互带来的影响，本文提出一种图对比学习策略：通过对随机边删除视图的表征进行对齐，抑制不可靠边的影响。实证结果表明，LLMHNI显著提升了去噪效果和推荐性能。",
        "translated_title": "硬样本与噪声：通过大语言模型解决推荐系统中硬样本与噪声样本的混淆问题",
        "label": [
            "精排",
            "LLM生成式推荐",
            "负采样与对比学习"
        ],
        "label_reason": "利用LLM区分硬样本与噪声，优化精排与负采样",
        "relevance_score": 9,
        "novelty_score": 8,
        "novelty_reason": "创新性结合LLM语义与图对比学习解决噪声问题"
    },
    {
        "title": "The Value of Personalized Recommendations: Evidence from Netflix",
        "url": "http://arxiv.org/abs/2511.07280v1",
        "pub_date": "2025-11-10",
        "summary": "Personalized recommendation systems shape much of user choice online, yet their targeted nature makes separating out the value of recommendation and the underlying goods challenging. We build a discrete choice model that embeds recommendation-induced utility, low-rank heterogeneity, and flexible state dependence and apply the model to viewership data at Netflix. We exploit idiosyncratic variation introduced by the recommendation algorithm to identify and separately value these components as well as to recover model-free diversion ratios that we can use to validate our structural model. We use the model to evaluate counterfactuals that quantify the incremental engagement generated by personalized recommendations. First, we show that replacing the current recommender system with a matrix factorization or popularity-based algorithm would lead to 4% and 12% reduction in engagement, respectively, and decreased consumption diversity. Second, most of the consumption increase from recommendations comes from effective targeting, not mechanical exposure, with the largest gains for mid-popularity goods (as opposed to broadly appealing or very niche goods).",
        "translated": "个性化推荐系统深刻塑造了用户在线选择，但其目标导向的特性使得分离推荐本身的价值与底层商品价值颇具挑战。我们构建了一个离散选择模型，该模型嵌入了推荐诱导效用、低秩异质性及灵活的状态依赖，并将其应用于Netflix的观看数据。我们利用推荐算法引入的个体化变异，识别并分别度量上述各组成部分的价值，同时恢复无需模型假设的分流比例，以验证我们的结构模型。我们借助该模型评估反事实情境，量化个性化推荐所生成的增量参与度。首先，我们表明若将当前推荐系统替换为矩阵分解或基于流行度的算法，将分别导致参与度下降4%和12%，且消费多样性亦会降低。其次，大多数由推荐带来的消费增长源于精准定向而非机械曝光，其中中等流行度商品（相较于广泛受欢迎或高度小众商品）获得的最大收益。",
        "translated_title": "个性化推荐的价值：来自 Netflix 的证据",
        "label": [
            "推荐系统评估",
            "通用推荐技术"
        ],
        "label_reason": "评估推荐系统价值，非算法设计",
        "relevance_score": 7,
        "novelty_score": 6,
        "novelty_reason": "结构模型改进，但非新推荐方法"
    },
    {
        "title": "When Sufficient is not Enough: Utilizing the Rashomon Effect for\n  Complete Evidence Extraction",
        "url": "http://arxiv.org/abs/2511.07055v1",
        "pub_date": "2025-11-10",
        "summary": "Feature attribution methods typically provide minimal sufficient evidence justifying a model decision. However, in many applications this is inadequate. For compliance and cataloging, the full set of contributing features must be identified - complete evidence. We perform a case study on a medical dataset which contains human-annotated complete evidence. We show that individual models typically recover only subsets of complete evidence and that aggregating evidence from several models improves evidence recall from $\\sim$0.60 (single best model) to $\\sim$0.86 (ensemble). We analyze the recall-precision trade-off, the role of training with evidence, dynamic ensembles with certainty thresholds, and discuss implications.",
        "translated": "特征归因方法通常仅提供最简充分证据以解释模型决策。然而，在许多应用场景中，此类证据尚不充分。为满足合规性与目录化需求，必须识别出所有贡献特征——即完整证据。我们在一个包含人工标注完整证据的医疗数据集上开展案例研究。结果表明，单个模型通常只能恢复完整证据的子集；而将多个模型的证据进行聚合，可使证据召回率从约0.60（单一最优模型）提升至约0.86（集成模型）。我们分析了召回率与精确率之间的权衡关系、基于证据训练的作用、结合置信度阈值的动态集成，并讨论其相关影响。",
        "translated_title": "当“充分”已不足时：利用拉什莫恩效应实现完整证据提取",
        "label": [],
        "label_reason": "研究模型解释性，与推荐系统无直接关联",
        "relevance_score": 3,
        "novelty_score": 5,
        "novelty_reason": "改进证据提取方法，非推荐领域创新"
    },
    {
        "title": "Wavelet Enhanced Adaptive Frequency Filter for Sequential Recommendation",
        "url": "http://arxiv.org/abs/2511.07028v1",
        "pub_date": "2025-11-10",
        "summary": "Sequential recommendation has garnered significant attention for its ability to capture dynamic preferences by mining users' historical interaction data. Given that users' complex and intertwined periodic preferences are difficult to disentangle in the time domain, recent research is exploring frequency domain analysis to identify these hidden patterns. However, current frequency-domain-based methods suffer from two key limitations: (i) They primarily employ static filters with fixed characteristics, overlooking the personalized nature of behavioral patterns; (ii) While the global discrete Fourier transform excels at modeling long-range dependencies, it can blur non-stationary signals and short-term fluctuations. To overcome these limitations, we propose a novel method called Wavelet Enhanced Adaptive Frequency Filter for Sequential Recommendation. Specifically, it consists of two vital modules: dynamic frequency-domain filtering and wavelet feature enhancement. The former is used to dynamically adjust filtering operations based on behavioral sequences to extract personalized global information, and the latter integrates wavelet transform to reconstruct sequences, enhancing blurred non-stationary signals and short-term fluctuations. Finally, these two modules work to achieve comprehensive performance and efficiency optimization in long sequential recommendation scenarios. Extensive experiments on four widely-used benchmark datasets demonstrate the superiority of our work.",
        "translated": "序列推荐因其能够通过挖掘用户的历史交互数据捕捉动态偏好而受到广泛关注。鉴于用户复杂且相互交织的周期性偏好在时域内难以分离，近期研究开始探索频域分析以识别这些隐藏模式。然而，当前基于频域的方法存在两个关键局限：(i) 主要采用具有固定特性的静态滤波器，忽视了行为模式的个性化特性；(ii) 虽然全局离散傅里叶变换擅长建模长程依赖关系，但其易模糊非平稳信号及短期波动。为克服上述局限，我们提出了一种名为“用于序列推荐的波形增强自适应频域滤波器”的新方法。具体而言，该方法包含两个核心模块：动态频域滤波与小波特征增强。前者根据行为序列动态调整滤波操作，提取个性化全局信息；后者结合小波变换重构序列，增强被模糊的非平稳信号和短期波动。最终，这两个模块协同工作，实现对长序列推荐场景下综合性能与效率的优化。在四个广泛使用的基准数据集上的大量实验表明，我们的方法具有优越性。",
        "translated_title": "小波增强自适应频域滤波器用于序列推荐",
        "label": [
            "序列推荐",
            "频率域方法"
        ],
        "label_reason": "提出频域滤波新方法用于序列推荐",
        "relevance_score": 9,
        "novelty_score": 8,
        "novelty_reason": "动态频域+小波增强，解决个性化与非平稳信号问题"
    },
    {
        "title": "Llama-Embed-Nemotron-8B: A Universal Text Embedding Model for\n  Multilingual and Cross-Lingual Tasks",
        "url": "http://arxiv.org/abs/2511.07025v1",
        "pub_date": "2025-11-10",
        "summary": "We introduce llama-embed-nemotron-8b, an open-weights text embedding model that achieves state-of-the-art performance on the Multilingual Massive Text Embedding Benchmark (MMTEB) leaderboard as of October 21, 2025. While recent models show strong performance, their training data or methodologies are often not fully disclosed. We aim to address this by developing a fully open-source model, publicly releasing its weights and detailed ablation studies, and planning to share the curated training datasets. Our model demonstrates superior performance across all major embedding tasks -- including retrieval, classification and semantic textual similarity (STS) -- and excels in challenging multilingual scenarios, such as low-resource languages and cross-lingual setups. This state-of-the-art performance is driven by a novel data mix of 16.1 million query-document pairs, split between 7.7 million samples from public datasets and 8.4 million synthetically generated examples from various open-weight LLMs. One of our key contributions is a detailed ablation study analyzing core design choices, including a comparison of contrastive loss implementations, an evaluation of synthetic data generation (SDG) strategies, and the impact of model merging. The llama-embed-nemotron-8b is an instruction-aware model, supporting user-defined instructions to enhance performance for specific use-cases. This combination of top-tier performance, broad applicability, and user-driven flexibility enables it to serve as a universal text embedding solution.",
        "translated": "我们引入了 llama-embed-nemotron-8b，这是一个开源权重的文本嵌入模型，截至2025年10月21日，在多语言大规模文本嵌入基准（MMTEB）排行榜上取得了最先进性能。尽管近期模型表现出色，但其训练数据或方法论往往未完全公开。为此，我们致力于开发一个完全开源的模型，公开发布其权重和详尽的消融研究，并计划共享经过整理的训练数据集。该模型在所有主要嵌入任务中均展现出卓越性能，包括检索、分类和语义文本相似度（STS），尤其在具有挑战性的多语言场景——如低资源语言与跨语言设置中表现突出。这一最先进的性能得益于一种新颖的数据混合策略：包含1610万组查询-文档对，其中770万样本来自公开数据集，另840万条为从多种开源大语言模型（LLM）中合成生成的示例。我们的核心贡献之一是开展了一项详细消融研究，分析了关键设计选择，包括对比损失实现方式的比较、合成数据生成（SDG）策略的评估，以及模型融合的影响。llama-embed-nemotron-8b 是一个指令感知模型，支持用户自定义指令以增强特定应用场景下的性能。这种顶级性能、广泛适用性与用户驱动灵活性的结合，使其能够作为通用文本嵌入解决方案。",
        "translated_title": "Llama-Embed-Nemotron-8B：一种面向多语言与跨语言任务的通用文本嵌入模型",
        "label": [
            "通用推荐技术"
        ],
        "label_reason": "文本嵌入可支持推荐召回，但非专为推荐设计。",
        "relevance_score": 4,
        "novelty_score": 6,
        "novelty_reason": "改进训练策略与数据混合，属常规模型优化。"
    },
    {
        "title": "CGLE: Class-label Graph Link Estimator for Link Prediction",
        "url": "http://arxiv.org/abs/2511.06982v1",
        "pub_date": "2025-11-10",
        "summary": "Link prediction is a pivotal task in graph mining with wide-ranging applications in social networks, recommendation systems, and knowledge graph completion. However, many leading Graph Neural Network (GNN) models often neglect the valuable semantic information aggregated at the class level. To address this limitation, this paper introduces CGLE (Class-label Graph Link Estimator), a novel framework designed to augment GNN-based link prediction models. CGLE operates by constructing a class-conditioned link probability matrix, where each entry represents the probability of a link forming between two node classes. This matrix is derived from either available ground-truth labels or from pseudo-labels obtained through clustering. The resulting class-based prior is then concatenated with the structural link embedding from a backbone GNN, and the combined representation is processed by a Multi-Layer Perceptron (MLP) for the final prediction. Crucially, CGLE's logic is encapsulated in an efficient preprocessing stage, leaving the computational complexity of the underlying GNN model unaffected. We validate our approach through extensive experiments on a broad suite of benchmark datasets, covering both homophilous and sparse heterophilous graphs. The results show that CGLE yields substantial performance gains over strong baselines such as NCN and NCNC, with improvements in HR@100 of over 10 percentage points on homophilous datasets like Pubmed and DBLP. On sparse heterophilous graphs, CGLE delivers an MRR improvement of over 4% on the Chameleon dataset. Our work underscores the efficacy of integrating global, data-driven semantic priors, presenting a compelling alternative to the pursuit of increasingly complex model architectures. Code to reproduce our findings is available at: https://github.com/data-iitd/cgle-icdm2025.",
        "translated": "链接预测是图挖掘中的核心任务，在社交网络、推荐系统和知识图谱补全等领域具有广泛应用。然而，许多主流的图神经网络（GNN）模型往往忽视了在类别级别聚合的有价值语义信息。为解决这一局限性，本文提出CGLE（Class-label Graph Link Estimator），一种旨在增强基于GNN的链接预测模型的新框架。CGLE通过构建一个类别条件化的链接概率矩阵实现其功能，其中每个元素表示两个节点类别之间形成链接的概率。该矩阵可从现有真实标签获得，或通过聚类生成伪标签。随后，所得到的基于类别的先验信息与骨干GNN提取的结构化链接嵌入进行拼接，再由多层感知机（MLP）处理以输出最终预测结果。关键的是，CGLE的逻辑封装于高效的预处理阶段，从而不影响底层GNN模型的计算复杂度。我们在涵盖同质图与稀疏异质图的广泛基准数据集上进行了大量实验验证。结果表明，CGLE在诸如Pubmed和DBLP等同质图数据集上显著优于强基线模型如NCN和NCNC，HR@100指标提升超过10个百分点；在稀疏异质图Chameleon数据集上，MRR指标提升超过4%。我们的工作强调了整合全局、数据驱动语义先验的有效性，为追求日益复杂的模型架构提供了一种更具吸引力的替代方案。复现本研究结果的代码可在以下地址获取：https://github.com/data-iitd/cgle-icdm2025。",
        "translated_title": "CGLE：用于链接预测的类标签图链接估计器",
        "label": [
            "召回",
            "图神经网络推荐"
        ],
        "label_reason": "引入类标签语义先验增强GNN链接预测，适用于推荐系统召回阶段。",
        "relevance_score": 7,
        "novelty_score": 8,
        "novelty_reason": "创新性构建类条件概率矩阵，高效融合全局语义信息提升模型性能。"
    },
    {
        "title": "Fine-Tuning Diffusion-Based Recommender Systems via Reinforcement\n  Learning with Reward Function Optimization",
        "url": "http://arxiv.org/abs/2511.06937v1",
        "pub_date": "2025-11-10",
        "summary": "Diffusion models recently emerged as a powerful paradigm for recommender systems, offering state-of-the-art performance by modeling the generative process of user-item interactions. However, training such models from scratch is both computationally expensive and yields diminishing returns once convergence is reached. To remedy these challenges, we propose ReFiT, a new framework that integrates Reinforcement learning (RL)-based Fine-Tuning into diffusion-based recommender systems. In contrast to prior RL approaches for diffusion models depending on external reward models, ReFiT adopts a task-aligned design: it formulates the denoising trajectory as a Markov decision process (MDP) and incorporates a collaborative signal-aware reward function that directly reflects recommendation quality. By tightly coupling the MDP structure with this reward signal, ReFiT empowers the RL agent to exploit high-order connectivity for fine-grained optimization, while avoiding the noisy or uninformative feedback common in naive reward designs. Leveraging policy gradient optimization, ReFiT maximizes exact log-likelihood of observed interactions, thereby enabling effective post hoc fine-tuning of diffusion recommenders. Comprehensive experiments on wide-ranging real-world datasets demonstrate that the proposed ReFiT framework (a) exhibits substantial performance gains over strong competitors (up to 36.3% on sequential recommendation), (b) demonstrates strong efficiency with linear complexity in the number of users or items, and (c) generalizes well across multiple diffusion-based recommendation scenarios. The source code and datasets are publicly available at https://anonymous.4open.science/r/ReFiT-4C60.",
        "translated": "扩散模型近期已成为推荐系统中一个强大的范式，通过建模用户-物料交互的生成过程，实现了当前最先进性能。然而，从零开始训练此类模型既计算开销巨大，且在收敛后收益递减。为解决这些挑战，我们提出了 ReFiT，一种将基于强化学习（RL）的微调集成到扩散型推荐系统中的新框架。与此前依赖外部奖励模型的扩散模型 RL 方法不同，ReFiT 采用任务对齐的设计：它将去噪轨迹形式化为马尔可夫决策过程（MDP），并引入一种协同信号感知的奖励函数，直接反映推荐质量。通过紧密耦合 MDP 结构与该奖励信号，ReFiT 使 RL 代理能够利用高阶连接性实现精细化优化，同时避免了朴素奖励设计中常见的噪声或无信息反馈问题。借助策略梯度优化，ReFiT 最大化观测交互的精确对数似然，从而有效支持扩散推荐系统的后处理微调。在广泛真实世界数据集上的全面实验表明，所提出的 ReFiT 框架：(a) 在强竞争方法上展现出显著性能提升（在序列推荐任务中最高达 36.3%），(b) 具备高效性，其复杂度随用户或物品数量呈线性增长，(c) 能够良好泛化至多种基于扩散的推荐场景。源代码和数据集已公开于 https://anonymous.4open.science/r/ReFiT-4C60。",
        "translated_title": "通过强化学习与奖励函数优化微调基于扩散的推荐系统",
        "label": [
            "LLM生成式推荐",
            "精排",
            "重排"
        ],
        "label_reason": "直接改进扩散模型推荐系统，强化学习优化生成过程",
        "relevance_score": 10,
        "novelty_score": 9,
        "novelty_reason": "首次将RL用于扩散模型后训练，奖励函数设计新颖"
    },
    {
        "title": "Have We Really Understood Collaborative Information? An Empirical\n  Investigation",
        "url": "http://arxiv.org/abs/2511.06905v1",
        "pub_date": "2025-11-10",
        "summary": "Collaborative information serves as the cornerstone of recommender systems which typically focus on capturing it from user-item interactions to deliver personalized services. However, current understanding of this crucial resource remains limited. Specifically, a quantitative definition of collaborative information is missing, its manifestation within user-item interactions remains unclear, and its impact on recommendation performance is largely unknown. To bridge this gap, this work conducts a systematic investigation of collaborative information. We begin by clarifying collaborative information in terms of item co-occurrence patterns, identifying its main characteristics, and presenting a quantitative definition. We then estimate the distribution of collaborative information from several aspects, shedding light on how collaborative information is structured in practice. Furthermore, we evaluate the impact of collaborative information on the performance of various recommendation algorithms. Finally, we highlight challenges in effectively capturing collaborative information and outlook promising directions for future research. By establishing an empirical framework, we uncover many insightful observations that advance our understanding of collaborative information and offer valuable guidelines for developing more effective recommender systems.",
        "translated": "协同信息是推荐系统的核心基础，通常通过捕获用户-物料交互数据来提供个性化服务。然而，当前对这一关键资源的理解仍显不足：具体而言，尚缺乏协同信息的量化定义，其在用户-物料交互中的表现形式尚不清晰，且其对推荐性能的影响也大多未知。为弥合这一研究缺口，本文对协同信息开展系统性探究。我们首先从物料共现模式角度厘清协同信息，识别其主要特性并提出量化定义；随后，从多个维度估计协同信息的分布，揭示其在实际场景中的结构特征；进一步地，我们评估协同信息对各类推荐算法性能的影响；最后，我们指出有效捕捉协同信息所面临的挑战，并展望未来研究的可行方向。通过构建实证框架，我们揭示了多项具有启发性的观察结果，深化了对协同信息的理解，并为开发更有效的推荐系统提供了宝贵指导。",
        "translated_title": "我们真的理解了协同信息吗？一项实证研究",
        "label": [
            "通用推荐技术"
        ],
        "label_reason": "系统研究协同信息对推荐性能影响，属基础理论探索",
        "relevance_score": 9,
        "novelty_score": 7,
        "novelty_reason": "首次量化协同信息并评估其对算法的影响"
    },
    {
        "title": "Learning to Fast Unrank in Collaborative Filtering Recommendation",
        "url": "http://arxiv.org/abs/2511.06803v1",
        "pub_date": "2025-11-10",
        "summary": "Modern data-driven recommendation systems risk memorizing sensitive user behavioral patterns, raising privacy concerns. Existing recommendation unlearning methods, while capable of removing target data influence, suffer from inefficient unlearning speed and degraded performance, failing to meet real-time unlearning demands. Considering the ranking-oriented nature of recommendation systems, we present unranking, the process of reducing the ranking positions of target items while ensuring the formal guarantees of recommendation unlearning. To achieve efficient unranking, we propose Learning to Fast Unrank in Collaborative Filtering Recommendation (L2UnRank), which operates through three key stages: (a) identifying the influenced scope via interaction-based p-hop propagation, (b) computing structural and semantic influences for entities within this scope, and (c) performing efficient, ranking-aware parameter updates guided by influence information. Extensive experiments across multiple datasets and backbone models demonstrate L2UnRank's model-agnostic nature, achieving state-of-the-art unranking effectiveness and maintaining recommendation quality comparable to retraining, while also delivering a 50x speedup over existing methods. Codes are available at https://github.com/Juniper42/L2UnRank.",
        "translated": "现代数据驱动的推荐系统存在记忆敏感用户行为模式的风险，引发隐私担忧。现有推荐遗忘方法虽能去除目标数据的影响，但存在遗忘效率低下、性能下降的问题，难以满足实时遗忘需求。鉴于推荐系统的排序导向特性，我们提出“去排序”（unranking），即在确保推荐遗忘形式化保证的前提下，降低目标物品的排序位置。为实现高效去排序，我们提出《协同过滤推荐中的快速学习式去排序》（Learning to Fast Unrank in Collaborative Filtering Recommendation, L2UnRank），该方法通过三个关键阶段运作：(a) 通过基于交互的 p-hop 传播识别受影响范围；(b) 计算该范围内实体的结构与语义影响；(c) 根据影响信息执行高效且排序感知的参数更新。大量实验在多个数据集和骨干模型上表明，L2UnRank 具有模型无关性，在保持与重训练相当的推荐质量的同时，达到当前最优的去排序效果，并相较现有方法提速50倍。代码开源于 https://github.com/Juniper42/L2UnRank。",
        "translated_title": "学习在协同过滤推荐中快速去排序",
        "label": [
            "推荐系统公平性/可解释性",
            "负采样与对比学习"
        ],
        "label_reason": "聚焦推荐系统中敏感数据遗忘的高效实现",
        "relevance_score": 9,
        "novelty_score": 8,
        "novelty_reason": "提出模型无关且高效的影响传播与参数更新机制"
    },
    {
        "title": "Accessibility Gaps in U.S. Government Dashboards for Blind and\n  Low-Vision Residents",
        "url": "http://arxiv.org/abs/2511.06688v1",
        "pub_date": "2025-11-10",
        "summary": "Public dashboards are now a common way for US government agencies to share high stakes information with residents. We audited six live systems at federal, state, and city levels: CDC respiratory illness, HUD homelessness PIT and HIC, California HCD Annual Progress Report, New York City Mayor's Management Report, Houston Permitting, and Chicago public health and budget dashboards. Using a rubric based on screen reader needs and WCAG, we checked five items: (1) discoverability of key metrics by assistive tech, (2) keyboard access without mouse hover, (3) clear semantic labels for axes, series, and categories, (4) short plain language status and trend notes, and (5) machine readable tables or CSVs that mirror what sighted users see. Findings are mixed. Many charts fail basic discoverability or depend on hover, which blocks keyboard and screen reader use. Plain language summaries are common in CDC and Chicago, but rare in HUD and Houston. Machine readable data is strong for NYC, California, and HUD; it is weaker or unclear for Houston. Several sites promise service for the public or for customers yet do not name accessibility in their descriptions. Across systems we also observe urgency inversion: faster, operational dashboards tend to provide fewer accessible affordances than slower accountability dashboards. These patterns matter for equal participation and for ADA Title II compliance that references WCAG 2.1 AA. We propose three steps for any public dashboard: add a brief status and trend text at the same update cadence, publish a matching table or CSV of the visual metrics, and state an explicit accessibility commitment.",
        "translated": "公共仪表板现已成为美国政府机构向居民共享重要信息的常见方式。我们审计了六个处于运行状态的系统，涵盖联邦、州及市级层级：CDC呼吸道疾病、HUD无家可归人口点查（PIT）与HIC、加州HCD年度进展报告、纽约市市长管理报告、休斯顿许可审批系统以及芝加哥公共卫生与预算仪表板。我们依据屏幕阅读器使用需求和WCAG标准制定评分准则，检查以下五个项目：(1) 关键指标对辅助技术的可发现性；(2) 无需鼠标悬停即可通过键盘访问；(3) 坐标轴、数据序列与类别的语义清晰标签；(4) 简洁明了的通俗语言状态与趋势说明；(5) 可机读的表格或CSV文件，其内容需与视力用户所见一致。结果呈现混合态势。多数图表在基本可发现性方面表现不足，或依赖悬停功能，从而阻碍了键盘及屏幕阅读器的使用。通俗语言摘要在CDC和芝加哥较为普遍，但在HUD和休斯顿则罕见。机器可读数据在纽约市、加州及HUD表现强劲；而在休斯顿则较弱或不明确。多个网站虽宣称服务公众或客户，却未在其描述中提及无障碍访问。在各类系统中，我们还观察到“紧急程度倒置”现象：操作响应迅速的运营型仪表板往往提供的无障碍功能少于节奏较慢的问责型仪表板。这些模式关乎平等参与，并影响《美国残疾人法案》第二条关于引用WCAG 2.1 AA标准的合规性。我们建议所有公共仪表板应采取三步措施：在相同更新频率下添加简要的状态与趋势文本，发布与可视化指标匹配的表格或CSV文件，并明确声明无障碍访问承诺。",
        "translated_title": "美国政府仪表板对盲人和低视力居民的可访问性差距",
        "label": [],
        "label_reason": "论文聚焦政府数据可访问性，无关推荐系统",
        "relevance_score": 1,
        "novelty_score": 1,
        "novelty_reason": "无推荐算法创新，属无障碍设计研究"
    },
    {
        "title": "When Evidence Contradicts: Toward Safer Retrieval-Augmented Generation\n  in Healthcare",
        "url": "http://arxiv.org/abs/2511.06668v1",
        "pub_date": "2025-11-10",
        "summary": "In high-stakes information domains such as healthcare, where large language models (LLMs) can produce hallucinations or misinformation, retrieval-augmented generation (RAG) has been proposed as a mitigation strategy, grounding model outputs in external, domain-specific documents. Yet, this approach can introduce errors when source documents contain outdated or contradictory information. This work investigates the performance of five LLMs in generating RAG-based responses to medicine-related queries. Our contributions are three-fold: i) the creation of a benchmark dataset using consumer medicine information documents from the Australian Therapeutic Goods Administration (TGA), where headings are repurposed as natural language questions, ii) the retrieval of PubMed abstracts using TGA headings, stratified across multiple publication years, to enable controlled temporal evaluation of outdated evidence, and iii) a comparative analysis of the frequency and impact of outdated or contradictory content on model-generated responses, assessing how LLMs integrate and reconcile temporally inconsistent information. Our findings show that contradictions between highly similar abstracts do, in fact, degrade performance, leading to inconsistencies and reduced factual accuracy in model answers. These results highlight that retrieval similarity alone is insufficient for reliable medical RAG and underscore the need for contradiction-aware filtering strategies to ensure trustworthy responses in high-stakes domains.",
        "translated": "在医疗等高风险信息领域，大语言模型（LLMs）可能产生幻觉或错误信息，因此检索增强生成（RAG）被提出作为一种缓解策略，通过将模型输出锚定于外部、特定领域的文档。然而，当源文档包含过时或相互矛盾的信息时，该方法可能引入错误。本文研究了五种 LLMs 在针对医学相关查询生成 RAG 基础响应时的表现。我们的贡献有三方面：i) 利用澳大利亚治疗用品管理局（TGA）的消费者药品信息文档构建基准数据集，其中标题被重新用于作为自然语言问题；ii) 采用 TGA 标题从 PubMed 中检索摘要，并按多个发表年份分层，以实现对过时证据进行可控的时间维度评估；iii) 对比分析过时或相互矛盾内容在模型生成响应中出现的频率及其影响，评估 LLMs 如何整合并协调时间上不一致的信息。研究结果表明，高度相似摘要之间的矛盾确会降低模型性能，导致模型答案出现不一致性和事实准确性下降。这些结果说明，仅依赖检索相似性不足以保障医学领域 RAG 的可靠性，并凸显了需要引入矛盾感知过滤机制的重要性，以确保在高风险领域内生成值得信赖的响应。",
        "translated_title": "当证据相互矛盾时：迈向更安全的医疗领域检索增强生成",
        "label": [
            "LLM生成式推荐",
            "负采样与对比学习"
        ],
        "label_reason": "研究LLM在医疗RAG中处理矛盾信息，间接关联推荐系统可信排序。",
        "relevance_score": 4,
        "novelty_score": 6,
        "novelty_reason": "改进RAG检索策略，但未专为推荐系统设计。"
    },
    {
        "title": "Can LLM Annotations Replace User Clicks for Learning to Rank?",
        "url": "http://arxiv.org/abs/2511.06635v1",
        "pub_date": "2025-11-10",
        "summary": "Large-scale supervised data is essential for training modern ranking models, but obtaining high-quality human annotations is costly. Click data has been widely used as a low-cost alternative, and with recent advances in large language models (LLMs), LLM-based relevance annotation has emerged as another promising annotation. This paper investigates whether LLM annotations can replace click data for learning to rank (LTR) by conducting a comprehensive comparison across multiple dimensions. Experiments on both a public dataset, TianGong-ST, and an industrial dataset, Baidu-Click, show that click-supervised models perform better on high-frequency queries, while LLM annotation-supervised models are more effective on medium- and low-frequency queries. Further analysis shows that click-supervised models are better at capturing document-level signals such as authority or quality, while LLM annotation-supervised models are more effective at modeling semantic matching between queries and documents and at distinguishing relevant from non-relevant documents. Motivated by these observations, we explore two training strategies -- data scheduling and frequency-aware multi-objective learning -- that integrate both supervision signals. Both approaches enhance ranking performance across queries at all frequency levels, with the latter being more effective. Our code is available at https://github.com/Trustworthy-Information-Access/LLMAnn_Click.",
        "translated": "大规模监督数据对于训练现代排序模型至关重要，但获取高质量人工标注成本高昂。点击数据已被广泛用作低成本替代方案；随着大语言模型（LLM）的最新进展，基于LLM的相关性标注也已成为另一种有前景的标注方式。本文通过多维度全面比较，探究LLM标注是否能够替代点击数据用于学习排序（LTR）。在公开数据集TianGong-ST与工业数据集Baidu-Click上的实验表明：点击监督模型在高频查询上表现更优，而LLM标注监督模型在中频和低频查询上更具优势。进一步分析显示，点击监督模型更擅长捕捉文档级别的信号（如权威性或质量），而LLM标注监督模型则更有效建模查询与文档之间的语义匹配，并能更好地区分相关与不相关的文档。受此启发，我们探索了两种结合两类监督信号的训练策略——数据调度与频率感知多目标学习。两种方法均提升了各类频率查询下的排序性能，其中后者效果更为显著。我们的代码已开源于 https://github.com/Trustworthy-Information-Access/LLMAnn_Click。",
        "translated_title": "大语言模型注释能否替代用户点击用于排序学习？",
        "label": [
            "精排（Ranking）",
            "LLM生成式推荐（LLM-based Generative Recommendation）"
        ],
        "label_reason": "研究LLM标注在LTR中的应用，直接关联精排与LLM推荐。",
        "relevance_score": 9,
        "novelty_score": 7,
        "novelty_reason": "提出混合监督策略，结合LLM与点击信号提升排序效果。"
    },
    {
        "title": "TabRAG: Tabular Document Retrieval via Structured Language\n  Representations",
        "url": "http://arxiv.org/abs/2511.06582v1",
        "pub_date": "2025-11-10",
        "summary": "Ingesting data for Retrieval-Augmented Generation (RAG) involves either fine-tuning the embedding model directly on the target corpus or parsing documents for embedding model encoding. The former, while accurate, incurs high computational hardware requirements, while the latter suffers from suboptimal performance when extracting tabular data. In this work, we address the latter by presenting TabRAG, a parsing-based RAG pipeline designed to tackle table-heavy documents via structured language representations. TabRAG outperforms existing popular parsing-based methods for generation and retrieval. Code is available at https://github.com/jacobyhsi/TabRAG.",
        "translated": "为检索增强生成（RAG）摄入数据，通常采用两种方式：其一是在目标语料库上直接微调嵌入模型；其二则是对文档进行解析，以供嵌入模型编码。前者虽然准确度高，但计算硬件需求高昂；后者在抽取表格数据时则表现欠佳。本文针对后者提出TabRAG，这是一种基于解析的RAG管道，旨在通过结构化语言表示处理表格密集型文档。TabRAG在生成与检索任务中均优于现有主流的解析类方法。代码已开源，详见 https://github.com/jacobyhsi/TabRAG。",
        "translated_title": "TabRAG：通过结构化语言表示实现表格文档检索",
        "label": [],
        "label_reason": "聚焦表格文档检索，非推荐系统核心问题",
        "relevance_score": 3,
        "novelty_score": 6,
        "novelty_reason": "改进解析方法，但无推荐系统创新"
    },
    {
        "title": "TOOL4POI: A Tool-Augmented LLM Framework for Next POI Recommendation",
        "url": "http://arxiv.org/abs/2511.06405v1",
        "pub_date": "2025-11-09",
        "summary": "Next Point-of-Interest (POI) recommendation is a fundamental task in location-based services. While recent advances leverage Large Language Model (LLM) for sequential modeling, existing LLM-based approaches face two key limitations: (i) strong reliance on the contextual completeness of user histories, resulting in poor performance on out-of-history (OOH) scenarios; (ii) limited scalability, due to the restricted context window of LLMs, which limits their ability to access and process a large number of candidate POIs. To address these challenges, we propose Tool4POI, a novel tool-augmented framework that enables LLMs to perform open-set POI recommendation through external retrieval and reasoning. Tool4POI consists of three key modules: preference extraction module, multi-turn candidate retrieval module, and reranking module, which together summarize long-term user interests, interact with external tools to retrieve relevant POIs, and refine final recommendations based on recent behaviors. Unlike existing methods, Tool4POI requires no task-specific fine-tuning and is compatible with off-the-shelf LLMs in a plug-and-play manner. Extensive experiments on three real-world datasets show that Tool4POI substantially outperforms state-of-the-art baselines, achieving up to 40% accuracy on challenging OOH scenarios where existing methods fail, and delivering average improvements of 20% and 30% on Acc@5 and Acc@10, respectively.",
        "translated": "下一个兴趣点（POI）推荐是基于位置服务中的基础任务。尽管近期研究利用大语言模型（LLM）进行序列建模，现有基于LLM的方法面临两个关键限制：(i) 强依赖用户历史上下文的完整性，导致在超出历史（OOH）场景下性能较差；(ii) 受限于LLM上下文窗口的规模，难以访问和处理大量候选POI，因而存在可扩展性不足的问题。为应对这些挑战，我们提出了Tool4POI，一个新颖的工具增强框架，通过外部检索与推理使LLM能够执行开放集POI推荐。Tool4POI包含三个核心模块：偏好提取模块、多轮候选检索模块和重排模块，共同实现对用户长期兴趣的归纳、借助外部工具检索相关POI，并基于近期行为优化最终推荐结果。与现有方法不同，Tool4POI无需特定任务微调，且以即插即用方式兼容现成LLM。在三个真实数据集上的广泛实验表明，Tool4POI显著优于当前最先进基线方法，在具有挑战性的OOH场景中准确率提升高达40%（现有方法在此类场景下失败），并在Acc@5与Acc@10指标上分别平均提升20%与30%。",
        "translated_title": "TOOL4POI：一种增强工具的大语言模型框架用于下一兴趣点推荐",
        "label": [
            "LLM生成式推荐",
            "重排",
            "序列推荐"
        ],
        "label_reason": "基于LLM的POI推荐，含召回与重排模块",
        "relevance_score": 9,
        "novelty_score": 8,
        "novelty_reason": "工具增强框架，解决OOH与可扩展性问题"
    },
    {
        "title": "HyMoERec: Hybrid Mixture-of-Experts for Sequential Recommendation",
        "url": "http://arxiv.org/abs/2511.06388v1",
        "pub_date": "2025-11-09",
        "summary": "We propose HyMoERec, a novel sequential recommendation framework that addresses the limitations of uniform Position-wise Feed-Forward Networks in existing models. Current approaches treat all user interactions and items equally, overlooking the heterogeneity in user behavior patterns and diversity in item complexity. HyMoERec initially introduces a hybrid mixture-of-experts architecture that combines shared and specialized expert branches with an adaptive expert fusion mechanism for the sequential recommendation task. This design captures diverse reasoning for varied users and items while ensuring stable training. Experiments on MovieLens-1M and Beauty datasets demonstrate that HyMoERec consistently outperforms state-of-the-art baselines.",
        "translated": "我们提出 HyMoERec，一种新型的序列推荐框架，旨在解决现有模型中统一位置前馈网络的局限性。当前方法将所有用户交互与物料视为同等重要，忽视了用户行为模式的异质性以及物料复杂度的多样性。HyMoERec 首先引入了一种混合专家架构，结合共享与专用专家分支，并配备自适应专家融合机制以应对序列推荐任务。该设计在捕捉不同用户与物料多样化推理的同时，确保训练过程的稳定性。在 MovieLens-1M 和 Beauty 数据集上的实验表明，HyMoERec 一致优于现有最先进基线模型。",
        "translated_title": "HyMoERec：面向序列推荐的混合专家模型",
        "label": [
            "序列推荐",
            "通用推荐技术"
        ],
        "label_reason": "专为序列推荐设计，改进专家混合架构",
        "relevance_score": 9,
        "novelty_score": 8,
        "novelty_reason": "创新性融合专家机制，提升多样性建模能力"
    },
    {
        "title": "Exploiting Inter-Session Information with Frequency-enhanced Dual-Path\n  Networks for Sequential Recommendation",
        "url": "http://arxiv.org/abs/2511.06285v1",
        "pub_date": "2025-11-09",
        "summary": "Sequential recommendation (SR) aims to predict a user's next item preference by modeling historical interaction sequences. Recent advances often integrate frequency-domain modules to compensate for self-attention's low-pass nature by restoring the high-frequency signals critical for personalized recommendations. Nevertheless, existing frequency-aware solutions process each session in isolation and optimize exclusively with time-domain objectives. Consequently, they overlook cross-session spectral dependencies and fail to enforce alignment between predicted and actual spectral signatures, leaving valuable frequency information under-exploited. To this end, we propose FreqRec, a Frequency-Enhanced Dual-Path Network for sequential Recommendation that jointly captures inter-session and intra-session behaviors via a learnable Frequency-domain Multi-layer Perceptrons. Moreover, FreqRec is optimized under a composite objective that combines cross entropy with a frequency-domain consistency loss, explicitly aligning predicted and true spectral signatures. Extensive experiments on three benchmarks show that FreqRec surpasses strong baselines and remains robust under data sparsity and noisy-log conditions.",
        "translated": "序列推荐（SR）旨在通过建模用户的历史交互序列来预测其下一物品偏好。近期研究常引入频域模块，以弥补自注意力机制的低通特性，恢复对个性化推荐至关重要的高频信号。然而，现有频域感知方法通常独立处理每个会话，并仅以时域目标进行优化，因而忽略了跨会话的频谱依赖关系，且未能强制对齐预测与真实频谱特征，导致宝贵的频域信息被严重低估。为此，我们提出 FreqRec，一种面向序列推荐的频域增强双路径网络，通过可学习的频域多层感知机联合捕捉跨会话与内会话行为。此外，FreqRec 在复合目标函数下进行优化，该目标结合了交叉熵损失与频域一致性损失，显式对齐预测频谱与真实频谱签名。在三个基准数据集上的大量实验表明，FreqRec 超越多个强基线模型，并在数据稀疏性和噪声日志条件下保持鲁棒性。",
        "translated_title": "利用频率增强的双通道网络挖掘跨会话信息以实现序列推荐",
        "label": [
            "序列推荐",
            "频率增强"
        ],
        "label_reason": "聚焦序列推荐中频域建模，提升跨会话信息利用。",
        "relevance_score": 9,
        "novelty_score": 8,
        "novelty_reason": "引入频域一致性损失与双路径结构，显著改进频谱对齐。"
    },
    {
        "title": "LLaDA-Rec: Discrete Diffusion for Parallel Semantic ID Generation in\n  Generative Recommendation",
        "url": "http://arxiv.org/abs/2511.06254v1",
        "pub_date": "2025-11-09",
        "summary": "Generative recommendation represents each item as a semantic ID, i.e., a sequence of discrete tokens, and generates the next item through autoregressive decoding. While effective, existing autoregressive models face two intrinsic limitations: (1) unidirectional constraints, where causal attention restricts each token to attend only to its predecessors, hindering global semantic modeling; and (2) error accumulation, where the fixed left-to-right generation order causes prediction errors in early tokens to propagate to the predictions of subsequent token. To address these issues, we propose LLaDA-Rec, a discrete diffusion framework that reformulates recommendation as parallel semantic ID generation. By combining bidirectional attention with the adaptive generation order, the approach models inter-item and intra-item dependencies more effectively and alleviates error accumulation. Specifically, our approach comprises three key designs: (1) a parallel tokenization scheme that produces semantic IDs for bidirectional modeling, addressing the mismatch between residual quantization and bidirectional architectures; (2) two masking mechanisms at the user-history and next-item levels to capture both inter-item sequential dependencies and intra-item semantic relationships; and (3) an adapted beam search strategy for adaptive-order discrete diffusion decoding, resolving the incompatibility of standard beam search with diffusion-based generation. Experiments on three real-world datasets show that LLaDA-Rec consistently outperforms both ID-based and state-of-the-art generative recommenders, establishing discrete diffusion as a new paradigm for generative recommendation.",
        "translated": "生成式推荐将每个物料表示为一个语义ID，即一系列离散token，并通过自回归解码生成下一个物料。尽管效果显著，现有自回归模型面临两个固有局限：(1) 单向约束，因果注意力机制限制每个token仅能关注其前序token，阻碍全局语义建模；(2) 错误累积，固定的左到右生成顺序导致早期token的预测误差传播至后续token的预测中。为解决上述问题，我们提出LLaDA-Rec，一种离散扩散框架，将推荐重述为并行语义ID生成。通过结合双向注意力与自适应生成顺序，该方法更有效地建模物料间及物料内依赖关系，并缓解错误累积。具体而言，我们的方法包含三个关键设计：(1) 一种并行分词方案，为双向建模生成语义ID，解决残差量化与双向架构之间的不匹配问题；(2) 在用户历史和下一物料层面设置两种掩码机制，以捕捉物料间的序列依赖关系和物料内的语义关联；(3) 一种针对自适应顺序离散扩散解码的改进束搜索策略，解决标准束搜索与基于扩散的生成方法的兼容性问题。在三个真实世界数据集上的实验表明，LLaDA-Rec在ID类推荐器和当前最先进的生成式推荐器上均表现更优，确立了离散扩散作为生成式推荐的新范式。",
        "translated_title": "LLaDA-Rec：生成式推荐中并行语义ID生成的离散扩散方法",
        "label": [
            "LLM生成式推荐",
            "重排"
        ],
        "label_reason": "直接提出生成式推荐新范式：离散扩散模型",
        "relevance_score": 10,
        "novelty_score": 10,
        "novelty_reason": "首创离散扩散框架解决生成式推荐瓶颈"
    },
    {
        "title": "Time Matters: A Novel Real-Time Long- and Short-term User Interest Model\n  for Click-Through Rate Prediction",
        "url": "http://arxiv.org/abs/2511.06213v1",
        "pub_date": "2025-11-09",
        "summary": "Click-Through Rate (CTR) prediction is a core task in online personalization platform. A key step for CTR prediction is to learn accurate user representation to capture their interests. Generally, the interest expressed by a user is time-variant, i.e., a user activates different interests at different time. However, most previous CTR prediction methods overlook the correlation between the activated interest and the occurrence time, resulting in what they actually learn is the mixture of the interests expressed by the user at all time, rather than the real-time interest at the certain prediction time. To capture the correlation between the activated interest and the occurrence time, in this paper we investigate users' interest evolution from the perspective of the whole time line and develop two regular patterns: periodic pattern and time-point pattern. Based on the two patterns, we propose a novel time-aware long- and short-term user interest modeling method to model users' dynamic interests at different time. Extensive experiments on public datasets as well as an industrial dataset verify the effectiveness of exploiting the two patterns and demonstrate the superiority of our proposed method compared with other state-of-the-art ones.",
        "translated": "点击率（CTR）预测是在线个性化平台的核心任务。CTR预测的关键步骤在于学习准确的用户表示，以捕捉其兴趣。通常情况下，用户的兴趣具有时间变化性，即用户在不同时间激活不同的兴趣。然而，大多数先前的CTR预测方法忽视了激活兴趣与发生时间之间的关联，导致其实际学习的是用户在所有时间段所表达兴趣的混合体，而非特定预测时刻的真实兴趣。为捕捉激活兴趣与发生时间之间的关联，本文从整体时间线视角研究用户兴趣演化，并提出两种规律模式：周期模式与时点模式。基于这两种模式，我们提出了一种新颖的时间感知长短期用户兴趣建模方法，用于刻画用户在不同时刻的动态兴趣。在公开数据集及工业数据集上的大量实验验证了利用这两种模式的有效性，并表明所提方法相较其他最新方法具有优越性。",
        "translated_title": "时间至关重要：一种用于点击率预测的新型实时长短期用户兴趣模型",
        "label": [
            "精排",
            "通用推荐技术"
        ],
        "label_reason": "聚焦CTR预测中的时序兴趣建模，属精排核心任务",
        "relevance_score": 9,
        "novelty_score": 8,
        "novelty_reason": "提出周期与时间点双模式建模，有效捕捉动态兴趣"
    },
    {
        "title": "MemoriesDB: A Temporal-Semantic-Relational Database for Long-Term Agent\n  Memory / Modeling Experience as a Graph of Temporal-Semantic Surfaces",
        "url": "http://arxiv.org/abs/2511.06179v1",
        "pub_date": "2025-11-09",
        "summary": "We introduce MemoriesDB, a unified data architecture designed to avoid decoherence across time, meaning, and relation in long-term computational memory. Each memory is a time-semantic-relational entity-a structure that simultaneously encodes when an event occurred, what it means, and how it connects to other events. Built initially atop PostgreSQL with pgvector extensions, MemoriesDB combines the properties of a time-series datastore, a vector database, and a graph system within a single append-only schema. Each memory is represented as a vertex uniquely labeled by its microsecond timestamp and accompanied by low- and high-dimensional normalized embeddings that capture semantic context. Directed edges between memories form labeled relations with per-edge metadata, enabling multiple contextual links between the same vertices. Together these constructs form a time-indexed stack of temporal-semantic surfaces, where edges project as directional arrows in a 1+1-dimensional similarity field, tracing the evolution of meaning through time while maintaining cross-temporal coherence. This formulation supports efficient time-bounded retrieval, hybrid semantic search, and lightweight structural reasoning in a single query path. A working prototype demonstrates scalable recall and contextual reinforcement using standard relational infrastructure, and we discuss extensions toward a columnar backend, distributed clustering, and emergent topic modeling.",
        "translated": "我们引入 MemoriesDB，这是一种统一的数据架构，旨在避免在长期计算记忆中跨时间、语义和关系层面的失相干。每个记忆是一个时间-语义-关系实体——一种同时编码事件发生时间、其含义以及与其他事件关联方式的结构。MemoriesDB 初始构建于 PostgreSQL 之上并集成 pgvector 扩展，其设计将时序数据存储、向量数据库与图系统三者的特性融合于单一的追加-only 模式中。每个记忆以一个顶点表示，该顶点由微秒级时间戳唯一标识，并附带低维与高维归一化嵌入向量，用以捕捉语义上下文。记忆之间的有向边构成带有每边元数据的标记关系，从而允许同一顶点间建立多重上下文链接。这些构造共同形成一个按时间索引的时间-语义表面堆叠，在 1+1 维相似性场中，边表现为方向箭头，追踪含义随时间的演化过程，同时保持跨时间的一致性。该建模支持单次查询路径下的高效时间约束检索、混合语义搜索及轻量级结构推理。一个可运行原型展示了利用标准关系基础设施实现可扩展召回与上下文强化的能力，我们亦探讨了向列式后端、分布式聚类及涌现主题建模等拓展方向。",
        "translated_title": "MemoriesDB：面向长期智能体记忆的时序语义关系数据库 / 将经验建模为时序语义表面的图",
        "label": [],
        "label_reason": "论文聚焦长期记忆建模，非推荐系统核心问题",
        "relevance_score": 3,
        "novelty_score": 7,
        "novelty_reason": "提出新数据架构，但未用于推荐场景"
    },
    {
        "title": "PM-DETR: Domain Adaptive Prompt Memory for Object Detection with Transformers",
        "url": "http://arxiv.org/abs/2307.00313v1",
        "pub_date": "2023-07-01",
        "summary": "The Transformer-based detectors (i.e., DETR) have demonstrated impressive performance on end-to-end object detection. However, transferring DETR to different data distributions may lead to a significant performance degradation. Existing adaptation techniques focus on model-based approaches, which aim to leverage feature alignment to narrow the distribution shift between different domains. In this study, we propose a hierarchical Prompt Domain Memory (PDM) for adapting detection transformers to different distributions. PDM comprehensively leverages the prompt memory to extract domain-specific knowledge and explicitly constructs a long-term memory space for the data distribution, which represents better domain diversity compared to existing methods. Specifically, each prompt and its corresponding distribution value are paired in the memory space, and we inject top M distribution-similar prompts into the input and multi-level embeddings of DETR. Additionally, we introduce the Prompt Memory Alignment (PMA) to reduce the discrepancy between the source and target domains by fully leveraging the domain-specific knowledge extracted from the prompt domain memory. Extensive experiments demonstrate that our method outperforms state-of-the-art domain adaptive object detection methods on three benchmarks, including scene, synthetic to real, and weather adaptation. Codes will be released.",
        "translated": "基于Transformer的检测器（即DETR）在端到端目标检测任务中表现出卓越性能。然而，将DETR迁移到不同数据分布时可能导致显著的性能退化。现有适应技术主要聚焦于模型驱动方法，旨在通过特征对齐缩小不同领域间的分布偏移。在本研究中，我们提出了一种分层提示域记忆（Hierarchical Prompt Domain Memory, PDM），用于使检测Transformer适应不同分布。PDM全面利用提示记忆提取领域特定知识，并显式构建长期记忆空间以表征数据分布，相比现有方法具有更优的领域多样性。具体而言，每个提示及其对应分布值在记忆空间中进行配对，我们将与目标分布最相似的前M个提示注入至DETR的输入及多级嵌入中。此外，我们引入提示记忆对齐（Prompt Memory Alignment, PMA），通过充分挖掘从提示域记忆中提取的领域特定知识，减小源域与目标域之间的差异。大量实验表明，我们的方法在三个基准数据集（场景、合成到真实、天气适应）上均优于当前最先进的领域自适应目标检测方法。代码将在后续公开。",
        "translated_title": "PM-DETR：用于Transformer目标检测的领域自适应提示记忆",
        "label": [],
        "label_reason": "目标检测属高阶视觉任务，非像素级图像处理",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "提出新提示记忆机制提升域适应性能"
    },
    {
        "title": "NLLG Quarterly arXiv Report 09/24: What are the most influential current AI Papers?",
        "url": "http://arxiv.org/abs/2412.12121v1",
        "pub_date": "2024-12-02",
        "summary": "The NLLG (Natural Language Learning &amp; Generation) arXiv reports assist in navigating the rapidly evolving landscape of NLP and AI research across cs.CL, cs.CV, cs.AI, and cs.LG categories. This fourth installment captures a transformative period in AI history - from January 1, 2023, following ChatGPT's debut, through September 30, 2024. Our analysis reveals substantial new developments in the field - with 45% of the top 40 most-cited papers being new entries since our last report eight months ago and offers insights into emerging trends and major breakthroughs, such as novel multimodal architectures, including diffusion and state space models. Natural Language Processing (NLP; cs.CL) remains the dominant main category in the list of our top-40 papers but its dominance is on the decline in favor of Computer vision (cs.CV) and general machine learning (cs.LG). This report also presents novel findings on the integration of generative AI in academic writing, documenting its increasing adoption since 2022 while revealing an intriguing pattern: top-cited papers show notably fewer markers of AI-generated content compared to random samples. Furthermore, we track the evolution of AI-associated language, identifying declining trends in previously common indicators such as \"delve\".",
        "translated": "arXiv 上的 NLLG（自然语言学习与生成）报告有助于梳理 cs.CL、cs.CV、cs.AI 和 cs.LG 等类别的 NLP 与 AI 研究领域快速演进的现状。本第四期报告涵盖了自 2023 年 1 月 1 日 ChatGPT 首次发布以来至 2024 年 9 月 30 日这一人工智能发展史上的关键转型时期。我们的分析显示，该领域出现了大量新进展——在我们上一份报告发布八个月后，前 40 篇被引用次数最高的论文中有 45% 是新增内容，并揭示了新兴趋势与重大突破，例如扩散模型和状态空间模型等新型多模态架构。自然语言处理（NLP；cs.CL）依然是我们前 40 篇高引论文中的主导类别，但其主导地位正逐渐被计算机视觉（cs.CV）和通用机器学习（cs.LG）所取代。此外，本报告还呈现了生成式人工智能在学术写作中整合应用的新发现：自 2022 年以来其采用率持续上升，同时揭示了一个有趣的现象——高引论文中明显少有 AI 生成内容的标识符，相较随机样本而言尤为显著。最后，我们追踪了与人工智能相关术语的语言演变过程，发现诸如“delve”等曾广泛使用的表达词频正在下降。",
        "translated_title": "NLLG 季度 arXiv 报告 09/24：当前最具影响力的 AI 论文有哪些？",
        "label": [],
        "label_reason": "论文聚焦AI领域综述，非图像处理任务。",
        "relevance_score": 1,
        "novelty_score": 1,
        "novelty_reason": "为综述报告，无图像恢复或增强创新。"
    },
    {
        "title": "Revisiting Image Classifier Training for Improved Certified Robust Defense against Adversarial Patches",
        "url": "http://arxiv.org/abs/2306.12610v1",
        "pub_date": "2023-06-22",
        "summary": "Certifiably robust defenses against adversarial patches for image classifiers ensure correct prediction against any changes to a constrained neighborhood of pixels. PatchCleanser arXiv:2108.09135 [cs.CV], the state-of-the-art certified defense, uses a double-masking strategy for robust classification. The success of this strategy relies heavily on the model's invariance to image pixel masking. In this paper, we take a closer look at model training schemes to improve this invariance. Instead of using Random Cutout arXiv:1708.04552v2 [cs.CV] augmentations like PatchCleanser, we introduce the notion of worst-case masking, i.e., selecting masked images which maximize classification loss. However, finding worst-case masks requires an exhaustive search, which might be prohibitively expensive to do on-the-fly during training. To solve this problem, we propose a two-round greedy masking strategy (Greedy Cutout) which finds an approximate worst-case mask location with much less compute. We show that the models trained with our Greedy Cutout improves certified robust accuracy over Random Cutout in PatchCleanser across a range of datasets and architectures. Certified robust accuracy on ImageNet with a ViT-B16-224 model increases from 58.1\\% to 62.3\\% against a 3\\% square patch applied anywhere on the image.",
        "translated": "针对图像分类器的对抗补丁可证鲁棒防御，确保在受约束像素邻域内的任何扰动下仍能正确预测。PatchCleanser（arXiv:2108.09135 [cs.CV]）作为当前最先进的可证防御方法，采用双重掩码策略实现鲁棒分类。该策略的成功高度依赖于模型对图像像素掩码的不变性。本文深入探讨了模型训练方案，旨在提升这种不变性。不同于PatchCleanser所使用的Random Cutout（arXiv:1708.04552v2 [cs.CV]）数据增强方法，我们引入“最坏情况掩码”的概念，即选择最大化分类损失的掩码图像。然而，寻找最坏情况掩码需要进行穷举搜索，这在训练过程中实时执行可能成本过高。为解决此问题，我们提出一种两轮贪心掩码策略（Greedy Cutout），可在显著降低计算开销的前提下近似找到最坏情况掩码位置。实验表明，在多个数据集和网络架构上，使用我们的Greedy Cutout训练的模型相较于PatchCleanser中的Random Cutout，在可证鲁棒准确率方面均有提升。具体而言，在ImageNet数据集上，使用ViT-B16-224模型、面对任意位置叠加3%面积正方形补丁攻击时，其可证鲁棒准确率从58.1%提升至62.3%。",
        "translated_title": "重访图像分类器训练以提升对对抗补丁的认证鲁棒防御",
        "label": [],
        "label_reason": "目标为提升对抗鲁棒性，属高阶视觉任务。",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出近似最坏掩码策略提升模型鲁棒性。"
    },
    {
        "title": "AANet: Attribute Attention Network for Person Re-Identifications",
        "url": "http://arxiv.org/abs/1912.09021v1",
        "pub_date": "2019-12-19",
        "summary": "This paper proposes Attribute Attention Network (AANet), a new architecture that integrates person attributes and attribute attention maps into a classification framework to solve the person re-identification (re-ID) problem. Many person re-ID models typically employ semantic cues such as body parts or human pose to improve the re-ID performance. Attribute information, however, is often not utilized. The proposed AANet leverages on a baseline model that uses body parts and integrates the key attribute information in an unified learning framework. The AANet consists of a global person ID task, a part detection task and a crucial attribute detection task. By estimating the class responses of individual attributes and combining them to form the attribute attention map (AAM), a very strong discriminatory representation is constructed. The proposed AANet outperforms the best state-of-the-art method arXiv:1711.09349v3 [cs.CV] using ResNet-50 by 3.36% in mAP and 3.12% in Rank-1 accuracy on DukeMTMC-reID dataset. On Market1501 dataset, AANet achieves 92.38% mAP and 95.10% Rank-1 accuracy with re-ranking, outperforming arXiv:1804.00216v1 [cs.CV], another state of the art method using ResNet-152, by 1.42% in mAP and 0.47% in Rank-1 accuracy. In addition, AANet can perform person attribute prediction (e.g., gender, hair length, clothing length etc.), and localize the attributes in the query image.",
        "translated": "本文提出了一种属性注意力网络（AANet），这是一种将人物属性及其属性注意力图集成到分类框架中的新型架构，用于解决人物重识别（re-ID）问题。许多人物重识别模型通常利用语义线索（如身体部位或人体姿态）以提升重识别性能，但属性信息往往未被充分利用。所提出的AANet基于一个采用身体部位的基线模型，并在一个统一的学习框架中整合关键属性信息。AANet包含全局人物ID任务、部位检测任务和关键属性检测任务。通过估计各属性类别的响应并将其组合形成属性注意力图（AAM），构建出强大的判别性表示。在DukeMTMC-reID数据集上，与arXiv:1711.09349v3 [cs.CV]中使用ResNet-50的最佳现有方法相比，AANet在mAP指标上提升了3.36%，Rank-1准确率提升了3.12%。在Market1501数据集上，AANet在重排序后实现了92.38%的mAP和95.10%的Rank-1准确率，优于另一项使用ResNet-152的最新方法arXiv:1804.00216v1 [cs.CV]，分别在mAP和Rank-1准确率上高出1.42%和0.47%。此外，AANet还能进行人物属性预测（如性别、发长、衣长等），并在查询图像中定位这些属性。",
        "translated_title": "AANet：用于行人重识别的属性注意力网络",
        "label": [],
        "label_reason": "高阶任务，非像素级图像处理",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "属性融合框架，非图像恢复创新"
    },
    {
        "title": "Adapting HouseDiffusion for conditional Floor Plan generation on Modified Swiss Dwellings dataset",
        "url": "http://arxiv.org/abs/2312.03938v1",
        "pub_date": "2023-12-06",
        "summary": "Automated floor plan generation has recently gained momentum with several methods that have been proposed. The CVAAD Floor Plan Auto-Completion workshop challenge introduced MSD, a new dataset that includes existing structural walls of the building as an additional input constraint. This technical report presents an approach for extending a recent work, HouseDiffusion (arXiv:2211.13287 [cs.CV]), to the MSD dataset. The adaption involves modifying the model's transformer layers to condition on a set of wall lines. The report introduces a pre-processing pipeline to extract wall lines from the binary mask of the building structure provided as input. Additionally, it was found that a data processing procedure that simplifies all room polygons to rectangles leads to better performance. This indicates that future work should explore better representations of variable-length polygons in diffusion models. The code will be made available at a later date.",
        "translated": "自动化平面图生成近年来因多项方法的提出而备受关注。CVAAD“平面图自动补全”工作坊挑战赛引入了MSD数据集，该数据集将建筑物已有的结构墙体作为额外的输入约束条件。本技术报告提出了一种扩展近期工作HouseDiffusion（arXiv:2211.13287 [cs.CV]）以适配MSD数据集的方法。该适配方案涉及修改模型的Transformer层，使其根据一组墙体线条进行条件建模。报告还介绍了一套预处理流程，用于从输入提供的建筑结构二值掩码中提取墙体线条。此外，研究发现，将所有房间多边形简化为矩形的数据处理流程可获得更优性能，这表明未来工作应探索扩散模型中对变长多边形更优的表示方式。相关代码将在稍后公开。",
        "translated_title": "将 HouseDiffusion 适配于在修改后的瑞士住宅数据集上进行条件化户型图生成",
        "label": [],
        "label_reason": "生成楼面图属高阶任务，非像素级图像恢复。",
        "relevance_score": 2,
        "novelty_score": 4,
        "novelty_reason": "微调扩散模型结构，创新度有限。"
    },
    {
        "title": "A Hierarchical Deep Temporal Model for Group Activity Recognition",
        "url": "http://arxiv.org/abs/1511.06040v2",
        "pub_date": "2015-11-19",
        "summary": "In group activity recognition, the temporal dynamics of the whole activity can be inferred based on the dynamics of the individual people representing the activity. We build a deep model to capture these dynamics based on LSTM (long-short term memory) models. To make use of these ob- servations, we present a 2-stage deep temporal model for the group activity recognition problem. In our model, a LSTM model is designed to represent action dynamics of in- dividual people in a sequence and another LSTM model is designed to aggregate human-level information for whole activity understanding. We evaluate our model over two datasets: the collective activity dataset and a new volley- ball dataset. Experimental results demonstrate that our proposed model improves group activity recognition perfor- mance with compared to baseline methods.",
        "translated": "在群体活动识别中，可根据代表该活动的个体人员的动态推断整个活动的时间动态。我们构建了一个基于LSTM（长短期记忆）模型的深度框架以捕捉这些动态。为充分利用上述观察，我们提出了一个两阶段深度时序模型用于解决群体活动识别问题。在我们的模型中，一个LSTM模型被设计用于表示序列中个体人员的动作动态，另一个LSTM模型则用于聚合人类层面的信息以实现对整体活动的理解。我们在两个数据集上评估了所提模型：集体活动数据集和一个新的排球数据集。实验结果表明，与基线方法相比，所提出的模型显著提升了群体活动识别性能。",
        "translated_title": "一种用于群体活动识别的层次化深度时序模型",
        "label": [],
        "label_reason": "高阶行为识别，非像素级图像处理",
        "relevance_score": 1,
        "novelty_score": 3,
        "novelty_reason": "LSTM组合改进，无本质创新"
    },
    {
        "title": "RAUNet: Residual Attention U-Net for Semantic Segmentation of Cataract Surgical Instruments",
        "url": "http://arxiv.org/abs/1909.10360v3",
        "pub_date": "2019-09-23",
        "summary": "Semantic segmentation of surgical instruments plays a crucial role in robot-assisted surgery. However, accurate segmentation of cataract surgical instruments is still a challenge due to specular reflection and class imbalance issues. In this paper, an attention-guided network is proposed to segment the cataract surgical instrument. A new attention module is designed to learn discriminative features and address the specular reflection issue. It captures global context and encodes semantic dependencies to emphasize key semantic features, boosting the feature representation. This attention module has very few parameters, which helps to save memory. Thus, it can be flexibly plugged into other networks. Besides, a hybrid loss is introduced to train our network for addressing the class imbalance issue, which merges cross entropy and logarithms of Dice loss. A new dataset named Cata7 is constructed to evaluate our network. To the best of our knowledge, this is the first cataract surgical instrument dataset for semantic segmentation. Based on this dataset, RAUNet achieves state-of-the-art performance 97.71% mean Dice and 95.62% mean IOU.",
        "translated": "手术器械的语义分割在机器人辅助手术中起着至关重要的作用。然而，由于镜面反射和类别不平衡问题，白内障手术器械的精确分割仍具挑战性。本文提出一种基于注意力引导的网络，用于分割白内障手术器械。设计了一种新的注意力模块，以学习判别性特征并解决镜面反射问题。该模块捕获全局上下文并编码语义依赖关系，以突出关键语义特征，提升特征表示能力。该注意力模块参数量极少，有助于节省内存，因此可灵活嵌入其他网络中。此外，引入了一种混合损失函数来训练网络以应对类别不平衡问题，该损失函数结合了交叉熵与Dice损失的对数形式。构建了一个名为Cata7的新数据集用于评估所提网络。据我们所知，这是首个面向语义分割任务的白内障手术器械数据集。基于该数据集，RAUNet实现了当前最佳性能：平均Dice系数97.71%、平均IoU 95.62%。",
        "translated_title": "RAUNet：用于白内障手术器械语义分割的残差注意力U-Net",
        "label": [],
        "label_reason": "任务为语义分割，属高阶视觉任务",
        "relevance_score": 3,
        "novelty_score": 7,
        "novelty_reason": "新损失函数与注意力模块提升性能"
    },
    {
        "title": "A Topology-Aware Graph Convolutional Network for Human Pose Similarity and Action Quality Assessment",
        "url": "http://arxiv.org/abs/2511.01194v1",
        "pub_date": "2025-11-03",
        "summary": "Action Quality Assessment (AQA) requires fine-grained understanding of human motion and precise evaluation of pose similarity. This paper proposes a topology-aware Graph Convolutional Network (GCN) framework, termed GCN-PSN, which models the human skeleton as a graph to learn discriminative, topology-sensitive pose embeddings. Using a Siamese architecture trained with a contrastive regression objective, our method outperforms coordinate-based baselines and achieves competitive performance on AQA-7 and FineDiving benchmarks. Experimental results and ablation studies validate the effectiveness of leveraging skeletal topology for pose similarity and action quality assessment.",
        "translated": "动作质量评估（AQA）需要对人类运动进行细粒度理解，并精确评估姿态相似性。本文提出了一种拓扑感知的图卷积网络（GCN）框架，称为 GCN-PSN，该框架将人体骨架建模为图结构，以学习具有判别能力且拓扑敏感的姿态嵌入表示。通过采用基于 Siamese 架构并使用对比回归目标进行训练的方法，我们的方法优于基于坐标的基线方法，并在 AQA-7 和 FineDiving 两个基准数据集上取得了有竞争力的性能。实验结果与消融研究验证了利用骨骼拓扑结构进行姿态相似性建模及动作质量评估的有效性。",
        "translated_title": "一种面向拓扑结构的图卷积网络用于人体姿态相似性评估与动作质量评分",
        "label": [],
        "label_reason": "评估动作质量属高阶行为理解任务",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "GCN结构改进但非图像像素级恢复"
    },
    {
        "title": "UNO: Uncertainty-aware Noisy-Or Multimodal Fusion for Unanticipated Input Degradation",
        "url": "http://arxiv.org/abs/1911.05611v2",
        "pub_date": "2019-11-06",
        "summary": "The fusion of multiple sensor modalities, especially through deep learning architectures, has been an active area of study. However, an under-explored aspect of such work is whether the methods can be robust to degradations across their input modalities, especially when they must generalize to degradations not seen during training. In this work, we propose an uncertainty-aware fusion scheme to effectively fuse inputs that might suffer from a range of known and unknown degradations. Specifically, we analyze a number of uncertainty measures, each of which captures a different aspect of uncertainty, and we propose a novel way to fuse degraded inputs by scaling modality-specific output softmax probabilities. We additionally propose a novel data-dependent spatial temperature scaling method to complement these existing uncertainty measures. Finally, we integrate the uncertainty-scaled output from each modality using a probabilistic noisy-or fusion method. In a photo-realistic simulation environment (AirSim), we show that our method achieves significantly better results on a semantic segmentation task, compared to state-of-art fusion architectures, on a range of degradations (e.g. fog, snow, frost, and various other types of noise), some of which are unknown during training. We specifically improve upon the state-of-art[1] by 28% in mean IoU on various degradations. [1] Abhinav Valada, Rohit Mohan, and Wolfram Burgard. Self-Supervised Model Adaptation for Multimodal Semantic Segmentation. In: arXiv e-prints, arXiv:1808.03833 (Aug. 2018), arXiv:1808.03833. arXiv: 1808.03833 [cs.CV].",
        "translated": "多传感器模态的融合，特别是通过深度学习架构，一直是研究热点。然而，此类工作的一个尚未充分探索的方面是：所提出的方法是否能够对输入模态中的各种退化具备鲁棒性，尤其是在必须泛化到训练阶段未见过的退化情形时。在本工作中，我们提出一种不确定性感知的融合方案，以有效融合可能遭受多种已知和未知退化的输入。具体而言，我们分析了若干种不确定性度量方法，每种均捕捉不同方面的不确定性，并提出了一种新颖的融合方法——通过缩放模态特定输出的softmax概率，来融合退化输入。此外，我们还提出了一种新颖的数据依赖型空间温度缩放方法，以补充现有的不确定性度量。最后，我们采用概率性的“带噪声或”融合方法，将各模态经不确定性缩放后的输出进行集成。在逼真的照片级仿真环境（AirSim）中，我们在语义分割任务上展示了我们的方法在多种退化类型（如雾、雪、霜及各类噪声等）下显著优于当前最先进的融合架构，其中部分退化类型在训练过程中并未出现。我们尤其在各类退化下将平均IoU性能较现有最优方法[1]提升了28%。  \n[1] Abhinav Valada, Rohit Mohan, and Wolfram Burgard. Self-Supervised Model Adaptation for Multimodal Semantic Segmentation. In: arXiv e-prints, arXiv:1808.03833 (Aug. 2018), arXiv:1808.03833. arXiv: 1808.03833 [cs.CV].",
        "translated_title": "UNO：面向意外输入退化的不确定性感知噪声或型多模态融合",
        "label": [],
        "label_reason": "聚焦多模态融合与语义分割，非像素级图像恢复",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出不确定性感知融合框架，提升泛化能力"
    },
    {
        "title": "Distributed Edge-based Video Analytics on the Move",
        "url": "http://arxiv.org/abs/2206.14414v1",
        "pub_date": "2022-06-29",
        "summary": "In recent years, we have witnessed an explosive growth of data. Much of this data is video data generated by security cameras, smartphones, and dash cams. The timely analysis of such data is of great practical importance for many emerging applications, such as real-time facial recognition and object detection. In this study, we address the problem of real-time in-situ video analytics with dash cam videos and present EdgeDashAnalytics (EDA), an edge-based system that enables near real-time video analytics using a local network of mobile devices. In particular, it simultaneously processes videos produced by two dash cams of different angles with one or more mobile devices on the move in a near real-time manner. One camera faces outward to capture the view in front of the vehicle, while the other camera faces inward to capture the driver. The outer videos are analysed to detect potential driving hazards, while the inner videos are used to identify driver distractedness. EDA achieves near real-time video analytics using resource-constrained, transient mobile devices by devising and incorporating several optimisations, with a tolerable loss in accuracy. We have implemented EDA as an Android app and evaluated it using two dash cams and several heterogeneous mobile devices with the BDD100K dash cam video dataset (arXiv:1805.04687 [cs.CV]) and the DMD driver monitoring dataset (arXiv:2008.12085 [cs.CV]). Experiment results demonstrate the feasibility of real-time video analytics in terms of turnaround time and energy consumption (or battery usage), using resource-constrained mobile devices on the move.",
        "translated": "近年来，我们见证了数据量的爆炸式增长。其中大部分数据由安防摄像头、智能手机和行车记录仪生成。对这类数据进行及时分析，对于实时人脸识别、目标检测等新兴应用具有重要的实际意义。在本研究中，我们针对行车记录仪视频的实时在场视频分析问题，提出了 EdgeDashAnalytics（EDA），这是一个基于边缘计算的系统，能够在移动设备组成的本地网络上实现近乎实时的视频分析。具体而言，该系统能够借助一个或多个在移动中的手机设备，在近实时条件下同时处理两路来自不同角度的行车记录仪视频流。一路摄像头朝向车外，用于捕捉车辆前方视野；另一路摄像头朝向车内，用于拍摄驾驶员。车外视频被用于检测潜在驾驶危险，而车内视频则用于识别驾驶员注意力分散状况。通过设计并整合多种优化策略，EDA能够在资源受限、短暂连接的移动设备上实现近乎实时的视频分析，并在精度损失可接受的前提下完成任务。我们已将 EDA 实现为 Android 应用程序，并使用 BDD100K 行车记录仪视频数据集（arXiv:1805.04687 [cs.CV]）以及 DMD 驾驶员监控数据集（arXiv:2008.12085 [cs.CV]），结合两台行车记录仪与若干异构移动设备进行了评估。实验结果表明，利用资源受限的移动设备在行驶过程中即可实现具备可行性的实时视频分析，在响应时间及能耗（或电池消耗）方面表现良好。",
        "translated_title": "移动场景下的分布式边缘视频分析",
        "label": [],
        "label_reason": "任务为视频分析与实时检测，属高阶视觉任务。",
        "relevance_score": 1,
        "novelty_score": 1,
        "novelty_reason": "系统优化方案无本质创新，属工程实现。"
    },
    {
        "title": "Learning Object Placement Programs for Indoor Scene Synthesis with Iterative Self Training",
        "url": "http://arxiv.org/abs/2503.04496v1",
        "pub_date": "2025-03-06",
        "summary": "Data driven and autoregressive indoor scene synthesis systems generate indoor scenes automatically by suggesting and then placing objects one at a time. Empirical observations show that current systems tend to produce incomplete next object location distributions. We introduce a system which addresses this problem. We design a Domain Specific Language (DSL) that specifies functional constraints. Programs from our language take as input a partial scene and object to place. Upon execution they predict possible object placements. We design a generative model which writes these programs automatically. Available 3D scene datasets do not contain programs to train on, so we build upon previous work in unsupervised program induction to introduce a new program bootstrapping algorithm. In order to quantify our empirical observations we introduce a new evaluation procedure which captures how well a system models per-object location distributions. We ask human annotators to label all the possible places an object can go in a scene and show that our system produces per-object location distributions more consistent with human annotators. Our system also generates indoor scenes of comparable quality to previous systems and while previous systems degrade in performance when training data is sparse, our system does not degrade to the same degree.",
        "translated": "数据驱动且自回归的室内场景合成系统通过依次建议并放置物体，自动生成室内场景。经验观察表明，当前系统倾向于产生不完整的下一物体位置分布。我们提出了一种解决该问题的系统。我们设计了一种领域特定语言（DSL），用于指定功能约束。从该语言编写的程序以部分场景和待放置物体作为输入，在执行时预测可能的物体放置位置。我们设计了一个生成模型，可自动编写这些程序。现有3D场景数据集并不包含可供训练的程序，因此我们基于先前在无监督程序归纳方面的研究，引入一种新的程序引导算法。为量化我们的经验观察，我们引入了一种新的评估流程，用以衡量系统在建模每个物体位置分布方面的表现。我们邀请人工标注者标注场景中物体所有可能的放置位置，并证明我们的系统所生成的每个物体位置分布更符合人工标注者的标注结果。此外，我们的系统生成的室内场景质量与以往系统相当；而在训练数据稀疏情况下，以往系统的性能会显著下降，而我们的系统则不会出现同等程度的性能退化。",
        "translated_title": "学习用于室内场景合成的物体布局程序并结合迭代自训练",
        "label": [],
        "label_reason": "生成室内场景属高阶视觉任务，非像素级图像恢复",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出程序化自训练框架，提升物体放置分布建模能力"
    },
    {
        "title": "ISO/IEC-Compliant Match-on-Card Face Verification with Short Binary Templates",
        "url": "http://arxiv.org/abs/2510.16078v1",
        "pub_date": "2025-10-17",
        "summary": "We present a practical match-on-card design for face verification in which compact 64/128-bit templates are produced off-card by PCA-ITQ and compared on-card via constant-time Hamming distance. We specify ISO/IEC 7816-4 and 14443-4 command APDUs with fixed-length payloads and decision-only status words (no score leakage), together with a minimal per-identity EEPROM map. Using real binary codes from a CelebA working set (55 identities, 412 images), we (i) derive operating thresholds from ROC/DET, (ii) replay enroll-&gt;verify transactions at those thresholds, and (iii) bound end-to-end time by pure link latency plus a small constant on-card budget. Even at the slowest contact rate (9.6 kbps), total verification time is 43.9 ms (64 b) and 52.3 ms (128 b); at 38.4 kbps both are &lt;14 ms. At FAR = 1%, both code lengths reach TPR = 0.836, while 128 b lowers EER relative to 64 b. An optional +6 B helper (targeted symbol-level parity over empirically unstable bits) is latency-negligible. Overall, short binary templates, fixed-payload decision-only APDUs, and constant-time matching satisfy ISO/IEC transport constraints with wide timing margin and align with ISO/IEC 24745 privacy goals. Limitations: single-dataset evaluation and design-level (pre-hardware) timing; we outline AgeDB/CFP-FP and on-card microbenchmarks as next steps.",
        "translated": "我们提出了一种面向人脸验证的实用型卡上匹配设计：通过 PCA-ITQ 算法在卡外生成紧凑的 64/128 位模板，并在卡内通过恒定时间汉明距离进行比较。我们指定了符合 ISO/IEC 7816-4 和 14443-4 标准的命令 APDU，其负载长度固定，状态字仅用于决策（无分数泄露），并辅以最小化的每身份 EEPROM 映射表。基于 CelebA 数据集中的真实二进制编码（55 个身份，412 张图像），我们（i）从 ROC/DET 曲线推导操作阈值，（ii）在该阈值下重放注册→验证事务流程，（iii）将端到端总耗时限定为纯链路延迟加上卡内微小常数预算。即使在最慢接触速率（9.6 kbps）下，总验证时间为 43.9 ms（64 b）和 52.3 ms（128 b）；在 38.4 kbps 下两者均小于 14 ms。在 FAR = 1% 时，两种码长均达到 TPR = 0.836，且 128 b 相对于 64 b 能降低 EER。可选的 +6 B 辅助机制（针对经验性不稳定比特位的目标符号级奇偶校验）对延迟影响可忽略不计。总体而言，短二进制模板、固定负载长度的决策型 APDU 及恒定时间匹配方案满足 ISO/IEC 传输约束条件，具备宽裕的时间余量，并与 ISO/IEC 24745 隐私目标保持一致。局限性：单数据集评估与设计阶段（预硬件）定时分析；下一步工作包括引入 AgeDB/CFP-FP 数据集及卡内微基准测试。",
        "translated_title": "符合 ISO/IEC 标准的卡片式人脸验证方法，采用短二进制模板",
        "label": [],
        "label_reason": "人脸验证属高阶任务，非图像像素级处理",
        "relevance_score": 1,
        "novelty_score": 3,
        "novelty_reason": "方法常规组合，无本质创新"
    },
    {
        "title": "Is there really a Citation Age Bias in NLP?",
        "url": "http://arxiv.org/abs/2401.03545v1",
        "pub_date": "2024-01-07",
        "summary": "Citations are a key ingredient of scientific research to relate a paper to others published in the community. Recently, it has been noted that there is a citation age bias in the Natural Language Processing (NLP) community, one of the currently fastest growing AI subfields, in that the mean age of the bibliography of NLP papers has become ever younger in the last few years, leading to `citation amnesia' in which older knowledge is increasingly forgotten. In this work, we put such claims into perspective by analyzing the bibliography of $\\sim$300k papers across 15 different scientific fields submitted to the popular preprint server Arxiv in the time period from 2013 to 2022. We find that all AI subfields (in particular: cs.AI, cs.CL, cs.CV, cs.LG) have similar trends of citation amnesia, in which the age of the bibliography has roughly halved in the last 10 years (from above 12 in 2013 to below 7 in 2022), on average. Rather than diagnosing this as a citation age bias in the NLP community, we believe this pattern is an artefact of the dynamics of these research fields, in which new knowledge is produced in ever shorter time intervals.",
        "translated": "引用是科学研究中的关键要素，用于将一篇论文与其他已发表的文献建立关联。近期人们注意到，在自然语言处理（NLP）领域——当前增长最快的AI子领域之一——存在引用年龄偏差：过去几年中，NLP论文参考文献的平均年龄持续降低，导致“引用遗忘”现象，即较早的知识正被逐步遗忘。在本研究中，我们通过分析2013年至2022年间提交至热门预印本平台Arxiv的约30万篇跨15个不同科研领域的论文的参考文献，对该现象进行评估。我们发现，所有AI子领域（特别是：cs.AI、cs.CL、cs.CV、cs.LG）均呈现出类似的引用遗忘趋势：近十年间，其参考文献的平均年龄大致减半（从2013年的12年以上降至2022年的7岁以下）。我们认为，这一模式并非NLP社区特有的引用年龄偏见，而是这些研究领域动态发展的必然结果——新知识的产出周期正不断缩短。",
        "translated_title": "自然语言处理领域真的存在引文年龄偏倚吗？",
        "label": [],
        "label_reason": "研究论文引用趋势，非图像处理任务。",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "分析趋势提出解释，无图像恢复创新。"
    },
    {
        "title": "On the Origin of Species of Self-Supervised Learning",
        "url": "http://arxiv.org/abs/2103.17143v1",
        "pub_date": "2021-03-31",
        "summary": "In the quiet backwaters of cs.CV, cs.LG and stat.ML, a cornucopia of new learning systems is emerging from a primordial soup of mathematics-learning systems with no need for external supervision. To date, little thought has been given to how these self-supervised learners have sprung into being or the principles that govern their continuing diversification. After a period of deliberate study and dispassionate judgement during which each author set their Zoom virtual background to a separate Galapagos island, we now entertain no doubt that each of these learning machines are lineal descendants of some older and generally extinct species. We make five contributions: (1) We gather and catalogue row-major arrays of machine learning specimens, each exhibiting heritable discriminative features; (2) We document a mutation mechanism by which almost imperceptible changes are introduced to the genotype of new systems, but their phenotype (birdsong in the form of tweets and vestigial plumage such as press releases) communicates dramatic changes; (3) We propose a unifying theory of self-supervised machine evolution and compare to other unifying theories on standard unifying theory benchmarks, where we establish a new (and unifying) state of the art; (4) We discuss the importance of digital biodiversity, in light of the endearingly optimistic Paris Agreement.",
        "translated": "在计算机视觉（cs.CV）、计算学习理论（cs.LG）与统计机器学习（stat.ML）的静谧边缘，一种丰富多样的新型学习系统正从数学-学习系统的原始汤中涌现，而无需外部监督。迄今为止，人们鲜少关注这些自监督学习者是如何诞生的，亦未深入探讨支配其持续多样化发展的原理。在每位作者将Zoom虚拟背景设置为不同加拉帕戈斯岛屿、经过一段刻意研究与冷静评判之后，我们如今确信：每种此类学习机器均为某种更古老且普遍已灭绝物种的直系后代。\n\n我们做出五项贡献：(1) 我们收集并编目了按行主序排列的机器学习样本矩阵，每个样本均展现出可遗传的判别性特征；(2) 我们记录了一种突变机制：该机制通过几乎难以察觉的方式改变新系统的基因型，但其表型（如推特形式的“鸟鸣”与诸如新闻稿般的退化羽毛）却传达出显著变化；(3) 我们提出一种统一的自监督机器演化理论，并将其与其他统一理论在标准统一理论基准上进行对比，从而确立了新的（且统一的）最先进状态；(4) 我们讨论了数字生物多样性的重要性，以此呼应那令人充满乐观期待的《巴黎协定》。",
        "translated_title": "物种起源：自监督学习的渊源",
        "label": [],
        "label_reason": "论文讨论自监督学习演化，非图像处理任务。",
        "relevance_score": 1,
        "novelty_score": 1,
        "novelty_reason": "理论研究无图像恢复创新点。"
    },
    {
        "title": "Identifying the Development and Application of Artificial Intelligence in Scientific Text",
        "url": "http://arxiv.org/abs/2002.07143v2",
        "pub_date": "2020-02-17",
        "summary": "We describe a strategy for identifying the universe of research publications relevant to the application and development of artificial intelligence. The approach leverages the arXiv corpus of scientific preprints, in which authors choose subject tags for their papers from a set defined by editors. We compose a functional definition of AI relevance by learning these subjects from paper metadata, and then inferring the arXiv-subject labels of papers in larger corpora: Clarivate Web of Science, Digital Science Dimensions, and Microsoft Academic Graph. This yields predictive classification $F_1$ scores between .75 and .86 for Natural Language Processing (cs.CL), Computer Vision (cs.CV), and Robotics (cs.RO). For a single model that learns these and four other AI-relevant subjects (cs.AI, cs.LG, stat.ML, and cs.MA), we see precision of .83 and recall of .85. We evaluate the out-of-domain performance of our classifiers against other sources of topic information and predictions from alternative methods. We find that a supervised solution can generalize to identify publications that belong to the high-level fields of study represented on arXiv. This offers a method for identifying AI-relevant publications that updates at the pace of research output, without reliance on subject-matter experts for query development or labeling.",
        "translated": "我们提出了一种识别与人工智能应用与发展相关的研究文献全集的策略。该方法利用 arXiv 上的科学预印本语料库，其中作者从编辑定义的标签集合中为论文选择主题标签。我们通过从论文元数据中学习这些主题，构建了 AI 相关性的功能性定义，并进一步推断 Clarivate Web of Science、Digital Science Dimensions 以及 Microsoft Academic Graph 等更大语料库中论文的 arXiv 主题标签。这一方法在自然语言处理（cs.CL）、计算机视觉（cs.CV）和机器人学（cs.RO）三个领域取得了预测分类 $F_1$ 分数介于 .75 至 .86 的表现。对于一个同时学习这五个 AI 相关主题（cs.AI、cs.LG、stat.ML 和 cs.MA）的单一模型，我们观察到其精度为 .83，召回率为 .85。我们评估了我们的分类器在跨领域场景下的性能，并与其他主题信息来源及替代方法的预测结果进行了对比。结果显示，监督式解决方案能够泛化并准确识别属于 arXiv 所代表的高层次研究领域的相关文献。这提供了一种更新速度与研究成果产出同步、无需依赖领域专家进行查询构建或标注的 AI 相关文献识别方法。",
        "translated_title": "人工智能在科学文本中的发展与应用识别",
        "label": [],
        "label_reason": "论文处理文献分类，非图像像素级恢复任务。",
        "relevance_score": 1,
        "novelty_score": 6,
        "novelty_reason": "方法基于元数据学习，创新度有限。"
    },
    {
        "title": "DecoMind: A Generative AI System for Personalized Interior Design Layouts",
        "url": "http://arxiv.org/abs/2508.16696v1",
        "pub_date": "2025-08-22",
        "summary": "This paper introduces a system for generating interior design layouts based on user inputs, such as room type, style, and furniture preferences. CLIP extracts relevant furniture from a dataset, and a layout that contains furniture and a prompt are fed to Stable Diffusion with ControlNet to generate a design that incorporates the selected furniture. The design is then evaluated by classifiers to ensure alignment with the user's inputs, offering an automated solution for realistic interior design.",
        "translated": "本文提出了一种基于用户输入（如房间类型、风格及家具偏好）生成室内设计布局的系统。CLIP 从数据集中提取相关家具，将包含家具的布局与提示词一同输入 Stable Diffusion 与 ControlNet，以生成融合选定家具的设计方案。随后，该设计由分类器进行评估，确保其与用户输入保持一致，从而提供一种自动化、逼真的室内设计方案。",
        "translated_title": "DecoMind：一种用于个性化室内设计布局的生成式人工智能系统",
        "label": [],
        "label_reason": "生成室内布局属高阶视觉任务，非像素级图像处理。",
        "relevance_score": 1,
        "novelty_score": 3,
        "novelty_reason": "复现扩散模型应用，无本质创新。"
    },
    {
        "title": "ZACH-ViT: A Zero-Token Vision Transformer with ShuffleStrides Data Augmentation for Robust Lung Ultrasound Classification",
        "url": "http://arxiv.org/abs/2510.17650v1",
        "pub_date": "2025-10-20",
        "summary": "Differentiating cardiogenic pulmonary oedema (CPE) from non-cardiogenic and structurally normal lungs in lung ultrasound (LUS) videos remains challenging due to the high visual variability of non-cardiogenic inflammatory patterns (NCIP/ARDS-like), interstitial lung disease, and healthy lungs. This heterogeneity complicates automated classification as overlapping B-lines and pleural artefacts are common. We introduce ZACH-ViT (Zero-token Adaptive Compact Hierarchical Vision Transformer), a 0.25 M-parameter Vision Transformer variant that removes both positional embeddings and the [CLS] token, making it fully permutation-invariant and suitable for unordered medical image data. To enhance generalization, we propose ShuffleStrides Data Augmentation (SSDA), which permutes probe-view sequences and frame orders while preserving anatomical validity. ZACH-ViT was evaluated on 380 LUS videos from 95 critically ill patients against nine state-of-the-art baselines. Despite the heterogeneity of the non-cardiogenic group, ZACH-ViT achieved the highest validation and test ROC-AUC (0.80 and 0.79) with balanced sensitivity (0.60) and specificity (0.91), while all competing models collapsed to trivial classification. It trains 1.35x faster than Minimal ViT (0.62M parameters) with 2.5x fewer parameters, supporting real-time clinical deployment. These results show that aligning architectural design with data structure can outperform scale in small-data medical imaging.",
        "translated": "区分肺超声（LUS）视频中的心源性肺水肿（CPE）与非心源性及结构正常的肺组织仍具挑战性，原因在于非心源性炎症模式（NCIP/ARDS样）、间质性肺病及健康肺组织在视觉表现上存在高度异质性。这种异质性导致自动化分类困难，因为重叠的B线和胸膜伪影普遍存在。我们提出ZACH-ViT（Zero-token Adaptive Compact Hierarchical Vision Transformer），这是一种参数量仅为0.25M的视觉Transformer变体，去除了位置嵌入与[CLS]标记，使其完全具备排列不变性，适用于无序的医学图像数据。为增强泛化能力，我们提出ShuffleStrides 数据增强方法（SSDA），该方法在保持解剖学有效性的同时，对探头视角序列和帧顺序进行随机置换。ZACH-ViT在95名危重症患者的380段LUS视频上，对比九种最新基线模型进行了评估。尽管非心源性组存在显著异质性，ZACH-ViT仍取得了最高的验证集与测试集ROC-AUC值（分别为0.80与0.79），同时实现平衡的敏感度（0.60）与特异性（0.91），而所有竞争模型均退化至平凡分类水平。其训练速度比Minimal ViT（0.62M参数）快1.35倍，参数量少2.5倍，支持临床实时部署。这些结果表明，在小样本医学影像任务中，架构设计与数据结构的匹配优于单纯提升模型规模。",
        "translated_title": "ZACH-ViT：一种结合 ShuffleStrides 数据增强的零标记视觉 Transformer，用于鲁棒的肺部超声分类",
        "label": [],
        "label_reason": "任务为医学图像分类，属高阶视觉任务",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出新架构与数据增强，提升小样本分类性能"
    },
    {
        "title": "N-shot Palm Vein Verification Using Siamese Networks",
        "url": "http://arxiv.org/abs/2109.12808v1",
        "pub_date": "2021-09-27",
        "summary": "The use of deep learning methods to extract vascular biometric patterns from the palm surface has been of interest among researchers in recent years. In many biometric recognition tasks, there is a limit in the number of training samples. This is because of limited vein biometric databases being available for research. This restricts the application of deep learning methods to design algorithms that can effectively identify or authenticate people for vein recognition. This paper proposes an architecture using Siamese neural network structure for few shot palm vein identification. The proposed network uses images from both the palms and consists of two sub-nets that share weights to identify a person. The architecture performance was tested on the HK PolyU multi spectral palm vein database with limited samples. The results suggest that the method is effective since it has 91.9% precision, 91.1% recall, 92.2% specificity, 91.5%, F1-Score, and 90.5% accuracy values.",
        "translated": "近年来，研究人员对利用深度学习方法从手掌表面提取血管生物特征模式表现出浓厚兴趣。在许多生物识别任务中，训练样本数量有限，这是由于可用于研究的静脉生物特征数据库资源稀缺所致。这限制了深度学习方法在设计能够有效识别人或进行静脉身份认证的算法中的应用。本文提出了一种基于孪生神经网络结构的架构，用于少样本手掌静脉识别。所提出的网络使用双手图像，并包含两个共享权重的子网络，用于识别个体。该架构在样本数量有限的香港理工大学多光谱手掌静脉数据库上进行了性能测试。结果表明，该方法有效，其精度为91.9%，召回率为91.1%，特异度为92.2%，F1分数为91.5%，准确率为90.5%。",
        "translated_title": "使用孪生网络的少样本掌静脉验证",
        "label": [],
        "label_reason": "人手静脉识别属高阶生物识别任务，非像素级图像处理",
        "relevance_score": 1,
        "novelty_score": 3,
        "novelty_reason": "Siamese网络用于少样本识别，无新范式或架构创新"
    },
    {
        "title": "From Cheap to Pro: A Learning-based Adaptive Camera Parameter Network for Professional-Style Imaging",
        "url": "http://arxiv.org/abs/2510.20550v1",
        "pub_date": "2025-10-23",
        "summary": "Consumer-grade camera systems often struggle to maintain stable image quality under complex illumination conditions such as low light, high dynamic range, and backlighting, as well as spatial color temperature variation. These issues lead to underexposure, color casts, and tonal inconsistency, which degrade the performance of downstream vision tasks. To address this, we propose ACamera-Net, a lightweight and scene-adaptive camera parameter adjustment network that directly predicts optimal exposure and white balance from RAW inputs. The framework consists of two modules: ACamera-Exposure, which estimates ISO to alleviate underexposure and contrast loss, and ACamera-Color, which predicts correlated color temperature and gain factors for improved color consistency. Optimized for real-time inference on edge devices, ACamera-Net can be seamlessly integrated into imaging pipelines. Trained on diverse real-world data with annotated references, the model generalizes well across lighting conditions. Extensive experiments demonstrate that ACamera-Net consistently enhances image quality and stabilizes perception outputs, outperforming conventional auto modes and lightweight baselines without relying on additional image enhancement modules.",
        "translated": "消费级相机系统在低光照、高动态范围及背光等复杂光照条件下，往往难以维持稳定的图像质量，同时伴随空间色温变化。这些问题导致欠曝光、色彩偏差与色调不一致，从而降低下游视觉任务的性能。为解决该问题，我们提出 ACamera-Net，这是一种轻量且场景自适应的相机参数调整网络，可直接从 RAW 输入预测最优曝光和白平衡参数。该框架包含两个模块：ACamera-Exposure 模块用于估计 ISO 值以缓解欠曝光和对比度损失；ACamera-Color 模块则预测相关色温与增益因子，以提升色彩一致性。该模型针对边缘设备上的实时推理进行优化，可无缝集成至成像流程中。在标注参考数据的多样化真实世界数据集上训练后，该模型对多种光照条件具有良好的泛化能力。大量实验表明，ACamera-Net 可持续提升图像质量并稳定感知输出，在无需依赖额外图像增强模块的前提下，显著优于传统自动模式及轻量基线方法。",
        "translated_title": "从廉价到专业：一种基于学习的自适应相机参数网络，用于专业风格成像",
        "label": [
            "低光照增强",
            "对比度增强"
        ],
        "label_reason": "针对低光与色彩一致性问题，提升像素级图像质量。",
        "relevance_score": 7,
        "novelty_score": 6,
        "novelty_reason": "新架构预测曝光与白平衡，但属参数调整非图像重建。"
    },
    {
        "title": "HateClipSeg: A Segment-Level Annotated Dataset for Fine-Grained Hate Video Detection",
        "url": "http://arxiv.org/abs/2508.01712v2",
        "pub_date": "2025-08-03",
        "summary": "Detecting hate speech in videos remains challenging due to the complexity of multimodal content and the lack of fine-grained annotations in existing datasets. We present HateClipSeg, a large-scale multimodal dataset with both video-level and segment-level annotations, comprising over 11,714 segments labeled as Normal or across five Offensive categories: Hateful, Insulting, Sexual, Violence, Self-Harm, along with explicit target victim labels. Our three-stage annotation process yields high inter-annotator agreement (Krippendorff's alpha = 0.817). We propose three tasks to benchmark performance: (1) Trimmed Hateful Video Classification, (2) Temporal Hateful Video Localization, and (3) Online Hateful Video Classification. Results highlight substantial gaps in current models, emphasizing the need for more sophisticated multimodal and temporally aware approaches. The HateClipSeg dataset are publicly available at https://github.com/Social-AI-Studio/HateClipSeg.git.",
        "translated": "视频中的仇恨言论检测仍具挑战性，原因在于多模态内容的复杂性以及现有数据集中缺乏细粒度标注。我们提出 HateClipSeg 数据集，这是一个大规模多模态数据集，包含视频级和片段级标注，涵盖超过 11,714 个片段，每个片段被标注为“正常”或五类仇恨类别：仇恨、侮辱、性相关、暴力、自残，并附带明确的目标受害者标签。我们的三阶段标注流程获得了较高的标注者间一致性（Krippendorff’s alpha = 0.817）。我们提出了三项任务以评估模型性能：（1）截取式仇恨视频分类，（2）时间维度仇恨视频定位，（3）在线式仇恨视频分类。实验结果揭示当前模型存在显著性能差距，凸显了对更复杂多模态及时间感知方法的需求。HateClipSeg 数据集已公开于 https://github.com/Social-AI-Studio/HateClipSeg.git。",
        "translated_title": "HateClipSeg：用于细粒度仇恨视频检测的片段级标注数据集",
        "label": [],
        "label_reason": "高阶视频内容理解任务，非像素级图像处理",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "数据集构建与标注流程创新，但无图像恢复技术"
    },
    {
        "title": "The Haar Wavelet Transform of a Dendrogram: Additional Notes",
        "url": "http://arxiv.org/abs/cs/0702067v1",
        "pub_date": "2007-02-10",
        "summary": "We consider the wavelet transform of a finite, rooted, node-ranked, $p$-way tree, focusing on the case of binary ($p = 2$) trees. We study a Haar wavelet transform on this tree. Wavelet transforms allow for multiresolution analysis through translation and dilation of a wavelet function. We explore how this works in our tree context.",
        "translated": "我们考虑一个有限的、有根的、节点排序的 $p$-叉树的小波变换，重点关注二叉（$p = 2$）树的情形。我们研究该树上的 Haar 小波变换。小波变换通过小波函数的平移与伸缩实现多分辨率分析。我们探讨这一机制在当前树结构中的具体运作方式。",
        "translated_title": " dendrogram 的 Haar 小波变换：补充说明",
        "label": [],
        "label_reason": "论文涉及树结构波浪变换，与推荐系统无关",
        "relevance_score": 1,
        "novelty_score": 1,
        "novelty_reason": "方法属数学分析领域，无推荐系统创新"
    },
    {
        "title": "Similarity of Objects and the Meaning of Words",
        "url": "http://arxiv.org/abs/cs/0602065v1",
        "pub_date": "2006-02-17",
        "summary": "We survey the emerging area of compression-based, parameter-free, similarity distance measures useful in data-mining, pattern recognition, learning and automatic semantics extraction. Given a family of distances on a set of objects, a distance is universal up to a certain precision for that family if it minorizes every distance in the family between every two objects in the set, up to the stated precision (we do not require the universal distance to be an element of the family). We consider similarity distances for two types of objects: literal objects that as such contain all of their meaning, like genomes or books, and names for objects. The latter may have literal embodyments like the first type, but may also be abstract like ``red'' or ``christianity.'' For the first type we consider a family of computable distance measures corresponding to parameters expressing similarity according to particular featuresdistances generated by web users corresponding to particular semantic relations between the (names for) the designated objects. For both families we give universal similarity distance measures, incorporating all particular distance measures in the family. In the first case the universal distance is based on compression and in the second case it is based on Google page counts related to search terms. In both cases experiments on a massive scale give evidence of the viability of the approaches. between pairs of literal objects. For the second type we consider similarity",
        "translated": "我们综述了基于压缩、无参数、用于数据挖掘、模式识别、学习及自动语义提取的相似性距离度量这一新兴领域。对于一个对象集合上的距离族，若某一距离在给定精度下对族内任意两对象间的每一对距离均起到下界作用，则称其为该族在该精度下的“通用距离”（我们不要求该通用距离本身属于该族）。我们考虑两类对象的相似性距离：第一类是“字面对象”，它们自身即包含全部语义，例如基因组或书籍；第二类是对象的“名称”。后者可能具有如第一类那样的字面体现形式，也可能抽象如“红色”或“基督教”。针对第一类对象，我们考虑一族可计算的距离度量，对应于表达根据特定特征相似性的参数——由网络用户生成的距离，反映指定对象（名称）间特定语义关系。对于这两类对象，我们分别给出了能涵盖族内所有具体距离度量的通用相似性距离度量。第一类情形中，通用距离基于压缩构建；第二类情形中，通用距离则基于与搜索词相关的谷歌页面计数构建。在两种情况下，大规模实验均表明上述方法的有效性。对于第二类对象，我们考虑相似性...",
        "translated_title": "物体间的相似性与词语的意义",
        "label": [],
        "label_reason": "研究对象相似度与语义，非推荐系统直接相关",
        "relevance_score": 3,
        "novelty_score": 4,
        "novelty_reason": "压缩理论创新，但未应用于推荐场景"
    },
    {
        "title": "Learning Decomposed Contextual Token Representations from Pretrained and Collaborative Signals for Generative Recommendation",
        "url": "http://arxiv.org/abs/2509.10468v1",
        "pub_date": "2025-08-22",
        "summary": "Recent advances in generative recommenders adopt a two-stage paradigm: items are first tokenized into semantic IDs using a pretrained tokenizer, and then large language models (LLMs) are trained to generate the next item via sequence-to-sequence modeling. However, these two stages are optimized for different objectives: semantic reconstruction during tokenizer pretraining versus user interaction modeling during recommender training. This objective misalignment leads to two key limitations: (i) suboptimal static tokenization, where fixed token assignments fail to reflect diverse usage contexts; and (ii) discarded pretrained semantics, where pretrained knowledge - typically from language model embeddings - is overwritten during recommender training on user interactions. To address these limitations, we propose to learn DEcomposed COntextual Token Representations (DECOR), a unified framework that preserves pretrained semantics while enhancing the adaptability of token embeddings. DECOR introduces contextualized token composition to refine token embeddings based on user interaction context, and decomposed embedding fusion that integrates pretrained codebook embeddings with newly learned collaborative embeddings. Experiments on three real-world datasets demonstrate that DECOR consistently outperforms state-of-the-art baselines in recommendation performance. Our code will be made available upon publication.",
        "translated": "近期生成式推荐系统的研究进展采用两阶段范式：首先，利用预训练的分词器将物料（item）编码为语义ID；随后，通过序列到序列建模训练大语言模型（LLM），以生成下一物料。然而，这两个阶段分别针对不同目标进行优化：分词器预训练阶段侧重语义重建，而推荐系统训练阶段则聚焦于用户交互建模。这种目标错配导致两大关键局限：(i) 静态分词不充分，固定分词映射无法反映多样化的使用情境；(ii) 预训练语义被丢弃，即在推荐系统训练过程中，通常来自语言模型嵌入的预训练知识会被用户交互数据覆盖。为解决上述问题，我们提出学习分解式上下文化令牌表示（DECOR），一种统一框架，能够在保留预训练语义的同时增强令牌嵌入的适应性。DECOR引入上下文化令牌组合机制，根据用户交互上下文精炼令牌嵌入，并采用分解式嵌入融合机制，将预训练码本嵌入与新学习的协同嵌入相结合。在三个真实世界数据集上的实验表明，DECOR在推荐性能上持续优于现有最先进基线方法。我们的代码将在论文发表后公开。",
        "translated_title": "从预训练与协同信号中学习分解式上下文Token表示用于生成式推荐",
        "label": [
            "LLM生成式推荐",
            "通用推荐技术"
        ],
        "label_reason": "直接解决生成式推荐中token表示与用户交互的对齐问题",
        "relevance_score": 10,
        "novelty_score": 9,
        "novelty_reason": "提出DECOR框架统一预训练语义与协同信号，显著提升效果"
    },
    {
        "title": "Practical Code RAG at Scale: Task-Aware Retrieval Design Choices under Compute Budgets",
        "url": "http://arxiv.org/abs/2510.20609v1",
        "pub_date": "2025-10-23",
        "summary": "We study retrieval design for code-focused generation tasks under realistic compute budgets. Using two complementary tasks from Long Code Arena -- code completion and bug localization -- we systematically compare retrieval configurations across various context window sizes along three axes: (i) chunking strategy, (ii) similarity scoring, and (iii) splitting granularity. (1) For PL-PL, sparse BM25 with word-level splitting is the most effective and practical, significantly outperforming dense alternatives while being an order of magnitude faster. (2) For NL-PL, proprietary dense encoders (Voyager-3 family) consistently beat sparse retrievers, however requiring 100x larger latency. (3) Optimal chunk size scales with available context: 32-64 line chunks work best at small budgets, and whole-file retrieval becomes competitive at 16000 tokens. (4) Simple line-based chunking matches syntax-aware splitting across budgets. (5) Retrieval latency varies by up to 200x across configurations; BPE-based splitting is needlessly slow, and BM25 + word splitting offers the best quality-latency trade-off. Thus, we provide evidence-based recommendations for implementing effective code-oriented RAG systems based on task requirements, model constraints, and computational efficiency.",
        "translated": "我们在现实的计算预算约束下研究面向代码生成任务的检索设计。借助 Long Code Arena 中两个互补任务——代码补全与 Bug 定位——我们系统性地比较了不同上下文窗口大小下多种检索配置，沿三个维度展开分析：(i) 分块策略、(ii) 相似度评分方法、(iii) 分割粒度。(1) 对于 PL-PL 任务，基于词级别的稀疏 BM25 检索是最有效且实用的选择，性能显著优于稠密替代方案，同时速度快一个数量级。(2) 对于 NL-PL 任务，专有稠密编码器（Voyager-3 系列）始终优于稀疏检索器，但其延迟高出 100 倍。(3) 最优分块大小随可用上下文规模而变化：在小预算下，32–64 行分块效果最佳；当上下文长度达到 16000 tokens 时，整文件检索变得具有竞争力。(4) 基于行的简单分块在各类预算下均能媲美语法感知型分块。(5) 不同配置下的检索延迟差异可达 200 倍；基于 BPE 的分块效率低下且无必要，而 BM25 与词级分块则提供了最佳的质量-延迟权衡。因此，我们根据任务需求、模型限制及计算效率，提供了实施高效代码导向 RAG 系统的实证性建议。",
        "translated_title": "大规模实用代码RAG：在计算预算约束下的任务感知检索设计选择",
        "label": [],
        "label_reason": "研究代码RAG检索优化，非推荐系统核心问题",
        "relevance_score": 3,
        "novelty_score": 6,
        "novelty_reason": "针对代码生成任务优化检索策略，非推荐算法创新"
    },
    {
        "title": "Management Of Volatile Information In Incremental Web Crawler",
        "url": "http://arxiv.org/abs/0910.1869v2",
        "pub_date": "2009-10-09",
        "summary": "Paper has been withdrawn.",
        "translated": "该论文已被撤回。",
        "translated_title": "增量网络爬虫中易失信息的管理",
        "label": [],
        "label_reason": "论文已撤回，无推荐系统相关内容",
        "relevance_score": 1,
        "novelty_score": 1,
        "novelty_reason": "论文撤回，无有效创新内容可评估"
    },
    {
        "title": "A Survey on Adversarial Information Retrieval on the Web",
        "url": "http://arxiv.org/abs/1911.11060v3",
        "pub_date": "2019-11-21",
        "summary": "This survey paper discusses different forms of malicious techniques that can affect how an information retrieval model retrieves documents for a query and their remedies.",
        "translated": "本综述论文讨论了可能影响信息检索模型根据查询检索文档的不同形式的恶意技术及其解决方案。",
        "translated_title": "网络上对抗性信息检索综述",
        "label": [],
        "label_reason": "聚焦对抗性信息检索，非推荐系统相关",
        "relevance_score": 2,
        "novelty_score": 2,
        "novelty_reason": "无推荐系统创新点或方法"
    },
    {
        "title": "Mathematical knowledge management is needed",
        "url": "http://arxiv.org/abs/cs/0410055v1",
        "pub_date": "2004-10-21",
        "summary": "In this lecture I discuss some aspects of MKM, Mathematical Knowledge Management, with particuar emphasis on information storage and information retrieval.",
        "translated": "在本讲中，我将讨论数学知识管理（MKM）的一些方面，特别侧重于信息存储与信息检索。",
        "translated_title": "数学知识管理是必需的",
        "label": [],
        "label_reason": "聚焦数学知识存储与检索，非推荐系统相关",
        "relevance_score": 2,
        "novelty_score": 3,
        "novelty_reason": "领域通用，无推荐系统创新点"
    },
    {
        "title": "A comment to \"A General Theory of IR Evaluation Measures\"",
        "url": "http://arxiv.org/abs/2303.16061v1",
        "pub_date": "2023-03-28",
        "summary": "The paper \"A General Theory of IR Evaluation Measures\" develops a formal framework to determine whether IR evaluation measures are interval scales. This comment shows some limitations about its conclusions.",
        "translated": "论文《信息检索评价指标的一般理论》构建了一个形式化框架，用于判断信息检索评价指标是否为区间尺度。本文对该文结论提出若干局限性。",
        "translated_title": "对《信息检索评估度量的一般理论》的评论",
        "label": [],
        "label_reason": "纯信息检索评估理论，与推荐系统无关",
        "relevance_score": 2,
        "novelty_score": 3,
        "novelty_reason": "非推荐领域，创新点模糊且无实际应用"
    },
    {
        "title": "A Simple Derivation of the Heap's Law from the Generalized Zipf's Law",
        "url": "http://arxiv.org/abs/1711.03066v1",
        "pub_date": "2017-11-08",
        "summary": "I reproduce a rather simple formal derivation of the Heaps' law from the generalized Zipf's law, which I previously published in Russian.",
        "translated": "我重新推导了从广义齐普夫定律出发得到赫帕斯定律的一个相当简单的形式化证明，此前我曾用俄语发表过该内容。",
        "translated_title": "堆律从广义齐普夫定律的简单推导",
        "label": [],
        "label_reason": "论文涉及语言统计模型，与推荐系统无关。",
        "relevance_score": 1,
        "novelty_score": 1,
        "novelty_reason": "复现已有推导，无新贡献。"
    },
    {
        "title": "High-performance automatic categorization and attribution of inventory catalogs",
        "url": "http://arxiv.org/abs/2202.08965v1",
        "pub_date": "2022-02-09",
        "summary": "Techniques of machine learning for automatic text categorization are applied and adapted for the problem of inventory catalog data attribution, with different approaches explored and optimal solution addressing the tradeoff between accuracy and performance is selected.",
        "translated": "机器学习中用于自动文本分类的技术被应用于库存目录数据归属问题，并针对不同方法进行了探索，最终选择了在准确率与性能之间取得最佳平衡的最优解。",
        "translated_title": "高性能的库存目录自动分类与归因",
        "label": [],
        "label_reason": "与推荐系统无直接关联，属通用文本分类任务。",
        "relevance_score": 2,
        "novelty_score": 3,
        "novelty_reason": "常规ML方法优化，无推荐系统创新点。"
    },
    {
        "title": "Using of Neuro-Indexes",
        "url": "http://arxiv.org/abs/1509.01649v1",
        "pub_date": "2015-09-05",
        "summary": "The article describes a new data structure called neuro-index. It is an alternative to well-known file indexes. The neuro-index is fundamentally different because it stores weight coefficients in neural network. It is not a reference type like \"keyword-position in a file\".",
        "translated": "本文描述了一种名为神经索引（neuro-index）的新数据结构，它可作为传统文件索引的一种替代方案。神经索引在本质上与之不同，因为它将神经网络中的权重系数作为存储内容。它并非如“文件中关键词的位置”那样的引用型结构。",
        "translated_title": "神经索引的使用",
        "label": [],
        "label_reason": "与推荐系统无直接关联，属数据库索引结构创新",
        "relevance_score": 2,
        "novelty_score": 4,
        "novelty_reason": "神经网络权重存储索引，属数据结构改进"
    },
    {
        "title": "Stanford Matrix Considered Harmful",
        "url": "http://arxiv.org/abs/0710.1962v1",
        "pub_date": "2007-10-10",
        "summary": "This note argues about the validity of web-graph data used in the literature.",
        "translated": "本笔记讨论文献中所使用的网页图数据的有效性。",
        "translated_title": "斯坦福矩阵被认为有害",
        "label": [],
        "label_reason": "论文讨论网页图数据有效性，与推荐系统无直接关联。",
        "relevance_score": 2,
        "novelty_score": 3,
        "novelty_reason": "观点陈旧，未提出新方法或理论创新。"
    },
    {
        "title": "Visual Display and Retrieval of Music Information",
        "url": "http://arxiv.org/abs/1807.10204v1",
        "pub_date": "2018-07-26",
        "summary": "This paper describes computational methods for the visual display and analysis of music information. We provide a concise description of software, music descriptors and data visualization techniques commonly used in music information retrieval. Finally, we provide use cases where the described software, descriptors and visualizations are showcased.",
        "translated": "本文描述了用于音乐信息的视觉展示与分析的计算方法。我们简要介绍了音乐信息检索中常用的软件、音乐描述符及数据可视化技术。最后，我们展示了上述软件、描述符和可视化方法的应用实例。",
        "translated_title": "音乐信息的可视化展示与检索",
        "label": [],
        "label_reason": "聚焦音乐信息可视化，非推荐系统核心环节",
        "relevance_score": 3,
        "novelty_score": 4,
        "novelty_reason": "常规可视化技术，无推荐系统创新"
    },
    {
        "title": "Experimenting with Selected Automated Approaches for Bias Analysis",
        "url": "http://arxiv.org/abs/2210.07089v2",
        "pub_date": "2022-10-13",
        "summary": "This work first presents our attempts to establish an automated model using state-of-the-art approaches for analysing bias in search results of Bing and Google. Experimental results indicate that the current class-wise F1-scores of our best model are not sufficient to establish an automated model for bias analysis. Thus, we decided not to continue with this approach.",
        "translated": "本工作首先展示了我们尝试利用最先进方法构建一个自动化模型，用于分析 Bing 和 Google 搜索结果中的偏见。实验结果表明，当前我们最优模型的类别级 F1 分数不足以支撑建立用于偏见分析的自动化模型。因此，我们决定不再继续采用此方法。",
        "translated_title": "对选定的自动化偏倚分析方法进行实验",
        "label": [],
        "label_reason": "仅涉及搜索偏见分析，非推荐系统核心环节",
        "relevance_score": 3,
        "novelty_score": 2,
        "novelty_reason": "方法陈旧，未提出新范式或显著改进"
    },
    {
        "title": "Evolution of the user's content: An Overview of the state of the art",
        "url": "http://arxiv.org/abs/1305.1787v1",
        "pub_date": "2013-05-08",
        "summary": "The evolution of the user's content still remains a problem for an accurate recommendation.This is why the current research aims to design Recommender Systems (RS) able to continually adapt information that matches the user's interests. This paper aims to explain this problematic point in outlining the proposals that have been made in research with their advantages and disadvantages.",
        "translated": "用户内容的演化仍然是实现精准推荐所面临的问题。正因如此，当前研究致力于设计能够持续适应符合用户兴趣信息的推荐系统（RS）。本文旨在通过概述研究中提出的各项方案及其优缺点，阐明这一关键问题。",
        "translated_title": "用户内容的演变：当前研究现状综述",
        "label": [
            "通用推荐技术"
        ],
        "label_reason": "聚焦用户内容演化与推荐系统自适应",
        "relevance_score": 8,
        "novelty_score": 5,
        "novelty_reason": "综述现有方案，无新架构或方法提出"
    },
    {
        "title": "Label Visualization and Exploration in IR",
        "url": "http://arxiv.org/abs/1612.03316v1",
        "pub_date": "2016-12-10",
        "summary": "There is a renaissance in visual analytics systems for data analysis and sharing, in particular, in the current wave of big data applications. We introduce RAVE, a prototype that automates the generation of an interface that uses facets and visualization techniques for exploring and analyzing relevance assessments data sets collected via crowdsourcing. We present a technical description of the main components and demonstrate its use.",
        "translated": "在数据分析与共享的视觉分析系统领域正经历一场复兴，尤其在当前大数据应用浪潮中。我们引入了RAVE，一个原型系统，可自动化生成基于分面与可视化技术的界面，用于探索和分析通过众包收集的相关性评估数据集。我们对该系统主要组件的技术细节进行描述，并展示其实际应用。",
        "translated_title": "信息检索中的标签可视化与探索",
        "label": [],
        "label_reason": "聚焦信息检索可视化，非推荐系统核心环节",
        "relevance_score": 3,
        "novelty_score": 5,
        "novelty_reason": "自动化界面生成，但无推荐算法创新"
    },
    {
        "title": "Proceedings of FACTS-IR 2019",
        "url": "http://arxiv.org/abs/1907.05755v1",
        "pub_date": "2019-07-12",
        "summary": "The proceedings list for the program of FACTS-IR 2019, the Workshop on Fairness, Accountability, Confidentiality, Transparency, and Safety in Information Retrieval held at SIGIR 2019.",
        "translated": "FACTS-IR 2019 议程手册，该研讨会于 SIGIR 2019 会议期间举办，主题为信息检索中的公平性、问责制、保密性、透明性和安全性。",
        "translated_title": "FACTS-IR 2019 论文集",
        "label": [],
        "label_reason": "仅为会议论文集，无具体推荐系统研究。",
        "relevance_score": 3,
        "novelty_score": 1,
        "novelty_reason": "无创新方法，仅为会议收录论文汇编。"
    },
    {
        "title": "An Annotated Glossary for Data Commons, Data Meshes, and Other Data Platforms",
        "url": "http://arxiv.org/abs/2404.15475v1",
        "pub_date": "2024-04-23",
        "summary": "Cloud-based data commons, data meshes, data hubs, and other data platforms are important ways to manage, analyze and share data to accelerate research and to support reproducible research. This is an annotated glossary of some of the more common terms used in articles and discussions about these platforms.",
        "translated": "基于云的数据公共库、数据网格、数据枢纽及其他数据平台，是管理、分析和共享数据以加速研究并支持可复现研究的重要方式。以下是对这些平台相关文章与讨论中常用术语的注释词汇表。",
        "translated_title": "数据共用、数据网格及其他数据平台的注释术语表",
        "label": [],
        "label_reason": "与推荐系统无直接关联，仅数据平台术语词典",
        "relevance_score": 1,
        "novelty_score": 1,
        "novelty_reason": "无创新内容，仅为术语注释集合"
    },
    {
        "title": "The Users Aren't Alright: Dangerous Mental Illness Behaviors and Recommendations",
        "url": "http://arxiv.org/abs/2209.03941v1",
        "pub_date": "2022-09-08",
        "summary": "In this paper, we argue that recommendation systems are in a unique position to propagate dangerous and cruel behaviors to people with mental illnesses.",
        "translated": "本文认为，推荐系统处于一个独特的位置，可能向有心理疾病的人群传播危险且残酷的行为。",
        "translated_title": "用户并不安好：危险的精神疾病行为与推荐",
        "label": [],
        "label_reason": "论文讨论推荐系统对心理疾病人群的风险，非推荐算法本身",
        "relevance_score": 3,
        "novelty_score": 4,
        "novelty_reason": "提出伦理问题，但无新算法或架构创新"
    },
    {
        "title": "Unlocking Insights into Business Trajectories with Transformer-based Spatio-temporal Data Analysis",
        "url": "http://arxiv.org/abs/2306.10034v1",
        "pub_date": "2023-06-07",
        "summary": "The world of business is constantly evolving and staying ahead of the curve requires a deep understanding of market trends and performance. This article addresses this requirement by modeling business trajectories using news articles data.",
        "translated": "商业世界不断演变，要保持领先需深刻理解市场趋势与表现。本文通过新闻文章数据建模业务轨迹，以应对这一需求。",
        "translated_title": "基于Transformer的时空数据分析以洞察商业轨迹",
        "label": [],
        "label_reason": "仅涉及时空数据分析，与推荐系统无关",
        "relevance_score": 2,
        "novelty_score": 3,
        "novelty_reason": "方法通用，无推荐系统创新点"
    },
    {
        "title": "Information Retrieval of Jumbled Words",
        "url": "http://arxiv.org/abs/1101.0766v1",
        "pub_date": "2011-01-04",
        "summary": "It is known that humans can easily read words where the letters have been jumbled in a certain way. This paper examines this problem by associating a distance measure with the jumbling process. Modifications to text were generated according to the Damerau-Levenshtein distance and it was checked if the users are able to read it. Graphical representations of the results are provided.",
        "translated": "众所周知，人类能够轻易阅读字母被打乱但以某种方式重组的单词。本文通过将距离度量与字母打乱过程相结合来研究该问题。根据 Damerau-Levenshtein 距离生成文本修改，并检验用户是否能够读懂这些修改后的文本。结果以图形形式呈现。",
        "translated_title": "词序打乱的信息检索",
        "label": [],
        "label_reason": "研究字母打乱阅读，与推荐系统无关。",
        "relevance_score": 2,
        "novelty_score": 3,
        "novelty_reason": "方法陈旧，无推荐系统创新点。"
    },
    {
        "title": "Where did you get that? Towards Summarization Attribution for Analysts",
        "url": "http://arxiv.org/abs/2511.08589v1",
        "pub_date": "2025-10-28",
        "summary": "Analysts require attribution, as nothing can be reported without knowing the source of the information. In this paper, we will focus on automatic methods for attribution, linking each sentence in the summary to a portion of the source text, which may be in one or more documents. We explore using a hybrid summarization, i.e., an automatic paraphrase of an extractive summary, to ease attribution. We also use a custom topology to identify the proportion of different categories of attribution-related errors.",
        "translated": "分析人员需要溯源，因为任何报告都必须知道信息的来源。在本文中，我们将重点关注自动溯源方法，即将摘要中的每一句与源文本中的相应部分关联起来，该部分可能来自一个或多个文档。我们探索使用混合摘要——即对抽取式摘要进行自动改写——以简化溯源过程。同时，我们还采用一种自定义拓扑结构来识别不同类型溯源相关错误所占的比例。",
        "translated_title": "那东西是从哪里来的？面向分析师的摘要溯源",
        "label": [],
        "label_reason": "论文聚焦文本摘要溯源，与推荐系统无直接关联。",
        "relevance_score": 2,
        "novelty_score": 3,
        "novelty_reason": "方法属通用NLP任务，创新度有限。"
    },
    {
        "title": "Enhancing the Outcome Reward-based RL Training of MLLMs with Self-Consistency Sampling",
        "url": "http://arxiv.org/abs/2511.10648v1",
        "pub_date": "2025-11-13",
        "summary": "Outcome-reward reinforcement learning (RL) is a common and increasingly significant way to refine the step-by-step reasoning of multimodal large language models (MLLMs). In the multiple-choice setting - a dominant format for multimodal reasoning benchmarks - the paradigm faces a significant yet often overlooked obstacle: unfaithful trajectories that guess the correct option after a faulty chain of thought receive the same reward as genuine reasoning, which is a flaw that cannot be ignored. We propose Self-Consistency Sampling (SCS) to correct this issue. For each question, SCS (i) introduces small visual perturbations and (ii) performs repeated truncation and resampling of an initial trajectory; agreement among the resulting trajectories yields a differentiable consistency score that down-weights unreliable traces during policy updates. Based on Qwen2.5-VL-7B-Instruct, plugging SCS into RLOO, GRPO, and REINFORCE++ series improves accuracy by up to 7.7 percentage points on six multimodal benchmarks with negligible extra computation. SCS also yields notable gains on both Qwen2.5-VL-3B-Instruct and InternVL3-8B, offering a simple, general remedy for outcome-reward RL in MLLMs.",
        "translated": "基于结果-奖励的强化学习（RL）是一种常见且日益重要的方法，用于优化多模态大语言模型（MLLMs）的逐步推理过程。在以多项选择题为主导形式的多模态推理基准测试中，该范式面临一个显著却常被忽视的障碍：那些在错误推理链之后猜对正确答案的轨迹，会与真实推理轨迹获得相同的奖励，这种缺陷不容忽视。我们提出自我一致性采样（Self-Consistency Sampling, SCS）来解决此问题。对于每个问题，SCS（i）引入微小视觉扰动，并（ii）对初始轨迹进行重复截断和重采样；由此生成的多个轨迹间达成一致性的程度可计算为一个可微分的一致性得分，在策略更新过程中降低不可靠轨迹的影响权重。基于 Qwen2.5-VL-7B-Instruct 模型，将 SCS 集成至 RLOO、GRPO 和 REINFORCE++ 系列算法后，在六个多模态基准数据集上准确率提升最高达 7.7 个百分点，且额外计算开销可忽略不计。此外，SCS 在 Qwen2.5-VL-3B-Instruct 与 InternVL3-8B 上亦取得显著性能增益，提供了一种简单通用的解决方案，以应对 MLLMs 中基于结果-奖励的强化学习问题。",
        "translated_title": "通过自一致性采样增强基于回报的强化学习训练中的多模态大语言模型",
        "label": [],
        "label_reason": "任务为多模态推理优化，非图像像素级处理。",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出采样一致性策略，提升RL训练效果。"
    },
    {
        "title": "Depth Anything 3: Recovering the Visual Space from Any Views",
        "url": "http://arxiv.org/abs/2511.10647v1",
        "pub_date": "2025-11-13",
        "summary": "We present Depth Anything 3 (DA3), a model that predicts spatially consistent geometry from an arbitrary number of visual inputs, with or without known camera poses. In pursuit of minimal modeling, DA3 yields two key insights: a single plain transformer (e.g., vanilla DINO encoder) is sufficient as a backbone without architectural specialization, and a singular depth-ray prediction target obviates the need for complex multi-task learning. Through our teacher-student training paradigm, the model achieves a level of detail and generalization on par with Depth Anything 2 (DA2). We establish a new visual geometry benchmark covering camera pose estimation, any-view geometry and visual rendering. On this benchmark, DA3 sets a new state-of-the-art across all tasks, surpassing prior SOTA VGGT by an average of 44.3% in camera pose accuracy and 25.1% in geometric accuracy. Moreover, it outperforms DA2 in monocular depth estimation. All models are trained exclusively on public academic datasets.",
        "translated": "我们提出了 Depth Anything 3（DA3），该模型可从任意数量的视觉输入中预测空间一致的几何结构，无论是否已知相机位姿。为追求最小化建模复杂度，DA3得出两个关键洞察：单个基础 Transformer（例如 vanilla DINO 编码器）即可作为骨干网络，无需架构特化；单一深度射线预测目标足以消除对复杂多任务学习的需求。通过我们的教师-学生训练范式，该模型在细节表现与泛化能力上达到了与 Depth Anything 2（DA2）相当的水平。我们建立了一个全新的视觉几何基准测试集，涵盖相机位姿估计、任意视角几何重建及视觉渲染任务。在该基准测试中，DA3 在所有任务上均刷新了现有最先进技术（SOTA）记录，其相机位姿准确率平均高出先前 SOTA 方法 VGGT 44.3%，几何准确率提升 25.1%。此外，DA3 在单目深度估计任务上也超越了 DA2。所有模型均仅在公开学术数据集上进行训练。",
        "translated_title": "Depth Anything 3：从任意视角恢复视觉空间",
        "label": [],
        "label_reason": "任务为几何预测，非像素级图像恢复",
        "relevance_score": 1,
        "novelty_score": 9,
        "novelty_reason": "提出单Transformer架构与新训练范式"
    },
    {
        "title": "One Small Step in Latent, One Giant Leap for Pixels: Fast Latent Upscale Adapter for Your Diffusion Models",
        "url": "http://arxiv.org/abs/2511.10629v1",
        "pub_date": "2025-11-13",
        "summary": "Diffusion models struggle to scale beyond their training resolutions, as direct high-resolution sampling is slow and costly, while post-hoc image super-resolution (ISR) introduces artifacts and additional latency by operating after decoding. We present the Latent Upscaler Adapter (LUA), a lightweight module that performs super-resolution directly on the generator's latent code before the final VAE decoding step. LUA integrates as a drop-in component, requiring no modifications to the base model or additional diffusion stages, and enables high-resolution synthesis through a single feed-forward pass in latent space. A shared Swin-style backbone with scale-specific pixel-shuffle heads supports 2x and 4x factors and remains compatible with image-space SR baselines, achieving comparable perceptual quality with nearly 3x lower decoding and upscaling time (adding only +0.42 s for 1024 px generation from 512 px, compared to 1.87 s for pixel-space SR using the same SwinIR architecture). Furthermore, LUA shows strong generalization across the latent spaces of different VAEs, making it easy to deploy without retraining from scratch for each new decoder. Extensive experiments demonstrate that LUA closely matches the fidelity of native high-resolution generation while offering a practical and efficient path to scalable, high-fidelity image synthesis in modern diffusion pipelines.",
        "translated": "扩散模型难以在超出其训练分辨率的情况下进行扩展，因为直接高分辨率采样速度慢且成本高昂，而事后图像超分辨率（ISR）则在解码后操作，不仅引入伪影，还会增加额外延迟。我们提出了潜空间上采样适配器（Latent Upscaler Adapter, LUA），这是一个轻量级模块，可在最终VAE解码步骤前直接对生成器的潜在编码进行超分辨率处理。LUA作为即插即用组件集成，无需修改基础模型或增加额外扩散阶段，并可通过一次潜空间前馈传递实现高分辨率合成。其共享的Swin风格主干网络结合尺度特异性的像素重排头，支持2倍和4倍上采样因子，同时保持与图像空间SR基线方法兼容，在感知质量相当的前提下，解码与上采样时间降低近3倍（例如，在512像素输入下生成1024像素图像时，仅需额外+0.42秒，相较使用相同SwinIR架构的图像空间SR方案所需1.87秒）。此外，LUA在不同VAE潜空间中展现出良好的泛化能力，使其易于部署——无需为每个新解码器从零开始重新训练。大量实验表明，LUA在保持接近原生高分辨率生成保真度的同时，为现代扩散管道提供了实用且高效的可扩展、高保真图像合成路径。",
        "translated_title": " latent 中的一小步，像素上的一大飞跃：面向扩散模型的快速潜在空间放大适配器",
        "label": [
            "超分辨率"
        ],
        "label_reason": "直接在潜在空间做超分，提升效率且保持质量。",
        "relevance_score": 9,
        "novelty_score": 8,
        "novelty_reason": "轻量模块集成，显著降低解码与上采样时间。"
    },
    {
        "title": "Querying Labeled Time Series Data with Scenario Programs",
        "url": "http://arxiv.org/abs/2511.10627v1",
        "pub_date": "2025-11-13",
        "summary": "Simulation-based testing has become a crucial complement to road testing for ensuring the safety of cyber physical systems (CPS). As a result, significant research efforts have been directed toward identifying failure scenarios within simulation environments. However, a critical question remains. Are the AV failure scenarios discovered in simulation reproducible on actual systems in the real world? The sim-to-real gap caused by differences between simulated and real sensor data means that failure scenarios identified in simulation might either be artifacts of synthetic sensor data or actual issues that also occur with real sensor data. To address this, an effective approach to validating simulated failure scenarios is to locate occurrences of these scenarios within real-world datasets and verify whether the failure persists on the datasets. To this end, we introduce a formal definition of how labeled time series sensor data can match an abstract scenario, represented as a scenario program using the Scenic probabilistic programming language. We present a querying algorithm that, given a scenario program and a labeled dataset, identifies the subset of data that matches the specified scenario. Our experiment shows that our algorithm is more accurate and orders of magnitude faster in querying scenarios than the state-of-the-art commercial vision large language models, and can scale with the duration of queried time series data.",
        "translated": "基于仿真的测试已成为确保物理信息系统的安全性的关键补充手段。因此，大量研究致力于在仿真环境中识别故障场景。然而，一个关键问题仍然存在：在仿真中发现的自动驾驶（AV）故障场景能否在真实世界系统中重现？由于仿真传感器数据与真实传感器数据之间的差异所导致的“仿真到现实”鸿沟，意味着仿真中识别出的故障场景可能要么是合成传感器数据的伪影，要么也是在真实传感器数据中同样存在的实际问题。为解决这一问题，验证仿真故障场景的有效方法是在真实世界数据集中定位这些场景的发生实例，并验证其在数据集中的持续性。为此，我们提出了一种形式化定义，说明如何将带有标签的时间序列传感器数据与抽象场景进行匹配，该抽象场景使用Scenic概率编程语言表示为场景程序。我们进一步提出一种查询算法，给定一个场景程序和一个带标签的数据集，该算法可识别符合指定场景的数据子集。实验表明，我们的算法在查询场景时比当前最先进的商业视觉大语言模型更加准确，且速度提升数个数量级，同时能够随所查询时间序列数据时长扩展而线性扩展。",
        "translated_title": "使用场景程序查询标注时间序列数据",
        "label": [],
        "label_reason": "研究聚焦场景验证，非图像像素级处理",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "提出新查询算法，效率显著提升"
    },
    {
        "title": "Towards Blind and Low-Vision Accessibility of Lightweight VLMs and Custom LLM-Evals",
        "url": "http://arxiv.org/abs/2511.10615v1",
        "pub_date": "2025-11-13",
        "summary": "Large Vision-Language Models (VLMs) excel at understanding and generating video descriptions but their high memory, computation, and deployment demands hinder practical use particularly for blind and low-vision (BLV) users who depend on detailed, context-aware descriptions. To study the effect of model size on accessibility-focused description quality, we evaluate SmolVLM2 variants with 500M and 2.2B parameters across two diverse datasets: AVCaps (outdoor), and Charades (indoor). In this work, we introduce two novel evaluation frameworks specifically designed for BLV accessibility assessment: the Multi-Context BLV Framework evaluating spatial orientation, social interaction, action events, and ambience contexts; and the Navigational Assistance Framework focusing on mobility-critical information. Additionally, we conduct a systematic evaluation of four different prompt design strategies and deploy both models on a smartphone, evaluating FP32 and INT8 precision variants to assess real-world performance constraints on resource-limited mobile devices.",
        "translated": "大型视觉-语言模型（VLMs）在理解和生成视频描述方面表现出色，但其高内存、高计算需求及部署成本限制了其在实际场景中的应用，尤其对依赖详细且上下文感知描述的盲人与低视力（BLV）用户而言。为研究模型规模对无障碍性导向描述质量的影响，我们在两个具有代表性的数据集——AVCaps（户外场景）和Charades（室内场景）上评估了SmolVLM2系列模型（参数量分别为500M与2.2B）。本研究引入两项专为BLV无障碍评估设计的新评价框架：多语境BLV框架，用于评估空间方向、社交互动、动作事件及氛围语境；以及导航辅助框架，聚焦于影响行动能力的关键信息。此外，我们系统性地评估了四种不同的提示设计策略，并在智能手机平台上部署两种模型（FP32与INT8精度变体），以考察其在资源受限移动设备上的实际性能约束。",
        "translated_title": "迈向轻量级视觉语言模型与定制化大语言模型评估的盲测与低视力可访问性",
        "label": [],
        "label_reason": "高阶视觉任务，聚焦无障碍评估与模型部署。",
        "relevance_score": 1,
        "novelty_score": 3,
        "novelty_reason": "复现式评测框架，无像素级图像处理创新。"
    },
    {
        "title": "Multitask GLocal OBIA-Mamba for Sentinel-2 Landcover Mapping",
        "url": "http://arxiv.org/abs/2511.10604v1",
        "pub_date": "2025-11-13",
        "summary": "Although Sentinel-2 based land use and land cover (LULC) classification is critical for various environmental monitoring applications, it is a very difficult task due to some key data challenges (e.g., spatial heterogeneity, context information, signature ambiguity). This paper presents a novel Multitask Glocal OBIA-Mamba (MSOM) for enhanced Sentinel-2 classification with the following contributions. First, an object-based image analysis (OBIA) Mamba model (OBIA-Mamba) is designed to reduce redundant computation without compromising fine-grained details by using superpixels as Mamba tokens. Second, a global-local (GLocal) dual-branch convolutional neural network (CNN)-mamba architecture is designed to jointly model local spatial detail and global contextual information. Third, a multitask optimization framework is designed to employ dual loss functions to balance local precision with global consistency. The proposed approach is tested on Sentinel-2 imagery in Alberta, Canada, in comparison with several advanced classification approaches, and the results demonstrate that the proposed approach achieves higher classification accuracy and finer details that the other state-of-the-art methods.",
        "translated": "尽管基于 Sentinel-2 的土地利用与土地覆盖（LULC）分类对多种环境监测应用至关重要，但由于存在若干关键数据挑战（如空间异质性、上下文信息缺失、光谱特征模糊性），该任务十分困难。本文提出了一种新颖的多任务全局-局部 OBIA-Mamba 模型（MSOM），用于提升 Sentinel-2 图像分类性能，具体贡献如下：首先，设计了一种基于对象的图像分析（OBIA）Mamba 模型（OBIA-Mamba），通过将超像素作为 Mamba 令牌，在不损失细粒度细节的前提下减少冗余计算；其次，构建了全局-局部（GLocal）双分支卷积神经网络（CNN）-Mamba 架构，以联合建模局部空间细节与全局上下文信息；第三，设计了一个多任务优化框架，采用双重损失函数平衡局部精度与全局一致性。所提方法在加拿大阿尔伯塔省的 Sentinel-2 影像上进行了测试，并与若干先进分类方法进行对比，结果表明该方法在分类精度和细节表现方面优于其他现有最先进的方法。",
        "translated_title": "面向Sentinel-2卫星的多任务全局-局部OBIA-Mamba土地覆盖制图方法",
        "label": [],
        "label_reason": "任务为高阶语义分类，非像素级图像处理",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "创新架构但用于土地覆盖分类，属高阶视觉任务"
    },
    {
        "title": "From 2D to 3D Without Extra Baggage: Data-Efficient Cancer Detection in Digital Breast Tomosynthesis",
        "url": "http://arxiv.org/abs/2511.10597v1",
        "pub_date": "2025-11-13",
        "summary": "Digital Breast Tomosynthesis (DBT) enhances finding visibility for breast cancer detection by providing volumetric information that reduces the impact of overlapping tissues; however, limited annotated data has constrained the development of deep learning models for DBT. To address data scarcity, existing methods attempt to reuse 2D full-field digital mammography (FFDM) models by either flattening DBT volumes or processing slices individually, thus discarding volumetric information. Alternatively, 3D reasoning approaches introduce complex architectures that require more DBT training data. Tackling these drawbacks, we propose M&amp;M-3D, an architecture that enables learnable 3D reasoning while remaining parameter-free relative to its FFDM counterpart, M&amp;M. M&amp;M-3D constructs malignancy-guided 3D features, and 3D reasoning is learned through repeatedly mixing these 3D features with slice-level information. This is achieved by modifying operations in M&amp;M without adding parameters, thus enabling direct weight transfer from FFDM. Extensive experiments show that M&amp;M-3D surpasses 2D projection and 3D slice-based methods by 11-54% for localization and 3-10% for classification. Additionally, M&amp;M-3D outperforms complex 3D reasoning variants by 20-47% for localization and 2-10% for classification in the low-data regime, while matching their performance in high-data regime. On the popular BCS-DBT benchmark, M&amp;M-3D outperforms previous top baseline by 4% for classification and 10% for localization.",
        "translated": "数字乳腺断层合成（DBT）通过提供体积信息，降低组织重叠的影响，从而增强乳腺癌检测的可见性；然而，标注数据有限制约了深度学习模型在 DBT 领域的发展。为应对数据稀缺问题，现有方法尝试复用二维全视野数字乳腺摄影（FFDM）模型，或通过展平 DBT 体数据、逐层处理切片的方式进行，从而丢弃了体积信息。另一种方案是引入复杂的三维推理架构，但其需要更多的 DBT 训练数据。针对上述缺陷，我们提出 M&M-3D 架构，在保持相对于其 FFDM 对应模型 M&M 的参数量不变的前提下，实现可学习的三维推理。M&M-3D 构建恶性引导的三维特征，并通过反复将这些三维特征与切片级信息混合，学习三维推理过程。该方法通过对 M&M 中的操作进行修改而无需增加参数，从而支持从 FFDM 模型直接迁移权重。大量实验表明，M&M-3D 在定位任务上较二维投影和基于切片的三维方法分别提升 11–54%，分类任务提升 3–10%。此外，在低数据场景下，M&M-3D 相对于复杂三维推理变体在定位任务上提升 20–47%，分类任务提升 2–10%，而在高数据场景下性能持平。在主流的 BCS-DBT 数据集基准测试中，M&M-3D 在分类任务上优于此前最优基线 4%，在定位任务上提升 10%。",
        "translated_title": "从二维到三维无需额外负担：数字乳腺断层合成中的高效数据癌症检测",
        "label": [],
        "label_reason": "属于医学图像分类，非像素级图像处理",
        "relevance_score": 1,
        "novelty_score": 6,
        "novelty_reason": "架构改进但无低层图像处理创新"
    },
    {
        "title": "Impact of Layer Norm on Memorization and Generalization in Transformers",
        "url": "http://arxiv.org/abs/2511.10566v1",
        "pub_date": "2025-11-13",
        "summary": "Layer Normalization (LayerNorm) is one of the fundamental components in transformers that stabilizes training and improves optimization. In recent times, Pre-LayerNorm transformers have become the preferred choice over Post-LayerNorm transformers due to their stable gradient flow. However, the impact of LayerNorm on learning and memorization across these architectures remains unclear. In this work, we investigate how LayerNorm influences memorization and learning for Pre- and Post-LayerNorm transformers. We identify that LayerNorm serves as a key factor for stable learning in Pre-LayerNorm transformers, while in Post-LayerNorm transformers, it impacts memorization. Our analysis reveals that eliminating LayerNorm parameters in Pre-LayerNorm models exacerbates memorization and destabilizes learning, while in Post-LayerNorm models, it effectively mitigates memorization by restoring genuine labels. We further precisely identify that early layers LayerNorm are the most critical over middle/later layers and their influence varies across Pre and Post LayerNorm models. We have validated it through 13 models across 6 Vision and Language datasets. These insights shed new light on the role of LayerNorm in shaping memorization and learning in transformers.",
        "translated": "层归一化（LayerNorm）是Transformer架构中的基础组件之一，能够稳定训练并提升优化性能。近年来，由于其稳定的梯度流动特性，预层归一化（Pre-LayerNorm）Transformer逐渐成为优于后层归一化（Post-LayerNorm）Transformer的首选架构。然而，LayerNorm在不同架构中对学习与记忆能力的影响仍不明确。在本研究中，我们探讨了LayerNorm如何影响预层归一化与后层归一化Transformer的记忆与学习过程。我们发现，LayerNorm在预层归一化模型中是实现稳定学习的关键因素，而在后层归一化模型中则主要影响记忆能力。我们的分析表明，在预层归一化模型中移除LayerNorm参数会加剧记忆效应并破坏学习稳定性；而在后层归一化模型中，移除LayerNorm参数则通过恢复真实标签有效缓解了记忆效应。进一步地，我们精确识别出早期层的LayerNorm相比中间及后期层更为关键，且其影响在预层归一化与后层归一化模型之间存在显著差异。我们通过6个视觉与语言数据集上的13个模型验证了上述结论。这些发现为理解LayerNorm在塑造Transformer内存与学习机制中的作用提供了新的视角。",
        "translated_title": "层归一化对Transformer中记忆与泛化能力的影响",
        "label": [],
        "label_reason": "研究Transformer层归一化对记忆与泛化影响，属高阶模型优化。",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出LayerNorm在不同架构中对记忆机制的差异化作用。"
    },
    {
        "title": "OmniVGGT: Omni-Modality Driven Visual Geometry Grounded",
        "url": "http://arxiv.org/abs/2511.10560v1",
        "pub_date": "2025-11-13",
        "summary": "General 3D foundation models have started to lead the trend of unifying diverse vision tasks, yet most assume RGB-only inputs and ignore readily available geometric cues (e.g., camera intrinsics, poses, and depth maps). To address this issue, we introduce OmniVGGT, a novel framework that can effectively benefit from an arbitrary number of auxiliary geometric modalities during both training and inference. In our framework, a GeoAdapter is proposed to encode depth and camera intrinsics/extrinsics into a spatial foundation model. It employs zero-initialized convolutions to progressively inject geometric information without disrupting the foundation model's representation space. This design ensures stable optimization with negligible overhead, maintaining inference speed comparable to VGGT even with multiple additional inputs. Additionally, a stochastic multimodal fusion regimen is proposed, which randomly samples modality subsets per instance during training. This enables an arbitrary number of modality inputs during testing and promotes learning robust spatial representations instead of overfitting to auxiliary cues. Comprehensive experiments on monocular/multi-view depth estimation, multi-view stereo, and camera pose estimation demonstrate that OmniVGGT outperforms prior methods with auxiliary inputs and achieves state-of-the-art results even with RGB-only input. To further highlight its practical utility, we integrated OmniVGGT into vision-language-action (VLA) models. The enhanced VLA model by OmniVGGT not only outperforms the vanilla point-cloud-based baseline on mainstream benchmarks, but also effectively leverages accessible auxiliary inputs to achieve consistent gains on robotic tasks.",
        "translated": "通用三维基础模型已开始引领统一多样化视觉任务的趋势，但大多数模型仅假设RGB输入，并忽略易获取的几何线索（如相机内参、位姿和深度图）。为解决这一问题，我们提出了OmniVGGT，一种新颖框架，可在训练与推理阶段有效利用任意数量的辅助几何模态。在该框架中，我们提出GeoAdapter，用于将深度与相机内参/外参编码至空间基础模型中。其采用零初始化卷积逐步注入几何信息，同时不破坏基础模型的表示空间。此设计确保优化过程稳定且开销极小，在引入多个额外输入的情况下仍能保持与VGGT相当的推理速度。此外，我们还提出了一种随机多模态融合机制，训练时按实例随机采样模态子集。这使得测试阶段可支持任意数量的模态输入，并促进学习鲁棒的空间表征，而非过度依赖辅助线索。在单目/多视角深度估计、多视角立体匹配及相机位姿估计等任务上的全面实验表明，OmniVGGT在引入辅助输入时优于现有方法，并在仅使用RGB输入时亦达到当前最优性能。为进一步凸显其实际应用价值，我们将OmniVGGT集成至视觉-语言-动作（VLA）模型中。经OmniVGGT增强后的VLA模型不仅在主流基准上超越基于点云的传统基线，还能有效利用可获取的辅助输入，在机器人任务中持续取得性能提升。",
        "translated_title": "OmniVGGT：多模态驱动的视觉几何感知模型",
        "label": [],
        "label_reason": "处理3D几何任务，非像素级图像恢复",
        "relevance_score": 2,
        "novelty_score": 8,
        "novelty_reason": "提出多模态融合框架，提升空间表示鲁棒性"
    },
    {
        "title": "A Style is Worth One Code: Unlocking Code-to-Style Image Generation with Discrete Style Space",
        "url": "http://arxiv.org/abs/2511.10555v1",
        "pub_date": "2025-11-13",
        "summary": "Innovative visual stylization is a cornerstone of artistic creation, yet generating novel and consistent visual styles remains a significant challenge. Existing generative approaches typically rely on lengthy textual prompts, reference images, or parameter-efficient fine-tuning to guide style-aware image generation, but often struggle with style consistency, limited creativity, and complex style representations. In this paper, we affirm that a style is worth one numerical code by introducing the novel task, code-to-style image generation, which produces images with novel, consistent visual styles conditioned solely on a numerical style code. To date, this field has only been primarily explored by the industry (e.g., Midjourney), with no open-source research from the academic community. To fill this gap, we propose CoTyle, the first open-source method for this task. Specifically, we first train a discrete style codebook from a collection of images to extract style embeddings. These embeddings serve as conditions for a text-to-image diffusion model (T2I-DM) to generate stylistic images. Subsequently, we train an autoregressive style generator on the discrete style embeddings to model their distribution, allowing the synthesis of novel style embeddings. During inference, a numerical style code is mapped to a unique style embedding by the style generator, and this embedding guides the T2I-DM to generate images in the corresponding style. Unlike existing methods, our method offers unparalleled simplicity and diversity, unlocking a vast space of reproducible styles from minimal input. Extensive experiments validate that CoTyle effectively turns a numerical code into a style controller, demonstrating a style is worth one code.",
        "translated": "创新的视觉风格化是艺术创作的核心，然而生成新颖且一致的视觉风格仍是一个重大挑战。现有生成方法通常依赖于冗长的文本提示、参考图像或参数高效的微调来引导风格感知的图像生成，但往往在风格一致性、创造力受限及风格表征复杂性方面存在不足。本文提出“一个风格等价于一个数值编码”的观点，并引入全新任务——编码到风格图像生成（code-to-style image generation），该任务仅通过一个数值风格编码即可生成新颖且风格一致的图像。截至目前，该领域主要由工业界探索（如 Midjourney），学术界尚无开源研究。为填补这一空白，我们提出了 CoTyle，这是首个面向该任务的开源方法。具体而言，我们首先从一组图像中训练离散风格码本以提取风格嵌入。这些嵌入作为条件输入给文生图扩散模型（T2I-DM）以生成风格化图像。随后，我们在离散风格嵌入上训练自回归风格生成器，以建模其分布，从而合成全新的风格嵌入。推理阶段，风格生成器将输入的数值风格码映射至唯一的风格嵌入，该嵌入引导 T2I-DM 生成相应风格的图像。与现有方法不同，我们的方法具备前所未有的简洁性和多样性，仅需极简输入即可解锁大量可复现风格的空间。大量实验证明，CoTyle 成功将数值码转化为风格控制器，印证了“一个风格值等于一个代码”的理念。",
        "translated_title": "一种风格抵得上一行代码：通过离散风格空间解锁代码到风格的图像生成",
        "label": [],
        "label_reason": "生成新风格图像属高阶视觉任务",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "首次开放源代码实现代码到风格生成"
    },
    {
        "title": "Benchmarking Diversity in Image Generation via Attribute-Conditional Human Evaluation",
        "url": "http://arxiv.org/abs/2511.10547v1",
        "pub_date": "2025-11-13",
        "summary": "Despite advances in generation quality, current text-to-image (T2I) models often lack diversity, generating homogeneous outputs. This work introduces a framework to address the need for robust diversity evaluation in T2I models. Our framework systematically assesses diversity by evaluating individual concepts and their relevant factors of variation. Key contributions include: (1) a novel human evaluation template for nuanced diversity assessment; (2) a curated prompt set covering diverse concepts with their identified factors of variation (e.g. prompt: An image of an apple, factor of variation: color); and (3) a methodology for comparing models in terms of human annotations via binomial tests.   Furthermore, we rigorously compare various image embeddings for diversity measurement. Notably, our principled approach enables ranking of T2I models by diversity, identifying categories where they particularly struggle. This research offers a robust methodology and insights, paving the way for improvements in T2I model diversity and metric development.",
        "translated": "尽管文本到图像（T2I）模型的生成质量已取得进步，但当前模型往往缺乏多样性，输出结果趋于同质化。本文提出一种框架，以应对 T2I 模型中对稳健多样性评估的需求。我们的框架通过系统性评估个体概念及其相关变化因素，来衡量多样性。主要贡献包括：（1）一种新颖的人类评价模板，用于细致评估多样性；（2）一套精心策划的提示词集合，涵盖多样化的概念及其识别出的变化因素（例如：提示词“一个苹果的图像”，变化因素“颜色”）；以及（3）一种基于二项检验方法，通过人类标注比较不同模型多样性的方法。  \n此外，我们严格对比了多种图像嵌入方式在多样性测量中的表现。值得注意的是，我们的系统方法能够依据多样性对 T2I 模型进行排序，并识别出它们特别难以处理的概念类别。本研究提供了一种稳健的方法论与洞察，为提升 T2I 模型的多样性及开发相关评估指标奠定了基础。",
        "translated_title": "通过属性条件人工评估对图像生成多样性进行基准测试",
        "label": [],
        "label_reason": "属图像生成评估，非像素级图像处理",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出多样性评估框架，但非图像恢复任务"
    },
    {
        "title": "Dynamic Avatar-Scene Rendering from Human-centric Context",
        "url": "http://arxiv.org/abs/2511.10539v1",
        "pub_date": "2025-11-13",
        "summary": "Reconstructing dynamic humans interacting with real-world environments from monocular videos is an important and challenging task. Despite considerable progress in 4D neural rendering, existing approaches either model dynamic scenes holistically or model scenes and backgrounds separately aim to introduce parametric human priors. However, these approaches either neglect distinct motion characteristics of various components in scene especially human, leading to incomplete reconstructions, or ignore the information exchange between the separately modeled components, resulting in spatial inconsistencies and visual artifacts at human-scene boundaries. To address this, we propose {\\bf Separate-then-Map} (StM) strategy that introduces a dedicated information mapping mechanism to bridge separately defined and optimized models. Our method employs a shared transformation function for each Gaussian attribute to unify separately modeled components, enhancing computational efficiency by avoiding exhaustive pairwise interactions while ensuring spatial and visual coherence between humans and their surroundings. Extensive experiments on monocular video datasets demonstrate that StM significantly outperforms existing state-of-the-art methods in both visual quality and rendering accuracy, particularly at challenging human-scene interaction boundaries.",
        "translated": "从单目视频中重建与真实环境交互的动态人体是一项重要且具有挑战性的任务。尽管在4D神经渲染领域已取得显著进展，现有方法要么整体建模动态场景，要么将场景与背景分离建模以引入参数化的人体先验。然而，这些方法要么忽视了场景各组成部分（尤其是人体）独特的运动特性，导致重建不完整；要么忽略了独立建模组件间的信息交互，从而在人-场景边界处产生空间不一致性和视觉伪影。为解决此问题，我们提出**Separate-then-Map**（StM）策略，引入专门的信息映射机制，以弥合独立定义和优化模型之间的鸿沟。该方法对每个高斯属性采用共享变换函数，统一独立建模的组成部分，在避免穷举式成对交互的同时提升计算效率，并确保人体与其周围环境在空间和视觉上的一致性。在多个单目视频数据集上的广泛实验表明，StM在视觉质量与渲染精度方面均显著优于现有最先进方法，尤其在复杂的人-场景交互边界处表现突出。",
        "translated_title": "以人为中心的上下文驱动的动态角色-场景渲染",
        "label": [],
        "label_reason": "任务属4D神经渲染，非像素级图像处理",
        "relevance_score": 2,
        "novelty_score": 8,
        "novelty_reason": "提出StM机制提升人-场景交互一致性"
    },
    {
        "title": "SemanticVLA: Semantic-Aligned Sparsification and Enhancement for Efficient Robotic Manipulation",
        "url": "http://arxiv.org/abs/2511.10518v1",
        "pub_date": "2025-11-13",
        "summary": "Vision-Language-Action (VLA) models have advanced in robotic manipulation, yet practical deployment remains hindered by two key limitations: 1) perceptual redundancy, where irrelevant visual inputs are processed inefficiently, and 2) superficial instruction-vision alignment, which hampers semantic grounding of actions. In this paper, we propose SemanticVLA, a novel VLA framework that performs Semantic-Aligned Sparsification and Enhancement for Efficient Robotic Manipulation. Specifically: 1) To sparsify redundant perception while preserving semantic alignment, Semantic-guided Dual Visual Pruner (SD-Pruner) performs: Instruction-driven Pruner (ID-Pruner) extracts global action cues and local semantic anchors in SigLIP; Spatial-aggregation Pruner (SA-Pruner) compacts geometry-rich features into task-adaptive tokens in DINOv2. 2) To exploit sparsified features and integrate semantics with spatial geometry, Semantic-complementary Hierarchical Fuser (SH-Fuser) fuses dense patches and sparse tokens across SigLIP and DINOv2 for coherent representation. 3) To enhance the transformation from perception to action, Semantic-conditioned Action Coupler (SA-Coupler) replaces the conventional observation-to-DoF approach, yielding more efficient and interpretable behavior modeling for manipulation tasks. Extensive experiments on simulation and real-world tasks show that SemanticVLA sets a new SOTA in both performance and efficiency. SemanticVLA surpasses OpenVLA on LIBERO benchmark by 21.1% in success rate, while reducing training cost and inference latency by 3.0-fold and 2.7-fold.SemanticVLA is open-sourced and publicly available at https://github.com/JiuTian-VL/SemanticVLA",
        "translated": "视觉-语言-动作（VLA）模型在机器人操作领域已取得显著进展，但其实际部署仍受两大关键限制制约：1）感知冗余，即无关视觉输入被低效处理；2）表层指令-视觉对齐，阻碍了动作的语义锚定。本文提出 SemanticVLA，一种面向高效机器人操作的新型 VLA 框架，实现语义对齐下的稀疏化与增强。具体而言：1）为在保留语义对齐的前提下压缩冗余感知，我们设计了语义引导双视觉剪枝器（SD-Pruner），包括：指令驱动剪枝器（ID-Pruner），从 SigLIP 中提取全局动作线索与局部语义锚点；空间聚合剪枝器（SA-Pruner），将富含几何结构的特征压缩为适配任务的稀疏令牌，应用于 DINOv2。2）为利用稀疏化特征并融合语义与空间几何信息，我们构建了语义互补分层融合器（SH-Fuser），在 SigLIP 与 DINOv2 间融合密集补丁与稀疏令牌，生成连贯表征。3）为提升感知到动作的映射效率，我们引入语义条件动作耦合器（SA-Coupler），替代传统观测至自由度（DoF）的转换范式，从而实现更高效、可解释的行为建模，适用于操作任务。在仿真与真实任务上的大量实验表明，SemanticVLA 在性能与效率上均达到当前最优（SOTA）。SemanticVLA 在 LIBERO 基准测试中以 21.1% 的成功率超越 OpenVLA，同时训练成本和推理延迟分别降低 3.0 倍与 2.7 倍。SemanticVLA 已开源并公开获取于 https://github.com/JiuTian-VL/SemanticVLA",
        "translated_title": "SemanticVLA：面向高效机器人操作的语义对齐稀疏化与增强",
        "label": [],
        "label_reason": "任务属高阶视觉与机器人控制，非像素级图像处理",
        "relevance_score": 1,
        "novelty_score": 6,
        "novelty_reason": "方法优化感知压缩与语义对齐，非图像恢复类创新"
    },
    {
        "title": "Learnable Total Variation with Lambda Mapping for Low-Dose CT Denoising",
        "url": "http://arxiv.org/abs/2511.10500v1",
        "pub_date": "2025-11-13",
        "summary": "Although Total Variation (TV) performs well in noise reduction and edge preservation on images, its dependence on the lambda parameter limits its efficiency and makes it difficult to use effectively. In this study, we present a Learnable Total Variation (LTV) framework that couples an unrolled TV solver with a data-driven Lambda Mapping Network (LambdaNet) predicting a per-pixel regularization map. The pipeline is trained end-to-end so that reconstruction and regularization are optimized jointly, yielding spatially adaptive smoothing: strong in homogeneous regions, relaxed near anatomical boundaries. Experiments on the DeepLesion dataset, using a realistic noise model adapted from the LoDoPaB-CT methodology, show consistent gains over classical TV and FBP+U-Net: +2.9 dB PSNR and +6% SSIM on average. LTV provides an interpretable alternative to black-box CNNs and a basis for 3D and data-consistency-driven reconstruction. Our codes are available at: https://github.com/itu-biai/deep_tv_for_ldct",
        "translated": "尽管总变差（TV）在图像去噪和边缘保持方面表现良好，但其对lambda参数的依赖性限制了其效率，并使其难以有效应用。在本研究中，我们提出了一种可学习的总变差（LTV）框架，该框架将未卷积的TV求解器与一个数据驱动的Lambda映射网络（LambdaNet）相结合，后者预测每个像素点的正则化图。该流程采用端到端训练方式，使重建与正则化联合优化，从而实现空间自适应平滑：在均匀区域表现出强平滑效果，在解剖边界附近则较为松弛。在DeepLesion数据集上的实验中，我们采用了借鉴LoDoPaB-CT方法论的现实噪声模型，结果显示LTV相比经典TV及FBP+U-Net方法均取得一致提升：平均PSNR提高2.9 dB，SSIM提高6%。LTV为黑盒CNN提供了一种可解释的替代方案，并为三维重建及数据一致性驱动的重建奠定了基础。我们的代码开源于：https://github.com/itu-biai/deep_tv_for_ldct",
        "translated_title": "基于可学习总变差与Lambda映射的低剂量CT图像去噪",
        "label": [
            "图像去噪",
            "CT金属伪影消除"
        ],
        "label_reason": "专为低剂量CT去噪设计，改进TV正则化",
        "relevance_score": 10,
        "novelty_score": 9,
        "novelty_reason": "提出可学习TV框架，联合优化正则化参数"
    },
    {
        "title": "SPOT: Sparsification with Attention Dynamics via Token Relevance in Vision Transformers",
        "url": "http://arxiv.org/abs/2511.10488v1",
        "pub_date": "2025-11-13",
        "summary": "While Vision Transformers (ViT) have demonstrated remarkable performance across diverse tasks, their computational demands are substantial, scaling quadratically with the number of processed tokens. Compact attention representations, reflecting token interaction distributions, can guide early detection and reduction of less salient tokens prior to attention computation. Motivated by this, we present SParsification with attentiOn dynamics via Token relevance (SPOT), a framework for early detection of redundant tokens within ViTs that leverages token embeddings, interactions, and attention dynamics across layers to infer token importance, resulting in a more context-aware and interpretable relevance detection process. SPOT informs token sparsification and facilitates the elimination of such tokens, improving computational efficiency without sacrificing performance. SPOT employs computationally lightweight predictors that can be plugged into various ViT architectures and learn to derive effective input-specific token prioritization across layers. Its versatile design supports a range of performance levels adaptable to varying resource constraints. Empirical evaluations demonstrate significant efficiency gains of up to 40% compared to standard ViTs, while maintaining or even improving accuracy. Code and models are available at https://github.com/odedsc/SPOT .",
        "translated": "尽管视觉Transformer（ViT）在多种任务中展现出卓越性能，其计算开销巨大，且随处理token数量呈二次方增长。紧凑的注意力表示能够反映token间的交互分布，从而指导在注意力计算前对不显著token进行早期检测与削减。受此启发，我们提出了一种基于Token相关性动态的稀疏化框架——SPOT（SParsification with attentiOn dynamics via Token relevance），该框架通过利用跨层的token嵌入、交互关系及注意力动态来推断token的重要性，实现更具备上下文感知性和可解释性的相关性检测过程。SPOT引导token稀疏化，并促进冗余token的消除，在提升计算效率的同时不牺牲性能。SPOT采用计算开销低的预测器，可无缝集成至各类ViT架构中，学习在不同层间生成针对输入的有效token优先级排序。其灵活的设计支持多种性能层级，以适应不同的资源约束条件。实证评估表明，相比标准ViT，SPOT可实现高达40%的效率提升，同时保持或进一步提高准确率。代码和模型可在 https://github.com/odedsc/SPOT 获取。",
        "translated_title": "SPOT：通过令牌相关性实现视觉Transformer中的稀疏化与注意力动态调整",
        "label": [],
        "label_reason": "聚焦模型压缩，非图像像素级质量恢复",
        "relevance_score": 2,
        "novelty_score": 8,
        "novelty_reason": "提出轻量预测器优化ViT效率，无图像处理创新"
    },
    {
        "title": "Utility of Pancreas Surface Lobularity as a CT Biomarker for Opportunistic Screening of Type 2 Diabetes",
        "url": "http://arxiv.org/abs/2511.10484v1",
        "pub_date": "2025-11-13",
        "summary": "Type 2 Diabetes Mellitus (T2DM) is a chronic metabolic disease that affects millions of people worldwide. Early detection is crucial as it can alter pancreas function through morphological changes and increased deposition of ectopic fat, eventually leading to organ damage. While studies have shown an association between T2DM and pancreas volume and fat content, the role of increased pancreatic surface lobularity (PSL) in patients with T2DM has not been fully investigated. In this pilot work, we propose a fully automated approach to delineate the pancreas and other abdominal structures, derive CT imaging biomarkers, and opportunistically screen for T2DM. Four deep learning-based models were used to segment the pancreas in an internal dataset of 584 patients (297 males, 437 non-diabetic, age: 45$\\pm$15 years). PSL was automatically detected and it was higher for diabetic patients (p=0.01) at 4.26 $\\pm$ 8.32 compared to 3.19 $\\pm$ 3.62 for non-diabetic patients. The PancAP model achieved the highest Dice score of 0.79 $\\pm$ 0.17 and lowest ASSD error of 1.94 $\\pm$ 2.63 mm (p$&lt;$0.05). For predicting T2DM, a multivariate model trained with CT biomarkers attained 0.90 AUC, 66.7\\% sensitivity, and 91.9\\% specificity. Our results suggest that PSL is useful for T2DM screening and could potentially help predict the early onset of T2DM.",
        "translated": "2型糖尿病（T2DM）是一种影响全球数千万人群的慢性代谢性疾病。早期检测至关重要，因为其可通过形态学改变和异位脂肪沉积影响胰腺功能，最终导致器官损伤。尽管已有研究表明T2DM与胰腺体积及脂肪含量相关，但其在T2DM患者中胰腺表面叶状性（PSL）增加的作用尚未被充分研究。在本项初步工作中，我们提出了一种全自动方法，用于勾画胰腺及其他腹部结构、提取CT影像生物标志物，并借此机会筛查T2DM。我们在包含584名患者的内部数据集中使用了四种基于深度学习的模型对胰腺进行分割（其中男性297例，非糖尿病患者437例，年龄：45±15岁）。PSL自动检测结果显示，糖尿病患者（p=0.01）的值为4.26 ± 8.32，显著高于非糖尿病患者（3.19 ± 3.62）。PancAP模型取得了最高的Dice得分（0.79 ± 0.17）和最低的ASSD误差（1.94 ± 2.63 mm，p<0.05）。在预测T2DM方面，利用CT生物标志物训练的多元模型达到了0.90的AUC值、66.7%的敏感性和91.9%的特异性。我们的结果表明，PSL可用于T2DM筛查，且可能有助于预测T2DM的早期发生。",
        "translated_title": "胰腺表面分叶性作为CT影像生物标志物在2型糖尿病机会性筛查中的应用价值",
        "label": [],
        "label_reason": "属于医学图像分割与疾病预测，非低层图像处理",
        "relevance_score": 1,
        "novelty_score": 3,
        "novelty_reason": "复现式分割+临床预测，无新算法创新"
    },
    {
        "title": "Intrinsic Dimensionality as a Model-Free Measure of Class Imbalance",
        "url": "http://arxiv.org/abs/2511.10475v1",
        "pub_date": "2025-11-13",
        "summary": "Imbalance in classification tasks is commonly quantified by the cardinalities of examples across classes. This, however, disregards the presence of redundant examples and inherent differences in the learning difficulties of classes. Alternatively, one can use complex measures such as training loss and uncertainty, which, however, depend on training a machine learning model. Our paper proposes using data Intrinsic Dimensionality (ID) as an easy-to-compute, model-free measure of imbalance that can be seamlessly incorporated into various imbalance mitigation methods. Our results across five different datasets with a diverse range of imbalance ratios show that ID consistently outperforms cardinality-based re-weighting and re-sampling techniques used in the literature. Moreover, we show that combining ID with cardinality can further improve performance. Code: https://github.com/cagries/IDIM.",
        "translated": "分类任务中的不平衡性通常通过各类别样本数量的基数来量化。然而，这种方法忽略了冗余样本的存在以及各类别固有的学习难度差异。另一种方法是采用训练损失和不确定性等复杂度量，但这些度量依赖于机器学习模型的训练过程。本文提出使用数据内在维度（Intrinsic Dimensionality, ID）作为一种易于计算、无需模型的不平衡性度量，并可无缝集成到各种不平衡缓解方法中。我们在五个具有不同不平衡比率的多样化数据集上的实验结果表明，ID始终优于文献中常用的基于基数的重加权与重采样技术。此外，我们还证明，将ID与基数结合使用可以进一步提升性能。代码：https://github.com/cagries/IDIM。",
        "translated_title": "内在维度作为无模型的类别不平衡度量",
        "label": [],
        "label_reason": "属于高阶分类任务，非图像处理",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出ID作为无模型不平衡度量，有效且易计算"
    },
    {
        "title": "OpenSR-SRGAN: A Flexible Super-Resolution Framework for Multispectral Earth Observation Data",
        "url": "http://arxiv.org/abs/2511.10461v1",
        "pub_date": "2025-11-13",
        "summary": "We present OpenSR-SRGAN, an open and modular framework for single-image super-resolution in Earth Observation. The software provides a unified implementation of SRGAN-style models that is easy to configure, extend, and apply to multispectral satellite data such as Sentinel-2. Instead of requiring users to modify model code, OpenSR-SRGAN exposes generators, discriminators, loss functions, and training schedules through concise configuration files, making it straightforward to switch between architectures, scale factors, and band setups. The framework is designed as a practical tool and benchmark implementation rather than a state-of-the-art model. It ships with ready-to-use configurations for common remote sensing scenarios, sensible default settings for adversarial training, and built-in hooks for logging, validation, and large-scene inference. By turning GAN-based super-resolution into a configuration-driven workflow, OpenSR-SRGAN lowers the entry barrier for researchers and practitioners who wish to experiment with SRGANs, compare models in a reproducible way, and deploy super-resolution pipelines across diverse Earth-observation datasets.",
        "translated": "我们提出了 OpenSR-SRGAN，这是一个面向地球观测单图像超分辨率任务的开放且模块化的框架。该软件提供了一种统一实现的 SRGAN 类模型，易于配置、扩展，并可应用于 Sentinel-2 等多光谱卫星数据。与要求用户修改模型代码不同，OpenSR-SRGAN 通过简洁的配置文件暴露生成器、判别器、损失函数和训练计划，使切换不同架构、尺度因子和波段设置变得简单直接。本框架旨在作为实用工具和基准实现，而非追求最先进性能的模型。它内置了适用于常见遥感场景的即用型配置、对抗训练的合理默认设置，以及用于日志记录、验证和大范围场景推理的内置钩子。通过将基于 GAN 的超分辨率转化为由配置驱动的工作流程，OpenSR-SRGAN 降低了研究人员和从业者尝试 SRGAN 模型、以可复现方式比较模型以及在多样化的地球观测数据集上部署超分辨率管线的入门门槛。",
        "translated_title": "OpenSR-SRGAN：一种面向多光谱地球观测数据的灵活超分辨率框架",
        "label": [
            "超分辨率",
            "遥感图像复原"
        ],
        "label_reason": "专注遥感图像超分辨，属低层图像恢复任务。",
        "relevance_score": 9,
        "novelty_score": 4,
        "novelty_reason": "框架模块化设计，非算法创新但实用性强。"
    },
    {
        "title": "Histology-informed tiling of whole tissue sections improves the interpretability and predictability of cancer relapse and genetic alterations",
        "url": "http://arxiv.org/abs/2511.10432v1",
        "pub_date": "2025-11-13",
        "summary": "Histopathologists establish cancer grade by assessing histological structures, such as glands in prostate cancer. Yet, digital pathology pipelines often rely on grid-based tiling that ignores tissue architecture. This introduces irrelevant information and limits interpretability. We introduce histology-informed tiling (HIT), which uses semantic segmentation to extract glands from whole slide images (WSIs) as biologically meaningful input patches for multiple-instance learning (MIL) and phenotyping. Trained on 137 samples from the ProMPT cohort, HIT achieved a gland-level Dice score of 0.83 +/- 0.17. By extracting 380,000 glands from 760 WSIs across ICGC-C and TCGA-PRAD cohorts, HIT improved MIL models AUCs by 10% for detecting copy number variation (CNVs) in genes related to epithelial-mesenchymal transitions (EMT) and MYC, and revealed 15 gland clusters, several of which were associated with cancer relapse, oncogenic mutations, and high Gleason. Therefore, HIT improved the accuracy and interpretability of MIL predictions, while streamlining computations by focussing on biologically meaningful structures during feature extraction.",
        "translated": "病理科医生通过评估组织学结构（如前列腺癌中的腺体）来确定癌症等级。然而，数字病理分析流程通常依赖基于网格的图像分块方式，该方法忽略了组织架构信息，引入了无关信息并限制了可解释性。我们提出了一种“组织学引导的分块”方法（Histology-informed Tiling, HIT），该方法利用语义分割从全切片图像（WSIs）中提取腺体，作为多实例学习（MIL）和表型分析中具有生物学意义的输入块。在 ProMPT 数据集中 137 个样本上训练后，HIT 在腺体级别实现了 Dice 分数 0.83 ± 0.17。通过对 ICGC-C 和 TCGA-PRAD 数据集中共计 760 个 WSI 提取 380,000 个腺体，HIT 将用于检测与上皮-间质转化（EMT）及 MYC 相关基因拷贝数变异（CNVs）的 MIL 模型 AUC 提升了 10%，并揭示了 15 类腺体聚类，其中若干聚类与癌症复发、致癌突变及高 Gleason 分数相关。因此，HIT 不仅提升了 MIL 预测的准确性与可解释性，还在特征提取过程中聚焦于具有生物学意义的结构，从而简化了计算流程。",
        "translated_title": "基于组织学信息的全组织切片分块方法可提升癌症复发及遗传变异的可解释性与预测性",
        "label": [],
        "label_reason": "任务为病理图像分割与预测，属高阶医学分析",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "改进采样策略，非像素级图像处理创新"
    },
    {
        "title": "RodEpil: A Video Dataset of Laboratory Rodents for Seizure Detection and Benchmark Evaluation",
        "url": "http://arxiv.org/abs/2511.10431v1",
        "pub_date": "2025-11-13",
        "summary": "We introduce a curated video dataset of laboratory rodents for automatic detection of convulsive events. The dataset contains short (10~s) top-down and side-view video clips of individual rodents, labeled at clip level as normal activity or seizure. It includes 10,101 negative samples and 2,952 positive samples collected from 19 subjects. We describe the data curation, annotation protocol and preprocessing pipeline, and report baseline experiments using a transformer-based video classifier (TimeSformer). Experiments employ five-fold cross-validation with strict subject-wise partitioning to prevent data leakage (no subject appears in more than one fold). Results show that the TimeSformer architecture enables discrimination between seizure and normal activity with an average F1-score of 97%. The dataset and baseline code are publicly released to support reproducible research on non-invasive, video-based monitoring in preclinical epilepsy research. RodEpil Dataset access - DOI: 10.5281/zenodo.17601357",
        "translated": "我们引入了一个经过整理的实验用啮齿动物视频数据集，用于自动检测癫痫样抽搐事件。该数据集包含多个个体啮齿动物的短时（10秒左右）俯视和侧视视频片段，并在片段级别标注为正常活动或癫痫发作。数据集中包含来自19个受试对象的10,101个负样本和2,952个正样本。我们描述了数据整理、标注协议与预处理流程，并使用基于Transformer的视频分类器（TimeSformer）进行了基线实验。实验采用五折交叉验证，且严格按受试者划分以防止数据泄露（每个受试者仅出现在一个折中）。结果表明，TimeSformer架构能够在癫痫发作与正常活动之间实现高区分能力，平均F1分数达到97%。该数据集及基线代码已公开发布，旨在支持对临床前癫痫研究中无创视频监测方法的可重复性研究。RodEpil 数据集访问 - DOI: 10.5281/zenodo.17601357",
        "translated_title": "RodEpil：用于癫痫检测与基准评估的实验鼠视频数据集",
        "label": [],
        "label_reason": "高阶视频分类任务，非图像像素级处理",
        "relevance_score": 1,
        "novelty_score": 3,
        "novelty_reason": "复现性实验，无本质创新"
    },
    {
        "title": "Don't Waste It: Guiding Generative Recommenders with Structured Human Priors via Multi-head Decoding",
        "url": "http://arxiv.org/abs/2511.10492v1",
        "pub_date": "2025-11-13",
        "summary": "Optimizing recommender systems for objectives beyond accuracy, such as diversity, novelty, and personalization, is crucial for long-term user satisfaction. To this end, industrial practitioners have accumulated vast amounts of structured domain knowledge, which we term human priors (e.g., item taxonomies, temporal patterns). This knowledge is typically applied through post-hoc adjustments during ranking or post-ranking. However, this approach remains decoupled from the core model learning, which is particularly undesirable as the industry shifts to end-to-end generative recommendation foundation models. On the other hand, many methods targeting these beyond-accuracy objectives often require architecture-specific modifications and discard these valuable human priors by learning user intent in a fully unsupervised manner.   Instead of discarding the human priors accumulated over years of practice, we introduce a backbone-agnostic framework that seamlessly integrates these human priors directly into the end-to-end training of generative recommenders. With lightweight, prior-conditioned adapter heads inspired by efficient LLM decoding strategies, our approach guides the model to disentangle user intent along human-understandable axes (e.g., interaction types, long- vs. short-term interests). We also introduce a hierarchical composition strategy for modeling complex interactions across different prior types. Extensive experiments on three large-scale datasets demonstrate that our method significantly enhances both accuracy and beyond-accuracy objectives. We also show that human priors allow the backbone model to more effectively leverage longer context lengths and larger model sizes.",
        "translated": "优化推荐系统以实现除准确率之外的目标（如多样性、新颖性与个性化），对长期用户满意度至关重要。为此，工业界从业者已积累大量结构化的领域知识，我们将其称为“人类先验”（例如：物料分类体系、时间模式等）。此类知识通常在排序或重排阶段通过事后调整加以应用。然而，该方法仍与核心模型学习过程相分离，这尤其在行业向端到端生成式推荐基础模型演进的过程中尤为不可取。另一方面，许多旨在达成上述非准确率目标的方法往往需要特定架构的修改，并通过完全无监督的方式学习用户意图，从而舍弃这些宝贵的“人类先验”。我们并未抛弃多年实践中积累的“人类先验”，而是提出一种与骨干模型无关的框架，将这些先验直接无缝整合至生成式推荐系统的端到端训练流程中。该方法采用受高效大语言模型解码策略启发的轻量级、先验条件化适配器头，引导模型沿人类可理解的维度（如交互类型、长期兴趣与短期兴趣）解耦用户意图。此外，我们还引入了一种分层组合策略，用于建模不同先验类型之间的复杂交互关系。在三个大规模数据集上的广泛实验表明，我们的方法显著提升了准确率及非准确率目标的表现。我们也证明，“人类先验”使骨干模型能够更有效地利用更长的上下文长度和更大的模型规模。",
        "translated_title": "别浪费它：通过多头解码引导生成式推荐器利用结构化人类先验",
        "label": [
            "LLM生成式推荐",
            "通用推荐技术"
        ],
        "label_reason": "直接解决生成式推荐中融入结构化先验知识问题",
        "relevance_score": 10,
        "novelty_score": 9,
        "novelty_reason": "首创多头解码引导框架整合人类先验于端到端训练"
    },
    {
        "title": "Fixed-Persona SLMs with Modular Memory: Scalable NPC Dialogue on Consumer Hardware",
        "url": "http://arxiv.org/abs/2511.10277v1",
        "pub_date": "2025-11-13",
        "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in generating human-like text, yet their applicability to dialogue systems in computer games remains limited. This limitation arises from their substantial hardware requirements, latency constraints, and the necessity to maintain clearly defined knowledge boundaries within a game setting. In this paper, we propose a modular NPC dialogue system that leverages Small Language Models (SLMs), fine-tuned to encode specific NPC personas and integrated with runtime-swappable memory modules. These memory modules preserve character-specific conversational context and world knowledge, enabling expressive interactions and long-term memory without retraining or model reloading during gameplay. We comprehensively evaluate our system using three open-source SLMs: DistilGPT-2, TinyLlama-1.1B-Chat, and Mistral-7B-Instruct, trained on synthetic persona-aligned data and benchmarked on consumer-grade hardware. While our approach is motivated by applications in gaming, its modular design and persona-driven memory architecture hold significant potential for broader adoption in domains requiring expressive, scalable, and memory-rich conversational agents, such as virtual assistants, customer support bots, or interactive educational systems.",
        "translated": "大语言模型（Large Language Models, LLMs）在生成类人文本方面展现出显著能力，但其在计算机游戏对话系统中的适用性仍受限。这一局限源于其高昂的硬件需求、延迟约束，以及在游戏场景中维持明确知识边界的必要性。本文提出一种模块化非玩家角色（NPC）对话系统，该系统基于小型语言模型（Small Language Models, SLMs），经微调以编码特定NPC人设，并与可在运行时动态切换的记忆模块集成。这些记忆模块保存角色专属的对话上下文及世界观知识，从而在游戏过程中无需重新训练或加载模型即可实现富有表现力的交互与长期记忆。我们在三款开源SLM上全面评估了该系统：DistilGPT-2、TinyLlama-1.1B-Chat 和 Mistral-7B-Instruct，这些模型均在合成的人设对齐数据上进行训练，并在消费级硬件上进行了基准测试。虽然我们的方法最初面向游戏应用，但其模块化设计与基于人设的记忆架构，对于需要具备表达力、可扩展性和丰富记忆能力的对话代理的其他领域（如虚拟助手、客户服务机器人或互动教育系统）也具有广泛的推广潜力。",
        "translated_title": "具有模块化记忆的固定人格SLM：在消费级硬件上实现可扩展NPC对话",
        "label": [
            "LLM生成式推荐",
            "通用推荐技术"
        ],
        "label_reason": "基于SLM的对话系统，可适配推荐场景中的用户画像与交互建模。",
        "relevance_score": 4,
        "novelty_score": 7,
        "novelty_reason": "模块化记忆架构提升效率，适合多角色场景应用。"
    },
    {
        "title": "GPR: Towards a Generative Pre-trained One-Model Paradigm for Large-Scale Advertising Recommendation",
        "url": "http://arxiv.org/abs/2511.10138v1",
        "pub_date": "2025-11-13",
        "summary": "As an intelligent infrastructure connecting users with commercial content, advertising recommendation systems play a central role in information flow and value creation within the digital economy. However, existing multi-stage advertising recommendation systems suffer from objective misalignment and error propagation, making it difficult to achieve global optimality, while unified generative recommendation models still struggle to meet the demands of practical industrial applications. To address these issues, we propose GPR (Generative Pre-trained Recommender), the first one-model framework that redefines advertising recommendation as an end-to-end generative task, replacing the traditional cascading paradigm with a unified generative approach. To realize GPR, we introduce three key innovations spanning unified representation, network architecture, and training strategy. First, we design a unified input schema and tokenization method tailored to advertising scenarios, mapping both ads and organic content into a shared multi-level semantic ID space, thereby enhancing semantic alignment and modeling consistency across heterogeneous data. Second, we develop the Heterogeneous Hierarchical Decoder (HHD), a dual-decoder architecture that decouples user intent modeling from ad generation, achieving a balance between training efficiency and inference flexibility while maintaining strong modeling capacity. Finally, we propose a multi-stage joint training strategy that integrates Multi-Token Prediction (MTP), Value-Aware Fine-Tuning and the Hierarchy Enhanced Policy Optimization (HEPO) algorithm, forming a complete generative recommendation pipeline that unifies interest modeling, value alignment, and policy optimization. GPR has been fully deployed in the Tencent Weixin Channels advertising system, delivering significant improvements in key business metrics including GMV and CTCVR.",
        "translated": "作为连接用户与商业内容的智能基础设施，广告推荐系统在数字经济的信息流动与价值创造中扮演着核心角色。然而，现有的多阶段广告推荐系统存在目标不一致与误差传播问题，难以实现全局最优；同时，统一的生成式推荐模型仍难以满足实际工业应用的需求。为解决上述问题，我们提出 GPR（Generative Pre-trained Recommender），这是首个将广告推荐重新定义为端到端生成任务的一体化模型框架，用统一的生成范式替代传统级联架构。为实现 GPR，我们引入三项关键技术革新，涵盖统一表征、网络架构与训练策略：首先，我们设计了一种面向广告场景的统一输入格式与分词方法，将广告与自然内容映射至共享的多层次语义 ID 空间，从而增强异构数据间的语义对齐与建模一致性；其次，我们构建了异构层次解码器（Heterogeneous Hierarchical Decoder, HHD），一种双解码器架构，将用户意图建模与广告生成解耦，在保证强建模能力的同时兼顾训练效率与推理灵活性；最后，我们提出一种多阶段联合训练策略，整合多令牌预测（Multi-Token Prediction, MTP）、价值感知微调与层次增强策略优化（Hierarchy Enhanced Policy Optimization, HEPO）算法，形成一套完整的一体化生成式推荐流水线，统一兴趣建模、价值对齐与策略优化。GPR 已在腾讯微信频道广告系统中全面部署，显著提升了 GMV 与 CTCVR 等关键业务指标。",
        "translated_title": "GPR：面向大规模广告推荐的生成式预训练单模型范式",
        "label": [
            "LLM生成式推荐",
            "精排",
            "通用推荐技术"
        ],
        "label_reason": "提出统一生成式模型解决广告推荐端到端优化问题",
        "relevance_score": 10,
        "novelty_score": 9,
        "novelty_reason": "首创一模型范式，整合意图建模与价值对齐"
    },
    {
        "title": "Practical RAG Evaluation: A Rarity-Aware Set-Based Metric and Cost-Latency-Quality Trade-offs",
        "url": "http://arxiv.org/abs/2511.09545v1",
        "pub_date": "2025-11-12",
        "summary": "This paper addresses the guessing game in building production RAG. Classical rank-centric IR metrics (nDCG/MAP/MRR) are a poor fit for RAG, where LLMs consume a set of passages rather than a browsed list; position discounts and prevalence-blind aggregation miss what matters: whether the prompt at cutoff K contains the decisive evidence. Second, there is no standardized, reproducible way to build and audit golden sets. Third, leaderboards exist but lack end-to-end, on-corpus benchmarking that reflects production trade-offs. Fourth, how state-of-the-art embedding models handle proper-name identity signals and conversational noise remains opaque. To address these, we contribute: (1) RA-nWG@K, a rarity-aware, per-query-normalized set score, and operational ceilings via the pool-restricted oracle ceiling (PROC) and the percentage of PROC (%PROC) to separate retrieval from ordering headroom within a Cost-Latency-Quality (CLQ) lens; (2) rag-gs (MIT), a lean golden-set pipeline with Plackett-Luce listwise refinement whose iterative updates outperform single-shot LLM ranking; (3) a comprehensive benchmark on a production RAG (scientific-papers corpus) spanning dense retrieval, hybrid dense+BM25, embedding models and dimensions, cross-encoder rerankers, ANN (HNSW), and quantization; and (4) targeted diagnostics that quantify proper-name identity signal and conversational-noise sensitivity via identity-destroying and formatting ablations. Together, these components provide practitioner Pareto guidance and auditable guardrails to support reproducible, budget/SLA-aware decisions.",
        "translated": "本文探讨了构建生产级 RAG 系统中的“猜测游戏”问题。传统的以排序为中心的 IR 评估指标（如 nDCG/MAP/MRR）并不适用于 RAG 场景，因为大语言模型（LLM）消费的是一个段落集合而非浏览列表；位置折扣与忽视流行度的聚合方式忽略了真正关键的信息：在截断位置 K 处的提示是否包含决定性证据。其次，目前尚无标准化、可复现的方法来构建和审计“黄金集”。第三，虽有排行榜存在，但缺乏端到端、基于真实语料的基准测试，难以反映生产环境中的权衡取舍。第四，当前最先进的嵌入模型如何处理专有名词身份信号及对话噪声仍不透明。为解决上述问题，我们贡献如下：(1) RA-nWG@K，一种考虑稀有度且按查询归一化的集合评分方法，并通过池限制 oracle 上限（PROC）与 PROC 占比（%PROC），从成本-延迟-质量（CLQ）视角划分检索与排序模块各自的设计空间；(2) rag-gs（MIT），一个轻量级黄金集构建流水线，采用 Plackett-Luce 列式优化方法，其迭代更新性能优于单次 LLM 排序；(3) 针对生产级 RAG（科学论文语料库）的全面基准测试，涵盖稠密检索、混合稠密+BM25、嵌入模型与维度、交叉编码器重排器、近似最近邻（HNSW）以及量化技术；(4) 面向具体诊断的分析方法，通过破坏身份信息与格式化消融实验，量化专有名词身份信号与对话噪声敏感性。这些组件共同为从业者提供帕累托最优指导，并提供可审计的安全边界，支持可复现、预算/服务等级协议（SLA）感知的决策。",
        "translated_title": "实用RAG评估：一种基于稀有度感知的集合型指标与成本-延迟-质量权衡",
        "label": [
            "推荐系统评估",
            "通用推荐技术"
        ],
        "label_reason": "聚焦RAG评估，非推荐核心环节但相关",
        "relevance_score": 4,
        "novelty_score": 7,
        "novelty_reason": "提出新评估指标与基准框架，实用性强"
    },
    {
        "title": "Sim4IA-Bench: A User Simulation Benchmark Suite for Next Query and Utterance Prediction",
        "url": "http://arxiv.org/abs/2511.09329v1",
        "pub_date": "2025-11-12",
        "summary": "Validating user simulation is a difficult task due to the lack of established measures and benchmarks, which makes it challenging to assess whether a simulator accurately reflects real user behavior. As part of the Sim4IA Micro-Shared Task at the Sim4IA Workshop, SIGIR 2025, we present Sim4IA-Bench, a simulation benchmark suit for the prediction of the next queries and utterances, the first of its kind in the IR community. Our dataset as part of the suite comprises 160 real-world search sessions from the CORE search engine. For 70 of these sessions, up to 62 simulator runs are available, divided into Task A and Task B, in which different approaches predicted users next search queries or utterances. Sim4IA-Bench provides a basis for evaluating and comparing user simulation approaches and for developing new measures of simulator validity. Although modest in size, the suite represents the first publicly available benchmark that links real search sessions with simulated next-query predictions. In addition to serving as a testbed for next query prediction, it also enables exploratory studies on query reformulation behavior, intent drift, and interaction-aware retrieval evaluation. We also introduce a new measure for evaluating next-query predictions in this task. By making the suite publicly available, we aim to promote reproducible research and stimulate further work on realistic and explainable user simulation for information access: https://github.com/irgroup/Sim4IA-Bench.",
        "translated": "验证用户模拟是一项困难的任务，原因在于缺乏既定的评估指标和基准数据集，这使得难以判断模拟器是否真实反映了真实用户行为。作为 Sim4IA Workshop 2025 上 Sim4IA Micro-Shared Task 的一部分，我们提出了 Sim4IA-Bench，这是信息检索领域首个用于预测下一轮查询与话语的模拟基准套件。本套件中的数据集来自 CORE 搜索引擎的 160 个真实搜索会话；其中 70 个会话提供了多达 62 次模拟器运行记录，分为任务 A 和任务 B，分别对应不同方法对用户下一轮搜索查询或话语的预测。Sim4IA-Bench 为评估和比较用户模拟方法、开发新的模拟器有效性度量标准提供了基础。尽管规模有限，该套件是首个公开可用且将真实搜索会话与模拟的下一轮查询预测相链接的基准。除作为下一轮查询预测的测试平台外，它亦支持探索性研究，如查询改写行为、意图漂移及交互感知的检索评估。此外，我们还引入了一种新度量标准，用于评估该任务中的下一轮查询预测性能。通过公开发布此套件，我们旨在推动可复现研究，并促进关于真实且可解释的用户模拟在信息获取领域的进一步工作：https://github.com/irgroup/Sim4IA-Bench。",
        "translated_title": "Sim4IA-Bench：面向下一句查询与话语预测的用户模拟基准套件",
        "label": [],
        "label_reason": "专注用户模拟与查询预测，非推荐系统核心环节",
        "relevance_score": 3,
        "novelty_score": 5,
        "novelty_reason": "首次提供IR领域用户模拟基准，但无推荐系统创新"
    },
    {
        "title": "NeuroCLIP: Brain-Inspired Prompt Tuning for EEG-to-Image Multimodal Contrastive Learning",
        "url": "http://arxiv.org/abs/2511.09250v1",
        "pub_date": "2025-11-12",
        "summary": "Recent advances in brain-inspired artificial intelligence have sought to align neural signals with visual semantics using multimodal models such as CLIP. However, existing methods often treat CLIP as a static feature extractor, overlooking its adaptability to neural representations and the inherent physiological-symbolic gap in EEG-image alignment. To address these challenges, we present NeuroCLIP, a prompt tuning framework tailored for EEG-to-image contrastive learning. Our approach introduces three core innovations: (1) We design a dual-stream visual embedding pipeline that combines dynamic filtering and token-level fusion to generate instance-level adaptive prompts, which guide the adjustment of patch embedding tokens based on image content, thereby enabling fine-grained modulation of visual representations under neural constraints; (2) We are the first to introduce visual prompt tokens into EEG-image alignment, acting as global, modality-level prompts that work in conjunction with instance-level adjustments. These visual prompt tokens are inserted into the Transformer architecture to facilitate neural-aware adaptation and parameter optimization at a global level; (3) Inspired by neuroscientific principles of human visual encoding, we propose a refined contrastive loss that better model the semantic ambiguity and cross-modal noise present in EEG signals. On the THINGS-EEG2 dataset, NeuroCLIP achieves a Top-1 accuracy of 63.2% in zero-shot image retrieval, surpassing the previous best method by +12.3%, and demonstrates strong generalization under inter-subject conditions (+4.6% Top-1), highlighting the potential of physiology-aware prompt tuning for bridging brain signals and visual semantics.",
        "translated": "近年来，受脑启发的人工智能研究致力于通过多模态模型（如 CLIP）将神经信号与视觉语义对齐。然而，现有方法通常将 CLIP 视为静态特征提取器，忽视其对神经表征的适应性以及 EEG 与图像对齐中存在的固有生理-符号鸿沟。为应对这些挑战，我们提出 NeuroCLIP，一种专为 EEG 到图像对比学习设计的提示调优框架。我们的方法包含三项核心创新：（1）我们设计了一种双流视觉嵌入管道，结合动态滤波与 token 级融合，生成实例级自适应提示，引导根据图像内容调整 patch 嵌入 token，从而在神经约束下实现视觉表征的精细化调制；（2）我们首次将视觉提示 token 引入 EEG-图像对齐任务，作为全局、模态级别的提示，与实例级调整协同作用。这些视觉提示 token 被插入 Transformer 架构中，以支持全局层面的神经感知适配与参数优化；（3）受人类视觉编码神经科学原理的启发，我们提出一种改进的对比损失函数，更有效地建模 EEG 信号中固有的语义模糊性与跨模态噪声。在 THINGS-EEG2 数据集上，NeuroCLIP 在零样本图像检索任务中达到 63.2% 的 Top-1 准确率，较此前最优方法提升 +12.3%，并在跨受试者条件下展现出强大的泛化能力（+4.6% Top-1），凸显了生理感知提示调优在弥合脑信号与视觉语义之间的巨大潜力。",
        "translated_title": "NeuroCLIP：受脑启发的提示调优用于脑电图到图像的多模态对比学习",
        "label": [],
        "label_reason": "研究聚焦EEG-图像对齐，非推荐系统相关",
        "relevance_score": 3,
        "novelty_score": 7,
        "novelty_reason": "首次引入视觉提示用于神经信号对齐"
    },
    {
        "title": "Thinking Forward and Backward: Multi-Objective Reinforcement Learning for Retrieval-Augmented Reasoning",
        "url": "http://arxiv.org/abs/2511.09109v2",
        "pub_date": "2025-11-12",
        "summary": "Retrieval-augmented generation (RAG) has proven to be effective in mitigating hallucinations in large language models, yet its effectiveness remains limited in complex, multi-step reasoning scenarios. Recent efforts have incorporated search-based interactions into RAG, enabling iterative reasoning with real-time retrieval. Most approaches rely on outcome-based supervision, offering no explicit guidance for intermediate steps. This often leads to reward hacking and degraded response quality. We propose Bi-RAR, a novel retrieval-augmented reasoning framework that evaluates each intermediate step jointly in both forward and backward directions. To assess the information completeness of each step, we introduce a bidirectional information distance grounded in Kolmogorov complexity, approximated via language model generation probabilities. This quantification measures both how far the current reasoning is from the answer and how well it addresses the question. To optimize reasoning under these bidirectional signals, we adopt a multi-objective reinforcement learning framework with a cascading reward structure that emphasizes early trajectory alignment. Empirical results on seven question answering benchmarks demonstrate that Bi-RAR surpasses previous methods and enables efficient interaction and reasoning with the search engine during training and inference.",
        "translated": "检索增强生成（RAG）已被证明在减轻大语言模型中的幻觉现象方面有效，但在复杂的多步骤推理场景中其效果仍显有限。近期研究已将基于搜索的交互引入RAG，从而支持与实时检索结合的迭代推理。大多数方法依赖于结果导向的监督信号，未能为中间步骤提供明确引导，这常导致奖励欺骗和响应质量下降。我们提出了Bi-RAR，一种新颖的检索增强推理框架，该框架从正向与反向两个方向联合评估每个中间步骤。为衡量每一步信息的完整性，我们引入了一种基于Kolmogorov复杂度的双向信息距离，并通过语言模型生成概率进行近似。该量化指标同时测量当前推理距离答案有多远，以及其对问题的覆盖程度。为在上述双向信号下优化推理过程，我们采用多目标强化学习框架，并设计级联式奖励结构，强调早期轨迹对齐。在七个问答基准上的实验证明，Bi-RAR优于现有方法，并能在训练与推理过程中高效地与搜索引擎交互并完成推理。",
        "translated_title": "面向前与面向后：用于检索增强推理的多目标强化学习",
        "label": [],
        "label_reason": "论文聚焦RAG与强化学习，非推荐系统核心环节",
        "relevance_score": 3,
        "novelty_score": 7,
        "novelty_reason": "双向信息距离+多目标RL创新，但非为推荐设计"
    },
    {
        "title": "Efficient Model-Agnostic Continual Learning for Next POI Recommendation",
        "url": "http://arxiv.org/abs/2511.08941v1",
        "pub_date": "2025-11-12",
        "summary": "Next point-of-interest (POI) recommendation improves personalized location-based services by predicting users' next destinations based on their historical check-ins. However, most existing methods rely on static datasets and fixed models, limiting their ability to adapt to changes in user behavior over time. To address this limitation, we explore a novel task termed continual next POI recommendation, where models dynamically adapt to evolving user interests through continual updates. This task is particularly challenging, as it requires capturing shifting user behaviors while retaining previously learned knowledge. Moreover, it is essential to ensure efficiency in update time and memory usage for real-world deployment. To this end, we propose GIRAM (Generative Key-based Interest Retrieval and Adaptive Modeling), an efficient, model-agnostic framework that integrates context-aware sustained interests with recent interests. GIRAM comprises four components: (1) an interest memory to preserve historical preferences; (2) a context-aware key encoding module for unified interest key representation; (3) a generative key-based retrieval module to identify diverse and relevant sustained interests; and (4) an adaptive interest update and fusion module to update the interest memory and balance sustained and recent interests. In particular, GIRAM can be seamlessly integrated with existing next POI recommendation models. Experiments on three real-world datasets demonstrate that GIRAM consistently outperforms state-of-the-art methods while maintaining high efficiency in both update time and memory consumption.",
        "translated": "下一步兴趣点（POI）推荐通过基于用户历史签到行为预测其下一目的地，从而提升个性化位置服务的效果。然而，现有大多数方法依赖静态数据集和固定模型，难以适应用户行为随时间演变的动态特性。为解决这一局限性，我们探索了一项名为持续下一步POI推荐的新任务，在该任务中，模型通过持续更新的方式动态适应用户兴趣的变化。该任务极具挑战性，不仅需要捕捉用户行为的动态迁移，同时必须保留先前所学知识。此外，为实现实际部署，还需确保更新时长与内存开销的高效性。为此，我们提出GIRAM（生成式键驱动的兴趣检索与自适应建模），这是一种高效且模型无关的框架，用于融合上下文感知的持续兴趣与近期兴趣。GIRAM包含四个核心组件：（1）兴趣记忆模块，用于保存历史偏好；（2）上下文感知的键编码模块，实现统一的兴趣键表示；（3）生成式键驱动的检索模块，用于识别多样且相关的持续兴趣；以及（4）自适应兴趣更新与融合模块，用于更新兴趣记忆并平衡持续兴趣与近期兴趣。特别地，GIRAM可无缝集成至现有下一步POI推荐模型中。在三个真实世界数据集上的实验表明，GIRAM在性能上持续优于当前最先进方法，同时在更新时长与内存消耗方面保持高效率。",
        "translated_title": "面向下一POI推荐的高效模型无关持续学习",
        "label": [
            "召回",
            "精排",
            "通用推荐技术"
        ],
        "label_reason": "针对POI推荐的持续学习，改进模型适应性与效率。",
        "relevance_score": 9,
        "novelty_score": 8,
        "novelty_reason": "提出模型无关框架GIRAM，高效融合历史与近期兴趣。"
    },
    {
        "title": "Compression then Matching: An Efficient Pre-training Paradigm for Multimodal Embedding",
        "url": "http://arxiv.org/abs/2511.08480v1",
        "pub_date": "2025-11-11",
        "summary": "Vision-language models advance multimodal representation learning by acquiring transferable semantic embeddings, thereby substantially enhancing performance across a range of vision-language tasks, including cross-modal retrieval, clustering, and classification. An effective embedding is expected to comprehensively preserve the semantic content of the input while simultaneously emphasizing features that are discriminative for downstream tasks. Recent approaches demonstrate that VLMs can be adapted into competitive embedding models via large-scale contrastive learning, enabling the simultaneous optimization of two complementary objectives. We argue that the two aforementioned objectives can be decoupled: a comprehensive understanding of the input facilitates the embedding model in achieving superior performance in downstream tasks via contrastive learning. In this paper, we propose CoMa, a compressed pre-training phase, which serves as a warm-up stage for contrastive learning. Experiments demonstrate that with only a small amount of pre-training data, we can transform a VLM into a competitive embedding model. CoMa achieves new state-of-the-art results among VLMs of comparable size on the MMEB, realizing optimization in both efficiency and effectiveness.",
        "translated": "视觉-语言模型通过获取可迁移的语义嵌入，推动了多模态表征学习的发展，从而显著提升了跨模态检索、聚类和分类等视觉-语言任务的性能。一个有效的嵌入应全面保留输入的语义内容，同时强调对下游任务具有判别性的特征。近期方法表明，可通过大规模对比学习将视觉-语言模型（VLMs）适配为具有竞争力的嵌入模型，从而实现两项互补目标的同时优化。我们认为，上述两项目标可以解耦：对输入的全面理解有助于嵌入模型通过对比学习在下游任务中实现更优表现。本文提出 CoMa，一种压缩式预训练阶段，作为对比学习的预热阶段。实验表明，仅需少量预训练数据，即可将视觉-语言模型转化为具有竞争力的嵌入模型。CoMa 在同等规模的视觉-语言模型中，在 MMEB 数据集上取得了新的最先进性能，实现了效率与效果的双重优化。",
        "translated_title": "压缩而后匹配：一种高效的多模态嵌入预训练范式",
        "label": [
            "多模态推荐",
            "通用推荐技术"
        ],
        "label_reason": "论文聚焦多模态嵌入，可适配推荐系统中的跨模态召回或排序。",
        "relevance_score": 4,
        "novelty_score": 7,
        "novelty_reason": "提出压缩预训练范式，提升效率与效果，具创新性。"
    },
    {
        "title": "Advancing Scientific Knowledge Retrieval and Reuse with a Novel Digital Library for Machine-Readable Knowledge",
        "url": "http://arxiv.org/abs/2511.08476v1",
        "pub_date": "2025-11-11",
        "summary": "Digital libraries for research, such as the ACM Digital Library or Semantic Scholar, do not enable the machine-supported, efficient reuse of scientific knowledge (e.g., in synthesis research). This is because these libraries are based on document-centric models with narrative text knowledge expressions that require manual or semi-automated knowledge extraction, structuring, and organization. We present ORKG reborn, an emerging digital library that supports finding, accessing, and reusing accurate, fine-grained, and reproducible machine-readable expressions of scientific knowledge that relate scientific statements and their supporting evidence in terms of data and code. The rich expressions of scientific knowledge are published as reborn (born-reusable) articles and provide novel possibilities for scientific knowledge retrieval, for instance by statistical methods, software packages, variables, or data matching specific constraints. We describe the proposed system and demonstrate its practical viability and potential for information retrieval in contrast to state-of-the-art digital libraries and document-centric scholarly communication using several published articles in research fields ranging from computer science to soil science. Our work underscores the enormous potential of scientific knowledge databases and a viable approach to their construction.",
        "translated": "研究型数字图书馆，如ACM数字图书馆或Semantic Scholar，并未支持科学知识的机器辅助高效复用（例如在综合研究中）。这是因为这些图书馆基于以文档为中心的模型，其知识表达依赖叙事性文本，需人工或半自动化的知识抽取、结构化与组织。我们提出ORKG reborn，这是一种新兴的数字图书馆，支持查找、访问并复用精确、细粒度、可复现的机器可读科学知识表达形式，该形式关联科学命题及其支持证据（包括数据与代码）。丰富的科学知识表达以“重生”（born-reusable）文章形式发布，为科学知识检索提供全新可能性，例如通过统计方法、软件包、变量或符合特定约束的数据进行匹配。我们描述了所提出的系统，并通过多个跨学科领域（从计算机科学到土壤科学）的已发表文章，展示其相较于现有数字图书馆及以文档为中心的学术传播模式，在信息检索方面的实用可行性与潜力。本工作凸显了科学知识数据库的巨大潜力，并提出了一种可行的构建路径。",
        "translated_title": "以面向机器可读知识的新型数字图书馆推进科学知识的检索与重用",
        "label": [],
        "label_reason": "论文聚焦科学知识库构建，非推荐系统相关",
        "relevance_score": 3,
        "novelty_score": 5,
        "novelty_reason": "改进知识表达结构，但未针对推荐场景设计"
    },
    {
        "title": "Bid Farewell to Seesaw: Towards Accurate Long-tail Session-based Recommendation via Dual Constraints of Hybrid Intents",
        "url": "http://arxiv.org/abs/2511.08378v1",
        "pub_date": "2025-11-11",
        "summary": "Session-based recommendation (SBR) aims to predict anonymous users' next interaction based on their interaction sessions. In the practical recommendation scenario, low-exposure items constitute the majority of interactions, creating a long-tail distribution that severely compromises recommendation diversity. Existing approaches attempt to address this issue by promoting tail items but incur accuracy degradation, exhibiting a \"see-saw\" effect between long-tail and accuracy performance. We attribute such conflict to session-irrelevant noise within the tail items, which existing long-tail approaches fail to identify and constrain effectively. To resolve this fundamental conflict, we propose \\textbf{HID} (\\textbf{H}ybrid \\textbf{I}ntent-based \\textbf{D}ual Constraint Framework), a plug-and-play framework that transforms the conventional \"see-saw\" into \"win-win\" through introducing the hybrid intent-based dual constraints for both long-tail and accuracy. Two key innovations are incorporated in this framework: (i) \\textit{Hybrid Intent Learning}, where we reformulate the intent extraction strategies by employing attribute-aware spectral clustering to reconstruct the item-to-intent mapping. Furthermore, discrimination of session-irrelevant noise is achieved through the assignment of the target and noise intents to each session. (ii) \\textit{Intent Constraint Loss}, which incorporates two novel constraint paradigms regarding the \\textit{diversity} and \\textit{accuracy} to regulate the representation learning process of both items and sessions. These two objectives are unified into a single training loss through rigorous theoretical derivation. Extensive experiments across multiple SBR models and datasets demonstrate that HID can enhance both long-tail performance and recommendation accuracy, establishing new state-of-the-art performance in long-tail recommender systems.",
        "translated": "基于会话的推荐（SBR）旨在根据用户的交互会话预测匿名用户的下一次交互行为。在实际推荐场景中，曝光率较低的物料构成了大多数交互，形成长尾分布，严重损害推荐多样性。现有方法虽尝试通过促进尾部物料来缓解此问题，但往往导致准确率下降，表现出长尾性能与准确率之间的“跷跷板”效应。我们归因于尾部物料中存在的会话无关噪声，而现有长尾推荐方法未能有效识别并约束此类噪声。为解决这一根本性冲突，我们提出 \\textbf{HID}（\\textbf{H}ybrid \\textbf{I}ntent-based \\textbf{D}ual Constraint Framework），一种即插即用框架，通过引入混合意图双约束机制，在长尾性能与准确率之间实现从“跷跷板”到“双赢”的转变。该框架包含两项关键创新：(i) \\textit{混合意图学习}，我们通过引入属性感知谱聚类重构物料至意图映射，并通过为每个会话分配目标意图与噪声意图，实现对会话无关噪声的区分；(ii) \\textit{意图约束损失}，其融合两种新颖的约束范式——\\textit{多样性}与\\textit{准确率}，用于调控物料与会话的表示学习过程。这两项目标通过严谨的理论推导统一至单一训练损失函数中。在多个 SBR 模型和数据集上的广泛实验表明，HID 可同时提升长尾性能与推荐准确率，建立长尾推荐系统新的最先进性能。",
        "translated_title": "告别摇摆：通过混合意图的双约束实现精准长尾会话推荐",
        "label": [
            "精排（Ranking）",
            "序列推荐（Sequential Recommendation）"
        ],
        "label_reason": "聚焦会话推荐中长尾问题与精度平衡，属精排核心优化",
        "relevance_score": 9,
        "novelty_score": 8,
        "novelty_reason": "提出混合意图双约束框架，理论统一且提升效果显著"
    },
    {
        "title": "TurkEmbed: Turkish Embedding Model on NLI &amp; STS Tasks",
        "url": "http://arxiv.org/abs/2511.08376v1",
        "pub_date": "2025-11-11",
        "summary": "This paper introduces TurkEmbed, a novel Turkish language embedding model designed to outperform existing models, particularly in Natural Language Inference (NLI) and Semantic Textual Similarity (STS) tasks. Current Turkish embedding models often rely on machine-translated datasets, potentially limiting their accuracy and semantic understanding. TurkEmbed utilizes a combination of diverse datasets and advanced training techniques, including matryoshka representation learning, to achieve more robust and accurate embeddings. This approach enables the model to adapt to various resource-constrained environments, offering faster encoding capabilities. Our evaluation on the Turkish STS-b-TR dataset, using Pearson and Spearman correlation metrics, demonstrates significant improvements in semantic similarity tasks. Furthermore, TurkEmbed surpasses the current state-of-the-art model, Emrecan, on All-NLI-TR and STS-b-TR benchmarks, achieving a 1-4\\% improvement. TurkEmbed promises to enhance the Turkish NLP ecosystem by providing a more nuanced understanding of language and facilitating advancements in downstream applications.",
        "translated": "本文介绍了TurkEmbed，一种旨在超越现有模型的新型土耳其语嵌入模型，尤其在自然语言推理（NLI）和语义文本相似度（STS）任务中表现优异。当前的土耳其语嵌入模型常依赖机器翻译的数据集，可能限制其准确性和语义理解能力。TurkEmbed结合了多样化的数据集与先进的训练技术，包括套娃式表示学习（matryoshka representation learning），以实现更鲁棒且精确的嵌入。该方法使模型能够适应各种资源受限环境，并提供更快的编码能力。我们在土耳其语STS-b-TR数据集上进行评估，使用皮尔逊相关系数和斯皮尔曼相关系数作为评价指标，结果表明模型在语义相似度任务上显著提升。此外，TurkEmbed在All-NLI-TR和STS-b-TR基准测试中超越当前最先进模型Emrecan，性能提升达1%-4%。TurkEmbed有望通过提供对语言更细腻的理解，推动土耳其自然语言处理生态系统的进步，并促进下游应用的发展。",
        "translated_title": "TurkEmbed：在自然语言推理与语义文本相似度任务上的土耳其语嵌入模型",
        "label": [],
        "label_reason": "论文聚焦土耳其语NLP任务，与推荐系统无关。",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出新嵌入模型，但非为推荐系统设计。"
    },
    {
        "title": "AgentPRM: Process Reward Models for LLM Agents via Step-Wise Promise and Progress",
        "url": "http://arxiv.org/abs/2511.08325v1",
        "pub_date": "2025-11-11",
        "summary": "Despite rapid development, large language models (LLMs) still encounter challenges in multi-turn decision-making tasks (i.e., agent tasks) like web shopping and browser navigation, which require making a sequence of intelligent decisions based on environmental feedback. Previous work for LLM agents typically relies on elaborate prompt engineering or fine-tuning with expert trajectories to improve performance. In this work, we take a different perspective: we explore constructing process reward models (PRMs) to evaluate each decision and guide the agent's decision-making process. Unlike LLM reasoning, where each step is scored based on correctness, actions in agent tasks do not have a clear-cut correctness. Instead, they should be evaluated based on their proximity to the goal and the progress they have made. Building on this insight, we propose a re-defined PRM for agent tasks, named AgentPRM, to capture both the interdependence between sequential decisions and their contribution to the final goal. This enables better progress tracking and exploration-exploitation balance. To scalably obtain labeled data for training AgentPRM, we employ a Temporal Difference-based (TD-based) estimation method combined with Generalized Advantage Estimation (GAE), which proves more sample-efficient than prior methods. Extensive experiments across different agentic tasks show that AgentPRM is over $8\\times$ more compute-efficient than baselines, and it demonstrates robust improvement when scaling up test-time compute. Moreover, we perform detailed analyses to show how our method works and offer more insights, e.g., applying AgentPRM to the reinforcement learning of LLM agents.",
        "translated": "尽管大语言模型（LLMs）发展迅速，但在多轮决策任务（即代理任务）如网页购物和浏览器导航中，仍面临挑战，这些任务要求根据环境反馈做出一系列智能决策。以往针对LLM代理的研究通常依赖复杂的提示工程或使用专家轨迹进行微调以提升性能。在本工作中，我们采取了不同的视角：探索构建过程奖励模型（PRMs），以评估每个决策并引导代理的决策过程。与LLM推理中每一步基于正确性评分不同，代理任务中的动作并无明确的“正确”或“错误”标准，而应依据其接近目标的程度及所取得的进展进行评估。基于这一洞察，我们提出了一种重新定义的PRM，命名为AgentPRM，用于捕捉序列决策之间的相互依赖关系及其对最终目标的贡献。这有助于更有效地追踪进度并平衡探索与利用。为可扩展地获取训练AgentPRM所需的标注数据，我们采用基于时序差分（TD-based）的估计方法结合广义优势估计（GAE），该方法比先前方法更具样本效率。在多种代理任务上的广泛实验表明，AgentPRM相比基线方法计算效率高出8倍以上，并且在测试时计算资源扩展时表现出稳健的性能提升。此外，我们进行了详细分析，揭示了方法的工作机制并提供进一步见解，例如将AgentPRM应用于大语言模型代理的强化学习。",
        "translated_title": "AgentPRM：通过分步承诺与进度驱动的大语言模型代理过程奖励建模",
        "label": [
            "LLM生成式推荐",
            "通用推荐技术"
        ],
        "label_reason": "提出过程奖励模型指导LLM代理决策，适用于推荐场景",
        "relevance_score": 4,
        "novelty_score": 7,
        "novelty_reason": "创新性构建PRM评估多步决策，提升效率与探索平衡"
    },
    {
        "title": "MARC: Multimodal and Multi-Task Agentic Retrieval-Augmented Generation for Cold-Start Recommender System",
        "url": "http://arxiv.org/abs/2511.08181v1",
        "pub_date": "2025-11-11",
        "summary": "Recommender systems (RS) are currently being studied to mitigate limitations during cold-start conditions by leveraging modality information or introducing Agent concepts based on the exceptional reasoning capabilities of Large Language Models (LLMs). Meanwhile, food and beverage recommender systems have traditionally used knowledge graph and ontology concepts due to the domain's unique data attributes and relationship characteristics. On this background, we propose MARC, a multimodal and multi-task cocktail recommender system based on Agentic Retrieval-Augmented Generation (RAG) utilizing graph database under cold-start conditions. The proposed system generates high-quality, contextually appropriate answers through two core processes: a task recognition router and a reflection process. The graph database was constructed by processing cocktail data from Kaggle, and its effectiveness was evaluated using 200 manually crafted questions. The evaluation used both LLM-as-a-judge and human evaluation to demonstrate that answers generated via the graph database outperformed those from a simple vector database in terms of quality. The code is available at https://github.com/diddbwls/cocktail_rec_agentrag",
        "translated": "推荐系统（RS）当前正通过利用模态信息或引入基于大语言模型（LLMs）卓越推理能力的代理概念，以缓解冷启动场景下的局限性。同时，由于食品饮料领域数据具有独特的属性与关系特征，传统食品饮料推荐系统多采用知识图谱和本体概念。在此背景下，我们提出MARC，这是一种基于代理式检索增强生成（RAG）并结合图数据库的多模态、多任务鸡尾酒推荐系统，在冷启动条件下运行。该系统通过两个核心过程生成高质量且符合上下文的回答：任务识别路由器与反思过程。图数据库由Kaggle上的鸡尾酒数据构建而成，其有效性通过200个手工设计的问题进行评估。评估采用LLM作为裁判与人工评价相结合的方式，证明了图数据库生成的答案在质量上优于简单向量数据库生成的答案。代码详见https://github.com/diddbwls/cocktail_rec_agentrag",
        "translated_title": "MARC：面向冷启动推荐系统的多模态与多任务代理检索增强生成模型",
        "label": [
            "多模态推荐",
            "LLM生成式推荐",
            "召回"
        ],
        "label_reason": "结合多模态与LLM-RAG解决冷启动召回问题",
        "relevance_score": 9,
        "novelty_score": 8,
        "novelty_reason": "首创Agentic RAG架构应对冷启动，任务路由+反思机制新颖"
    },
    {
        "title": "DiffuGR: Generative Document Retrieval with Diffusion Language Models",
        "url": "http://arxiv.org/abs/2511.08150v1",
        "pub_date": "2025-11-11",
        "summary": "Generative retrieval (GR) re-frames document retrieval as a sequence-based document identifier (DocID) generation task, memorizing documents with model parameters and enabling end-to-end retrieval without explicit indexing. Existing GR methods are based on auto-regressive generative models, i.e., the token generation is performed from left to right. However, such auto-regressive methods suffer from: (1) mismatch between DocID generation and natural language generation, e.g., an incorrect DocID token generated in early left steps would lead to totally erroneous retrieval; and (2) failure to balance the trade-off between retrieval efficiency and accuracy dynamically, which is crucial for practical applications. To address these limitations, we propose generative document retrieval with diffusion language models, dubbed DiffuGR. It models DocID generation as a discrete diffusion process: during training, DocIDs are corrupted through a stochastic masking process, and a diffusion language model is learned to recover them under a retrieval-aware objective. For inference, DiffuGR attempts to generate DocID tokens in parallel and refines them through a controllable number of denoising steps. In contrast to conventional left-to-right auto-regressive decoding, DiffuGR provides a novel mechanism to first generate more confident DocID tokens and refine the generation through diffusion-based denoising. Moreover, DiffuGR also offers explicit runtime control over the qualitylatency tradeoff. Extensive experiments on benchmark retrieval datasets show that DiffuGR is competitive with strong auto-regressive generative retrievers, while offering flexible speed and accuracy tradeoffs through variable denoising budgets. Overall, our results indicate that non-autoregressive diffusion models are a practical and effective alternative for generative document retrieval.",
        "translated": "生成式检索（GR）将文档检索重构为基于序列的文档标识符（DocID）生成任务，通过模型参数记忆文档内容，并实现无需显式索引的端到端检索。现有GR方法均基于自回归生成模型，即从左至右逐词生成token。然而，此类自回归方法存在以下缺陷：(1) DocID生成与自然语言生成之间存在不匹配，例如早期左侧步骤生成的错误DocID token将导致完全错误的检索结果；(2) 无法动态平衡检索效率与准确率之间的权衡，这对于实际应用至关重要。为解决上述局限性，我们提出基于扩散语言模型的生成式文档检索方法，命名为DiffuGR。该方法将DocID生成建模为离散扩散过程：训练阶段，DocIDs通过随机掩码过程进行破坏化处理，随后学习一个具备检索感知目标的扩散语言模型以恢复其原始形式。在推理阶段，DiffuGR尝试并行生成DocID token，并通过可控数量的去噪步骤对其进行优化。相较于传统的左到右自回归解码方式，DiffuGR提供了全新的机制：首先生成更置信度高的DocID token，再通过基于扩散的去噪过程逐步优化生成结果。此外，DiffuGR还支持对质量-延迟权衡的显式运行时控制。在多个基准检索数据集上的广泛实验表明，DiffuGR在性能上可媲美强大的自回归生成式检索器，同时可通过调节去噪预算灵活调整速度与精度的权衡。总体而言，我们的结果表明，非自回归扩散模型是生成式文档检索的一种实用且有效的替代方案。",
        "translated_title": "DiffuGR：基于扩散语言模型的生成式文档检索",
        "label": [
            "重排",
            "LLM生成式推荐"
        ],
        "label_reason": "基于扩散模型的生成式检索，适配推荐系统召回环节",
        "relevance_score": 7,
        "novelty_score": 8,
        "novelty_reason": "首次将扩散模型用于文档生成检索，提升效率与精度平衡"
    },
    {
        "title": "BiCA: Effective Biomedical Dense Retrieval with Citation-Aware Hard Negatives",
        "url": "http://arxiv.org/abs/2511.08029v1",
        "pub_date": "2025-11-11",
        "summary": "Hard negatives are essential for training effective retrieval models. Hard-negative mining typically relies on ranking documents using cross-encoders or static embedding models based on similarity metrics such as cosine distance. Hard negative mining becomes challenging for biomedical and scientific domains due to the difficulty in distinguishing between source and hard negative documents. However, referenced documents naturally share contextual relevance with the source document but are not duplicates, making them well-suited as hard negatives. In this work, we propose BiCA: Biomedical Dense Retrieval with Citation-Aware Hard Negatives, an approach for hard-negative mining by utilizing citation links in 20,000 PubMed articles for improving a domain-specific small dense retriever. We fine-tune the GTE_small and GTE_Base models using these citation-informed negatives and observe consistent improvements in zero-shot dense retrieval using nDCG@10 for both in-domain and out-of-domain tasks on BEIR and outperform baselines on long-tailed topics in LoTTE using Success@5. Our findings highlight the potential of leveraging document link structure to generate highly informative negatives, enabling state-of-the-art performance with minimal fine-tuning and demonstrating a path towards highly data-efficient domain adaptation.",
        "translated": "硬负样本对于训练有效的检索模型至关重要。硬负样本挖掘通常依赖于使用交叉编码器或基于相似度度量（如余弦距离）的静态嵌入模型对文档进行排序。在生物医学和科学领域，由于区分源文档与硬负样本文档存在困难，硬负样本挖掘变得尤为具有挑战性。然而，参考文献自然与源文档共享上下文相关性，但又非重复内容，因此非常适合作为硬负样本。在本工作中，我们提出了BiCA：基于引文感知硬负样本的生物医学稠密检索方法，该方法利用2万篇PubMed文章中的引文链接，用于改进特定领域的轻量级稠密检索器。我们使用这些引文引导的负样本微调GTE_small和GTE_Base模型，并发现在BEIR数据集上的零样本稠密检索任务中，nDCG@10指标在域内和跨域任务上均取得一致提升；同时，在LoTTE数据集的长尾主题任务中，Success@5指标显著优于现有基线方法。我们的研究结果表明，利用文档链接结构生成高度信息丰富的负样本具有巨大潜力，能够在极小规模微调的情况下实现当前最优性能，并为高数据效率的领域自适应提供了可行路径。",
        "translated_title": "BiCA：基于引文感知难负样本的有效生物医学密集检索",
        "label": [],
        "label_reason": "聚焦生物医学检索，非推荐系统核心问题",
        "relevance_score": 3,
        "novelty_score": 6,
        "novelty_reason": "改进负采样策略，但领域非推荐"
    },
    {
        "title": "From IDs to Semantics: A Generative Framework for Cross-Domain Recommendation with Adaptive Semantic Tokenization",
        "url": "http://arxiv.org/abs/2511.08006v1",
        "pub_date": "2025-11-11",
        "summary": "Cross-domain recommendation (CDR) is crucial for improving recommendation accuracy and generalization, yet traditional methods are often hindered by the reliance on shared user/item IDs, which are unavailable in most real-world scenarios. Consequently, many efforts have focused on learning disentangled representations through multi-domain joint training to bridge the domain gaps. Recent Large Language Model (LLM)-based approaches show promise, they still face critical challenges, including: (1) the \\textbf{item ID tokenization dilemma}, which leads to vocabulary explosion and fails to capture high-order collaborative knowledge; and (2) \\textbf{insufficient domain-specific modeling} for the complex evolution of user interests and item semantics. To address these limitations, we propose \\textbf{GenCDR}, a novel \\textbf{Gen}erative \\textbf{C}ross-\\textbf{D}omain \\textbf{R}ecommendation framework. GenCDR first employs a \\textbf{Domain-adaptive Tokenization} module, which generates disentangled semantic IDs for items by dynamically routing between a universal encoder and domain-specific adapters. Symmetrically, a \\textbf{Cross-domain Autoregressive Recommendation} module models user preferences by fusing universal and domain-specific interests. Finally, a \\textbf{Domain-aware Prefix-tree} enables efficient and accurate generation. Extensive experiments on multiple real-world datasets demonstrate that GenCDR significantly outperforms state-of-the-art baselines. Our code is available in the supplementary materials.",
        "translated": "跨域推荐（CDR）对于提升推荐准确性和泛化能力至关重要，但传统方法通常受限于对共享用户/物料ID的依赖，而这些ID在大多数现实场景中并不可用。因此，许多研究致力于通过多域联合训练学习解耦表征，以弥合域间差异。近年来基于大语言模型（LLM）的方法展现出潜力，但仍面临关键挑战：（1）**物料ID分词困境**，导致词汇爆炸，并无法捕捉高阶协同知识；（2）**领域特定建模不足**，难以应对用户兴趣与物料语义的复杂演化。为解决上述局限，我们提出**GenCDR**，一种新颖的**Gen**erative **C**ross-**D**omain **R**ecommendation框架。GenCDR 首先采用**领域自适应分词模块**，通过动态路由在通用编码器与领域特异性适配器之间生成解耦语义ID；对称地，**跨域自回归推荐模块**通过融合通用与领域特定兴趣来建模用户偏好；最后，**领域感知前缀树**实现高效且精准的生成。在多个真实世界数据集上的广泛实验表明，GenCDR 显著优于现有最先进基线方法。我们的代码见附录材料。",
        "translated_title": "从ID到语义：一种面向跨域推荐的生成式框架与自适应语义分词",
        "label": [
            "跨域/联邦推荐",
            "LLM生成式推荐"
        ],
        "label_reason": "提出基于LLM的跨域生成推荐框架，解决ID语义建模问题。",
        "relevance_score": 9,
        "novelty_score": 8,
        "novelty_reason": "创新性地结合自适应语义分词与跨域自回归生成，提升泛化能力。"
    },
    {
        "title": "TurkEmbed4Retrieval: Turkish Embedding Model for Retrieval Task",
        "url": "http://arxiv.org/abs/2511.07595v1",
        "pub_date": "2025-11-10",
        "summary": "In this work, we introduce TurkEmbed4Retrieval, a retrieval specialized variant of the TurkEmbed model originally designed for Natural Language Inference (NLI) and Semantic Textual Similarity (STS) tasks. By fine-tuning the base model on the MS MARCO TR dataset using advanced training techniques, including Matryoshka representation learning and a tailored multiple negatives ranking loss, we achieve SOTA performance for Turkish retrieval tasks. Extensive experiments demonstrate that our model outperforms Turkish colBERT by 19,26% on key retrieval metrics for the Scifact TR dataset, thereby establishing a new benchmark for Turkish information retrieval.",
        "translated": "在本工作中，我们引入了 TurkEmbed4Retrieval，这是最初为自然语言推理（NLI）和语义文本相似度（STS）任务设计的 TurkEmbed 模型的一种面向检索任务的专用变体。通过在 MS MARCO TR 数据集上采用先进的训练技术对基础模型进行微调，包括 Matryoshka 表示学习和定制化的多负样本排序损失，我们在土耳其语检索任务中取得了当前最优性能。大量实验表明，我们的模型在 Scifact TR 数据集的关键检索指标上相较土耳其语 colBERT 提升了 19.26%，从而确立了土耳其语信息检索的新基准。",
        "translated_title": "TurkEmbed4Retrieval：面向检索任务的土耳其语嵌入模型",
        "label": [],
        "label_reason": "专注土耳其语检索，非推荐系统相关",
        "relevance_score": 3,
        "novelty_score": 6,
        "novelty_reason": "改进嵌入模型，但无推荐场景应用"
    },
    {
        "title": "Think Before You Retrieve: Learning Test-Time Adaptive Search with Small Language Models",
        "url": "http://arxiv.org/abs/2511.07581v1",
        "pub_date": "2025-11-10",
        "summary": "Effective information retrieval requires reasoning over partial evidence and refining strategies as information emerges. Yet current approaches fall short: neural retrievers lack reasoning capabilities, large language models (LLMs) provide semantic depth but at prohibitive cost, and query rewriting or decomposition limits improvement to static transformations. As a result, existing methods fail to capture the iterative dynamics of exploration, feedback, and revision that complex user queries demand. We introduce Orion, a training framework that enables compact models (350M-1.2B parameters) to perform iterative retrieval through learned search strategies. Orion combines: (1) synthetic trajectory generation and supervised fine-tuning to encourage diverse exploration patterns in models, (2) reinforcement learning (RL) that rewards effective query refinement and backtracking behaviors, and (3) inference-time beam search algorithms that exploit the self-reflection capabilities learned during RL. Despite using only 3% of the training data available, our 1.2B model achieves 77.6% success on SciFact (vs. 72.6% for prior retrievers), 25.2% on BRIGHT (vs. 22.1%), 63.2% on NFCorpus (vs. 57.8%), and remains competitive on FEVER, HotpotQA, and MSMarco. It outperforms retrievers up to 200-400x larger on five of six benchmarks. These findings suggest that retrieval performance can emerge from learned strategies, not just model scale, when models are trained to search, reflect, and revise.",
        "translated": "有效的信息检索需要基于部分证据进行推理，并随着信息的呈现不断调整策略。然而当前方法存在不足：神经检索器缺乏推理能力，大语言模型（LLM）虽具备语义深度，但代价高昂；而查询重写或分解仅限于静态变换，难以带来实质性改进。因此，现有方法无法捕捉复杂用户查询所要求的探索、反馈与修订的迭代动态过程。我们提出了Orion，一种训练框架，使参数规模较小的模型（350M至1.2B参数）能够通过学习到的搜索策略实现迭代式检索。Orion结合了以下三方面机制：(1) 通过合成轨迹生成与监督微调，促使模型形成多样化的探索模式；(2) 采用强化学习（RL），奖励有效查询优化与回溯行为；(3) 在推理阶段使用束搜索算法，利用在RL过程中习得的自反思能力。尽管仅使用现有训练数据的3%，我们的1.2B模型在SciFact上达到77.6%的成功率（优于先前检索器的72.6%），在BRIGHT上达到25.2%（优于22.1%），在NFCorpus上达到63.2%（优于57.8%），并在FEVER、HotpotQA和MSMarco等基准测试中保持竞争力。该模型在六个基准中的五个上性能优于参数规模大200-400倍的检索器。这些结果表明，当模型被训练用于搜索、反思与修订时，其检索性能可源于习得的策略，而不仅依赖模型规模。",
        "translated_title": "检索前先思考：利用小语言模型学习测试时自适应搜索",
        "label": [
            "召回",
            "LLM生成式推荐"
        ],
        "label_reason": "聚焦检索策略优化，适配推荐系统召回环节",
        "relevance_score": 4,
        "novelty_score": 8,
        "novelty_reason": "引入RL与自反思机制提升检索策略学习"
    },
    {
        "title": "A Decentralized Retrieval Augmented Generation System with Source Reliabilities Secured on Blockchain",
        "url": "http://arxiv.org/abs/2511.07577v1",
        "pub_date": "2025-11-10",
        "summary": "Existing retrieval-augmented generation (RAG) systems typically use a centralized architecture, causing a high cost of data collection, integration, and management, as well as privacy concerns. There is a great need for a decentralized RAG system that enables foundation models to utilize information directly from data owners who maintain full control over their sources. However, decentralization brings a challenge: the numerous independent data sources vary significantly in reliability, which can diminish retrieval accuracy and response quality. To address this, our decentralized RAG system has a novel reliability scoring mechanism that dynamically evaluates each source based on the quality of responses it contributes to generate and prioritizes high-quality sources during retrieval. To ensure transparency and trust, the scoring process is securely managed through blockchain-based smart contracts, creating verifiable and tamper-proof reliability records without relying on a central authority. We evaluate our decentralized system with two Llama models (3B and 8B) in two simulated environments where six data sources have different levels of reliability. Our system achieves a +10.7\\% performance improvement over its centralized counterpart in the real world-like unreliable data environments. Notably, it approaches the upper-bound performance of centralized systems under ideally reliable data environments. The decentralized infrastructure enables secure and trustworthy scoring management, achieving approximately 56\\% marginal cost savings through batched update operations. Our code and system are open-sourced at github.com/yining610/Reliable-dRAG.",
        "translated": "现有检索增强生成（RAG）系统通常采用集中式架构，导致数据收集、整合与管理成本高昂，并引发隐私问题。亟需一种去中心化的 RAG 系统，使基础模型能够直接利用由数据所有者自主掌控的数据源所提供的信息。然而，去中心化带来了新的挑战：众多独立的数据源在可靠性方面差异显著，可能导致检索准确率下降及响应质量受损。为此，我们的去中心化 RAG 系统引入了一种新颖的可靠性评分机制，该机制动态评估每个数据源所贡献生成响应的质量，并在检索过程中优先选择高质量来源。为确保透明性与可信度，评分过程通过基于区块链的智能合约安全执行，生成可验证且不可篡改的可靠性记录，无需依赖中央权威机构。我们在两个模拟环境中，使用两种 Llama 模型（3B 和 8B）对系统进行评估，其中六个数据源具有不同程度的可靠性。在类似真实世界中不稳定的低可靠性数据环境下，本系统相较其集中式对应系统的性能提升达 +10.7%；值得注意的是，在理想可靠数据环境下，其性能接近集中式系统的理论上限。去中心化基础设施实现了安全可信的评分管理机制，并通过批量更新操作实现约 56% 的边际成本节约。我们的代码和系统已开源于 github.com/yining610/Reliable-dRAG。",
        "translated_title": "一种基于区块链保障源可靠性去中心化检索增强生成系统",
        "label": [],
        "label_reason": "论文聚焦RAG系统去中心化，非推荐系统相关",
        "relevance_score": 3,
        "novelty_score": 7,
        "novelty_reason": "创新性评分因方法通用，非推荐场景定制"
    },
    {
        "title": "Bridging Hidden States in Vision-Language Models",
        "url": "http://arxiv.org/abs/2511.11526v1",
        "pub_date": "2025-11-14",
        "summary": "Vision-Language Models (VLMs) are a new family of models that align image content with natural language. Existing approaches typically fuse either (a) early: by mixing tokens/features inside the encoders, or (b) late: by comparing pooled embeddings. Many methods also tie fusion to an autoregressive decoder. However, the hidden states of both modalities already carry rich, modality-specific structure (spatial layout in vision; syntax and semantics in text), so directly aligning these states is a natural way to match what the two modalities \"think\". We propose a lightweight fusion module: a few cross-only, bidirectional attention layers placed near the top of both encoders. Each layer projects the vision and text encoder hidden-state sequences into a shared space, attends across modalities, and sends gated residual updates back, with simple stabilizers to improve alignment. The encoders remain non-causal and strong for understanding, while generation stays cleanly decoupled via an optional decoder. Across standard retrieval, VQA, and visual reasoning benchmarks, BRIDGE outperforms comparable VLMs while preserving the bi-encoder efficiency of contrastive models. We make our code publicly available at https://github.com/jfeinashley/BRIDGE.",
        "translated": "视觉-语言模型（VLMs）是一类将图像内容与自然语言对齐的新型模型。现有方法通常通过两种方式融合模态信息：(a) 早期融合，即在编码器内部混合token或特征；或 (b) 晚期融合，即比较池化后的嵌入向量。许多方法还将融合过程与自回归解码器绑定。然而，视觉和语言两个模态的隐藏状态本身已包含丰富的模态特异性结构（视觉中的空间布局；文本中的句法与语义），因此直接对齐这些状态是使两个模态“思考”内容匹配的自然方式。我们提出一种轻量级融合模块：在两个编码器顶部附近放置若干仅跨模态、双向注意力层。每一层将视觉与文本编码器的隐藏状态序列投影到共享空间中，跨模态进行注意力计算，并发送门控残差更新回传，同时辅以简单的稳定器以提升对齐效果。编码器保持非因果性并保留强大的理解能力，而生成过程则通过可选解码器清晰解耦。在标准的检索、视觉问答（VQA）及视觉推理基准测试中，BRIDGE 在性能上超越同类 VLMs，同时保留了对比学习模型双编码器架构的高效性。我们的代码已公开发布于 https://github.com/jfeinashley/BRIDGE。",
        "translated_title": "视觉语言模型中的隐状态桥接",
        "label": [],
        "label_reason": "属于高阶视觉任务，非像素级图像处理",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "提出跨模态对齐新架构，性能提升显著"
    },
    {
        "title": "CVChess: A Deep Learning Framework for Converting Chessboard Images to Forsyth-Edwards Notation",
        "url": "http://arxiv.org/abs/2511.11522v1",
        "pub_date": "2025-11-14",
        "summary": "Chess has experienced a large increase in viewership since the pandemic, driven largely by the accessibility of online learning platforms. However, no equivalent assistance exists for physical chess games, creating a divide between analog and digital chess experiences. This paper presents CVChess, a deep learning framework for converting chessboard images to Forsyth-Edwards Notation (FEN), which is later input into online chess engines to provide you with the best next move. Our approach employs a convolutional neural network (CNN) with residual layers to perform piece recognition from smartphone camera images. The system processes RGB images of a physical chess board through a multistep process: image preprocessing using the Hough Line Transform for edge detection, projective transform to achieve a top-down board alignment, segmentation into 64 individual squares, and piece classification into 13 classes (6 unique white pieces, 6 unique black pieces and an empty square) using the residual CNN. Residual connections help retain low-level visual features while enabling deeper feature extraction, improving accuracy and stability during training. We train and evaluate our model using the Chess Recognition Dataset (ChessReD), containing 10,800 annotated smartphone images captured under diverse lighting conditions and angles. The resulting classifications are encoded as an FEN string, which can be fed into a chess engine to generate the most optimal move",
        "translated": "自疫情以来，国际象棋的观众数量大幅增加，这主要得益于在线学习平台的普及。然而，现实中的实体象棋游戏尚无类似辅助工具，导致模拟与数字象棋体验之间形成割裂。本文提出CVChess，一种基于深度学习的框架，可将棋盘图像转换为福斯蒂-爱德华兹记法（FEN），随后输入在线象棋引擎，为你提供最佳下一步走法。我们的方法采用带有残差层的卷积神经网络（CNN），用于从智能手机摄像头拍摄的图像中识别棋子。系统通过多步骤处理RGB格式的实体棋盘图像：首先使用霍夫线变换进行边缘检测作为图像预处理，接着应用透视变换实现棋盘的俯视对齐，然后分割为64个独立方格，最后利用残差CNN将棋子分类至13类（6种白色棋子、6种黑色棋子和一个空位）。残差连接有助于保留低级视觉特征，同时支持更深层次特征提取，在训练过程中提升了模型准确性和稳定性。我们使用包含10,800张在不同光照条件与拍摄角度下由智能手机采集并标注的图像组成的“象棋识别数据集”（ChessReD）对模型进行训练与评估。最终的分类结果被编码为FEN字符串，可直接输入象棋引擎以生成最优走法。",
        "translated_title": "CVChess：一种将棋盘图像转换为福瑟-爱德华兹记谱法的深度学习框架",
        "label": [],
        "label_reason": "任务为棋盘图像识别，属高阶视觉应用",
        "relevance_score": 3,
        "novelty_score": 6,
        "novelty_reason": "CNN结构常规，无像素级恢复创新"
    },
    {
        "title": "Collaborative Representation Learning for Alignment of Tactile, Language, and Vision Modalities",
        "url": "http://arxiv.org/abs/2511.11512v1",
        "pub_date": "2025-11-14",
        "summary": "Tactile sensing offers rich and complementary information to vision and language, enabling robots to perceive fine-grained object properties. However, existing tactile sensors lack standardization, leading to redundant features that hinder cross-sensor generalization. Moreover, existing methods fail to fully integrate the intermediate communication among tactile, language, and vision modalities. To address this, we propose TLV-CoRe, a CLIP-based Tactile-Language-Vision Collaborative Representation learning method. TLV-CoRe introduces a Sensor-Aware Modulator to unify tactile features across different sensors and employs tactile-irrelevant decoupled learning to disentangle irrelevant tactile features. Additionally, a Unified Bridging Adapter is introduced to enhance tri-modal interaction within the shared representation space. To fairly evaluate the effectiveness of tactile models, we further propose the RSS evaluation framework, focusing on Robustness, Synergy, and Stability across different methods. Experimental results demonstrate that TLV-CoRe significantly improves sensor-agnostic representation learning and cross-modal alignment, offering a new direction for multimodal tactile representation.",
        "translated": "触觉感知为视觉与语言提供了丰富且互补的信息，使机器人能够感知物体的细粒度属性。然而，现有触觉传感器缺乏标准化，导致冗余特征，阻碍了跨传感器的泛化能力。此外，现有方法未能充分融合触觉、语言与视觉模态之间的中间交互信息。为此，我们提出 TLV-CoRe，一种基于 CLIP 的触觉-语言-视觉协同表征学习方法。TLV-CoRe 引入了传感器感知调制器（Sensor-Aware Modulator），以统一不同传感器的触觉特征，并采用与触觉无关的解耦学习机制，分离无关的触觉特征。同时，引入统一桥接适配器（Unified Bridging Adapter），增强三模态在共享表征空间内的交互能力。为进一步公平评估触觉模型的有效性，我们进一步提出了 RSS 评估框架，聚焦于不同方法在鲁棒性、协同性与稳定性方面的表现。实验结果表明，TLV-CoRe 显著提升了传感器无关的表征学习能力与跨模态对齐效果，为多模态触觉表征提供了新的研究方向。",
        "translated_title": "触觉、语言与视觉模态的协同表征学习以实现对齐",
        "label": [],
        "label_reason": "多模态融合属高阶任务，非像素级图像处理",
        "relevance_score": 1,
        "novelty_score": 3,
        "novelty_reason": "方法聚焦模态对齐，无图像恢复创新"
    },
    {
        "title": "OpenUS: A Fully Open-Source Foundation Model for Ultrasound Image Analysis via Self-Adaptive Masked Contrastive Learning",
        "url": "http://arxiv.org/abs/2511.11510v1",
        "pub_date": "2025-11-14",
        "summary": "Ultrasound (US) is one of the most widely used medical imaging modalities, thanks to its low cost, portability, real-time feedback, and absence of ionizing radiation. However, US image interpretation remains highly operator-dependent and varies significantly across anatomical regions, acquisition protocols, and device types. These variations, along with unique challenges such as speckle, low contrast, and limited standardized annotations, hinder the development of generalizable, label-efficient ultrasound AI models. In this paper, we propose OpenUS, the first reproducible, open-source ultrasound foundation model built on a large collection of public data. OpenUS employs a vision Mamba backbone, capturing both local and global long-range dependencies across the image. To extract rich features during pre-training, we introduce a novel self-adaptive masking framework that combines contrastive learning with masked image modeling. This strategy integrates the teacher's attention map with student reconstruction loss, adaptively refining clinically-relevant masking to enhance pre-training effectiveness. OpenUS also applies a dynamic learning schedule to progressively adjust the difficulty of the pre-training process. To develop the foundation model, we compile the largest to-date public ultrasound dataset comprising over 308K images from 42 publicly available datasets, covering diverse anatomical regions, institutions, imaging devices, and disease types. Our pre-trained OpenUS model can be easily adapted to specific downstream tasks by serving as a backbone for label-efficient fine-tuning. Code is available at https://github.com/XZheng0427/OpenUS.",
        "translated": "超声（US）是应用最广泛的医学成像模态之一，得益于其成本低廉、便携性好、可提供实时反馈且无电离辐射的特点。然而，超声图像的解读高度依赖操作者，且在不同解剖区域、采集协议和设备类型间存在显著差异。此外，诸如斑点噪声、低对比度以及缺乏标准化标注等独特挑战，进一步阻碍了通用性强、标注效率高的超声人工智能模型的发展。本文提出 OpenUS，这是首个基于大规模公开数据构建的可复现、开源的超声基础模型。OpenUS 采用视觉 Mamba 主干网络，能够捕捉图像中局部与全局长程依赖关系。为增强预训练期间特征提取能力，我们引入一种新颖的自适应掩码框架，结合对比学习与掩码图像建模策略；该方法将教师注意力图与学生重建损失相结合，自适应地优化临床相关掩码，从而提升预训练效果。此外，OpenUS 还采用动态学习调度机制，逐步调整预训练难度。为构建该基础模型，我们整合了迄今为止最大的公开超声数据集，涵盖来自42个公开数据集的超过30.8万张图像，覆盖多种解剖区域、医疗机构、成像设备及疾病类型。预训练后的 OpenUS 模型可通过作为骨干网络轻松适配至特定下游任务，实现低标注量微调。代码已开源于 https://github.com/XZheng0427/OpenUS。",
        "translated_title": "OpenUS：一种基于自适应掩码对比学习的完全开源超声图像分析基础模型",
        "label": [],
        "label_reason": "聚焦医学图像分析，非像素级图像恢复任务",
        "relevance_score": 3,
        "novelty_score": 7,
        "novelty_reason": "提出自适应掩码对比学习框架提升预训练效果"
    },
    {
        "title": "PAS : Prelim Attention Score for Detecting Object Hallucinations in Large Vision--Language Models",
        "url": "http://arxiv.org/abs/2511.11502v1",
        "pub_date": "2025-11-14",
        "summary": "Large vision-language models (LVLMs) are powerful, yet they remain unreliable due to object hallucinations. In this work, we show that in many hallucinatory predictions the LVLM effectively ignores the image and instead relies on previously generated output (prelim) tokens to infer new objects. We quantify this behavior via the mutual information between the image and the predicted object conditioned on the prelim, demonstrating that weak image dependence strongly correlates with hallucination. Building on this finding, we introduce the Prelim Attention Score (PAS), a lightweight, training-free signal computed from attention weights over prelim tokens. PAS requires no additional forward passes and can be computed on the fly during inference. Exploiting this previously overlooked signal, PAS achieves state-of-the-art object-hallucination detection across multiple models and datasets, enabling real-time filtering and intervention.",
        "translated": "大型视觉-语言模型（LVLMs）虽具备强大能力，却因物体幻觉问题而可靠性不足。在本研究中，我们发现，在许多幻觉预测中，LVLM 实际上忽略了图像内容，转而依赖先前生成的输出（prelim）token 来推断新物体。我们通过测量在给定 prelim 条件下图像与预测物体之间的互信息，量化了该行为，结果表明：图像依赖性越弱，幻觉现象越显著。基于这一发现，我们提出 Prelim 注意力得分（PAS），这是一种轻量级、无需训练的信号，由对 prelim token 的注意力权重计算得出。PAS 不需要额外的前向传播，可在推理过程中实时计算。利用这一此前被忽视的信号，PAS 在多个模型和数据集上实现了当前最优的物体幻觉检测性能，从而支持实时过滤与干预。",
        "translated_title": "PAS：用于检测大型视觉-语言模型中物体幻觉的初步注意力得分",
        "label": [],
        "label_reason": "检测对象幻觉属高阶视觉任务，非像素级图像处理。",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "提出轻量PAS信号，高效检测幻觉，具工程实用性。"
    },
    {
        "title": "Multimodal Posterior Sampling-based Uncertainty in PD-L1 Segmentation from H&amp;E Images",
        "url": "http://arxiv.org/abs/2511.11486v1",
        "pub_date": "2025-11-14",
        "summary": "Accurate assessment of PD-L1 expression is critical for guiding immunotherapy, yet current immunohistochemistry (IHC) based methods are resource-intensive. We present nnUNet-B: a Bayesian segmentation framework that infers PD-L1 expression directly from H&amp;E-stained histology images using Multimodal Posterior Sampling (MPS). Built upon nnUNet-v2, our method samples diverse model checkpoints during cyclic training to approximate the posterior, enabling both accurate segmentation and epistemic uncertainty estimation via entropy and standard deviation. Evaluated on a dataset of lung squamous cell carcinoma, our approach achieves competitive performance against established baselines with mean Dice Score and mean IoU of 0.805 and 0.709, respectively, while providing pixel-wise uncertainty maps. Uncertainty estimates show strong correlation with segmentation error, though calibration remains imperfect. These results suggest that uncertainty-aware H&amp;E-based PD-L1 prediction is a promising step toward scalable, interpretable biomarker assessment in clinical workflows.",
        "translated": "准确评估PD-L1表达对于指导免疫治疗至关重要，但当前基于免疫组化（IHC）的方法资源消耗巨大。我们提出nnUNet-B：一种贝叶斯分割框架，利用多模态后验采样（MPS）直接从苏木精-伊红（H&E）染色的组织切片图像推断PD-L1表达。该方法基于nnUNet-v2构建，在周期性训练过程中采样多样化的模型检查点以近似后验分布，从而实现高精度分割并结合熵和标准差估计认知不确定性。在肺鳞状细胞癌数据集上的评估表明，我们的方法在Dice评分和IoU指标上分别达到0.805和0.709，优于现有基准方法，同时提供像素级不确定性图。不确定性估计与分割误差呈强相关性，尽管校准仍不够完善。这些结果表明，基于H&E图像且具备不确定性感知的PD-L1预测是迈向临床工作流中可扩展、可解释生物标志物评估的重要一步。",
        "translated_title": "基于多模态后验采样的PD-L1分割中的不确定性",
        "label": [],
        "label_reason": "属于医学图像分割，非低级图像处理",
        "relevance_score": 2,
        "novelty_score": 5,
        "novelty_reason": "改进框架但未解决像素级质量恢复问题"
    },
    {
        "title": "ImAgent: A Unified Multimodal Agent Framework for Test-Time Scalable Image Generation",
        "url": "http://arxiv.org/abs/2511.11483v1",
        "pub_date": "2025-11-14",
        "summary": "Recent text-to-image (T2I) models have made remarkable progress in generating visually realistic and semantically coherent images. However, they still suffer from randomness and inconsistency with the given prompts, particularly when textual descriptions are vague or underspecified. Existing approaches, such as prompt rewriting, best-of-N sampling, and self-refinement, can mitigate these issues but usually require additional modules and operate independently, hindering test-time scaling efficiency and increasing computational overhead. In this paper, we introduce ImAgent, a training-free unified multimodal agent that integrates reasoning, generation, and self-evaluation within a single framework for efficient test-time scaling. Guided by a policy controller, multiple generation actions dynamically interact and self-organize to enhance image fidelity and semantic alignment without relying on external models. Extensive experiments on image generation and editing tasks demonstrate that ImAgent consistently improves over the backbone and even surpasses other strong baselines where the backbone model fails, highlighting the potential of unified multimodal agents for adaptive and efficient image generation under test-time scaling.",
        "translated": "近年来，文本到图像（T2I）模型在生成视觉上逼真且语义连贯的图像方面取得了显著进展。然而，它们仍存在对给定提示的随机性和不一致性问题，尤其是在文本描述模糊或未明确指定时。现有方法如提示重写、Best-of-N 采样和自精炼可在一定程度上缓解这些问题，但通常需要额外模块，并独立运作，从而阻碍了测试时的可扩展效率并增加计算开销。本文提出 ImAgent，这是一种无需训练的统一多模态代理，在单一框架内集成推理、生成与自我评估，以实现高效的测试时扩展。在策略控制器引导下，多个生成动作动态交互并自我组织，无需依赖外部模型即可提升图像保真度与语义一致性。在图像生成与编辑任务上的大量实验表明，ImAgent 相对于基线模型始终表现更优，甚至在基线模型失效的情况下超越其他强基线方法，凸显了统一多模态代理在测试时扩展场景下实现自适应高效图像生成的巨大潜力。",
        "translated_title": "ImAgent：一种面向测试时可扩展图像生成的统一多模态代理框架",
        "label": [],
        "label_reason": "生成新图像，非像素级恢复任务",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "统一框架提升生成效率，但属高阶图像生成"
    },
    {
        "title": "Rethinking Progression of Memory State in Robotic Manipulation: An Object-Centric Perspective",
        "url": "http://arxiv.org/abs/2511.11478v1",
        "pub_date": "2025-11-14",
        "summary": "As embodied agents operate in increasingly complex environments, the ability to perceive, track, and reason about individual object instances over time becomes essential, especially in tasks requiring sequenced interactions with visually similar objects. In these non-Markovian settings, key decision cues are often hidden in object-specific histories rather than the current scene. Without persistent memory of prior interactions (what has been interacted with, where it has been, or how it has changed) visuomotor policies may fail, repeat past actions, or overlook completed ones. To surface this challenge, we introduce LIBERO-Mem, a non-Markovian task suite for stress-testing robotic manipulation under object-level partial observability. It combines short- and long-horizon object tracking with temporally sequenced subgoals, requiring reasoning beyond the current frame. However, vision-language-action (VLA) models often struggle in such settings, with token scaling quickly becoming intractable even for tasks spanning just a few hundred frames. We propose Embodied-SlotSSM, a slot-centric VLA framework built for temporal scalability. It maintains spatio-temporally consistent slot identities and leverages them through two mechanisms: (1) slot-state-space modeling for reconstructing short-term history, and (2) a relational encoder to align the input tokens with action decoding. Together, these components enable temporally grounded, context-aware action prediction. Experiments show Embodied-SlotSSM's baseline performance on LIBERO-Mem and general tasks, offering a scalable solution for non-Markovian reasoning in object-centric robotic policies.",
        "translated": "随着具身智能体在日益复杂的环境中运作，感知、追踪并随时间推理单个物体实例的能力变得至关重要，尤其是在需要与视觉上相似的物体进行序列化交互的任务中。在这些非马尔可夫环境中，关键决策线索往往隐含于物体特有的历史记录中，而非当前场景。若缺乏对先前交互的记忆（如已交互过什么、位置如何、状态如何变化），视觉-运动策略可能失效、重复过去行为或忽略已完成的动作。为凸显这一挑战，我们提出了LIBERO-Mem，一套面向机器人操作的非马尔可夫任务集，旨在检验在物体级部分可观测条件下系统的鲁棒性。该任务集结合了短时与长时物体跟踪，并要求按时间顺序执行子目标，需超越当前帧进行推理。然而，视觉-语言-动作（VLA）模型在这样的设置中常表现不佳，即使仅处理数百帧任务，其token扩展也迅速变得不可行。为此，我们提出Embodied-SlotSSM，一种以槽位为中心的VLA框架，专为时间可扩展性设计。该框架通过以下两种机制维持时空一致的槽位身份并加以利用：(1) 采用槽位-状态空间建模重构短期历史；(2) 使用关系编码器将输入token与动作解码对齐。上述组件协同作用，实现基于时间的、上下文感知的动作预测。实验表明，Embodied-SlotSSM在LIBERO-Mem及通用任务中的基线性能优异，为其提供了一种适用于物体中心型机器人策略中非马尔可夫推理的可扩展解决方案。",
        "translated_title": "重新思考机器人操作中记忆状态的演化：以物体为中心的视角",
        "label": [],
        "label_reason": "任务为机器人操作与序列决策，非图像像素级处理",
        "relevance_score": 1,
        "novelty_score": 4,
        "novelty_reason": "框架提升时序推理能力，但非图像恢复或增强"
    },
    {
        "title": "Sat2RealCity: Geometry-Aware and Appearance-Controllable 3D Urban Generation from Satellite Imagery",
        "url": "http://arxiv.org/abs/2511.11470v1",
        "pub_date": "2025-11-14",
        "summary": "Recent advances in generative modeling have substantially enhanced 3D urban generation, enabling applications in digital twins, virtual cities, and large-scale simulations. However, existing methods face two key challenges: (1) the need for large-scale 3D city assets for supervised training, which are difficult and costly to obtain, and (2) reliance on semantic or height maps, which are used exclusively for generating buildings in virtual worlds and lack connection to real-world appearance, limiting the realism and generalizability of generated cities. To address these limitations, we propose Sat2RealCity, a geometry-aware and appearance-controllable framework for 3D urban generation from real-world satellite imagery. Unlike previous city-level generation methods, Sat2RealCity builds generation upon individual building entities, enabling the use of rich priors and pretrained knowledge from 3D object generation while substantially reducing dependence on large-scale 3D city assets. Specifically, (1) we introduce the OSM-based spatial priors strategy to achieve interpretable geometric generation from spatial topology to building instances; (2) we design an appearance-guided controllable modeling mechanism for fine-grained appearance realism and style control; and (3) we construct an MLLM-powered semantic-guided generation pipeline, bridging semantic interpretation and geometric reconstruction. Extensive quantitative and qualitative experiments demonstrate that Sat2RealCity significantly surpasses existing baselines in structural consistency and appearance realism, establishing a strong foundation for real-world aligned 3D urban content creation. The code will be released soon.",
        "translated": "近年来生成建模技术的进展显著提升了三维城市生成的能力，使其在数字孪生、虚拟城市及大规模仿真等应用中展现出巨大潜力。然而，现有方法面临两大关键挑战：（1）监督训练需依赖大规模三维城市资产，而获取此类资产既困难又昂贵；（2）现有方法依赖语义图或高度图，仅用于虚拟世界中建筑物的生成，与真实世界外观无直接关联，导致生成城市的真实性与泛化能力受限。为解决上述问题，我们提出 Sat2RealCity，这是一种基于几何感知且外观可控的框架，可从真实世界卫星图像生成三维城市。与以往城市级生成方法不同，Sat2RealCity以单个建筑实体为基础构建生成过程，从而能够充分利用三维物体生成中的丰富先验知识和预训练模型，同时大幅降低对大规模三维城市资产的依赖。具体而言，（1）我们引入基于 OSM 的空间先验策略，实现从空间拓扑到建筑实例的可解释性几何生成；（2）我们设计了一种外观引导的可控建模机制，以实现精细的外观真实感与风格控制；（3）我们构建了一个由多模态大语言模型（MLLM）驱动的语义引导生成流程，弥合语义理解与几何重建之间的鸿沟。大量定量与定性实验表明，Sat2RealCity 在结构一致性与外观真实感方面显著超越现有基线方法，为面向真实世界的三维城市内容创作奠定了坚实基础。代码将很快开源。",
        "translated_title": "Sat2RealCity：从卫星图像生成几何感知且外观可控的三维城市",
        "label": [],
        "label_reason": "生成3D城市属高阶视觉任务，非像素级图像恢复。",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出几何与外观可控框架，提升生成质量但非低层处理。"
    },
    {
        "title": "Benchmarking Visual LLMs Resilience to Unanswerable Questions on Visually Rich Documents",
        "url": "http://arxiv.org/abs/2511.11468v1",
        "pub_date": "2025-11-14",
        "summary": "The evolution of Visual Large Language Models (VLLMs) has revolutionized the automatic understanding of Visually Rich Documents (VRDs), which contain both textual and visual elements. Although VLLMs excel in Visual Question Answering (VQA) on multi-page VRDs, their ability to detect unanswerable questions is still an open research question. Our research delves into the robustness of the VLLMs to plausible yet unanswerable questions, i.e., questions that appear valid but cannot be answered due to subtle corruptions caused by swaps between related concepts or plausible question formulations. Corruptions are generated by replacing the original natural language entities with other ones of the same type, belonging to different document elements, and in different layout positions or pages of the related document. To this end, we present VRD-UQA (VISUALLY RICH DOCUMENT UNANSWERABLE QUESTION ANSWERING), a benchmark for evaluating VLLMs' resilience to plausible yet unanswerable questions across multiple dimensions. It automatically alters the questions of existing VQA datasets consisting of multi-page VRDs, verifies their unanswerability using a VLLM-as-a-judge approach, and then thoroughly evaluates VLLMs' performance. Experiments, run on 12 models, analyze: (1) The VLLMs' accuracy in detecting unanswerable questions at both page and document levels; (2) The effect of different types of corruption (NLP entity, document element, layout); (3) The effectiveness of different knowledge injection strategies based on in-context learning (OCR, multi-page selection, or the possibility of unanswerability). Our findings reveal VLLMs' limitations and demonstrate that VRD-UQA can serve as an evaluation framework for developing resilient document VQA systems.",
        "translated": "视觉大语言模型（VLLMs）的发展已彻底革新了对视觉丰富文档（VRDs）的自动理解，此类文档同时包含文本与视觉元素。尽管VLLMs在多页VRD上的视觉问答（VQA）任务中表现出色，其检测无答案问题的能力仍属开放性研究课题。本研究深入探讨VLLMs面对“看似合理却无法回答”的问题时的鲁棒性，即那些形式上有效但因相关概念或合理提问方式间细微替换而无法解答的问题。此类“退化”由将原始自然语言实体替换为同类型但属于文档其他元素、布局位置或不同页面的实体生成。为此，我们提出VRD-UQA（VISUALLY RICH DOCUMENT UNANSWERABLE QUESTION ANSWERING），一个用于从多维度评估VLLMs对上述可理解却不可答问题鲁棒性的基准测试平台。该框架自动修改现有包含多页VRD的VQA数据集中的问题，通过“VLLM作为裁判”的方法验证其无答案性，并全面评估VLLMs的表现。在12个模型上进行的实验分析如下：（1）VLLMs在页面级与文档级识别无答案问题的准确率；（2）不同类型退化（NLP实体、文档元素、布局）的影响；（3）基于上下文学习的不同知识注入策略（OCR、跨多页选择或无答案可能性）的有效性。我们的研究结果揭示了VLLMs的局限性，并表明VRD-UQA可作为构建鲁棒文档VQA系统的重要评估框架。",
        "translated_title": "视觉大语言模型在视觉丰富文档中对无答案问题的鲁棒性评估",
        "label": [],
        "label_reason": "研究VLLM在文档问答中的鲁棒性，属高阶视觉任务",
        "relevance_score": 1,
        "novelty_score": 3,
        "novelty_reason": "方法为评测框架，无像素级图像处理创新"
    },
    {
        "title": "Rethinking Efficient Mixture-of-Experts for Remote Sensing Modality-Missing Classification",
        "url": "http://arxiv.org/abs/2511.11460v1",
        "pub_date": "2025-11-14",
        "summary": "Multimodal classification in remote sensing often suffers from missing modalities caused by environmental interference, sensor failures, or atmospheric effects, which severely degrade classification performance. Existing two-stage adaptation methods are computationally expensive and assume complete multimodal data during training, limiting their generalization to real-world incompleteness. To overcome these issues, we propose a Missing-aware Mixture-of-Loras (MaMOL) framework that reformulates modality missing as a multi-task learning problem. MaMOL introduces a dual-routing mechanism: a task-oriented dynamic router that adaptively activates experts for different missing patterns, and a modality-specific-shared static router that maintains stable cross-modal knowledge sharing. Unlike prior methods that train separate networks for each missing configuration, MaMOL achieves parameter-efficient adaptation via lightweight expert updates and shared expert reuse. Experiments on multiple remote sensing benchmarks demonstrate superior robustness and generalization under varying missing rates, with minimal computational overhead. Moreover, transfer experiments on natural image datasets validate its scalability and cross-domain applicability, highlighting MaMOL as a general and efficient solution for incomplete multimodal learning.",
        "translated": "遥感中的多模态分类常因环境干扰、传感器故障或大气效应导致模态缺失，严重降低分类性能。现有两阶段自适应方法计算开销大，且假设训练时具备完整的多模态数据，限制其对现实世界中数据不完整情况的泛化能力。为解决上述问题，我们提出一种感知缺失的LoRA混合（MaMOL）框架，将模态缺失重新建模为多任务学习问题。MaMOL引入双路由机制：一个面向任务的动态路由器，可根据不同缺失模式自适应激活专家模块；另一个针对模态的共享静态路由器，保障跨模态知识稳定共享。与以往为每种缺失配置训练独立网络的方法不同，MaMOL通过轻量级专家更新和共享专家复用实现参数高效的自适应。在多个遥感基准数据集上的实验表明，MaMOL在不同缺失率下具有优越的鲁棒性和泛化能力，且计算开销极小。此外，在自然图像数据集上的迁移实验验证了其可扩展性与跨域适用性，凸显MaMOL作为处理不完整多模态学习的一般高效解决方案的优势。",
        "translated_title": "重新思考遥感模态缺失分类中的高效专家混合机制",
        "label": [],
        "label_reason": "任务为多模态分类，非图像像素级恢复",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出轻量专家更新机制提升效率"
    },
    {
        "title": "Synergy vs. Noise: Performance-Guided Multimodal Fusion For Biochemical Recurrence-Free Survival in Prostate Cancer",
        "url": "http://arxiv.org/abs/2511.11452v1",
        "pub_date": "2025-11-14",
        "summary": "Multimodal deep learning (MDL) has emerged as a transformative approach in computational pathology. By integrating complementary information from multiple data sources, MDL models have demonstrated superior predictive performance across diverse clinical tasks compared to unimodal models. However, the assumption that combining modalities inherently improves performance remains largely unexamined. We hypothesise that multimodal gains depend critically on the predictive quality of individual modalities, and that integrating weak modalities may introduce noise rather than complementary information. We test this hypothesis on a prostate cancer dataset with histopathology, radiology, and clinical data to predict time-to-biochemical recurrence. Our results confirm that combining high-performing modalities yield superior performance compared to unimodal approaches. However, integrating a poor-performing modality with other higher-performing modalities degrades predictive accuracy. These findings demonstrate that multimodal benefit requires selective, performance-guided integration rather than indiscriminate modality combination, with implications for MDL design across computational pathology and medical imaging.",
        "translated": "多模态深度学习（MDL）已成为计算病理学领域的一种变革性方法。通过整合来自多种数据源的互补信息，MDL模型在各类临床任务中展现出优于单模态模型的预测性能。然而，将不同模态组合以提升性能这一假设仍缺乏充分检验。我们假设：多模态增益高度依赖于各模态自身的预测能力，若整合性能较差的模态反而可能引入噪声而非补充信息。我们在一个包含组织病理学、放射学和临床数据的前列腺癌数据集上验证了该假设，旨在预测生化复发时间。结果证实：当整合高预测性能的模态时，其效果显著优于单模态方法；但若将低性能模态与高绩效模态结合，则会降低预测准确性。本研究结果表明，实现多模态收益需采用有选择性的、基于性能引导的模态融合策略，而非盲目组合所有模态，这对计算病理学及医学影像领域的MDL架构设计具有重要启示。",
        "translated_title": "协同还是噪声：面向前列腺癌生化无复发生存的性能引导型多模态融合",
        "label": [],
        "label_reason": "属于高阶医学预测任务，非图像处理",
        "relevance_score": 1,
        "novelty_score": 1,
        "novelty_reason": "无图像恢复创新，仅多模态融合策略"
    },
    {
        "title": "VoxTell: Free-Text Promptable Universal 3D Medical Image Segmentation",
        "url": "http://arxiv.org/abs/2511.11450v1",
        "pub_date": "2025-11-14",
        "summary": "We introduce VoxTell, a vision-language model for text-prompted volumetric medical image segmentation. It maps free-form descriptions, from single words to full clinical sentences, to 3D masks. Trained on 62K+ CT, MRI, and PET volumes spanning over 1K anatomical and pathological classes, VoxTell uses multi-stage vision-language fusion across decoder layers to align textual and visual features at multiple scales. It achieves state-of-the-art zero-shot performance across modalities on unseen datasets, excelling on familiar concepts while generalizing to related unseen classes. Extensive experiments further demonstrate strong cross-modality transfer, robustness to linguistic variations and clinical language, as well as accurate instance-specific segmentation from real-world text. Code is available at: https://www.github.com/MIC-DKFZ/VoxTell",
        "translated": "我们提出了VoxTell，一种用于文本提示三维医学图像分割的视觉-语言模型。该模型可将自由形式的文本描述（从单个单词到完整的临床句子）映射为三维掩码。VoxTell在超过62,000个CT、MRI和PET体积数据上进行训练，涵盖1,000多个解剖学与病理学类别，通过解码器层间的多阶段视觉-语言融合机制，在多个尺度上对齐文本与视觉特征。该模型在未见数据集上实现了跨模态的最先进零样本性能，不仅在熟悉概念上表现优异，还能有效泛化至相关未见类别。大量实验进一步验证了其强大的跨模态迁移能力、对语言变体及临床语言的鲁棒性，以及从真实世界文本中实现精确实例级分割的能力。代码开源地址：https://www.github.com/MIC-DKFZ/VoxTell",
        "translated_title": "VoxTell：支持自由文本提示的通用3D医学图像分割",
        "label": [],
        "label_reason": "任务为医学图像语义分割，属高阶视觉任务。",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "提出多阶段视觉语言融合架构，跨模态迁移能力显著。"
    },
    {
        "title": "From Synthetic Scenes to Real Performance: Enhancing Spatial Reasoning in VLMs",
        "url": "http://arxiv.org/abs/2511.11440v1",
        "pub_date": "2025-11-14",
        "summary": "Fine-tuning Vision-Language Models (VLMs) is a common strategy to improve performance following an ad-hoc data collection and annotation of real-world scenes. However, this process is often prone to biases, errors, and distribution imbalance, resulting in overfitting and imbalanced performance. Although a few studies have tried to address this problem by generating synthetic data, they lacked control over distribution bias and annotation quality. To address these challenges, we redesign the fine-tuning process in two ways. First, we control the generation of data and its annotations, ensuring it is free from bias, distribution imbalance, and annotation errors. We automatically construct the dataset by comprehensively sampling objects' attributes, including color, shape, size, and position within the scene. Secondly, using this annotated dataset, we fine-tune state-of-the-art VLMs and assess performance transferability to real-world data on the absolute position task. We conduct exhaustive evaluations on both synthetic and real-world benchmarks. Our experiments reveal two key findings: 1) fine-tuning on balanced synthetic data yields uniform performance across the visual scene and mitigates common biases; and 2) fine-tuning on synthetic stimuli significantly improves performance on real-world data (COCO), outperforming models fine-tuned in the matched setting.",
        "translated": "对视觉-语言模型（VLMs）进行微调是一种在收集并标注现实场景数据后提升性能的常用策略。然而，该过程往往容易产生偏倚、错误和分布失衡，从而导致过拟合及性能不均衡。尽管已有部分研究尝试通过生成合成数据来缓解这一问题，但其缺乏对分布偏倚与标注质量的有效控制。为应对上述挑战，我们从两个方面重新设计了微调流程。首先，我们控制数据及其标注的生成过程，确保其无偏倚、无分布失衡且标注准确。我们通过全面采样场景中物体的属性（包括颜色、形状、尺寸和位置），自动构建数据集。其次，利用该标注数据集，我们对当前最先进的 VLMs 进行微调，并评估其在绝对位置任务上的真实世界数据迁移能力。我们在合成与真实世界基准数据集上均进行了详尽评估。实验结果揭示两点关键发现：1）在平衡的合成数据上进行微调可实现视觉场景内均匀的性能表现，并有效缓解常见偏倚；2）在合成刺激数据上进行微调显著提升了在真实世界数据（COCO）上的性能，优于在匹配设置下微调的模型。",
        "translated_title": "从合成场景到真实性能：增强视觉语言模型中的空间推理能力",
        "label": [],
        "label_reason": "研究VLMs微调，非图像像素级处理",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "合成数据控制与迁移学习新策略"
    },
    {
        "title": "VP-Bench: A Comprehensive Benchmark for Visual Prompting in Multimodal Large Language Models",
        "url": "http://arxiv.org/abs/2511.11438v1",
        "pub_date": "2025-11-14",
        "summary": "Multimodal large language models (MLLMs) have enabled a wide range of advanced vision-language applications, including fine-grained object recognition and contextual understanding. When querying specific regions or objects in an image, human users naturally use \"visual prompts\" (VPs), such as bounding boxes, to provide reference. However, no existing benchmark systematically evaluates the ability of MLLMs to interpret such VPs. This gap leaves it unclear whether current MLLMs can effectively recognize VPs, an intuitive prompting method for humans, and use them to solve problems. To address this limitation, we introduce VP-Bench, a benchmark for assessing MLLMs' capability in VP perception and utilization. VP-Bench employs a two-stage evaluation framework: Stage 1 examines models' ability to perceive VPs in natural scenes, using 30k visualized prompts spanning eight shapes and 355 attribute combinations. Stage 2 investigates the impact of VPs on downstream tasks, measuring their effectiveness in real-world problem-solving scenarios. Using VP-Bench, we evaluate 28 MLLMs, including proprietary systems (e.g., GPT-4o) and open-source models (e.g., InternVL3 and Qwen2.5-VL), and provide a comprehensive analysis of factors that affect VP understanding, such as variations in VP attributes, question arrangement, and model scale. VP-Bench establishes a new reference framework for studying how MLLMs comprehend and resolve grounded referring questions.",
        "translated": "多模态大语言模型（MLLMs）已推动了广泛的视觉-语言高级应用，包括细粒度目标识别与上下文理解。当用户在图像中查询特定区域或物体时，通常会自然使用“视觉提示”（VPs），如边界框，作为参考依据。然而，目前尚无任何基准系统对 MLLMs 解读此类 VPs 的能力进行系统性评估。这一空白导致当前 MLLMs 是否能有效识别人类直观使用的 VPs、并利用其解决问题，仍不明确。为解决此局限，我们提出了 VP-Bench，一个用于评估 MLLMs 在 VPs 理解与应用能力的基准测试。VP-Bench 采用两阶段评估框架：第一阶段考察模型在自然场景中感知 VPs 的能力，涵盖 30,000 个可视化提示，涉及八种形状和 355 种属性组合；第二阶段探究 VPs 对下游任务的影响，衡量其在现实问题解决场景中的有效性。借助 VP-Bench，我们评估了 28 个 MLLMs，包括专有系统（如 GPT-4o）及开源模型（如 InternVL3 和 Qwen2.5-VL），并全面分析影响 VPs 理解的关键因素，例如 VPs 属性变化、问题排列方式以及模型规模。VP-Bench 建立了一个新的参考框架，用于研究 MLLMs 如何理解和解决基于视觉提示的指代性问题。",
        "translated_title": "VP-Bench：多模态大语言模型中视觉提示的综合基准",
        "label": [],
        "label_reason": "评估视觉提示理解，属高阶视觉任务",
        "relevance_score": 1,
        "novelty_score": 3,
        "novelty_reason": "复现性评测框架，无像素级创新"
    },
    {
        "title": "Hi-DREAM: Brain Inspired Hierarchical Diffusion for fMRI Reconstruction via ROI Encoder and visuAl Mapping",
        "url": "http://arxiv.org/abs/2511.11437v1",
        "pub_date": "2025-11-14",
        "summary": "Mapping human brain activity to natural images offers a new window into vision and cognition, yet current diffusion-based decoders face a core difficulty: most condition directly on fMRI features without analyzing how visual information is organized across the cortex. This overlooks the brain's hierarchical processing and blurs the roles of early, middle, and late visual areas. We propose Hi-DREAM, a brain-inspired conditional diffusion framework that makes the cortical organization explicit. A region-of-interest (ROI) adapter groups fMRI into early/mid/late streams and converts them into a multi-scale cortical pyramid aligned with the U-Net depth (shallow scales preserve layout and edges; deeper scales emphasize objects and semantics). A lightweight, depth-matched ControlNet injects these scale-specific hints during denoising. The result is an efficient and interpretable decoder in which each signal plays a brain-like role, allowing the model not only to reconstruct images but also to illuminate functional contributions of different visual areas. Experiments on the Natural Scenes Dataset (NSD) show that Hi-DREAM attains state-of-the-art performance on high-level semantic metrics while maintaining competitive low-level fidelity. These findings suggest that structuring conditioning by cortical hierarchy is a powerful alternative to purely data-driven embeddings and provides a useful lens for studying the visual cortex.",
        "translated": "将人脑活动映射到自然图像为视觉与认知研究开辟了新的窗口，但当前基于扩散模型的解码器面临一个核心难题：大多数方法直接以fMRI特征作为条件，而未分析视觉信息在皮层中的组织方式。这忽略了大脑分层处理的特性，并模糊了早期、中期和晚期视觉区域的功能分工。我们提出Hi-DREAM，一种受脑启发的条件扩散框架，显式建模皮层组织结构。该框架通过感兴趣区域（ROI）适配器将fMRI特征划分为早期/中期/晚期流，并将其转换为与U-Net深度对齐的多尺度皮层金字塔（浅层保留布局与边缘；深层强调物体与语义）。一个轻量级、深度匹配的ControlNet在去噪过程中注入各尺度特定的引导信号。最终得到一个高效且可解释的解码器，其中每个信号扮演类似脑区的角色，使模型不仅能重建图像，还能揭示不同视觉区域的功能贡献。在自然场景数据集（NSD）上的实验表明，Hi-DREAM在高层语义指标上达到最先进性能，同时保持具有竞争力的低层保真度。这些发现表明，按皮层层级结构化条件输入是一种优于纯数据驱动嵌入的强大替代方案，并为研究视觉皮层提供了有益视角。",
        "translated_title": "Hi-DREAM：基于脑启发式分层扩散与ROI编码器及视觉映射的fMRI重建方法",
        "label": [],
        "label_reason": "目标为fMRI到图像重建，属高阶视觉任务",
        "relevance_score": 3,
        "novelty_score": 7,
        "novelty_reason": "提出脑启发层级扩散框架，结构创新但非低层图像处理"
    },
    {
        "title": "Unsupervised Motion-Compensated Decomposition for Cardiac MRI Reconstruction via Neural Representation",
        "url": "http://arxiv.org/abs/2511.11436v1",
        "pub_date": "2025-11-14",
        "summary": "Cardiac magnetic resonance (CMR) imaging is widely used to characterize cardiac morphology and function. To accelerate CMR imaging, various methods have been proposed to recover high-quality spatiotemporal CMR images from highly undersampled k-t space data. However, current CMR reconstruction techniques either fail to achieve satisfactory image quality or are restricted by the scarcity of ground truth data, leading to limited applicability in clinical scenarios. In this work, we proposed MoCo-INR, a new unsupervised method that integrates implicit neural representations (INR) with the conventional motion-compensated (MoCo) framework. Using explicit motion modeling and the continuous prior of INRs, MoCo-INR can produce accurate cardiac motion decomposition and high-quality CMR reconstruction. Furthermore, we introduce a new INR network architecture tailored to the CMR problem, which significantly stabilizes model optimization. Experiments on retrospective (simulated) datasets demonstrate the superiority of MoCo-INR over state-of-the-art methods, achieving fast convergence and fine-detailed reconstructions at ultra-high acceleration factors (e.g., 20x in VISTA sampling). Additionally, evaluations on prospective (real-acquired) free-breathing CMR scans highlight the clinical practicality of MoCo-INR for real-time imaging. Several ablation studies further confirm the effectiveness of the critical components of MoCo-INR.",
        "translated": "心脏磁共振成像（CMR）广泛用于表征心脏形态与功能。为加速CMR成像，已有多种方法被提出，旨在从高度欠采样的k-t空间数据中重建高质量的时空CMR图像。然而，当前的CMR重建技术要么无法获得令人满意的图像质量，要么受限于真实标注数据的匮乏，导致其在临床场景中的适用性有限。在本研究中，我们提出了MoCo-INR，一种新颖的无监督方法，将隐式神经表示（INR）与传统的运动补偿（MoCo）框架相结合。通过显式运动建模以及INR所固有的连续先验，MoCo-INR能够实现精确的心脏运动分解并重建高质量的CMR图像。此外，我们设计了一种专为CMR问题定制的INR网络架构，显著提升了模型优化的稳定性。在回顾性（模拟）数据集上的实验表明，MoCo-INR优于现有最先进方法，能够在极高的加速因子下（例如VISTA采样中的20倍）快速收敛并生成细节丰富的重建结果。同时，在前瞻性（真实采集）自由呼吸CMR扫描中的评估进一步凸显了MoCo-INR在实时成像中的临床实用性。若干消融实验进一步验证了MoCo-INR关键组件的有效性。",
        "translated_title": "基于神经表示的无监督运动补偿分解用于心脏MRI重建",
        "label": [
            "医学图像增强",
            "图像重建"
        ],
        "label_reason": "基于神经表示的无监督心脏MRI重建，属低层医学图像复原",
        "relevance_score": 9,
        "novelty_score": 8,
        "novelty_reason": "引入定制INR架构与显式运动建模提升重建性能"
    },
    {
        "title": "The Persistence of Cultural Memory: Investigating Multimodal Iconicity in Diffusion Models",
        "url": "http://arxiv.org/abs/2511.11435v1",
        "pub_date": "2025-11-14",
        "summary": "Our work addresses the ambiguity between generalization and memorization in text-to-image diffusion models, focusing on a specific case we term multimodal iconicity. This refers to instances where images and texts evoke culturally shared associations, such as when a title recalls a familiar artwork or film scene. While prior research on memorization and unlearning emphasizes forgetting, we examine what is remembered and how, focusing on the balance between recognizing cultural references and reproducing them. We introduce an evaluation framework that separates recognition, whether a model identifies a reference, from realization, how it depicts it through replication or reinterpretation, quantified through measures capturing both dimensions. By evaluating five diffusion models across 767 Wikidata-derived cultural references spanning static and dynamic imagery, we show that our framework distinguishes replication from transformation more effectively than existing similarity-based methods. To assess linguistic sensitivity, we conduct prompt perturbation experiments using synonym substitutions and literal image descriptions, finding that models often reproduce iconic visual structures even when textual cues are altered. Finally, our analysis shows that cultural alignment correlates not only with training data frequency, but also textual uniqueness, reference popularity, and creation date. Our work reveals that the value of diffusion models lies not only in what they reproduce but in how they transform and recontextualize cultural knowledge, advancing evaluation beyond simple text-image matching toward richer contextual understanding.",
        "translated": "我们的工作聚焦于文本到图像扩散模型中泛化与记忆之间的模糊边界，尤其关注我们所定义的一种特定情形——多模态标志性（multimodal iconicity）。这指的是图像与文本唤起文化共享联想的情形，例如标题唤起熟悉的艺术作品或电影场景。尽管以往关于记忆与遗忘的研究强调“遗忘”，我们则考察被记住的内容及其呈现方式，重点关注在识别文化参照物与再现其视觉形式之间如何取得平衡。我们引入了一种评估框架，将“识别”（模型是否识别出文化参照）与“再现”（模型通过复制或重构的方式如何表现该参照）明确区分开来，并通过同时捕捉这两个维度的度量指标进行量化。通过对767个源自Wikidata的文化参考项（涵盖静态与动态图像）在五种扩散模型上进行评估，我们证明，相较于现有基于相似性的方法，我们的框架更有效地区分了复制与转化。为评估语言敏感性，我们采用同义词替换和字面图像描述等提示扰动实验，发现即使文本线索发生改变，模型仍常复现标志性视觉结构。最终，我们的分析表明，文化契合度不仅与训练数据频次相关，还与文本独特性、参照物流行度及创作日期等因素密切相关。本研究揭示，扩散模型的价值不仅在于其再现能力，更在于其对文化知识的转化与再语境化能力，从而推动评估从简单的图文匹配迈向更丰富的语境理解。",
        "translated_title": "文化记忆的持久性：探究扩散模型中的多模态图标性",
        "label": [],
        "label_reason": "研究文化记忆与扩散模型，非图像像素级处理",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出新评估框架区分复制与重构，具方法创新"
    },
    {
        "title": "WEAVE: Unleashing and Benchmarking the In-context Interleaved Comprehension and Generation",
        "url": "http://arxiv.org/abs/2511.11434v1",
        "pub_date": "2025-11-14",
        "summary": "Recent advances in unified multimodal models (UMMs) have enabled impressive progress in visual comprehension and generation. However, existing datasets and benchmarks focus primarily on single-turn interactions, failing to capture the multi-turn, context-dependent nature of real-world image creation and editing. To address this gap, we present WEAVE, the first suite for in-context interleaved cross-modality comprehension and generation. Our suite consists of two complementary parts. WEAVE-100k is a large-scale dataset of 100K interleaved samples spanning over 370K dialogue turns and 500K images, covering comprehension, editing, and generation tasks that require reasoning over historical context. WEAVEBench is a human-annotated benchmark with 100 tasks based on 480 images, featuring a hybrid VLM judger evaluation framework based on both the reference image and the combination of the original image with editing instructions that assesses models' abilities in multi-turn generation, visual memory, and world-knowledge reasoning across diverse domains. Experiments demonstrate that training on WEAVE-100k enables vision comprehension, image editing, and comprehension-generation collaboration capabilities. Furthermore, it facilitates UMMs to develop emergent visual-memory capabilities, while extensive evaluations on WEAVEBench expose the persistent limitations and challenges of current approaches in multi-turn, context-aware image generation and editing. We believe WEAVE provides a view and foundation for studying in-context interleaved comprehension and generation for multi-modal community.",
        "translated": "近年来，统一多模态模型（UMMs）的进展显著推动了视觉理解与生成能力的发展。然而，现有数据集与基准测试主要聚焦于单轮交互，未能充分捕捉现实世界图像创作与编辑所具有的多轮、上下文依赖特性。为弥合这一差距，我们提出 WEAVE，这是首个支持上下文内交错式跨模态理解与生成的工具套件。该套件由两个互补部分组成：WEAVE-100k 是一个包含 10 万组交错样本的大规模数据集，涵盖超过 37 万轮对话与 50 万张图像，覆盖需基于历史上下文推理的理解、编辑与生成任务；WEAVEBench 是一个人工标注的基准评测集，基于 480 张图像构建了 100 项任务，采用混合视觉语言模型（VLM）裁判评估框架——结合参考图像以及原始图像与编辑指令的组合，全面评估模型在多轮生成、视觉记忆及跨领域世界知识推理等方面的能力。实验表明，以 WEAVE-100k 进行训练可有效提升视觉理解、图像编辑及理解-生成协同能力；同时，它亦有助于 UMMs 发展出涌现式的视觉记忆能力。而对 WEAVEBench 的广泛评测则揭示了当前方法在多轮、上下文感知图像生成与编辑方面仍存在的固有局限与挑战。我们认为，WEAVE 为多模态社区研究上下文内交错式理解与生成提供了新的视角与基础。",
        "translated_title": "WEAVE：释放并评估上下文交错的综合与生成能力",
        "label": [],
        "label_reason": "研究多轮视觉对话，非像素级图像处理",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "首次构建多轮跨模态数据集与基准，推动模型能力"
    },
    {
        "title": "Comprehension of Multilingual Expressions Referring to Target Objects in Visual Inputs",
        "url": "http://arxiv.org/abs/2511.11427v1",
        "pub_date": "2025-11-14",
        "summary": "Referring Expression Comprehension (REC) requires models to localize objects in images based on natural language descriptions. Research on the area remains predominantly English-centric, despite increasing global deployment demands. This work addresses multilingual REC through two main contributions. First, we construct a unified multilingual dataset spanning 10 languages, by systematically expanding 12 existing English REC benchmarks through machine translation and context-based translation enhancement. The resulting dataset comprises approximately 8 million multilingual referring expressions across 177,620 images, with 336,882 annotated objects. Second, we introduce an attention-anchored neural architecture that uses multilingual SigLIP2 encoders. Our attention-based approach generates coarse spatial anchors from attention distributions, which are subsequently refined through learned residuals. Experimental evaluation demonstrates competitive performance on standard benchmarks, e.g. achieving 86.9% accuracy at IoU@50 on RefCOCO aggregate multilingual evaluation, compared to an English-only result of 91.3%. Multilingual evaluation shows consistent capabilities across languages, establishing the practical feasibility of multilingual visual grounding systems. The dataset and model are available at $\\href{https://multilingual.franreno.com}{multilingual.franreno.com}$.",
        "translated": "指代表达理解（Referring Expression Comprehension, REC）要求模型根据自然语言描述在图像中定位目标物体。尽管全球部署需求日益增长，该领域研究仍以英语为中心。本文通过两大贡献解决多语言REC问题。首先，我们构建了一个涵盖10种语言的统一多语言数据集，通过机器翻译及基于上下文的翻译增强，系统扩展了12个现有的英文REC基准数据集。所获数据集包含约800万条多语言指代表达，覆盖177,620张图像，标注物体共336,882个。其次，我们提出了一种基于注意力锚点的神经架构，采用多语言SigLIP2编码器。我们的注意力机制从注意力分布生成粗略的空间锚点，随后通过学习到的残差进行精细化调整。实验评估表明，该方法在标准基准上表现具有竞争力，例如在RefCOCO聚合多语言评估中，IoU@50准确率达86.9%，相较仅使用英语结果（91.3%）仍保持良好性能。多语言评估显示其在不同语言间具备一致能力，验证了多语言视觉接地系统的实际可行性。数据集与模型可在 $\\href{https://multilingual.franreno.com}{multilingual.franreno.com}$ 获取。",
        "translated_title": "视觉输入中指向目标物体的多语言表达的理解",
        "label": [],
        "label_reason": "任务为多语言视觉 grounding，属高阶视觉理解",
        "relevance_score": 1,
        "novelty_score": 3,
        "novelty_reason": "方法为标准多语言模型组合，无本质创新"
    },
    {
        "title": "GRIN Transfer: A production-ready tool for libraries to retrieve digital copies from Google Books",
        "url": "http://arxiv.org/abs/2511.11447v1",
        "pub_date": "2025-11-14",
        "summary": "Publicly launched in 2004, the Google Books project has scanned tens of millions of items in partnership with libraries around the world. As part of this project, Google created the Google Return Interface (GRIN). Through this platform, libraries can access their scanned collections, the associated metadata, and the ongoing OCR and metadata improvements that become available as Google reprocesses these collections using new technologies. When downloading the Harvard Library Google Books collection from GRIN to develop the Institutional Books dataset, we encountered several challenges related to rate-limiting and atomized metadata within the GRIN platform. To overcome these challenges and help other libraries make more robust use of their Google Books collections, this technical report introduces the initial release of GRIN Transfer. This open-source and production-ready Python pipeline allows partner libraries to efficiently retrieve their Google Books collections from GRIN. This report also introduces an updated version of our Institutional Books 1.0 pipeline, initially used to analyze, augment, and assemble the Institutional Books 1.0 dataset. We have revised this pipeline for compatibility with the output format of GRIN Transfer. A library could pair these two tools to create an end-to-end processing pipeline for their Google Books collection to retrieve, structure, and enhance data available from GRIN. This report gives an overview of how GRIN Transfer was designed to optimize for reliability and usability in different environments, as well as guidance on configuration for various use cases.",
        "translated": "2004年公开启动的Google Books项目，已与全球多家图书馆合作，扫描了数千万件文献。作为该项目的一部分，谷歌创建了Google Return Interface（GRIN）平台。通过该平台，图书馆可访问其扫描的文献集合、相关元数据，以及谷歌在使用新技术重新处理这些文献时持续更新的OCR结果和元数据改进内容。在从GRIN平台下载哈佛大学图书馆的Google Books文献集合以构建Institutional Books数据集的过程中，我们遇到了与GRIN平台速率限制及碎片化元数据相关的若干挑战。为克服这些挑战，并帮助其他图书馆更稳健地利用其Google Books文献资源，本技术报告介绍了GRIN Transfer的首个版本。这是一个开源且具备生产环境稳定性的Python管道工具，允许合作图书馆高效地从GRIN平台提取其Google Books文献集合。本报告还介绍了我们原先用于分析、扩充并整合Institutional Books 1.0数据集的Pipeline的更新版本。我们已对该Pipeline进行了修订，使其兼容GRIN Transfer的输出格式。图书馆可将这两款工具组合使用，构建端到端的数据处理流水线，实现对其Google Books文献集合的高效获取、结构化整理与数据增强。本报告概述了GRIN Transfer的设计理念——如何优化其在不同环境下的可靠性与易用性，并提供了针对各类应用场景的配置指南。",
        "translated_title": "GRIN Transfer：一个面向库馆从谷歌图书检索数字副本的生产级工具",
        "label": [],
        "label_reason": "论文仅涉及图书馆数字资源获取工具，与推荐系统无关。",
        "relevance_score": 1,
        "novelty_score": 1,
        "novelty_reason": "无创新点，仅为实用型数据提取工具。"
    },
    {
        "title": "Unlocking Advanced Graph Machine Learning Insights through Knowledge Completion on Neo4j Graph Database",
        "url": "http://arxiv.org/abs/2511.11399v1",
        "pub_date": "2025-11-14",
        "summary": "Graph Machine Learning (GML) with Graph Databases (GDBs) has gained significant relevance in recent years, due to its ability to handle complex interconnected data and apply ML techniques using Graph Data Science (GDS). However, a critical gap exists in the current way GDB-GML applications analyze data, especially in terms of Knowledge Completion (KC) in Knowledge Graphs (KGs). In particular, current architectures ignore KC, working on datasets that appear incomplete or fragmented, despite they actually contain valuable hidden knowledge. This limitation may cause wrong interpretations when these data are used as input for GML models.   This paper proposes an innovative architecture that integrates a KC phase into GDB-GML applications, demonstrating how revealing hidden knowledge can heavily impact datasets' behavior and metrics. For this purpose, we introduce scalable transitive relationships, which are links that propagate information over the network and modelled by a decay function, allowing a deterministic knowledge flows across multiple nodes.   Experimental results demonstrate that our intuition radically reshapes both topology and overall dataset dynamics, underscoring the need for this new GDB-GML architecture to produce better models and unlock the full potential of graph-based data analysis.",
        "translated": "近年来，图数据库（GDBs）与图机器学习（GML）的结合因能够处理复杂互联数据并借助图数据科学（GDS）技术应用机器学习方法而日益重要。然而，当前GDB-GML应用场景在数据分析方面存在一个关键缺陷，尤其是在知识图谱（KGs）中的知识补全（KC）问题上。具体而言，现有架构忽视了知识补全，其工作对象往往是看似不完整或碎片化的数据集，尽管这些数据实际上蕴含着有价值但未被显式表达的隐性知识。这一局限可能导致当此类数据作为输入应用于GML模型时产生错误的解读。\n\n本文提出了一种创新架构，将知识补全阶段集成到GDB-GML应用中，并证明揭示隐藏知识可显著影响数据集的行为与评估指标。为此，我们引入了可扩展的传递关系，即能够在网络中传播信息的连接，通过衰减函数建模，从而实现跨多个节点的确定性知识流动。\n\n实验结果表明，我们的直觉极大重塑了数据集的拓扑结构与整体动态行为，凸显了该新型GDB-GML架构的必要性——它能生成更优模型，并充分释放基于图的数据分析潜力。",
        "translated_title": "通过在Neo4j图数据库上完成知识补全解锁高级图机器学习洞见",
        "label": [],
        "label_reason": "论文聚焦图数据库知识补全，非推荐系统核心环节。",
        "relevance_score": 3,
        "novelty_score": 6,
        "novelty_reason": "提出可扩展传递关系，但未应用于推荐场景。"
    },
    {
        "title": "SRLF: An Agent-Driven Set-Wise Reflective Learning Framework for Sequential Recommendation",
        "url": "http://arxiv.org/abs/2511.11370v1",
        "pub_date": "2025-11-14",
        "summary": "LLM-based agents are emerging as a promising paradigm for simulating user behavior to enhance recommender systems. However, their effectiveness is often limited by existing studies that focus on modeling user ratings for individual items. This point-wise approach leads to prevalent issues such as inaccurate user preference comprehension and rigid item-semantic representations.   To address these limitations, we propose the novel Set-wise Reflective Learning Framework (SRLF). Our framework operationalizes a closed-loop \"assess-validate-reflect\" cycle that harnesses the powerful in-context learning capabilities of LLMs. SRLF departs from conventional point-wise assessment by formulating a holistic judgment on an entire set of items. It accomplishes this by comprehensively analyzing both the intricate interrelationships among items within the set and their collective alignment with the user's preference profile. This method of set-level contextual understanding allows our model to capture complex relational patterns essential to user behavior, making it significantly more adept for sequential recommendation. Extensive experiments validate our approach, confirming that this set-wise perspective is crucial for achieving state-of-the-art performance in sequential recommendation tasks.",
        "translated": "基于大语言模型（LLM）的智能体正逐渐成为模拟用户行为以提升推荐系统的一种有前景范式。然而，其有效性常受限于现有研究主要聚焦于对单个物品进行用户评分建模这一逐点式方法，导致普遍存在的问题包括用户偏好理解不准确以及物品语义表示僵化。\n\n为解决上述局限性，我们提出了一种新颖的集合式反思学习框架（Set-wise Reflective Learning Framework, SRLF）。该框架构建了一个闭环的“评估-验证-反思”循环，充分利用大语言模型强大的上下文学习能力。SRLF不同于传统的逐点评估方式，而是对一组物品整体进行综合判断。它通过全面分析组内物品间的复杂关联关系及其与用户偏好画像的整体契合度来实现这一目标。这种集合层面的上下文理解方式使我们的模型能够捕捉对用户行为至关重要的复杂关系模式，从而在序列推荐任务中表现得更为出色。大量实验验证了我们的方法，证实集合式视角对于实现序列推荐领域的最先进性能至关重要。",
        "translated_title": "SRLF：一种基于代理驱动的集合级反思学习框架用于序列推荐",
        "label": [
            "LLM生成式推荐",
            "序列推荐"
        ],
        "label_reason": "提出基于LLM的集合级反思框架，专为序列推荐设计。",
        "relevance_score": 10,
        "novelty_score": 9,
        "novelty_reason": "首创集合级评估与反思机制，显著提升序列推荐能力。"
    },
    {
        "title": "MOON Embedding: Multimodal Representation Learning for E-commerce Search Advertising",
        "url": "http://arxiv.org/abs/2511.11305v1",
        "pub_date": "2025-11-14",
        "summary": "We introduce MOON, our comprehensive set of sustainable iterative practices for multimodal representation learning for e-commerce applications. MOON has already been fully deployed across all stages of Taobao search advertising system, including retrieval, relevance, ranking, and so on. The performance gains are particularly significant on click-through rate (CTR) prediction task, which achieves an overall +20.00% online CTR improvement. Over the past three years, this project has delivered the largest improvement on CTR prediction task and undergone five full-scale iterations. Throughout the exploration and iteration of our MOON, we have accumulated valuable insights and practical experience that we believe will benefit the research community. MOON contains a three-stage training paradigm of \"Pretraining, Post-training, and Application\", allowing effective integration of multimodal representations with downstream tasks. Notably, to bridge the misalignment between the objectives of multimodal representation learning and downstream training, we define the exchange rate to quantify how effectively improvements in an intermediate metric can translate into downstream gains. Through this analysis, we identify the image-based search recall as a critical intermediate metric guiding the optimization of multimodal models. Over three years and five iterations, MOON has evolved along four critical dimensions: data processing, training strategy, model architecture, and downstream application. The lessons and insights gained through the iterative improvements will also be shared. As part of our exploration into scaling effects in the e-commerce field, we further conduct a systematic study of the scaling laws governing multimodal representation learning, examining multiple factors such as the number of training tokens, negative samples, and the length of user behavior sequences.",
        "translated": "我们提出了MOON，一套面向电子商务应用的多模态表征学习的可持续迭代实践体系。MOON目前已全面部署于淘宝搜索广告系统的各环节，包括召回、相关性建模、排序等阶段。其在点击率（CTR）预测任务上表现尤为突出，线上CTR整体提升达+20.00%。过去三年间，该项目在CTR预测任务上实现了最大幅度的性能提升，并已完成五轮完整迭代。在MOON的探索与迭代过程中，我们积累了宝贵的经验与实用洞见，相信这些成果将对学术界有所裨益。MOON包含“预训练、后训练与应用”三阶段训练范式，实现多模态表征与下游任务的有效融合。值得注意的是，为弥合多模态表征学习目标与下游任务训练目标之间的错位，我们引入“兑换率”概念，用以量化中间指标改进转化为下游收益的有效程度。通过该分析，我们识别出基于图像的搜索召回作为关键中间指标，指导多模态模型优化方向。历经三年五轮迭代，MOON沿四个核心维度持续演进：数据处理、训练策略、模型架构及下游应用。我们在迭代优化中所获得的经验与洞察也将予以分享。作为对电商领域扩展效应研究的一部分，我们进一步开展了一项系统性研究，探究多模态表征学习所遵循的缩放规律，考察训练token数量、负样本量以及用户行为序列长度等多个关键因素的影响。",
        "translated_title": "MOON Embedding：面向电商搜索广告的多模态表征学习",
        "label": [
            "召回",
            "精排",
            "多模态推荐",
            "负采样与对比学习"
        ],
        "label_reason": "覆盖搜索广告全链路，含召回与排序，多模态建模",
        "relevance_score": 9,
        "novelty_score": 7,
        "novelty_reason": "三阶段训练范式+交换率指标具创新性"
    },
    {
        "title": "SQuaD: The Software Quality Dataset",
        "url": "http://arxiv.org/abs/2511.11265v1",
        "pub_date": "2025-11-14",
        "summary": "Software quality research increasingly relies on large-scale datasets that measure both the product and process aspects of software systems. However, existing resources often focus on limited dimensions, such as code smells, technical debt, or refactoring activity, thereby restricting comprehensive analyses across time and quality dimensions. To address this gap, we present the Software Quality Dataset (SQuaD), a multi-dimensional, time-aware collection of software quality metrics extracted from 450 mature open-source projects across diverse ecosystems, including Apache, Mozilla, FFmpeg, and the Linux kernel. By integrating nine state-of-the-art static analysis tools, i.e., SonarQube, CodeScene, PMD, Understand, CK, JaSoMe, RefactoringMiner, RefactoringMiner++, and PyRef, our dataset unifies over 700 unique metrics at method, class, file, and project levels. Covering a total of 63,586 analyzed project releases, SQuaD also provides version control and issue-tracking histories, software vulnerability data (CVE/CWE), and process metrics proven to enhance Just-In-Time (JIT) defect prediction. The SQuaD enables empirical research on maintainability, technical debt, software evolution, and quality assessment at unprecedented scale. We also outline emerging research directions, including automated dataset updates and cross-project quality modeling to support the continuous evolution of software analytics. The dataset is publicly available on ZENODO (DOI: 10.5281/zenodo.17566690).",
        "translated": "软件质量研究日益依赖大规模数据集，用以衡量软件系统的产物与过程维度。然而，现有资源往往仅聚焦于有限维度，如代码异味、技术债或重构活动，从而限制了在时间与质量维度上的综合性分析。为弥合这一差距，我们提出软件质量数据集（SQuaD），该数据集从450个成熟开源项目中提取多维、时序感知的软件质量度量指标，涵盖Apache、Mozilla、FFmpeg及Linux内核等多样化生态系统。通过整合九种前沿静态分析工具——SonarQube、CodeScene、PMD、Understand、CK、JaSoMe、RefactoringMiner、RefactoringMiner++和PyRef——本数据集统一了700余个独特的度量指标，覆盖方法、类、文件及项目层级。针对共计63,586个已分析的项目发布版本，SQuaD还提供了版本控制与问题跟踪历史、软件漏洞数据（CVE/CWE）以及已被证实能提升即时缺陷预测（Just-In-Time, JIT）性能的过程度量指标。SQuaD支持对可维护性、技术债、软件演化与质量评估等领域开展前所未有的大规模实证研究。此外，我们还概述了新兴研究方向，包括自动化数据集更新机制与跨项目质量建模，以支撑软件分析领域的持续演进。该数据集已公开发布于ZENODO平台（DOI: 10.5281/zenodo.17566690）。",
        "translated_title": "SQuaD：软件质量数据集",
        "label": [],
        "label_reason": "论文聚焦软件质量数据集，与推荐系统无关。",
        "relevance_score": 1,
        "novelty_score": 3,
        "novelty_reason": "常规数据集构建，无推荐系统创新。"
    },
    {
        "title": "Align$^3$GR: Unified Multi-Level Alignment for LLM-based Generative Recommendation",
        "url": "http://arxiv.org/abs/2511.11255v1",
        "pub_date": "2025-11-14",
        "summary": "Large Language Models (LLMs) demonstrate significant advantages in leveraging structured world knowledge and multi-step reasoning capabilities. However, fundamental challenges arise when transforming LLMs into real-world recommender systems due to semantic and behavioral misalignment. To bridge this gap, we propose Align$^3$GR, a novel framework that unifies token-level, behavior modeling-level, and preference-level alignment. Our approach introduces: Dual tokenization fusing user-item semantic and collaborative signals. Enhanced behavior modeling with bidirectional semantic alignment. Progressive DPO strategy combining self-play (SP-DPO) and real-world feedback (RF-DPO) for dynamic preference adaptation. Experiments show Align$^3$GR outperforms the SOTA baseline by +17.8% in Recall@10 and +20.2% in NDCG@10 on the public dataset, with significant gains in online A/B tests and full-scale deployment on an industrial large-scale recommendation platform.",
        "translated": "大语言模型（LLMs）在利用结构化世界知识和多步推理能力方面展现出显著优势。然而，当将 LLMs 应用于实际推荐系统时，由于语义与行为层面的错位，仍面临根本性挑战。为弥合这一差距，我们提出 Align$^3$GR，一种统一了词元级、行为建模级与偏好级对齐的新框架。我们的方法包含：融合用户-物料语义与协同信号的双阶段词元化；通过双向语义对齐增强行为建模；结合自对弈（SP-DPO）与真实世界反馈（RF-DPO）的渐进式 DPO 策略，实现动态偏好适应。实验表明，Align$^3$GR 在公开数据集上相较当前最优基线，在 Recall@10 上提升 +17.8%，NDCG@10 上提升 +20.2%，并在在线 A/B 测试及工业级大规模推荐平台全量部署中取得显著收益。",
        "translated_title": "Align$^3$GR：基于大语言模型的生成式推荐的统一多层次对齐",
        "label": [
            "LLM生成式推荐",
            "重排",
            "精排"
        ],
        "label_reason": "直接解决LLM生成推荐的对齐问题，提升排序效果",
        "relevance_score": 10,
        "novelty_score": 9,
        "novelty_reason": "首创三层次对齐框架，结合DPO策略动态优化偏好"
    },
    {
        "title": "Enhancing Group Recommendation using Soft Impute Singular Value Decomposition",
        "url": "http://arxiv.org/abs/2511.11172v1",
        "pub_date": "2025-11-14",
        "summary": "The growing popularity of group activities increased the need to develop methods for providing recommendations to a group of users based on the collective preferences of the group members. Several group recommender systems have been proposed, but these methods often struggle due to sparsity and high-dimensionality of the available data, common in many real-world applications. In this paper, we propose a group recommender system called Group Soft-Impute SVD, which leverages soft-impute singular value decomposition to enhance group recommendations. This approach addresses the challenge of sparse high-dimensional data using low-rank matrix completion. We compared the performance of Group Soft-Impute SVD with Group MF based approaches and found that our method outperforms the baselines in recall for small user groups while achieving comparable results across all group sizes when tasked on Goodbooks, Movielens, and Synthetic datasets. Furthermore, our method recovers lower matrix ranks than the baselines, demonstrating its effectiveness in handling high-dimensional data.",
        "translated": "随着群体活动的日益流行，基于群体成员集体偏好的方法为一组用户提供推荐的需求不断增长。已有多种群体推荐系统被提出，但这些方法往往因现实应用中普遍存在数据稀疏性和高维度问题而表现不佳。在本文中，我们提出了一种名为 Group Soft-Impute SVD 的群体推荐系统，该系统利用软填充奇异值分解来提升群体推荐效果。该方法通过低秩矩阵补全技术应对稀疏高维数据的挑战。我们在 Goodbooks、Movielens 和 Synthetic 数据集上，将 Group Soft-Impute SVD 与基于 Group MF 的方法进行了对比，结果表明：在用户群体规模较小时，我们的方法在召回率上优于基线方法；而在所有群体规模下，其性能均与基线方法相当。此外，我们的方法恢复的矩阵秩低于基线方法，证明了其在处理高维数据方面的有效性。",
        "translated_title": "利用软填充奇异值分解增强群体推荐",
        "label": [
            "通用推荐技术",
            "召回"
        ],
        "label_reason": "针对群体推荐中稀疏数据问题，提出基于SVD的改进方法。",
        "relevance_score": 8,
        "novelty_score": 6,
        "novelty_reason": "利用soft-impute SVD完成矩阵补全，属常规矩阵分解优化。"
    },
    {
        "title": "GovScape: A Public Multimodal Search System for 70 Million Pages of Government PDFs",
        "url": "http://arxiv.org/abs/2511.11010v1",
        "pub_date": "2025-11-14",
        "summary": "Efforts over the past three decades have produced web archives containing billions of webpage snapshots and petabytes of data. The End of Term Web Archive alone contains, among other file types, millions of PDFs produced by the federal government. While preservation with web archives has been successful, significant challenges for access and discoverability remain. For example, current affordances for browsing the End of Term PDFs are limited to downloading and browsing individual PDFs, as well as performing basic keyword search across them. In this paper, we introduce GovScape, a public search system that supports multimodal searches across 10,015,993 federal government PDFs from the 2020 End of Term crawl (70,958,487 total PDF pages) - to our knowledge, all renderable PDFs in the 2020 crawl that are 50 pages or under. GovScape supports four primary forms of search over these 10 million PDFs: in addition to providing (1) filter conditions over metadata facets including domain and crawl date and (2) exact text search against the PDF text, we provide (3) semantic text search and (4) visual search against the PDFs across individual pages, enabling users to structure queries such as \"redacted documents\" or \"pie charts.\" We detail the constituent components of GovScape, including the search affordances, embedding pipeline, system architecture, and open source codebase. Significantly, the total estimated compute cost for GovScape's pre-processing pipeline for 10 million PDFs was approximately $1,500, equivalent to 47,000 PDF pages per dollar spent on compute, demonstrating the potential for immediate scalability. Accordingly, we outline steps that we have already begun pursuing toward multimodal search at the 100+ million PDF scale. GovScape can be found at https://www.govscape.net.",
        "translated": "过去三十年的努力已生成包含数十亿网页快照和数拍字节数据的网络档案。仅“任期结束网络档案”（End of Term Web Archive）就包含多种文件类型，其中包括由联邦政府生成的数百万份PDF文件。虽然网络档案在保存方面已取得成功，但访问与发现仍面临重大挑战。例如，当前对“任期结束”PDF文件的浏览支持仅限于下载和逐个浏览单个PDF文件，以及在这些文件中进行基础关键词搜索。\n\n本文介绍GovScape，一个支持跨10,159,93份联邦政府PDF文件（2020年任期结束爬取共70,958,487页PDF）的多模态检索系统——据我们所知，这些是2020年爬取中所有可渲染且不超过50页的PDF文档。GovScape支持针对这1000多万份PDF的四种主要检索形式：除了提供(1)基于元数据维度（如域名和爬取日期）的过滤条件，以及(2)针对PDF文本的精确文本搜索外，还提供(3)语义文本搜索与(4)跨单页PDF的视觉搜索功能，使用户能够构造如“被遮蔽文件”或“饼状图”等复杂查询。\n\n我们详细介绍了GovScape的核心组成部分，包括检索功能、嵌入处理流水线、系统架构及开源代码库。重要的是，GovScape对1000万份PDF进行预处理所需的整体估算计算成本约为1500美元，相当于每美元计算成本可处理约47,000页PDF，展现了其具备即时扩展潜力。为此，我们已开始探索面向1亿以上PDF规模的多模态检索方案。GovScape可访问地址为 https://www.govscape.net。",
        "translated_title": "GovScape：面向7000万页政府PDF文档的公共多模态搜索系统",
        "label": [],
        "label_reason": "论文聚焦政府文档搜索，非推荐系统核心问题",
        "relevance_score": 3,
        "novelty_score": 5,
        "novelty_reason": "多模态搜索技术改进，但无推荐系统创新"
    },
    {
        "title": "LEMUR: Large scale End-to-end MUltimodal Recommendation",
        "url": "http://arxiv.org/abs/2511.10962v1",
        "pub_date": "2025-11-14",
        "summary": "Traditional ID-based recommender systems often struggle with cold-start and generalization challenges. Multimodal recommendation systems, which leverage textual and visual data, offer a promising solution to mitigate these issues. However, existing industrial approaches typically adopt a two-stage training paradigm: first pretraining a multimodal model, then applying its frozen representations to train the recommendation model. This decoupled framework suffers from misalignment between multimodal learning and recommendation objectives, as well as an inability to adapt dynamically to new data. To address these limitations, we propose LEMUR, the first large-scale multimodal recommender system trained end-to-end from raw data. By jointly optimizing both the multimodal and recommendation components, LEMUR ensures tighter alignment with downstream objectives while enabling real-time parameter updates. Constructing multimodal sequential representations from user history often entails prohibitively high computational costs. To alleviate this bottleneck, we propose a novel memory bank mechanism that incrementally accumulates historical multimodal representations throughout the training process. After one month of deployment in Douyin Search, LEMUR has led to a 0.843% reduction in query change rate decay and a 0.81% improvement in QAUC. Additionally, LEMUR has shown significant gains across key offline metrics for Douyin Advertisement. Our results validate the superiority of end-to-end multimodal recommendation in real-world industrial scenarios.",
        "translated": "传统基于ID的推荐系统往往难以应对冷启动和泛化问题。多模态推荐系统通过利用文本与视觉数据，为缓解这些问题提供了有前景的解决方案。然而，现有工业方法通常采用两阶段训练范式：首先预训练一个多模态模型，再将其冻结后的表示用于训练推荐模型。这种解耦框架存在多模态学习与推荐目标不一致的问题，并且无法动态适应新数据。为解决这些局限性，我们提出LEMUR，这是首个从原始数据端到端训练的大规模多模态推荐系统。通过联合优化多模态与推荐组件，LEMUR确保了更紧密的目标对齐，并支持实时参数更新。构建用户历史的多模态序列表示常面临高昂的计算开销。为缓解这一瓶颈，我们提出一种新颖的记忆库机制，在训练过程中逐步累积历史多模态表示。在抖音搜索上线一个月后，LEMUR实现了查询变更率衰减降低0.843%、QAUC提升0.81%的效果。此外，LEMUR在抖音广告的关键离线指标上也取得了显著改进。实验结果验证了端到端多模态推荐在真实工业场景中的优越性。",
        "translated_title": "LEMUR：大规模端到端多模态推荐",
        "label": [
            "多模态推荐",
            "精排"
        ],
        "label_reason": "提出端到端多模态推荐框架，直接解决推荐核心问题",
        "relevance_score": 10,
        "novelty_score": 9,
        "novelty_reason": "首次端到端训练，联合优化多模态与推荐目标"
    },
    {
        "title": "LARM: A Large Articulated-Object Reconstruction Model",
        "url": "http://arxiv.org/abs/2511.11563v1",
        "pub_date": "2025-11-14",
        "summary": "Modeling 3D articulated objects with realistic geometry, textures, and kinematics is essential for a wide range of applications. However, existing optimization-based reconstruction methods often require dense multi-view inputs and expensive per-instance optimization, limiting their scalability. Recent feedforward approaches offer faster alternatives but frequently produce coarse geometry, lack texture reconstruction, and rely on brittle, complex multi-stage pipelines. We introduce LARM, a unified feedforward framework that reconstructs 3D articulated objects from sparse-view images by jointly recovering detailed geometry, realistic textures, and accurate joint structures. LARM extends LVSM a recent novel view synthesis (NVS) approach for static 3D objects into the articulated setting by jointly reasoning over camera pose and articulation variation using a transformer-based architecture, enabling scalable and accurate novel view synthesis. In addition, LARM generates auxiliary outputs such as depth maps and part masks to facilitate explicit 3D mesh extraction and joint estimation. Our pipeline eliminates the need for dense supervision and supports high-fidelity reconstruction across diverse object categories. Extensive experiments demonstrate that LARM outperforms state-of-the-art methods in both novel view and state synthesis as well as 3D articulated object reconstruction, generating high-quality meshes that closely adhere to the input images. project page: https://sylviayuan-sy.github.io/larm-site/",
        "translated": "建模具有真实几何结构、纹理和运动学特性的三维可变形物体，对众多应用至关重要。然而，现有的基于优化的重建方法通常需要密集多视角输入，并且需针对每个实例进行昂贵的优化过程，限制了其可扩展性。近期的前馈式方法虽能提供更快速的替代方案，但往往生成粗略的几何结构、缺乏纹理重建能力，并依赖脆弱而复杂的多阶段流程。我们提出LARM，这是一种统一的前馈框架，能够从稀疏视角图像中重建三维可变形物体，同时联合恢复精细几何结构、逼真纹理与准确的关节布局。LARM将近期为静态三维物体设计的新型视图合成（NVS）方法LVSM拓展至可变形场景，通过基于Transformer的架构联合推理相机位姿与运动变化，实现了可扩展且精确的新视图合成。此外，LARM还输出辅助结果，如深度图与部件掩码，以支持明确的三维网格提取及关节估计。我们的流程无需密集监督信号，并支持在各类物体类别上实现高保真重建。大量实验表明，LARM在新视图合成、状态合成以及三维可变形物体重建任务中均优于当前最先进方法，所生成的高质量网格紧密贴合输入图像。项目主页：https://sylviayuan-sy.github.io/larm-site/",
        "translated_title": "LARM：一种大型可变形物体重建模型",
        "label": [],
        "label_reason": "3D重建属高阶视觉任务，非像素级图像恢复。",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "首次将Transformer用于稀疏视图下关节物体重建。"
    },
    {
        "title": "DocLens : A Tool-Augmented Multi-Agent Framework for Long Visual Document Understanding",
        "url": "http://arxiv.org/abs/2511.11552v1",
        "pub_date": "2025-11-14",
        "summary": "Comprehending long visual documents, where information is distributed across extensive pages of text and visual elements, is a critical but challenging task for modern Vision-Language Models (VLMs). Existing approaches falter on a fundamental challenge: evidence localization. They struggle to retrieve relevant pages and overlook fine-grained details within visual elements, leading to limited performance and model hallucination. To address this, we propose DocLens, a tool-augmented multi-agent framework that effectively ``zooms in'' on evidence like a lens. It first navigates from the full document to specific visual elements on relevant pages, then employs a sampling-adjudication mechanism to generate a single, reliable answer. Paired with Gemini-2.5-Pro, DocLens achieves state-of-the-art performance on MMLongBench-Doc and FinRAGBench-V, surpassing even human experts. The framework's superiority is particularly evident on vision-centric and unanswerable queries, demonstrating the power of its enhanced localization capabilities.",
        "translated": "理解长视觉文档——其中信息分布于大量文本与视觉元素的页面上——是现代视觉-语言模型（VLMs）的一项关键但极具挑战性的任务。现有方法在一项根本性难题上表现不足：证据定位。它们难以检索到相关页面，且忽略视觉元素中的细粒度细节，导致性能受限和模型幻觉。为解决这一问题，我们提出 DocLens，一种工具增强型多智能体框架，能够如同“镜头”一般“放大”证据。该框架首先从完整文档导航至相关页面上的特定视觉元素，随后通过采样-裁决机制生成一个可靠单一答案。结合 Gemini-2.5-Pro，DocLens 在 MMLongBench-Doc 和 FinRAGBench-V 上达到当前最优性能，甚至超越人类专家水平。该框架的优势在以视觉为中心及无答案查询中尤为明显，充分展现了其增强定位能力的强大作用。",
        "translated_title": "DocLens：一种增强工具的多智能体框架，用于长视觉文档理解",
        "label": [],
        "label_reason": "任务为文档理解，属高阶视觉语言模型应用。",
        "relevance_score": 1,
        "novelty_score": 1,
        "novelty_reason": "框架创新在多智能体协作，非像素级图像处理。"
    },
    {
        "title": "EmoVerse: A MLLMs-Driven Emotion Representation Dataset for Interpretable Visual Emotion Analysis",
        "url": "http://arxiv.org/abs/2511.12554v1",
        "pub_date": "2025-11-16",
        "summary": "Visual Emotion Analysis (VEA) aims to bridge the affective gap between visual content and human emotional responses. Despite its promise, progress in this field remains limited by the lack of open-source and interpretable datasets. Most existing studies assign a single discrete emotion label to an entire image, offering limited insight into how visual elements contribute to emotion. In this work, we introduce EmoVerse, a large-scale open-source dataset that enables interpretable visual emotion analysis through multi-layered, knowledge-graph-inspired annotations. By decomposing emotions into Background-Attribute-Subject (B-A-S) triplets and grounding each element to visual regions, EmoVerse provides word-level and subject-level emotional reasoning. With over 219k images, the dataset further includes dual annotations in Categorical Emotion States (CES) and Dimensional Emotion Space (DES), facilitating unified discrete and continuous emotion representation. A novel multi-stage pipeline ensures high annotation reliability with minimal human effort. Finally, we introduce an interpretable model that maps visual cues into DES representations and provides detailed attribution explanations. Together, the dataset, pipeline, and model form a comprehensive foundation for advancing explainable high-level emotion understanding.",
        "translated": "视觉情感分析（Visual Emotion Analysis, VEA）旨在弥合视觉内容与人类情绪反应之间的感知鸿沟。尽管该领域前景广阔，但其进展受限于缺乏公开且可解释的数据集。现有大多数研究仅对整张图像分配单一离散情绪标签，难以深入揭示视觉元素如何影响情绪生成。本文中，我们提出了EmoVerse——一个大规模开源数据集，通过多层级、受知识图谱启发的标注方式，实现可解释的视觉情感分析。该数据集将情绪分解为背景-属性-主体（Background-Attribute-Subject, B-A-S）三元组，并将每个元素锚定至对应的视觉区域，从而提供词级和主体级的情绪推理能力。该数据集包含超过21.9万张图像，并进一步提供两类双标注：类别情绪状态（Categorical Emotion States, CES）与维度情绪空间（Dimensional Emotion Space, DES），以支持离散与连续情绪表示的统一建模。一种新颖的多阶段标注流程在最小化人工投入的前提下确保了高标注可靠性。最后，我们引入了一个可解释模型，该模型能够将视觉线索映射到DES表示并提供详细的归因解释。综上，本数据集、标注流程与模型共同构建了一个全面的基础框架，推动可解释高层情绪理解的发展。",
        "translated_title": "EmoVerse：一种由多模态大语言模型驱动的情绪表示数据集，用于可解释的视觉情绪分析",
        "label": [],
        "label_reason": "属于高阶情感分析任务，非图像像素级处理",
        "relevance_score": 1,
        "novelty_score": 6,
        "novelty_reason": "构建新数据集与可解释模型，非图像恢复创新"
    },
    {
        "title": "HiGFA: Hierarchical Guidance for Fine-grained Data Augmentation with Diffusion Models",
        "url": "http://arxiv.org/abs/2511.12547v1",
        "pub_date": "2025-11-16",
        "summary": "Generative diffusion models show promise for data augmentation. However, applying them to fine-grained tasks presents a significant challenge: ensuring synthetic images accurately capture the subtle, category-defining features critical for high fidelity. Standard approaches, such as text-based Classifier-Free Guidance (CFG), often lack the required specificity, potentially generating misleading examples that degrade fine-grained classifier performance. To address this, we propose Hierarchically Guided Fine-grained Augmentation (HiGFA). HiGFA leverages the temporal dynamics of the diffusion sampling process. It employs strong text and transformed contour guidance with fixed strengths in the early-to-mid sampling stages to establish overall scene, style, and structure. In the final sampling stages, HiGFA activates a specialized fine-grained classifier guidance and dynamically modulates the strength of all guidance signals based on prediction confidence. This hierarchical, confidence-driven orchestration enables HiGFA to generate diverse yet faithful synthetic images by intelligently balancing global structure formation with precise detail refinement. Experiments on several FGVC datasets demonstrate the effectiveness of HiGFA.",
        "translated": "生成式扩散模型在数据增强方面展现出良好前景。然而，将其应用于细粒度任务时面临显著挑战：需确保合成图像能准确捕捉对高保真度至关重要的、定义类别的细微特征。标准方法（如基于文本的无分类器引导，Classifier-Free Guidance, CFG）往往缺乏所需的具体性，可能生成误导性示例，从而降低细粒度分类器的性能。为解决这一问题，我们提出层次化引导细粒度增强（Hierarchically Guided Fine-grained Augmentation, HiGFA）。HiGFA 利用扩散采样过程中的时序动态特性，在采样前期至中期阶段采用强文本与变换轮廓引导，并保持固定强度，以确立整体场景、风格与结构；而在最终采样阶段，HiGFA 激活专用细粒度分类器引导，并依据预测置信度动态调节所有引导信号的强度。这种分层且基于置信度驱动的协同机制使 HiGFA 能够智能平衡全局结构构建与细节精准优化，生成多样而忠实的合成图像。在多个 FGVC 数据集上的实验验证了 HiGFA 的有效性。",
        "translated_title": "HiGFA：基于扩散模型的分层引导细粒度数据增强",
        "label": [],
        "label_reason": "生成式扩散模型用于数据增强，非图像像素级恢复。",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出分层引导机制提升细粒度合成质量。"
    },
    {
        "title": "ReaSon: Reinforced Causal Search with Information Bottleneck for Video Understanding",
        "url": "http://arxiv.org/abs/2511.12530v1",
        "pub_date": "2025-11-16",
        "summary": "Keyframe selection has become essential for video understanding with vision-language models (VLMs) due to limited input tokens and the temporal sparsity of relevant information across video frames. Video understanding often relies on effective keyframes that are not only informative but also causally decisive. To this end, we propose Reinforced Causal Search with Information Bottleneck (ReaSon), a framework that formulates keyframe selection as an optimization problem with the help of a novel Causal Information Bottleneck (CIB), which explicitly defines keyframes as those satisfying both predictive sufficiency and causal necessity. Specifically, ReaSon employs a learnable policy network to select keyframes from a visually relevant pool of candidate frames to capture predictive sufficiency, and then assesses causal necessity via counterfactual interventions. Finally, a composite reward aligned with the CIB principle is designed to guide the selection policy through reinforcement learning. Extensive experiments on NExT-QA, EgoSchema, and Video-MME demonstrate that ReaSon consistently outperforms existing state-of-the-art methods under limited-frame settings, validating its effectiveness and generalization ability.",
        "translated": "关键帧选择因视觉-语言模型（VLMs）输入token有限，且视频帧间相关信息在时间维度上稀疏，已成为视频理解中的核心环节。视频理解通常依赖于既具信息量又具备因果决定性的有效关键帧。为此，我们提出了一种名为Reinforced Causal Search with Information Bottleneck（ReaSon）的框架，该框架借助一种新颖的因果信息瓶颈（Causal Information Bottleneck, CIB），将关键帧选择形式化为一个优化问题，并显式地将关键帧定义为同时满足预测充分性和因果必要性条件的帧。具体而言，ReaSon利用可学习策略网络从视觉相关的候选帧池中选择关键帧以捕捉预测充分性；随后通过反事实干预评估因果必要性。最终，设计了一种与CIB原则对齐的复合奖励函数，以引导策略网络通过强化学习完成关键帧选择。在NExT-QA、EgoSchema和Video-MME等数据集上的大量实验表明，ReaSon在有限帧数设置下始终优于现有最先进方法，验证了其有效性与泛化能力。",
        "translated_title": "ReaSon：基于信息瓶颈的强化因果搜索用于视频理解",
        "label": [],
        "label_reason": "视频理解属高阶视觉任务，非像素级图像处理",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "提出因果搜索新框架，显著提升关键帧选择效率"
    },
    {
        "title": "D$^{2}$-VPR: A Parameter-efficient Visual-foundation-model-based Visual Place Recognition Method via Knowledge Distillation and Deformable Aggregation",
        "url": "http://arxiv.org/abs/2511.12528v1",
        "pub_date": "2025-11-16",
        "summary": "Visual Place Recognition (VPR) aims to determine the geographic location of a query image by retrieving its most visually similar counterpart from a geo-tagged reference database. Recently, the emergence of the powerful visual foundation model, DINOv2, trained in a self-supervised manner on massive datasets, has significantly improved VPR performance. This improvement stems from DINOv2's exceptional feature generalization capabilities but is often accompanied by increased model complexity and computational overhead that impede deployment on resource-constrained devices. To address this challenge, we propose $D^{2}$-VPR, a $D$istillation- and $D$eformable-based framework that retains the strong feature extraction capabilities of visual foundation models while significantly reducing model parameters and achieving a more favorable performance-efficiency trade-off. Specifically, first, we employ a two-stage training strategy that integrates knowledge distillation and fine-tuning. Additionally, we introduce a Distillation Recovery Module (DRM) to better align the feature spaces between the teacher and student models, thereby minimizing knowledge transfer losses to the greatest extent possible. Second, we design a Top-Down-attention-based Deformable Aggregator (TDDA) that leverages global semantic features to dynamically and adaptively adjust the Regions of Interest (ROI) used for aggregation, thereby improving adaptability to irregular structures. Extensive experiments demonstrate that our method achieves competitive performance compared to state-of-the-art approaches. Meanwhile, it reduces the parameter count by approximately 64.2% and FLOPs by about 62.6% (compared to CricaVPR).Code is available at https://github.com/tony19980810/D2VPR.",
        "translated": "视觉位置识别（Visual Place Recognition, VPR）旨在通过从带有地理标签的参考数据库中检索与查询图像视觉上最相似的图像，确定查询图像的地理位置。近期，强大的视觉基础模型 DINOv2 的出现显著提升了 VPR 性能，该模型在大规模数据集上以自监督方式训练而成。这一性能提升源于 DINOv2 极其出色的特征泛化能力，但往往伴随着模型复杂度增加和计算开销增大，从而阻碍其在资源受限设备上的部署。为解决该问题，我们提出 $D^{2}$-VPR，一种基于知识蒸馏与可变形机制的框架，在保留视觉基础模型强大特征提取能力的同时显著减少模型参数量，并实现更优的性能-效率权衡。具体而言，首先，我们采用两阶段训练策略，结合知识蒸馏与微调；此外，我们引入了蒸馏恢复模块（Distillation Recovery Module, DRM），以更好地对齐教师模型与学生模型的特征空间，最大程度地最小化知识迁移损失。其次，我们设计了一种基于自上而下注意力机制的可变形聚合器（Top-Down-attention-based Deformable Aggregator, TDDA），利用全局语义特征动态调整用于聚合的感兴趣区域（Regions of Interest, ROI），从而增强对不规则结构的适应性。大量实验表明，我们的方法在性能上媲美当前最优方法，同时相比 CricaVPR，参数量减少约 64.2%，FLOPs 减少约 62.6%。代码开源于 https://github.com/tony19980810/D2VPR。",
        "translated_title": "D²-VPR：一种基于视觉基础模型、通过知识蒸馏与可变形聚合的参数高效视觉定位方法",
        "label": [],
        "label_reason": "属于高阶视觉任务，非像素级图像处理",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出轻量化框架并改进特征对齐机制"
    },
    {
        "title": "MdaIF: Robust One-Stop Multi-Degradation-Aware Image Fusion with Language-Driven Semantics",
        "url": "http://arxiv.org/abs/2511.12525v1",
        "pub_date": "2025-11-16",
        "summary": "Infrared and visible image fusion aims to integrate complementary multi-modal information into a single fused result. However, existing methods 1) fail to account for the degradation visible images under adverse weather conditions, thereby compromising fusion performance; and 2) rely on fixed network architectures, limiting their adaptability to diverse degradation scenarios. To address these issues, we propose a one-stop degradation-aware image fusion framework for multi-degradation scenarios driven by a large language model (MdaIF). Given the distinct scattering characteristics of different degradation scenarios (e.g., haze, rain, and snow) in atmospheric transmission, a mixture-of-experts (MoE) system is introduced to tackle image fusion across multiple degradation scenarios. To adaptively extract diverse weather-aware degradation knowledge and scene feature representations, collectively referred to as the semantic prior, we employ a pre-trained vision-language model (VLM) in our framework. Guided by the semantic prior, we propose degradation-aware channel attention module (DCAM), which employ degradation prototype decomposition to facilitate multi-modal feature interaction in channel domain. In addition, to achieve effective expert routing, the semantic prior and channel-domain modulated features are utilized to guide the MoE, enabling robust image fusion in complex degradation scenarios. Extensive experiments validate the effectiveness of our MdaIF, demonstrating superior performance over SOTA methods.",
        "translated": "红外与可见光图像融合旨在将互补的多模态信息整合为单一融合结果。然而，现有方法存在两点不足：1）未能考虑恶劣天气条件下可见光图像所遭受的退化，从而影响融合性能；2）依赖固定网络架构，难以适应多样化的退化场景。为解决上述问题，我们提出了一种基于大语言模型的端到端退化感知图像融合框架（MdaIF），适用于多种退化场景。鉴于不同退化场景（如雾霾、雨雾、积雪）在大气传输中具有显著不同的散射特性，我们引入了混合专家（MoE）系统以应对跨多种退化场景的图像融合任务。为自适应提取多样化的天气感知退化知识与场景特征表示（统称为语义先验），我们在框架中引入预训练视觉-语言模型（VLM）。在语义先验引导下，我们提出退化感知通道注意力模块（DCAM），通过退化原型分解机制促进多模态特征在通道域中的交互。此外，为实现有效的专家路由，利用语义先验与通道域调制特征指导MoE，从而在复杂退化场景中实现鲁棒的图像融合效果。大量实验验证了我们提出的MdaIF框架的有效性，在多个指标上均优于当前最先进方法。",
        "translated_title": "MdaIF：鲁棒的多退化感知一站式图像融合方法，结合语言驱动语义",
        "label": [],
        "label_reason": "处理多模态融合，非像素级图像恢复任务",
        "relevance_score": 4,
        "novelty_score": 7,
        "novelty_reason": "引入语言模型与MoE架构提升适应性"
    },
    {
        "title": "DINO-Detect: A Simple yet Effective Framework for Blur-Robust AI-Generated Image Detection",
        "url": "http://arxiv.org/abs/2511.12511v1",
        "pub_date": "2025-11-16",
        "summary": "With growing concerns over image authenticity and digital safety, the field of AI-generated image (AIGI) detection has progressed rapidly. Yet, most AIGI detectors still struggle under real-world degradations, particularly motion blur, which frequently occurs in handheld photography, fast motion, and compressed video. Such blur distorts fine textures and suppresses high-frequency artifacts, causing severe performance drops in real-world settings. We address this limitation with a blur-robust AIGI detection framework based on teacher-student knowledge distillation. A high-capacity teacher (DINOv3), trained on clean (i.e., sharp) images, provides stable and semantically rich representations that serve as a reference for learning. By freezing the teacher to maintain its generalization ability, we distill its feature and logit responses from sharp images to a student trained on blurred counterparts, enabling the student to produce consistent representations under motion degradation. Extensive experiments benchmarks show that our method achieves state-of-the-art performance under both motion-blurred and clean conditions, demonstrating improved generalization and real-world applicability. Source codes will be released at: https://github.com/JiaLiangShen/Dino-Detect-for-blur-robust-AIGC-Detection.",
        "translated": "随着对图像真实性与数字安全的关注日益增长，人工智能生成图像（AIGI）检测领域已迅速发展。然而，大多数AIGI检测器在真实世界退化条件下仍表现不佳，尤其是运动模糊——这种现象常见于手持拍摄、快速运动及压缩视频中。此类模糊会破坏细纹理并抑制高频伪影，导致检测器在实际场景中的性能大幅下降。为解决这一局限，我们提出一种基于教师-学生知识蒸馏的抗模糊AIGI检测框架。高容量教师模型（DINOv3）在清晰图像（即锐利图像）上训练，可提供稳定且语义丰富的特征表示，作为学习参考。通过冻结教师模型以维持其泛化能力，我们将教师从锐利图像中提取的特征与logit响应蒸馏至一个在模糊图像上训练的学生模型，从而使学生能够在运动模糊退化下输出一致的表征。大量实验基准测试表明，我们的方法在运动模糊和无模糊条件下均达到当前最优性能，展现出更强的泛化能力和实际适用性。源代码将发布于：https://github.com/JiaLiangShen/Dino-Detect-for-blur-robust-AIGC-Detection。",
        "translated_title": "DINO-Detect：一种简单而有效的抗模糊AI生成图像检测框架",
        "label": [],
        "label_reason": "检测任务属高阶视觉，非像素级图像恢复",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "知识蒸馏框架有效提升抗模糊检测性能"
    },
    {
        "title": "Visible Structure Retrieval for Lightweight Image-Based Relocalisation",
        "url": "http://arxiv.org/abs/2511.12503v1",
        "pub_date": "2025-11-16",
        "summary": "Accurate camera pose estimation from an image observation in a previously mapped environment is commonly done through structure-based methods: by finding correspondences between 2D keypoints on the image and 3D structure points in the map. In order to make this correspondence search tractable in large scenes, existing pipelines either rely on search heuristics, or perform image retrieval to reduce the search space by comparing the current image to a database of past observations. However, these approaches result in elaborate pipelines or storage requirements that grow with the number of past observations. In this work, we propose a new paradigm for making structure-based relocalisation tractable. Instead of relying on image retrieval or search heuristics, we learn a direct mapping from image observations to the visible scene structure in a compact neural network. Given a query image, a forward pass through our novel visible structure retrieval network allows obtaining the subset of 3D structure points in the map that the image views, thus reducing the search space of 2D-3D correspondences. We show that our proposed method enables performing localisation with an accuracy comparable to the state of the art, while requiring lower computational and storage footprint.",
        "translated": "从先前已构建地图环境中的图像观测中精确估计相机位姿，通常通过基于结构的方法实现：即在图像中的二维关键点与地图中的三维结构点之间寻找对应关系。为了使大规模场景下的该对应搜索变得可行，现有流程要么依赖于搜索启发式方法，要么通过图像检索来缩小搜索空间——即通过将当前图像与过去观测的历史数据库进行比对。然而，这些方法会导致复杂的处理流程，或存储需求随历史观测数量增长而扩大。在本文中，我们提出了一种新的范式，以使基于结构的重定位变得可行。我们不再依赖图像检索或搜索启发式方法，而是学习一个从图像观测直接映射到可见场景结构的紧凑神经网络。给定一个查询图像，通过对我们的新型可见结构检索网络进行前向传播，即可获得地图中该图像所观察到的三维结构点子集，从而有效缩小二维-三维对应关系的搜索空间。我们证明，所提出的方法能够在计算开销和存储占用显著降低的前提下，实现与当前最先进方法相当的定位精度。",
        "translated_title": "轻量级基于图像的重定位中的可见结构检索",
        "label": [],
        "label_reason": "目标为相机位姿估计，属高阶视觉任务",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出直接映射网络，提升效率与精度"
    },
    {
        "title": "BSO: Binary Spiking Online Optimization Algorithm",
        "url": "http://arxiv.org/abs/2511.12502v1",
        "pub_date": "2025-11-16",
        "summary": "Binary Spiking Neural Networks (BSNNs) offer promising efficiency advantages for resource-constrained computing. However, their training algorithms often require substantial memory overhead due to latent weights storage and temporal processing requirements. To address this issue, we propose Binary Spiking Online (BSO) optimization algorithm, a novel online training algorithm that significantly reduces training memory. BSO directly updates weights through flip signals under the online training framework. These signals are triggered when the product of gradient momentum and weights exceeds a threshold, eliminating the need for latent weights during training. To enhance performance, we propose T-BSO, a temporal-aware variant that leverages the inherent temporal dynamics of BSNNs by capturing gradient information across time steps for adaptive threshold adjustment. Theoretical analysis establishes convergence guarantees for both BSO and T-BSO, with formal regret bounds characterizing their convergence rates. Extensive experiments demonstrate that both BSO and T-BSO achieve superior optimization performance compared to existing training methods for BSNNs. The codes are available at https://github.com/hamings1/BSO.",
        "translated": "二进制脉冲神经网络（BSNNs）在资源受限的计算环境中展现出显著的效率优势。然而，其训练算法通常因隐式权重存储和时序处理需求而产生较大的内存开销。为解决该问题，我们提出了一种名为“二进制脉冲在线优化”（Binary Spiking Online, BSO）的新型在线训练算法，该算法可大幅降低训练所需的内存。BSO 在在线训练框架下，直接通过翻转信号更新权重；当梯度动量与权重的乘积超过设定阈值时，触发这些信号，从而在训练过程中无需存储隐式权重。为进一步提升性能，我们提出了具有时间感知能力的变体 T-BSO，该方法利用 BSNNs 内在的时间动态特性，通过跨时间步长捕获梯度信息以实现自适应阈值调整。理论分析证明了 BSO 与 T-BSO 的收敛性保证，并通过形式化的遗憾界刻画了其收敛速率。大量实验表明，BSO 与 T-BSO 均优于现有 BSNN 训练方法，在优化性能上表现更优。相关代码已开源于 https://github.com/hamings1/BSO。",
        "translated_title": "BSO：二进制脉冲在线优化算法",
        "label": [],
        "label_reason": "研究BSNN训练算法，非图像处理任务。",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "提出在线优化新框架，提升训练效率。"
    },
    {
        "title": "Towards Temporal Fusion Beyond the Field of View for Camera-based Semantic Scene Completion",
        "url": "http://arxiv.org/abs/2511.12498v1",
        "pub_date": "2025-11-16",
        "summary": "Recent camera-based 3D semantic scene completion (SSC) methods have increasingly explored leveraging temporal cues to enrich the features of the current frame. However, while these approaches primarily focus on enhancing in-frame regions, they often struggle to reconstruct critical out-of-frame areas near the sides of the ego-vehicle, although previous frames commonly contain valuable contextual information about these unseen regions. To address this limitation, we propose the Current-Centric Contextual 3D Fusion (C3DFusion) module, which generates hidden region-aware 3D feature geometry by explicitly aligning 3D-lifted point features from both current and historical frames. C3DFusion performs enhanced temporal fusion through two complementary techniques-historical context blurring and current-centric feature densification-which suppress noise from inaccurately warped historical point features by attenuating their scale, and enhance current point features by increasing their volumetric contribution. Simply integrated into standard SSC architectures, C3DFusion demonstrates strong effectiveness, significantly outperforming state-of-the-art methods on the SemanticKITTI and SSCBench-KITTI-360 datasets. Furthermore, it exhibits robust generalization, achieving notable performance gains when applied to other baseline models.",
        "translated": "近年来，基于相机的3D语义场景补全（SSC）方法日益探索利用时序线索以丰富当前帧的特征。然而，尽管这些方法主要聚焦于增强帧内区域，却往往难以重建位于自车两侧的关键帧外区域，而先前帧通常包含关于这些未见区域的重要上下文信息。为解决这一局限性，我们提出当前中心上下文3D融合（C3DFusion）模块，该模块通过显式对齐当前帧与历史帧的3D升维点特征，生成隐含区域感知的3D特征几何结构。C3DFusion 通过两种互补技术实现增强的时序融合——历史上下文模糊化与当前中心特征稠密化：前者通过降低尺度来抑制因不准确位移导致的历史点特征噪声，后者则通过提升体积贡献度增强当前点特征。当简单集成至标准SSC架构中时，C3DFusion展现出显著有效性，在SemanticKITTI和SSCBench-KITTI-360数据集上大幅超越现有最先进方法。此外，其具备强泛化能力，在应用于其他基线模型时亦可取得显著性能增益。",
        "translated_title": "面向超视场的基于相机语义场景补全的时间融合方法",
        "label": [],
        "label_reason": "任务为3D场景补全，属高阶语义理解",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出新融合模块提升历史帧利用效率"
    },
    {
        "title": "MaskAnyNet: Rethinking Masked Image Regions as Valuable Information in Supervised Learning",
        "url": "http://arxiv.org/abs/2511.12480v1",
        "pub_date": "2025-11-16",
        "summary": "In supervised learning, traditional image masking faces two key issues: (i) discarded pixels are underutilized, leading to a loss of valuable contextual information; (ii) masking may remove small or critical features, especially in fine-grained tasks. In contrast, masked image modeling (MIM) has demonstrated that masked regions can be reconstructed from partial input, revealing that even incomplete data can exhibit strong contextual consistency with the original image. This highlights the potential of masked regions as sources of semantic diversity. Motivated by this, we revisit the image masking approach, proposing to treat masked content as auxiliary knowledge rather than ignored. Based on this, we propose MaskAnyNet, which combines masking with a relearning mechanism to exploit both visible and masked information. It can be easily extended to any model with an additional branch to jointly learn from the recomposed masked region. This approach leverages the semantic diversity of the masked regions to enrich features and preserve fine-grained details. Experiments on CNN and Transformer backbones show consistent gains across multiple benchmarks. Further analysis confirms that the proposed method improves semantic diversity through the reuse of masked content.",
        "translated": "在监督学习中，传统图像掩码面临两个关键问题：(i) 被丢弃的像素未被充分利用，导致有价值的空间上下文信息丢失；(ii) 掩码可能移除小尺度或关键性特征，尤其是在细粒度任务中。相比之下，掩码图像建模（MIM）表明，从部分输入即可重建被掩码区域，揭示即使不完整数据也能与原始图像保持强语义一致性。这凸显了掩码区域作为语义多样性来源的潜力。受此启发，我们重新审视图像掩码方法，提出将掩码内容视为辅助知识而非被忽略的信息。在此基础上，我们提出了 MaskAnyNet，该模型结合掩码机制与重学习机制，以同时利用可见和掩码区域的信息。该方法可轻松扩展至任意具备额外分支的模型，用于联合学习重组后的掩码区域。本方法通过利用掩码区域的语义多样性，增强特征表达并保留细粒度细节。在 CNN 和 Transformer 主干网络上的实验表明，该方法在多个基准测试中均取得一致性能提升。进一步分析证实，所提方法通过重复利用掩码内容提升了语义多样性。",
        "translated_title": "MaskAnyNet：重新思考监督学习中被掩码图像区域作为有价值的信息",
        "label": [],
        "label_reason": "聚焦监督学习中掩码策略改进，非图像像素级恢复任务。",
        "relevance_score": 3,
        "novelty_score": 6,
        "novelty_reason": "新框架提升语义多样性，但未解决低层图像处理核心问题。"
    },
    {
        "title": "Co-Layout: LLM-driven Co-optimization for Interior Layout",
        "url": "http://arxiv.org/abs/2511.12474v1",
        "pub_date": "2025-11-16",
        "summary": "We present a novel framework for automated interior design that combines large language models (LLMs) with grid-based integer programming to jointly optimize room layout and furniture placement. Given a textual prompt, the LLM-driven agent workflow extracts structured design constraints related to room configurations and furniture arrangements. These constraints are encoded into a unified grid-based representation inspired by ``Modulor\". Our formulation accounts for key design requirements, including corridor connectivity, room accessibility, spatial exclusivity, and user-specified preferences. To improve computational efficiency, we adopt a coarse-to-fine optimization strategy that begins with a low-resolution grid to solve a simplified problem and guides the solution at the full resolution. Experimental results across diverse scenarios demonstrate that our joint optimization approach significantly outperforms existing two-stage design pipelines in solution quality, and achieves notable computational efficiency through the coarse-to-fine strategy.",
        "translated": "我们提出了一种新颖的自动化室内设计框架，该框架结合了大语言模型（LLMs）与基于网格的整数规划，以联合优化房间布局与家具摆放。给定一个文本提示，由 LLM 驱动的代理工作流提取与房间配置及家具布置相关的结构化设计约束。这些约束被编码为受“Modulor”启发的统一网格表示形式。我们的建模方案考虑了若干关键设计要求，包括走廊连通性、房间可达性、空间互斥性以及用户指定的偏好。为提升计算效率，我们采用由粗到细的优化策略：首先在低分辨率网格上求解简化问题，再逐步引导至全分辨率下的最终解。实验结果表明，在多种场景下，我们的联合优化方法在解的质量上显著优于现有的两阶段设计流程，并通过由粗到细的策略实现了显著的计算效率提升。",
        "translated_title": "Co-Layout：基于大语言模型的室内布局协同优化",
        "label": [],
        "label_reason": "属于高阶设计生成任务，非像素级图像处理。",
        "relevance_score": 1,
        "novelty_score": 3,
        "novelty_reason": "方法为布局优化，无图像恢复或增强创新。"
    },
    {
        "title": "DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions",
        "url": "http://arxiv.org/abs/2511.12452v1",
        "pub_date": "2025-11-16",
        "summary": "With the rapid adoption of multimodal large language models (MLLMs) across diverse applications, there is a pressing need for task-centered, high-quality training data. A key limitation of current training datasets is their reliance on sparse annotations mined from the Internet or entered via manual typing that capture only a fraction of an image's visual content. Dense annotations are more valuable but remain scarce. Traditional text-based annotation pipelines are poorly suited for creating dense annotations: typing limits expressiveness, slows annotation speed, and underrepresents nuanced visual features, especially in specialized areas such as multicultural imagery and 3D asset annotation. In this paper, we present DenseAnnotate, an audio-driven online annotation platform that enables efficient creation of dense, fine-grained annotations for images and 3D assets. Annotators narrate observations aloud while synchronously linking spoken phrases to image regions or 3D scene parts. Our platform incorporates speech-to-text transcription and region-of-attention marking. To demonstrate the effectiveness of DenseAnnotate, we conducted case studies involving over 1,000 annotators across two domains: culturally diverse images and 3D scenes. We curate a human-annotated multi-modal dataset of 3,531 images, 898 3D scenes, and 7,460 3D objects, with audio-aligned dense annotations in 20 languages, including 8,746 image captions, 2,000 scene captions, and 19,000 object captions. Models trained on this dataset exhibit improvements of 5% in multilingual, 47% in cultural alignment, and 54% in 3D spatial capabilities. Our results show that our platform offers a feasible approach for future vision-language research and can be applied to various tasks and diverse types of data.",
        "translated": "随着多模态大语言模型（MLLMs）在各类应用中的快速普及，面向任务中心、高质量训练数据的需求日益迫切。当前训练数据集的一个关键局限在于其依赖于从互联网挖掘或通过手动输入获得的稀疏标注，仅能捕捉图像视觉内容的一小部分。密集标注更具价值，但目前仍极为稀缺。传统的基于文本的标注流程难以胜任密集标注的创建：打字限制了表达能力、降低了标注效率，并且未能充分覆盖细微的视觉特征，尤其是在多元文化图像和3D资产标注等专业领域。本文提出DenseAnnotate，一个以音频驱动的在线标注平台，可高效生成图像与3D资产的密集、细粒度标注。标注者在口头叙述观察的同时，将语音短语同步关联至图像区域或3D场景部件。我们的平台整合了语音转文本转录功能与注意力区域标记机制。为验证DenseAnnotate的有效性，我们在两个领域开展了涉及1000多名标注者的案例研究：文化多样性的图像及3D场景。我们构建了一个由人工标注的多模态数据集，包含3,531张图像、898个3D场景和7,460个3D物体，提供20种语言对齐的密集音频标注，涵盖8,746条图像描述、2,000条场景描述和19,000条物体描述。在该数据集上训练的模型，在多语言能力方面提升5%、文化契合度提升47%、3D空间能力提升54%。实验结果表明，我们的平台为未来视觉-语言研究提供了一种可行方案，可应用于多种任务及多样化的数据类型。",
        "translated_title": "DenseAnnotate：通过语音描述实现图像与三维场景的可扩展密集字幕采集",
        "label": [],
        "label_reason": "任务为多模态标注，非图像像素级处理",
        "relevance_score": 1,
        "novelty_score": 6,
        "novelty_reason": "平台创新但非图像处理核心改进"
    },
    {
        "title": "MOON2.0: Dynamic Modality-balanced Multimodal Representation Learning for E-commerce Product Understanding",
        "url": "http://arxiv.org/abs/2511.12449v1",
        "pub_date": "2025-11-16",
        "summary": "The rapid growth of e-commerce calls for multimodal models that comprehend rich visual and textual product information. Although recent multimodal large language models (MLLMs) for product understanding exhibit strong capability in representation learning for e-commerce, they still face three challenges: (i) the modality imbalance induced by modality mixed training; (ii) underutilization of the intrinsic alignment relationships among visual and textual information within a product; and (iii) limited handling of noise in e-commerce multimodal data. To address these, we propose MOON2.0, a dynamic modality-balanced multimodal representation learning framework for e-commerce product understanding. MOON2.0 comprises: (1) a Modality-driven Mixture-of-Experts (MoE) module that adaptively processes input samples by their modality composition, enabling Multimodal Joint Learning to mitigate the modality imbalance; (2) a Dual-level Alignment method to better leverage semantic alignment properties inside individual products; and (3) an MLLM-based Image-text Co-augmentation strategy that integrates textual enrichment with visual expansion, coupled with Dynamic Sample Filtering to improve training data quality. We further introduce MBE2.0, a co-augmented multimodal representation benchmark for e-commerce representation learning and evaluation. Experiments show that MOON2.0 delivers state-of-the-art zero-shot performance on MBE2.0 and multiple public datasets. Furthermore, attention-based heatmap visualization provides qualitative evidence of improved multimodal alignment of MOON2.0.",
        "translated": "电子商务的快速发展要求能够理解丰富视觉与文本产品信息的多模态模型。尽管近期用于产品理解的多模态大语言模型（MLLMs）在电商平台中的表征学习方面展现出强大能力，但仍面临三大挑战：(i) 由混合模态训练引发的模态不平衡问题；(ii) 对产品内部视觉与文本信息固有对齐关系的利用不足；(iii) 对电商多模态数据中噪声处理能力有限。为应对上述问题，我们提出 MOON2.0，一种面向电商产品理解的动态模态平衡多模态表征学习框架。MOON2.0 包含三个核心模块：(1) 一种基于模态驱动的专家混合（MoE）模块，可根据输入样本的模态组成自适应处理，实现多模态联合学习以缓解模态不平衡；(2) 双层次对齐方法，更有效地利用单个产品内部语义对齐特性；(3) 基于 MLLM 的图文协同增强策略，将文本丰富化与视觉扩展相结合，并辅以动态样本过滤机制以提升训练数据质量。此外，我们进一步引入 MBE2.0，作为电商表征学习与评估的协同增强多模态表征基准。实验表明，MOON2.0 在 MBE2.0 和多个公开数据集上实现了零样本状态下的最优性能。此外，基于注意力机制的热力图可视化结果从定性层面验证了 MOON2.0 多模态对齐能力的提升。",
        "translated_title": "MOON2.0：面向电子商务产品理解的动态模态均衡多模态表征学习",
        "label": [],
        "label_reason": "高阶多模态理解任务，非像素级图像处理",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "改进多模态对齐与数据增强，无低层级图像恢复创新"
    },
    {
        "title": "CoTBox-TTT: Grounding Medical VQA with Visual Chain-of-Thought Boxes During Test-time Training",
        "url": "http://arxiv.org/abs/2511.12446v1",
        "pub_date": "2025-11-16",
        "summary": "Medical visual question answering could support clinical decision making, yet current systems often fail under domain shift and produce answers that are weakly grounded in image evidence. This reliability gap arises when models attend to spurious regions and when retraining or additional labels are impractical at deployment time. We address this setting with CoTBox-TTT, an evidence-first test-time training approach that adapts a vision-language model at inference while keeping all backbones frozen. The method updates only a small set of continuous soft prompts. It identifies question-relevant regions through a visual chain-of-thought signal and encourages answer consistency across the original image and a localized crop. The procedure is label free, and plug and play with diverse backbones. Experiments on medical VQA show that the approach is practical for real deployments. For instance, adding CoTBox-TTT to LLaVA increases closed-ended accuracy by 12.3% on pathVQA.",
        "translated": "医学视觉问答可支持临床决策，但当前系统常在领域偏移下失效，并产生与图像证据弱关联的答案。这种可靠性差距源于模型关注了虚假区域，且在部署时重新训练或增加标注往往不可行。我们针对该场景提出 CoTBox-TTT，这是一种以证据为先的测试时训练方法，在推理过程中适配视觉-语言模型，同时保持所有骨干网络冻结不变。该方法仅更新一小部分连续软提示。它通过视觉链式思维信号识别问题相关区域，并鼓励原始图像与局部裁剪区域之间的答案一致性。该过程无需标注，且可与多种骨干网络即插即用。在医学 VQA 上的实验表明，该方法适用于实际部署。例如，将 CoTBox-TTT 与 LLaVA 结合后，在 pathVQA 数据集上的闭合式问答准确率提升了 12.3%。",
        "translated_title": "CoTBox-TTT：测试时训练阶段通过视觉思维链框实现医学视觉问答的定位",
        "label": [],
        "label_reason": "高阶视觉任务，非像素级图像处理",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出新提示机制提升VQA鲁棒性"
    },
    {
        "title": "Real-Time Drivers' Drowsiness Detection and Analysis through Deep Learning",
        "url": "http://arxiv.org/abs/2511.12438v1",
        "pub_date": "2025-11-16",
        "summary": "A long road trip is fun for drivers. However, a long drive for days can be tedious for a driver to accommodate stringent deadlines to reach distant destinations. Such a scenario forces drivers to drive extra miles, utilizing extra hours daily without sufficient rest and breaks. Once a driver undergoes such a scenario, it occasionally triggers drowsiness during driving. Drowsiness in driving can be life-threatening to any individual and can affect other drivers' safety; therefore, a real-time detection system is needed. To identify fatigued facial characteristics in drivers and trigger the alarm immediately, this research develops a real-time driver drowsiness detection system utilizing deep convolutional neural networks (DCNNs) and OpenCV.Our proposed and implemented model takes real- time facial images of a driver using a live camera and utilizes a Python-based library named OpenCV to examine the facial images for facial landmarks like sufficient eye openings and yawn-like mouth movements. The DCNNs framework then gathers the data and utilizes a per-trained model to detect the drowsiness of a driver using facial landmarks. If the driver is identified as drowsy, the system issues a continuous alert in real time, embedded in the Smart Car technology.By potentially saving innocent lives on the roadways, the proposed technique offers a non-invasive, inexpensive, and cost-effective way to identify drowsiness. Our proposed and implemented DCNNs embedded drowsiness detection model successfully react with NTHU-DDD dataset and Yawn-Eye-Dataset with drowsiness detection classification accuracy of 99.6% and 97% respectively.",
        "translated": "长途驾驶对司机而言乐趣无穷。然而，连续数日的长途驾驶会使司机为赶在截止日期前抵达远方目的地而感到疲惫不堪。这种情境迫使司机每日额外行驶数十英里，且缺乏充足休息与休憩时间。一旦司机经历此类状况，便可能在驾驶过程中偶发困倦。驾驶时的困倦对任何个体均具致命威胁，并可能危及他人行车安全；因此亟需一套实时检测系统。为识别司机面部疲劳特征并即时触发警报，本研究开发了一套基于深度卷积神经网络（DCNNs）与 OpenCV 的实时驾驶员困倦检测系统。\n\n我们所提出并实现的模型通过实时摄像头采集司机面部图像，并利用基于 Python 的 OpenCV 库分析面部图像，提取如眼睛充分睁开、类似打哈欠的嘴部运动等面部关键点。随后，DCNN 框架收集数据，并借助预训练模型根据前述面部关键点检测司机是否处于困倦状态。若系统判定司机已困倦，则会立即在车内智能汽车技术平台上持续发出警报。\n\n该方法有望拯救道路上无辜的生命，同时提供一种非侵入式、廉价且经济高效的方式来识别困倦状态。我们所提出的嵌入 DCNN 的困倦检测模型，在 NTHU-DDD 数据集与 Yawn-Eye-Dataset 上分别实现了 99.6% 和 97% 的困倦检测分类准确率。",
        "translated_title": "基于深度学习的实时驾驶员困倦检测与分析",
        "label": [],
        "label_reason": "属于高阶视觉任务，非像素级图像处理",
        "relevance_score": 1,
        "novelty_score": 1,
        "novelty_reason": "无创新，仅复现DCNN分类模型"
    },
    {
        "title": "Text-Guided Channel Perturbation and Pretrained Knowledge Integration for Unified Multi-Modality Image Fusion",
        "url": "http://arxiv.org/abs/2511.12432v1",
        "pub_date": "2025-11-16",
        "summary": "Multi-modality image fusion enhances scene perception by combining complementary information. Unified models aim to share parameters across modalities for multi-modality image fusion, but large modality differences often cause gradient conflicts, limiting performance. Some methods introduce modality-specific encoders to enhance feature perception and improve fusion quality. However, this strategy reduces generalisation across different fusion tasks. To overcome this limitation, we propose a unified multi-modality image fusion framework based on channel perturbation and pre-trained knowledge integration (UP-Fusion). To suppress redundant modal information and emphasize key features, we propose the Semantic-Aware Channel Pruning Module (SCPM), which leverages the semantic perception capability of a pre-trained model to filter and enhance multi-modality feature channels. Furthermore, we proposed the Geometric Affine Modulation Module (GAM), which uses original modal features to apply affine transformations on initial fusion features to maintain the feature encoder modal discriminability. Finally, we apply a Text-Guided Channel Perturbation Module (TCPM) during decoding to reshape the channel distribution, reducing the dependence on modality-specific channels. Extensive experiments demonstrate that the proposed algorithm outperforms existing methods on both multi-modality image fusion and downstream tasks.",
        "translated": "多模态图像融合通过整合互补信息增强场景感知能力。统一模型旨在跨模态共享参数以实现多模态图像融合，但模态间差异较大时常导致梯度冲突，限制了性能表现。部分方法引入模态特异性编码器以增强特征感知并提升融合质量，然而该策略降低了模型在不同融合任务间的泛化能力。为克服这一局限，我们提出一种基于通道扰动与预训练知识集成的统一多模态图像融合框架（UP-Fusion）。为抑制冗余模态信息并突出关键特征，我们提出语义感知通道剪枝模块（SCPM），利用预训练模型的语义感知能力对多模态特征通道进行筛选与增强。此外，我们提出几何仿射调制模块（GAM），利用原始模态特征对初始融合特征施加仿射变换，以保持特征编码器的模态判别性。最后，在解码阶段应用文本引导通道扰动模块（TCPM）重塑通道分布，降低对模态特异性通道的依赖。大量实验表明，所提算法在多模态图像融合及其下游任务中均优于现有方法。",
        "translated_title": "文本引导的通道扰动与预训练知识融合用于统一多模态图像融合",
        "label": [],
        "label_reason": "聚焦多模态融合，非像素级图像恢复任务",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "模块改进常规，无本质新范式创新"
    },
    {
        "title": "RedVTP: Training-Free Acceleration of Diffusion Vision-Language Models Inference via Masked Token-Guided Visual Token Pruning",
        "url": "http://arxiv.org/abs/2511.12428v1",
        "pub_date": "2025-11-16",
        "summary": "Vision-Language Models (VLMs) have achieved remarkable progress in multimodal reasoning and generation, yet their high computational demands remain a major challenge. Diffusion Vision-Language Models (DVLMs) are particularly attractive because they enable parallel token decoding, but the large number of visual tokens still significantly hinders their inference efficiency. While visual token pruning has been extensively studied for autoregressive VLMs (AVLMs), it remains largely unexplored for DVLMs. In this work, we propose RedVTP, a response-driven visual token pruning strategy that leverages the inference dynamics of DVLMs. Our method estimates visual token importance using attention from the masked response tokens. Based on the observation that these importance scores remain consistent across steps, RedVTP prunes the less important visual tokens from the masked tokens after the first inference step, thereby maximizing inference efficiency. Experiments show that RedVTP improves token generation throughput of LLaDA-V and LaViDa by up to 186% and 28.05%, respectively, and reduces inference latency by up to 64.97% and 21.87%, without compromising-and in some cases improving-accuracy.",
        "translated": "视觉-语言模型（VLMs）在多模态推理与生成任务中取得了显著进展，但其高昂的计算开销仍是一个主要挑战。扩散视觉-语言模型（DVLMs）尤其具有吸引力，因为它们支持并行的标记解码，然而大量视觉标记仍显著制约了其推理效率。虽然视觉标记剪枝已在自回归视觉-语言模型（AVLMs）中被广泛研究，但在DVLMs中仍鲜有探索。在本工作中，我们提出RedVTP，一种基于响应驱动的视觉标记剪枝策略，该策略利用DVLMs推理过程中的动态特性。我们的方法通过从掩码响应标记中提取注意力信息来评估视觉标记的重要性；基于观察到这些重要性评分在推理步骤间保持一致性的特点，RedVTP在首个推理步骤后从掩码标记中剪除低重要性的视觉标记，从而最大化推理效率。实验表明，RedVTP可将LLaDA-V和LaViDa的标记生成吞吐量分别提升高达186%和28.05%，同时将推理延迟分别降低高达64.97%和21.87%，且未牺牲——甚至在某些情况下提升了——准确率。",
        "translated_title": "RedVTP：通过掩码标记引导的视觉标记剪枝，在无需训练的情况下加速扩散式视觉-语言模型推理",
        "label": [],
        "label_reason": "处理视觉语言模型推理加速，非图像像素级恢复",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出基于响应驱动的视觉令牌剪枝新策略"
    },
    {
        "title": "MFI-ResNet: Efficient ResNet Architecture Optimization via MeanFlow Compression and Selective Incubation",
        "url": "http://arxiv.org/abs/2511.12422v1",
        "pub_date": "2025-11-16",
        "summary": "ResNet has achieved tremendous success in computer vision through its residual connection mechanism. ResNet can be viewed as a discretized form of ordinary differential equations (ODEs). From this perspective, the multiple residual blocks within a single ResNet stage essentially perform multi-step discrete iterations of the feature transformation for that stage. The recently proposed flow matching model, MeanFlow, enables one-step generative modeling by learning the mean velocity field to transform distributions. Inspired by this, we propose MeanFlow-Incubated ResNet (MFI-ResNet), which employs a compression-expansion strategy to jointly improve parameter efficiency and discriminative performance. In the compression phase, we simplify the multi-layer structure within each ResNet stage to one or two MeanFlow modules to construct a lightweight meta model. In the expansion phase, we apply a selective incubation strategy to the first three stages, expanding them to match the residual block configuration of the baseline ResNet model, while keeping the last stage in MeanFlow form, and fine-tune the incubated model. Experimental results show that on CIFAR-10 and CIFAR-100 datasets, MFI-ResNet achieves remarkable parameter efficiency, reducing parameters by 46.28% and 45.59% compared to ResNet-50, while still improving accuracy by 0.23% and 0.17%, respectively. This demonstrates that generative flow-fields can effectively characterize the feature transformation process in ResNet, providing a new perspective for understanding the relationship between generative modeling and discriminative learning.",
        "translated": "残差网络（ResNet）通过其残差连接机制在计算机视觉领域取得了巨大成功。从这一视角看，ResNet可视为常微分方程（ODEs）的离散化形式。在此框架下，单个ResNet阶段内的多个残差块本质上执行该阶段特征变换的多步离散迭代。近期提出的流匹配模型MeanFlow通过学习均值速度场来实现一步生成建模。受此启发，我们提出MeanFlow孵化残差网络（MFI-ResNet），采用压缩-扩展策略，协同提升参数效率与判别性能。在压缩阶段，我们将每个ResNet阶段内部的多层结构简化为一个或两个MeanFlow模块，构建轻量级元模型；在扩展阶段，对前三个阶段应用选择性孵化策略，将其扩展至匹配基线ResNet模型的残差块配置，而保留最后一阶段采用MeanFlow形式，并对孵化后的模型进行微调。实验结果表明，在CIFAR-10和CIFAR-100数据集上，MFI-ResNet显著提升了参数效率，相较于ResNet-50分别减少46.28%和45.59%的参数量，同时准确率分别提升0.23%和0.17%。这证明了生成流场能够有效刻画ResNet中的特征变换过程，为理解生成建模与判别学习之间的关系提供了新视角。",
        "translated_title": "MFI-ResNet：通过均流压缩与选择性孵化实现高效的 ResNet 架构优化",
        "label": [],
        "label_reason": "目标为分类任务，非图像像素级恢复",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "结构优化与参数效率提升，非图像处理创新"
    },
    {
        "title": "Seeing Through the Rain: Resolving High-Frequency Conflicts in Deraining and Super-Resolution via Diffusion Guidance",
        "url": "http://arxiv.org/abs/2511.12419v1",
        "pub_date": "2025-11-16",
        "summary": "Clean images are crucial for visual tasks such as small object detection, especially at high resolutions. However, real-world images are often degraded by adverse weather, and weather restoration methods may sacrifice high-frequency details critical for analyzing small objects. A natural solution is to apply super-resolution (SR) after weather removal to recover both clarity and fine structures. However, simply cascading restoration and SR struggle to bridge their inherent conflict: removal aims to remove high-frequency weather-induced noise, while SR aims to hallucinate high-frequency textures from existing details, leading to inconsistent restoration contents. In this paper, we take deraining as a case study and propose DHGM, a Diffusion-based High-frequency Guided Model for generating clean and high-resolution images. DHGM integrates pre-trained diffusion priors with high-pass filters to simultaneously remove rain artifacts and enhance structural details. Extensive experiments demonstrate that DHGM achieves superior performance over existing methods, with lower costs.",
        "translated": "清晰图像对于小目标检测等视觉任务至关重要，尤其是在高分辨率场景下。然而，现实世界中的图像常因恶劣天气而退化，而天气恢复方法往往牺牲了对小目标分析至关重要的高频细节。一种自然的解决方案是在去除天气影响后应用超分辨率（SR）技术，以同时恢复图像的清晰度和精细结构。然而，简单地将恢复与超分辨率串联处理，难以弥合二者固有的矛盾：恢复旨在消除由天气引起的高频噪声，而超分辨率则试图从现有细节中“生成”高频纹理，导致恢复内容不一致。在本文中，我们以去雨作为案例研究，提出 DHGM（基于扩散的高频引导模型），用于生成清晰且高分辨率的图像。DHGM 将预训练的扩散先验与高通滤波器相结合，在同时去除雨滴伪影并增强结构细节方面表现出色。大量实验表明，DHGM 在性能上优于现有方法，且计算成本更低。",
        "translated_title": "雨中透视：通过扩散引导解决去雨与超分辨率中的高频冲突",
        "label": [
            "图像去雨",
            "超分辨率"
        ],
        "label_reason": "提出扩散模型联合去雨与超分，解决高频冲突",
        "relevance_score": 10,
        "novelty_score": 9,
        "novelty_reason": "首次用扩散引导实现去雨与超分协同增强"
    },
    {
        "title": "Towards Rotation-only Imaging Geometry: Rotation Estimation",
        "url": "http://arxiv.org/abs/2511.12415v1",
        "pub_date": "2025-11-16",
        "summary": "Structure from Motion (SfM) is a critical task in computer vision, aiming to recover the 3D scene structure and camera motion from a sequence of 2D images. The recent pose-only imaging geometry decouples 3D coordinates from camera poses and demonstrates significantly better SfM performance through pose adjustment. Continuing the pose-only perspective, this paper explores the critical relationship between the scene structures, rotation and translation. Notably, the translation can be expressed in terms of rotation, allowing us to condense the imaging geometry representation onto the rotation manifold. A rotation-only optimization framework based on reprojection error is proposed for both two-view and multi-view scenarios. The experiment results demonstrate superior accuracy and robustness performance over the current state-of-the-art rotation estimation methods, even comparable to multiple bundle adjustment iteration results. Hopefully, this work contributes to even more accurate, efficient and reliable 3D visual computing.",
        "translated": "结构从运动（SfM）是计算机视觉中的关键任务，旨在从一系列二维图像中恢复三维场景结构与相机运动。近期的仅姿态成像几何方法将三维坐标与相机位姿解耦，并通过姿态调整实现了显著优于传统方法的SfM性能。延续仅姿态视角，本文探索了场景结构、旋转与平移之间的关键关系。值得注意的是，平移可由旋转表达，从而允许我们将成像几何表示压缩至旋转流形上。为此，我们提出了一种基于重投影误差的仅旋转优化框架，适用于两视图及多视图场景。实验结果表明，该方法在精度和鲁棒性方面均优于当前最先进的旋转估计方法，甚至可媲美多次束调整迭代的结果。希望本工作能为更精确、高效且可靠的三维视觉计算提供助力。",
        "translated_title": "面向仅旋转的成像几何：旋转估计",
        "label": [],
        "label_reason": "任务为3D视觉几何优化，非图像像素级恢复",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出旋转主导的优化框架，提升SfM精度"
    },
    {
        "title": "Compact Multimodal Language Models as Robust OCR Alternatives for Noisy Textual Clinical Reports",
        "url": "http://arxiv.org/abs/2511.13523v1",
        "pub_date": "2025-11-17",
        "summary": "Digitization of medical records often relies on smartphone photographs of printed reports, producing images degraded by blur, shadows, and other noise. Conventional OCR systems, optimized for clean scans, perform poorly under such real-world conditions. This study evaluates compact multimodal language models as privacy-preserving alternatives for transcribing noisy clinical documents. Using obstetric ultrasound reports written in regionally inflected medical English common to Indian healthcare settings, we compare eight systems in terms of transcription accuracy, noise sensitivity, numeric accuracy, and computational efficiency. Compact multimodal models consistently outperform both classical and neural OCR pipelines. Despite higher computational costs, their robustness and linguistic adaptability position them as viable candidates for on-premises healthcare digitization.",
        "translated": "医疗记录的数字化常依赖于对打印报告的智能手机拍照，由此生成的图像常因模糊、阴影及其他噪声而失真。传统的光学字符识别（OCR）系统针对清晰扫描文档优化，在此类真实环境条件下表现不佳。本研究评估了紧凑型多模态语言模型作为隐私保护替代方案，用于转录含噪临床文档。我们以印度医疗环境中常见的区域性变体医学英语撰写的产科超声报告为测试对象，从转录准确率、噪声敏感度、数值准确性及计算效率四个维度比较八种系统。结果表明，紧凑型多模态模型在各项指标上持续优于传统及神经网络OCR流程。尽管其计算开销更高，但其鲁棒性与语言适应能力使其成为本地化医疗数字化的可行候选方案。",
        "translated_title": "紧凑型多模态语言模型作为嘈杂文本临床报告的鲁棒OCR替代方案",
        "label": [],
        "label_reason": "论文聚焦OCR技术，非推荐系统相关",
        "relevance_score": 3,
        "novelty_score": 5,
        "novelty_reason": "改进OCR模型，无推荐系统创新"
    },
    {
        "title": "PolicyBot - Reliable Question Answering over Policy Documents",
        "url": "http://arxiv.org/abs/2511.13489v1",
        "pub_date": "2025-11-17",
        "summary": "All citizens of a country are affected by the laws and policies introduced by their government. These laws and policies serve essential functions for citizens. Such as granting them certain rights or imposing specific obligations. However, these documents are often lengthy, complex, and difficult to navigate, making it challenging for citizens to locate and understand relevant information. This work presents PolicyBot, a retrieval-augmented generation (RAG) system designed to answer user queries over policy documents with a focus on transparency and reproducibility. The system combines domain-specific semantic chunking, multilingual dense embeddings, multi-stage retrieval with reranking, and source-aware generation to provide responses grounded in the original documents. We implemented citation tracing to reduce hallucinations and improve user trust, and evaluated alternative retrieval and generation configurations to identify effective design choices. The end-to-end pipeline is built entirely with open-source tools, enabling easy adaptation to other domains requiring document-grounded question answering. This work highlights design considerations, practical challenges, and lessons learned in deploying trustworthy RAG systems for governance-related contexts.",
        "translated": "一个国家的所有公民均受其政府所制定的法律与政策影响。这些法律和政策对公民具有基础性功能，例如赋予其特定权利或施加特定义务。然而，此类文件往往篇幅冗长、结构复杂且难以查阅，使普通公民难以定位并理解相关条文。本文提出 PolicyBot，一种检索增强生成（RAG）系统，旨在针对政策文档回答用户查询，并注重透明性与可复现性。该系统结合领域语义分块、多语言稠密嵌入、多阶段召回与重排机制，以及源感知生成技术，确保其响应内容基于原始文档。我们通过引用溯源机制降低幻觉现象、提升用户信任度，并评估多种召回与生成配置以识别高效设计策略。该端到端流程完全依托开源工具构建，便于快速适配其他需文档支撑问答的领域。本研究强调了在治理相关场景中部署可信 RAG 系统的设计考量、实际挑战及经验教训。",
        "translated_title": "PolicyBot - 基于政策文档的可靠问答系统",
        "label": [],
        "label_reason": "论文聚焦政策文档问答，非推荐系统范畴",
        "relevance_score": 3,
        "novelty_score": 5,
        "novelty_reason": "RAG架构改进，但无推荐系统创新"
    },
    {
        "title": "Exploring Multi-Table Retrieval Through Iterative Search",
        "url": "http://arxiv.org/abs/2511.13418v1",
        "pub_date": "2025-11-17",
        "summary": "Open-domain question answering over datalakes requires retrieving and composing information from multiple tables, a challenging subtask that demands semantic relevance and structural coherence (e.g., joinability). While exact optimization methods like Mixed-Integer Programming (MIP) can ensure coherence, their computational complexity is often prohibitive. Conversely, simpler greedy heuristics that optimize for query coverage alone often fail to find these coherent, joinable sets. This paper frames multi-table retrieval as an iterative search process, arguing this approach offers advantages in scalability, interpretability, and flexibility. We propose a general framework and a concrete instantiation: a fast, effective Greedy Join-Aware Retrieval algorithm that holistically balances relevance, coverage, and joinability. Experiments across 5 NL2SQL benchmarks demonstrate that our iterative method achieves competitive retrieval performance compared to the MIP-based approach while being 4-400x faster depending on the benchmark and search space settings. This work highlights the potential of iterative heuristics for practical, scalable, and composition-aware retrieval.",
        "translated": "在数据湖上进行开放域问答需要从多个表中检索并组合信息，这是一个具有挑战性的子任务，要求语义相关性与结构连贯性（例如可连接性）。虽然精确优化方法如混合整数规划（MIP）能够确保连贯性，但其计算复杂度通常不可接受。相反，仅针对查询覆盖进行优化的简单贪心启发式方法往往无法找到这些连贯且可连接的集合。本文将多表检索建模为一个迭代搜索过程，认为该方法在可扩展性、可解释性和灵活性方面具有优势。我们提出了一般性框架及其具体实现：一种快速有效的贪心连接感知检索算法，该算法全面平衡相关性、覆盖范围和可连接性。在5个NL2SQL基准测试中的实验表明，我们的迭代方法在检索性能上与基于MIP的方法相当，但在不同基准和搜索空间设置下速度提升4至400倍。本工作凸显了迭代启发式方法在实用、可扩展且具备组合感知能力检索方面的潜力。",
        "translated_title": "通过迭代搜索探索多表检索",
        "label": [],
        "label_reason": "聚焦多表检索，与推荐系统无直接关联",
        "relevance_score": 3,
        "novelty_score": 6,
        "novelty_reason": "迭代搜索框架有实用改进，但非推荐领域创新"
    },
    {
        "title": "Attention Grounded Enhancement for Visual Document Retrieval",
        "url": "http://arxiv.org/abs/2511.13415v1",
        "pub_date": "2025-11-17",
        "summary": "Visual document retrieval requires understanding heterogeneous and multi-modal content to satisfy information needs. Recent advances use screenshot-based document encoding with fine-grained late interaction, significantly improving retrieval performance. However, retrievers are still trained with coarse global relevance labels, without revealing which regions support the match. As a result, retrievers tend to rely on surface-level cues and struggle to capture implicit semantic connections, hindering their ability to handle non-extractive queries. To alleviate this problem, we propose a \\textbf{A}ttention-\\textbf{G}rounded \\textbf{RE}triever \\textbf{E}nhancement (AGREE) framework. AGREE leverages cross-modal attention from multimodal large language models as proxy local supervision to guide the identification of relevant document regions. During training, AGREE combines local signals with the global signals to jointly optimize the retriever, enabling it to learn not only whether documents match, but also which content drives relevance. Experiments on the challenging ViDoRe V2 benchmark show that AGREE significantly outperforms the global-supervision-only baseline. Quantitative and qualitative analyses further demonstrate that AGREE promotes deeper alignment between query terms and document regions, moving beyond surface-level matching toward more accurate and interpretable retrieval. Our code is available at: https://anonymous.4open.science/r/AGREE-2025.",
        "translated": "视觉文档检索需要理解异构且多模态的内容以满足用户的信息需求。近期研究通过基于截图的文档编码与细粒度后交互机制，显著提升了检索性能。然而，当前检索器仍仅依赖粗粒度的全局相关性标签进行训练，未能揭示哪些区域支持匹配关系。因此，检索器往往依赖于表面特征，难以捕捉隐含语义关联，限制了其处理非抽取式查询的能力。为缓解该问题，我们提出一种**注意力引导的检索增强框架**（A ttention-Grounded REtriever ENhancement, AGREE）。AGREE 利用多模态大语言模型中的跨模态注意力机制作为代理局部监督信号，引导检索器识别相关文档区域。在训练过程中，AGREE 将局部信号与全局信号结合，联合优化检索器，使其不仅能学习文档是否匹配，还能识别驱动相关性的具体内容。在具有挑战性的 ViDoRe V2 数据集上的实验表明，AGREE 显著优于仅使用全局监督的基线方法。定量与定性分析进一步证明，AGREE 促进了查询词与文档区域之间更深层次的对齐，超越了表面匹配，实现了更准确、可解释的检索效果。我们的代码已公开：https://anonymous.4open.science/r/AGREE-2025。",
        "translated_title": "视觉文档检索中的注意力引导增强",
        "label": [
            "通用推荐技术",
            "多模态推荐"
        ],
        "label_reason": "聚焦视觉文档检索，属多模态信息匹配问题",
        "relevance_score": 4,
        "novelty_score": 7,
        "novelty_reason": "引入跨模态注意力作为局部监督信号提升检索精度"
    },
    {
        "title": "Uncovering Causal Drivers of Energy Efficiency for Industrial Process in Foundry via Time-Series Causal Inference",
        "url": "http://arxiv.org/abs/2511.13389v1",
        "pub_date": "2025-11-17",
        "summary": "Improving energy efficiency in industrial foundry processes is a critical challenge, as these operations are highly energy-intensive and marked by complex interdependencies among process variables. Correlation-based analyses often fail to distinguish true causal drivers from spurious associations, limiting their usefulness for decision-making. This paper applies a time-series causal inference framework to identify the operational factors that directly affect energy efficiency in induction furnace melting. Using production data from a Danish foundry, the study integrates time-series clustering to segment melting cycles into distinct operational modes with the PCMCI+ algorithm, a state-of-the-art causal discovery method, to uncover cause-effect relationships within each mode. Across clusters, robust causal relations among energy consumption, furnace temperature, and material weight define the core drivers of efficiency, while voltage consistently influences cooling water temperature with a delayed response. Cluster-specific differences further distinguish operational regimes: efficient clusters are characterized by stable causal structures, whereas inefficient ones exhibit reinforcing feedback loops and atypical dependencies. The contributions of this study are twofold. First, it introduces an integrated clustering-causal inference pipeline as a methodological innovation for analyzing energy-intensive processes. Second, it provides actionable insights that enable foundry operators to optimize performance, reduce energy consumption, and lower emissions.",
        "translated": "提升工业铸造工艺的能量效率是一项关键挑战，因为这些工艺高度耗能，且过程变量间存在复杂的相互依赖关系。基于相关性的分析往往难以区分真实的因果驱动因素与虚假的相关性，限制了其在决策中的实用性。本文应用时间序列因果推断框架，识别感应炉熔炼过程中直接影响能源效率的操作因素。研究利用丹麦一家铸造厂的生产数据，结合时间序列聚类方法将熔炼周期划分为不同的操作模式，并采用当前最先进的因果发现方法PCMCI+，在各模式内揭示因果关系。在各个聚类中，能源消耗、炉温与物料重量之间稳健的因果关系构成了效率的核心驱动因素，而电压则始终对冷却水温度产生延迟响应的影响。不同聚类之间的差异进一步区分了不同操作工况：高效聚类表现出稳定的因果结构，而不高效聚类则呈现强化的反馈循环及非典型依赖关系。本研究贡献体现在两方面：首先，提出一种集成聚类-因果推断的流程，作为分析高耗能工艺的方法论创新；其次，提供可操作的洞察，助力铸造厂操作人员优化性能、降低能耗并减少排放。",
        "translated_title": "通过时间序列因果推断揭示铸造工业过程中能效的因果驱动因素",
        "label": [],
        "label_reason": "研究聚焦工业过程因果推理，与推荐系统无直接关联。",
        "relevance_score": 2,
        "novelty_score": 5,
        "novelty_reason": "方法创新在工业分析领域，非推荐系统专用技术。"
    },
    {
        "title": "FLOWER: Flow-Oriented Entity-Relationship Tool",
        "url": "http://arxiv.org/abs/2511.13357v1",
        "pub_date": "2025-11-17",
        "summary": "Exploring relationships across data sources is a crucial optimization for entities recognition. Since databases can store big amount of information with synthetic and organic data, serving all quantity of objects correctly is an important task to deal with. However, the decision of how to construct entity relationship model is associated with human factor. In this paper, we present flow-oriented entity-relationship tool. This is first and unique end-to-end solution that eliminates routine and resource-intensive problems of processing, creating and visualizing both of explicit and implicit dependencies for prominent SQL dialects on-the-fly. Once launched, FLOWER automatically detects built-in constraints and starting to create own correct and necessary one using dynamic sampling and robust data analysis techniques. This approach applies to improve entity-relationship model and data storytelling to better understand the foundation of data and get unseen insights from DB sources using SQL or natural language. Evaluated on state-of-the-art STATS benchmark, experiments show that FLOWER is superior to reservoir sampling by 2.4x for distribution representation and 2.6x for constraint learning with 2.15x acceleration. For data storytelling, our tool archives 1.19x for accuracy enhance with 1.86x context decrease compare to LLM. Presented tool is also support 23 languages and compatible with both of CPU and GPU. Those results show that FLOWER can manage with real-world data a way better to ensure with quality, scalability and applicability for different use-cases.",
        "translated": "探索跨数据源的关系是实体识别的关键优化。由于数据库可存储大量合成与有机数据，准确处理所有对象是一项重要任务。然而，实体关系模型的构建方式往往受人为因素影响。本文提出了一种面向流程的实体关系工具FLOWER。这是首个也是唯一一个端到端解决方案，能够即时处理、创建并可视化主流SQL方言中的显式与隐式依赖关系，从而消除传统方法中繁琐且资源密集的问题。一旦启动，FLOWER将自动检测内置约束，并利用动态采样与稳健数据分析技术自动生成正确且必要的新约束。该方法可用于改进实体关系模型及数据叙事能力，帮助用户更深入理解数据基础，并从数据库中挖掘前所未见的洞察，支持使用SQL或自然语言实现。在最新STATS基准测试中，实验表明：FLOWER在分布表征方面比水库采样快2.4倍，在约束学习方面快2.6倍，整体加速达2.15倍。对于数据叙事，本工具相较大语言模型（LLM）在准确性提升上达到1.19倍，上下文减少1.86倍。所呈现的工具支持23种语言，并兼容CPU与GPU。实验结果表明，FLOWER能更高效地应对真实世界数据，确保其质量、可扩展性及在不同应用场景下的适用性。",
        "translated_title": "FLOWER：基于流导向的实体关系工具",
        "label": [],
        "label_reason": "论文聚焦数据库ER建模，与推荐系统无关",
        "relevance_score": 2,
        "novelty_score": 5,
        "novelty_reason": "改进采样与可视化工具，非推荐领域创新"
    },
    {
        "title": "Examining the Usage of Generative AI Models in Student Learning Activities for Software Programming",
        "url": "http://arxiv.org/abs/2511.13271v1",
        "pub_date": "2025-11-17",
        "summary": "The rise of Generative AI (GenAI) tools like ChatGPT has created new opportunities and challenges for computing education. Existing research has primarily focused on GenAI's ability to complete educational tasks and its impact on student performance, often overlooking its effects on knowledge gains. In this study, we investigate how GenAI assistance compares to conventional online resources in supporting knowledge gains across different proficiency levels. We conducted a controlled user experiment with 24 undergraduate students of two different levels of programming experience (beginner, intermediate) to examine how students interact with ChatGPT while solving programming tasks. We analyzed task performance, conceptual understanding, and interaction behaviors. Our findings reveal that generating complete solutions with GenAI significantly improves task performance, especially for beginners, but does not consistently result in knowledge gains. Importantly, usage strategies differ by experience: beginners tend to rely heavily on GenAI toward task completion often without knowledge gain in the process, while intermediates adopt more selective approaches. We find that both over-reliance and minimal use result in weaker knowledge gains overall. Based on our results, we call on students and educators to adopt GenAI as a learning rather than a problem solving tool. Our study highlights the urgent need for guidance when integrating GenAI into programming education to foster deeper understanding.",
        "translated": "生成式人工智能（GenAI）工具如ChatGPT的兴起，为计算教育带来了新的机遇与挑战。现有研究主要聚焦于GenAI完成教育任务的能力及其对学生学业表现的影响，常忽视其对知识获取效果的作用。本研究探讨了在不同熟练程度水平下，GenAI辅助相较于传统在线资源，在支持知识获取方面的差异。我们通过对24名编程经验水平不同的本科生（初学者、进阶者）开展受控用户实验，考察学生在解决编程任务过程中如何使用ChatGPT。我们分析了任务表现、概念理解及交互行为。研究发现，利用GenAI生成完整解决方案显著提升了任务表现，尤其对初学者更为明显，但并未持续带来知识增长。重要的是，使用策略因经验水平而异：初学者倾向于过度依赖GenAI以完成任务，此过程往往缺乏知识积累；而进阶者则采取更选择性的使用方式。我们发现，无论是过度依赖还是极少使用，均导致整体知识收益较弱。基于研究结果，我们呼吁学生和教育者将GenAI视为学习工具，而非问题解决工具。本研究凸显了在编程教育中整合GenAI时亟需指导以促进深度理解的重要性。",
        "translated_title": "探究生成式人工智能模型在学生软件编程学习活动中的应用",
        "label": [],
        "label_reason": "研究聚焦教育场景，非推荐系统核心问题",
        "relevance_score": 3,
        "novelty_score": 4,
        "novelty_reason": "方法常规，无推荐系统创新设计"
    },
    {
        "title": "Cog-RAG: Cognitive-Inspired Dual-Hypergraph with Theme Alignment Retrieval-Augmented Generation",
        "url": "http://arxiv.org/abs/2511.13201v1",
        "pub_date": "2025-11-17",
        "summary": "Retrieval-Augmented Generation (RAG) enhances the response quality and domain-specific performance of large language models (LLMs) by incorporating external knowledge to combat hallucinations. In recent research, graph structures have been integrated into RAG to enhance the capture of semantic relations between entities. However, it primarily focuses on low-order pairwise entity relations, limiting the high-order associations among multiple entities. Hypergraph-enhanced approaches address this limitation by modeling multi-entity interactions via hyperedges, but they are typically constrained to inter-chunk entity-level representations, overlooking the global thematic organization and alignment across chunks. Drawing inspiration from the top-down cognitive process of human reasoning, we propose a theme-aligned dual-hypergraph RAG framework (Cog-RAG) that uses a theme hypergraph to capture inter-chunk thematic structure and an entity hypergraph to model high-order semantic relations. Furthermore, we design a cognitive-inspired two-stage retrieval strategy that first activates query-relevant thematic content from the theme hypergraph, and then guides fine-grained recall and diffusion in the entity hypergraph, achieving semantic alignment and consistent generation from global themes to local details. Our extensive experiments demonstrate that Cog-RAG significantly outperforms existing state-of-the-art baseline approaches.",
        "translated": "检索增强生成（RAG）通过引入外部知识来对抗幻觉，从而提升大语言模型（LLM）的响应质量与领域特异性表现。近期研究中，图结构被整合进 RAG 以增强实体间语义关系的捕捉能力。然而，此类方法主要聚焦于低阶成对实体关系，难以建模多实体间的高阶关联。超图增强方法通过超边建模多实体交互，缓解了这一限制，但通常受限于跨块级别的实体表征，忽略了块间全局主题组织与对齐关系。受人类推理自上而下认知过程的启发，我们提出一种主题对齐双超图 RAG 框架（Cog-RAG），其利用主题超图捕捉跨块的主题结构，并借助实体超图建模高阶语义关系。此外，我们设计了一种受认知启发的两阶段检索策略：首先从主题超图激活与查询相关的主题内容，随后引导在实体超图中的细粒度召回与扩散，实现从全局主题到局部细节的语义对齐与一致生成。大量实验表明，Cog-RAG 显著优于现有最先进基线方法。",
        "translated_title": "Cog-RAG：受认知启发的双超图与主题对齐检索增强生成",
        "label": [
            "LLM生成式推荐",
            "通用推荐技术"
        ],
        "label_reason": "RAG框架用于提升LLM生成质量，具推荐系统间接应用价值。",
        "relevance_score": 7,
        "novelty_score": 8,
        "novelty_reason": "创新性双超图结构与认知启发检索策略，显著提升效果。"
    },
    {
        "title": "Large Language Models Meet Extreme Multi-label Classification: Scaling and Multi-modal Framework",
        "url": "http://arxiv.org/abs/2511.13189v1",
        "pub_date": "2025-11-17",
        "summary": "Foundation models have revolutionized artificial intelligence across numerous domains, yet their transformative potential remains largely untapped in Extreme Multi-label Classification (XMC). Queries in XMC are associated with relevant labels from extremely large label spaces, where it is critical to strike a balance between efficiency and performance. Therefore, many recent approaches efficiently pose XMC as a maximum inner product search between embeddings learned from small encoder-only transformer architectures. In this paper, we address two important aspects in XMC: how to effectively harness larger decoder-only models, and how to exploit visual information while maintaining computational efficiency. We demonstrate that both play a critical role in XMC separately and can be combined for improved performance. We show that a few billion-size decoder can deliver substantial improvements while keeping computational overhead manageable. Furthermore, our Vision-enhanced eXtreme Multi-label Learning framework (ViXML) efficiently integrates foundation vision models by pooling a single embedding per image. This limits computational growth while unlocking multi-modal capabilities. Remarkably, ViXML with small encoders outperforms text-only decoder in most cases, showing that an image is worth billions of parameters. Finally, we present an extension of existing text-only datasets to exploit visual metadata and make them available for future benchmarking. Comprehensive experiments across four public text-only datasets and their corresponding image enhanced versions validate our proposals' effectiveness, surpassing previous state-of-the-art by up to +8.21\\% in P@1 on the largest dataset. ViXML's code is available at https://github.com/DiegoOrtego/vixml.",
        "translated": "基础模型已在众多领域革新了人工智能，但在极端多标签分类（XMC）中，其变革潜力仍远未被充分挖掘。在XMC任务中，查询需从极其庞大的标签空间中关联相关标签，因此必须在效率与性能之间取得平衡。为此，许多近期方法高效地将XMC建模为嵌入向量间的最大内积搜索问题，其中嵌入由小型仅编码器结构的Transformer架构学习得到。本文聚焦于XMC中的两个关键方面：如何有效利用更大的仅解码器模型，以及如何在保持计算效率的同时融合视觉信息。我们证明，这两者在XMC中均扮演着关键角色，并可结合使用以提升整体性能。我们表明，仅使用数亿参数规模的解码器即可带来显著性能提升，同时控制计算开销在合理范围内。此外，我们的视觉增强型极端多标签学习框架（ViXML）通过为每张图像聚合单一嵌入向量，高效整合基础视觉模型，在限制计算增长的同时解锁多模态能力。尤为值得注意的是，当搭配小型编码器时，ViXML在大多数情况下优于纯文本解码器，表明“一张图像胜过数十亿参数”。最后，我们扩展了现有纯文本数据集，使其包含视觉元数据，并将其公开用于未来基准测试。我们在四个公开的纯文本数据集及其对应的视觉增强版本上进行了全面实验，结果验证了所提方法的有效性，在最大数据集上的P@1指标较此前最先进方法提升了高达+8.21%。ViXML代码已开源，地址为 https://github.com/DiegoOrtego/vixml。",
        "translated_title": "大语言模型遇见极端多标签分类：规模扩展与多模态框架",
        "label": [],
        "label_reason": "聚焦XMC任务，非推荐系统核心问题",
        "relevance_score": 3,
        "novelty_score": 7,
        "novelty_reason": "结合视觉与大模型提升效率，有实用改进"
    },
    {
        "title": "Local Collaborative Filtering: A Collaborative Filtering Method that Utilizes Local Similarities among Users",
        "url": "http://arxiv.org/abs/2511.13166v1",
        "pub_date": "2025-11-17",
        "summary": "To leverage user behavior data from the Internet more effectively in recommender systems, this paper proposes a novel collaborative filtering (CF) method called Local Collaborative Filtering (LCF). LCF utilizes local similarities among users and integrates their data using the law of large numbers (LLN), thereby improving the utilization of user behavior data. Experiments are conducted on the Steam game dataset, and the results of LCF align with real-world needs.",
        "translated": "为更有效地利用互联网中的用户行为数据，本文提出了一种新颖的协同过滤（CF）方法，称为局部协同过滤（LCF）。LCF 利用用户间的局部相似性，并基于大数定律（LLN）整合其数据，从而提升用户行为数据的利用率。实验在 Steam 游戏数据集上进行，结果符合实际需求。",
        "translated_title": "局部协同过滤：一种利用用户间局部相似性的协同过滤方法",
        "label": [
            "精排（Ranking）",
            "通用推荐技术（General Recommendation Techniques）"
        ],
        "label_reason": "提出基于用户局部相似性的CF方法，用于提升推荐精度。",
        "relevance_score": 8,
        "novelty_score": 6,
        "novelty_reason": "改进传统CF方法，引入局部相似性与大数定律，属常规优化。"
    },
    {
        "title": "Region-Point Joint Representation for Effective Trajectory Similarity Learning",
        "url": "http://arxiv.org/abs/2511.13125v1",
        "pub_date": "2025-11-17",
        "summary": "Recent learning-based methods have reduced the computational complexity of traditional trajectory similarity computation, but state-of-the-art (SOTA) methods still fail to leverage the comprehensive spectrum of trajectory information for similarity modeling. To tackle this problem, we propose \\textbf{RePo}, a novel method that jointly encodes \\textbf{Re}gion-wise and \\textbf{Po}int-wise features to capture both spatial context and fine-grained moving patterns. For region-wise representation, the GPS trajectories are first mapped to grid sequences, and spatial context are captured by structural features and semantic context enriched by visual features. For point-wise representation, three lightweight expert networks extract local, correlation, and continuous movement patterns from dense GPS sequences. Then, a router network adaptively fuses the learned point-wise features, which are subsequently combined with region-wise features using cross-attention to produce the final trajectory embedding. To train RePo, we adopt a contrastive loss with hard negative samples to provide similarity ranking supervision. Experiment results show that RePo achieves an average accuracy improvement of 22.2\\% over SOTA baselines across all evaluation metrics.",
        "translated": "近期基于学习的方法已降低了传统轨迹相似度计算的计算复杂度，但当前最先进的（SOTA）方法仍未能充分利用轨迹信息的全面光谱以进行相似性建模。为解决这一问题，我们提出了一种新颖的方法 \\textbf{RePo}，该方法联合编码 \\textbf{Re}gion-wise 和 \\textbf{Po}int-wise 特征，以同时捕捉空间上下文与细粒度移动模式。对于区域级表示，GPS 轨迹首先被映射为网格序列，空间上下文通过结构特征捕获，并借助视觉特征增强语义上下文。对于点级表示，三个轻量专家网络从密集的 GPS 序列中提取局部、关联及连续运动模式。随后，一个路由网络自适应融合所学习到的点级特征，这些特征再通过交叉注意力机制与区域级特征结合，生成最终的轨迹嵌入。为训练 RePo，我们采用带有难负样本的对比损失，以提供相似性排序监督。实验结果表明，RePo 在所有评估指标上相较 SOTA 基线方法平均准确率提升了 22.2\\%。",
        "translated_title": "区域-点联合表示用于有效的轨迹相似性学习",
        "label": [
            "通用推荐技术",
            "负采样与对比学习"
        ],
        "label_reason": "轨迹相似性学习可用于推荐场景，如POI推荐。",
        "relevance_score": 7,
        "novelty_score": 8,
        "novelty_reason": "联合区域与点特征，引入路由网络与对比学习。"
    },
    {
        "title": "FGNet: Leveraging Feature-Guided Attention to Refine SAM2 for 3D EM Neuron Segmentation",
        "url": "http://arxiv.org/abs/2511.13063v1",
        "pub_date": "2025-11-17",
        "summary": "Accurate segmentation of neural structures in Electron Microscopy (EM) images is paramount for neuroscience. However, this task is challenged by intricate morphologies, low signal-to-noise ratios, and scarce annotations, limiting the accuracy and generalization of existing methods. To address these challenges, we seek to leverage the priors learned by visual foundation models on a vast amount of natural images to better tackle this task. Specifically, we propose a novel framework that can effectively transfer knowledge from Segment Anything 2 (SAM2), which is pre-trained on natural images, to the EM domain. We first use SAM2 to extract powerful, general-purpose features. To bridge the domain gap, we introduce a Feature-Guided Attention module that leverages semantic cues from SAM2 to guide a lightweight encoder, the Fine-Grained Encoder (FGE), in focusing on these challenging regions. Finally, a dual-affinity decoder generates both coarse and refined affinity maps. Experimental results demonstrate that our method achieves performance comparable to state-of-the-art (SOTA) approaches with the SAM2 weights frozen. Upon further fine-tuning on EM data, our method significantly outperforms existing SOTA methods. This study validates that transferring representations pre-trained on natural images, when combined with targeted domain-adaptive guidance, can effectively address the specific challenges in neuron segmentation.",
        "translated": "在电子显微镜（EM）图像中对神经结构进行精确分割，对于神经科学至关重要。然而，该任务面临形态复杂、信噪比低以及标注稀缺等挑战，限制了现有方法的准确性和泛化能力。为应对这些挑战，我们试图利用视觉基础模型在海量自然图像上学习到的先验知识，以更好地解决该问题。具体而言，我们提出了一种新颖框架，可有效将已在自然图像上预训练的Segment Anything 2（SAM2）的知识迁移到EM领域。首先，我们使用SAM2提取强大的通用特征；为弥合领域差距，我们引入了一个特征引导注意力模块，该模块利用SAM2提供的语义线索，引导轻量级编码器——细粒度编码器（Fine-Grained Encoder, FGE）聚焦于这些具有挑战性的区域；最终，一个双亲和度解码器生成粗略与精细的亲和度图。实验结果表明，在冻结SAM2权重的情况下，我们的方法已达到现有最先进（SOTA）方法的性能水平；而在进一步在EM数据上微调后，我们的方法显著超越现有SOTA方法。本研究验证了：当结合有针对性的领域自适应引导时，从自然图像上预训练得到的表示能够有效应对神经元分割中的特定挑战。",
        "translated_title": "FGNet：利用特征引导注意力优化SAM2用于三维电镜神经元分割",
        "label": [],
        "label_reason": "论文聚焦图像分割，与推荐系统无关。",
        "relevance_score": 1,
        "novelty_score": 3,
        "novelty_reason": "方法为医学图像处理，无推荐相关创新。"
    },
    {
        "title": "Dimension vs. Precision: A Comparative Analysis of Autoencoders and Quantization for Efficient Vector Retrieval on BEIR SciFact",
        "url": "http://arxiv.org/abs/2511.13057v1",
        "pub_date": "2025-11-17",
        "summary": "Dense retrieval models have become a standard for state-of-the-art information retrieval. However, their high-dimensional, high-precision (float32) vector embeddings create significant storage and memory challenges for real-world deployment. To address this, we conduct a rigorous empirical study on the BEIR SciFact benchmark, evaluating the trade-offs between two primary compression strategies: (1) Dimensionality Reduction via deep Autoencoders (AE), reducing original 384-dim vectors to latent spaces from 384 down to 12, and (2) Precision Reduction via Quantization (float16, int8, and binary). We systematically compare each method by measuring the \"performance loss\" (or gain) relative to a float32 baseline across a full suite of retrieval metrics (NDCG, MAP, MRR, Recall, Precision) at various k cutoffs. Our results show that int8 scalar quantization provides the most effective \"sweet spot,\" achieving a 4x compression with a negligible [~1-2%] drop in nDCG@10. In contrast, Autoencoders show a graceful degradation but suffer a more significant performance loss at equivalent 4x compression ratios (AE-96). binary quantization was found to be unsuitable for this task due to catastrophic performance drops. This work provides a practical guide for deploying efficient, high-performance retrieval systems.",
        "translated": "密集检索模型已成为信息检索领域最先进方法的标准。然而，其高维、高精度（float32）的向量嵌入在实际部署中会带来显著的存储与内存挑战。为解决此问题，我们在 BEIR SciFact 数据集上开展了严谨的实证研究，评估两种主要压缩策略的权衡：(1) 通过深度自编码器（AE）进行降维，将原始 384 维向量压缩至潜空间，维度从 384 下降至 12；(2) 通过量化实现精度降低（float16、int8 和二值化）。我们系统性地比较了每种方法，在多个 k 截断点下，针对完整的一组检索指标（NDCG、MAP、MRR、Recall、Precision），测量其相对于 float32 基线的“性能损失”（或增益）。实验结果表明，int8 标量量化提供了最有效的“最佳折中点”，实现了 4 倍压缩，同时 nDCG@10 的下降幅度极小（约 1-2%）。相比之下，自编码器虽表现出平滑退化趋势，但在等效 4 倍压缩率下（AE-96）性能损失更为显著。二值量化则因性能急剧下降而不适合本任务。本工作为部署高效、高性能的检索系统提供了实用指导。",
        "translated_title": "维度与精度：在 BEIR SciFact 上对自编码器与量化方法进行高效向量检索的对比分析",
        "label": [
            "通用推荐技术"
        ],
        "label_reason": "研究向量检索压缩，属信息检索范畴，非推荐系统核心",
        "relevance_score": 4,
        "novelty_score": 6,
        "novelty_reason": "常规压缩方法比较，无推荐系统创新"
    },
    {
        "title": "Mitigating Recommendation Biases via Group-Alignment and Global-Uniformity in Representation Learning",
        "url": "http://arxiv.org/abs/2511.13041v1",
        "pub_date": "2025-11-17",
        "summary": "Collaborative Filtering~(CF) plays a crucial role in modern recommender systems, leveraging historical user-item interactions to provide personalized suggestions. However, CF-based methods often encounter biases due to imbalances in training data. This phenomenon makes CF-based methods tend to prioritize recommending popular items and performing unsatisfactorily on inactive users. Existing works address this issue by rebalancing training samples, reranking recommendation results, or making the modeling process robust to the bias. Despite their effectiveness, these approaches can compromise accuracy or be sensitive to weighting strategies, making them challenging to train. In this paper, we deeply analyze the causes and effects of the biases and propose a framework to alleviate biases in recommendation from the perspective of representation distribution, namely Group-Alignment and Global-Uniformity Enhanced Representation Learning for Debiasing Recommendation (AURL). Specifically, we identify two significant problems in the representation distribution of users and items, namely group-discrepancy and global-collapse. These two problems directly lead to biases in the recommendation results. To this end, we propose two simple but effective regularizers in the representation space, respectively named group-alignment and global-uniformity. The goal of group-alignment is to bring the representation distribution of long-tail entities closer to that of popular entities, while global-uniformity aims to preserve the information of entities as much as possible by evenly distributing representations. Our method directly optimizes both the group-alignment and global-uniformity regularization terms to mitigate recommendation biases. Extensive experiments on three real datasets and various recommendation backbones verify the superiority of our proposed framework.",
        "translated": "协同过滤（CF）在现代推荐系统中扮演着至关重要的角色，其通过历史用户-物料交互数据提供个性化推荐。然而，基于CF的方法常因训练数据分布不均衡而产生偏差，导致倾向于优先推荐热门物料并对低活跃度用户表现不佳。现有研究通过重采样训练样本、重排序推荐结果或使建模过程对偏差鲁棒等方式缓解该问题。尽管这些方法有效，但往往牺牲准确性或对权重策略敏感，难以稳定训练。本文深入分析偏差的成因与影响，并从表示分布角度提出一种减轻推荐偏差的框架，命名为“面向去偏的分组对齐与全局均匀性增强表示学习”（AURL）。具体而言，我们识别出用户和物料表示分布中存在的两个关键问题：群体差异性（group-discrepancy）与全局坍缩（global-collapse），二者直接导致推荐结果出现偏差。为此，我们在表示空间中分别设计了两种简单而有效的正则化项，名为“分组对齐”（group-alignment）与“全局均匀性”（global-uniformity）。其中，分组对齐旨在使长尾实体的表示分布更贴近热门实体；全局均匀性则通过均匀分布表示，尽可能保留实体信息。我们的方法直接优化上述两项正则化项，以缓解推荐偏差。在三个真实数据集及多种推荐骨干模型上的大量实验验证了所提框架的有效性。",
        "translated_title": "通过表示学习中的组对齐与全局均匀性缓解推荐偏差",
        "label": [
            "精排",
            "推荐系统公平性/可解释性"
        ],
        "label_reason": "针对推荐偏差提出表示学习优化方法，适用于精排环节。",
        "relevance_score": 9,
        "novelty_score": 8,
        "novelty_reason": "创新性引入组对齐与全局均匀性正则化解决偏差问题。"
    },
    {
        "title": "Personalized Federated Recommendation With Knowledge Guidance",
        "url": "http://arxiv.org/abs/2511.12959v1",
        "pub_date": "2025-11-17",
        "summary": "Federated Recommendation (FedRec) has emerged as a key paradigm for building privacy-preserving recommender systems. However, existing FedRec models face a critical dilemma: memory-efficient single-knowledge models suffer from a suboptimal knowledge replacement practice that discards valuable personalization, while high-performance dual-knowledge models are often too memory-intensive for practical on-device deployment. We propose Federated Recommendation with Knowledge Guidance (FedRKG), a model-agnostic framework that resolves this dilemma. The core principle, Knowledge Guidance, avoids full replacement and instead fuses global knowledge into preserved local embeddings, attaining the personalization benefits of dual-knowledge within a single-knowledge memory footprint. Furthermore, we introduce Adaptive Guidance, a fine-grained mechanism that dynamically modulates the intensity of this guidance for each user-item interaction, overcoming the limitations of static fusion methods. Extensive experiments on benchmark datasets demonstrate that FedRKG significantly outperforms state-of-the-art methods, validating the effectiveness of our approach. The code is available at https://github.com/Jaehyung-Lim/fedrkg.",
        "translated": "联邦推荐（FedRec）已成为构建隐私保护推荐系统的关键范式。然而，现有的 FedRec 模型面临一个关键困境：内存效率高的单知识模型因采用次优的知识替换策略而丢失宝贵的个性化能力；而高性能的双知识模型则往往因内存开销过大，难以在设备端实际部署。我们提出联邦推荐与知识引导（FedRKG），这是一种模型无关的框架，旨在解决上述困境。其核心原则“知识引导”避免了完全替换，而是将全局知识融合进保留的局部嵌入中，在单知识内存开销下实现双知识模型的个性化优势。此外，我们引入自适应引导机制，作为一种细粒度机制，动态调节对每个用户-物料交互的引导强度，克服了静态融合方法的局限性。在多个基准数据集上的大量实验表明，FedRKG 显著优于现有最先进方法，验证了我们方法的有效性。代码开源地址为 https://github.com/Jaehyung-Lim/fedrkg。",
        "translated_title": "基于知识引导的个性化联邦推荐",
        "label": [
            "跨域/联邦推荐",
            "通用推荐技术"
        ],
        "label_reason": "解决联邦推荐中个性化与内存效率矛盾",
        "relevance_score": 9,
        "novelty_score": 8,
        "novelty_reason": "提出知识引导机制动态融合全局与局部知识"
    },
    {
        "title": "Can We Predict the Next Question? A Collaborative Filtering Approach to Modeling User Behavior",
        "url": "http://arxiv.org/abs/2511.12949v1",
        "pub_date": "2025-11-17",
        "summary": "In recent years, large language models (LLMs) have excelled in language understanding and generation, powering advanced dialogue and recommendation systems. However, a significant limitation persists: these systems often model user preferences statically, failing to capture the dynamic and sequential nature of interactive behaviors. The sequence of a user's historical questions provides a rich, implicit signal of evolving interests and cognitive patterns, yet leveraging this temporal data for predictive tasks remains challenging due to the inherent disconnect between language modeling and behavioral sequence modeling.   To bridge this gap, we propose a Collaborative Filtering-enhanced Question Prediction (CFQP) framework. CFQP dynamically models evolving user-question interactions by integrating personalized memory modules with graph-based preference propagation. This dual mechanism allows the system to adaptively learn from user-specific histories while refining predictions through collaborative signals from similar users. Experimental results demonstrate that our approach effectively generates agents that mimic real-user questioning patterns, highlighting its potential for building proactive and adaptive dialogue systems.",
        "translated": "近年来，大语言模型（LLM）在语言理解和生成方面表现卓越，推动了先进对话与推荐系统的研发。然而，其仍存在一个显著局限：这些系统往往静态建模用户偏好，难以捕捉交互行为的动态性和序列性特征。用户历史提问的序列蕴含了丰富的隐式信号，反映了兴趣演化与认知模式的变化，但由于语言建模与行为序列建模之间固有的割裂，如何有效利用此类时序数据进行预测任务仍具挑战。  \n\n为弥合此差距，我们提出一种基于协同过滤增强的问答预测框架（Collaborative Filtering-enhanced Question Prediction, CFQP）。CFQP 通过融合个性化记忆模块与基于图的偏好传播机制，动态建模用户-问题交互的演化过程。该双机制使系统既能自适应学习用户专属历史，又能借助相似用户的协同信号优化预测效果。实验结果表明，我们的方法能有效生成模仿真实用户提问模式的代理，凸显其在构建主动、自适应对话系统方面的潜力。",
        "translated_title": "我们能否预测下一个问题？一种基于协同过滤的用户行为建模方法",
        "label": [
            "序列推荐",
            "通用推荐技术"
        ],
        "label_reason": "建模用户行为序列，但未聚焦推荐核心环节。",
        "relevance_score": 6,
        "novelty_score": 5,
        "novelty_reason": "整合协同过滤与记忆模块，但非推荐专用创新。"
    },
    {
        "title": "A Plug-and-Play Spatially-Constrained Representation Enhancement Framework for Local-Life Recommendation",
        "url": "http://arxiv.org/abs/2511.12947v1",
        "pub_date": "2025-11-17",
        "summary": "Local-life recommendation have witnessed rapid growth, providing users with convenient access to daily essentials. However, this domain faces two key challenges: (1) spatial constraints, driven by the requirements of the local-life scenario, where items are usually shown only to users within a limited geographic area, indirectly reducing their exposure probability; and (2) long-tail sparsity, where few popular items dominate user interactions, while many high-quality long-tail items are largely overlooked due to imbalanced interaction opportunities. Existing methods typically adopt a user-centric perspective, such as modeling spatial user preferences or enhancing long-tail representations with collaborative filtering signals. However, we argue that an item-centric perspective is more suitable for this domain, focusing on enhancing long-tail items representation that align with the spatially-constrained characteristics of local lifestyle services. To tackle this issue, we propose ReST, a Plug-And-Play Spatially-Constrained Representation Enhancement Framework for Long-Tail Local-Life Recommendation. Specifically, we first introduce a Meta ID Warm-up Network, which initializes fundamental ID representations by injecting their basic attribute-level semantic information. Subsequently, we propose a novel Spatially-Constrained ID Representation Enhancement Network (SIDENet) based on contrastive learning, which incorporates two efficient strategies: a spatially-constrained hard sampling strategy and a dynamic representation alignment strategy. This design adaptively identifies weak ID representations based on their attribute-level information during training. It additionally enhances them by capturing latent item relationships within the spatially-constrained characteristics of local lifestyle services, while preserving compatibility with popular items.",
        "translated": "本地生活推荐系统近年来发展迅速，为用户提供便捷的日常必需品访问服务。然而，该领域面临两大核心挑战：（1）空间约束问题，由本地生活场景的需求驱动，物品通常仅向限定地理区域内的用户展示，间接降低了其曝光概率；（2）长尾稀疏性问题，少数热门物品主导用户交互行为，而大量高质量长尾物品因交互机会分布不均而被严重忽视。现有方法通常采用以用户为中心的视角，例如建模空间用户偏好或利用协同过滤信号增强长尾物品表征。然而，我们认为，对于本领域而言，以物品为中心的视角更为合适，应聚焦于提升与本地生活服务空间约束特性相契合的长尾物品表征能力。为解决此问题，我们提出 ReST，一种面向长尾本地生活推荐的“即插即用”空间约束表征增强框架。具体而言，我们首先引入元 ID 预热网络（Meta ID Warm-up Network），通过注入基础属性层面语义信息初始化基本 ID 表征；随后，我们提出一种基于对比学习的新型空间约束 ID 表征增强网络（SIDENet），融合两项高效策略：空间约束硬采样策略与动态表征对齐策略。该设计在训练过程中根据物品属性信息自适应识别弱表征，并通过捕捉本地生活服务空间约束特性下的潜在物品关系加以增强，同时保持与热门物品的兼容性。",
        "translated_title": "一种即插即用的空间约束表示增强框架用于本地生活推荐",
        "label": [
            "召回",
            "精排",
            "负采样与对比学习"
        ],
        "label_reason": "聚焦本地生活推荐中的空间约束与长尾问题，设计新框架提升召回/排序效果。",
        "relevance_score": 9,
        "novelty_score": 8,
        "novelty_reason": "提出SIDENet结合空间约束对比学习，创新性增强长尾物品表征。"
    },
    {
        "title": "AIF: Asynchronous Inference Framework for Cost-Effective Pre-Ranking",
        "url": "http://arxiv.org/abs/2511.12934v1",
        "pub_date": "2025-11-17",
        "summary": "In industrial recommendation systems, pre-ranking models based on deep neural networks (DNNs) commonly adopt a sequential execution framework: feature fetching and model forward computation are triggered only after receiving candidates from the upstream retrieval stage. This design introduces inherent bottlenecks, including redundant computations of identical users/items and increased latency due to strictly sequential operations, which jointly constrain the model's capacity and system efficiency. To address these limitations, we propose the Asynchronous Inference Framework (AIF), a cost-effective computational architecture that decouples interaction-independent components, those operating within a single user or item, from real-time prediction. AIF reorganizes the model inference process by performing user-side computations in parallel with the retrieval stage and conducting item-side computations in a nearline manner. This means that interaction-independent components are calculated just once and completed before the real-time prediction phase of the pre-ranking stage. As a result, AIF enhances computational efficiency and reduces latency, freeing up resources to significantly improve the feature set and model architecture of interaction-independent components. Moreover, we delve into model design within the AIF framework, employing approximated methods for interaction-dependent components in online real-time predictions. By co-designing both the framework and the model, our solution achieves notable performance gains without significantly increasing computational and latency costs. This has enabled the successful deployment of AIF in the Taobao display advertising system.",
        "translated": "在工业推荐系统中，基于深度神经网络（DNNs）的粗排模型通常采用顺序执行框架：特征获取与模型前向计算仅在从上游召回阶段接收到候选物料后才被触发。该设计引入了固有的瓶颈，包括相同用户/物料重复计算以及因严格顺序操作导致的延迟增加，二者共同制约了模型能力与系统效率。为解决这些局限性，我们提出异步推理框架（Asynchronous Inference Framework, AIF），这是一种低成本的计算架构，将交互无关组件——即在单个用户或物料内独立运行的部分——与实时预测解耦。AIF通过在召回阶段并行执行用户侧计算、以近线方式处理物料侧计算，重新组织模型推理流程。这意味着交互无关组件仅计算一次，并在粗排阶段实时预测阶段之前完成。因此，AIF提升了计算效率、降低了延迟，释放出资源用于显著增强交互无关组件的特征集与模型架构。此外，我们在AIF框架内深入探讨模型设计，针对在线实时预测部分采用近似方法处理交互相关组件。通过框架与模型协同设计，我们的方案实现了显著性能提升，而无需大幅增加计算开销和延迟成本。此方案已在淘宝展示广告系统中成功部署。",
        "translated_title": "AIF：一种低成本粗排的异步推理框架",
        "label": [
            "粗排（Pre-ranking）"
        ],
        "label_reason": "聚焦预排序阶段异步推理架构优化，直接提升推荐系统效率。",
        "relevance_score": 9,
        "novelty_score": 7,
        "novelty_reason": "提出异步框架解耦计算，巧妙结合模型与系统设计创新。"
    },
    {
        "title": "Tokenize Once, Recommend Anywhere: Unified Item Tokenization for Multi-domain LLM-based Recommendation",
        "url": "http://arxiv.org/abs/2511.12922v1",
        "pub_date": "2025-11-17",
        "summary": "Large language model (LLM)-based recommender systems have achieved high-quality performance by bridging the discrepancy between the item space and the language space through item tokenization. However, existing item tokenization methods typically require training separate models for each item domain, limiting generalization. Moreover, the diverse distributions and semantics across item domains make it difficult to construct a unified tokenization that preserves domain-specific information. To address these challenges, we propose UniTok, a Unified item Tokenization framework that integrates our own mixture-of-experts (MoE) architecture with a series of codebooks to convert items into discrete tokens, enabling scalable tokenization while preserving semantic information across multiple item domains. Specifically, items from different domains are first projected into a unified latent space through a shared encoder. They are then routed to domain-specific experts to capture the unique semantics, while a shared expert, which is always active, encodes common knowledge transferable across domains. Additionally, to mitigate semantic imbalance across domains, we present a mutual information calibration mechanism, which guides the model towards retaining similar levels of semantic information for each domain. Comprehensive experiments on wide-ranging real-world datasets demonstrate that the proposed UniTok framework is (a) highly effective: achieving up to 51.89% improvements over strong benchmarks, (b) theoretically sound: showing the analytical validity of our architectural design and optimization; and (c) highly generalizable: demonstrating robust performance across diverse domains without requiring per-domain retraining, a capability not supported by existing baselines.",
        "translated": "基于大语言模型（LLM）的推荐系统通过将物料空间与语言空间之间的差异桥接，借助物料分词技术实现了高质量的性能。然而，现有物料分词方法通常需要为每个物料领域单独训练独立模型，限制了其泛化能力。此外，不同物料领域间分布与语义的多样性使得构建一个统一的分词方案以保留各领域特定信息变得困难。为解决上述挑战，我们提出 UniTok，一种统一物料分词框架，该框架结合了我们设计的混合专家（MoE）架构及一系列代码本，将物料转换为离散令牌，从而在保持多领域语义信息的同时实现可扩展的分词能力。具体而言，来自不同领域的物料首先通过共享编码器投影至统一潜在空间，随后被路由至领域特定专家以捕获独特语义，同时一个始终激活的共享专家负责编码跨领域迁移的知识。此外，为缓解各领域间语义不平衡的问题，我们提出了一种互信息校准机制，引导模型在各领域间保留相似层次的语义信息。在广泛真实世界数据集上的全面实验表明，所提出的 UniTok 框架具备：(a) 高效性：相较强基线模型最高提升达 51.89%；(b) 理论严谨性：展示了架构设计与优化的分析有效性；以及 (c) 高泛化性：在无需为各领域重新训练的前提下，于多样化领域中均展现出稳健表现，这是现有基线方法所不具备的能力。",
        "translated_title": "一次 Token 化，随处推荐：面向多领域大语言模型推荐的统一物料 Token 化",
        "label": [
            "LLM生成式推荐",
            "通用推荐技术"
        ],
        "label_reason": "提出统一Token化框架，适配多域LLM推荐",
        "relevance_score": 10,
        "novelty_score": 9,
        "novelty_reason": "首创MoE+Codebook架构解决跨域token化难题"
    },
    {
        "title": "Auditing Google's AI Overviews and Featured Snippets: A Case Study on Baby Care and Pregnancy",
        "url": "http://arxiv.org/abs/2511.12920v1",
        "pub_date": "2025-11-17",
        "summary": "Google Search increasingly surfaces AI-generated content through features like AI Overviews (AIO) and Featured Snippets (FS), which users frequently rely on despite having no control over their presentation. Through a systematic algorithm audit of 1,508 real baby care and pregnancy-related queries, we evaluate the quality and consistency of these information displays. Our robust evaluation framework assesses multiple quality dimensions, including answer consistency, relevance, presence of medical safeguards, source categories, and sentiment alignment. Our results reveal concerning gaps in information consistency, with information in AIO and FS displayed on the same search result page being inconsistent with each other in 33% of cases. Despite high relevance scores, both features critically lack medical safeguards (present in just 11% of AIO and 7% of FS responses). While health and wellness websites dominate source categories for both, AIO and FS, FS also often link to commercial sources. These findings have important implications for public health information access and demonstrate the need for stronger quality controls in AI-mediated health information. Our methodology provides a transferable framework for auditing AI systems across high-stakes domains where information quality directly impacts user well-being.",
        "translated": "谷歌搜索正越来越多地通过AI摘要（AIO）和精选片段（FS）等功能展示由人工智能生成的内容，尽管用户对此类内容的呈现方式并无控制权，却频繁依赖这些功能。通过对1,508条真实的婴儿护理与孕期相关查询进行系统性算法审计，我们评估了这些信息展示的质量与一致性。我们的稳健评估框架从多个质量维度进行衡量，包括答案一致性、相关性、是否包含医疗安全机制、来源类别分布及情感倾向匹配度。结果显示，存在令人担忧的信息不一致问题：在33%的案例中，同一搜索结果页面上AIO与FS所呈现的信息彼此矛盾。尽管相关性评分较高，但这两项功能均严重缺乏医疗安全措施（仅分别有11%和7%的AIO与FS响应包含此类保障）。虽然健康与福祉类网站在两类功能的来源类别中占据主导地位，但FS亦常链接至商业来源。上述发现对公众获取健康信息具有重要影响，并表明在AI中介的健康信息领域亟需强化质量管控。我们的方法论提供了一种可迁移的框架，可用于高风险领域的AI系统审计——其中信息质量直接影响用户福祉。",
        "translated_title": "审计谷歌AI摘要与精选片段：以育儿与孕期为主题的研究案例",
        "label": [],
        "label_reason": "研究对象为搜索结果质量审计，非推荐系统核心环节。",
        "relevance_score": 2,
        "novelty_score": 3,
        "novelty_reason": "方法通用，未针对推荐系统提出创新性解决方案。"
    },
    {
        "title": "Back to Basics: Let Denoising Generative Models Denoise",
        "url": "http://arxiv.org/abs/2511.13720v1",
        "pub_date": "2025-11-17",
        "summary": "Today's denoising diffusion models do not \"denoise\" in the classical sense, i.e., they do not directly predict clean images. Rather, the neural networks predict noise or a noised quantity. In this paper, we suggest that predicting clean data and predicting noised quantities are fundamentally different. According to the manifold assumption, natural data should lie on a low-dimensional manifold, whereas noised quantities do not. With this assumption, we advocate for models that directly predict clean data, which allows apparently under-capacity networks to operate effectively in very high-dimensional spaces. We show that simple, large-patch Transformers on pixels can be strong generative models: using no tokenizer, no pre-training, and no extra loss. Our approach is conceptually nothing more than \"$\\textbf{Just image Transformers}$\", or $\\textbf{JiT}$, as we call it. We report competitive results using JiT with large patch sizes of 16 and 32 on ImageNet at resolutions of 256 and 512, where predicting high-dimensional noised quantities can fail catastrophically. With our networks mapping back to the basics of the manifold, our research goes back to basics and pursues a self-contained paradigm for Transformer-based diffusion on raw natural data.",
        "translated": "当前的去噪扩散模型并不以传统意义上的“去噪”方式工作，即它们并不直接预测干净图像。相反，神经网络预测的是噪声或被噪声污染后的量。本文我们认为，预测干净数据与预测受噪声污染的量在本质上是不同的。根据流形假设，自然数据应位于一个低维流形上，而受噪声污染的量则不具备这一性质。基于此假设，我们主张采用直接预测干净数据的模型，这使得原本看似容量不足的网络能够在极高维空间中有效运行。我们表明，仅使用像素级的大块Transformer（无需分词器、无需预训练、无需额外损失函数）即可构建强大的生成模型。我们的方法在概念层面无非是“纯粹图像Transformer”，我们称之为JiT。我们在ImageNet数据集上，以256和512分辨率，采用16和32大小的大块patch进行实验，结果表现具有竞争力；而在这些条件下，若尝试预测高维噪声量，则可能造成灾难性失败。通过将网络映射回流形的基本结构，我们的研究回归基础，并致力于为基于Transformer的原始自然数据扩散模型建立一个自洽的方法论框架。",
        "translated_title": "回归基础：让去噪生成模型执行去噪任务",
        "label": [
            "图像去噪",
            "图像恢复"
        ],
        "label_reason": "提出直接预测干净图像的扩散模型，属低层图像恢复任务。",
        "relevance_score": 7,
        "novelty_score": 9,
        "novelty_reason": "首次主张直接预测干净数据，理论创新显著。"
    },
    {
        "title": "Scaling Spatial Intelligence with Multimodal Foundation Models",
        "url": "http://arxiv.org/abs/2511.13719v1",
        "pub_date": "2025-11-17",
        "summary": "Despite remarkable progress, multimodal foundation models still exhibit surprising deficiencies in spatial intelligence. In this work, we explore scaling up multimodal foundation models to cultivate spatial intelligence within the SenseNova-SI family, built upon established multimodal foundations including visual understanding models (i.e., Qwen3-VL and InternVL3) and unified understanding and generation models (i.e., Bagel). We take a principled approach to constructing high-performing and robust spatial intelligence by systematically curating SenseNova-SI-8M: eight million diverse data samples under a rigorous taxonomy of spatial capabilities. SenseNova-SI demonstrates unprecedented performance across a broad range of spatial intelligence benchmarks: 68.7% on VSI-Bench, 43.3% on MMSI, 85.6% on MindCube, 54.6% on ViewSpatial, and 50.1% on SITE, while maintaining strong general multimodal understanding (e.g., 84.9% on MMBench-En). More importantly, we analyze the impact of data scaling, discuss early signs of emergent generalization capabilities enabled by diverse data training, analyze the risk of overfitting and language shortcuts, present a preliminary study on spatial chain-of-thought reasoning, and validate the potential downstream application. SenseNova-SI is an ongoing project, and this report will be updated continuously. All newly trained multimodal foundation models are publicly released to facilitate further research in this direction.",
        "translated": "尽管取得了显著进展，多模态基础模型在空间智能方面仍表现出令人惊讶的不足。在本研究中，我们探索通过扩展多模态基础模型，以在 SenseNova-SI 系列中培养空间智能，该系列构建于已有的多模态基础之上，包括视觉理解模型（如 Qwen3-VL 和 InternVL3）以及统一的理解与生成模型（如 Bagel）。我们采取系统性方法构建高性能、鲁棒的空间智能能力，通过严谨的空间能力分类体系，精心策划了包含八百万多样数据样本的 SenseNova-SI-8M 数据集。SenseNova-SI 在多个空间智能基准测试中展现出前所未有的性能：在 VSI-Bench 上达到 68.7%，在 MMSI 上为 43.3%，在 MindCube 上为 85.6%，在 ViewSpatial 上为 54.6%，在 SITE 上为 50.1%；同时保持强大的通用多模态理解能力（例如在 MMBench-En 上达到 84.9%）。更重要的是，我们分析了数据规模扩增的影响，探讨了多样化训练数据所引发的早期泛化能力迹象，评估了过拟合和语言捷径的风险，初步研究了空间链式思维推理，并验证了其潜在下游应用场景。SenseNova-SI 是一个持续进行中的项目，本报告将不断更新。所有新训练的多模态基础模型均已公开发布，以促进该方向的进一步研究。",
        "translated_title": "以多模态基础模型扩展空间智能",
        "label": [],
        "label_reason": "研究聚焦空间智能，非图像像素级处理",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "数据规模扩展与基准提升，无新架构或方法"
    },
    {
        "title": "Segment Anything Across Shots: A Method and Benchmark",
        "url": "http://arxiv.org/abs/2511.13715v1",
        "pub_date": "2025-11-17",
        "summary": "This work focuses on multi-shot semi-supervised video object segmentation (MVOS), which aims at segmenting the target object indicated by an initial mask throughout a video with multiple shots. The existing VOS methods mainly focus on single-shot videos and struggle with shot discontinuities, thereby limiting their real-world applicability. We propose a transition mimicking data augmentation strategy (TMA) which enables cross-shot generalization with single-shot data to alleviate the severe annotated multi-shot data sparsity, and the Segment Anything Across Shots (SAAS) model, which can detect and comprehend shot transitions effectively. To support evaluation and future study in MVOS, we introduce Cut-VOS, a new MVOS benchmark with dense mask annotations, diverse object categories, and high-frequency transitions. Extensive experiments on YouMVOS and Cut-VOS demonstrate that the proposed SAAS achieves state-of-the-art performance by effectively mimicking, understanding, and segmenting across complex transitions. The code and datasets are released at https://henghuiding.com/SAAS/.",
        "translated": "本工作聚焦于多镜头半监督视频目标分割（MVOS），旨在利用初始掩码在整个包含多个镜头的视频中持续对目标物体进行分割。现有VOS方法主要针对单镜头视频，难以处理镜头间断点，从而限制了其在现实场景中的适用性。我们提出一种模仿过渡的数据增强策略（TMA），通过单镜头数据实现跨镜头泛化，以缓解标注密集多镜头数据稀缺的问题；并设计了Segment Anything Across Shots（SAAS）模型，能够有效检测与理解镜头过渡。为支持MVOS的评估与未来研究，我们引入了一个新的MVOS基准数据集Cut-VOS，该数据集提供密集掩码标注、多样化的物体类别及高频镜头切换。在YouMVOS和Cut-VOS上的大量实验表明，所提出的SAAS模型能通过有效模拟、理解并分割复杂镜头过渡，在性能上达到当前最优水平。代码与数据集已开源于 https://henghuiding.com/SAAS/。",
        "translated_title": "跨镜头分割任意内容：一种方法与基准",
        "label": [],
        "label_reason": "任务为视频目标分割，属高阶视觉任务",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "提出新策略与模型提升跨镜头分割性能"
    },
    {
        "title": "UnSAMv2: Self-Supervised Learning Enables Segment Anything at Any Granularity",
        "url": "http://arxiv.org/abs/2511.13714v1",
        "pub_date": "2025-11-17",
        "summary": "The Segment Anything Model (SAM) family has become a widely adopted vision foundation model, but its ability to control segmentation granularity remains limited. Users often need to refine results manually - by adding more prompts or selecting from pre-generated masks - to achieve the desired level of detail. This process can be ambiguous, as the same prompt may correspond to several plausible masks, and collecting dense annotations across all granularities is prohibitively expensive, making supervised solutions infeasible. To address this limitation, we introduce UnSAMv2, which enables segment anything at any granularity without human annotations. UnSAMv2 extends the divide-and-conquer strategy of UnSAM by discovering abundant mask-granularity pairs and introducing a novel granularity control embedding that enables precise, continuous control over segmentation scale. Remarkably, with only $6$K unlabeled images and $0.02\\%$ additional parameters, UnSAMv2 substantially enhances SAM-2, achieving segment anything at any granularity across interactive, whole-image, and video segmentation tasks. Evaluated on over $11$ benchmarks, UnSAMv2 improves $\\text{NoC}_{90}$ (5.69 $\\rightarrow$ 4.75), 1-IoU (58.0 $\\rightarrow$ 73.1), and $\\text{AR}_{1000}$ (49.6 $\\rightarrow$ 68.3), showing that small amounts of unlabeled data with a granularity-aware self-supervised learning method can unlock the potential of vision foundation models.",
        "translated": "Segment Anything Model (SAM) 系列已成为广泛采用的视觉基础模型，但其对分割粒度的控制能力仍显有限。用户通常需手动优化结果——通过添加更多提示或从预生成掩码中选择——以达到期望的细节程度。这一过程往往存在歧义，因为同一提示可能对应多个合理的掩码；同时，在所有粒度级别上收集密集标注成本极高，导致监督式解决方案难以实施。为解决此局限性，我们提出 UnSAMv2，该方法可在无需人工标注的情况下实现任意粒度的分割。UnSAMv2 在 UnSAM 的“分而治之”策略基础上扩展，通过发现大量掩码-粒度配对，并引入一种新颖的粒度控制嵌入，从而实现对分割尺度的精确连续调控。值得注意的是，仅使用 $6$K 张无标签图像和 $0.02\\%$ 的额外参数，UnSAMv2 显著提升了 SAM-2 的性能，在交互式、整图及视频分割任务中均可实现任意粒度的分割。在超过 $11$ 个基准数据集上的评估表明，UnSAMv2 将 $\\text{NoC}_{90}$ 提升至（5.69 → 4.75），1-IoU 上升至（58.0 → 73.1），$\\text{AR}_{1000}$ 提高至（49.6 → 68.3），证明了少量无标签数据结合粒度感知自监督学习方法能够充分释放视觉基础模型的潜力。",
        "translated_title": "UnSAMv2：自监督学习实现任意粒度的分割万物",
        "label": [],
        "label_reason": "属于高阶分割任务，非像素级图像恢复。",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "提出自监督粒度控制机制，显著提升分割精度。"
    },
    {
        "title": "Free-Form Scene Editor: Enabling Multi-Round Object Manipulation like in a 3D Engine",
        "url": "http://arxiv.org/abs/2511.13713v1",
        "pub_date": "2025-11-17",
        "summary": "Recent advances in text-to-image (T2I) diffusion models have significantly improved semantic image editing, yet most methods fall short in performing 3D-aware object manipulation. In this work, we present FFSE, a 3D-aware autoregressive framework designed to enable intuitive, physically-consistent object editing directly on real-world images. Unlike previous approaches that either operate in image space or require slow and error-prone 3D reconstruction, FFSE models editing as a sequence of learned 3D transformations, allowing users to perform arbitrary manipulations, such as translation, scaling, and rotation, while preserving realistic background effects (e.g., shadows, reflections) and maintaining global scene consistency across multiple editing rounds. To support learning of multi-round 3D-aware object manipulation, we introduce 3DObjectEditor, a hybrid dataset constructed from simulated editing sequences across diverse objects and scenes, enabling effective training under multi-round and dynamic conditions. Extensive experiments show that the proposed FFSE significantly outperforms existing methods in both single-round and multi-round 3D-aware editing scenarios.",
        "translated": "近年来，文本到图像（T2I）扩散模型在语义图像编辑方面取得了显著进展，但大多数方法在执行具备3D感知能力的对象操控时仍显不足。在本工作中，我们提出了FFSE，一种面向3D感知的自回归框架，旨在使用户能够直观且物理一致地直接对真实世界图像中的对象进行编辑。与以往仅在图像空间中操作或需要缓慢且易出错的3D重建的方法不同，FFSE将编辑建模为一系列学习得到的3D变换序列，允许用户执行任意操作，如平移、缩放和旋转，同时保持逼真的背景效果（如阴影、反射），并在多轮编辑过程中维持全局场景一致性。为支持多轮3D感知对象操控的学习，我们引入了3DObjectEditor数据集——该数据集由跨多种物体与场景的模拟编辑序列混合构建而成，能够在多轮及动态条件下实现有效的训练。大量实验表明，所提出的FFSE在单轮和多轮3D感知编辑任务中均显著优于现有方法。",
        "translated_title": "自由形式场景编辑器：实现如3D引擎般支持多轮对象操作",
        "label": [],
        "label_reason": "目标为3D场景编辑，属high-level任务",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "提出新框架支持多轮3D操作，具显著改进"
    },
    {
        "title": "TiViBench: Benchmarking Think-in-Video Reasoning for Video Generative Models",
        "url": "http://arxiv.org/abs/2511.13704v1",
        "pub_date": "2025-11-17",
        "summary": "The rapid evolution of video generative models has shifted their focus from producing visually plausible outputs to tackling tasks requiring physical plausibility and logical consistency. However, despite recent breakthroughs such as Veo 3's chain-of-frames reasoning, it remains unclear whether these models can exhibit reasoning capabilities similar to large language models (LLMs). Existing benchmarks predominantly evaluate visual fidelity and temporal coherence, failing to capture higher-order reasoning abilities. To bridge this gap, we propose TiViBench, a hierarchical benchmark specifically designed to evaluate the reasoning capabilities of image-to-video (I2V) generation models. TiViBench systematically assesses reasoning across four dimensions: i) Structural Reasoning &amp; Search, ii) Spatial &amp; Visual Pattern Reasoning, iii) Symbolic &amp; Logical Reasoning, and iv) Action Planning &amp; Task Execution, spanning 24 diverse task scenarios across 3 difficulty levels. Through extensive evaluations, we show that commercial models (e.g., Sora 2, Veo 3.1) demonstrate stronger reasoning potential, while open-source models reveal untapped potential that remains hindered by limited training scale and data diversity. To further unlock this potential, we introduce VideoTPO, a simple yet effective test-time strategy inspired by preference optimization. By performing LLM self-analysis on generated candidates to identify strengths and weaknesses, VideoTPO significantly enhances reasoning performance without requiring additional training, data, or reward models. Together, TiViBench and VideoTPO pave the way for evaluating and advancing reasoning in video generation models, setting a foundation for future research in this emerging field.",
        "translated": "视频生成模型的快速发展已使其关注重点从生成视觉上合理的输出，转向解决需要物理合理性和逻辑一致性的任务。然而，尽管近期出现了如Veo 3的帧序列推理等突破性成果，这些模型是否具备类似于大型语言模型（LLMs）的推理能力仍不明确。现有基准测试主要评估视觉保真度与时间一致性，未能捕捉更高阶的推理能力。为弥合这一差距，我们提出了TiViBench，这是一个专门为评估图像到视频（I2V）生成模型推理能力而设计的层次化基准。TiViBench系统性地从四个维度评估推理能力：i) 结构推理与搜索，ii) 空间与视觉模式推理，iii) 符号与逻辑推理，iv) 行动规划与任务执行，涵盖3个难度等级下的24种多样化任务场景。通过广泛评估，我们表明商业模型（如Sora 2、Veo 3.1）展现出更强的推理潜力，而开源模型则揭示了尚未被充分挖掘的潜力，其受限于训练规模和数据多样性。为进一步释放该潜力，我们引入VideoTPO，这是一种受偏好优化启发的简单而有效的测试时策略。通过在生成候选结果上进行LLM自我分析以识别优势与不足，VideoTPO显著提升了推理性能，且无需额外训练、数据或奖励模型。TiViBench与VideoTPO共同为评估和推动视频生成模型中的推理能力奠定了基础，也为该新兴领域未来的深入研究提供了方向。",
        "translated_title": "TiViBench：面向视频生成模型的视频内推理能力基准测试",
        "label": [],
        "label_reason": "评估视频生成模型推理能力，非图像像素级处理",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "提出新评测框架与测试时策略提升推理性能"
    },
    {
        "title": "Crossing Borders: A Multimodal Challenge for Indian Poetry Translation and Image Generation",
        "url": "http://arxiv.org/abs/2511.13689v1",
        "pub_date": "2025-11-17",
        "summary": "Indian poetry, known for its linguistic complexity and deep cultural resonance, has a rich and varied heritage spanning thousands of years. However, its layered meanings, cultural allusions, and sophisticated grammatical constructions often pose challenges for comprehension, especially for non-native speakers or readers unfamiliar with its context and language. Despite its cultural significance, existing works on poetry have largely overlooked Indian language poems. In this paper, we propose the Translation and Image Generation (TAI) framework, leveraging Large Language Models (LLMs) and Latent Diffusion Models through appropriate prompt tuning. Our framework supports the United Nations Sustainable Development Goals of Quality Education (SDG 4) and Reduced Inequalities (SDG 10) by enhancing the accessibility of culturally rich Indian-language poetry to a global audience. It includes (1) a translation module that uses an Odds Ratio Preference Alignment Algorithm to accurately translate morphologically rich poetry into English, and (2) an image generation module that employs a semantic graph to capture tokens, dependencies, and semantic relationships between metaphors and their meanings, to create visually meaningful representations of Indian poems. Our comprehensive experimental evaluation, including both human and quantitative assessments, demonstrates the superiority of TAI Diffusion in poem image generation tasks, outperforming strong baselines. To further address the scarcity of resources for Indian-language poetry, we introduce the Morphologically Rich Indian Language Poems MorphoVerse Dataset, comprising 1,570 poems across 21 low-resource Indian languages. By addressing the gap in poetry translation and visual comprehension, this work aims to broaden accessibility and enrich the reader's experience.",
        "translated": "印度诗歌以其语言的复杂性与深厚的文化共鸣著称，拥有跨越数千年的丰富而多元的传统。然而，其多层次的含义、文化典故及复杂的语法结构，常对理解构成挑战，尤其对于非母语者或缺乏相关背景知识的读者而言。尽管在文化意义上具有重要价值，现有诗歌研究却普遍忽视了印度语言诗歌。本文提出了一种翻译与图像生成（TAI）框架，通过恰当的提示调优，结合大型语言模型（LLMs）和潜在扩散模型实现功能。该框架支持联合国可持续发展目标中的优质教育（SDG 4）与减少不平等（SDG 10），旨在提升全球受众对富含文化底蕴的印度语言诗歌的可及性。具体包括：（1）一个采用“似然比偏好对齐算法”的翻译模块，精准将形态丰富的印度诗歌译为英文；（2）一个利用语义图捕捉词元、依存关系及隐喻与其意义之间语义关联的图像生成模块，以生成具有视觉表现力的印度诗歌图像。我们通过涵盖人工评估与定量评估的全面实验验证表明，TAI Diffusion 在诗歌图像生成任务中显著优于现有强基线方法。为进一步应对印度语言诗歌资源稀缺的问题，我们引入了《形态丰富的印度语言诗歌MorphoVerse数据集》，包含21种低资源印度语言中的1,570首诗歌。本工作旨在弥合诗歌翻译与视觉理解之间的鸿沟，拓展其可及性，并丰富读者体验。",
        "translated_title": "跨越边界：印度诗歌翻译与图像生成的多模态挑战",
        "label": [],
        "label_reason": "高阶任务：诗歌翻译与图像生成",
        "relevance_score": 1,
        "novelty_score": 1,
        "novelty_reason": "无图像恢复创新，属文化内容生成"
    },
    {
        "title": "Training-Free Multi-View Extension of IC-Light for Textual Position-Aware Scene Relighting",
        "url": "http://arxiv.org/abs/2511.13684v1",
        "pub_date": "2025-11-17",
        "summary": "We introduce GS-Light, an efficient, textual position-aware pipeline for text-guided relighting of 3D scenes represented via Gaussian Splatting (3DGS). GS-Light implements a training-free extension of a single-input diffusion model to handle multi-view inputs. Given a user prompt that may specify lighting direction, color, intensity, or reference objects, we employ a large vision-language model (LVLM) to parse the prompt into lighting priors. Using off-the-shelf estimators for geometry and semantics (depth, surface normals, and semantic segmentation), we fuse these lighting priors with view-geometry constraints to compute illumination maps and generate initial latent codes for each view. These meticulously derived init latents guide the diffusion model to generate relighting outputs that more accurately reflect user expectations, especially in terms of lighting direction. By feeding multi-view rendered images, along with the init latents, into our multi-view relighting model, we produce high-fidelity, artistically relit images. Finally, we fine-tune the 3DGS scene with the relit appearance to obtain a fully relit 3D scene. We evaluate GS-Light on both indoor and outdoor scenes, comparing it to state-of-the-art baselines including per-view relighting, video relighting, and scene editing methods. Using quantitative metrics (multi-view consistency, imaging quality, aesthetic score, semantic similarity, etc.) and qualitative assessment (user studies), GS-Light demonstrates consistent improvements over baselines. Code and assets will be made available upon publication.",
        "translated": "我们提出了 GS-Light，一种高效且具备文本引导的、位置感知的 3D 场景重照明管线，适用于通过高斯点绘（Gaussian Splatting, 3DGS）表示的场景。GS-Light 实现了一种无需训练的单输入扩散模型扩展，以支持多视角输入。给定用户提示（可指定光照方向、颜色、强度或参考物体），我们采用大型视觉-语言模型（LVLM）将提示解析为光照先验。利用现成的几何与语义估计器（深度、表面法线和语义分割），我们将这些光照先验与视角几何约束融合，计算出照度图并为每个视角生成初始潜在编码。这些精心推导的初始潜在编码引导扩散模型生成更符合用户预期的重照明结果，尤其是在光照方向方面表现更为精准。通过将多视角渲染图像及初始潜在编码输入我们的多视角重照明模型，我们输出高保真、艺术感增强的重照明图像。最后，我们对 3DGS 场景进行微调，使其外观匹配重照明效果，从而获得完整重照明的 3D 场景。我们在室内与室外场景上评估了 GS-Light，并将其与当前最先进的基线方法（包括逐视角重照明、视频重照明和场景编辑方法）进行对比。使用定量指标（多视角一致性、成像质量、美学评分、语义相似度等）与定性评估（用户研究），GS-Light 在各方面均优于现有基线方法。代码与资源将在论文发表后公开。",
        "translated_title": "无训练的IC-Light多视角扩展：面向文本位置感知场景重光照",
        "label": [],
        "label_reason": "处理3D场景重光照，非像素级图像恢复",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "创新多视角提示驱动重光照框架"
    },
    {
        "title": "QUILL: An Algorithm-Architecture Co-Design for Cache-Local Deformable Attention",
        "url": "http://arxiv.org/abs/2511.13679v1",
        "pub_date": "2025-11-17",
        "summary": "Deformable transformers deliver state-of-the-art detection but map poorly to hardware due to irregular memory access and low arithmetic intensity. We introduce QUILL, a schedule-aware accelerator that turns deformable attention into cache-friendly, single-pass work. At its core, Distance-based Out-of-Order Querying (DOOQ) orders queries by spatial proximity; the look-ahead drives a region prefetch into an alternate buffer--forming a schedule-aware prefetch loop that overlaps memory and compute. A fused MSDeformAttn engine executes interpolation, Softmax, aggregation, and the final projection (W''m) in one pass without spilling intermediates, while small tensors are kept on-chip and surrounding dense layers run on integrated GEMMs. Implemented as RTL and evaluated end-to-end, QUILL achieves up to 7.29x higher throughput and 47.3x better energy efficiency than an RTX 4090, and exceeds prior accelerators by 3.26-9.82x in throughput and 2.01-6.07x in energy efficiency. With mixed-precision quantization, accuracy tracks FP32 within &lt;=0.9 AP across Deformable and Sparse DETR variants. By converting sparsity into locality--and locality into utilization--QUILL delivers consistent, end-to-end speedups.",
        "translated": "可变形变换器在检测任务中达到当前最优性能，但由于其不规则的内存访问模式和低算力强度，难以高效映射到硬件。我们提出QUILL，一种面向调度感知的加速器，将可变形注意力转化为缓存友好、单次遍历的计算范式。其核心是基于距离的非顺序查询（Distance-based Out-of-Order Querying, DOOQ），按空间邻近性对查询进行排序；前瞻机制驱动区域数据预取至备用缓冲区——从而构建一个面向调度的预取循环，实现内存访问与计算的重叠。融合型MSDeformAttn引擎在一个计算流中完成插值、Softmax、聚合及最终投影（W''m）操作，无需中间结果溢出，同时小张量保留在片上，周边密集层则通过集成GEMM执行。QUILL以RTL形式实现，并端到端评估，相较RTX 4090，在吞吐率方面最高提升7.29倍，能效比提升47.3倍；在吞吐率和能效比上均超越现有加速器3.26–9.82倍与2.01–6.07倍。结合混合精度量化，其在Deformable与Sparse DETR变体中的精度保持在FP32水平以内，偏差不超过0.9 AP。QUILL通过将稀疏性转为局部性、局部性转为利用率，实现了持续且端到端的加速效果。",
        "translated_title": "QUILL：面向缓存局部变形注意力的算法-架构协同设计",
        "label": [],
        "label_reason": "论文聚焦硬件加速器设计，非图像处理任务。",
        "relevance_score": 1,
        "novelty_score": 9,
        "novelty_reason": "提出新型调度与融合架构，显著提升计算效率。"
    },
    {
        "title": "OlmoEarth: Stable Latent Image Modeling for Multimodal Earth Observation",
        "url": "http://arxiv.org/abs/2511.13655v1",
        "pub_date": "2025-11-17",
        "summary": "Earth observation data presents a unique challenge: it is spatial like images, sequential like video or text, and highly multimodal. We present OlmoEarth: a multimodal, spatio-temporal foundation model that employs a novel self-supervised learning formulation, masking strategy, and loss all designed for the Earth observation domain. OlmoEarth achieves state-of-the-art performance compared to 12 other foundation models across a variety of research benchmarks and real-world tasks from external partners. When evaluating embeddings OlmoEarth achieves the best performance on 15 out of 24 tasks, and with full fine-tuning it is the best on 19 of 29 tasks. We deploy OlmoEarth as the backbone of an end-to-end platform for data collection, labeling, training, and inference of Earth observation models. The OlmoEarth Platform puts frontier foundation models and powerful data management tools into the hands of non-profits and NGOs working to solve the world's biggest problems. OlmoEarth source code, training data, and pre-trained weights are available at $\\href{https://github.com/allenai/olmoearth_pretrain}{\\text{https://github.com/allenai/olmoearth_pretrain}}$.",
        "translated": "地球观测数据呈现出独特的挑战：它在空间上类似于图像，在时间序列上类似于视频或文本，并具有高度多模态特性。我们提出了 OlmoEarth：一种面向地球观测领域的多模态、时空基础模型，其采用新颖的自监督学习框架、掩码策略与损失函数设计。在多个研究基准测试及来自外部合作伙伴的真实世界任务中，OlmoEarth 在12个其他基础模型中表现最优。在评估嵌入性能时，OlmoEarth 在24项任务中的15项取得最佳表现；在完全微调后，其在29项任务中的19项达到最优水平。我们将 OlmoEarth 部署为一个端到端平台的核心组件，用于地球观测模型的数据采集、标注、训练与推理。OlmoEarth 平台将前沿的基础模型与强大的数据管理工具提供给致力于解决全球重大问题的非营利组织和国际机构。OlmoEarth 的源代码、训练数据及预训练权重已公开于 $\\href{https://github.com/allenai/olmoearth_pretrain}{\\text{https://github.com/allenai/olmoearth_pretrain}}$。",
        "translated_title": "OlmoEarth：面向多模态地球观测的稳定潜在图像建模",
        "label": [],
        "label_reason": "处理地球观测多模态数据，非像素级图像恢复任务。",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出新自监督学习框架，适配地球观测场景。"
    },
    {
        "title": "Tuning for Two Adversaries: Enhancing the Robustness Against Transfer and Query-Based Attacks using Hyperparameter Tuning",
        "url": "http://arxiv.org/abs/2511.13654v1",
        "pub_date": "2025-11-17",
        "summary": "In this paper, we present the first detailed analysis of how optimization hyperparameters -- such as learning rate, weight decay, momentum, and batch size -- influence robustness against both transfer-based and query-based attacks. Supported by theory and experiments, our study spans a variety of practical deployment settings, including centralized training, ensemble learning, and distributed training. We uncover a striking dichotomy: for transfer-based attacks, decreasing the learning rate significantly enhances robustness by up to $64\\%$. In contrast, for query-based attacks, increasing the learning rate consistently leads to improved robustness by up to $28\\%$ across various settings and data distributions. Leveraging these findings, we explore -- for the first time -- the optimization hyperparameter design space to jointly enhance robustness against both transfer-based and query-based attacks. Our results reveal that distributed models benefit the most from hyperparameter tuning, achieving a remarkable tradeoff by simultaneously mitigating both attack types more effectively than other training setups.",
        "translated": "本文首次详细分析了优化超参数——如学习率、权重衰减、动量和批量大小——如何影响模型对基于迁移的攻击与基于查询的攻击的鲁棒性。该研究结合理论与实验，覆盖多种实际部署场景，包括集中式训练、集成学习和分布式训练。我们发现了一个显著的二分现象：对于基于迁移的攻击，降低学习率可使模型鲁棒性提升高达64%；相比之下，对于基于查询的攻击，提高学习率在各类设置与数据分布下均能持续增强鲁棒性，最高可达28%。基于上述发现，我们首次探索了优化超参数的设计空间，以同时提升模型对两类攻击的鲁棒性。结果表明，分布式模型从超参数调优中获益最大，在兼顾有效缓解两类攻击的同时，实现了优于其他训练架构的显著性能权衡。",
        "translated_title": "针对两个对抗者的调优：利用超参数调优增强对迁移攻击与查询式攻击的鲁棒性",
        "label": [],
        "label_reason": "研究优化超参对模型鲁棒性影响，非图像处理任务",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "首次系统分析超参对对抗攻击鲁棒性的联合优化"
    },
    {
        "title": "Distribution Matching Distillation Meets Reinforcement Learning",
        "url": "http://arxiv.org/abs/2511.13649v1",
        "pub_date": "2025-11-17",
        "summary": "Distribution Matching Distillation (DMD) distills a pre-trained multi-step diffusion model to a few-step one to improve inference efficiency. However, the performance of the latter is often capped by the former. To circumvent this dilemma, we propose DMDR, a novel framework that combines Reinforcement Learning (RL) techniques into the distillation process. We show that for the RL of the few-step generator, the DMD loss itself is a more effective regularization compared to the traditional ones. In turn, RL can help to guide the mode coverage process in DMD more effectively. These allow us to unlock the capacity of the few-step generator by conducting distillation and RL simultaneously. Meanwhile, we design the dynamic distribution guidance and dynamic renoise sampling training strategies to improve the initial distillation process. The experiments demonstrate that DMDR can achieve leading visual quality, prompt coherence among few-step methods, and even exhibit performance that exceeds the multi-step teacher.",
        "translated": "分布匹配蒸馏（DMD）将预训练的多步扩散模型蒸馏为几步模型，以提升推理效率。然而，后者的性能常受限于前者。为解决此困境，我们提出 DMDR，一种将强化学习（RL）技术融入蒸馏过程的新型框架。我们表明，对于几步生成器的 RL，DMD 损失本身相比传统方法更具有效的正则化作用。反之，RL 可更有效地引导 DMD 的模式覆盖过程。这些机制使我们能够在蒸馏与 RL 同时进行的过程中，充分释放几步生成器的能力。同时，我们设计了动态分布引导和动态重噪声采样训练策略，以优化初始蒸馏过程。实验表明，DMDR 可实现领先的视觉质量、几步方法中的提示一致性，甚至在性能上超越多步教师模型。",
        "translated_title": "分布匹配蒸馏结合强化学习",
        "label": [],
        "label_reason": "方法用于扩散模型压缩，非图像像素级恢复",
        "relevance_score": 3,
        "novelty_score": 8,
        "novelty_reason": "结合RL提升蒸馏效率，有显著改进"
    },
    {
        "title": "PhysX-Anything: Simulation-Ready Physical 3D Assets from Single Image",
        "url": "http://arxiv.org/abs/2511.13648v1",
        "pub_date": "2025-11-17",
        "summary": "3D modeling is shifting from static visual representations toward physical, articulated assets that can be directly used in simulation and interaction. However, most existing 3D generation methods overlook key physical and articulation properties, thereby limiting their utility in embodied AI. To bridge this gap, we introduce PhysX-Anything, the first simulation-ready physical 3D generative framework that, given a single in-the-wild image, produces high-quality sim-ready 3D assets with explicit geometry, articulation, and physical attributes. Specifically, we propose the first VLM-based physical 3D generative model, along with a new 3D representation that efficiently tokenizes geometry. It reduces the number of tokens by 193x, enabling explicit geometry learning within standard VLM token budgets without introducing any special tokens during fine-tuning and significantly improving generative quality. In addition, to overcome the limited diversity of existing physical 3D datasets, we construct a new dataset, PhysX-Mobility, which expands the object categories in prior physical 3D datasets by over 2x and includes more than 2K common real-world objects with rich physical annotations. Extensive experiments on PhysX-Mobility and in-the-wild images demonstrate that PhysX-Anything delivers strong generative performance and robust generalization. Furthermore, simulation-based experiments in a MuJoCo-style environment validate that our sim-ready assets can be directly used for contact-rich robotic policy learning. We believe PhysX-Anything can substantially empower a broad range of downstream applications, especially in embodied AI and physics-based simulation.",
        "translated": "三维建模正从静态视觉表示转向可直接用于仿真与交互的物理化、可动部件资产。然而，大多数现有的三维生成方法忽视了关键的物理属性和可动性特征，从而限制了其在具身人工智能中的实用性。为弥合这一差距，我们提出了PhysX-Anything，这是首个支持仿真的物理三维生成框架：仅需一张野外采集的图像，即可生成具备显式几何结构、可动性和物理属性的高质量仿真适配三维资产。具体而言，我们提出了首个基于视觉语言模型（VLM）的物理三维生成模型，并设计了一种新的三维表示方式，能够高效地对几何结构进行分词编码。该方法将所需tokens数量减少了193倍，在标准VLM的token预算范围内实现了显式几何学习，且在微调过程中无需引入任何特殊token，同时显著提升了生成质量。此外，为解决现有物理三维数据集中多样性不足的问题，我们构建了新的数据集PhysX-Mobility，其物体类别规模较之前物理三维数据集扩大超过2倍，并包含超过2000个常见真实世界物体，配有丰富的物理属性标注。在PhysX-Mobility数据集及野外图像上的大量实验表明，PhysX-Anything展现出强大的生成能力与良好的泛化性能。进一步地，在MuJoCo风格仿真环境中进行的基于仿真的实验验证了我们的仿真适配资产可直接用于接触密集型机器人策略的学习。我们相信PhysX-Anything将有力赋能广泛的下游应用，尤其是在具身人工智能和基于物理的仿真领域。",
        "translated_title": "PhysX-Anything：从单张图像生成可用于物理模拟的3D资产",
        "label": [],
        "label_reason": "生成3D物理资产，非像素级图像处理",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "首次VLM生成物理3D模型，提升效率与质量"
    },
    {
        "title": "Part-X-MLLM: Part-aware 3D Multimodal Large Language Model",
        "url": "http://arxiv.org/abs/2511.13647v1",
        "pub_date": "2025-11-17",
        "summary": "We introduce Part-X-MLLM, a native 3D multimodal large language model that unifies diverse 3D tasks by formulating them as programs in a structured, executable grammar. Given an RGB point cloud and a natural language prompt, our model autoregressively generates a single, coherent token sequence encoding part-level bounding boxes, semantic descriptions, and edit commands. This structured output serves as a versatile interface to drive downstream geometry-aware modules for part-based generation and editing. By decoupling the symbolic planning from the geometric synthesis, our approach allows any compatible geometry engine to be controlled through a single, language-native frontend. We pre-train a dual-encoder architecture to disentangle structure from semantics and instruction-tune the model on a large-scale, part-centric dataset. Experiments demonstrate that our model excels at producing high-quality, structured plans, enabling state-of-the-art performance in grounded Q\\&amp;A, compositional generation, and localized editing through one unified interface. Project page: https://chunshi.wang/Part-X-MLLM/",
        "translated": "我们引入了 Part-X-MLLM，这是一种原生的3D多模态大语言模型，通过将多样化的3D任务形式化为结构化、可执行的语法程序进行统一处理。给定一个RGB点云和自然语言提示，我们的模型自回归地生成一个单一且连贯的标记序列，该序列编码了部件级别的边界框、语义描述及编辑指令。这一结构化输出作为通用接口，可驱动下游几何感知模块实现基于部件的生成与编辑。通过将符号规划与几何合成解耦，我们的方法允许任何兼容的几何引擎通过单一的、以语言为本的前端进行控制。我们预训练了一种双编码器架构，用于分离结构与语义，并在大规模、以部件为中心的数据集上对模型进行指令微调。实验表明，我们的模型在生成高质量结构化规划方面表现卓越，从而通过统一接口实现了接地式问答、组合式生成和局部编辑等任务的最先进性能。项目页面：https://chunshi.wang/Part-X-MLLM/",
        "translated_title": "Part-X-MLLM：面向部件感知的三维多模态大语言模型",
        "label": [],
        "label_reason": "任务为3D多模态语言模型，非图像像素级处理",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "提出结构化语法驱动的3D编辑新范式"
    },
    {
        "title": "CacheFlow: Compressive Streaming Memory for Efficient Long-Form Video Understanding",
        "url": "http://arxiv.org/abs/2511.13644v1",
        "pub_date": "2025-11-17",
        "summary": "Long-form video question answering (VQA) overwhelms current vision-language models (VLMs) because attention and key-value (KV) caches grow with runtime, forcing either expensive inference or near-sighted sliding windows. We introduce CacheFlow, a training-free pipeline that pairs Dynamic Token Dropping (DTD) with a compressive long-term memory. DTD prunes per-patch tokens online via cosine similarity to the previous frame, and surviving tokens are packed into fixed-size blocks. This online, per-frame processing makes our approach fundamentally suited for live streaming VQA. As blocks are processed, each one's keys are summarized by a tiny recurrent encoder to form a retrieval index, while the block's full KV pairs are offloaded and later rehydrated for generation, preserving answer fidelity. At inference, a consensus-based retrieval mechanism retrieves only the Top-K most relevant blocks and attends over both the retrieved and local context for precise, long-range reasoning. CacheFlow is drop-in, architecture-agnostic, and requires no fine-tuning. Experiments on both offline and streaming VQA benchmarks demonstrate that CacheFlow outperforms current strong baselines, while processing up to 87% less tokens. Our dual approach enables VLMs to be both efficient and context-aware, paving the way for practical long-form video understanding.",
        "translated": "长视频问答（VQA）目前严重超出当前视觉-语言模型（VLMs）的能力，因为注意力机制与键值（KV）缓存会随运行时长增长，导致要么需要昂贵的推理开销，要么被迫使用短视窗口滑动。我们提出 CacheFlow，这是一种无需训练的管道方法，结合动态令牌剔除（DTD）与压缩型长期记忆模块。DTD 通过计算当前帧块与前一帧之间的余弦相似度，实时在线修剪每块令牌；保留的令牌被打包成固定大小的区块。这种逐帧、在线处理方式使我们的方法从根本上适用于直播场景下的 VQA。在处理每个区块时，其键向量由一个轻量级循环编码器进行总结，形成检索索引；而该区块完整的 KV 对则被卸载至外部存储，并在生成阶段重新加载以保持答案保真度。推理阶段，基于共识机制的检索机制仅选取 Top-K 最相关区块，并对所检索内容与局部上下文并行关注，从而实现精确且具备长程推理能力的问答。CacheFlow 可无缝集成于现有架构，无需微调。我们在离线与流式 VQA 数据集上的实验表明，CacheFlow 在性能上优于当前主流基线方法，同时处理令牌数量减少高达 87%。我们的双轨策略使视觉-语言模型既高效又具备上下文感知能力，为实用化的长视频理解奠定了基础。",
        "translated_title": "CacheFlow：面向高效长视频理解的压缩流式内存",
        "label": [],
        "label_reason": "属于视频理解的高阶视觉任务，非像素级图像处理",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "提出压缩记忆机制与动态令牌剪枝，显著提升效率"
    },
    {
        "title": "Alpha Divergence Losses for Biometric Verification",
        "url": "http://arxiv.org/abs/2511.13621v1",
        "pub_date": "2025-11-17",
        "summary": "Performance in face and speaker verification is largely driven by margin based softmax losses like CosFace and ArcFace. Recently introduced $α$-divergence loss functions offer a compelling alternative, particularly for their ability to induce sparse solutions (when $α&gt;1$). However, integrating an angular margin-crucial for verification tasks-is not straightforward. We find this integration can be achieved in at least two distinct ways: via the reference measure (prior probabilities) or via the logits (unnormalized log-likelihoods). In this paper, we explore both pathways, deriving two novel margin-based $α$-divergence losses: Q-Margin (margin in the reference measure) and A3M (margin in the logits). We identify and address a critical training instability in A3M-caused by the interplay of penalized logits and sparsity-with a simple yet effective prototype re-initialization strategy. Our methods achieve significant performance gains on the challenging IJB-B and IJB-C face verification benchmarks. We demonstrate similarly strong performance in speaker verification on VoxCeleb. Crucially, our models significantly outperform strong baselines at low false acceptance rates (FAR). This capability is crucial for practical high-security applications, such as banking authentication, when minimizing false authentications is paramount.",
        "translated": "人脸识别与说话人验证的性能在很大程度上依赖于基于边界的Softmax损失函数，如CosFace和ArcFace。近期提出的α-散度损失函数提供了具有吸引力的替代方案，特别是当α>1时，其能够诱导稀疏解。然而，将角度边界（对验证任务至关重要）融入该框架并非易事。我们发现，这种集成可通过两种不同的途径实现：通过参考测度（先验概率）或通过logits（未归一化的对数似然）。在本文中，我们探讨了这两种路径，推导出两种新颖的基于边界的α-散度损失函数：Q-Margin（在参考测度中引入边界）和A3M（在logits中引入边界）。我们识别并解决了A3M训练过程中由惩罚性logits与稀疏性相互作用引发的关键不稳定问题，采用了一种简洁而有效的原型重初始化策略予以应对。我们的方法在具有挑战性的IJB-B和IJB-C人脸验证基准测试中显著提升了性能。我们在VoxCeleb说话人验证数据集上也展现出同样强劲的表现。尤为重要的是，我们的模型在低误接受率（FAR）下显著优于现有强基线模型。这一能力对于实际高安全级别应用（如银行身份认证）至关重要，因为在这些场景中最小化误认是首要目标。",
        "translated_title": "用于生物识别验证的 Alpha 散度损失",
        "label": [],
        "label_reason": "任务为高阶生物识别验证，非低层图像处理",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出新损失函数但属分类任务创新"
    },
    {
        "title": "A Real-Time Driver Drowsiness Detection System Using MediaPipe and Eye Aspect Ratio",
        "url": "http://arxiv.org/abs/2511.13618v1",
        "pub_date": "2025-11-17",
        "summary": "One of the major causes of road accidents is driver fatigue that causes thousands of fatalities and injuries every year. This study shows development of a Driver Drowsiness Detection System meant to improve the safety of the road by alerting drivers who are showing signs of being drowsy. The system is based on a standard webcam that tracks the facial features of the driver with the main emphasis on the examination of eye movements that can be conducted with the help of the Eye Aspect Ratio (EAR) method. The Face Mesh by MediaPipe is a lightweight framework that can identify facial landmarks with high accuracy and efficiency, which is considered to be important in real time use. The system detects the moments of long eye shutdowns or a very low rate of blinking which are manifestations of drowsiness and alerts the driver through sound to get her attention back. This system achieves a high-performance and low-cost driver monitoring solution with the help of the computational power of OpenCV to process the image and the MediaPipe to identify faces. Test data experimental analyses indicate that the system is very accurate and responds quicker; this confirms that it can be a component of the current Advanced Driving Assistance System (ADAS).",
        "translated": "道路交通事故的一个主要原因在于驾驶员疲劳，每年导致数千人死亡和受伤。本研究开发了一种驾驶员困倦检测系统，旨在通过提醒表现出困倦迹象的驾驶员以提升道路安全。该系统基于标准网络摄像头，通过跟踪驾驶员面部特征，重点采用眼动分析方法（借助眼形比 EAR 方法）进行检测。MediaPipe 提供的 Face Mesh 是一个轻量级框架，可高精度、高效率地识别面部关键点，在实时应用中被认为是至关重要的。系统能够检测到长时间闭眼或眨眼频率极低等困倦表现，并通过声音提醒驾驶员重新集中注意力。该系统借助 OpenCV 的图像处理能力和 MediaPipe 的人脸识别功能，实现了高性能且低成本的驾驶员监测方案。实验数据分析表明，该系统准确率高、响应速度快，证实其可作为当前高级驾驶辅助系统（ADAS）的重要组成部分。",
        "translated_title": "一种基于 MediaPipe 和眼型比率的实时驾驶员困倦检测系统",
        "label": [],
        "label_reason": "高阶任务：驾驶员疲劳检测，非像素级图像处理",
        "relevance_score": 1,
        "novelty_score": 1,
        "novelty_reason": "无创新，仅使用现有工具实现人脸检测与眨眼分析"
    },
    {
        "title": "Tissue Aware Nuclei Detection and Classification Model for Histopathology Images",
        "url": "http://arxiv.org/abs/2511.13615v1",
        "pub_date": "2025-11-17",
        "summary": "Accurate nuclei detection and classification are fundamental to computational pathology, yet existing approaches are hindered by reliance on detailed expert annotations and insufficient use of tissue context. We present Tissue-Aware Nuclei Detection (TAND), a novel framework achieving joint nuclei detection and classification using point-level supervision enhanced by tissue mask conditioning. TAND couples a ConvNeXt-based encoder-decoder with a frozen Virchow-2 tissue segmentation branch, where semantic tissue probabilities selectively modulate the classification stream through a novel multi-scale Spatial Feature-wise Linear Modulation (Spatial-FiLM). On the PUMA benchmark, TAND achieves state-of-the-art performance, surpassing both tissue-agnostic baselines and mask-supervised methods. Notably, our approach demonstrates remarkable improvements in tissue-dependent cell types such as epithelium, endothelium, and stroma. To the best of our knowledge, this is the first method to condition per-cell classification on learned tissue masks, offering a practical pathway to reduce annotation burden.",
        "translated": "准确的细胞核检测与分类是计算病理学的基础，然而现有方法受限于对详细专家标注的依赖以及对组织上下文信息利用不足。我们提出了一种新的框架——组织感知细胞核检测（Tissue-Aware Nuclei Detection, TAND），该框架通过结合点级监督与组织掩码条件增强，在单一模型中实现联合细胞核检测与分类。TAND 将基于 ConvNeXt 的编码器-解码器结构与冻结的 Virchow-2 组织分割分支相结合，通过一种新颖的多尺度空间特征逐通道线性调制（Spatial-FiLM）机制，使语义组织概率选择性地调节分类支路。在 PUMA 评估基准上，TAND 达到当前最优性能，超越了无组织感知基线方法及掩码监督方法。值得注意的是，我们的方法在组织依赖型细胞类型（如上皮细胞、内皮细胞和间质细胞）上展现出显著提升。据我们所知，这是首个将每细胞分类依据学习得到的组织掩码进行条件约束的方法，为降低标注负担提供了切实可行的路径。",
        "translated_title": "组织感知的核检测与分类模型用于组织病理学图像",
        "label": [],
        "label_reason": "任务为细胞检测分类，属高阶病理分析",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "创新性在组织引导分类，非像素级图像处理"
    },
    {
        "title": "AtlasMorph: Learning conditional deformable templates for brain MRI",
        "url": "http://arxiv.org/abs/2511.13609v1",
        "pub_date": "2025-11-17",
        "summary": "Deformable templates, or atlases, are images that represent a prototypical anatomy for a population, and are often enhanced with probabilistic anatomical label maps. They are commonly used in medical image analysis for population studies and computational anatomy tasks such as registration and segmentation. Because developing a template is a computationally expensive process, relatively few templates are available. As a result, analysis is often conducted with sub-optimal templates that are not truly representative of the study population, especially when there are large variations within this population. We propose a machine learning framework that uses convolutional registration neural networks to efficiently learn a function that outputs templates conditioned on subject-specific attributes, such as age and sex. We also leverage segmentations, when available, to produce anatomical segmentation maps for the resulting templates. The learned network can also be used to register subject images to the templates. We demonstrate our method on a compilation of 3D brain MRI datasets, and show that it can learn high-quality templates that are representative of populations. We find that annotated conditional templates enable better registration than their unlabeled unconditional counterparts, and outperform other templates construction methods.",
        "translated": "可变形模板，或称图谱，是代表某人群典型解剖结构的图像，并常辅以概率性解剖标签图。它们广泛应用于医学图像分析中的群体研究和计算解剖学任务（如配准与分割）。由于构建模板是一个计算成本高昂的过程，目前可用的模板数量相对较少。因此，实际分析中常使用非最优模板，这些模板未能真正代表研究人群，尤其在该人群内部存在较大变异时更为明显。我们提出一种基于卷积配准神经网络的机器学习框架，高效地学习一个输出条件化模板的函数，其输入为受试者特异性属性（如年龄与性别）。此外，当有分割结果可用时，我们利用其生成所得模板的解剖分割图。所学习的网络还可用于将受试者图像配准至模板。我们在多个3D脑部MRI数据集上验证了该方法，表明其能够学习出高质量、高度代表人群特征的模板。实验结果显示，标注的条件模板相较于无标注的无条件模板能实现更优的配准效果，并优于其他模板构建方法。",
        "translated_title": "AtlasMorph：学习条件可变形模板用于脑部MRI",
        "label": [],
        "label_reason": "主要为医学图像配准与分割，属高阶任务",
        "relevance_score": 3,
        "novelty_score": 6,
        "novelty_reason": "条件模板学习改进注册，但非像素级恢复"
    },
    {
        "title": "ICLR: Inter-Chrominance and Luminance Interaction for Natural Color Restoration in Low-Light Image Enhancement",
        "url": "http://arxiv.org/abs/2511.13607v1",
        "pub_date": "2025-11-17",
        "summary": "Low-Light Image Enhancement (LLIE) task aims at improving contrast while restoring details and textures for images captured in low-light conditions. HVI color space has made significant progress in this task by enabling precise decoupling of chrominance and luminance. However, for the interaction of chrominance and luminance branches, substantial distributional differences between the two branches prevalent in natural images limit complementary feature extraction, and luminance errors are propagated to chrominance channels through the nonlinear parameter. Furthermore, for interaction between different chrominance branches, images with large homogeneous-color regions usually exhibit weak correlation between chrominance branches due to concentrated distributions. Traditional pixel-wise losses exploit strong inter-branch correlations for co-optimization, causing gradient conflicts in weakly correlated regions. Therefore, we propose an Inter-Chrominance and Luminance Interaction (ICLR) framework including a Dual-stream Interaction Enhancement Module (DIEM) and a Covariance Correction Loss (CCL). The DIEM improves the extraction of complementary information from two dimensions, fusion and enhancement, respectively. The CCL utilizes luminance residual statistics to penalize chrominance errors and balances gradient conflicts by constraining chrominance branches covariance. Experimental results on multiple datasets show that the proposed ICLR framework outperforms state-of-the-art methods.",
        "translated": "低光照图像增强（LLIE）任务旨在提升对比度，同时恢复在低光照条件下拍摄的图像中的细节与纹理。HVI色彩空间在此任务中取得了显著进展，其优势在于实现了色度与亮度成分的精确解耦。然而，在色度与亮度分支间的交互过程中，自然图像中两者普遍存在显著的分布差异，限制了互补特征的有效提取；此外，由于非线性参数的存在，亮度通道的误差会传递至色度通道。对于不同色度分支之间的交互而言，具有大面积同色区域的图像往往因分布高度集中而导致色度分支间相关性较弱。传统像素级损失函数依赖强分支间相关性进行联合优化，但在弱相关区域易引发梯度冲突。为此，我们提出了一种跨色度与亮度交互（ICLR）框架，包含双流交互增强模块（DIEM）和协方差校正损失（CCL）。DIEM分别从融合与增强两个维度提升互补信息的提取能力；CCL利用亮度残差统计量对色度误差施加惩罚，并通过约束色度分支间的协方差以平衡梯度冲突。在多个数据集上的实验结果表明，所提出的ICLR框架优于当前最先进方法。",
        "translated_title": "ICLR：用于低光照图像增强中的色度与亮度交互作用以实现自然色彩恢复",
        "label": [
            "低光照增强"
        ],
        "label_reason": "专注低光照图像对比度与细节恢复，属像素级low-level任务。",
        "relevance_score": 10,
        "novelty_score": 8,
        "novelty_reason": "提出DIEM与CCL新模块，显著提升特征融合与梯度一致性。"
    },
    {
        "title": "Improving segmentation of retinal arteries and veins using cardiac signal in doppler holograms",
        "url": "http://arxiv.org/abs/2511.14654v1",
        "pub_date": "2025-11-18",
        "summary": "Doppler holography is an emerging retinal imaging technique that captures the dynamic behavior of blood flow with high temporal resolution, enabling quantitative assessment of retinal hemodynamics. This requires accurate segmentation of retinal arteries and veins, but traditional segmentation methods focus solely on spatial information and overlook the temporal richness of holographic data. In this work, we propose a simple yet effective approach for artery-vein segmentation in temporal Doppler holograms using standard segmentation architectures. By incorporating features derived from a dedicated pulse analysis pipeline, our method allows conventional U-Nets to exploit temporal dynamics and achieve performance comparable to more complex attention- or iteration-based models. These findings demonstrate that time-resolved preprocessing can unlock the full potential of deep learning for Doppler holography, opening new perspectives for quantitative exploration of retinal hemodynamics. The dataset is publicly available at https://huggingface.co/datasets/DigitalHolography/",
        "translated": "多普勒全息成像是新兴的一种视网膜成像技术，能够以高时间分辨率捕捉血流的动态行为，从而实现对视网膜血流动力学的定量评估。这要求对视网膜动脉与静脉进行精确分割，但传统分割方法仅关注空间信息，忽略了全息数据所蕴含的时间丰富性。在本研究中，我们提出了一种简单而有效的方法，利用标准分割架构对时域多普勒全息图中的动脉与静脉进行分割。通过引入自定义脉冲分析流水线提取的特征，我们的方法使传统的 U-Net 模型能够利用时间动态信息，并达到与更复杂的注意力或迭代式模型相当的性能。这些结果表明，基于时间分辨预处理的方法可以充分发挥深度学习在多普勒全息成像中的潜力，为视网膜血流动力学的定量研究开辟新的视角。该数据集可公开获取于 https://huggingface.co/datasets/DigitalHolography/",
        "translated_title": "利用多普勒全息图中的心电信号改善视网膜动脉和静脉的分割",
        "label": [],
        "label_reason": "聚焦血管分割，属高层面医学图像分析任务",
        "relevance_score": 2,
        "novelty_score": 5,
        "novelty_reason": "引入心电信号预处理提升分割效果，创新性中等"
    },
    {
        "title": "RepAir: A Framework for Airway Segmentation and Discontinuity Correction in CT",
        "url": "http://arxiv.org/abs/2511.14649v1",
        "pub_date": "2025-11-18",
        "summary": "Accurate airway segmentation from chest computed tomography (CT) scans is essential for quantitative lung analysis, yet manual annotation is impractical and many automated U-Net-based methods yield disconnected components that hinder reliable biomarker extraction. We present RepAir, a three-stage framework for robust 3D airway segmentation that combines an nnU-Net-based network with anatomically informed topology correction. The segmentation network produces an initial airway mask, after which a skeleton-based algorithm identifies potential discontinuities and proposes reconnections. A 1D convolutional classifier then determines which candidate links correspond to true anatomical branches versus false or obstructed paths. We evaluate RepAir on two distinct datasets: ATM'22, comprising annotated CT scans from predominantly healthy subjects and AeroPath, encompassing annotated scans with severe airway pathology. Across both datasets, RepAir outperforms existing 3D U-Net-based approaches such as Bronchinet and NaviAirway on both voxel-level and topological metrics, and produces more complete and anatomically consistent airway trees while maintaining high segmentation accuracy.",
        "translated": "从胸部计算机断层扫描（CT）图像中精确分割气道，对定量肺部分析至关重要，但人工标注不切实际，且许多基于U-Net的自动化方法生成的气道组件相互分离，阻碍了可靠的生物标志物提取。本文提出RepAir，一种三阶段框架，用于稳健的3D气道分割，该框架结合了nnU-Net架构网络与解剖学引导的拓扑校正算法。分割网络首先生成初始气道掩码，随后基于骨架的算法识别潜在的不连续性并提出重新连接建议。一个1D卷积分类器进一步判断哪些候选连接对应真实的解剖分支，而非虚假或受阻路径。我们在两个不同数据集上评估RepAir：ATM'22包含主要来自健康受试者的标注CT扫描，AeroPath则涵盖具有严重气道病理的标注扫描。在两个数据集上，RepAir均优于现有基于3D U-Net的方法（如Bronchinet和NaviAirway），在体素级与拓扑指标上表现更优，并能生成更完整、解剖一致性更高的气道树，同时保持高分割精度。",
        "translated_title": "RepAir：一种用于CT图像气道分割与不连续性校正的框架",
        "label": [],
        "label_reason": "任务属医学图像分割，非像素级图像恢复",
        "relevance_score": 3,
        "novelty_score": 7,
        "novelty_reason": "拓扑校正创新，但属高阶分割任务"
    },
    {
        "title": "SLAM-AGS: Slide-Label Aware Multi-Task Pretraining Using Adaptive Gradient Surgery in Computational Cytology",
        "url": "http://arxiv.org/abs/2511.14639v1",
        "pub_date": "2025-11-18",
        "summary": "Computational cytology faces two major challenges: i) instance-level labels are unreliable and prohibitively costly to obtain, ii) witness rates are extremely low. We propose SLAM-AGS, a Slide-Label-Aware Multitask pretraining framework that jointly optimizes (i) a weakly supervised similarity objective on slide-negative patches and (ii) a self-supervised contrastive objective on slide-positive patches, yielding stronger performance on downstream tasks. To stabilize learning, we apply Adaptive Gradient Surgery to tackle conflicting task gradients and prevent model collapse. We integrate the pretrained encoder into an attention-based Multiple Instance Learning aggregator for bag-level prediction and attention-guided retrieval of the most abnormal instances in a bag. On a publicly available bone-marrow cytology dataset, with simulated witness rates from 10% down to 0.5%, SLAM-AGS improves bag-level F1-Score and Top 400 positive cell retrieval over other pretraining methods, with the largest gains at low witness rates, showing that resolving gradient interference enables stable pretraining and better performance on downstream tasks. To facilitate reproducibility, we share our complete implementation and evaluation framework as open source: https://github.com/Ace95/SLAM-AGS.",
        "translated": "计算细胞学面临两大主要挑战：i）实例级标签不可靠且获取成本极高；ii）见证率极低。我们提出 SLAM-AGS，一种滑片-标签感知的多任务预训练框架，该框架联合优化 (i) 在滑片负样本块上基于弱监督相似性目标，以及 (ii) 在滑片正样本块上基于自监督对比目标，从而在下游任务中获得更强性能。为稳定学习过程，我们采用自适应梯度手术以应对冲突的任务梯度并防止模型坍缩。我们将预训练编码器集成至基于注意力机制的多实例学习聚合器中，用于袋级别预测及引导注意力检索袋内最异常的实例。在公开可用的骨髓细胞学数据集上，当模拟见证率从 10% 下降至 0.5% 时，SLAM-AGS 在袋级别 F1 分数与 Top 400 正常细胞检索表现上均优于其他预训练方法，且在低见证率下提升最为显著，表明解决梯度干扰可实现稳定的预训练并在下游任务中取得更优性能。为促进复现，我们开源了完整的实现与评估框架：https://github.com/Ace95/SLAM-AGS。",
        "translated_title": "SLAM-AGS：基于自适应梯度手术的滑动标签感知多任务预训练方法在计算细胞学中的应用",
        "label": [],
        "label_reason": "任务属医学图像分类，非像素级图像处理",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出多任务预训练框架，但非图像恢复类创新"
    },
    {
        "title": "SparseSurf: Sparse-View 3D Gaussian Splatting for Surface Reconstruction",
        "url": "http://arxiv.org/abs/2511.14633v1",
        "pub_date": "2025-11-18",
        "summary": "Recent advances in optimizing Gaussian Splatting for scene geometry have enabled efficient reconstruction of detailed surfaces from images. However, when input views are sparse, such optimization is prone to overfitting, leading to suboptimal reconstruction quality. Existing approaches address this challenge by employing flattened Gaussian primitives to better fit surface geometry, combined with depth regularization to alleviate geometric ambiguities under limited viewpoints. Nevertheless, the increased anisotropy inherent in flattened Gaussians exacerbates overfitting in sparse-view scenarios, hindering accurate surface fitting and degrading novel view synthesis performance. In this paper, we propose \\net{}, a method that reconstructs more accurate and detailed surfaces while preserving high-quality novel view rendering. Our key insight is to introduce Stereo Geometry-Texture Alignment, which bridges rendering quality and geometry estimation, thereby jointly enhancing both surface reconstruction and view synthesis. In addition, we present a Pseudo-Feature Enhanced Geometry Consistency that enforces multi-view geometric consistency by incorporating both training and unseen views, effectively mitigating overfitting caused by sparse supervision. Extensive experiments on the DTU, BlendedMVS, and Mip-NeRF360 datasets demonstrate that our method achieves the state-of-the-art performance.",
        "translated": "近年来，针对高斯点云在场景几何优化方面的进展，已实现从图像中高效重建细节表面。然而，当输入视角稀疏时，此类优化易发生过拟合，导致重建质量下降。现有方法通过采用扁平化高斯基元以更好地贴合表面几何，并结合深度正则化以缓解有限视角下的几何歧义性。然而，扁平化高斯固有的各向异性增强，在稀疏视角条件下加剧了过拟合现象，阻碍了精确的表面拟合并降低了新视角合成性能。本文提出 \\net{}，一种能够在保持高质量新视角渲染的同时，重建更准确且更精细表面的方法。我们的核心思想是引入立体几何-纹理对齐机制，弥合渲染质量与几何估计之间的鸿沟，从而联合提升表面重建与视角合成性能。此外，我们提出伪特征增强的几何一致性机制，通过融合训练视图与未见视图来强制多视角几何一致性，有效缓解因稀疏监督所引发的过拟合问题。在 DTU、BlendedMVS 与 Mip-NeRF360 数据集上的大量实验表明，本方法实现了当前最优性能。",
        "translated_title": "SparseSurf：用于表面重建的稀疏视角三维高斯点绘",
        "label": [],
        "label_reason": "目标为3D表面重建，属高阶几何任务",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "提出立体几何-纹理对齐新机制提升一致性"
    },
    {
        "title": "Enhancing Agentic Autonomous Scientific Discovery with Vision-Language Model Capabilities",
        "url": "http://arxiv.org/abs/2511.14631v1",
        "pub_date": "2025-11-18",
        "summary": "We show that multi-agent systems guided by vision-language models (VLMs) improve end-to-end autonomous scientific discovery. By treating plots as verifiable checkpoints, a VLM-as-a-judge evaluates figures against dynamically generated domain-specific rubrics, enabling agents to correct their own errors and steer exploratory data analysis in real-time. Case studies in cosmology and astrochemistry demonstrate recovery from faulty reasoning paths and adaptation to new datasets without human intervention. On a 10-task benchmark for data-driven discovery, VLM-augmented systems achieve pass at 1 scores of 0.7-0.8, compared to 0.2-0.3 for code-only and 0.4-0.5 for code-and-text baselines, while also providing auditable reasoning traces that improve interpretability. Code available here: https://github.com/CMBAgents/cmbagent",
        "translated": "我们表明，由视觉-语言模型（VLMs）引导的多智能体系统可提升端到端自主科学发现的能力。通过将图表视为可验证的检查点，作为裁判的 VLM 根据动态生成的领域特定评分标准评估图形，使智能体能够实时修正自身错误并引导探索性数据分析。在宇宙学与天体化学领域的案例研究中，该方法展现了从错误推理路径中恢复以及无需人工干预即可适应新数据集的能力。在一项包含 10 个任务的数据驱动发现基准测试中，引入 VLM 的系统实现了 0.7–0.8 的通过率得分，显著优于仅使用代码基线（0.2–0.3）和代码与文本混合基线（0.4–0.5），同时提供了可审计的推理轨迹，从而提升了可解释性。相关代码见此处：https://github.com/CMBAgents/cmbagent",
        "translated_title": "借助视觉-语言模型能力增强自主科学发现中的代理行为",
        "label": [],
        "label_reason": "高阶科学发现任务，非图像像素级处理",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "VLM引导多智能体系统创新，但属高阶应用"
    },
    {
        "title": "Fusing Biomechanical and Spatio-Temporal Features for Fall Prediction: Characterizing and Mitigating the Simulation-to-Reality Gap",
        "url": "http://arxiv.org/abs/2511.14620v1",
        "pub_date": "2025-11-18",
        "summary": "Falls are a leading cause of injury and loss of independence among older adults. Vision-based fall prediction systems offer a non-invasive solution to anticipate falls seconds before impact, but their development is hindered by the scarcity of available fall data. Contributing to these efforts, this study proposes the Biomechanical Spatio-Temporal Graph Convolutional Network (BioST-GCN), a dual-stream model that combines both pose and biomechanical information using a cross-attention fusion mechanism. Our model outperforms the vanilla ST-GCN baseline by 5.32% and 2.91% F1-score on the simulated MCF-UA stunt-actor and MUVIM datasets, respectively. The spatio-temporal attention mechanisms in the ST-GCN stream also provide interpretability by identifying critical joints and temporal phases. However, a critical simulation-reality gap persists. While our model achieves an 89.0% F1-score with full supervision on simulated data, zero-shot generalization to unseen subjects drops to 35.9%. This performance decline is likely due to biases in simulated data, such as `intent-to-fall' cues. For older adults, particularly those with diabetes or frailty, this gap is exacerbated by their unique kinematic profiles. To address this, we propose personalization strategies and advocate for privacy-preserving data pipelines to enable real-world validation. Our findings underscore the urgent need to bridge the gap between simulated and real-world data to develop effective fall prediction systems for vulnerable elderly populations.",
        "translated": "跌倒已成为老年人受伤和丧失独立性的主要原因。基于视觉的跌倒预测系统可提供一种无创方案，在跌倒发生前数秒进行预警，但其发展受限于可用跌倒数据的稀缺性。为此，本研究提出了一种双流模型——生物力学时空图卷积网络（BioST-GCN），该模型利用跨注意力融合机制结合姿态与生物力学信息。在模拟的MCF-UA特技演员数据集和MUVIM数据集上，我们的模型相较基础的ST-GCN基线分别提升了5.32%和2.91%的F1分数。ST-GCN流中的时空注意力机制还提供了可解释性，能够识别关键关节与时间阶段。然而，一个关键的仿真-现实差距依然存在：尽管在仿真数据上采用全监督训练时模型达到89.0%的F1分数，但在零样本泛化至未见主体时性能骤降至35.9%。这一性能下降可能源于仿真数据中存在的偏差，例如“意图跌倒”提示信号。对于糖尿病患者或虚弱老年人等特殊群体，由于其独特的运动学特征，该差距更为显著。为解决此问题，我们提出个性化策略，并倡导隐私保护的数据管道以支持真实场景验证。本研究结果强调了弥合仿真与真实世界数据之间鸿沟的迫切需求，以开发适用于脆弱老年群体的有效跌倒预测系统。",
        "translated_title": "融合生物力学与时空特征进行跌倒预测：表征与缓解仿真到现实的差距",
        "label": [],
        "label_reason": "任务为动作预测，非图像像素级恢复",
        "relevance_score": 1,
        "novelty_score": 3,
        "novelty_reason": "方法改进图结构，无新图像处理范式"
    },
    {
        "title": "3D-Guided Scalable Flow Matching for Generating Volumetric Tissue Spatial Transcriptomics from Serial Histology",
        "url": "http://arxiv.org/abs/2511.14613v1",
        "pub_date": "2025-11-18",
        "summary": "A scalable and robust 3D tissue transcriptomics profile can enable a holistic understanding of tissue organization and provide deeper insights into human biology and disease. Most predictive algorithms that infer ST directly from histology treat each section independently and ignore 3D structure, while existing 3D-aware approaches are not generative and do not scale well. We present Holographic Tissue Expression Inpainting and Analysis (HoloTea), a 3D-aware flow-matching framework that imputes spot-level gene expression from H&amp;E while explicitly using information from adjacent sections. Our key idea is to retrieve morphologically corresponding spots on neighboring slides in a shared feature space and fuse this cross section context into a lightweight ControlNet, allowing conditioning to follow anatomical continuity. To better capture the count nature of the data, we introduce a 3D-consistent prior for flow matching that combines a learned zero-inflated negative binomial (ZINB) prior with a spatial-empirical prior constructed from neighboring sections. A global attention block introduces 3D H&amp;E scaling linearly with the number of spots in the slide, enabling training and inference on large 3D ST datasets. Across three spatial transcriptomics datasets spanning different tissue types and resolutions, HoloTea consistently improves 3D expression accuracy and generalization compared to 2D and 3D baselines. We envision HoloTea advancing the creation of accurate 3D virtual tissues, ultimately accelerating biomarker discovery and deepening our understanding of disease.",
        "translated": "一个可扩展且稳健的三维组织转录组学谱可以全面揭示组织结构，并为人类生物学与疾病研究提供更深入的洞察。目前大多数直接从组织切片推断空间转录组（ST）的预测算法将各切片独立处理，忽视了三维结构信息；而现有的三维感知方法多不具备生成能力，且难以扩展。我们提出 Holographic Tissue Expression Inpainting and Analysis (HoloTea)，这是一种三维感知的流匹配框架，能够在苏木精-伊红染色（H&E）图像中对斑点级基因表达进行插补，并显式利用相邻切片的信息。我们的核心思想是：在共享特征空间中检索邻近切片上形态学对应的斑点，并将其跨切片上下文融合到一个轻量级 ControlNet 中，以实现按解剖连续性进行条件建模。为了更好地捕捉数据的计数性质，我们引入了一种三维一致的流匹配先验，该先验结合了学习得到的零膨胀负二项分布（ZINB）先验与基于邻近切片构建的空间经验先验。一个全局注意力模块使三维 H&E 缩放方式与切片中的斑点数量呈线性关系，从而支持在大型三维 ST 数据集上进行训练和推理。在三个涵盖不同组织类型与分辨率的空间转录组数据集上，HoloTea 相较于二维及三维基线方法，在三维表达准确性与泛化性能方面均持续提升。我们期望 HoloTea 能推动精准三维虚拟组织的构建，最终加速生物标志物发现并深化对疾病的认知。",
        "translated_title": "基于三维引导的可扩展流匹配，用于从连续组织切片生成体积组织空间转录组学",
        "label": [],
        "label_reason": "生成3D组织转录组，属高阶生物信息建模",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "引入3D流匹配与ZINB先验提升表达预测"
    },
    {
        "title": "XAttn-BMD: Multimodal Deep Learning with Cross-Attention for Femoral Neck Bone Mineral Density Estimation",
        "url": "http://arxiv.org/abs/2511.14604v1",
        "pub_date": "2025-11-18",
        "summary": "Poor bone health is a significant public health concern, and low bone mineral density (BMD) leads to an increased fracture risk, a key feature of osteoporosis. We present XAttn-BMD (Cross-Attention BMD), a multimodal deep learning framework that predicts femoral neck BMD from hip X-ray images and structured clinical metadata. It utilizes a novel bidirectional cross-attention mechanism to dynamically integrate image and metadata features for cross-modal mutual reinforcement. A Weighted Smooth L1 loss is tailored to address BMD imbalance and prioritize clinically significant cases. Extensive experiments on the data from the Hertfordshire Cohort Study show that our model outperforms the baseline models in regression generalization and robustness. Ablation studies confirm the effectiveness of both cross-attention fusion and the customized loss function. Experimental results show that the integration of multimodal data via cross-attention outperforms naive feature concatenation without cross-attention, reducing MSE by 16.7%, MAE by 6.03%, and increasing the R2 score by 16.4%, highlighting the effectiveness of the approach for femoral neck BMD estimation. Furthermore, screening performance was evaluated using binary classification at clinically relevant femoral neck BMD thresholds, demonstrating the model's potential in real-world scenarios.",
        "translated": "骨健康状况不佳是重要的公共卫生问题，低骨矿物质密度（BMD）会显著增加骨折风险，这是骨质疏松症的关键特征。我们提出了 XAttn-BMD（跨注意力 BMD），一种多模态深度学习框架，可从髋部 X 线图像和结构化临床元数据中预测股骨颈 BMD。该模型采用新颖的双向跨注意力机制，动态融合图像与元数据特征，实现跨模态互促增强。为应对 BMD 数据分布不平衡并优先关注具有临床意义的病例，特别设计了加权平滑 L1 损失函数。在赫特福德郡队列研究数据上的大量实验表明，本模型在回归泛化能力与鲁棒性上均优于基线模型。消融实验验证了跨注意力融合模块与定制损失函数的有效性。实验结果表明，通过跨注意力机制整合多模态数据的效果优于无跨注意力机制的简单特征拼接方法，其均方误差（MSE）降低 16.7%，平均绝对误差（MAE）降低 6.03%，决定系数（R²）提升 16.4%，凸显该方法在股骨颈 BMD 估计任务中的有效性。此外，在临床相关的股骨颈 BMD 阈值下进行二分类评估，亦显示出该模型在真实应用场景中的潜在价值。",
        "translated_title": "XAttn-BMD：基于跨注意力机制的多模态深度学习用于股骨颈骨密度估计",
        "label": [],
        "label_reason": "任务为骨密度预测，属高阶医学分析",
        "relevance_score": 2,
        "novelty_score": 5,
        "novelty_reason": "多模态交叉注意力机制用于回归任务"
    },
    {
        "title": "MRI Embeddings Complement Clinical Predictors for Cognitive Decline Modeling in Alzheimer's Disease Cohorts",
        "url": "http://arxiv.org/abs/2511.14601v1",
        "pub_date": "2025-11-18",
        "summary": "Accurate modeling of cognitive decline in Alzheimer's disease is essential for early stratification and personalized management. While tabular predictors provide robust markers of global risk, their ability to capture subtle brain changes remains limited. In this study, we evaluate the predictive contributions of tabular and imaging-based representations, with a focus on transformer-derived Magnetic Resonance Imaging (MRI) embeddings. We introduce a trajectory-aware labeling strategy based on Dynamic Time Warping clustering to capture heterogeneous patterns of cognitive change, and train a 3D Vision Transformer (ViT) via unsupervised reconstruction on harmonized and augmented MRI data to obtain anatomy-preserving embeddings without progression labels. The pretrained encoder embeddings are subsequently assessed using both traditional machine learning classifiers and deep learning heads, and compared against tabular representations and convolutional network baselines. Results highlight complementary strengths across modalities. Clinical and volumetric features achieved the highest AUCs of around 0.70 for predicting mild and severe progression, underscoring their utility in capturing global decline trajectories. In contrast, MRI embeddings from the ViT model were most effective in distinguishing cognitively stable individuals with an AUC of 0.71. However, all approaches struggled in the heterogeneous moderate group. These findings indicate that clinical features excel in identifying high-risk extremes, whereas transformer-based MRI embeddings are more sensitive to subtle markers of stability, motivating multimodal fusion strategies for AD progression modeling.",
        "translated": "准确建模阿尔茨海默病中的认知衰退对于早期分层和个性化管理至关重要。虽然表格型预测因子能提供全球风险的稳健标志，但其捕捉大脑细微变化的能力仍显有限。本研究评估了表格型与基于影像的表征的预测贡献，重点关注由Transformer衍生的磁共振成像（MRI）嵌入。我们引入了一种基于动态时间规整聚类的轨迹感知标注策略，以捕捉认知变化的异质模式，并通过在标准化并增强的MRI数据上进行无监督重建，训练了一个三维视觉Transformer（ViT），从而获得无需进展标签即可保留解剖结构的嵌入表示。随后，使用传统机器学习分类器与深度学习头部对预训练编码器嵌入进行评估，并与表格型表征及卷积网络基线模型进行比较。结果表明，不同模态具有互补优势：临床与体积特征在预测轻度至重度进展时获得了约0.70的最高AUC值，凸显其在刻画全局衰退轨迹方面的实用性；相比之下，ViT模型生成的MRI嵌入在区分认知稳定个体方面表现最优，AUC达到0.71。然而，所有方法在异质性中等组别中均表现不佳。这些发现表明，临床特征擅长识别高危极端群体，而基于Transformer的MRI嵌入则更敏感于稳定性相关的细微指标，这促使我们在AD进展建模中探索多模态融合策略。",
        "translated_title": "MRI 引入的嵌入可补充临床预测因子，用于阿尔茨海默病队列中认知衰退建模",
        "label": [],
        "label_reason": "任务为多模态疾病预测，非图像像素级处理",
        "relevance_score": 1,
        "novelty_score": 3,
        "novelty_reason": "方法复用ViT架构，无本质创新"
    },
    {
        "title": "CCSD: Cross-Modal Compositional Self-Distillation for Robust Brain Tumor Segmentation with Missing Modalities",
        "url": "http://arxiv.org/abs/2511.14599v1",
        "pub_date": "2025-11-18",
        "summary": "The accurate segmentation of brain tumors from multi-modal MRI is critical for clinical diagnosis and treatment planning. While integrating complementary information from various MRI sequences is a common practice, the frequent absence of one or more modalities in real-world clinical settings poses a significant challenge, severely compromising the performance and generalizability of deep learning-based segmentation models. To address this challenge, we propose a novel Cross-Modal Compositional Self-Distillation (CCSD) framework that can flexibly handle arbitrary combinations of input modalities. CCSD adopts a shared-specific encoder-decoder architecture and incorporates two self-distillation strategies: (i) a hierarchical modality self-distillation mechanism that transfers knowledge across modality hierarchies to reduce semantic discrepancies, and (ii) a progressive modality combination distillation approach that enhances robustness to missing modalities by simulating gradual modality dropout during training. Extensive experiments on public brain tumor segmentation benchmarks demonstrate that CCSD achieves state-of-the-art performance across various missing-modality scenarios, with strong generalization and stability.",
        "translated": "从多模态MRI中精确分割脑肿瘤对临床诊断与治疗规划至关重要。虽然整合不同MRI序列的互补信息是常见做法，但在实际临床环境中，一种或多种模态的缺失频发，这给基于深度学习的分割模型性能和泛化能力带来了重大挑战。为应对这一挑战，我们提出了一种新颖的跨模态组合自蒸馏（Cross-Modal Compositional Self-Distillation, CCSD）框架，该框架能够灵活处理任意输入模态组合。CCSD采用共享-特定的编码器-解码器架构，并引入两种自蒸馏策略：(i) 层级模态自蒸馏机制，通过在模态层级间传递知识以降低语义差异；(ii) 逐步模态组合蒸馏方法，通过在训练过程中模拟渐进式模态缺失，增强模型对缺失模态的鲁棒性。在多个公开脑肿瘤分割基准数据集上的大量实验表明，CCSD在各种缺失模态场景下均达到当前最优性能，展现出强大的泛化能力和稳定性。",
        "translated_title": "CCSD：用于缺失模态下鲁棒脑肿瘤分割的跨模态组合式自蒸馏",
        "label": [],
        "label_reason": "任务为医学图像分割，属高阶视觉任务",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出新自蒸馏框架提升多模态鲁棒性"
    },
    {
        "title": "Deep Learning-Based Regional White Matter Hyperintensity Mapping as a Robust Biomarker for Alzheimer's Disease",
        "url": "http://arxiv.org/abs/2511.14588v1",
        "pub_date": "2025-11-18",
        "summary": "White matter hyperintensities (WMH) are key imaging markers in cognitive aging, Alzheimer's disease (AD), and related dementias. Although automated methods for WMH segmentation have advanced, most provide only global lesion load and overlook their spatial distribution across distinct white matter regions. We propose a deep learning framework for robust WMH segmentation and localization, evaluated across public datasets and an independent Alzheimer's Disease Neuroimaging Initiative (ADNI) cohort. Our results show that the predicted lesion loads are in line with the reference WMH estimates, confirming the robustness to variations in lesion load, acquisition, and demographics. Beyond accurate segmentation, we quantify WMH load within anatomically defined regions and combine these measures with brain structure volumes to assess diagnostic value. Regional WMH volumes consistently outperform global lesion burden for disease classification, and integration with brain atrophy metrics further improves performance, reaching area under the curve (AUC) values up to 0.97. Several spatially distinct regions, particularly within anterior white matter tracts, are reproducibly associated with diagnostic status, indicating localized vulnerability in AD. These results highlight the added value of regional WMH quantification. Incorporating localized lesion metrics alongside atrophy markers may enhance early diagnosis and stratification in neurodegenerative disorders.",
        "translated": "白质高信号（WMH）是认知老化、阿尔茨海默病（AD）及相关痴呆的重要影像学标志。尽管自动化的WMH分割方法已取得进展，但多数方法仅提供全局病变负荷，而忽视其在不同白质区域的空间分布特征。我们提出了一种深度学习框架，用于稳健的WMH分割与定位，并在多个公开数据集及独立的阿尔茨海默病神经影像倡议（ADNI）队列中进行了评估。结果表明，预测的病变负荷与参考的WMH估计值一致，证实了该方法对病变负荷、采集方式及人口统计学差异具有鲁棒性。除准确分割外，我们量化了在解剖学定义区域内的WMH负荷，并结合脑结构体积评估其诊断价值。区域性的WMH体积始终优于全局病变负担，用于疾病分类；当进一步与脑萎缩指标整合时，性能显著提升，达到曲线下面积（AUC）高达0.97。多个空间上相互分离的区域，特别是前部白质束内，可稳定地与诊断状态相关联，提示AD存在局部易损性。这些结果凸显了区域性WMH量化所具有的附加价值。将局部病灶指标与萎缩标志物相结合，或有助于提高神经退行性疾病早期诊断与分层能力。",
        "translated_title": "基于深度学习的区域白质高信号映射作为阿尔茨海默病的稳健生物标志物",
        "label": [],
        "label_reason": "聚焦区域分割与诊断，属高阶医学影像分析",
        "relevance_score": 2,
        "novelty_score": 5,
        "novelty_reason": "改进分割方法但未提升像素级图像质量"
    },
    {
        "title": "OmniZip: Audio-Guided Dynamic Token Compression for Fast Omnimodal Large Language Models",
        "url": "http://arxiv.org/abs/2511.14582v1",
        "pub_date": "2025-11-18",
        "summary": "Omnimodal large language models (OmniLLMs) have attracted increasing research attention of late towards unified audio-video understanding, wherein processing audio-video token sequences creates a significant computational bottleneck, however. Existing token compression methods have yet to accommodate this emerging need of jointly compressing multimodal tokens. To bridge this gap, we present OmniZip, a training-free, audio-guided audio-visual token-compression framework that optimizes multimodal token representation and accelerates inference. Specifically, OmniZip first identifies salient audio tokens, then computes an audio retention score for each time group to capture information density, thereby dynamically guiding video token pruning and preserving cues from audio anchors enhanced by cross-modal similarity. For each time window, OmniZip compresses the video tokens using an interleaved spatio-temporal scheme. Extensive empirical results demonstrate the merits of OmniZip - it achieves 3.42X inference speedup and 1.4X memory reduction over other top-performing counterparts, while maintaining performance with no training.",
        "translated": "近年来，多模态大型语言模型（OmniLLMs）在统一音视频理解方面吸引了越来越多的研究关注，然而处理音视频标记序列所带来的计算瓶颈问题却尚未得到充分解决。现有标记压缩方法尚无法满足当前对联合压缩多模态标记的新兴需求。为弥合这一差距，我们提出了 OmniZip——一种无需训练、以音频引导的音视频标记压缩框架，旨在优化多模态标记表示并加速推理过程。具体而言，OmniZip 首先识别显著音频标记，随后为每个时间组计算音频保留得分以捕捉信息密度，从而动态指导视频标记剪枝，并通过跨模态相似性增强来自音频锚点的有效线索保留。对于每个时间窗口，OmniZip 使用交错时空方案压缩视频标记。大量实验证明了 OmniZip 的优势：相比其他表现最优的方法，其推理速度提升达3.42倍，内存消耗降低1.4倍，且无需训练即可保持性能不变。",
        "translated_title": "OmniZip：音频引导的动态令牌压缩，用于加速多模态大语言模型",
        "label": [],
        "label_reason": "处理音频视频序列压缩，属高阶多模态任务",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出音频引导动态压缩框架，显著提升推理效率"
    },
    {
        "title": "Explaining Digital Pathology Models via Clustering Activations",
        "url": "http://arxiv.org/abs/2511.14558v1",
        "pub_date": "2025-11-18",
        "summary": "We present a clustering-based explainability technique for digital pathology models based on convolutional neural networks. Unlike commonly used methods based on saliency maps, such as occlusion, GradCAM, or relevance propagation, which highlight regions that contribute the most to the prediction for a single slide, our method shows the global behaviour of the model under consideration, while also providing more fine-grained information. The result clusters can be visualised not only to understand the model, but also to increase confidence in its operation, leading to faster adoption in clinical practice. We also evaluate the performance of our technique on an existing model for detecting prostate cancer, demonstrating its usefulness.",
        "translated": "我们提出了一种基于聚类的数字病理学模型可解释性技术，该技术基于卷积神经网络。与常用的基于显著性图的方法（如遮挡、GradCAM 或相关性传播）不同，后者仅突出单张玻片预测中贡献最大的区域，我们的方法则展示所研究模型的整体行为，并提供更细粒度的信息。所得聚类结果不仅可用于理解模型，还可增强对其运行的信任，从而加速其在临床实践中的应用。我们还在一个用于前列腺癌检测的现有模型上评估了该技术的有效性，证明了其实用性。",
        "translated_title": "通过聚类激活解释数字病理学模型",
        "label": [],
        "label_reason": "属于模型解释高阶任务，非图像像素级处理",
        "relevance_score": 1,
        "novelty_score": 2,
        "novelty_reason": "方法为解释性技术，无图像恢复创新"
    },
    {
        "title": "ForensicFlow: A Tri-Modal Adaptive Network for Robust Deepfake Detection",
        "url": "http://arxiv.org/abs/2511.14554v1",
        "pub_date": "2025-11-18",
        "summary": "Deepfakes generated by advanced GANs and autoencoders severely threaten information integrity and societal stability. Single-stream CNNs fail to capture multi-scale forgery artifacts across spatial, texture, and frequency domains, limiting robustness and generalization. We introduce the ForensicFlow, a tri-modal forensic framework that synergistically fuses RGB, texture, and frequency evidence for video Deepfake detection. The RGB branch (ConvNeXt-tiny) extracts global visual inconsistencies; the texture branch (Swin Transformer-tiny) detects fine-grained blending artifacts; the frequency branch (CNN + SE) identifies periodic spectral noise. Attention-based temporal pooling dynamically prioritizes high-evidence frames, while adaptive attention fusion balances branch contributions.Trained on Celeb-DF (v2) with Focal Loss, ForensicFlow achieves AUC 0.9752, F1-Score 0.9408, and accuracy 0.9208, outperforming single-stream baselines. Ablation validates branch synergy; Grad-CAM confirms forensic focus. This comprehensive feature fusion provides superior resilience against subtle forgeries.",
        "translated": "由先进生成对抗网络（GAN）和自编码器生成的 Deepfake 严重威胁信息完整性与社会稳定性。单流卷积神经网络（CNN）无法有效捕捉空间、纹理与频域中的多尺度伪造痕迹，限制了其鲁棒性与泛化能力。我们提出 ForensicFlow，一种三模态取证框架，协同融合 RGB、纹理与频域证据以实现视频 Deepfake 检测。RGB 分支（ConvNeXt-tiny）提取全局视觉不一致性；纹理分支（Swin Transformer-tiny）检测细粒度融合伪影；频域分支（CNN + SE）识别周期性频谱噪声。基于注意力机制的时序池化动态优先选择高证据帧，而自适应注意力融合则平衡各分支贡献。在 Celeb-DF (v2) 数据集上使用 Focal Loss 训练后，ForensicFlow 实现 AUC 0.9752、F1-Score 0.9408 与准确率 0.9208，优于单流基线方法。消融实验验证了各分支协同作用；Grad-CAM 结果确认了取证焦点区域。该全面的特征融合方案显著提升了对细微伪造内容的抗干扰能力。",
        "translated_title": "ForensicFlow：一种鲁棒的深度伪造检测三模态自适应网络",
        "label": [],
        "label_reason": "检测任务属高阶视觉，非像素级图像恢复。",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "多模态融合架构提升检测性能，有创新性。"
    },
    {
        "title": "Interaction-Aware 4D Gaussian Splatting for Dynamic Hand-Object Interaction Reconstruction",
        "url": "http://arxiv.org/abs/2511.14540v1",
        "pub_date": "2025-11-18",
        "summary": "This paper focuses on a challenging setting of simultaneously modeling geometry and appearance of hand-object interaction scenes without any object priors. We follow the trend of dynamic 3D Gaussian Splatting based methods, and address several significant challenges. To model complex hand-object interaction with mutual occlusion and edge blur, we present interaction-aware hand-object Gaussians with newly introduced optimizable parameters aiming to adopt piecewise linear hypothesis for clearer structural representation. Moreover, considering the complementarity and tightness of hand shape and object shape during interaction dynamics, we incorporate hand information into object deformation field, constructing interaction-aware dynamic fields to model flexible motions. To further address difficulties in the optimization process, we propose a progressive strategy that handles dynamic regions and static background step by step. Correspondingly, explicit regularizations are designed to stabilize the hand-object representations for smooth motion transition, physical interaction reality, and coherent lighting. Experiments show that our approach surpasses existing dynamic 3D-GS-based methods and achieves state-of-the-art performance in reconstructing dynamic hand-object interaction.",
        "translated": "本文聚焦于在无任何物体先验知识的情况下，同时建模手-物体交互场景的几何结构与外观特征这一具有挑战性的任务。我们遵循基于动态 3D 高斯点云（Dynamic 3D Gaussian Splatting）方法的发展趋势，并针对若干关键难点提出解决方案。为建模包含相互遮挡与边缘模糊的复杂手-物体交互场景，我们提出了具备交互感知能力的手-物体高斯点云模型，并引入了可优化参数，旨在采用分段线性假设以实现更清晰的结构表达。此外，考虑到手形与物体形在交互过程中的互补性与紧密关联性，我们将手部信息融入物体变形场中，构建了交互感知型动态场，从而有效建模柔性运动行为。为进一步解决优化过程中的困难，我们提出了一种渐进式策略，逐步处理动态区域与静态背景。相应地，我们设计了显式正则化项，以稳定手-物体表示，确保运动过渡平滑、物理交互真实且光照一致。实验结果表明，我们的方法优于现有基于动态 3D-GS 的方法，在重建动态手-物体交互场景方面达到了当前最优性能。",
        "translated_title": "面向动态手-物交互重建的交互感知四维高斯点云渲染",
        "label": [],
        "label_reason": "重建3D动态场景，非像素级图像处理",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "提出交互感知高斯点云优化策略，显著提升重建效果"
    },
    {
        "title": "Learning Compact Latent Space for Representing Neural Signed Distance Functions with High-fidelity Geometry Details",
        "url": "http://arxiv.org/abs/2511.14539v1",
        "pub_date": "2025-11-18",
        "summary": "Neural signed distance functions (SDFs) have been a vital representation to represent 3D shapes or scenes with neural networks. An SDF is an implicit function that can query signed distances at specific coordinates for recovering a 3D surface. Although implicit functions work well on a single shape or scene, they pose obstacles when analyzing multiple SDFs with high-fidelity geometry details, due to the limited information encoded in the latent space for SDFs and the loss of geometry details. To overcome these obstacles, we introduce a method to represent multiple SDFs in a common space, aiming to recover more high-fidelity geometry details with more compact latent representations. Our key idea is to take full advantage of the benefits of generalization-based and overfitting-based learning strategies, which manage to preserve high-fidelity geometry details with compact latent codes. Based on this framework, we also introduce a novel sampling strategy to sample training queries. The sampling can improve the training efficiency and eliminate artifacts caused by the influence of other SDFs. We report numerical and visual evaluations on widely used benchmarks to validate our designs and show advantages over the latest methods in terms of the representative ability and compactness.",
        "translated": "神经有符号距离函数（SDFs）已成为利用神经网络表示三维形状或场景的重要表达方式。SDF是一种隐式函数，可在特定坐标处查询有符号距离，从而重建三维表面。尽管隐式函数在单个形状或场景上表现良好，但在分析多个高保真几何细节的SDF时存在障碍，原因在于SDF的潜在空间编码信息有限，且易丢失几何细节。为克服这些障碍，我们提出一种方法，将多个SDF统一映射到共同空间中，旨在通过更紧凑的潜在表示恢复更丰富的高保真几何细节。我们的核心思想是充分利用基于泛化和过拟合的学习策略的优势，在保持高保真几何细节的同时实现紧凑的潜在编码。在此框架基础上，我们还引入了一种新颖的采样策略用于训练查询采样，该策略可提升训练效率，并消除因其他SDF干扰导致的伪影。我们在广泛使用的基准数据集上报告了数值与视觉评估结果，以验证设计的有效性，并在代表性能力和紧凑性方面优于最新方法。",
        "translated_title": "学习紧凑的潜在空间以高保真几何细节表示神经有符号距离函数",
        "label": [],
        "label_reason": "研究3D SDF表示，非图像像素级处理",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "改进SDF采样与编码，提升几何表征效率"
    },
    {
        "title": "DeCo-VAE: Learning Compact Latents for Video Reconstruction via Decoupled Representation",
        "url": "http://arxiv.org/abs/2511.14530v1",
        "pub_date": "2025-11-18",
        "summary": "Existing video Variational Autoencoders (VAEs) generally overlook the similarity between frame contents, leading to redundant latent modeling. In this paper, we propose decoupled VAE (DeCo-VAE) to achieve compact latent representation. Instead of encoding RGB pixels directly, we decompose video content into distinct components via explicit decoupling: keyframe, motion and residual, and learn dedicated latent representation for each. To avoid cross-component interference, we design dedicated encoders for each decoupled component and adopt a shared 3D decoder to maintain spatiotemporal consistency during reconstruction. We further utilize a decoupled adaptation strategy that freezes partial encoders while training the others sequentially, ensuring stable training and accurate learning of both static and dynamic features. Extensive quantitative and qualitative experiments demonstrate that DeCo-VAE achieves superior video reconstruction performance.",
        "translated": "现有视频变分自编码器（VAEs）通常忽视帧内容间的相似性，导致潜在表示冗余。本文提出解耦式VAE（DeCo-VAE），以实现紧凑的潜在表示。不同于直接对RGB像素进行编码，我们通过显式解耦将视频内容分解为三个独立成分：关键帧、运动与残差，并分别为其学习专用的潜在表示。为避免各成分间的相互干扰，我们为每个解耦成分设计专用编码器，并采用共享的3D解码器，在重建过程中保持时空一致性。此外，我们进一步引入解耦适应策略，在训练时冻结部分编码器，依次训练其余编码器，从而确保训练稳定并准确学习静态与动态特征。大量定量与定性实验表明，DeCo-VAE在视频重建性能上表现更优。",
        "translated_title": "DeCo-VAE：通过解耦表征学习紧凑潜在变量以实现视频重建",
        "label": [
            "多帧/视频图像恢复"
        ],
        "label_reason": "方法针对视频重建，属低级任务中的序列恢复",
        "relevance_score": 4,
        "novelty_score": 7,
        "novelty_reason": "提出解耦表示与分步训练策略提升性能"
    },
    {
        "title": "A Generative Data Framework with Authentic Supervision for Underwater Image Restoration and Enhancement",
        "url": "http://arxiv.org/abs/2511.14521v1",
        "pub_date": "2025-11-18",
        "summary": "Underwater image restoration and enhancement are crucial for correcting color distortion and restoring image details, thereby establishing a fundamental basis for subsequent underwater visual tasks. However, current deep learning methodologies in this area are frequently constrained by the scarcity of high-quality paired datasets. Since it is difficult to obtain pristine reference labels in underwater scenes, existing benchmarks often rely on manually selected results from enhancement algorithms, providing debatable reference images that lack globally consistent color and authentic supervision. This limits the model's capabilities in color restoration, image enhancement, and generalization. To overcome this limitation, we propose using in-air natural images as unambiguous reference targets and translating them into underwater-degraded versions, thereby constructing synthetic datasets that provide authentic supervision signals for model learning. Specifically, we establish a generative data framework based on unpaired image-to-image translation, producing a large-scale dataset that covers 6 representative underwater degradation types. The framework constructs synthetic datasets with precise ground-truth labels, which facilitate the learning of an accurate mapping from degraded underwater images to their pristine scene appearances. Extensive quantitative and qualitative experiments across 6 representative network architectures and 3 independent test sets show that models trained on our synthetic data achieve comparable or superior color restoration and generalization performance to those trained on existing benchmarks. This research provides a reliable and scalable data-driven solution for underwater image restoration and enhancement. The generated dataset is publicly available at: https://github.com/yftian2025/SynUIEDatasets.git.",
        "translated": "水下图像恢复与增强对于校正色彩畸变、重建图像细节至关重要，为后续水下视觉任务奠定基础。然而，当前该领域深度学习方法常受限于高质量配对数据集的稀缺性。由于水下场景中难以获取纯净的参考标签，现有基准数据集多依赖人工选取的增强算法结果作为参考图像，这些参考图像在全局色彩一致性及真实监督信号方面存在争议，限制了模型在色彩恢复、图像增强和泛化能力方面的表现。为克服这一局限，我们提出以空气中自然图像作为明确无歧义的参考目标，并将其转换为水下退化版本，从而构建提供真实监督信号的合成数据集，辅助模型学习。具体而言，我们基于非配对图像到图像翻译框架构建生成式数据系统，生成覆盖6种代表性水下退化类型的大型数据集。该框架构建的合成数据集包含精确的真值标签，有助于学习从退化水下图像到原始场景外观的准确映射关系。在6种代表性网络架构与3个独立测试集上的大量定量与定性实验表明，使用本合成数据训练的模型在色彩恢复与泛化性能上可达到或优于现有基准数据集训练所得模型的效果。本研究为水下图像恢复与增强提供了可靠且可扩展的数据驱动解决方案。所生成数据集已公开发布于：https://github.com/yftian2025/SynUIEDatasets.git。",
        "translated_title": "一种具有真实监督的生成式数据框架用于水下图像恢复与增强",
        "label": [
            "图像恢复",
            "图像增强",
            "遥感图像复原"
        ],
        "label_reason": "解决水下图像质量恢复与增强的低级视觉问题",
        "relevance_score": 10,
        "novelty_score": 9,
        "novelty_reason": "提出生成式数据框架，提供真实监督信号"
    },
    {
        "title": "D-PerceptCT: Deep Perceptual Enhancement for Low-Dose CT Images",
        "url": "http://arxiv.org/abs/2511.14518v1",
        "pub_date": "2025-11-18",
        "summary": "Low Dose Computed Tomography (LDCT) is widely used as an imaging solution to aid diagnosis and other clinical tasks. However, this comes at the price of a deterioration in image quality due to the low dose of radiation used to reduce the risk of secondary cancer development. While some efficient methods have been proposed to enhance LDCT quality, many overestimate noise and perform excessive smoothing, leading to a loss of critical details. In this paper, we introduce D-PerceptCT, a novel architecture inspired by key principles of the Human Visual System (HVS) to enhance LDCT images. The objective is to guide the model to enhance or preserve perceptually relevant features, thereby providing radiologists with CT images where critical anatomical structures and fine pathological details are perceptu- ally visible. D-PerceptCT consists of two main blocks: 1) a Visual Dual-path Extractor (ViDex), which integrates semantic priors from a pretrained DINOv2 model with local spatial features, allowing the network to incorporate semantic-awareness during enhancement; (2) a Global-Local State-Space block that captures long-range information and multiscale features to preserve the important structures and fine details for diagnosis. In addition, we propose a novel deep perceptual loss, designated as the Deep Perceptual Relevancy Loss Function (DPRLF), which is inspired by human contrast sensitivity, to further emphasize perceptually important features. Extensive experiments on the Mayo2016 dataset demonstrate the effectiveness of D-PerceptCT method for LDCT enhancement, showing better preservation of structural and textural information within LDCT images compared to SOTA methods.",
        "translated": "低剂量计算机断层扫描（LDCT）被广泛应用于辅助诊断及其他临床任务的成像方案。然而，其代价是图像质量因使用较低辐射剂量以降低二次癌症风险而下降。尽管已有若干高效方法提出用于提升LDCT图像质量，但许多方法仍过度估计噪声并进行过度平滑，导致关键细节丢失。在本文中，我们提出了D-PerceptCT，一种受人类视觉系统（HVS）关键原理启发的新型架构，旨在增强LDCT图像。该架构的目标是引导模型增强或保留感知上重要的特征，从而为放射科医生提供清晰可见关键解剖结构与精细病理细节的CT图像。D-PerceptCT包含两个主要模块：1）视觉双路径提取器（ViDex），结合预训练DINOv2模型提供的语义先验与局部空间特征，使网络在增强过程中具备语义感知能力；2）全局-局部状态空间块，用于捕捉长距离信息与多尺度特征，以保留对诊断至关重要的结构与细粒度细节。此外，我们提出了一种新的深度感知损失函数——深度感知相关性损失函数（DPRLF），其灵感源于人眼对比敏感度机制，进一步强调感知上重要特征。在Mayo2016数据集上的大量实验表明，D-PerceptCT方法在LDCT图像增强方面优于当前最先进的方法，能更有效地保留LDCT图像中的结构与纹理信息。",
        "translated_title": "D-PerceptCT：面向低剂量 CT 图像的深度感知增强",
        "label": [
            "CT金属伪影消除",
            "医学图像增强"
        ],
        "label_reason": "直接解决低剂量CT图像质量恢复问题",
        "relevance_score": 10,
        "novelty_score": 9,
        "novelty_reason": "提出基于HVS的感知增强架构与新损失函数"
    },
    {
        "title": "IMSE: Efficient U-Net-based Speech Enhancement using Inception Depthwise Convolution and Amplitude-Aware Linear Attention",
        "url": "http://arxiv.org/abs/2511.14515v1",
        "pub_date": "2025-11-18",
        "summary": "Achieving a balance between lightweight design and high performance remains a significant challenge for speech enhancement (SE) tasks on resource-constrained devices. Existing state-of-the-art methods, such as MUSE, have established a strong baseline with only 0.51M parameters by introducing a Multi-path Enhanced Taylor (MET) transformer and Deformable Embedding (DE). However, an in-depth analysis reveals that MUSE still suffers from efficiency bottlenecks: the MET module relies on a complex \"approximate-compensate\" mechanism to mitigate the limitations of Taylor-expansion-based attention, while the offset calculation for deformable embedding introduces additional computational burden. This paper proposes IMSE, a systematically optimized and ultra-lightweight network. We introduce two core innovations: 1) Replacing the MET module with Amplitude-Aware Linear Attention (MALA). MALA fundamentally rectifies the \"amplitude-ignoring\" problem in linear attention by explicitly preserving the norm information of query vectors in the attention calculation, achieving efficient global modeling without an auxiliary compensation branch. 2) Replacing the DE module with Inception Depthwise Convolution (IDConv). IDConv borrows the Inception concept, decomposing large-kernel operations into efficient parallel branches (square, horizontal, and vertical strips), thereby capturing spectrogram features with extremely low parameter redundancy. Extensive experiments on the VoiceBank+DEMAND dataset demonstrate that, compared to the MUSE baseline, IMSE significantly reduces the parameter count by 16.8\\% (from 0.513M to 0.427M) while achieving competitive performance comparable to the state-of-the-art on the PESQ metric (3.373). This study sets a new benchmark for the trade-off between model size and speech quality in ultra-lightweight speech enhancement.",
        "translated": "在资源受限设备上实现轻量级设计与高性能之间的平衡，仍是语音增强（SE）任务面临的重要挑战。现有最先进方法如 MUSE 通过引入多路径增强泰勒（MET）变换器和可变形嵌入（DE），仅以 0.51M 参数即建立了强劲基线。然而深入分析表明，MUSE 仍存在效率瓶颈：MET 模块依赖复杂的“近似补偿”机制以缓解基于泰勒展开的注意力机制的局限性；而可变形嵌入的偏移量计算亦引入了额外的计算开销。本文提出 IMSE，一种系统性优化的超轻量网络。我们引入两项核心创新：1）用振幅感知线性注意力（MALA）替代 MET 模块。MALA 在注意力计算中显式保留查询向量的范数信息，从根本上修正了线性注意力中的“振幅忽略”问题，从而在无需辅助补偿分支的情况下高效实现全局建模。2）用深度可分离卷积结构 Inception Depthwise Convolution（IDConv）替代 DE 模块。IDConv 借鉴 Inception 思想，将大核操作分解为高效的并行分支（方形、水平与垂直条带），从而以极低参数冗余度捕获谱图特征。在 VoiceBank+DEMAND 数据集上的大量实验表明，相比 MUSE 基线，IMSE 将参数量显著降低 16.8%（从 0.513M 降至 0.427M），同时在 PESQ 指标上达到与当前最先进方法相当的性能（3.373）。本研究为超轻量语音增强领域中模型规模与语音质量的权衡设定了新基准。",
        "translated_title": "IMSE：基于U-Net的高效语音增强方法，采用Inception深度卷积与幅值感知线性注意力机制",
        "label": [],
        "label_reason": "处理语音增强，非图像处理任务",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出轻量结构改进，但属音频领域"
    },
    {
        "title": "NeuCLIRBench: A Modern Evaluation Collection for Monolingual, Cross-Language, and Multilingual Information Retrieval",
        "url": "http://arxiv.org/abs/2511.14758v1",
        "pub_date": "2025-11-18",
        "summary": "To measure advances in retrieval, test collections with relevance judgments that can faithfully distinguish systems are required. This paper presents NeuCLIRBench, an evaluation collection for cross-language and multilingual retrieval. The collection consists of documents written natively in Chinese, Persian, and Russian, as well as those same documents machine translated into English. The collection supports several retrieval scenarios including: monolingual retrieval in English, Chinese, Persian, or Russian; cross-language retrieval with English as the query language and one of the other three languages as the document language; and multilingual retrieval, again with English as the query language and relevant documents in all three languages. NeuCLIRBench combines the TREC NeuCLIR track topics of 2022, 2023, and 2024. The 250,128 judgments across approximately 150 queries for the monolingual and cross-language tasks and 100 queries for multilingual retrieval provide strong statistical discriminatory power to distinguish retrieval approaches. A fusion baseline of strong neural retrieval systems is included with the collection so that developers of reranking algorithms are no longer reliant on BM25 as their first-stage retriever. NeuCLIRBench is publicly available.",
        "translated": "为衡量检索技术的进步，需要具备相关性判断且能真实区分不同系统的测试集合。本文提出了NeuCLIRBench，这是一个面向跨语言与多语言检索的评估数据集。该数据集包含原生以中文、波斯语和俄语撰写的文档，以及这些文档经机器翻译后的英文版本。该数据集支持多种检索场景，包括：在英语、中文、波斯语或俄语中进行单语言检索；以英语为查询语言、其他三种语言之一为文档语言的跨语言检索；以及以英语为查询语言、同时检索中文、波斯语和俄语相关文档的多语言检索。NeuCLIRBench整合了TREC NeuCLIR赛道2022、2023及2024年的评测主题。针对单语言与跨语言任务，约150个查询对应250,128条相关性判断；针对多语言检索任务，则有100个查询。这些数据提供了强大的统计区分能力，以便有效区分不同的检索方法。数据集中还包含若干强神经检索系统的融合基线，使重排序算法开发者不再依赖BM25作为其第一阶段检索器。NeuCLIRBench已公开发布。",
        "translated_title": "NeuCLIRBench：面向单语、跨语言与多语言信息检索的现代评估基准集",
        "label": [],
        "label_reason": "聚焦信息检索评估，非推荐系统核心问题",
        "relevance_score": 3,
        "novelty_score": 5,
        "novelty_reason": "提供多语言评测集，但无推荐系统创新"
    },
    {
        "title": "LiveRAG: A diverse Q&amp;A dataset with varying difficulty level for RAG evaluation",
        "url": "http://arxiv.org/abs/2511.14531v1",
        "pub_date": "2025-11-18",
        "summary": "With Retrieval Augmented Generation (RAG) becoming more and more prominent in generative AI solutions, there is an emerging need for systematically evaluating their effectiveness. We introduce the LiveRAG benchmark, a publicly available dataset of 895 synthetic questions and answers designed to support systematic evaluation of RAG-based Q&amp;A systems. This synthetic benchmark is derived from the one used during the SIGIR'2025 LiveRAG Challenge, where competitors were evaluated under strict time constraints. It is augmented with information that was not made available to competitors during the Challenge, such as the ground-truth answers, together with their associated supporting claims which were used for evaluating competitors' answers. In addition, each question is associated with estimated difficulty and discriminability scores, derived from applying an Item Response Theory model to competitors' responses. Our analysis highlights the benchmark's questions diversity, the wide range of their difficulty levels, and their usefulness in differentiating between system capabilities. The LiveRAG benchmark will hopefully help the community advance RAG research, conduct systematic evaluation, and develop more robust Q&amp;A systems.",
        "translated": "随着检索增强生成（RAG）在生成式人工智能解决方案中的日益普及，系统性评估其有效性变得愈发迫切。我们引入了LiveRAG基准数据集，该数据集包含895组合成问题与答案，旨在支持对基于RAG的问答系统的系统性评估。该合成基准源自SIGIR’2025 LiveRAG挑战赛所使用的数据集，在挑战赛中参赛者需在严格的时间约束下进行评测。本基准额外补充了在挑战赛期间未向参赛者公开的信息，例如真实答案及其对应的支撑论据，后者用于评估参赛者答案的质量。此外，每个问题均关联有难度估计值和区分度评分，这些评分是通过将项目反应理论模型应用于参赛者回答所推导得出的。我们的分析突显了该基准在问题多样性、难度梯度覆盖范围以及区分系统能力方面的实用性。LiveRAG基准有望助力学术界推进RAG研究，开展系统性评估，并开发出更鲁棒的问答系统。",
        "translated_title": "LiveRAG：一个具有不同难度等级的多样化问答数据集，用于评估检索增强生成（RAG）系统",
        "label": [],
        "label_reason": "论文聚焦RAG评估，非推荐系统核心环节。",
        "relevance_score": 3,
        "novelty_score": 5,
        "novelty_reason": "提供新基准数据集，但无推荐系统创新。"
    },
    {
        "title": "Effective Diversification of Multi-Carousel Book Recommendation",
        "url": "http://arxiv.org/abs/2511.14461v1",
        "pub_date": "2025-11-18",
        "summary": "Using multiple carousels, lists that wrap around and can be scrolled, is the basis for offering content in most contemporary movie streaming platforms. Carousels allow for highlighting different aspects of users' taste, that fall in categories such as genres and authors. However, while carousels offer structure and greater ease of navigation, they alone do not increase diversity in recommendations, while this is essential to keep users engaged. In this work we propose several approaches to effectively increase item diversity within the domain of book recommendations, on top of a collaborative filtering algorithm. These approaches are intended to improve book recommendations in the web catalogs of public libraries. Furthermore, we introduce metrics to evaluate the resulting strategies, and show that the proposed system finds a suitable balance between accuracy and beyond-accuracy aspects.",
        "translated": "使用多个轮播图和可滚动的列表，是当前大多数电影流媒体平台呈现内容的基础。轮播图能够突出展示用户偏好的不同方面，如类型与作者等分类。然而，尽管轮播图提供了结构化布局并提升了导航便捷性，但它们本身并不能有效提升推荐结果的多样性，而多样性对于维持用户参与度至关重要。在本研究中，我们提出若干方法，在协同过滤算法的基础上，有效提升图书推荐领域的物料多样性，以改善公共图书馆网站目录中的图书推荐效果。此外，我们引入了评估所提策略的指标，并表明该系统能够在准确率与超越准确率的其他维度之间取得合适的平衡。",
        "translated_title": "多轮书推荐的有效多样性增强",
        "label": [
            "召回",
            "通用推荐技术"
        ],
        "label_reason": "聚焦图书推荐多样性，属召回层优化",
        "relevance_score": 6,
        "novelty_score": 5,
        "novelty_reason": "改进CF算法多样性，非全新范式"
    },
    {
        "title": "Jasper-Token-Compression-600M Technical Report",
        "url": "http://arxiv.org/abs/2511.14405v1",
        "pub_date": "2025-11-18",
        "summary": "This technical report presents the training methodology and evaluation results of the open-source Jasper-Token-Compression-600M model, released in November 2025. Building on previous distillation-based recipes from the English Stella and Jasper models, we successfully extend this approach to a bilingual (English and Chinese) domain, further enhancing model performance through the incorporation of contrastive learning. A key innovation of our model is the introduction of a one-dimensional convolution-based token compression module. We dynamically adjust the compression rate during training, enabling the model to learn more robust and efficient compressed text representations. By combining knowledge distillation with token compression techniques, we achieve significant improvements in both embedding quality and inference efficiency. Our model performs with higher efficiency than a traditional 0.6B model while achieving performance comparable to that of an 8B model. For more information on the model release, visit: https://huggingface.co/infgrad/Jasper-Token-Compression-600M.",
        "translated": "本技术报告介绍了于2025年11月发布的开源Jasper-Token-Compression-600M模型的训练方法与评估结果。在先前基于蒸馏的英文Stella和Jasper模型经验基础上，我们成功将该方法扩展至双语（英文与中文）领域，并通过引入对比学习进一步提升模型性能。本模型的关键创新在于引入了一种基于一维卷积的Token压缩模块。我们在训练过程中动态调整压缩率，使模型能够学习到更鲁棒且高效的压缩文本表示。通过结合知识蒸馏与Token压缩技术，我们在嵌入质量与推理效率方面均取得了显著提升。该模型在效率上优于传统0.6B模型，同时其性能可媲美8B模型。有关模型发布详情，请访问：https://huggingface.co/infgrad/Jasper-Token-Compression-600M。",
        "translated_title": "Jasper-Token-Compression-600M 技术报告",
        "label": [],
        "label_reason": "模型压缩技术非推荐系统核心环节",
        "relevance_score": 3,
        "novelty_score": 6,
        "novelty_reason": "常规压缩方法，无推荐领域创新"
    },
    {
        "title": "Infer As You Train: A Symmetric Paradigm of Masked Generative for Click-Through Rate Prediction",
        "url": "http://arxiv.org/abs/2511.14403v1",
        "pub_date": "2025-11-18",
        "summary": "Generative models are increasingly being explored in click-through rate (CTR) prediction field to overcome the limitations of the conventional discriminative paradigm, which rely on a simple binary classification objective. However, existing generative models typically confine the generative paradigm to the training phase, primarily for representation learning. During online inference, they revert to a standard discriminative paradigm, failing to leverage their powerful generative capabilities to further improve prediction accuracy. This fundamental asymmetry between the training and inference phases prevents the generative paradigm from realizing its full potential. To address this limitation, we propose the Symmetric Masked Generative Paradigm for CTR prediction (SGCTR), a novel framework that establishes symmetry between the training and inference phases. Specifically, after acquiring generative capabilities by learning feature dependencies during training, SGCTR applies the generative capabilities during online inference to iteratively redefine the features of input samples, which mitigates the impact of noisy features and enhances prediction accuracy. Extensive experiments validate the superiority of SGCTR, demonstrating that applying the generative paradigm symmetrically across both training and inference significantly unlocks its power in CTR prediction.",
        "translated": "生成式模型正越来越多地被应用于点击率（CTR）预测领域，以克服传统判别式范式的局限性，后者依赖于简单的二分类目标。然而，现有的生成式模型通常仅将生成范式局限于训练阶段，主要用于表示学习；在在线推理阶段，它们则退回到标准的判别式范式，未能充分利用其强大的生成能力进一步提升预测精度。这种训练与推理阶段之间根本性的不对称性，阻碍了生成范式潜力的充分发挥。为解决这一局限，我们提出了用于CTR预测的对称掩码生成范式（SGCTR），这是一种新颖框架，旨在建立训练与推理阶段之间的对称性。具体而言，在训练阶段通过学习特征依赖关系获得生成能力后，SGCTR在在线推理阶段应用该生成能力，迭代性地重新定义输入样本的特征，从而缓解噪声特征的影响并提升预测准确性。大量实验验证了SGCTR的优越性，表明在训练与推理阶段对称地应用生成范式，能显著释放其在CTR预测中的潜力。",
        "translated_title": "边训练边推理：一种用于点击率预测的对称式掩码生成范式",
        "label": [
            "精排（Ranking）"
        ],
        "label_reason": "聚焦CTR预测，属推荐系统精排环节",
        "relevance_score": 8,
        "novelty_score": 9,
        "novelty_reason": "提出训练推理对称生成范式，显著提升效果"
    },
    {
        "title": "PathMind: A Retrieve-Prioritize-Reason Framework for Knowledge Graph Reasoning with Large Language Models",
        "url": "http://arxiv.org/abs/2511.14256v1",
        "pub_date": "2025-11-18",
        "summary": "Knowledge graph reasoning (KGR) is the task of inferring new knowledge by performing logical deductions on knowledge graphs. Recently, large language models (LLMs) have demonstrated remarkable performance in complex reasoning tasks. Despite promising success, current LLM-based KGR methods still face two critical limitations. First, existing methods often extract reasoning paths indiscriminately, without assessing their different importance, which may introduce irrelevant noise that misleads LLMs. Second, while many methods leverage LLMs to dynamically explore potential reasoning paths, they require high retrieval demands and frequent LLM calls. To address these limitations, we propose PathMind, a novel framework designed to enhance faithful and interpretable reasoning by selectively guiding LLMs with important reasoning paths. Specifically, PathMind follows a \"Retrieve-Prioritize-Reason\" paradigm. First, it retrieves a query subgraph from KG through the retrieval module. Next, it introduces a path prioritization mechanism that identifies important reasoning paths using a semantic-aware path priority function, which simultaneously considers the accumulative cost and the estimated future cost for reaching the target. Finally, PathMind generates accurate and logically consistent responses via a dual-phase training strategy, including task-specific instruction tuning and path-wise preference alignment. Extensive experiments on benchmark datasets demonstrate that PathMind consistently outperforms competitive baselines, particularly on complex reasoning tasks with fewer input tokens, by identifying essential reasoning paths.",
        "translated": "知识图谱推理（KGR）是在知识图谱上进行逻辑推演以推断新知识的任务。近期，大语言模型（LLM）在复杂推理任务中展现出卓越性能。尽管成果显著，当前基于LLM的KGR方法仍面临两大关键局限：首先，现有方法常无差别提取推理路径，未评估不同路径的重要性，可能引入无关噪声干扰LLM；其次，虽多数方法借助LLM动态探索潜在推理路径，但其对检索需求高、频繁调用LLM。为解决上述问题，我们提出PathMind框架，旨在通过选择性引导重要推理路径，提升LLM推理的忠实性与可解释性。具体而言，PathMind遵循“检索-优先-推理”范式：首先，通过检索模块从知识图谱中获取查询子图；其次，引入路径优先机制，利用语义感知的路径优先函数识别重要推理路径，该函数同时考虑累积成本与抵达目标节点的预估未来成本；最后，通过双阶段训练策略生成准确且逻辑一致的响应，包含任务特定指令调优与路径级偏好对齐。大量实验表明，PathMind在基准数据集上持续优于主流基线方法，尤其在输入token较少的复杂推理任务中，通过精准识别关键推理路径实现显著性能提升。",
        "translated_title": "PathMind：一种基于大语言模型的知识图谱推理检索-优先-推理框架",
        "label": [],
        "label_reason": "论文聚焦知识图谱推理，非推荐系统相关",
        "relevance_score": 3,
        "novelty_score": 7,
        "novelty_reason": "路径优先机制创新，但非推荐场景应用"
    },
    {
        "title": "LLM-Aligned Geographic Item Tokenization for Local-Life Recommendation",
        "url": "http://arxiv.org/abs/2511.14221v1",
        "pub_date": "2025-11-18",
        "summary": "Recent advances in Large Language Models (LLMs) have enhanced text-based recommendation by enriching traditional ID-based methods with semantic generalization capabilities. Text-based methods typically encode item textual information via prompt design and generate discrete semantic IDs through item tokenization. However, in domain-specific tasks such as local-life services, simply injecting location information into prompts fails to capture fine-grained spatial characteristics and real-world distance awareness among items. To address this, we propose LGSID, an LLM-Aligned Geographic Item Tokenization Framework for Local-life Recommendation. This framework consists of two key components: (1) RL-based Geographic LLM Alignment, and (2) Hierarchical Geographic Item Tokenization. In the RL-based alignment module, we initially train a list-wise reward model to capture real-world spatial relationships among items. We then introduce a novel G-DPO algorithm that uses pre-trained reward model to inject generalized spatial knowledge and collaborative signals into LLMs while preserving their semantic understanding. Furthermore, we propose a hierarchical geographic item tokenization strategy, where primary tokens are derived from discrete spatial and content attributes, and residual tokens are refined using the aligned LLM's geographic representation vectors. Extensive experiments on real-world Kuaishou industry datasets show that LGSID consistently outperforms state-of-the-art discriminative and generative recommendation models. Ablation studies, visualizations, and case studies further validate its effectiveness.",
        "translated": "大语言模型（LLMs）的最新进展通过为传统的基于ID的推荐方法赋予语义泛化能力，提升了文本驱动的推荐效果。文本类方法通常通过提示设计编码物品文本信息，并通过物品分词生成离散语义ID。然而，在本地生活服务等特定领域任务中，仅将地理位置信息注入提示无法捕捉物品间精细的空间特征和现实世界中的距离感知能力。为此，我们提出LGSID——一种面向本地生活推荐的、与LLM对齐的地理物品分词框架。该框架包含两个关键组件：(1) 基于强化学习的地理LLM对齐模块，以及(2) 分层地理物品分词策略。在基于强化学习的对齐模块中，我们首先训练一个列表级奖励模型以捕捉物品间的现实空间关系；随后引入一种新颖的G-DPO算法，利用预训练奖励模型将泛化的空间知识及协同信号注入LLM，同时保留其语义理解能力。此外，我们提出一种分层地理物品分词策略，其中主分词来源于离散的空间与内容属性，残差分词则借助对齐后的LLM地理表征向量进行精细化处理。在真实世界快手行业数据集上的大量实验表明，LGSID持续优于当前最先进的判别式与生成式推荐模型。消融研究、可视化分析及案例分析进一步验证了其有效性。",
        "translated_title": "面向本地生活推荐的大语言模型对齐地理物料分词",
        "label": [
            "LLM生成式推荐",
            "召回",
            "精排"
        ],
        "label_reason": "结合LLM与地理信息优化推荐，聚焦本地生活场景。",
        "relevance_score": 9,
        "novelty_score": 8,
        "novelty_reason": "提出G-DPO算法与分层地理Token化，显著提升空间建模能力。"
    },
    {
        "title": "WebRec: Enhancing LLM-based Recommendations with Attention-guided RAG from Web",
        "url": "http://arxiv.org/abs/2511.14182v1",
        "pub_date": "2025-11-18",
        "summary": "Recommender systems play a vital role in alleviating information overload and enriching users' online experience. In the era of large language models (LLMs), LLM-based recommender systems have emerged as a prevalent paradigm for advancing personalized recommendations. Recently, retrieval-augmented generation (RAG) has drawn growing interest to facilitate the recommendation capability of LLMs, incorporating useful information retrieved from external knowledge bases. However, as a rich source of up-to-date information, the web remains under-explored by existing RAG-based recommendations. In particular, unique challenges are posed from two perspectives: one is to generate effective queries for web retrieval, considering the inherent knowledge gap between web search and recommendations; another challenge lies in harnessing online websites that contain substantial noisy content. To tackle these limitations, we propose WebRec, a novel web-based RAG framework, which takes advantage of the reasoning capability of LLMs to interpret recommendation tasks into queries of user preferences that cater to web retrieval. Moreover, given noisy web-retrieved information, where relevant pieces of evidence are scattered far apart, an insightful MP-Head is designed to enhance LLM attentions between distant tokens of relevant information via message passing. Extensive experiments have been conducted to demonstrate the effectiveness of our proposed web-based RAG methods in recommendation scenarios.",
        "translated": "推荐系统在缓解信息过载、丰富用户在线体验方面发挥着至关重要的作用。在大语言模型（LLMs）时代，基于LLM的推荐系统已成为推动个性化推荐发展的主流范式。近年来，检索增强生成（RAG）因能够通过从外部知识库中检索有用信息来提升LLM的推荐能力而受到广泛关注。然而，作为丰富的实时信息来源，网络在现有基于RAG的推荐方法中仍被严重忽视。尤其地，这一问题从两个角度提出了独特挑战：其一是如何为网络检索生成有效的查询语句，需考虑网络搜索与推荐任务之间固有的知识鸿沟；另一挑战在于有效利用包含大量噪声内容的在线网页。为应对上述局限性，我们提出WebRec，一种新颖的基于网络的RAG框架，该框架借助LLM的推理能力将推荐任务转化为适配网络检索的用户偏好查询语句。此外，鉴于网络检索结果常包含分散且不连续的相关证据片段，我们设计了一种具有洞察力的MP-Head模块，通过消息传递机制增强LLM对远距离相关信息片段间注意力权重的捕捉能力。大量的实验已验证了我们在推荐场景下所提出的基于网络的RAG方法的有效性。",
        "translated_title": "WebRec：基于注意力引导的检索增强生成技术提升大语言模型驱动的推荐系统",
        "label": [
            "LLM生成式推荐",
            "重排"
        ],
        "label_reason": "直接构建LLM驱动的RAG推荐框架，聚焦召回与重排环节",
        "relevance_score": 10,
        "novelty_score": 9,
        "novelty_reason": "创新性设计MP-Head增强长距离注意力，提升信息整合能力"
    },
    {
        "title": "Applying Relation Extraction and Graph Matching to Answering Multiple Choice Questions",
        "url": "http://arxiv.org/abs/2511.14144v1",
        "pub_date": "2025-11-18",
        "summary": "In this research, we combine Transformer-based relation extraction with matching of knowledge graphs (KGs) and apply them to answering multiple-choice questions (MCQs) while maintaining the traceability of the output process. KGs are structured representations of factual knowledge consisting of entities and relations. Due to the high construction cost, they had been regarded as static databases with validated links. However, the recent development of Transformer-based relation extraction (RE) methods has enabled us to generate KGs dynamically by giving them natural language texts, and thereby opened the possibility for representing the meaning of the input sentences with the created KGs. Using this effect, we propose a method that answers MCQs in the \"fill-in-the-blank\" format, taking care of the point that RE methods generate KGs that represent false information if provided with factually incorrect texts. We measure the truthfulness of each question sentence by (i) converting the sentence into a relational graph using an RE method and (ii) verifying it against factually correct KGs under the closed-world assumption. The experimental results demonstrate that our method correctly answers up to around 70% of the questions, while providing traceability of the procedure. We also highlight that the question category has a vast influence on the accuracy.",
        "translated": "在本研究中，我们结合基于 Transformer 的关系抽取方法与知识图谱（KGs）的匹配，并将其应用于多选题（MCQs）解答，同时保持输出过程的可追溯性。知识图谱是对事实性知识的结构化表示，由实体和关系构成。由于构建成本高昂，此前其被视为具有验证链接的静态数据库。然而，近期基于 Transformer 的关系抽取（RE）方法的发展使我们能够通过输入自然语言文本动态生成知识图谱，从而为用所生成的知识图谱表征输入句子的语义提供了可能。利用这一特性，我们提出了一种以“填空”格式解答多选题的方法，并特别考虑了关系抽取方法若面对事实性错误的文本时，会生成代表虚假信息的知识图谱的问题。我们通过以下两步衡量每个问题句子的真实性：(i) 使用关系抽取方法将句子转换为关系图；(ii) 在闭世界假设下，将该图与事实正确的知识图谱进行比对验证。实验结果表明，我们的方法可正确解答约 70% 的题目，同时提供过程的可追溯性。此外，我们还指出题目类别对准确率具有显著影响。",
        "translated_title": "应用关系抽取与图匹配回答多选题",
        "label": [],
        "label_reason": "研究聚焦NLP与知识图谱，非推荐系统相关",
        "relevance_score": 3,
        "novelty_score": 6,
        "novelty_reason": "方法改进RE+KG匹配，但无推荐场景应用"
    },
    {
        "title": "PRISM: Prompt-Refined In-Context System Modelling for Financial Retrieval",
        "url": "http://arxiv.org/abs/2511.14130v1",
        "pub_date": "2025-11-18",
        "summary": "With the rapid progress of large language models (LLMs), financial information retrieval has become a critical industrial application. Extracting task-relevant information from lengthy financial filings is essential for both operational and analytical decision-making. The FinAgentBench dataset formalizes this problem through two tasks: document ranking and chunk ranking. We present PRISM, a training-free framework that integrates refined system prompting, in-context learning (ICL), and a lightweight multi-agent system. Each component is examined extensively to reveal their synergies: prompt engineering provides precise task instructions, ICL supplies semantically relevant few-shot examples, and the multi-agent system models coordinated scoring behaviour. Our best configuration achieves an NDCG@5 of 0.71818 on the restricted validation split. We further demonstrate that PRISM is feasible and robust for production-scale financial retrieval. Its modular, inference-only design makes it practical for real-world use cases. The source code is released at https://bit.ly/prism-ailens.",
        "translated": "随着大语言模型（LLM）的快速发展，金融信息检索已成为一项关键的工业应用。从冗长的财务文件中提取与任务相关的信息，对运营和分析决策至关重要。FinAgentBench 数据集通过两个任务形式化了该问题：文档排序与片段排序。我们提出了 PRISM，一个无需训练的框架，融合了精细化系统提示、上下文学习（ICL）以及轻量级多智能体系统。每个组件均经过深入研究以揭示其协同效应：提示工程提供精确的任务指令，上下文学习提供语义相关的少样本示例，而多智能体系统建模协调评分行为。在受限验证集上，我们的最佳配置实现了 0.71818 的 NDCG@5。我们进一步证明 PRISM 在生产规模金融检索场景中具备可行性与鲁棒性。其模块化、仅用于推理的设计使其适用于真实世界的应用场景。源代码已发布于 https://bit.ly/prism-ailens。",
        "translated_title": "PRISM：面向金融检索的提示精炼上下文系统建模",
        "label": [
            "通用推荐技术"
        ],
        "label_reason": "聚焦金融信息检索，与推荐系统无直接关联",
        "relevance_score": 4,
        "novelty_score": 6,
        "novelty_reason": "模块化设计实用，但非推荐系统专用创新"
    },
    {
        "title": "NeuroPath: Neurobiology-Inspired Path Tracking and Reflection for Semantically Coherent Retrieval",
        "url": "http://arxiv.org/abs/2511.14096v1",
        "pub_date": "2025-11-18",
        "summary": "Retrieval-augmented generation (RAG) greatly enhances large language models (LLMs) performance in knowledge-intensive tasks. However, naive RAG methods struggle with multi-hop question answering due to their limited capacity to capture complex dependencies across documents. Recent studies employ graph-based RAG to capture document connections. However, these approaches often result in a loss of semantic coherence and introduce irrelevant noise during node matching and subgraph construction. To address these limitations, we propose NeuroPath, an LLM-driven semantic path tracking RAG framework inspired by the path navigational planning of place cells in neurobiology. It consists of two steps: Dynamic Path Tracking and Post-retrieval Completion. Dynamic Path Tracking performs goal-directed semantic path tracking and pruning over the constructed knowledge graph (KG), improving noise reduction and semantic coherence. Post-retrieval Completion further reinforces these benefits by conducting second-stage retrieval using intermediate reasoning and the original query to refine the query goal and complete missing information in the reasoning path. NeuroPath surpasses current state-of-the-art baselines on three multi-hop QA datasets, achieving average improvements of 16.3% on recall@2 and 13.5% on recall@5 over advanced graph-based RAG methods. Moreover, compared to existing iter-based RAG methods, NeuroPath achieves higher accuracy and reduces token consumption by 22.8%. Finally, we demonstrate the robustness of NeuroPath across four smaller LLMs (Llama3.1, GLM4, Mistral0.3, and Gemma3), and further validate its scalability across tasks of varying complexity. Code is available at https://github.com/KennyCaty/NeuroPath.",
        "translated": "检索增强生成（RAG）极大地提升了大语言模型（LLM）在知识密集型任务中的表现。然而，朴素的 RAG 方法由于难以捕捉文档间复杂的依赖关系，在多跳问答任务中表现不佳。近期研究采用基于图结构的 RAG 来建模文档间的关联关系。但这些方法往往导致语义连贯性下降，并在节点匹配与子图构建过程中引入无关噪声。为解决上述局限，我们提出 NeuroPath，这是一种受神经生物学中位置细胞路径导航规划启发的、由 LLM 驱动的语义路径追踪 RAG 框架。该框架包含两个阶段：动态路径追踪与后检索完成。动态路径追踪在构建的知识图谱（KG）上执行目标导向的语义路径追踪与剪枝，提升噪声抑制能力并增强语义连贯性。后检索完成阶段进一步通过中间推理结果与原始查询进行第二阶段检索，以精炼查询目标并补全推理路径中的缺失信息。NeuroPath 在三个多跳问答数据集上超越现有最先进基线方法，相较先进的图结构 RAG 方法，其 recall@2 和 recall@5 平均分别提升 16.3% 和 13.5%。此外，与现有的迭代式 RAG 方法相比，NeuroPath 在准确率上更高，同时减少 22.8% 的 token 消耗。最后，我们在四个较小规模的 LLM（Llama3.1、GLM4、Mistral0.3、Gemma3）上验证了 NeuroPath 的鲁棒性，并进一步证明其在不同复杂度任务上的可扩展性。代码开源于 https://github.com/KennyCaty/NeuroPath。",
        "translated_title": "NeuroPath：受神经生物学启发的路径追踪与反思机制，用于语义连贯的检索",
        "label": [
            "通用推荐技术",
            "LLM生成式推荐"
        ],
        "label_reason": "论文聚焦RAG框架优化，与推荐系统间接相关。",
        "relevance_score": 4,
        "novelty_score": 7,
        "novelty_reason": "引入神经科学启发路径跟踪机制，提升多跳问答效果。"
    },
    {
        "title": "CORGI: Efficient Pattern Matching With Quadratic Guarantees",
        "url": "http://arxiv.org/abs/2511.13942v1",
        "pub_date": "2025-11-17",
        "summary": "Rule-based systems must solve complex matching problems within tight time constraints to be effective in real-time applications, such as planning and reactive control for AI agents, as well as low-latency relational database querying. Pattern-matching systems can encounter issues where exponential time and space are required to find matches for rules with many underconstrained variables, or which produce combinatorial intermediate partial matches (but are otherwise well-constrained). When online AI systems automatically generate rules from example-driven induction or code synthesis, they can easily produce worst-case matching patterns that slow or halt program execution by exceeding available memory. In our own work with cognitive systems that learn from example, we've found that aggressive forms of anti-unification-based generalization can easily produce these circumstances. To make these systems practical without hand-engineering constraints or succumbing to unpredictable failure modes, we introduce a new matching algorithm called CORGI (Collection-Oriented Relational Graph Iteration). Unlike RETE-based approaches, CORGI offers quadratic time and space guarantees for finding single satisficing matches, and the ability to iteratively stream subsequent matches without committing entire conflict sets to memory. CORGI differs from RETE in that it does not have a traditional $β$-memory for collecting partial matches. Instead, CORGI takes a two-step approach: a graph of grounded relations is built/maintained in a forward pass, and an iterator generates matches as needed by working backward through the graph. This approach eliminates the high-latency delays and memory overflows that can result from populating full conflict sets. In a performance evaluation, we demonstrate that CORGI significantly outperforms RETE implementations from SOAR and OPS5 on a simple combinatorial matching task.",
        "translated": "基于规则的系统必须在严格的时延约束下解决复杂的匹配问题，以在实时应用场景（如AI代理的规划与反应式控制、低延迟关系数据库查询）中发挥实效。模式匹配系统可能面临如下问题：当规则包含大量未约束变量时，其搜索过程需要指数级的时间和空间；或当规则虽整体约束良好但生成组合性的中间部分匹配时，也会导致性能瓶颈。当在线AI系统通过示例驱动的归纳或代码合成自动产生规则时，极易生成最坏情况下的匹配模式，从而因超出可用内存而导致程序执行缓慢甚至停滞。在我们自身关于从示例中学习的认知系统的研究中，发现基于反统一化的激进泛化方法极易引发上述情形。为使这类系统无需人工设计约束、亦不陷入不可预测的故障模式而具备实用性，我们提出一种新的匹配算法——CORGI（面向集合的关系图迭代）。与基于RETE的方法不同，CORGI对寻找单个可行匹配提供二次时间与空间保障，并能迭代式地流式输出后续匹配结果，而不必将完整的冲突集存储于内存中。CORGI不同于RETE之处在于，它不具备传统用于收集部分匹配的β记忆结构。相反，CORGI采用两步策略：首先，在前向传递阶段构建并维护一个接地关系图；随后，通过逆向遍历该图，由迭代器按需生成匹配结果。这种机制有效消除了因填充完整冲突集而导致的高延迟与内存溢出问题。在性能评估中，我们证明CORGI在一项简单的组合匹配任务上显著优于SOAR与OPS5等系统所实现的RETE方案。",
        "translated_title": "CORGI：具有二次保证的高效模式匹配",
        "label": [],
        "label_reason": "论文聚焦规则匹配算法，与推荐系统无关",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出新图迭代算法，但非为推荐设计"
    },
    {
        "title": "TaoSearchEmb: A Multi-Objective Reinforcement Learning Framework for Dense Retrieval in Taobao Search",
        "url": "http://arxiv.org/abs/2511.13885v1",
        "pub_date": "2025-11-17",
        "summary": "Dense retrieval, as the core component of e-commerce search engines, maps user queries and items into a unified semantic space through pre-trained embedding models to enable large-scale real-time semantic retrieval. Despite the rapid advancement of LLMs gradually replacing traditional BERT architectures for embedding, their training paradigms still adhere to BERT-like supervised fine-tuning and hard negative mining strategies. This approach relies on complex offline hard negative sample construction pipelines, which constrain model iteration efficiency and hinder the evolutionary potential of semantic representation capabilities. Besides, existing multi-task learning frameworks face the seesaw effect when simultaneously optimizing semantic relevance and non-relevance objectives. In this paper, we propose Retrieval-GRPO, a multi-objective reinforcement learning-based dense retrieval framework designed to address these challenges. The method eliminates offline hard negative sample construction by dynamically retrieving Top-K candidate products for each query during training, while introducing a relevance LLM as a reward model to generate real-time feedback. Specifically, the retrieval model dynamically optimizes embedding representations through reinforcement learning, with reward signals combining LLM-generated relevance scores, product quality scores, and multi-way exclusivity metrics to achieve multi-objective user preference alignment and real-time error correction. This mechanism not only removes dependency on hard negatives but also mitigates the seesaw effect through collaborative multi-objective optimization, significantly enhancing the model's semantic generalization capability for complex long-tail queries. Extensive offline and online experiments validate the effectiveness of Retrieval-GRPO, which has been deployed on China's largest e-commerce platform.",
        "translated": "密集检索作为电子商务搜索引擎的核心组件，通过预训练嵌入模型将用户查询与商品映射到统一的语义空间，以实现大规模实时语义检索。尽管大语言模型（LLM）正逐步取代传统的BERT架构用于嵌入生成，其训练范式仍沿用类似于BERT的监督微调及硬负例挖掘策略。该方法依赖复杂的离线硬负例样本构建流水线，制约了模型迭代效率，并限制了语义表征能力的演化潜力。此外，现有多任务学习框架在同时优化语义相关性与非相关性目标时面临“跷跷板效应”。本文提出Retrieval-GRPO，一种基于多目标强化学习的密集检索框架，旨在解决上述挑战。该方法通过在训练过程中为每个查询动态检索Top-K候选商品，消除对离线硬负例构造的依赖；并引入一个相关性LLM作为奖励模型，生成实时反馈。具体而言，检索模型通过强化学习动态优化嵌入表示，其奖励信号结合LLM生成的相关性评分、商品质量评分及多路互斥性度量指标，实现多目标用户偏好对齐与实时误差修正。该机制不仅消除了对硬负例的依赖，还通过协同多目标优化缓解“跷跷板效应”，显著提升了模型对复杂长尾查询的语义泛化能力。大量离线与在线实验验证了Retrieval-GRPO的有效性，该模型已部署于中国最大电商平台。",
        "translated_title": "TaoSearchEmb：面向淘宝搜索密集召回的多目标强化学习框架",
        "label": [
            "召回",
            "LLM生成式推荐"
        ],
        "label_reason": "基于RL的密集召回框架，结合LLM反馈优化嵌入",
        "relevance_score": 9,
        "novelty_score": 8,
        "novelty_reason": "创新性引入LLM奖励模型与多目标强化学习"
    },
    {
        "title": "ARC Is a Vision Problem!",
        "url": "http://arxiv.org/abs/2511.14761v1",
        "pub_date": "2025-11-18",
        "summary": "The Abstraction and Reasoning Corpus (ARC) is designed to promote research on abstract reasoning, a fundamental aspect of human intelligence. Common approaches to ARC treat it as a language-oriented problem, addressed by large language models (LLMs) or recurrent reasoning models. However, although the puzzle-like tasks in ARC are inherently visual, existing research has rarely approached the problem from a vision-centric perspective. In this work, we formulate ARC within a vision paradigm, framing it as an image-to-image translation problem. To incorporate visual priors, we represent the inputs on a \"canvas\" that can be processed like natural images. It is then natural for us to apply standard vision architectures, such as a vanilla Vision Transformer (ViT), to perform image-to-image mapping. Our model is trained from scratch solely on ARC data and generalizes to unseen tasks through test-time training. Our framework, termed Vision ARC (VARC), achieves 60.4% accuracy on the ARC-1 benchmark, substantially outperforming existing methods that are also trained from scratch. Our results are competitive with those of leading LLMs and close the gap to average human performance.",
        "translated": "抽象与推理语料库（ARC）旨在促进对抽象推理的研究，这是人类智能的基本组成部分。当前主流方法将 ARC 视为语言导向的问题，通常由大语言模型（LLMs）或循环推理模型解决。然而，尽管 ARC 中的谜题类任务本质上具有视觉特性，现有研究极少从视觉中心视角切入处理该问题。在本工作中，我们将 ARC 问题形式化为视觉范式下的图像到图像翻译任务。为引入视觉先验，我们以“画布”形式表示输入，使其可被视作自然图像进行处理。由此，我们自然可以采用标准视觉架构（如基础版视觉 Transformer，ViT）执行图像到图像映射。我们的模型仅基于 ARC 数据从头训练，并通过测试时训练泛化至未见任务。所提出的框架称为 Vision ARC（VARC），在 ARC-1 基准测试中达到 60.4% 的准确率，显著超越同样从头训练的现有方法。我们的结果可与顶尖 LLMs 相媲美，并大幅缩小与平均人类表现之间的差距。",
        "translated_title": "ARC 是一个视觉问题！",
        "label": [],
        "label_reason": "ARC为视觉推理任务，非图像像素级恢复或增强",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出视觉架构解决抽象推理，具方法创新性"
    },
    {
        "title": "UniGen-1.5: Enhancing Image Generation and Editing through Reward Unification in Reinforcement Learning",
        "url": "http://arxiv.org/abs/2511.14760v1",
        "pub_date": "2025-11-18",
        "summary": "We present UniGen-1.5, a unified multimodal large language model (MLLM) for advanced image understanding, generation and editing. Building upon UniGen, we comprehensively enhance the model architecture and training pipeline to strengthen the image understanding and generation capabilities while unlocking strong image editing ability. Especially, we propose a unified Reinforcement Learning (RL) strategy that improves both image generation and image editing jointly via shared reward models. To further enhance image editing performance, we propose a light Edit Instruction Alignment stage that significantly improves the editing instruction comprehension that is essential for the success of the RL training. Experimental results show that UniGen-1.5 demonstrates competitive understanding and generation performance. Specifically, UniGen-1.5 achieves 0.89 and 4.31 overall scores on GenEval and ImgEdit that surpass the state-of-the-art models such as BAGEL and reaching performance comparable to proprietary models such as GPT-Image-1.",
        "translated": "我们提出 UniGen-1.5，这是一个面向高级图像理解、生成与编辑的统一多模态大语言模型（MLLM）。在 UniGen 的基础上，我们全面增强了模型架构与训练流程，以强化图像理解与生成能力，并解锁强大的图像编辑能力。尤其地，我们提出了一种统一的强化学习（RL）策略，通过共享奖励模型，联合提升图像生成与图像编辑性能。为进一步增强图像编辑效果，我们设计了一个轻量级的 Edit Instruction Alignment 阶段，显著提升了对编辑指令的理解能力，这对其 RL 训练的成功至关重要。实验结果表明，UniGen-1.5 在图像理解与生成方面表现出竞争力。具体而言，UniGen-1.5 在 GenEval 和 ImgEdit 两项评测中分别取得了 0.89 和 4.31 的综合得分，超越了当前最先进的模型（如 BAGEL），其性能可媲美专有模型（如 GPT-Image-1）。",
        "translated_title": "UniGen-1.5：通过强化学习中的奖励统一提升图像生成与编辑能力",
        "label": [],
        "label_reason": "生成与编辑属高阶视觉任务",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "统一RL策略提升生成编辑能力"
    },
    {
        "title": "Co-Me: Confidence-Guided Token Merging for Visual Geometric Transformers",
        "url": "http://arxiv.org/abs/2511.14751v1",
        "pub_date": "2025-11-18",
        "summary": "We propose Confidence-Guided Token Merging (Co-Me), an acceleration mechanism for visual geometric transformers without retraining or finetuning the base model. Co-Me distilled a light-weight confidence predictor to rank tokens by uncertainty and selectively merge low-confidence ones, effectively reducing computation while maintaining spatial coverage. Compared to similarity-based merging or pruning, the confidence signal in Co-Me reliably indicates regions emphasized by the transformer, enabling substantial acceleration without degrading performance. Co-Me applies seamlessly to various multi-view and streaming visual geometric transformers, achieving speedups that scale with sequence length. When applied to VGGT and MapAnything, Co-Me achieves up to $11.3\\times$ and $7.2\\times$ speedup, making visual geometric transformers practical for real-time 3D perception and reconstruction.",
        "translated": "我们提出了一种名为 Confidence-Guided Token Merging (Co-Me) 的加速机制，该机制可在不重新训练或微调基础模型的前提下，加速视觉几何变换器的运算。Co-Me 通过蒸馏一个轻量级置信度预测器，根据不确定性对 token 进行排序，并选择性地合并低置信度 token，从而在保持空间覆盖范围的同时有效降低计算开销。相较于基于相似度的合并或剪枝方法，Co-Me 中的置信度信号能够可靠指示变换器所关注的区域，从而实现显著的加速效果而不会损害性能。Co-Me 可无缝应用于各类多视角和流式视觉几何变换器，其加速效果随序列长度呈比例提升。当应用于 VGGT 和 MapAnything 时，Co-Me 分别实现了最高 $11.3\\times$ 和 $7.2\\times$ 的加速，使视觉几何变换器得以实际用于实时三维感知与重建。",
        "translated_title": "Co-Me：基于置信度引导的视觉几何变换器中的令牌合并",
        "label": [],
        "label_reason": "加速几何变换器，非图像像素级恢复任务",
        "relevance_score": 3,
        "novelty_score": 7,
        "novelty_reason": "提出轻量置信度预测器优化计算效率"
    },
    {
        "title": "Vision Large Language Models Are Good Noise Handlers in Engagement Analysis",
        "url": "http://arxiv.org/abs/2511.14749v1",
        "pub_date": "2025-11-18",
        "summary": "Engagement recognition in video datasets, unlike traditional image classification tasks, is particularly challenged by subjective labels and noise limiting model performance. To overcome the challenges of subjective and noisy engagement labels, we propose a framework leveraging Vision Large Language Models (VLMs) to refine annotations and guide the training process. Our framework uses a questionnaire to extract behavioral cues and split data into high- and low-reliability subsets. We also introduce a training strategy combining curriculum learning with soft label refinement, gradually incorporating ambiguous samples while adjusting supervision to reflect uncertainty. We demonstrate that classical computer vision models trained on refined high-reliability subsets and enhanced with our curriculum strategy show improvements, highlighting benefits of addressing label subjectivity with VLMs. This method surpasses prior state of the art across engagement benchmarks such as EngageNet (three of six feature settings, maximum improvement of +1.21%), and DREAMS / PAFE with F1 gains of +0.22 / +0.06.",
        "translated": "与传统的图像分类任务不同，视频数据集中的参与度识别尤其受到主观标签和噪声的制约，从而限制了模型性能。为克服主观性和噪声标签带来的挑战，我们提出了一种利用视觉大语言模型（VLMs）优化标注并引导训练过程的框架。该框架通过问卷提取行为线索，并将数据划分为高可靠性和低可靠性子集。此外，我们引入了一种结合课程学习与软标签精炼的训练策略，在逐步引入模糊样本的同时动态调整监督信号以反映不确定性。实验表明，基于精炼后的高可靠性子集训练的经典计算机视觉模型，在配合我们的课程学习策略后性能显著提升，验证了利用 VLMs 应对标签主观性的有效性。该方法在多个参与度基准测试中超越现有最先进方法，例如在 EngageNet（六种特征设置中的三种）上实现最高+1.21%的提升，在 DREAMS 和 PAFE 上分别获得 +0.22 和 +0.06 的 F1 增益。",
        "translated_title": "视觉大语言模型在参与度分析中是优秀的噪声处理者",
        "label": [],
        "label_reason": "处理标签噪声属高阶任务，非像素级图像恢复",
        "relevance_score": 2,
        "novelty_score": 5,
        "novelty_reason": "微调VLM策略，非图像处理核心创新"
    },
    {
        "title": "A Neural Field-Based Approach for View Computation &amp; Data Exploration in 3D Urban Environments",
        "url": "http://arxiv.org/abs/2511.14742v1",
        "pub_date": "2025-11-18",
        "summary": "Despite the growing availability of 3D urban datasets, extracting insights remains challenging due to computational bottlenecks and the complexity of interacting with data. In fact, the intricate geometry of 3D urban environments results in high degrees of occlusion and requires extensive manual viewpoint adjustments that make large-scale exploration inefficient. To address this, we propose a view-based approach for 3D data exploration, where a vector field encodes views from the environment. To support this approach, we introduce a neural field-based method that constructs an efficient implicit representation of 3D environments. This representation enables both faster direct queries, which consist of the computation of view assessment indices, and inverse queries, which help avoid occlusion and facilitate the search for views that match desired data patterns. Our approach supports key urban analysis tasks such as visibility assessments, solar exposure evaluation, and assessing the visual impact of new developments. We validate our method through quantitative experiments, case studies informed by real-world urban challenges, and feedback from domain experts. Results show its effectiveness in finding desirable viewpoints, analyzing building facade visibility, and evaluating views from outdoor spaces. Code and data are publicly available at https://urbantk.org/neural-3d.",
        "translated": "尽管3D城市数据集日益丰富，但由于计算瓶颈及与数据交互的复杂性，从中提取洞察仍具挑战。事实上，3D城市环境复杂的几何结构导致高度遮挡，并需大量手动视角调整，从而使得大规模探索效率低下。为解决此问题，我们提出一种基于视角的3D数据探索方法，其中向量场编码环境中各视角信息。为支持该方法，我们引入一种基于神经场的方法，构建3D环境的高效隐式表示。该表示支持两类查询：一类是直接查询（即计算视角评估指标），另一类是反向查询（有助于规避遮挡并辅助寻找匹配所需数据模式的视角）。我们的方法支持关键的城市分析任务，如可视性评估、日照暴露评估以及新建建筑视觉影响分析。我们通过定量实验、结合现实城市挑战的案例研究以及领域专家反馈对方法进行验证。结果表明，该方法在寻找理想视角、分析建筑立面可视性及评估户外空间视野方面均表现出良好效果。代码与数据已公开发布于 https://urbantk.org/neural-3d。",
        "translated_title": "基于神经场的3D城市环境中视角计算与数据探索方法",
        "label": [],
        "label_reason": "任务属3D场景分析，非图像像素级恢复",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出神经场方法提升3D视角查询效率"
    },
    {
        "title": "Zero-shot Synthetic Video Realism Enhancement via Structure-aware Denoising",
        "url": "http://arxiv.org/abs/2511.14719v1",
        "pub_date": "2025-11-18",
        "summary": "We propose an approach to enhancing synthetic video realism, which can re-render synthetic videos from a simulator in photorealistic fashion. Our realism enhancement approach is a zero-shot framework that focuses on preserving the multi-level structures from synthetic videos into the enhanced one in both spatial and temporal domains, built upon a diffusion video foundational model without further fine-tuning. Specifically, we incorporate an effective modification to have the generation/denoising process conditioned on estimated structure-aware information from the synthetic video, such as depth maps, semantic maps, and edge maps, by an auxiliary model, rather than extracting the information from a simulator. This guidance ensures that the enhanced videos are consistent with the original synthetic video at both the structural and semantic levels. Our approach is a simple yet general and powerful approach to enhancing synthetic video realism: we show that our approach outperforms existing baselines in structural consistency with the original video while maintaining state-of-the-art photorealism quality in our experiments.",
        "translated": "我们提出了一种增强合成视频真实感的方法，能够以逼真的方式将模拟器生成的合成视频重新渲染。我们的现实感增强方法是一个零样本框架，专注于在空域和时域中将合成视频中的多层次结构保留到增强后的视频中，且无需对基于扩散视频的基础模型进行额外微调。具体而言，我们引入了一种有效改进：通过辅助模型从合成视频中估计结构感知信息（如深度图、语义图和边缘图），并将其作为生成/去噪过程的条件输入，而非直接从模拟器中提取这些信息。该引导机制确保增强后的视频在结构与语义层面均与原始合成视频保持一致。我们的方法是一种简单却通用且强大的合成视频真实感增强方案：实验表明，我们的方法在保持与原始视频结构一致性方面优于现有基线方法，同时在逼真度质量上达到当前最先进的水平。",
        "translated_title": "基于结构感知去噪的零样本合成视频真实感增强",
        "label": [
            "多帧/视频图像恢复"
        ],
        "label_reason": "提升合成视频真实感，涉及结构感知去噪，属视频恢复范畴。",
        "relevance_score": 4,
        "novelty_score": 7,
        "novelty_reason": "引入结构信息引导扩散模型，有效提升一致性与真实感。"
    },
    {
        "title": "Diffusion As Self-Distillation: End-to-End Latent Diffusion In One Model",
        "url": "http://arxiv.org/abs/2511.14716v1",
        "pub_date": "2025-11-18",
        "summary": "Standard Latent Diffusion Models rely on a complex, three-part architecture consisting of a separate encoder, decoder, and diffusion network, which are trained in multiple stages. This modular design is computationally inefficient, leads to suboptimal performance, and prevents the unification of diffusion with the single-network architectures common in vision foundation models. Our goal is to unify these three components into a single, end-to-end trainable network. We first demonstrate that a naive joint training approach fails catastrophically due to ``latent collapse'', where the diffusion training objective interferes with the network's ability to learn a good latent representation. We identify the root causes of this instability by drawing a novel analogy between diffusion and self-distillation based unsupervised learning method. Based on this insight, we propose Diffusion as Self-Distillation (DSD), a new framework with key modifications to the training objective that stabilize the latent space. This approach enables, for the first time, the stable end-to-end training of a single network that simultaneously learns to encode, decode, and perform diffusion. DSD achieves outstanding performance on the ImageNet $256\\times 256$ conditional generation task: FID=13.44/6.38/4.25 with only 42M/118M/205M parameters and 50 training epochs on ImageNet, without using classifier-free-guidance.",
        "translated": "标准潜在扩散模型依赖于一种复杂的三部分架构，包含独立的编码器、解码器和扩散网络，并在多个阶段进行训练。这种模块化设计计算效率低下，导致性能次优，且阻碍了扩散模型与视觉基础模型中常见的单网络架构的统一。我们的目标是将这三个组件整合为一个端到端可训练的单一网络。我们首先证明，直接联合训练的方法因“潜在空间坍塌”而彻底失败——扩散训练目标干扰了网络学习良好潜在表示的能力。我们通过建立扩散与基于自蒸馏的无监督学习方法之间新颖的类比，识别出该不稳定性产生的根本原因。基于这一洞察，我们提出Diffusion as Self-Distillation（DSD），这是一种全新的框架，通过对训练目标的关键修改来稳定潜在空间。该方法首次实现了对单一网络的稳定端到端训练，该网络同时具备编码、解码和执行扩散的能力。DSD在ImageNet $256\\times 256$ 条件生成任务上取得了卓越性能：仅使用42M/118M/205M参数和50个训练周期即可达到FID=13.44/6.38/4.25，且无需使用分类器自由引导机制。",
        "translated_title": "扩散即自蒸馏：一个模型中的端到端潜在扩散",
        "label": [],
        "label_reason": "生成新图像，非像素级恢复任务",
        "relevance_score": 3,
        "novelty_score": 9,
        "novelty_reason": "首次实现单网络端到端扩散训练框架"
    },
    {
        "title": "FreeSwim: Revisiting Sliding-Window Attention Mechanisms for Training-Free Ultra-High-Resolution Video Generation",
        "url": "http://arxiv.org/abs/2511.14712v1",
        "pub_date": "2025-11-18",
        "summary": "The quadratic time and memory complexity of the attention mechanism in modern Transformer based video generators makes end-to-end training for ultra high resolution videos prohibitively expensive. Motivated by this limitation, we introduce a training-free approach that leverages video Diffusion Transformers pretrained at their native scale to synthesize higher resolution videos without any additional training or adaptation. At the core of our method lies an inward sliding window attention mechanism, which originates from a key observation: maintaining each query token's training scale receptive field is crucial for preserving visual fidelity and detail. However, naive local window attention, unfortunately, often leads to repetitive content and exhibits a lack of global coherence in the generated results. To overcome this challenge, we devise a dual-path pipeline that backs up window attention with a novel cross-attention override strategy, enabling the semantic content produced by local attention to be guided by another branch with a full receptive field and, therefore, ensuring holistic consistency. Furthermore, to improve efficiency, we incorporate a cross-attention caching strategy for this branch to avoid the frequent computation of full 3D attention. Extensive experiments demonstrate that our method delivers ultra-high-resolution videos with fine-grained visual details and high efficiency in a training-free paradigm. Meanwhile, it achieves superior performance on VBench, even compared to training-based alternatives, with competitive or improved efficiency. Codes are available at: https://github.com/WillWu111/FreeSwim",
        "translated": "现代基于Transformer的视频生成器中注意力机制的二次时间与内存复杂度，使得对超高分辨率视频进行端到端训练的成本过高。受此限制启发，我们提出一种无需训练的方法，该方法利用在原生尺度上预训练好的视频扩散Transformer，无需任何额外训练或适配即可合成更高分辨率的视频。本方法的核心是一种向内滑动窗口注意力机制，其源于一个关键观察：保持每个查询token在训练尺度下的感受野对于维持视觉保真度和细节至关重要。然而，朴素的局部窗口注意力机制往往导致内容重复，并且生成结果缺乏全局一致性。为解决这一问题，我们设计了一种双路径管道架构，通过一种新颖的交叉注意力覆盖策略辅助窗口注意力，使局部注意力生成的语义内容能够受到另一支具有完整感受野的分支引导，从而确保整体一致性。此外，为提升效率，我们为该分支引入了交叉注意力缓存策略，以避免频繁计算完整的3D注意力。大量实验表明，在无需训练的范式下，我们的方法能高效生成具有精细视觉细节的超高分辨率视频；同时，在VBench数据集上其性能优于基于训练的替代方案，且具备竞争力甚至更优的效率。代码开源地址为：https://github.com/WillWu111/FreeSwim",
        "translated_title": "FreeSwim：重新审视无训练超高清视频生成中的滑动窗口注意力机制",
        "label": [],
        "label_reason": "生成新视频属高阶任务，非像素级图像恢复",
        "relevance_score": 3,
        "novelty_score": 8,
        "novelty_reason": "改进注意力机制提升效率，但非图像恢复任务"
    },
    {
        "title": "Seeing Beyond the Image: ECG and Anatomical Knowledge-Guided Myocardial Scar Segmentation from Late Gadolinium-Enhanced Images",
        "url": "http://arxiv.org/abs/2511.14702v1",
        "pub_date": "2025-11-18",
        "summary": "Accurate segmentation of myocardial scar from late gadolinium enhanced (LGE) cardiac MRI is essential for evaluating tissue viability, yet remains challenging due to variable contrast and imaging artifacts. Electrocardiogram (ECG) signals provide complementary physiological information, as conduction abnormalities can help localize or suggest scarred myocardial regions. In this work, we propose a novel multimodal framework that integrates ECG-derived electrophysiological information with anatomical priors from the AHA-17 atlas for physiologically consistent LGE-based scar segmentation. As ECGs and LGE-MRIs are not acquired simultaneously, we introduce a Temporal Aware Feature Fusion (TAFF) mechanism that dynamically weights and fuses features based on their acquisition time difference. Our method was evaluated on a clinical dataset and achieved substantial gains over the state-of-the-art image-only baseline (nnU-Net), increasing the average Dice score for scars from 0.6149 to 0.8463 and achieving high performance in both precision (0.9115) and sensitivity (0.9043). These results show that integrating physiological and anatomical knowledge allows the model to \"see beyond the image\", setting a new direction for robust and physiologically grounded cardiac scar segmentation.",
        "translated": "从晚期钆增强（LGE）心脏磁共振成像中精确分割心肌瘢痕对评估组织活力至关重要，但由于对比度变化和成像伪影的存在，该任务仍具挑战性。心电图（ECG）信号提供互补的生理信息，传导异常可辅助定位或提示瘢痕化心肌区域。在本研究中，我们提出了一种新颖的多模态框架，将由ECG导出的电生理信息与AHA-17解剖先验相结合，实现生理学一致的基于LGE的瘢痕分割。由于ECG与LGE-MRI并非同步采集，我们引入了时序感知特征融合（Temporal Aware Feature Fusion, TAFF）机制，依据二者采集时间差动态加权并融合特征。本方法在临床数据集上进行了评估，在优于当前最先进的图像单模基线模型（nnU-Net）的基础上，瘢痕平均Dice分数从0.6149提升至0.8463，并在精确率（0.9115）与敏感度（0.9043）方面均取得优异表现。这些结果表明，整合生理与解剖知识使模型能够“超越图像本身”，为稳健且具有生理基础的心肌瘢痕分割开辟了新方向。",
        "translated_title": "超越图像视野：基于心电图与解剖学知识引导的晚期钆增强图像心肌瘢痕分割",
        "label": [],
        "label_reason": "使用ECG辅助心肌瘢痕分割，属医学图像理解任务",
        "relevance_score": 3,
        "novelty_score": 7,
        "novelty_reason": "引入时间感知融合机制提升多模态整合效果"
    },
    {
        "title": "HyMAD: A Hybrid Multi-Activity Detection Approach for Border Surveillance and Monitoring",
        "url": "http://arxiv.org/abs/2511.14698v1",
        "pub_date": "2025-11-18",
        "summary": "Seismic sensing has emerged as a promising solution for border surveillance and monitoring; the seismic sensors that are often buried underground are small and cannot be noticed easily, making them difficult for intruders to detect, avoid, or vandalize. This significantly enhances their effectiveness compared to highly visible cameras or fences. However, accurately detecting and distinguishing between overlapping activities that are happening simultaneously, such as human intrusions, animal movements, and vehicle rumbling, remains a major challenge due to the complex and noisy nature of seismic signals. Correctly identifying simultaneous activities is critical because failing to separate them can lead to misclassification, missed detections, and an incomplete understanding of the situation, thereby reducing the reliability of surveillance systems. To tackle this problem, we propose HyMAD (Hybrid Multi-Activity Detection), a deep neural architecture based on spatio-temporal feature fusion. The framework integrates spectral features extracted with SincNet and temporal dependencies modeled by a recurrent neural network (RNN). In addition, HyMAD employs self-attention layers to strengthen intra-modal representations and a cross-modal fusion module to achieve robust multi-label classification of seismic events. e evaluate our approach on a dataset constructed from real-world field recordings collected in the context of border surveillance and monitoring, demonstrating its ability to generalize to complex, simultaneous activity scenarios involving humans, animals, and vehicles. Our method achieves competitive performance and offers a modular framework for extending seismic-based activity recognition in real-world security applications.",
        "translated": "地震传感已成为边境监控与监视的一项有前景的解决方案；通常埋设于地下的地震传感器体积小巧，不易被察觉，使得入侵者难以发现、规避或破坏。这使其相较于高可见度的摄像机或围栏等设备，在实际应用中表现出显著更高的有效性。然而，由于地震信号本身具有复杂性和噪声特性，准确检测并区分同时发生的重叠活动（如人类入侵、动物移动和车辆行驶）仍是一个重大挑战。正确识别同时发生的多种活动至关重要，因为若未能有效分离这些活动，将导致误分类、漏检以及对现场情况理解不完整，从而降低监控系统的可靠性。为解决这一问题，我们提出HyMAD（混合多活动检测），这是一种基于时空特征融合的深度神经网络架构。该框架整合了通过SincNet提取的频谱特征与由循环神经网络（RNN）建模的时间依赖关系。此外，HyMAD采用自注意力层以增强单模态内部表征，并引入跨模态融合模块，实现对地震事件的鲁棒性多标签分类。我们在一个基于真实野外记录构建的数据集上评估本方法，该数据集源自边境监控场景，结果证明其能够良好泛化至涉及人类、动物及车辆的复杂并发活动情景。我们的方法在性能上表现优异，同时提供了一个模块化框架，便于拓展应用于现实世界安防场景中的基于地震的活动识别系统。",
        "translated_title": "HyMAD：一种用于边境监控与监测的混合多活动检测方法",
        "label": [],
        "label_reason": "处理地震信号分类，非图像像素级恢复",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "模块化架构改进，但非图像处理创新"
    },
    {
        "title": "Attention via Synaptic Plasticity is All You Need: A Biologically Inspired Spiking Neuromorphic Transformer",
        "url": "http://arxiv.org/abs/2511.14691v1",
        "pub_date": "2025-11-18",
        "summary": "Attention is the brain's ability to selectively focus on a few specific aspects while ignoring irrelevant ones. This biological principle inspired the attention mechanism in modern Transformers. Transformers now underpin large language models (LLMs) such as GPT, but at the cost of massive training and inference energy, leading to a large carbon footprint. While brain attention emerges from neural circuits, Transformer attention relies on dot-product similarity to weight elements in the input sequence. Neuromorphic computing, especially spiking neural networks (SNNs), offers a brain-inspired path to energy-efficient intelligence. Despite recent work on attention-based spiking Transformers, the core attention layer remains non-neuromorphic. Current spiking attention (i) relies on dot-product or element-wise similarity suited to floating-point operations, not event-driven spikes; (ii) keeps attention matrices that suffer from the von Neumann bottleneck, limiting in-memory computing; and (iii) still diverges from brain-like computation. To address these issues, we propose the Spiking STDP Transformer (S$^{2}$TDPT), a neuromorphic Transformer that implements self-attention through spike-timing-dependent plasticity (STDP), embedding query--key correlations in synaptic weights. STDP, a core mechanism of memory and learning in the brain and widely studied in neuromorphic devices, naturally enables in-memory computing and supports non-von Neumann hardware. On CIFAR-10 and CIFAR-100, our model achieves 94.35\\% and 78.08\\% accuracy with only four timesteps and 0.49 mJ on CIFAR-100, an 88.47\\% energy reduction compared to a standard ANN Transformer. Grad-CAM shows that the model attends to semantically relevant regions, enhancing interpretability. Overall, S$^{2}$TDPT illustrates how biologically inspired attention can yield energy-efficient, hardware-friendly, and explainable neuromorphic models.",
        "translated": "注意力是大脑选择性聚焦于若干特定方面而忽略无关信息的能力。这一生物原理启发了现代 Transformer 中的注意力机制。如今，Transformer 构成了大型语言模型（LLM）如 GPT 的核心，但其代价是巨大的训练与推理能耗，导致碳足迹显著升高。虽然脑内注意力源于神经回路，Transformer 注意力则依赖点积相似度对输入序列中的元素进行加权。类脑计算，尤其是脉冲神经网络（SNN），为实现节能智能提供了仿生路径。尽管近期已有基于注意力机制的脉冲 Transformer 研究，但其核心注意力层仍非类脑架构。当前脉冲注意力存在以下三方面不足：(i) 依赖点积或逐元素相似度，适用于浮点运算，而非事件驱动型脉冲；(ii) 保留传统注意力矩阵，受限于冯·诺依曼瓶颈，难以支持存内计算；(iii) 仍未贴近类脑计算范式。为此，我们提出 Spiking STDP Transformer（S$^{2}$TDPT），一种类脑 Transformer，通过脉冲时间依赖可塑性（STDP）实现自注意力，并将查询—键相关性嵌入突触权重中。STDP 是大脑记忆与学习的核心机制，在类脑器件中亦被广泛研究，天然支持存内计算并适配非冯·诺依曼硬件架构。在 CIFAR-10 和 CIFAR-100 数据集上，该模型仅需四步脉冲时序即达到 94.35% 和 78.08% 的准确率，且在 CIFAR-100 上能耗仅为 0.49 mJ，相较标准 ANN Transformer 节能率达 88.47%。Grad-CAM 显示模型关注语义相关区域，从而提升可解释性。总体而言，S$^{2}$TDPT 展示了如何利用仿生注意力构建节能、硬件友好且可解释的类脑模型。",
        "translated_title": "通过突触可塑性实现注意力机制：一种受生物启发的脉冲神经形态Transformer",
        "label": [],
        "label_reason": "研究神经形态Transformer，非图像处理任务",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "提出STDP实现注意力机制，具硬件友好创新"
    },
    {
        "title": "Impact of Image Resolution on Age Estimation with DeepFace and InsightFace",
        "url": "http://arxiv.org/abs/2511.14689v1",
        "pub_date": "2025-11-18",
        "summary": "Automatic age estimation is widely used for age verification, where input images often vary considerably in resolution. This study evaluates the effect of image resolution on age estimation accuracy using DeepFace and InsightFace. A total of 1000 images from the IMDB-Clean dataset were processed in seven resolutions, resulting in 7000 test samples. Performance was evaluated using Mean Absolute Error (MAE), Standard Deviation (SD), and Median Absolute Error (MedAE). Based on this study, we conclude that input image resolution has a clear and consistent impact on the accuracy of age estimation in both DeepFace and InsightFace. Both frameworks achieve optimal performance at 224x224 pixels, with an MAE of 10.83 years (DeepFace) and 7.46 years (InsightFace). At low resolutions, MAE increases substantially, while very high resolutions also degrade accuracy. InsightFace is consistently faster than DeepFace across all resolutions.",
        "translated": "自动年龄估计广泛应用于年龄验证任务，其中输入图像的分辨率往往差异显著。本研究采用 DeepFace 和 InsightFace 评估图像分辨率对年龄估计准确率的影响。从 IMDB-Clean 数据集中选取共计 1000 张图像，在七种不同分辨率下进行处理，生成 7000 个测试样本。性能通过平均绝对误差（MAE）、标准差（SD）和中位数绝对误差（MedAE）进行评估。基于本研究结果，我们认为输入图像分辨率对 DeepFace 与 InsightFace 的年龄估计精度均具有清晰且一致的影响。两个框架在 224x224 像素分辨率下均达到最优性能，DeepFace 的 MAE 为 10.83 年，InsightFace 的 MAE 为 7.46 年。在低分辨率下，MAE 显著上升；而在极高分辨率下，准确率亦会下降。在整个分辨率范围内，InsightFace 的运行速度始终优于 DeepFace。",
        "translated_title": "图像分辨率对 DeepFace 与 InsightFace 年龄估计的影响",
        "label": [],
        "label_reason": "研究图像分辨率对年龄估计的影响，属高阶视觉任务。",
        "relevance_score": 1,
        "novelty_score": 1,
        "novelty_reason": "仅评估现有模型性能，无新方法或架构创新。"
    },
    {
        "title": "US-X Complete: A Multi-Modal Approach to Anatomical 3D Shape Recovery",
        "url": "http://arxiv.org/abs/2511.15600v1",
        "pub_date": "2025-11-19",
        "summary": "Ultrasound offers a radiation-free, cost-effective solution for real-time visualization of spinal landmarks, paraspinal soft tissues and neurovascular structures, making it valuable for intraoperative guidance during spinal procedures. However, ultrasound suffers from inherent limitations in visualizing complete vertebral anatomy, in particular vertebral bodies, due to acoustic shadowing effects caused by bone. In this work, we present a novel multi-modal deep learning method for completing occluded anatomical structures in 3D ultrasound by leveraging complementary information from a single X-ray image. To enable training, we generate paired training data consisting of: (1) 2D lateral vertebral views that simulate X-ray scans, and (2) 3D partial vertebrae representations that mimic the limited visibility and occlusions encountered during ultrasound spine imaging. Our method integrates morphological information from both imaging modalities and demonstrates significant improvements in vertebral reconstruction (p &lt; 0.001) compared to state of art in 3D ultrasound vertebral completion. We perform phantom studies as an initial step to future clinical translation, and achieve a more accurate, complete volumetric lumbar spine visualization overlayed on the ultrasound scan without the need for registration with preoperative modalities such as computed tomography. This demonstrates that integrating a single X-ray projection mitigates ultrasound's key limitation while preserving its strengths as the primary imaging modality. Code and data can be found at https://github.com/miruna20/US-X-Complete",
        "translated": "超声成像提供了一种无辐射、成本效益高的实时可视化手段，用于观察脊柱标志物、椎旁软组织及神经血管结构，因此在脊柱手术中具有重要的术中引导价值。然而，由于骨骼引起的声学阴影效应，超声在完整显示椎体解剖结构方面存在固有局限性。本文提出了一种新颖的多模态深度学习方法，通过利用单张X线图像提供的互补信息，实现三维超声图像中被遮挡解剖结构的重建。为支持训练，我们构建了配对训练数据集，包含：(1) 模拟X线扫描效果的二维侧位椎体视图；(2) 模拟超声脊柱成像过程中因视野受限与遮挡而呈现的三维部分椎体表示。该方法融合了两种成像模态的形态学信息，在三维超声椎体重建任务中显著优于当前最先进的方法（p < 0.001）。我们首先在仿真人模型上开展实验，以期为未来临床转化奠定基础，并实现了无需与术前影像（如CT）进行配准即可在超声图像上叠加更准确、完整的腰椎体积可视化。这表明，整合单幅X线投影可有效缓解超声的关键局限性，同时保留其作为主要成像模态的优势。相关代码和数据可访问 https://github.com/miruna20/US-X-Complete",
        "translated_title": "US-X Complete：一种用于解剖结构三维形状恢复的多模态方法",
        "label": [],
        "label_reason": "任务为3D结构重建，属高阶医学图像理解",
        "relevance_score": 3,
        "novelty_score": 7,
        "novelty_reason": "多模态融合方法提升椎体重建效果"
    },
    {
        "title": "Learning from Mistakes: Loss-Aware Memory Enhanced Continual Learning for LiDAR Place Recognition",
        "url": "http://arxiv.org/abs/2511.15597v1",
        "pub_date": "2025-11-19",
        "summary": "LiDAR place recognition plays a crucial role in SLAM, robot navigation, and autonomous driving. However, existing LiDAR place recognition methods often struggle to adapt to new environments without forgetting previously learned knowledge, a challenge widely known as catastrophic forgetting. To address this issue, we propose KDF+, a novel continual learning framework for LiDAR place recognition that extends the KDF paradigm with a loss-aware sampling strategy and a rehearsal enhancement mechanism. The proposed sampling strategy estimates the learning difficulty of each sample via its loss value and selects samples for replay according to their estimated difficulty. Harder samples, which tend to encode more discriminative information, are sampled with higher probability while maintaining distributional coverage across the dataset. In addition, the rehearsal enhancement mechanism encourages memory samples to be further refined during new-task training by slightly reducing their loss relative to previous tasks, thereby reinforcing long-term knowledge retention. Extensive experiments across multiple benchmarks demonstrate that KDF+ consistently outperforms existing continual learning methods and can be seamlessly integrated into state-of-the-art continual learning for LiDAR place recognition frameworks to yield significant and stable performance gains. The code will be available at https://github.com/repo/KDF-plus.",
        "translated": "激光雷达（LiDAR）场景识别在SLAM、机器人导航和自动驾驶中起着至关重要的作用。然而，现有的激光雷达场景识别方法往往难以在适应新环境的同时避免遗忘先前学到的知识，这一挑战被称为灾难性遗忘（catastrophic forgetting）。为解决该问题，我们提出了KDF+，一种面向激光雷达场景识别的新型持续学习框架，其在KDF范式基础上引入了损失感知采样策略与回放增强机制。所提出的采样策略通过样本的损失值估计其学习难度，并依据估计难度选择样本进行回放；较难的样本通常编码更丰富的判别信息，因此被以更高概率采样，同时保持在整个数据集上的分布覆盖。此外，回放增强机制通过略微降低记忆样本相对于先前任务的损失，鼓励其在新任务训练过程中进一步优化，从而强化长期知识保留能力。在多个基准数据集上的广泛实验表明，KDF+持续优于现有持续学习方法，且可无缝集成至当前最先进的激光雷达场景识别持续学习框架中，带来显著且稳定的性能提升。代码将开源于 https://github.com/repo/KDF-plus。",
        "translated_title": "从错误中学习：基于损失感知的记忆增强持续学习方法用于激光雷达定位识别",
        "label": [],
        "label_reason": "处理LiDAR场景识别，属高阶视觉任务",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出损失感知采样与记忆增强机制"
    },
    {
        "title": "MHR: Momentum Human Rig",
        "url": "http://arxiv.org/abs/2511.15586v1",
        "pub_date": "2025-11-19",
        "summary": "We present MHR, a parametric human body model that combines the decoupled skeleton/shape paradigm of ATLAS with a flexible, modern rig and pose corrective system inspired by the Momentum library. Our model enables expressive, anatomically plausible human animation, supporting non-linear pose correctives, and is designed for robust integration in AR/VR and graphics pipelines.",
        "translated": "我们提出了 MHR，一种结合了 ATLAS 解耦骨骼/形状范式与受 Momentum 库启发的灵活现代绑定系统及姿态修正机制的参数化人体模型。该模型支持富有表现力且符合解剖学的人体动画，支持非线性姿态修正，并专为在 AR/VR 和图形管线中稳健集成而设计。",
        "translated_title": "MHR：动量人体骨骼绑定系统",
        "label": [],
        "label_reason": "属于人体动画建模，非图像处理任务",
        "relevance_score": 1,
        "novelty_score": 3,
        "novelty_reason": "复现现有模型架构，无图像恢复创新"
    },
    {
        "title": "CompTrack: Information Bottleneck-Guided Low-Rank Dynamic Token Compression for Point Cloud Tracking",
        "url": "http://arxiv.org/abs/2511.15580v1",
        "pub_date": "2025-11-19",
        "summary": "3D single object tracking (SOT) in LiDAR point clouds is a critical task in computer vision and autonomous driving. Despite great success having been achieved, the inherent sparsity of point clouds introduces a dual-redundancy challenge that limits existing trackers: (1) vast spatial redundancy from background noise impairs accuracy, and (2) informational redundancy within the foreground hinders efficiency. To tackle these issues, we propose CompTrack, a novel end-to-end framework that systematically eliminates both forms of redundancy in point clouds. First, CompTrack incorporates a Spatial Foreground Predictor (SFP) module to filter out irrelevant background noise based on information entropy, addressing spatial redundancy. Subsequently, its core is an Information Bottleneck-guided Dynamic Token Compression (IB-DTC) module that eliminates the informational redundancy within the foreground. Theoretically grounded in low-rank approximation, this module leverages an online SVD analysis to adaptively compress the redundant foreground into a compact and highly informative set of proxy tokens. Extensive experiments on KITTI, nuScenes and Waymo datasets demonstrate that CompTrack achieves top-performing tracking performance with superior efficiency, running at a real-time 90 FPS on a single RTX 3090 GPU.",
        "translated": "激光雷达点云中的三维单目标跟踪（SOT）是计算机视觉与自动驾驶领域的一项关键任务。尽管已取得显著成果，但点云固有的稀疏性引入了双重冗余挑战，限制了现有跟踪器的性能：(1) 背景噪声带来的巨大空间冗余会损害跟踪精度；(2) 前景内部的信息冗余则影响计算效率。为解决这些问题，我们提出了CompTrack，一种新颖的端到端框架，系统性地消除点云中的两种冗余形式。首先，CompTrack引入了空间前景预测器（Spatial Foreground Predictor, SFP）模块，基于信息熵过滤无关背景噪声，以缓解空间冗余问题。随后，其核心是一个由信息瓶颈引导的动态令牌压缩模块（Information Bottleneck-guided Dynamic Token Compression, IB-DTC），用于消除前景内部的信息冗余。该模块在理论上基于低秩近似原理，利用在线奇异值分解（SVD）分析，自适应地将冗余前景压缩为一组紧凑且高度信息丰富的代理令牌。在KITTI、nuScenes和Waymo数据集上的大量实验表明，CompTrack在保持最优跟踪性能的同时具有卓越的效率，在单块RTX 3090 GPU上可实现实时90 FPS的处理速度。",
        "translated_title": "CompTrack：基于信息瓶颈引导的低秩动态令牌压缩点云跟踪方法",
        "label": [],
        "label_reason": "处理3D点云跟踪，非图像像素级恢复任务",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出动态压缩模块提升效率，但属高阶视觉任务"
    },
    {
        "title": "AVATAAR: Agentic Video Answering via Temporal Adaptive Alignment and Reasoning",
        "url": "http://arxiv.org/abs/2511.15578v1",
        "pub_date": "2025-11-19",
        "summary": "With the increasing prevalence of video content, effectively understanding and answering questions about long form videos has become essential for numerous applications. Although large vision language models (LVLMs) have enhanced performance, they often face challenges with nuanced queries that demand both a comprehensive understanding and detailed analysis. To overcome these obstacles, we introduce AVATAAR, a modular and interpretable framework that combines global and local video context, along with a Pre Retrieval Thinking Agent and a Rethink Module. AVATAAR creates a persistent global summary and establishes a feedback loop between the Rethink Module and the Pre Retrieval Thinking Agent, allowing the system to refine its retrieval strategies based on partial answers and replicate human-like iterative reasoning. On the CinePile benchmark, AVATAAR demonstrates significant improvements over a baseline, achieving relative gains of +5.6% in temporal reasoning, +5% in technical queries, +8% in theme-based questions, and +8.2% in narrative comprehension. Our experiments confirm that each module contributes positively to the overall performance, with the feedback loop being crucial for adaptability. These findings highlight AVATAAR's effectiveness in enhancing video understanding capabilities. Ultimately, AVATAAR presents a scalable solution for long-form Video Question Answering (QA), merging accuracy, interpretability, and extensibility.",
        "translated": "随着视频内容的日益普及，有效理解并回答长视频的相关问题已成为众多应用的关键需求。尽管大型视觉语言模型（LVLMs）已显著提升性能，但它们在需要全面理解与细致分析的复杂查询中仍面临挑战。为克服这些障碍，我们提出了 AVATAAR，一个结合全局与局部视频上下文、配备预检索思维代理与重思模块的模块化可解释框架。AVATAAR构建了一个持久化的全局摘要，并在重思模块与预检索思维代理之间建立反馈循环，使系统能够基于部分答案不断优化其检索策略，并模拟人类类似的迭代推理过程。在 CinePile 数据集基准测试中，AVATAAR 相较于基线方法实现了显著提升：时序推理能力提升 +5.6%，技术类查询准确率提高 +5%，主题类问题表现增强 +8%，叙事理解能力提升 +8.2%。实验表明，各模块均对整体性能产生积极贡献，其中反馈循环对于系统的自适应能力尤为关键。这些结果凸显了 AVATAAR 在增强视频理解能力方面的有效性。最终，AVATAAR 提供了一种可扩展的长视频问答（Video QA）解决方案，融合了准确性、可解释性与可扩展性。",
        "translated_title": "AVATAAR：通过时序自适应对齐与推理实现的代理视频问答",
        "label": [],
        "label_reason": "高阶视频问答任务，非像素级图像处理",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "模块化框架创新，但属高层视觉理解"
    },
    {
        "title": "From Low-Rank Features to Encoding Mismatch: Rethinking Feature Distillation in Vision Transformers",
        "url": "http://arxiv.org/abs/2511.15572v1",
        "pub_date": "2025-11-19",
        "summary": "Feature-map knowledge distillation (KD) is highly effective for convolutional networks but often fails for Vision Transformers (ViTs). To understand this failure and guide method design, we conduct a two-view representation analysis of ViTs. First, a layer-wise Singular Value Decomposition (SVD) of full feature matrices shows that final-layer representations are globally low-rank: for CaiT-S24, only $121/61/34/14$ dimensions suffice to capture $99\\%/95\\%/90\\%/80\\%$ of the energy. In principle, this suggests that a compact student plus a simple linear projector should be enough for feature alignment, contradicting the weak empirical performance of standard feature KD. To resolve this paradox, we introduce a token-level Spectral Energy Pattern (SEP) analysis that measures how each token uses channel capacity. SEP reveals that, despite the global low-rank structure, individual tokens distribute energy over most channels, forming a high-bandwidth encoding pattern. This results in an encoding mismatch between wide teachers and narrow students. Motivated by this insight, we propose two minimal, mismatch-driven strategies: (1) post-hoc feature lifting with a lightweight projector retained during inference, or (2) native width alignment that widens only the student's last block to the teacher's width. On ImageNet-1K, these strategies reactivate simple feature-map distillation in ViTs, raising DeiT-Tiny accuracy from $74.86\\%$ to $77.53\\%$ and $78.23\\%$ when distilling from CaiT-S24, while also improving standalone students trained without any teacher. Our analysis thus explains why ViT feature distillation fails and shows how exploiting low-rank structure yields effective, interpretable remedies and concrete design guidance for compact ViTs.",
        "translated": "特征图知识蒸馏（KD）在卷积网络中极为有效，但在视觉变换器（ViTs）中往往失效。为理解这一失效原因并指导方法设计，我们对 ViTs 进行了双视角表征分析。首先，对全特征矩阵进行逐层奇异值分解（SVD）表明，最终层的表征具有全局低秩特性：对于 CaiT-S24，仅需 $121/61/34/14$ 维即可捕捉 $99\\%/95\\%/90\\%/80\\%$ 的能量。从原理上讲，这暗示仅需一个紧凑的学生模型与一个简单的线性投影器即可完成特征对齐，然而标准特征 KD 的实证表现却十分薄弱，与理论预期相悖。为解决这一矛盾，我们引入了令牌级频谱能量模式（SEP）分析，用于衡量每个令牌如何利用通道容量。SEP 揭示，尽管整体结构呈低秩，但每个令牌均将能量分散至大部分通道，形成高带宽编码模式。这导致宽教师与窄学生之间存在编码不匹配。受此启发，我们提出两种极简、以不匹配驱动的设计策略：(1) 在推理过程中保留轻量级投影器的后置特征提升；或 (2) 原生宽度对齐，仅扩大学生模型最后一块至与教师相同宽度。在 ImageNet-1K 上，这两种策略重新激活了 ViTs 中简单特征图蒸馏的有效性：当从 CaiT-S24 蒸馏时，DeiT-Tiny 准确率分别从 $74.86\\%$ 提升至 $77.53\\%$ 和 $78.23\\%$，同时亦提升了无教师指导下训练的独立学生模型性能。我们的分析由此解释了 ViT 特征蒸馏为何失效，并展示了如何借助低秩结构获得高效、可解释的修复方案及紧凑型 ViT 的具体设计指导。",
        "translated_title": "从低秩特征到编码不匹配：重新思考视觉Transformer中的特征蒸馏",
        "label": [],
        "label_reason": "研究视觉Transformer知识蒸馏，非图像像素级恢复任务。",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出编码不匹配新视角并设计有效缓解策略。"
    },
    {
        "title": "Transferable Dual-Domain Feature Importance Attack against AI-Generated Image Detector",
        "url": "http://arxiv.org/abs/2511.15571v1",
        "pub_date": "2025-11-19",
        "summary": "Recent AI-generated image (AIGI) detectors achieve impressive accuracy under clean condition. In view of antiforensics, it is significant to develop advanced adversarial attacks for evaluating the security of such detectors, which remains unexplored sufficiently. This letter proposes a Dual-domain Feature Importance Attack (DuFIA) scheme to invalidate AIGI detectors to some extent. Forensically important features are captured by the spatially interpolated gradient and frequency-aware perturbation. The adversarial transferability is enhanced by jointly modeling spatial and frequency-domain feature importances, which are fused to guide the optimization-based adversarial example generation. Extensive experiments across various AIGI detectors verify the cross-model transferability, transparency and robustness of DuFIA.",
        "translated": "近年来，AI生成图像（AIGI）检测器在干净环境下取得了令人印象深刻的准确率。考虑到反取证需求，开发先进的对抗攻击以评估此类检测器的安全性具有重要意义，而这一方向尚缺乏充分探索。本文提出了一种双域特征重要性攻击（DuFIA）方案，可在一定程度上使AIGI检测器失效。该方法通过空间插值梯度与频域感知扰动捕获具有法证意义的特征。通过联合建模空域与频域的特征重要性，并将二者融合以引导基于优化的对抗样本生成，从而增强对抗迁移能力。在多种AIGI检测器上的广泛实验验证了DuFIA的跨模型迁移性、透明性与鲁棒性。",
        "translated_title": "针对AI生成图像检测器的可迁移双域特征重要性攻击",
        "label": [],
        "label_reason": "攻击AI图像检测器，属高阶安全评估任务。",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出双域特征重要性攻击框架，提升对抗迁移能力。"
    },
    {
        "title": "Computer-Use Agents as Judges for Generative User Interface",
        "url": "http://arxiv.org/abs/2511.15567v1",
        "pub_date": "2025-11-19",
        "summary": "Computer-Use Agents (CUA) are becoming increasingly capable of autonomously operating digital environments through Graphical User Interfaces (GUI). Yet, most GUI remain designed primarily for humans--prioritizing aesthetics and usability--forcing agents to adopt human-oriented behaviors that are unnecessary for efficient task execution. At the same time, rapid advances in coding-oriented language models (Coder) have transformed automatic GUI design. This raises a fundamental question: Can CUA as judges to assist Coder for automatic GUI design? To investigate, we introduce AUI-Gym, a benchmark for Automatic GUI development spanning 52 applications across diverse domains. Using language models, we synthesize 1560 tasks that simulate real-world scenarios. To ensure task reliability, we further develop a verifier that programmatically checks whether each task is executable within its environment. Building on this, we propose a Coder-CUA in Collaboration framework: the Coder acts as Designer, generating and revising websites, while the CUA serves as Judge, evaluating functionality and refining designs. Success is measured not by visual appearance, but by task solvability and CUA navigation success rate. To turn CUA feedback into usable guidance, we design a CUA Dashboard that compresses multi-step navigation histories into concise visual summaries, offering interpretable guidance for iterative redesign. By positioning agents as both designers and judges, our framework shifts interface design toward agent-native efficiency and reliability. Our work takes a step toward shifting agents from passive use toward active participation in digital environments. Our code and dataset are available at https://github.com/showlab/AUI.",
        "translated": "计算机使用代理（CUA）正日益具备通过图形用户界面（GUI）自主操作数字环境的能力。然而，大多数 GUI 仍主要为人类设计——侧重于美观与可用性——迫使代理采用面向人类的行为，而这些行为对高效任务执行并无必要。与此同时，以代码为导向的语言模型（Coder）的快速进步已重塑了自动 GUI 设计。这引发了一个根本性问题：CUA 是否能够作为评判者协助 Coder 实现自动 GUI 设计？为此，我们引入 AUI-Gym，一个涵盖 52 个跨多样领域应用程序的自动 GUI 开发基准。借助语言模型，我们合成 1560 项任务，模拟真实世界场景。为确保任务可靠性，我们进一步开发了一种验证器，可程序化地检查每项任务是否能在其环境中成功执行。在此基础上，我们提出“Coder-CUA 协同框架”：Coder 充当设计师，负责生成并修订网页；而 CUA 则担任评判者，评估功能并优化设计方案。成功与否不再以视觉外观衡量，而是依据任务可解性及 CUA 导航成功率。为将 CUA 的反馈转化为可操作指导，我们设计了 CUA 控制面板（Dashboard），该面板将多步导航历史压缩为简洁可视摘要，为迭代重设计提供可解释性引导。通过将代理定位为设计师与评判者双重角色，我们的框架推动界面设计向适配代理本体效率与可靠性的方向演进。本工作迈出了从被动使用向主动参与数字环境转变的重要一步。我们的代码与数据集可在 https://github.com/showlab/AUI 获取。",
        "translated_title": "计算机使用的代理作为生成式用户界面的评判者",
        "label": [],
        "label_reason": "属于高阶人机交互与界面设计，非像素级图像处理",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出协作框架创新，但非图像恢复任务"
    },
    {
        "title": "Scriboora: Rethinking Human Pose Forecasting",
        "url": "http://arxiv.org/abs/2511.15565v1",
        "pub_date": "2025-11-19",
        "summary": "Human pose forecasting predicts future poses based on past observations, and has many significant applications in areas such as action recognition, autonomous driving or human-robot interaction. This paper evaluates a wide range of pose forecasting algorithms in the task of absolute pose forecasting, revealing many reproducibility issues, and provides a unified training and evaluation pipeline. After drawing a high-level analogy to the task of speech understanding, it is shown that recent speech models can be efficiently adapted to the task of pose forecasting, and improve current state-of-the-art performance. At last the robustness of the models is evaluated, using noisy joint coordinates obtained from a pose estimator model, to reflect a realistic type of noise, which is more close to real-world applications. For this a new dataset variation is introduced, and it is shown that estimated poses result in a substantial performance degradation, and how much of it can be recovered again by unsupervised finetuning.",
        "translated": "人体姿态预测基于历史观测数据预测未来姿态，并在动作识别、自动驾驶或人机交互等领域具有诸多重要应用。本文评估了多种姿态预测算法在绝对姿态预测任务中的表现，揭示了大量可复现性问题，并提供了一个统一的训练与评估流程。通过与语音理解任务进行高层类比，研究表明近期的语音模型可高效适配至姿态预测任务，从而提升当前最优性能。最后，本文利用姿态估计器模型生成的噪声关节坐标对模型鲁棒性进行评估，以反映更贴近现实应用场景的噪声类型。为此引入了一个新的数据集变体，结果表明估计姿态导致显著的性能下降，且通过无监督微调可恢复多少性能。",
        "translated_title": "Scriboora：重新思考人体姿态预测",
        "label": [],
        "label_reason": "任务为高阶动作预测，非像素级图像处理",
        "relevance_score": 1,
        "novelty_score": 3,
        "novelty_reason": "方法迁移自语音模型，创新度有限"
    },
    {
        "title": "Multimodal Evaluation of Russian-language Architectures",
        "url": "http://arxiv.org/abs/2511.15552v1",
        "pub_date": "2025-11-19",
        "summary": "Multimodal large language models (MLLMs) are currently at the center of research attention, showing rapid progress in scale and capabilities, yet their intelligence, limitations, and risks remain insufficiently understood. To address these issues, particularly in the context of the Russian language, where no multimodal benchmarks currently exist, we introduce Mera Multi, an open multimodal evaluation framework for Russian-spoken architectures. The benchmark is instruction-based and encompasses default text, image, audio, and video modalities, comprising 18 newly constructed evaluation tasks for both general-purpose models and modality-specific architectures (image-to-text, video-to-text, and audio-to-text). Our contributions include: (i) a universal taxonomy of multimodal abilities; (ii) 18 datasets created entirely from scratch with attention to Russian cultural and linguistic specificity, unified prompts, and metrics; (iii) baseline results for both closed-source and open-source models; (iv) a methodology for preventing benchmark leakage, including watermarking and licenses for private sets. While our current focus is on Russian, the proposed benchmark provides a replicable methodology for constructing multimodal benchmarks in typologically diverse languages, particularly within the Slavic language family.",
        "translated": "多模态大语言模型（MLLMs）目前已成为研究热点，其规模与能力正迅速提升，然而其智能性、局限性及潜在风险仍尚未被充分理解。为解决这些问题——特别是在当前尚无任何多模态基准的俄语领域——我们提出了 Mera Multi，这是一个面向俄语架构的开放多模态评估框架。该基准基于指令设计，涵盖默认文本、图像、音频和视频模态，包含18项全新构建的评估任务，适用于通用型模型与特定模态架构（图像到文本、视频到文本、音频到文本）。我们的贡献包括：(i) 一套通用的多模态能力分类体系；(ii) 完全从零构建的18个数据集，特别关注俄语文化与语言特性，统一提示模板与评估指标；(iii) 针对闭源与开源模型的基线结果；(iv) 防止基准泄露的方法论，包括私有数据集的水印技术与许可机制。尽管当前工作聚焦于俄语，但所提出的基准提供了一种可复现的方法论，可用于在结构类型差异显著的语言中构建多模态基准，尤其适用于斯拉夫语族语言。",
        "translated_title": "俄语架构的多模态评估",
        "label": [],
        "label_reason": "研究多模态大语言模型，非图像像素级处理",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出首个俄语多模态评估框架，方法可复用"
    },
    {
        "title": "A Hybrid CNN-ViT-GNN Framework with GAN-Based Augmentation for Intelligent Weed Detection in Precision Agriculture",
        "url": "http://arxiv.org/abs/2511.15535v1",
        "pub_date": "2025-11-19",
        "summary": "The task of weed detection is an essential element of precision agriculture since accurate species identification allows a farmer to selectively apply herbicides and fits into sustainable agriculture crop management. This paper proposes a hybrid deep learning framework recipe for weed detection that utilizes Convolutional Neural Networks (CNNs), Vision Transformers (ViTs), and Graph Neural Networks (GNNs) to build robustness to multiple field conditions. A Generative Adversarial Network (GAN)-based augmentation method was imposed to balance class distributions and better generalize the model. Further, a self-supervised contrastive pre-training method helps to learn more features from limited annotated data. Experimental results yield superior results with 99.33% accuracy, precision, recall, and F1-score on multi-benchmark datasets. The proposed model architecture enables local, global, and relational feature representations and offers high interpretability and adaptability. Practically, the framework allows real-time, efficient deployment to edge devices for automated weed detecting, reducing over-reliance on herbicides and providing scalable, sustainable precision-farming options.",
        "translated": "杂草检测任务是精准农业的重要组成部分，因为准确的物种识别使农民能够有针对性地施用除草剂，并契合可持续农业生产管理的需求。本文提出了一种混合深度学习框架，结合卷积神经网络（CNNs）、视觉Transformer（ViTs）和图神经网络（GNNs），以增强模型在多种田间条件下的鲁棒性。采用基于生成对抗网络（GAN）的数据增强方法，以平衡类别分布并提升模型泛化能力。此外，自监督对比预训练方法有助于从有限标注数据中学习更丰富的特征。实验结果表明，该模型在多个基准数据集上取得了优越性能，准确率、精确率、召回率和F1分数均达到99.33%。所提出的模型架构能够有效表征局部、全局及关系型特征，具备高可解释性与强适应性。在实际应用中，该框架支持在边缘设备上实时高效部署，实现自动化杂草检测，减少对除草剂的过度依赖，并提供可扩展、可持续的精准农业解决方案。",
        "translated_title": "一种基于GAN增强的混合CNN-ViT-GNN框架用于精准农业中的智能杂草检测",
        "label": [],
        "label_reason": "高阶目标检测任务，非低层级图像处理",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "混合架构创新但属分类任务"
    },
    {
        "title": "Multi-Text Guided Few-Shot Semantic Segmentation",
        "url": "http://arxiv.org/abs/2511.15515v1",
        "pub_date": "2025-11-19",
        "summary": "Recent CLIP-based few-shot semantic segmentation methods introduce class-level textual priors to assist segmentation by typically using a single prompt (e.g., a photo of class). However, these approaches often result in incomplete activation of target regions, as a single textual description cannot fully capture the semantic diversity of complex categories. Moreover, they lack explicit cross-modal interaction and are vulnerable to noisy support features, further degrading visual prior quality. To address these issues, we propose the Multi-Text Guided Few-Shot Semantic Segmentation Network (MTGNet), a dual-branch framework that enhances segmentation performance by fusing diverse textual prompts to refine textual priors and guide the cross-modal optimization of visual priors. Specifically, we design a Multi-Textual Prior Refinement (MTPR) module that suppresses interference and aggregates complementary semantic cues to enhance foreground activation and expand semantic coverage for structurally complex objects. We introduce a Text Anchor Feature Fusion (TAFF) module, which leverages multi-text embeddings as semantic anchors to facilitate the transfer of discriminative local prototypes from support images to query images, thereby improving semantic consistency and alleviating intra-class variations. Furthermore, a Foreground Confidence-Weighted Attention (FCWA) module is presented to enhance visual prior robustness by leveraging internal self-similarity within support foreground features. It adaptively down-weights inconsistent regions and effectively suppresses interference in the query segmentation process. Extensive experiments on standard FSS benchmarks validate the effectiveness of MTGNet. In the 1-shot setting, it achieves 76.8% mIoU on PASCAL-5i and 57.4% on COCO-20i, with notable improvements in folds exhibiting high intra-class variations.",
        "translated": "近年来基于CLIP的少样本语义分割方法通常通过使用单一提示（例如某一类别的照片）引入类别级文本先验以辅助分割。然而，这类方法往往导致目标区域激活不完整，因为单个文本描述难以充分捕捉复杂类别的语义多样性。此外，这些方法缺乏显式的跨模态交互，且对噪声支持特征敏感，进一步降低了视觉先验的质量。为解决上述问题，我们提出多文本引导少样本语义分割网络（MTGNet），这是一种双分支框架，通过融合多样化的文本提示来优化文本先验并指导视觉先验的跨模态优化，从而提升分割性能。具体而言，我们设计了一个多文本先验精炼模块（MTPR），该模块抑制干扰、聚合互补语义线索，以增强前景激活并扩展结构复杂对象的语义覆盖范围。我们引入了文本锚点特征融合模块（TAFF），利用多文本嵌入作为语义锚点，促进从支持图像到查询图像中判别性局部原型的有效迁移，从而提升语义一致性并缓解类内差异。此外，我们提出了前景置信加权注意力模块（FCWA），通过利用支持前景特征内部的自相似性增强视觉先验鲁棒性，自适应地降低不一致区域的权重，并在查询分割过程中有效抑制干扰。在标准FSS基准数据集上的大量实验验证了MTGNet的有效性：在1-shot设置下，其在PASCAL-5i上达到76.8%的mIoU，在COCO-20i上达到57.4%，尤其在类内差异显著的数据折中表现出了显著提升。",
        "translated_title": "多文本引导的少样本语义分割",
        "label": [],
        "label_reason": "目标为语义分割，属高阶视觉任务",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "引入多文本引导机制提升分割性能"
    },
    {
        "title": "Learning to Expand Images for Efficient Visual Autoregressive Modeling",
        "url": "http://arxiv.org/abs/2511.15499v1",
        "pub_date": "2025-11-19",
        "summary": "Autoregressive models have recently shown great promise in visual generation by leveraging discrete token sequences akin to language modeling. However, existing approaches often suffer from inefficiency, either due to token-by-token decoding or the complexity of multi-scale representations. In this work, we introduce Expanding Autoregressive Representation (EAR), a novel generation paradigm that emulates the human visual system's center-outward perception pattern. EAR unfolds image tokens in a spiral order from the center and progressively expands outward, preserving spatial continuity and enabling efficient parallel decoding. To further enhance flexibility and speed, we propose a length-adaptive decoding strategy that dynamically adjusts the number of tokens predicted at each step. This biologically inspired design not only reduces computational cost but also improves generation quality by aligning the generation order with perceptual relevance. Extensive experiments on ImageNet demonstrate that EAR achieves state-of-the-art trade-offs between fidelity and efficiency on single-scale autoregressive models, setting a new direction for scalable and cognitively aligned autoregressive image generation.",
        "translated": "自回归模型近年来通过借鉴语言建模中类似的离散token序列，在视觉生成领域展现出巨大潜力。然而，现有方法往往因逐token解码或多尺度表示的复杂性而效率低下。在本研究中，我们提出了一种新颖的生成范式——扩展自回归表示（Expanding Autoregressive Representation, EAR），其模拟了人类视觉系统由中心向外感知的模式。EAR 从图像中心以螺旋顺序展开图像token，并逐步向外扩展，从而保留空间连续性并支持高效的并行解码。为进一步提升灵活性与速度，我们提出一种长度自适应解码策略，动态调整每一步预测的token数量。这一受生物启发的设计不仅降低了计算开销，还通过将生成顺序与感知相关性对齐，提升了生成质量。在ImageNet上的大量实验表明，EAR在单尺度自回归模型上实现了保真度与效率之间的最先进权衡，为可扩展且符合认知规律的自回归图像生成开辟了新方向。",
        "translated_title": "学习扩展图像以实现高效的视觉自回归建模",
        "label": [],
        "label_reason": "生成模型非图像恢复任务",
        "relevance_score": 1,
        "novelty_score": 9,
        "novelty_reason": "螺旋解码新范式提升效率"
    },
    {
        "title": "Evaluating Low-Light Image Enhancement Across Multiple Intensity Levels",
        "url": "http://arxiv.org/abs/2511.15496v1",
        "pub_date": "2025-11-19",
        "summary": "Imaging in low-light environments is challenging due to reduced scene radiance, which leads to elevated sensor noise and reduced color saturation. Most learning-based low-light enhancement methods rely on paired training data captured under a single low-light condition and a well-lit reference. The lack of radiance diversity limits our understanding of how enhancement techniques perform across varying illumination intensities. We introduce the Multi-Illumination Low-Light (MILL) dataset, containing images captured at diverse light intensities under controlled conditions with fixed camera settings and precise illuminance measurements. MILL enables comprehensive evaluation of enhancement algorithms across variable lighting conditions. We benchmark several state-of-the-art methods and reveal significant performance variations across intensity levels. Leveraging the unique multi-illumination structure of our dataset, we propose improvements that enhance robustness across diverse illumination scenarios. Our modifications achieve up to 10 dB PSNR improvement for DSLR and 2 dB for the smartphone on Full HD images.",
        "translated": "低光照环境下的成像具有挑战性，由于场景辐照度降低，导致传感器噪声增加、色彩饱和度下降。大多数基于学习的低光照增强方法依赖于在单一低光照条件下拍摄的配对训练数据与光照良好的参考图像。辐照度多样性不足限制了我们对增强技术在不同光照强度下表现的理解。我们提出了多光照低光照（MILL）数据集，该数据集包含在固定相机参数和精确照度测量条件下、于多种光照强度下采集的图像。MILL支持对增强算法在不同光照条件下的全面评估。我们对若干当前最先进的方法进行了基准测试，并揭示其性能随光照强度变化而显著波动。利用本数据集独特的多光照结构，我们提出改进方案以提升算法在多样化光照场景中的鲁棒性。我们的改进方法在全高清图像上实现了最高达 10 dB 的 PSNR 提升（针对 DSLR 相机）以及 2 dB 的提升（针对智能手机）。",
        "translated_title": "多强度级别下的低光照图像增强评估",
        "label": [
            "低光照增强"
        ],
        "label_reason": "专注低光照图像质量提升，属低层图像增强任务。",
        "relevance_score": 10,
        "novelty_score": 8,
        "novelty_reason": "提出多光照度数据集并改进算法鲁棒性。"
    },
    {
        "title": "NTK-Guided Implicit Neural Teaching",
        "url": "http://arxiv.org/abs/2511.15487v1",
        "pub_date": "2025-11-19",
        "summary": "Implicit Neural Representations (INRs) parameterize continuous signals via multilayer perceptrons (MLPs), enabling compact, resolution-independent modeling for tasks like image, audio, and 3D reconstruction. However, fitting high-resolution signals demands optimizing over millions of coordinates, incurring prohibitive computational costs. To address it, we propose NTK-Guided Implicit Neural Teaching (NINT), which accelerates training by dynamically selecting coordinates that maximize global functional updates. Leveraging the Neural Tangent Kernel (NTK), NINT scores examples by the norm of their NTK-augmented loss gradients, capturing both fitting errors and heterogeneous leverage (self-influence and cross-coordinate coupling). This dual consideration enables faster convergence compared to existing methods. Through extensive experiments, we demonstrate that NINT significantly reduces training time by nearly half while maintaining or improving representation quality, establishing state-of-the-art acceleration among recent sampling-based strategies.",
        "translated": "隐式神经表示（INRs）通过多层感知机（MLPs）对连续信号进行参数化，从而支持紧凑且与分辨率无关的建模，适用于图像、音频和3D重建等任务。然而，拟合高分辨率信号需要在数百万个坐标上进行优化，导致计算成本过高。为解决该问题，我们提出了一种基于神经切线核（NTK）引导的隐式神经教学方法（NINT），通过动态选择能够最大化全局函数更新的坐标以加速训练。借助神经切线核（NTK），NINT 通过其增强损失梯度的范数对样本进行评分，同时捕捉拟合误差与异质性杠杆效应（即自影响与跨坐标耦合）。这种双重考量使得 NINT 相较于现有方法能够实现更快的收敛速度。通过大量实验，我们证明 NINT 在显著缩短训练时间近一半的同时，保持或提升了表征质量，在近期基于采样的策略中建立了最先进的加速效果。",
        "translated_title": "NTK引导的隐式神经教学",
        "label": [],
        "label_reason": "聚焦INR训练加速，非图像像素级恢复任务",
        "relevance_score": 3,
        "novelty_score": 8,
        "novelty_reason": "引入NTK指导采样，显著提升训练效率"
    },
    {
        "title": "A Novel CustNetGC Boosted Model with Spectral Features for Parkinson's Disease Prediction",
        "url": "http://arxiv.org/abs/2511.15485v1",
        "pub_date": "2025-11-19",
        "summary": "Parkinson's disease is a neurodegenerative disorder that can be very tricky to diagnose and treat. Such early symptoms can include tremors, wheezy breathing, and changes in voice quality as critical indicators of neural damage. Notably, there has been growing interest in utilizing changes in vocal attributes as markers for the detection of PD early on. Based on this understanding, the present paper was designed to focus on the acoustic feature analysis based on voice recordings of patients diagnosed with PD and healthy controls (HC). In this paper, we introduce a novel classification and visualization model known as CustNetGC, combining a Convolutional Neural Network (CNN) with Custom Network Grad-CAM and CatBoost to enhance the efficiency of PD diagnosis. We use a publicly available dataset from Figshare, including voice recordings of 81 participants: 40 patients with PD and 41 healthy controls. From these recordings, we extracted the key spectral features: L-mHP and Spectral Slopes. The L-mHP feature combines three spectrogram representations: Log-Mel spectrogram, harmonic spectrogram, and percussive spectrogram, which are derived using Harmonic-Percussive Source Separation (HPSS). Grad-CAM was used to highlight the important regions in the data, thus making the PD predictions interpretable and effective. Our proposed CustNetGC model achieved an accuracy of 99.06% and precision of 95.83%, with the area under the ROC curve (AUC) recorded at 0.90 for the PD class and 0.89 for the HC class. Additionally, the combination of CatBoost, a gradient boosting algorithm, enhanced the robustness and the prediction performance by properly classifying PD and non-PD samples. Therefore, the results provide the potential improvement in the CustNetGC system in enhancing diagnostic accuracy and the interpretability of the Parkinson's Disease prediction model.",
        "translated": "帕金森病是一种神经退行性疾病，其诊断与治疗往往十分困难。早期症状可能包括震颤、喘息式呼吸以及声音质量的改变，这些可作为神经系统损伤的关键指标。值得注意的是，近年来学界对声学属性变化作为帕金森病（PD）早期检测标志物的兴趣日益增长。基于这一认识，本文聚焦于利用帕金森病患者与健康对照组（HC）的语音录音进行声学特征分析。在本文中，我们提出了一种新颖的分类与可视化模型——CustNetGC，该模型结合了卷积神经网络（CNN）、自定义网络Grad-CAM以及CatBoost算法，以提升帕金森病诊断效率。我们使用来自Figshare平台的公开数据集，包含81名参与者的语音录音：其中40名为帕金森病患者，41名为健康对照者。从这些录音中，我们提取了关键频谱特征：L-mHP和频谱斜率。L-mHP特征融合了三种声谱图表示形式：对数梅尔谱图、谐波谱图与打击谱图，这些由谐波-打击源分离（HPSS）方法推导得出。Grad-CAM用于突出数据中的重要区域，从而提升帕金森病预测结果的可解释性与有效性。所提出的CustNetGC模型在测试中达到99.06%的准确率与95.83%的精确率，帕金森病类别的曲线下面积（AUC）为0.90，健康对照组类别为0.89。此外，CatBoost作为一种梯度提升算法，通过合理区分帕金森病与非帕金森病样本，进一步增强了模型的鲁棒性与预测性能。因此，本研究结果表明，CustNetGC系统在提高帕金森病诊断准确性及预测模型可解释性方面具有改进潜力。",
        "translated_title": "一种结合谱特征的新型CustNetGC增强模型用于帕金森病预测",
        "label": [],
        "label_reason": "属于高阶医学分类任务，非图像像素级处理",
        "relevance_score": 1,
        "novelty_score": 3,
        "novelty_reason": "组合现有模型，无新范式或架构创新"
    },
    {
        "title": "FunnyNodules: A Customizable Medical Dataset Tailored for Evaluating Explainable AI",
        "url": "http://arxiv.org/abs/2511.15481v1",
        "pub_date": "2025-11-19",
        "summary": "Densely annotated medical image datasets that capture not only diagnostic labels but also the underlying reasoning behind these diagnoses are scarce. Such reasoning-related annotations are essential for developing and evaluating explainable AI (xAI) models that reason similarly to radiologists: making correct predictions for the right reasons. To address this gap, we introduce FunnyNodules, a fully parameterized synthetic dataset designed for systematic analysis of attribute-based reasoning in medical AI models. The dataset generates abstract, lung nodule-like shapes with controllable visual attributes such as roundness, margin sharpness, and spiculation. Target class is derived from a predefined attribute combination, allowing full control over the decision rule that links attributes to the diagnostic class. We demonstrate how FunnyNodules can be used in model-agnostic evaluations to assess whether models learn correct attribute-target relations, to interpret over- or underperformance in attribute prediction, and to analyze attention alignment with attribute-specific regions of interest. The framework is fully customizable, supporting variations in dataset complexity, target definitions, class balance, and beyond. With complete ground truth information, FunnyNodules provides a versatile foundation for developing, benchmarking, and conducting in-depth analyses of explainable AI methods in medical image analysis.",
        "translated": "目前缺乏能够同时捕捉诊断标签及其背后推理依据的密集标注医学图像数据集。此类与推理相关的标注对于开发和评估可解释人工智能（xAI）模型至关重要，这些模型需如放射科医生般进行推理：基于正确的原因做出正确的预测。为弥补这一不足，我们提出了 FunnyNodules，一个完全参数化的合成数据集，旨在系统性分析医学AI模型中的属性驱动推理机制。该数据集生成抽象的、类似肺结节的形状，并支持控制诸如圆润度、边缘锐度及毛刺等视觉属性。目标类别由预定义的属性组合推导得出，从而实现对“属性到诊断类别”决策规则的完全控制。我们展示了如何利用 FunnyNodules 进行模型无关的评估，以判断模型是否学习到正确的属性-目标关系，解释其在特定属性预测上的过拟合或欠拟合现象，并分析注意力机制与属性相关感兴趣区域的对齐情况。该框架高度可定制化，支持数据集复杂度、目标定义、类别平衡等多种参数的变化。凭借完整的地面真实信息，FunnyNodules 为医学图像分析中可解释AI方法的开发、基准测试及深度分析提供了通用基础。",
        "translated_title": "FunnyNodules：一个为评估可解释人工智能而定制的医学数据集",
        "label": [],
        "label_reason": "聚焦可解释AI评估，非图像像素级恢复任务",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出可定制合成数据集，用于模型解释性分析"
    },
    {
        "title": "RS-CA-HSICT: A Residual and Spatial Channel Augmented CNN Transformer Framework for Monkeypox Detection",
        "url": "http://arxiv.org/abs/2511.15476v1",
        "pub_date": "2025-11-19",
        "summary": "This work proposes a hybrid deep learning approach, namely Residual and Spatial Learning based Channel Augmented Integrated CNN-Transformer architecture, that leverages the strengths of CNN and Transformer towards enhanced MPox detection. The proposed RS-CA-HSICT framework is composed of an HSICT block, a residual CNN module, a spatial CNN block, and a CA, which enhances the diverse feature space, detailed lesion information, and long-range dependencies. The new HSICT module first integrates an abstract representation of the stem CNN and customized ICT blocks for efficient multihead attention and structured CNN layers with homogeneous (H) and structural (S) operations. The customized ICT blocks learn global contextual interactions and local texture extraction. Additionally, H and S layers learn spatial homogeneity and fine structural details by reducing noise and modeling complex morphological variations. Moreover, inverse residual learning enhances vanishing gradient, and stage-wise resolution reduction ensures scale invariance. Furthermore, the RS-CA-HSICT framework augments the learned HSICT channels with the TL-driven Residual and Spatial CNN maps for enhanced multiscale feature space capturing global and localized structural cues, subtle texture, and contrast variations. These channels, preceding augmentation, are refined through the Channel-Fusion-and-Attention block, which preserves discriminative channels while suppressing redundant ones, thereby enabling efficient computation. Finally, the spatial attention mechanism refines pixel selection to detect subtle patterns and intra-class contrast variations in Mpox. Experimental results on both the Kaggle benchmark and a diverse MPox dataset reported classification accuracy as high as 98.30% and an F1-score of 98.13%, which outperforms the existing CNNs and ViTs.",
        "translated": "本工作提出了一种混合深度学习方法，即基于残差与空间学习的通道增强集成卷积-Transformer架构（RS-CA-HSICT），该方法充分利用卷积神经网络（CNN）与Transformer的优势以提升猴痘病毒检测性能。所提出的RS-CA-HSICT框架由HSICT模块、残差CNN模块、空间CNN模块及通道注意力机制（CA）组成，旨在增强多样化的特征空间、病灶细节信息以及长距离依赖关系。新设计的HSICT模块首先融合了抽象化的主干CNN结构与定制化的ICT模块，实现高效的多头注意力机制和具有同质性（H）与结构性（S）操作的结构化CNN层。定制化的ICT模块可学习全局上下文交互与局部纹理提取能力；此外，H层与S层分别通过降噪处理并建模复杂的形态学变化，学习空间同质性与精细结构细节。同时，逆残差学习增强梯度传播，分阶段分辨率压缩确保尺度不变性。进一步地，RS-CA-HSICT框架通过引入由迁移学习驱动的残差与空间CNN特征图，对所学习的HSICT通道进行增强，从而更全面捕获多尺度特征，包括全局与局部结构线索、细微纹理及对比度差异。在通道增强前，这些通道经由“通道融合与注意力”模块进行优化，保留判别性强的通道并抑制冗余通道，从而实现高效计算。最后，空间注意力机制优化像素选择，以检测猴痘病毒中细微模式及类内对比度变化。实验结果表明，在Kaggle基准数据集与多样化猴痘数据集上，分类准确率高达98.30%，F1分数达到98.13%，优于现有CNN与ViT模型。",
        "translated_title": "RS-CA-HSICT：一种用于猴痘检测的残差与空间通道增强型CNN Transformer框架",
        "label": [],
        "label_reason": "任务为疾病分类，属高阶视觉任务",
        "relevance_score": 1,
        "novelty_score": 3,
        "novelty_reason": "架构改进但无像素级恢复创新"
    },
    {
        "title": "Deep Learning for Accurate Vision-based Catch Composition in Tropical Tuna Purse Seiners",
        "url": "http://arxiv.org/abs/2511.15468v1",
        "pub_date": "2025-11-19",
        "summary": "Purse seiners play a crucial role in tuna fishing, as approximately 69% of the world's tropical tuna is caught using this gear. All tuna Regional Fisheries Management Organizations have established minimum standards to use electronic monitoring (EM) in fisheries in addition to traditional observers. The EM systems produce a massive amount of video data that human analysts must process. Integrating artificial intelligence (AI) into their workflow can decrease that workload and improve the accuracy of the reports. However, species identification still poses significant challenges for AI, as achieving balanced performance across all species requires appropriate training data. Here, we quantify the difficulty experts face to distinguish bigeye tuna (BET, Thunnus Obesus) from yellowfin tuna (YFT, Thunnus Albacares) using images captured by EM systems. We found inter-expert agreements of 42.9% $\\pm$ 35.6% for BET and 57.1% $\\pm$ 35.6% for YFT. We then present a multi-stage pipeline to estimate the species composition of the catches using a reliable ground-truth dataset based on identifications made by observers on board. Three segmentation approaches are compared: Mask R-CNN, a combination of DINOv2 with SAM2, and a integration of YOLOv9 with SAM2. We found that the latest performs the best, with a validation mean average precision of 0.66 $\\pm$ 0.03 and a recall of 0.88 $\\pm$ 0.03. Segmented individuals are tracked using ByteTrack. For classification, we evaluate a standard multiclass classification model and a hierarchical approach, finding a superior generalization by the hierarchical. All our models were cross-validated during training and tested on fishing operations with fully known catch composition. Combining YOLOv9-SAM2 with the hierarchical classification produced the best estimations, with 84.8% of the individuals being segmented and classified with a mean average error of 4.5%.",
        "translated": " purse seiners 在金枪鱼捕捞中扮演着关键角色，因为全球约69%的热带金枪鱼是通过该种渔具捕获的。所有金枪鱼区域渔业管理组织均已建立最低标准，要求在传统观察员之外，渔业活动必须使用电子监控（EM）系统。EM系统产生海量视频数据，需由人工分析师处理。将人工智能（AI）整合至其工作流程可减轻工作负担并提升报告准确性。然而，物种识别仍对AI构成重大挑战，因实现对所有物种均衡性能需要适当训练数据。本文量化了专家在使用EM系统拍摄图像区分大眼金枪鱼（BET, Thunnus Obesus）与黄鳍金枪鱼（YFT, Thunnus Albacares）时所面临的困难。我们发现，对于BET，专家间一致性仅为42.9% ± 35.6%；对于YFT，则为57.1% ± 35.6%。随后，我们提出了一种多阶段管道，利用基于船上观察员鉴定结果构建的可靠真实标签数据集，估算渔获物的种类组成。对比了三种分割方法：Mask R-CNN、DINOv2与SAM2的组合，以及YOLOv9与SAM2的集成方案。结果表明，最新方案表现最优，验证集平均精度为0.66 ± 0.03，召回率为0.88 ± 0.03。分割后的个体采用ByteTrack进行追踪。在分类方面，我们评估了标准多类别分类模型和分层方法，发现分层方法具有更优泛化能力。所有模型在训练期间均进行了交叉验证，并在已知完整渔获物组成的捕捞作业中测试。将YOLOv9-SAM2与分层分类相结合的方法获得了最佳估计效果，其中84.8%的个体被成功分割并分类，平均误差为4.5%。",
        "translated_title": "深度学习在热带金枪鱼围网渔船基于视觉的渔获物组成精确识别中的应用",
        "label": [],
        "label_reason": "任务为物种识别，属高阶视觉分类。",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "组合现有模型，无本质创新架构。"
    },
    {
        "title": "SIGMMA: Hierarchical Graph-Based Multi-Scale Multi-modal Contrastive Alignment of Histopathology Image and Spatial Transcriptome",
        "url": "http://arxiv.org/abs/2511.15464v1",
        "pub_date": "2025-11-19",
        "summary": "Recent advances in computational pathology have leveraged vision-language models to learn joint representations of Hematoxylin and Eosin (HE) images with spatial transcriptomic (ST) profiles. However, existing approaches typically align HE tiles with their corresponding ST profiles at a single scale, overlooking fine-grained cellular structures and their spatial organization. To address this, we propose Sigmma, a multi-modal contrastive alignment framework for learning hierarchical representations of HE images and spatial transcriptome profiles across multiple scales. Sigmma introduces multi-scale contrastive alignment, ensuring that representations learned at different scales remain coherent across modalities. Furthermore, by representing cell interactions as a graph and integrating inter- and intra-subgraph relationships, our approach effectively captures cell-cell interactions, ranging from fine to coarse, within the tissue microenvironment. We demonstrate that Sigmm learns representations that better capture cross-modal correspondences, leading to an improvement of avg. 9.78\\% in the gene-expression prediction task and avg. 26.93\\% in the cross-modal retrieval task across datasets. We further show that it learns meaningful multi-tissue organization in downstream analyses.",
        "translated": "近年来，计算病理学领域的研究利用视觉-语言模型学习苏木精和伊红（HE）图像与空间转录组（ST）图谱的联合表征。然而，现有方法通常仅在单一尺度上对齐 HE 图块与其对应的 ST 图谱，忽略了精细的细胞结构及其空间组织关系。为此，我们提出 Sigmma，一种多模态对比对齐框架，用于在多个尺度上学习 HE 图像与空间转录组图谱的层次化表征。Sigmma 引入多尺度对比对齐机制，确保不同尺度下所学习到的表征在跨模态间保持一致性。此外，通过将细胞相互作用建模为图结构，并整合子图间的交互关系及子图内关系，我们的方法有效捕捉了组织微环境中从精细到粗粒度的细胞间相互作用。实验表明，Sigmma 学习到的表征更准确地捕捉了跨模态对应关系，在基因表达预测任务中平均提升 9.78%，在跨模态检索任务中平均提升 26.93%（跨数据集）。进一步的下游分析表明，该方法还能够学习到有意义的多组织组织结构。",
        "translated_title": "SIGMMA：基于层次图的多尺度多模态病理图像与空间转录组对比对齐",
        "label": [],
        "label_reason": "任务为跨模态病理图像与转录组对齐，属高阶语义理解",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出多尺度图结构对比学习框架，提升跨模态表示"
    },
    {
        "title": "CroPS: Improving Dense Retrieval with Cross-Perspective Positive Samples in Short-Video Search",
        "url": "http://arxiv.org/abs/2511.15443v1",
        "pub_date": "2025-11-19",
        "summary": "Dense retrieval has become a foundational paradigm in modern search systems, especially on short-video platforms. However, most industrial systems adopt a self-reinforcing training pipeline that relies on historically exposed user interactions for supervision. This paradigm inevitably leads to a filter bubble effect, where potentially relevant but previously unseen content is excluded from the training signal, biasing the model toward narrow and conservative retrieval. In this paper, we present CroPS (Cross-Perspective Positive Samples), a novel retrieval data engine designed to alleviate this problem by introducing diverse and semantically meaningful positive examples from multiple perspectives. CroPS enhances training with positive signals derived from user query reformulation behavior (query-level), engagement data in recommendation streams (system-level), and world knowledge synthesized by large language models (knowledge-level). To effectively utilize these heterogeneous signals, we introduce a Hierarchical Label Assignment (HLA) strategy and a corresponding H-InfoNCE loss that together enable fine-grained, relevance-aware optimization. Extensive experiments conducted on Kuaishou Search, a large-scale commercial short-video search platform, demonstrate that CroPS significantly outperforms strong baselines both offline and in live A/B tests, achieving superior retrieval performance and reducing query reformulation rates. CroPS is now fully deployed in Kuaishou Search, serving hundreds of millions of users daily.",
        "translated": "稠密检索已成为现代搜索系统的基础范式，尤其在短视频平台中尤为关键。然而，大多数工业级系统采用一种自强化训练流程，其监督信号依赖于历史上已曝光的用户交互行为。这种范式不可避免地导致“信息茧房”效应——那些潜在相关但此前未被曝光的内容被排除在训练信号之外，致使模型偏向窄化保守的检索倾向。本文提出 CroPS（Cross-Perspective Positive Samples），一种新型检索数据引擎，旨在通过引入来自多视角的多样化且语义丰富的正样本缓解该问题。CroPS 通过三类异构正样本增强训练：用户查询重写行为所衍生的查询级别正信号、推荐流中的用户参与数据所贡献的系统级别正信号，以及由大语言模型合成的世界知识所提供的知识级别正信号。为有效利用这些异质信号，我们设计了层级标签分配（Hierarchical Label Assignment, HLA）策略与对应的 H-InfoNCE 损失函数，共同实现细粒度、相关性感知的优化。在快手搜索（Kuaishou Search）这一大规模商业短视频搜索平台上进行的广泛实验表明，CroPS 在离线评估和线上 A/B 测试中均显著优于主流基线方法，取得更优的检索效果并降低用户查询重写率。目前，CroPS 已全面部署于快手搜索系统，日均服务数亿用户。",
        "translated_title": "CroPS：通过跨视角正样本提升短视频搜索中的稠密检索",
        "label": [
            "召回",
            "负采样与对比学习"
        ],
        "label_reason": "聚焦检索召回优化，引入多视角正样本缓解滤泡效应",
        "relevance_score": 7,
        "novelty_score": 8,
        "novelty_reason": "创新性引入三层次正样本及HLA策略提升训练信号多样性"
    },
    {
        "title": "HV-Attack: Hierarchical Visual Attack for Multimodal Retrieval Augmented Generation",
        "url": "http://arxiv.org/abs/2511.15435v1",
        "pub_date": "2025-11-19",
        "summary": "Advanced multimodal Retrieval-Augmented Generation (MRAG) techniques have been widely applied to enhance the capabilities of Large Multimodal Models (LMMs), but they also bring along novel safety issues. Existing adversarial research has revealed the vulnerability of MRAG systems to knowledge poisoning attacks, which fool the retriever into recalling injected poisoned contents. However, our work considers a different setting: visual attack of MRAG by solely adding imperceptible perturbations at the image inputs of users, without manipulating any other components. This is challenging due to the robustness of fine-tuned retrievers and large-scale generators, and the effect of visual perturbation may be further weakened by propagation through the RAG chain. We propose a novel Hierarchical Visual Attack that misaligns and disrupts the two inputs (the multimodal query and the augmented knowledge) of MRAG's generator to confuse its generation. We further design a hierarchical two-stage strategy to obtain misaligned augmented knowledge. We disrupt the image input of the retriever to make it recall irrelevant knowledge from the original database, by optimizing the perturbation which first breaks the cross-modal alignment and then disrupts the multimodal semantic alignment. We conduct extensive experiments on two widely-used MRAG datasets: OK-VQA and InfoSeek. We use CLIP-based retrievers and two LMMs BLIP-2 and LLaVA as generators. Results demonstrate the effectiveness of our visual attack on MRAG through the significant decrease in both retrieval and generation performance.",
        "translated": "先进的多模态检索增强生成（MRAG）技术已被广泛应用于提升大型多模态模型（LMMs）的能力，但也随之带来了新的安全风险。现有对抗性研究已揭示MRAG系统易受知识投毒攻击的脆弱性，此类攻击通过误导检索器召回注入的有毒内容实现。然而，我们的工作考虑的是另一种场景：仅通过对用户图像输入添加不可感知扰动，实施视觉攻击以破坏MRAG系统，而不对其他组件进行任何操作。这极具挑战性，原因在于经过微调的检索器与大规模生成器具有较强的鲁棒性，且视觉扰动在RAG链路中传播时其效果可能进一步被削弱。我们提出了一种新颖的分层视觉攻击方法，通过干扰和破坏MRAG生成器的两个输入（多模态查询与增强知识），混淆其生成过程。我们进一步设计了一种分层两阶段策略，以获取错位的增强知识：首先破坏检索器的图像输入，使其从原始数据库中召回无关知识；该扰动优化过程先打破跨模态对齐，再干扰多模态语义对齐。我们在两个常用的MRAG数据集OK-VQA和InfoSeek上进行了大量实验，使用基于CLIP的检索器及两种LMM——BLIP-2和LLaVA作为生成器。实验结果表明，我们的视觉攻击显著降低了检索与生成性能，验证了其有效性。",
        "translated_title": "HV-Attack：面向多模态检索增强生成的分层视觉攻击",
        "label": [
            "多模态推荐",
            "推荐系统评估"
        ],
        "label_reason": "研究视觉攻击对多模态检索生成的影响，间接关联推荐系统安全评估。",
        "relevance_score": 4,
        "novelty_score": 7,
        "novelty_reason": "提出分层视觉对抗策略，针对性破坏跨模态对齐，具创新性改进。"
    },
    {
        "title": "NAMeGEn: Creative Name Generation via A Novel Agent-based Multiple Personalized Goal Enhancement Framework",
        "url": "http://arxiv.org/abs/2511.15408v1",
        "pub_date": "2025-11-19",
        "summary": "Trained on diverse human-authored texts, Large Language Models (LLMs) unlocked the potential for Creative Natural Language Generation (CNLG), benefiting various applications like advertising and storytelling. Nevertheless, CNLG still remains difficult due to two main challenges. (1) Multi-objective flexibility: user requirements are often personalized, fine-grained, and pluralistic, which LLMs struggle to satisfy simultaneously; (2) Interpretive complexity: beyond generation, creativity also involves understanding and interpreting implicit meaning to enhance users' perception. These challenges significantly limit current methods, especially in short-form text generation, in generating creative and insightful content. To address this, we focus on Chinese baby naming, a representative short-form CNLG task requiring adherence to explicit user constraints (e.g., length, semantics, anthroponymy) while offering meaningful aesthetic explanations. We propose NAMeGEn, a novel multi-agent optimization framework that iteratively alternates between objective extraction, name generation, and evaluation to meet diverse requirements and generate accurate explanations. To support this task, we further construct a classical Chinese poetry corpus with 17k+ poems to enhance aesthetics, and introduce CBNames, a new benchmark with tailored metrics. Extensive experiments demonstrate that NAMeGEn effectively generates creative names that meet diverse, personalized requirements while providing meaningful explanations, outperforming six baseline methods spanning various LLM backbones without any training.",
        "translated": "在多样的人类创作文本上训练的大语言模型（LLMs）释放了创意自然语言生成（CNLG）的潜力，并为广告、故事创作等应用场景提供了助力。然而，CNLG 仍面临两大核心挑战：(1) 多目标灵活性：用户需求往往高度个性化、细粒度且多元，而 LLMs 难以同时满足这些复杂要求；(2) 解释性复杂性：除生成之外，创造力还涉及对隐含语义的理解与阐释，以增强用户的感知体验。这些挑战严重制约了现有方法——特别是在短文本生成领域——创造兼具创意与洞察力的内容的能力。为此，我们聚焦中文起名这一具有代表性的短文本 CNLG 任务，该任务需严格遵循明确的用户约束（如长度、语义、人名规范），并提供具审美价值的解释说明。我们提出 NAMeGEn，一种新颖的多智能体优化框架，通过交替进行目标提取、姓名生成与评估，以满足多样化需求并生成精准的解释。为支持此任务，我们进一步构建了一个包含逾 17,000 首古典中文诗歌的语料库，以提升美学质量，并引入 CBNames 数据集，配备定制化评估指标。大量实验表明，NAMeGEn 能有效生成符合多样化、个性化需求的创意姓名，并提供有意义的解释，在无需任何训练的前提下显著优于六种基于不同 LLM 架构的基线方法。",
        "translated_title": "NAMeGEn：基于新颖的代理多个性化目标增强框架的创意名称生成",
        "label": [],
        "label_reason": "论文聚焦创意文本生成，与推荐系统无直接关联。",
        "relevance_score": 3,
        "novelty_score": 7,
        "novelty_reason": "提出多智能体框架优化个性化生成，方法具创新性。"
    },
    {
        "title": "Unveiling Inference Scaling for Difference-Aware User Modeling in LLM Personalization",
        "url": "http://arxiv.org/abs/2511.15389v1",
        "pub_date": "2025-11-19",
        "summary": "Large Language Models (LLMs) are increasingly integrated into users' daily lives, driving a growing demand for personalized outputs. Prior work has primarily leveraged a user's own history, often overlooking inter-user differences that are critical for effective personalization. While recent methods have attempted to model such differences, their feature extraction processes typically rely on fixed dimensions and quick, intuitive inference (System-1 thinking), limiting both the coverage and granularity of captured user differences. To address these limitations, we propose Difference-aware Reasoning Personalization (DRP), a framework that reconstructs the difference extraction mechanism by leveraging inference scaling to enhance LLM personalization. DRP autonomously identifies relevant difference feature dimensions and generates structured definitions and descriptions, enabling slow, deliberate reasoning (System-2 thinking) over user differences. Experiments on personalized review generation demonstrate that DRP consistently outperforms baseline methods across multiple metrics.",
        "translated": "大语言模型（LLMs）正日益融入用户的日常生活，推动了对个性化输出日益增长的需求。以往的研究主要依赖于用户自身的交互历史，往往忽视了对有效个性化至关重要的用户间差异。尽管近期方法尝试建模此类差异，但其特征提取过程通常依赖固定维度和快速、直观的推理（System-1 thinking），从而限制了所捕捉用户差异的覆盖范围与颗粒度。为解决上述局限，我们提出差异感知推理个性化（Difference-aware Reasoning Personalization, DRP）框架，通过利用推理扩展重构差异提取机制，以增强LLM的个性化能力。DRP可自主识别相关差异特征维度，并生成结构化的定义与描述，支持对用户差异进行缓慢、审慎的推理（System-2 thinking）。在个性化评论生成任务上的实验表明，DRP在多个评估指标上均显著优于基线方法。",
        "translated_title": "揭示大语言模型个性化中差异感知用户建模的推理扩展机制",
        "label": [
            "LLM生成式推荐",
            "通用推荐技术"
        ],
        "label_reason": "聚焦LLM个性化，改进用户建模用于推荐输出生成",
        "relevance_score": 9,
        "novelty_score": 8,
        "novelty_reason": "创新推理机制提升用户差异建模精度与结构化"
    },
    {
        "title": "A Compliance-Preserving Retrieval System for Aircraft MRO Task Search",
        "url": "http://arxiv.org/abs/2511.15383v1",
        "pub_date": "2025-11-19",
        "summary": "Aircraft Maintenance Technicians (AMTs) spend up to 30% of work time searching manuals, a documented efficiency bottleneck in MRO operations where every procedure must be traceable to certified sources. We present a compliance-preserving retrieval system that adapts LLM reranking and semantic search to aviation MRO environments by operating alongside, rather than replacing, certified legacy viewers. The system constructs revision-robust embeddings from ATA chapter hierarchies and uses vision-language parsing to structure certified content, allowing technicians to preview ranked tasks and access verified procedures in existing viewers. Evaluation on 49k synthetic queries achieves &gt;90% retrieval accuracy, while bilingual controlled studies with 10 licensed AMTs demonstrate 90.9% top-10 success rate and 95% reduction in lookup time, from 6-15 minutes to 18 seconds per task. These gains provide concrete evidence that semantic retrieval can operate within strict regulatory constraints and meaningfully reduce operational workload in real-world multilingual MRO workflows.",
        "translated": "航空维修技术人员（AMTs）将高达30%的工作时间用于查阅手册，这是MRO作业中已证实的效率瓶颈，因为所有操作流程必须可追溯至认证来源。我们提出了一种合规性保持的检索系统，通过与经认证的传统查看器协同运行而非替代其功能，将大语言模型重排与语义搜索适配于航空MRO环境。该系统基于ATA章节层级构建抗修订的嵌入表示，并利用视觉-语言解析对认证内容进行结构化处理，使技术人员能够预览排序后的任务并直接在现有查看器中访问验证过的操作规程。在4.9万个合成查询上的评估实现了超过90%的召回准确率；而针对10名持证AMT开展的双语对照实验表明，其前10名命中成功率高达90.9%，单项任务查找时间从6–15分钟缩短至18秒，降幅达95%。这些成果为语义检索在严格监管约束下可行提供了实证依据，并切实减轻了现实世界多语言MRO工作流中的操作负荷。",
        "translated_title": "符合性保持的航空MRO任务检索系统",
        "label": [
            "重排",
            "LLM生成式推荐"
        ],
        "label_reason": "利用LLM重排与语义检索优化航空维修任务搜索",
        "relevance_score": 6,
        "novelty_score": 7,
        "novelty_reason": "结合LLM与合规约束的检索架构具创新性"
    },
    {
        "title": "Opinion Dynamics Models for Sentiment Evolution in Weibo Blogs",
        "url": "http://arxiv.org/abs/2511.15303v1",
        "pub_date": "2025-11-19",
        "summary": "Online social media platforms enable influencers to distribute content and quickly capture audience reactions, significantly shaping their promotional strategies and advertising agreements. Understanding how sentiment dynamics and emotional contagion unfold among followers is vital for influencers and marketers, as these processes shape engagement, brand perception, and purchasing behavior. While sentiment analysis tools effectively track sentiment fluctuations, dynamical models explaining their evolution remain limited, often neglecting network structures and interactions both among blogs and between their topic-focused follower groups. In this study, we tracked influential tech-focused Weibo bloggers over six months, quantifying follower sentiment from text-mined feedback. By treating each blogger's audience as a single \"macro-agent\", we find that sentiment trajectories follow the principle of iterative averaging -- a foundational mechanism in many dynamical models of opinion formation, a theoretical framework at the intersection of social network analysis and dynamical systems theory. The sentiment evolution aligns closely with opinion-dynamics models, particularly modified versions of the classical French-DeGroot model that incorporate delayed perception and distinguish between expressed and private opinions. The inferred influence structures reveal interdependencies among blogs that may arise from homophily, whereby emotionally similar users subscribe to the same blogs and collectively shape the shared sentiment expressed within these communities.",
        "translated": "在线社交媒体平台使意见领袖能够分发内容并迅速捕捉受众反应，显著影响其推广策略和广告合作。理解追随者群体中情感动态与情绪传染的演变过程，对意见领袖和营销人员至关重要，因为这些过程塑造用户参与度、品牌认知及购买行为。尽管情感分析工具可有效追踪情感波动，但解释其演化的动力学模型仍较为有限，常忽略博客间以及以主题为中心的粉丝群体间的网络结构与互动关系。在本研究中，我们对若干聚焦科技领域的微博意见领袖进行了为期六个月的跟踪，通过文本挖掘其粉丝反馈，量化了粉丝情感变化。我们将每位意见领袖的受众视为一个“宏观代理”，发现其情感轨迹遵循迭代平均原则——这是众多意见形成动力学模型中的基础机制，属于社会网络分析与动力系统理论交叉领域的理论框架。所观察到的情感演化与意见动力学模型高度契合，特别是引入延迟感知机制并区分公开意见与私人意见的改进型法国-德格鲁特经典模型。推断出的影响结构揭示了博客之间可能因同质性而产生的相互依赖关系：情感相似的用户倾向于订阅相同博客，共同塑造社区内共有的集体情感倾向。",
        "translated_title": "微博博文中的情感演化意见动力学模型",
        "label": [],
        "label_reason": "研究社交媒体情绪演化，非推荐系统核心问题",
        "relevance_score": 3,
        "novelty_score": 6,
        "novelty_reason": "改进意见动力学模型，但未应用于推荐场景"
    },
    {
        "title": "Selective Mixup for Debiasing Question Selection in Computerized Adaptive Testing",
        "url": "http://arxiv.org/abs/2511.15241v1",
        "pub_date": "2025-11-19",
        "summary": "Computerized Adaptive Testing (CAT) is a widely used technology for evaluating learners' proficiency in online education platforms. By leveraging prior estimates of proficiency to select questions and updating the estimates iteratively based on responses, CAT enables personalized learner modeling and has attracted substantial attention. Despite this progress, most existing works focus primarily on improving diagnostic accuracy, while overlooking the selection bias inherent in the adaptive process. Selection Bias arises because the question selection is strongly influenced by the estimated proficiency, such as assigning easier questions to learners with lower proficiency and harder ones to learners with higher proficiency. Since the selection depends on prior estimation, this bias propagates into the diagnosis model, which is further amplified during iterative updates, leading to misalignment and biased predictions. Moreover, the imbalanced nature of learners' historical interactions often exacerbates the bias in diagnosis models. To address this issue, we propose a debiasing framework consisting of two key modules: Cross-Attribute Examinee Retrieval and Selective Mixup-based Regularization. First, we retrieve balanced examinees with relatively even distributions of correct and incorrect responses and use them as neutral references for biased examinees. Then, mixup is applied between each biased examinee and its matched balanced counterpart under label consistency. This augmentation enriches the diversity of bias-conflicting samples and smooths selection boundaries. Finally, extensive experiments on two benchmark datasets with multiple advanced diagnosis models demonstrate that our method substantially improves both the generalization ability and fairness of question selection in CAT.",
        "translated": "计算机化自适应测试（CAT）是在线教育平台中广泛使用的评估学习者能力的技术。通过利用先前对学习者能力水平的估计来选择题目，并基于答题反馈迭代更新该估计，CAT实现了个性化学习者建模并受到广泛关注。尽管已有诸多进展，大多数现有工作主要聚焦于提高诊断准确性，却忽视了自适应过程中固有的选择偏差问题。选择偏差源于题目选择强烈依赖于能力估计：例如，为能力较低的学习者分配较简单题目，而为能力较高者分配较难题目。由于题目选择依赖于先前估计，该偏差会传递至诊断模型，并在迭代更新过程中进一步放大，导致预测结果出现错位与偏倚。此外，学习者历史交互行为往往分布不平衡，这进一步加剧了诊断模型中的偏差问题。为解决此问题，我们提出一种去偏框架，包含两个关键模块：跨属性考生检索与基于选择性Mixup的正则化。首先，我们检索出正确与错误回答分布相对均衡的平衡考生，并将其作为偏倚考生的中性参照样本；随后，在标签一致性约束下，将每个偏倚考生与其匹配的平衡考生进行Mixup操作。该数据增强策略丰富了偏倚冲突样本的多样性，并平滑了题目的选择边界。最后，在两个基准数据集上针对多种先进诊断模型的大量实验表明，我们的方法显著提升了CAT中题目选择的泛化能力和公平性。",
        "translated_title": "用于计算机化自适应测试中题目选择去偏的Selective Mixup方法",
        "label": [],
        "label_reason": "论文聚焦教育测评，非推荐系统相关",
        "relevance_score": 3,
        "novelty_score": 6,
        "novelty_reason": "方法针对测试偏差，无推荐系统创新"
    },
    {
        "title": "ItemRAG: Item-Based Retrieval-Augmented Generation for LLM-Based Recommendation",
        "url": "http://arxiv.org/abs/2511.15141v1",
        "pub_date": "2025-11-19",
        "summary": "Recently, large language models (LLMs) have been widely used as recommender systems, owing to their strong reasoning capability and their effectiveness in handling cold-start items. To better adapt LLMs for recommendation, retrieval-augmented generation (RAG) has been incorporated. Most existing RAG methods are user-based, retrieving purchase patterns of users similar to the target user and providing them to the LLM. In this work, we propose ItemRAG, an item-based RAG method for LLM-based recommendation that retrieves relevant items (rather than users) from item-item co-purchase histories. ItemRAG helps LLMs capture co-purchase patterns among items, which are beneficial for recommendations. Especially, our retrieval strategy incorporates semantically similar items to better handle cold-start items and uses co-purchase frequencies to improve the relevance of the retrieved items. Through extensive experiments, we demonstrate that ItemRAG consistently (1) improves the zero-shot LLM-based recommender by up to 43% in Hit-Ratio-1 and (2) outperforms user-based RAG baselines under both standard and cold-start item recommendation settings.",
        "translated": "近年来，大语言模型（LLM）因其强大的推理能力及处理冷启动物料的有效性，被广泛应用于推荐系统。为更好地适配LLM进行推荐，研究者引入了检索增强生成（RAG）方法。现有大多数RAG方法均基于用户，通过检索与目标用户相似的用户购买模式，并将其提供给LLM。在本文中，我们提出ItemRAG，这是一种面向LLM推荐的基于物料的RAG方法，其从物料-物料共购历史中检索相关物料（而非用户）。ItemRAG有助于LLM捕捉物料间的共购模式，从而提升推荐效果。特别地，我们的检索策略结合语义相似物料以更有效地处理冷启动物料，并利用共购频次提升所检索物料的相关性。通过大量实验，我们证明ItemRAG始终能够：（1）将零样本LLM推荐系统的Hit-Ratio-1提升最高达43%；（2）在标准和冷启动物料推荐设置下均优于基于用户的RAG基线方法。",
        "translated_title": "ItemRAG：基于物品的检索增强生成推荐方法（面向大语言模型）",
        "label": [
            "LLM生成式推荐",
            "召回"
        ],
        "label_reason": "直接提出基于物品的RAG方法用于LLM推荐",
        "relevance_score": 10,
        "novelty_score": 9,
        "novelty_reason": "创新性地将RAG应用于物品召回，显著提升冷启动效果"
    },
    {
        "title": "Multi-Aspect Cross-modal Quantization for Generative Recommendation",
        "url": "http://arxiv.org/abs/2511.15122v1",
        "pub_date": "2025-11-19",
        "summary": "Generative Recommendation (GR) has emerged as a new paradigm in recommender systems. This approach relies on quantized representations to discretize item features, modeling users' historical interactions as sequences of discrete tokens. Based on these tokenized sequences, GR predicts the next item by employing next-token prediction methods. The challenges of GR lie in constructing high-quality semantic identifiers (IDs) that are hierarchically organized, minimally conflicting, and conducive to effective generative model training. However, current approaches remain limited in their ability to harness multimodal information and to capture the deep and intricate interactions among diverse modalities, both of which are essential for learning high-quality semantic IDs and for effectively training GR models. To address this, we propose Multi-Aspect Cross-modal quantization for generative Recommendation (MACRec), which introduces multimodal information and incorporates it into both semantic ID learning and generative model training from different aspects. Specifically, we first introduce cross-modal quantization during the ID learning process, which effectively reduces conflict rates and thus improves codebook usability through the complementary integration of multimodal information. In addition, to further enhance the generative ability of our GR model, we incorporate multi-aspect cross-modal alignments, including the implicit and explicit alignments. Finally, we conduct extensive experiments on three well-known recommendation datasets to demonstrate the effectiveness of our proposed method.",
        "translated": "生成式推荐（GR）已成为推荐系统领域的一种新范式。该方法依赖量化表示，将物料特征离散化，并将用户的历史交互建模为离散token序列。基于这些token化序列，GR通过下一token预测方法预测下一个物料。GR面临的核心挑战在于构建高质量的语义标识符（IDs），这些ID需具备层次结构、冲突最小化，且有利于生成模型的有效训练。然而，当前方法在利用多模态信息以及捕捉不同模态间深层次、复杂的交互关系方面仍显不足，而这两者对于学习高质量语义ID及有效训练GR模型至关重要。为此，我们提出了一种面向生成式推荐的多方面跨模态量化方法（MACRec），从多个角度引入多模态信息并将其融入语义ID学习与生成模型训练之中。具体而言，我们在ID学习阶段引入跨模态量化机制，通过融合多模态信息实现互补，有效降低冲突率，从而提升码本的可用性。此外，为进一步增强GR模型的生成能力，我们引入了多方面的跨模态对齐机制，包括隐式对齐与显式对齐。最后，我们在三个知名推荐数据集上进行了广泛的实验，以验证所提方法的有效性。",
        "translated_title": "多方面跨模态量化用于生成式推荐",
        "label": [
            "LLM生成式推荐",
            "多模态推荐"
        ],
        "label_reason": "直接解决生成式推荐中多模态语义ID构建问题",
        "relevance_score": 10,
        "novelty_score": 9,
        "novelty_reason": "首创跨模态量化与对齐机制用于生成式推荐"
    },
    {
        "title": "Beyond GeneGPT: A Multi-Agent Architecture with Open-Source LLMs for Enhanced Genomic Question Answering",
        "url": "http://arxiv.org/abs/2511.15061v1",
        "pub_date": "2025-11-19",
        "summary": "Genomic question answering often requires complex reasoning and integration across diverse biomedical sources. GeneGPT addressed this challenge by combining domain-specific APIs with OpenAI's code-davinci-002 large language model to enable natural language interaction with genomic databases. However, its reliance on a proprietary model limits scalability, increases operational costs, and raises concerns about data privacy and generalization.   In this work, we revisit and reproduce GeneGPT in a pilot study using open source models, including Llama 3.1, Qwen2.5, and Qwen2.5 Coder, within a monolithic architecture; this allows us to identify the limitations of this approach. Building on this foundation, we then develop OpenBioLLM, a modular multi-agent framework that extends GeneGPT by introducing agent specialization for tool routing, query generation, and response validation. This enables coordinated reasoning and role-based task execution.   OpenBioLLM matches or outperforms GeneGPT on over 90% of the benchmark tasks, achieving average scores of 0.849 on Gene-Turing and 0.830 on GeneHop, while using smaller open-source models without additional fine-tuning or tool-specific pretraining. OpenBioLLM's modular multi-agent design reduces latency by 40-50% across benchmark tasks, significantly improving efficiency without compromising model capability. The results of our comprehensive evaluation highlight the potential of open-source multi-agent systems for genomic question answering. Code and resources are available at https://github.com/ielab/OpenBioLLM.",
        "translated": "基因组问答通常需要跨多样化的生物医学数据源进行复杂推理与整合。GeneGPT 通过结合领域专用 API 与 OpenAI 的 code-davinci-002 大语言模型，实现了与基因组数据库的自然语言交互，从而应对这一挑战。然而，其对专有模型的依赖限制了可扩展性，增加了运营成本，并引发了关于数据隐私和泛化能力的担忧。\n\n在本研究中，我们通过使用开源模型（包括 Llama 3.1、Qwen2.5 和 Qwen2.5 Coder）在一个单体架构下重新实现并复现了 GeneGPT，以此识别该方法的局限性。在此基础上，我们进一步构建了 OpenBioLLM——一个模块化的多智能体框架，通过引入工具路由、查询生成与响应验证等智能体专业化机制，拓展了 GeneGPT 的能力。该设计支持协同推理与基于角色的任务执行。\n\nOpenBioLLM 在超过 90% 的基准任务上表现匹配或优于 GeneGPT，在 Gene-Turing 任务上的平均得分达 0.849，在 GeneHop 上达 0.830；且无需额外微调或针对特定工具的预训练，即以更小的开源模型实现上述性能。OpenBioLLM 的模块化多智能体架构显著降低了基准任务中的延迟 40–50%，大幅提升了效率，同时未牺牲模型能力。综合评估结果凸显了开源多智能体系统在基因组问答领域的应用潜力。代码与资源详见 https://github.com/ielab/OpenBioLLM。",
        "translated_title": "超越GeneGPT：一种基于开源大语言模型的多智能体架构，用于增强基因组学问答能力",
        "label": [],
        "label_reason": "论文聚焦基因问答，非推荐系统相关",
        "relevance_score": 3,
        "novelty_score": 7,
        "novelty_reason": "多智能体架构创新，但领域非推荐"
    },
    {
        "title": "SilverTorch: A Unified Model-based System to Democratize Large-Scale Recommendation on GPUs",
        "url": "http://arxiv.org/abs/2511.14881v1",
        "pub_date": "2025-11-18",
        "summary": "Serving deep learning based recommendation models (DLRM) at scale is challenging. Existing systems rely on CPU-based ANN indexing and filtering services, suffering from non-negligible costs and forgoing joint optimization opportunities. Such inefficiency makes them difficult to support more complex model architectures, such as learned similarities and multi-task retrieval.   In this paper, we propose SilverTorch, a model-based system for serving recommendation models on GPUs. SilverTorch unifies model serving by replacing standalone indexing and filtering services with layers of served models. We propose a Bloom index algorithm on GPUs for feature filtering and a tensor-native fused Int8 ANN kernel on GPUs for nearest neighbor search. We further co-design the ANN search index and filtering index to reduce GPU memory utilization and eliminate unnecessary computation. Benefit from SilverTorch's serving paradigm, we introduce a OverArch scoring layer and a Value Model to aggregate results across multi-tasks. These advancements improve the accuracy for retrieval and enable future studies for serving more complex models. For ranking, SilverTorch's design accelerates item embedding calculation by caching the pre-calculated embeddings inside the serving model.   Our evaluation on the industry-scale datasets show that SilverTorch achieves up to 5.6x lower latency and 23.7x higher throughput compared to the state-of-the-art approaches. We also demonstrate that SilverTorch's solution is 13.35x more cost-efficient than CPU-based solution while improving accuracy via serving more complex models. SilverTorch serves over hundreds of models online across major products and recommends contents for billions of daily active users.",
        "translated": "在大规模服务基于深度学习的推荐模型（DLRM）时面临严峻挑战。现有系统依赖于基于CPU的ANN索引与过滤服务，存在不可忽视的成本开销，并错失联合优化的机会。这种低效性使其难以支持更复杂的模型架构，如学习到的相似度和多任务检索。\n\n本文提出SilverTorch，一种面向GPU的模型驱动型推荐模型服务系统。SilverTorch通过以服务模型层取代独立的索引与过滤服务，统一了模型服务流程。我们提出了一种基于GPU的Bloom索引算法用于特征过滤，并设计了一种针对GPU的张量原生融合Int8 ANN内核用于最近邻搜索。此外，我们进一步协同设计ANN搜索索引与过滤索引，以降低GPU内存占用并消除不必要的计算。\n\n得益于SilverTorch的服务范式，我们引入了OverArch评分层与价值模型，以跨多任务聚合结果。这些改进提升了检索准确率，并为未来服务更复杂模型的研究奠定了基础。对于排序环节，SilverTorch的设计通过将预计算的物料嵌入缓存至服务模型内部，加速了物料嵌入的计算过程。\n\n我们在业界规模数据集上的评估表明，SilverTorch相较当前最优方法可实现高达5.6倍的延迟降低和23.7倍的吞吐量提升。同时，我们证明SilverTorch方案相较基于CPU的方案成本效率高出13.35倍，且通过服务更复杂模型提升了准确性。目前，SilverTorch已在线服务于数百个模型，覆盖主要产品线，每日向数十亿活跃用户提供内容推荐。",
        "translated_title": "SilverTorch：一种统一的基于模型的系统，用于在GPU上普及大规模推荐",
        "label": [
            "召回",
            "精排",
            "通用推荐技术"
        ],
        "label_reason": "聚焦推荐系统服务架构优化，提升召回与排序效率。",
        "relevance_score": 9,
        "novelty_score": 8,
        "novelty_reason": "创新GPU统一模型服务架构，高效支持复杂推荐模型。"
    },
    {
        "title": "RoMa v2: Harder Better Faster Denser Feature Matching",
        "url": "http://arxiv.org/abs/2511.15706v1",
        "pub_date": "2025-11-19",
        "summary": "Dense feature matching aims to estimate all correspondences between two images of a 3D scene and has recently been established as the gold-standard due to its high accuracy and robustness. However, existing dense matchers still fail or perform poorly for many hard real-world scenarios, and high-precision models are often slow, limiting their applicability. In this paper, we attack these weaknesses on a wide front through a series of systematic improvements that together yield a significantly better model. In particular, we construct a novel matching architecture and loss, which, combined with a curated diverse training distribution, enables our model to solve many complex matching tasks. We further make training faster through a decoupled two-stage matching-then-refinement pipeline, and at the same time, significantly reduce refinement memory usage through a custom CUDA kernel. Finally, we leverage the recent DINOv3 foundation model along with multiple other insights to make the model more robust and unbiased. In our extensive set of experiments we show that the resulting novel matcher sets a new state-of-the-art, being significantly more accurate than its predecessors. Code is available at https://github.com/Parskatt/romav2",
        "translated": "稠密特征匹配旨在估计三维场景两幅图像之间的所有对应关系，因其高精度和鲁棒性，近年来已被确立为业界标准。然而，现有稠密匹配器在许多困难的真实世界场景中仍会失败或表现不佳；同时，高精度模型通常计算速度较慢，限制了其实际应用。本文通过一系列系统性的改进，在多个方面针对上述缺陷展开攻关，最终构建了一个性能显著提升的新型模型。具体而言，我们设计了一种新颖的匹配架构与损失函数，并结合精心策划的多样化训练分布，使模型能够有效解决多种复杂的匹配任务。此外，我们通过解耦的“两阶段”匹配-精修流程加速训练过程，并借助自定义CUDA内核大幅降低精修阶段的内存消耗。最后，我们融合近期发布的DINOv3基础模型及其他多项技术洞察，进一步提升了模型的鲁棒性与无偏性。在大量实验验证中，我们证明所提出的新型匹配器取得了当前最先进的性能，显著优于其前代方法。代码已开源于 https://github.com/Parskatt/romav2",
        "translated_title": "RoMa v2：更难、更好、更快、更密集的特征匹配",
        "label": [],
        "label_reason": "任务为稠密匹配，属高阶视觉场景理解",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "改进架构与训练策略，显著提升精度与速度"
    },
    {
        "title": "GeoVista: Web-Augmented Agentic Visual Reasoning for Geolocalization",
        "url": "http://arxiv.org/abs/2511.15705v1",
        "pub_date": "2025-11-19",
        "summary": "Current research on agentic visual reasoning enables deep multimodal understanding but primarily focuses on image manipulation tools, leaving a gap toward more general-purpose agentic models. In this work, we revisit the geolocalization task, which requires not only nuanced visual grounding but also web search to confirm or refine hypotheses during reasoning. Since existing geolocalization benchmarks fail to meet the need for high-resolution imagery and the localization challenge for deep agentic reasoning, we curate GeoBench, a benchmark that includes photos and panoramas from around the world, along with a subset of satellite images of different cities to rigorously evaluate the geolocalization ability of agentic models. We also propose GeoVista, an agentic model that seamlessly integrates tool invocation within the reasoning loop, including an image-zoom-in tool to magnify regions of interest and a web-search tool to retrieve related web information. We develop a complete training pipeline for it, including a cold-start supervised fine-tuning (SFT) stage to learn reasoning patterns and tool-use priors, followed by a reinforcement learning (RL) stage to further enhance reasoning ability. We adopt a hierarchical reward to leverage multi-level geographical information and improve overall geolocalization performance. Experimental results show that GeoVista surpasses other open-source agentic models on the geolocalization task greatly and achieves performance comparable to closed-source models such as Gemini-2.5-flash and GPT-5 on most metrics.",
        "translated": "当前针对代理视觉推理的研究虽能实现深层次多模态理解，但主要聚焦于图像编辑工具，尚未覆盖更具通用性的代理模型需求。在本研究中，我们重新审视地理定位任务——该任务不仅要求精细的视觉锚定，还需借助网络搜索以验证或优化推理过程中的假设。由于现有地理定位基准数据集无法满足高分辨率图像的需求，亦难以应对深度代理推理所面临的定位挑战，我们构建了GeoBench基准数据集，其中包含全球各地的摄影作品与全景图，并辅以若干城市不同分辨率的卫星影像，旨在严格评估代理模型的地理定位能力。同时，我们提出了GeoVista这一代理模型，其在推理循环中无缝集成工具调用功能，包括图像放大工具（用于放大感兴趣区域）与网络搜索工具（用于检索相关网页信息）。我们为其构建了一套完整的训练流程：首先通过冷启动监督微调（SFT）阶段学习推理模式及工具使用先验知识；随后进入强化学习（RL）阶段，进一步提升推理能力。我们采用分层奖励机制，以融合多层次地理信息并全面提升整体地理定位性能。实验结果表明，GeoVista在地理定位任务上显著超越其他开源代理模型，在多数评测指标上表现可比闭源模型（如Gemini-2.5-flash和GPT-5）。",
        "translated_title": "GeoVista：基于网页增强的代理视觉推理用于地理定位",
        "label": [],
        "label_reason": "任务为高阶地理定位与工具推理，非像素级图像处理",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "提出新工具集成与分层奖励机制提升推理能力"
    },
    {
        "title": "In-N-On: Scaling Egocentric Manipulation with in-the-wild and on-task Data",
        "url": "http://arxiv.org/abs/2511.15704v1",
        "pub_date": "2025-11-19",
        "summary": "Egocentric videos are a valuable and scalable data source to learn manipulation policies. However, due to significant data heterogeneity, most existing approaches utilize human data for simple pre-training, which does not unlock its full potential. This paper first provides a scalable recipe for collecting and using egocentric data by categorizing human data into two categories: in-the-wild and on-task alongside with systematic analysis on how to use the data. We first curate a dataset, PHSD, which contains over 1,000 hours of diverse in-the-wild egocentric data and over 20 hours of on-task data directly aligned to the target manipulation tasks. This enables learning a large egocentric language-conditioned flow matching policy, Human0. With domain adaptation techniques, Human0 minimizes the gap between humans and humanoids. Empirically, we show Human0 achieves several novel properties from scaling human data, including language following of instructions from only human data, few-shot learning, and improved robustness using on-task data. Project website: https://xiongyicai.github.io/In-N-On/",
        "translated": "以自我为中心的视频是一种学习操作策略的宝贵且可扩展的数据来源。然而，由于数据异质性显著，现有大多数方法仅利用人类数据进行简单的预训练，未能充分释放其潜力。本文首先提供了一种可扩展的方法，用于收集和使用以自我为中心的数据：将人类数据划分为两类——野外采集数据（in-the-wild）与任务相关数据（on-task），并系统分析如何有效利用这些数据。我们首先构建了一个名为 PHSD 的数据集，其中包含超过 1,000 小时多样化的野外采集的以自我为中心视频，以及超过 20 小时直接对齐目标操作任务的任务相关数据。这使得能够学习一个大规模的以自我为中心、语言条件驱动的流匹配策略 Human0。借助领域自适应技术，Human0 最小化了人类与人形机器人之间的性能差距。实验证明，通过扩展人类数据规模，Human0 达到了若干新颖特性，包括仅基于人类数据即可遵循指令、少量样本学习能力，以及借助任务相关数据提升鲁棒性。项目网站：https://xiongyicai.github.io/In-N-On/",
        "translated_title": "In-N-On：利用真实场景与任务相关数据扩展第一人称操作的规模",
        "label": [],
        "label_reason": "任务为视频动作学习，非图像像素级处理",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出数据分类与域适应方法，提升策略泛化能力"
    },
    {
        "title": "Think Visually, Reason Textually: Vision-Language Synergy in ARC",
        "url": "http://arxiv.org/abs/2511.15703v1",
        "pub_date": "2025-11-19",
        "summary": "Abstract reasoning from minimal examples remains a core unsolved problem for frontier foundation models such as GPT-5 and Grok 4. These models still fail to infer structured transformation rules from a handful of examples, which is a key hallmark of human intelligence. The Abstraction and Reasoning Corpus for Artificial General Intelligence (ARC-AGI) provides a rigorous testbed for this capability, demanding conceptual rule induction and transfer to novel tasks. Most existing methods treat ARC-AGI as a purely textual reasoning task, overlooking the fact that humans rely heavily on visual abstraction when solving such puzzles. However, our pilot experiments reveal a paradox: naively rendering ARC-AGI grids as images degrades performance due to imprecise rule execution. This leads to our central hypothesis that vision and language possess complementary strengths across distinct reasoning stages: vision supports global pattern abstraction and verification, whereas language specializes in symbolic rule formulation and precise execution. Building on this insight, we introduce two synergistic strategies: (1) Vision-Language Synergy Reasoning (VLSR), which decomposes ARC-AGI into modality-aligned subtasks; and (2) Modality-Switch Self-Correction (MSSC), which leverages vision to verify text-based reasoning for intrinsic error correction. Extensive experiments demonstrate that our approach yields up to a 4.33% improvement over text-only baselines across diverse flagship models and multiple ARC-AGI tasks. Our findings suggest that unifying visual abstraction with linguistic reasoning is a crucial step toward achieving generalizable, human-like intelligence in future foundation models. Source code will be released soon.",
        "translated": "从最小实例进行抽象推理仍是前沿基础模型（如 GPT-5 和 Grok 4）的核心未解问题。这些模型仍无法从少量示例中推断出结构化的变换规则，而这一能力是人类智能的关键标志。面向人工通用智能的抽象与推理语料库（ARC-AGI）为此类能力提供了一个严谨的评测平台，要求模型从示例中归纳概念性规则，并将其迁移到新任务中。现有大多数方法将 ARC-AGI 视为纯粹的文本推理任务，忽视了人类在解决此类谜题时高度依赖视觉抽象的事实。然而，我们的初步实验揭示了一个悖论：若将 ARC-AGI 栅格直接渲染为图像，则因规则执行不精确而导致性能下降。这促使我们提出核心假设：视觉与语言在不同推理阶段具有互补优势——视觉支持全局模式的抽象与验证，而语言则擅长符号化规则的构建与精确执行。基于该洞察，我们提出了两种协同策略：(1) 视觉-语言协同推理（VLSR），将 ARC-AGI 分解为模态对齐的子任务；以及 (2) 模态切换自校正（MSSC），利用视觉验证基于文本的推理过程以实现内在错误修正。大量实验证明，我们的方法在多种旗舰模型及多个 ARC-AGI 任务上相较纯文本基线最高提升达 4.33%。我们的研究结果表明，将视觉抽象与语言推理统一起来，是未来基础模型实现可迁移、类人智能的关键一步。源代码即将开源。",
        "translated_title": "视觉思考，文本推理：ARC中的视觉-语言协同作用",
        "label": [],
        "label_reason": "任务为文本推理，非图像像素级处理",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "提出视觉-语言协同推理新范式"
    },
    {
        "title": "First Frame Is the Place to Go for Video Content Customization",
        "url": "http://arxiv.org/abs/2511.15700v1",
        "pub_date": "2025-11-19",
        "summary": "What role does the first frame play in video generation models? Traditionally, it's viewed as the spatial-temporal starting point of a video, merely a seed for subsequent animation. In this work, we reveal a fundamentally different perspective: video models implicitly treat the first frame as a conceptual memory buffer that stores visual entities for later reuse during generation. Leveraging this insight, we show that it's possible to achieve robust and generalized video content customization in diverse scenarios, using only 20-50 training examples without architectural changes or large-scale finetuning. This unveils a powerful, overlooked capability of video generation models for reference-based video customization.",
        "translated": "第一帧在视频生成模型中扮演着怎样的角色？传统上，它被视为视频的时空起点，仅作为后续动画生成的种子。在本工作中，我们揭示了一种根本不同的视角：视频模型隐式地将第一帧视为一个概念性记忆缓冲区，在生成过程中用于存储视觉实体以供后续复用。基于这一洞察，我们证明了仅使用20至50个训练样本，无需架构修改或大规模微调，即可在多种场景下实现稳健且泛化的视频内容定制。这揭示了视频生成模型在基于参考的视频定制任务中被忽视的强大能力。",
        "translated_title": "第一帧是视频内容定制的理想选择",
        "label": [],
        "label_reason": "视频内容定制属高阶生成任务，非像素级图像恢复。",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出第一帧作为记忆缓冲的创新视角，提升定制泛化能力。"
    },
    {
        "title": "Hyperspectral Image Classification using Spectral-Spatial Mixer Network",
        "url": "http://arxiv.org/abs/2511.15692v1",
        "pub_date": "2025-11-19",
        "summary": "This paper introduces SS-MixNet, a lightweight and effective deep learning model for hyperspectral image (HSI) classification. The architecture integrates 3D convolutional layers for local spectral-spatial feature extraction with two parallel MLP-style mixer blocks that capture long-range dependencies in spectral and spatial dimensions. A depthwise convolution-based attention mechanism is employed to enhance discriminative capability with minimal computational overhead. The model is evaluated on the QUH-Tangdaowan and QUH-Qingyun datasets using only 1% of labeled data for training and validation. SS-MixNet achieves the highest performance among compared methods, including 2D-CNN, 3D-CNN, IP-SWIN, SimPoolFormer, and HybridKAN, reaching 95.68% and 93.86% overall accuracy on the Tangdaowan and Qingyun datasets, respectively. The results, supported by quantitative metrics and classification maps, confirm the model's effectiveness in delivering accurate and robust predictions with limited supervision. The code will be made publicly available at: https://github.com/mqalkhatib/SS-MixNet",
        "translated": "本文提出了一种轻量且高效的深度学习模型 SS-MixNet，用于高光谱图像（HSI）分类。该架构结合了 3D 卷积层以提取局部光谱-空间特征，并引入两个并行的 MLP 风格混合模块，用于捕捉光谱与空间维度上的长程依赖关系。模型采用基于深度卷积的注意力机制，在保持极低计算开销的同时增强判别能力。实验在 QUH-Tangdaowan 和 QUH-Qingyun 数据集上进行，仅使用 1% 的标注数据进行训练与验证。SS-MixNet 在对比方法（包括 2D-CNN、3D-CNN、IP-SWIN、SimPoolFormer 和 HybridKAN）中表现最优，在 Tangdaowan 和 Qingyun 数据集上分别达到 95.68% 和 93.86% 的整体准确率。定量指标与分类图的结果证实，该模型在监督信息有限的情况下仍能实现准确且鲁棒的预测。代码将公开于：https://github.com/mqalkhatib/SS-MixNet",
        "translated_title": "基于光谱-空间混合网络的高光谱图像分类",
        "label": [],
        "label_reason": "任务为高阶分类，非像素级图像恢复",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "结构创新但用于分类，非图像恢复任务"
    },
    {
        "title": "MoDES: Accelerating Mixture-of-Experts Multimodal Large Language Models via Dynamic Expert Skipping",
        "url": "http://arxiv.org/abs/2511.15690v1",
        "pub_date": "2025-11-19",
        "summary": "Mixture-of-Experts (MoE) Multimodal large language models (MLLMs) excel at vision-language tasks, but they suffer from high computational inefficiency. To reduce inference overhead, expert skipping methods have been proposed to deactivate redundant experts based on the current input tokens. However, we find that applying these methods-originally designed for unimodal large language models (LLMs)-to MLLMs results in considerable performance degradation. This is primarily because such methods fail to account for the heterogeneous contributions of experts across MoE layers and modality-specific behaviors of tokens within these layers. Motivated by these findings, we propose MoDES, the first training-free framework that adaptively skips experts to enable efficient and accurate MoE MLLM inference. It incorporates a globally-modulated local gating (GMLG) mechanism that integrates global layer-wise importance into local routing probabilities to accurately estimate per-token expert importance. A dual-modality thresholding (DMT) method is then applied, which processes tokens from each modality separately, to derive the skipping schedule. To set the optimal thresholds, we introduce a frontier search algorithm that exploits monotonicity properties, cutting convergence time from several days to a few hours. Extensive experiments for 3 model series across 13 benchmarks demonstrate that MoDES far outperforms previous approaches. For instance, when skipping 88% experts for Qwen3-VL-MoE-30B-A3B-Instruct, the performance boost is up to 10.67% (97.33% vs. 86.66%). Furthermore, MoDES significantly enhances inference speed, improving the prefilling time by 2.16$\\times$ and the decoding time by 1.26$\\times$.",
        "translated": "混合专家（MoE）多模态大语言模型（MLLMs）在视觉-语言任务中表现优异，但其计算效率低下。为降低推理开销，已有研究提出专家跳过方法，根据当前输入 token 选择性禁用冗余专家。然而，我们发现将这些最初为单模态大语言模型（LLMs）设计的方法直接应用于 MLLMs，会导致显著的性能下降。这主要是因为此类方法未能充分考虑 MoE 层内各专家异质性的贡献，以及不同模态 token 在层内特有的行为模式。受此启发，我们提出了 MoDES —— 首个无需训练即可自适应跳过专家、实现高效准确推理的 MoE MLLM 框架。该框架引入全局调制局部门控（GMLG）机制，将全局层间重要性融入局部路由概率，从而精确估计每个 token 对应的专家重要性；随后采用双模态阈值法（DMT），对每种模态中的 token 分别处理，生成跳过方案。为设定最优阈值，我们引入前沿搜索算法，利用单调性特性，将收敛时间从数天缩短至数小时。在 13 项基准测试中针对 3 类模型系列进行的广泛实验表明，MoDES 远优于现有方法。例如，在 Qwen3-VL-MoE-30B-A3B-Instruct 中跳过 88% 的专家时，性能提升高达 10.67%（97.33% vs. 86.66%）。此外，MoDES 显著加速推理过程，预填充时间提升 2.16 倍，解码时间提升 1.26 倍。",
        "translated_title": "MoDES：通过动态专家跳过加速混合专家多模态大语言模型",
        "label": [],
        "label_reason": "属于高阶语言模型优化，非图像处理任务。",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "首次提出训练免专家跳过框架，显著提升推理效率。"
    },
    {
        "title": "MF-GCN: A Multi-Frequency Graph Convolutional Network for Tri-Modal Depression Detection Using Eye-Tracking, Facial, and Acoustic Features",
        "url": "http://arxiv.org/abs/2511.15675v1",
        "pub_date": "2025-11-19",
        "summary": "Eye tracking data quantifies the attentional bias towards negative stimuli that is frequently observed in depressed groups. Audio and video data capture the affective flattening and psychomotor retardation characteristic of depression. Statistical validation confirmed their significant discriminative power in distinguishing depressed from non depressed groups. We address a critical limitation of existing graph-based models that focus on low-frequency information and propose a Multi-Frequency Graph Convolutional Network (MF-GCN). This framework consists of a novel Multi-Frequency Filter Bank Module (MFFBM), which can leverage both low and high frequency signals. Extensive evaluation against traditional machine learning algorithms and deep learning frameworks demonstrates that MF-GCN consistently outperforms baselines. In binary (depressed and non depressed) classification, the model achieved a sensitivity of 0.96 and F2 score of 0.94. For the 3 class (no depression, mild to moderate depression and severe depression) classification task, the proposed method achieved a sensitivity of 0.79 and specificity of 0.87 and siginificantly suprassed other models. To validate generalizability, the model was also evaluated on the Chinese Multimodal Depression Corpus (CMDC) dataset and achieved a sensitivity of 0.95 and F2 score of 0.96. These results confirm that our trimodal, multi frequency framework effectively captures cross modal interaction for accurate depression detection.",
        "translated": "眼动追踪数据量化了抑郁群体中常观察到的对负性刺激的注意偏向。音频与视频数据记录了抑郁症患者特有的情感淡漠和运动迟滞特征。统计验证证实，这些数据在区分抑郁与非抑郁群体时具有显著的鉴别能力。我们针对现有基于图结构模型主要关注低频信息的关键局限，提出了一种多频谱图卷积网络（MF-GCN）。该框架包含一种新颖的多频谱滤波器组模块（MFFBM），能够同时利用低频和高频信号。与传统机器学习算法及深度学习框架的广泛对比实验表明，MF-GCN 在各项指标上均持续优于基线方法。在二分类任务（抑郁 vs. 非抑郁）中，模型达到 0.96 的敏感度与 0.94 的 F2 分数；在三分类任务（无抑郁、轻至中度抑郁、重度抑郁）中，所提方法实现 0.79 的敏感度与 0.87 的特异度，并显著超越其他模型。为验证其泛化能力，模型亦在中文多模态抑郁语料库（CMDC）数据集上进行评估，获得 0.95 的敏感度与 0.96 的 F2 分数。上述结果证实，我们的三模态、多频谱框架能有效捕捉跨模态交互，从而实现精准的抑郁检测。",
        "translated_title": "MF-GCN：一种基于多频图卷积网络的眼动、面部与声学三模态抑郁检测方法",
        "label": [],
        "label_reason": "任务为高阶抑郁分类，非图像像素级处理",
        "relevance_score": 1,
        "novelty_score": 3,
        "novelty_reason": "框架改进多模态处理，无图像恢复创新"
    },
    {
        "title": "VisPlay: Self-Evolving Vision-Language Models from Images",
        "url": "http://arxiv.org/abs/2511.15661v1",
        "pub_date": "2025-11-19",
        "summary": "Reinforcement learning (RL) provides a principled framework for improving Vision-Language Models (VLMs) on complex reasoning tasks. However, existing RL approaches often rely on human-annotated labels or task-specific heuristics to define verifiable rewards, both of which are costly and difficult to scale. We introduce VisPlay, a self-evolving RL framework that enables VLMs to autonomously improve their reasoning abilities using large amounts of unlabeled image data. Starting from a single base VLM, VisPlay assigns the model into two interacting roles: an Image-Conditioned Questioner that formulates challenging yet answerable visual questions, and a Multimodal Reasoner that generates silver responses. These roles are jointly trained with Group Relative Policy Optimization (GRPO), which incorporates diversity and difficulty rewards to balance the complexity of generated questions with the quality of the silver answers. VisPlay scales efficiently across two model families. When trained on Qwen2.5-VL and MiMo-VL, VisPlay achieves consistent improvements in visual reasoning, compositional generalization, and hallucination reduction across eight benchmarks, including MM-Vet and MMMU, demonstrating a scalable path toward self-evolving multimodal intelligence. The project page is available at https://bruno686.github.io/VisPlay/",
        "translated": "强化学习（RL）为提升视觉-语言模型（VLMs）在复杂推理任务上的表现提供了一个原则性的框架。然而，现有的 RL 方法通常依赖人工标注标签或任务特定启发式规则来定义可验证的奖励，这两种方式均成本高昂且难以扩展。我们提出了 VisPlay，一种自演进的 RL 框架，使 VLMs 能够利用大量无标注图像数据自主提升其推理能力。从单个基础 VLM 出发，VisPlay 将模型分配为两个相互作用的角色：一个图像条件化提问者（Image-Conditioned Questioner），负责生成具有挑战性但可回答的视觉问题；以及一个多模态推理者（Multimodal Reasoner），负责生成银色响应（silver responses）。这两个角色通过组相对策略优化（Group Relative Policy Optimization, GRPO）联合训练，GRPO 通过引入多样性和难度奖励，在所生成问题的复杂度与银色答案质量之间实现平衡。VisPlay 在两个模型家族中均能高效扩展。当在 Qwen2.5-VL 和 MiMo-VL 上训练时，VisPlay 在包括 MM-Vet 和 MMMU 在内的八个基准测试中，持续提升了视觉推理能力、组合泛化能力和幻觉减少效果，展示了迈向自演进多模态智能的可扩展路径。项目页面见 https://bruno686.github.io/VisPlay/",
        "translated_title": "VisPlay：从图像中自演化的视觉-语言模型",
        "label": [],
        "label_reason": "属高阶视觉任务，非像素级图像处理",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "提出自进化框架，显著提升VLM推理能力"
    },
    {
        "title": "GEO-Bench-2: From Performance to Capability, Rethinking Evaluation in Geospatial AI",
        "url": "http://arxiv.org/abs/2511.15658v1",
        "pub_date": "2025-11-19",
        "summary": "Geospatial Foundation Models (GeoFMs) are transforming Earth Observation (EO), but evaluation lacks standardized protocols. GEO-Bench-2 addresses this with a comprehensive framework spanning classification, segmentation, regression, object detection, and instance segmentation across 19 permissively-licensed datasets. We introduce ''capability'' groups to rank models on datasets that share common characteristics (e.g., resolution, bands, temporality). This enables users to identify which models excel in each capability and determine which areas need improvement in future work. To support both fair comparison and methodological innovation, we define a prescriptive yet flexible evaluation protocol. This not only ensures consistency in benchmarking but also facilitates research into model adaptation strategies, a key and open challenge in advancing GeoFMs for downstream tasks.   Our experiments show that no single model dominates across all tasks, confirming the specificity of the choices made during architecture design and pretraining. While models pretrained on natural images (ConvNext ImageNet, DINO V3) excel on high-resolution tasks, EO-specific models (TerraMind, Prithvi, and Clay) outperform them on multispectral applications such as agriculture and disaster response. These findings demonstrate that optimal model choice depends on task requirements, data modalities, and constraints. This shows that the goal of a single GeoFM model that performs well across all tasks remains open for future research. GEO-Bench-2 enables informed, reproducible GeoFM evaluation tailored to specific use cases. Code, data, and leaderboard for GEO-Bench-2 are publicly released under a permissive license.",
        "translated": "地理空间基础模型（GeoFMs）正在变革地球观测（EO），但其评估缺乏标准化协议。GEO-Bench-2 通过一个涵盖分类、分割、回归、目标检测和实例分割的综合框架解决了这一问题，涉及19个具有宽松授权的数据集。我们引入“能力”组，以便在共享共同特征（如分辨率、波段、时序性）的数据集上对模型进行排名。这使用户能够识别哪些模型在每种能力上表现最佳，并确定未来工作中需要改进的领域。为兼顾公平比较与方法创新，我们定义了一套既规范又灵活的评估协议。该协议不仅确保基准测试的一致性，还促进了对模型适配策略的研究——这是推动GeoFMs应用于下游任务的关键且开放性挑战。\n\n我们的实验表明，不存在单一模型能主导所有任务，证实了架构设计和预训练阶段所作选择的高度特定性。虽然在自然图像上预训练的模型（ConvNext ImageNet、DINO V3）在高分辨率任务中表现优异，但在农业与灾害响应等多光谱应用中，面向地球观测的专用模型（TerraMind、Prithvi 和 Clay）表现更优。这些发现表明，最优模型的选择取决于任务需求、数据模态与约束条件。这也说明，构建一个能在所有任务中均表现优异的通用GeoFM模型仍是未来研究的重要课题。GEO-Bench-2支持针对具体应用场景的有依据、可复现的GeoFM评估。GEO-Bench-2的代码、数据及排行榜已按宽松许可协议公开发布。",
        "translated_title": "GEO-Bench-2：从性能到能力，重新思考地理空间人工智能的评估",
        "label": [],
        "label_reason": "评估框架属高阶任务，非图像像素级处理",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出新评估协议，支持模型比较与适应研究"
    },
    {
        "title": "INQUIRE-Search: A Framework for Interactive Discovery in Large-Scale Biodiversity Databases",
        "url": "http://arxiv.org/abs/2511.15656v1",
        "pub_date": "2025-11-19",
        "summary": "Large community science platforms such as iNaturalist contain hundreds of millions of biodiversity images that often capture ecological context on behaviors, interactions, phenology, and habitat. Yet most ecological workflows rely on metadata filtering or manual inspection, leaving this secondary information inaccessible at scale. We introduce INQUIRE-Search, an open-source system that enables scientists to rapidly and interactively search within an ecological image database for specific concepts using natural language, verify and export relevant observations, and utilize this discovered data for novel scientific analysis. Compared to traditional methods, INQUIRE-Search takes a fraction of the time, opening up new possibilities for scientific questions that can be explored. Through five case studies, we show the diversity of scientific applications that a tool like INQUIRE-Search can support, from seasonal variation in behavior across species to forest regrowth after wildfires. These examples demonstrate a new paradigm for interactive, efficient, and scalable scientific discovery that can begin to unlock previously inaccessible scientific value in large-scale biodiversity datasets. Finally, we emphasize using such AI-enabled discovery tools for science call for experts to reframe the priorities of the scientific process and develop novel methods for experiment design, data collection, survey effort, and uncertainty analysis.",
        "translated": "诸如 iNaturalist 等大型公民科学平台包含数亿张生物多样性图像，这些图像通常捕捉了物种行为、相互作用、物候及栖息地等生态背景信息。然而，大多数生态学工作流程依赖于元数据筛选或人工检查，导致此类次级信息无法大规模获取。我们提出 INQUIRE-Search，一个开源系统，使科研人员能够通过自然语言快速交互式地在生态图像数据库中检索特定概念，验证并导出相关观测结果，并利用所发现的数据开展新颖的科学研究。与传统方法相比，INQUIRE-Search 所需时间仅为其中一小部分，从而开辟了探索新科学问题的可能性。通过五个案例研究，我们展示了像 INQUIRE-Search 这样的工具所能支持的科学应用场景之多样性，从不同物种间的行为季节性变化到野火后森林的再生过程。这些示例体现了一种新的科学发现范式：交互式、高效且可扩展，有望逐步释放大规模生物多样性数据集中的先前不可及的科学价值。最后，我们强调使用这类基于人工智能的发现工具进行科学研究，呼吁专家重新审视科学过程的优先事项，并开发用于实验设计、数据采集、调查投入和不确定性分析等环节的新方法。",
        "translated_title": "INQUIRE-Search：大规模生物多样性数据库中交互式发现的框架",
        "label": [],
        "label_reason": "非图像处理任务，属科学数据交互系统",
        "relevance_score": 1,
        "novelty_score": 3,
        "novelty_reason": "复用现有AI工具，无像素级创新"
    },
    {
        "title": "MambaIO: Global-Coordinate Inertial Odometry for Pedestrians via Multi-Scale Frequency-Decoupled Modeling",
        "url": "http://arxiv.org/abs/2511.15645v1",
        "pub_date": "2025-11-19",
        "summary": "Inertial Odometry (IO) enables real-time localization using only acceleration and angular velocity measurements from an Inertial Measurement Unit (IMU), making it a promising solution for localization in consumer-grade applications. Traditionally, IMU measurements in IO have been processed under two coordinate system paradigms: the body coordinate frame and the global coordinate frame, with the latter being widely adopted. However, recent studies in drone scenarios have demonstrated that the body frame can significantly improve localization accuracy, prompting a re-evaluation of the suitability of the global frame for pedestrian IO. To address this issue, this paper systematically evaluates the effectiveness of the global coordinate frame in pedestrian IO through theoretical analysis, qualitative inspection, and quantitative experiments. Building upon these findings, we further propose MambaIO, which decomposes IMU measurements into high-frequency and low-frequency components using a Laplacian pyramid. The low-frequency component is processed by a Mamba architecture to extract implicit contextual motion cues, while the high-frequency component is handled by a convolutional structure to capture fine-grained local motion details. Experiments on multiple public datasets show that MambaIO substantially reduces localization error and achieves state-of-the-art (SOTA) performance. To the best of our knowledge, this is the first application of the Mamba architecture to the inertial odometry task.",
        "translated": "惯性里程计（Inertial Odometry, IO）仅利用惯性测量单元（IMU）提供的加速度与角速度测量值，即可实现实时定位，因此在消费级应用中的定位任务中具有巨大潜力。传统上，IO 中的 IMU 测量值通常基于两种坐标系范式进行处理：机体坐标系与全局坐标系，后者被广泛采用。然而，近期针对无人机场景的研究表明，机体坐标系可显著提升定位精度，这促使我们重新评估全局坐标系在行人级惯性里程计中的适用性。为解决该问题，本文通过理论分析、定性检验与定量实验，系统评估了全局坐标系在行人级 IO 中的有效性。基于上述发现，我们进一步提出 MambaIO 方法，该方法利用拉普拉斯金字塔将 IMU 测量值分解为高频与低频分量；低频分量由 Mamba 架构处理，用于提取隐式的上下文运动线索；高频分量则由卷积结构处理，以捕捉精细的局部运动细节。在多个公开数据集上的实验结果表明，MambaIO 显著降低了定位误差，并取得了当前最先进（SOTA）性能。据我们所知，这是 Mamba 架构首次应用于惯性里程计任务。",
        "translated_title": "MambaIO：基于多尺度频域解耦建模的行人全局坐标惯性里程计",
        "label": [],
        "label_reason": "任务为惯性里程计，非图像处理",
        "relevance_score": 1,
        "novelty_score": 9,
        "novelty_reason": "首次将Mamba用于惯性里程计，架构创新"
    },
    {
        "title": "Multi-Stage Residual-Aware Unsupervised Deep Learning Framework for Consistent Ultrasound Strain Elastography",
        "url": "http://arxiv.org/abs/2511.15640v1",
        "pub_date": "2025-11-19",
        "summary": "Ultrasound Strain Elastography (USE) is a powerful non-invasive imaging technique for assessing tissue mechanical properties, offering crucial diagnostic value across diverse clinical applications. However, its clinical application remains limited by tissue decorrelation noise, scarcity of ground truth, and inconsistent strain estimation under different deformation conditions. Overcoming these barriers, we propose MUSSE-Net, a residual-aware, multi-stage unsupervised sequential deep learning framework designed for robust and consistent strain estimation. At its backbone lies our proposed USSE-Net, an end-to-end multi-stream encoder-decoder architecture that parallelly processes pre- and post-deformation RF sequences to estimate displacement fields and axial strains. The novel architecture incorporates Context-Aware Complementary Feature Fusion (CACFF)-based encoder with Tri-Cross Attention (TCA) bottleneck with a Cross-Attentive Fusion (CAF)-based sequential decoder. To ensure temporal coherence and strain stability across varying deformation levels, this architecture leverages a tailored consistency loss. Finally, with the MUSSE-Net framework, a secondary residual refinement stage further enhances accuracy and suppresses noise. Extensive validation on simulation, in vivo, and private clinical datasets from Bangladesh University of Engineering and Technology (BUET) medical center, demonstrates MUSSE-Net's outperformed existing unsupervised approaches. On MUSSE-Net achieves state-of-the-art performance with a target SNR of 24.54, background SNR of 132.76, CNR of 59.81, and elastographic SNR of 9.73 on simulation data. In particular, on the BUET dataset, MUSSE-Net produces strain maps with enhanced lesion-to-background contrast and significant noise suppression yielding clinically interpretable strain patterns.",
        "translated": "超声应变弹性成像（Ultrasound Strain Elastography, USE）是一种强大的无创成像技术，用于评估组织的机械特性，在多种临床应用中具有关键诊断价值。然而，其临床应用受限于组织失相关噪声、缺乏真实标注数据，以及在不同形变条件下应变估计结果不一致等问题。为克服上述障碍，我们提出 MUSSE-Net，一种基于残差感知、多阶段无监督序列深度学习框架，旨在实现稳健且一致的应变估计。该框架的核心是所提出的 USSE-Net，一个端到端的多流编码器-解码器架构，可并行处理形变前后的 RF 序列，以估计位移场和轴向应变。该新颖架构包含基于上下文感知互补特征融合（Context-Aware Complementary Feature Fusion, CACFF）的编码器、结合三通道交叉注意力（Tri-Cross Attention, TCA）瓶颈的模块，以及基于交叉注意融合（Cross-Attentive Fusion, CAF）的序列解码器。为确保在不同形变水平下时间一致性与应变稳定性，该架构引入了定制化的自洽损失函数。最终，在 MUSSE-Net 框架基础上，增加二级残差精炼阶段，进一步提升精度并抑制噪声。在仿真数据、体内实验及来自孟加拉国工程技术大学（Bangladesh University of Engineering and Technology, BUET）医学中心的私有临床数据集上的广泛验证表明，MUSSE-Net 在现有无监督方法中表现最优。在仿真数据上，MUSSE-Net 实现了当前最先进性能：目标信噪比（SNR）为 24.54，背景信噪比为 132.76，对比度噪声比（CNR）为 59.81，弹性成像信噪比为 9.73。特别地，在 BUET 数据集上，MUSSE-Net 输出的应变图具有增强的病灶与背景对比度，并显著抑制噪声，生成具备临床解释性的应变分布模式。",
        "translated_title": "多阶段残差感知无监督深度学习框架用于一致的超声应变弹性成像",
        "label": [
            "医学图像增强"
        ],
        "label_reason": "针对超声弹性成像的噪声与一致性问题，提升像素级质量。",
        "relevance_score": 4,
        "novelty_score": 7,
        "novelty_reason": "提出多阶段残差感知架构及一致性损失，显著改进性能。"
    },
    {
        "title": "Hierarchical Semantic Tree Anchoring for CLIP-Based Class-Incremental Learning",
        "url": "http://arxiv.org/abs/2511.15633v1",
        "pub_date": "2025-11-19",
        "summary": "Class-Incremental Learning (CIL) enables models to learn new classes continually while preserving past knowledge. Recently, vision-language models like CLIP offer transferable features via multi-modal pre-training, making them well-suited for CIL. However, real-world visual and linguistic concepts are inherently hierarchical: a textual concept like \"dog\" subsumes fine-grained categories such as \"Labrador\" and \"Golden Retriever,\" and each category entails its images. But existing CLIP-based CIL methods fail to explicitly capture this inherent hierarchy, leading to fine-grained class features drift during incremental updates and ultimately to catastrophic forgetting. To address this challenge, we propose HASTEN (Hierarchical Semantic Tree Anchoring) that anchors hierarchical information into CIL to reduce catastrophic forgetting. First, we employ an external knowledge graph as supervision to embed visual and textual features in hyperbolic space, effectively preserving hierarchical structure as data evolves. Second, to mitigate catastrophic forgetting, we project gradients onto the null space of the shared hyperbolic mapper, preventing interference with prior tasks. These two steps work synergistically to enable the model to resist forgetting by maintaining hierarchical relationships. Extensive experiments show that HASTEN consistently outperforms existing methods while providing a unified structured representation.",
        "translated": "类增量学习（Class-Incremental Learning, CIL）使模型能够在持续学习新类别时保留过往知识。近年来，视觉-语言模型（如 CLIP）通过多模态预训练提供可迁移特征，使其非常适合用于 CIL。然而，现实世界中的视觉与语言概念本质上具有层次结构：一个文本概念（如“狗”）涵盖细粒度子类（如“拉布拉多犬”和“金毛寻回犬”），而每个类别均对应其图像实例。但现有的基于 CLIP 的 CIL 方法未能显式捕捉这一固有层次结构，在增量更新过程中导致细粒度类别特征漂移，并最终引发灾难性遗忘。为应对该挑战，我们提出 HASTEN（Hierarchical Semantic Tree Anchoring），将层次信息锚定于 CIL 以减少灾难性遗忘。首先，我们利用外部知识图谱作为监督信号，将视觉与文本特征嵌入到双曲空间中，从而在数据演化过程中有效保持层次结构。其次，为缓解灾难性遗忘，我们将梯度投影至共享双曲映射器的零空间，避免对先前任务造成干扰。这两个步骤协同作用，使模型能够通过维持层次关系来抵抗遗忘。大量实验表明，HASTEN 在性能上持续优于现有方法，同时提供统一的结构化表示。",
        "translated_title": "基于CLIP的类增量学习中的层次语义树锚定",
        "label": [],
        "label_reason": "属于高阶视觉任务，非图像像素级处理",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出层次结构锚定机制，有效缓解灾难性遗忘"
    },
    {
        "title": "The SA-FARI Dataset: Segment Anything in Footage of Animals for Recognition and Identification",
        "url": "http://arxiv.org/abs/2511.15622v1",
        "pub_date": "2025-11-19",
        "summary": "Automated video analysis is critical for wildlife conservation. A foundational task in this domain is multi-animal tracking (MAT), which underpins applications such as individual re-identification and behavior recognition. However, existing datasets are limited in scale, constrained to a few species, or lack sufficient temporal and geographical diversity - leaving no suitable benchmark for training general-purpose MAT models applicable across wild animal populations. To address this, we introduce SA-FARI, the largest open-source MAT dataset for wild animals. It comprises 11,609 camera trap videos collected over approximately 10 years (2014-2024) from 741 locations across 4 continents, spanning 99 species categories. Each video is exhaustively annotated culminating in ~46 hours of densely annotated footage containing 16,224 masklet identities and 942,702 individual bounding boxes, segmentation masks, and species labels. Alongside the task-specific annotations, we publish anonymized camera trap locations for each video. Finally, we present comprehensive benchmarks on SA-FARI using state-of-the-art vision-language models for detection and tracking, including SAM 3, evaluated with both species-specific and generic animal prompts. We also compare against vision-only methods developed specifically for wildlife analysis. SA-FARI is the first large-scale dataset to combine high species diversity, multi-region coverage, and high-quality spatio-temporal annotations, offering a new foundation for advancing generalizable multianimal tracking in the wild. The dataset is available at $\\href{https://www.conservationxlabs.com/sa-fari}{\\text{conservationxlabs.com/SA-FARI}}$.",
        "translated": "自动化视频分析对野生动物保护至关重要。该领域的一项基础任务是多动物跟踪（Multi-Animal Tracking, MAT），其支撑着个体重识别与行为识别等应用。然而，现有数据集普遍存在规模有限、仅覆盖少数物种、或缺乏充分的时间与地理多样性等问题——尚未有适合训练通用MAT模型的基准数据集，以适用于野外各类动物种群。为解决这一问题，我们引入了SA-FARI，这是目前最大的开源野生动物多动物跟踪数据集。该数据集包含自2014年至2024年约十年间，在四大洲741个地点采集的11,609段相机陷阱视频，涵盖99个物种类别。每段视频均经过详尽标注，累计约46小时密集标注内容，包括16,224个masklet身份标识、942,702个个体边界框、分割掩码及物种标签。除任务特定标注外，我们还发布了每段视频对应的匿名化相机陷阱位置信息。最后，我们在SA-FARI上全面评估了当前最先进的视觉-语言模型在检测与跟踪任务中的表现，包括SAM 3，并采用物种特异性与通用动物提示进行评估；同时亦对比了专为野生动物分析设计的纯视觉方法。SA-FARI是首个融合高物种多样性、多区域覆盖与高质量时空标注的大规模数据集，为推动野外可泛化的多动物跟踪研究提供了全新基础。该数据集可在 $\\href{https://www.conservationxlabs.com/sa-fari}{\\text{conservationxlabs.com/SA-FARI}}$ 获取。",
        "translated_title": "SA-FARI 数据集：用于动物识别与鉴定的动物影像分割任何内容",
        "label": [],
        "label_reason": "高阶视频动物跟踪识别任务，非图像像素级处理",
        "relevance_score": 1,
        "novelty_score": 1,
        "novelty_reason": "仅提供数据集，无新方法或架构创新"
    },
    {
        "title": "FlashMesh: Faster and Better Autoregressive Mesh Synthesis via Structured Speculation",
        "url": "http://arxiv.org/abs/2511.15618v1",
        "pub_date": "2025-11-19",
        "summary": "Autoregressive models can generate high-quality 3D meshes by sequentially producing vertices and faces, but their token-by-token decoding results in slow inference, limiting practical use in interactive and large-scale applications. We present FlashMesh, a fast and high-fidelity mesh generation framework that rethinks autoregressive decoding through a predict-correct-verify paradigm. The key insight is that mesh tokens exhibit strong structural and geometric correlations that enable confident multi-token speculation. FlashMesh leverages this by introducing a speculative decoding scheme tailored to the commonly used hourglass transformer architecture, enabling parallel prediction across face, point, and coordinate levels. Extensive experiments show that FlashMesh achieves up to a 2 x speedup over standard autoregressive models while also improving generation fidelity. Our results demonstrate that structural priors in mesh data can be systematically harnessed to accelerate and enhance autoregressive generation.",
        "translated": "自回归模型可通过逐个生成顶点与面片的方式，生成高质量的3D网格，但其按token顺序解码的方式导致推理速度缓慢，限制了其在交互式及大规模应用场景中的实际应用。我们提出FlashMesh，一种快速且高保真度的网格生成框架，通过“预测-校正-验证”范式重新思考自回归解码机制。核心思想在于：网格token间存在强结构与几何相关性，从而支持对多个token进行置信度高的推测。FlashMesh利用这一特性，为常用于构建的沙漏型Transformer架构量身定制了一种推测式解码方案，实现了在面、点和坐标层级上的并行预测。大量实验表明，FlashMesh相较标准自回归模型可实现高达2倍的速度提升，同时进一步提升了生成保真度。我们的结果证明，网格数据中的结构先验知识可系统性地被用于加速并增强自回归生成过程。",
        "translated_title": "FlashMesh：通过结构化推测实现更快更优的自回归网格合成",
        "label": [],
        "label_reason": "生成3D网格属高阶任务，非像素级图像处理",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "创新性解码框架，但非图像恢复领域"
    },
    {
        "title": "When to Think and When to Look: Uncertainty-Guided Lookback",
        "url": "http://arxiv.org/abs/2511.15613v1",
        "pub_date": "2025-11-19",
        "summary": "Test-time thinking (that is, generating explicit intermediate reasoning chains) is known to boost performance in large language models and has recently shown strong gains for large vision language models (LVLMs). However, despite these promising results, there is still no systematic analysis of how thinking actually affects visual reasoning. We provide the first such analysis with a large scale, controlled comparison of thinking for LVLMs, evaluating ten variants from the InternVL3.5 and Qwen3-VL families on MMMU-val under generous token budgets and multi pass decoding. We show that more thinking is not always better; long chains often yield long wrong trajectories that ignore the image and underperform the same models run in standard instruct mode. A deeper analysis reveals that certain short lookback phrases, which explicitly refer back to the image, are strongly enriched in successful trajectories and correlate with better visual grounding. Building on this insight, we propose uncertainty guided lookback, a training free decoding strategy that combines an uncertainty signal with adaptive lookback prompts and breadth search. Our method improves overall MMMU performance, delivers the largest gains in categories where standard thinking is weak, and outperforms several strong decoding baselines, setting a new state of the art under fixed model families and token budgets. We further show that this decoding strategy generalizes, yielding consistent improvements on five additional benchmarks, including two broad multimodal suites and math focused visual reasoning datasets.",
        "translated": "测试时思维（即生成显式的中间推理链）已被证明能提升大型语言模型的性能，并在近期对大型视觉语言模型（LVLMs）展现出显著收益。然而，尽管这些结果令人鼓舞，目前尚缺乏系统性分析以探究思维机制如何实际影响视觉推理。本文首次通过大规模、可控的对比实验，对LVLMs中的思维机制进行深入分析：我们在宽松的token预算与多轮解码条件下，评估了来自InternVL3.5和Qwen3-VL家族的十种不同变体在MMMU-val数据集上的表现。我们发现，更多的思维并不总是更好；过长的推理链往往导致偏离图像内容的错误轨迹，且其性能劣于相同模型在标准指令模式下的表现。进一步分析表明，某些明确回溯图像的短语（short lookback phrases）在成功轨迹中显著富集，并与更强的视觉 grounding 正相关。基于这一洞察，我们提出一种无需训练的解码策略——不确定性引导回溯（uncertainty guided lookback），该策略结合不确定性信号、自适应回溯提示与广度搜索机制。我们的方法提升了整体MMMU性能，在标准思维能力较弱的任务类别中取得最大增益，并优于多个强基线解码方法，在固定模型族与token预算下创下新的最先进水平。此外，我们进一步验证该解码策略具备泛化能力，在五个额外基准上均带来一致性能提升，涵盖两个广泛的多模态评测套件及面向数学推理的视觉任务数据集。",
        "translated_title": "何时思考，何时观察：不确定性引导的回溯机制",
        "label": [],
        "label_reason": "处理视觉语言模型推理，非像素级图像处理",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出不确定性引导回溯解码策略，提升多模态推理性能"
    },
    {
        "title": "SRPO: Self-Referential Policy Optimization for Vision-Language-Action Models",
        "url": "http://arxiv.org/abs/2511.15605v1",
        "pub_date": "2025-11-19",
        "summary": "Vision-Language-Action (VLA) models excel in robotic manipulation but are constrained by their heavy reliance on expert demonstrations, leading to demonstration bias and limiting performance. Reinforcement learning (RL) is a vital post-training strategy to overcome these limits, yet current VLA-RL methods, including group-based optimization approaches, are crippled by severe reward sparsity. Relying on binary success indicators wastes valuable information in failed trajectories, resulting in low training efficiency. To solve this, we propose Self-Referential Policy Optimization (SRPO), a novel VLA-RL framework. SRPO eliminates the need for external demonstrations or manual reward engineering by leveraging the model's own successful trajectories, generated within the current training batch, as a self-reference. This allows us to assign a progress-wise reward to failed attempts. A core innovation is the use of latent world representations to measure behavioral progress robustly. Instead of relying on raw pixels or requiring domain-specific fine-tuning, we utilize the compressed, transferable encodings from a world model's latent space. These representations naturally capture progress patterns across environments, enabling accurate, generalized trajectory comparison. Empirical evaluations on the LIBERO benchmark demonstrate SRPO's efficiency and effectiveness. Starting from a supervised baseline with 48.9% success, SRPO achieves a new state-of-the-art success rate of 99.2% in just 200 RL steps, representing a 103% relative improvement without any extra supervision. Furthermore, SRPO shows substantial robustness, achieving a 167% performance improvement on the LIBERO-Plus benchmark.",
        "translated": "视觉-语言-动作（VLA）模型在机器人操作任务中表现出色，但受限于其对专家演示的高度依赖，导致演示偏差并限制了性能上限。强化学习（RL）是克服这些限制的关键后训练策略，然而当前的 VLA-RL 方法——包括基于群体优化的方法——均因奖励稀疏性严重而受阻。依赖二元成功指示器浪费了失败轨迹中的宝贵信息，从而导致训练效率低下。为解决此问题，我们提出了一种新颖的 VLA-RL 框架——自参考策略优化（Self-Referential Policy Optimization, SRPO）。SRPO 无需外部演示或人工奖励工程，而是利用当前训练批次内模型自身生成的成功轨迹作为自参考基准。该机制允许我们为失败尝试分配按进度划分的奖励。其核心创新在于采用潜在世界表征以稳健衡量行为进展。不同于依赖原始像素或需要特定领域微调，我们直接使用世界模型潜在空间中压缩且可迁移的编码表示。这些表征天然捕捉跨环境的行为进步模式，从而支持准确、泛化的轨迹比较。在 LIBERO 基准上的实证评估表明，SRPO 在效率与效果上均表现优异：从监督基线（成功率 48.9%）出发，仅需 200 步 RL 训练，SRPO 即实现 99.2% 的新最优成功率，相较基准提升 103%，且无需额外监督。此外，SRPO 展现出显著的鲁棒性，在 LIBERO-Plus 基准上取得 167% 的性能提升。",
        "translated_title": "SRPO：面向视觉-语言-动作模型的自参照策略优化",
        "label": [],
        "label_reason": "属于高阶视觉任务，非像素级图像处理",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "提出新RL框架，改进轨迹奖励机制"
    },
    {
        "title": "MaskMed: Decoupled Mask and Class Prediction for Medical Image Segmentation",
        "url": "http://arxiv.org/abs/2511.15603v1",
        "pub_date": "2025-11-19",
        "summary": "Medical image segmentation typically adopts a point-wise convolutional segmentation head to predict dense labels, where each output channel is heuristically tied to a specific class. This rigid design limits both feature sharing and semantic generalization. In this work, we propose a unified decoupled segmentation head that separates multi-class prediction into class-agnostic mask prediction and class label prediction using shared object queries. Furthermore, we introduce a Full-Scale Aware Deformable Transformer module that enables low-resolution encoder features to attend across full-resolution encoder features via deformable attention, achieving memory-efficient and spatially aligned full-scale fusion. Our proposed method, named MaskMed, achieves state-of-the-art performance, surpassing nnUNet by +2.0% Dice on AMOS 2022 and +6.9% Dice on BTCV.",
        "translated": "医学图像分割通常采用逐点卷积分割头来预测密集标签，其中每个输出通道被启发式地绑定至特定类别。这种刚性设计限制了特征共享和语义泛化能力。在本文中，我们提出了一种统一的解耦分割头，利用共享的对象查询将多类预测分解为类别无关的掩码预测与类别标签预测。此外，我们引入了一种全尺度感知可变形Transformer模块，通过可变形注意力机制使低分辨率编码器特征能够关注全分辨率编码器特征，从而实现内存高效且空间对齐的全尺度融合。我们所提出的方法命名为MaskMed，在AMOS 2022数据集上比nnUNet的Dice系数高出+2.0%，在BTCV数据集上高出+6.9%。",
        "translated_title": "MaskMed：面向医学图像分割的分离式掩码与类别预测",
        "label": [],
        "label_reason": "目标为医学图像分割，属高阶语义理解任务",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出解耦头与变形Transformer，提升分割性能"
    },
    {
        "title": "Dataset Distillation for Pre-Trained Self-Supervised Vision Models",
        "url": "http://arxiv.org/abs/2511.16674v1",
        "pub_date": "2025-11-20",
        "summary": "The task of dataset distillation aims to find a small set of synthetic images such that training a model on them reproduces the performance of the same model trained on a much larger dataset of real samples. Existing distillation methods focus on synthesizing datasets that enable training randomly initialized models. In contrast, state-of-the-art vision approaches are increasingly building on large, pre-trained self-supervised models rather than training from scratch. In this paper, we investigate the problem of distilling datasets that enable us to optimally train linear probes on top of such large, pre-trained vision models. We introduce a method of dataset distillation for this task called Linear Gradient Matching that optimizes the synthetic images such that, when passed through a pre-trained feature extractor, they induce gradients in the linear classifier similar to those produced by the real data. Our method yields synthetic data that outperform all real-image baselines and, remarkably, generalize across pre-trained vision models, enabling us, for instance, to train a linear CLIP probe that performs competitively using a dataset distilled via a DINO backbone. Further, we show that our distilled datasets are exceptionally effective for fine-grained classification and provide a valuable tool for model interpretability, predicting, among other things, how similar two models' embedding spaces are under the platonic representation hypothesis or whether a model is sensitive to spurious correlations in adversarial datasets.",
        "translated": "数据集蒸馏任务旨在寻找一组小型合成图像，使得在这些图像上训练模型能够复现其在大量真实样本数据集上训练所得的性能。现有蒸馏方法主要关注生成可支持从零初始化模型训练的数据集。相比之下，当前最先进的视觉方法正越来越多地依赖于大型预训练自监督模型，而非从头开始训练。本文研究了如何蒸馏出适用于在上述大型预训练视觉模型之上最优训练线性探针的数据集。我们为此任务提出一种名为“线性梯度匹配”的数据集蒸馏方法，该方法通过优化合成图像，使其经过预训练特征提取器后，在线性分类器中诱导的梯度与真实数据所诱导的梯度相似。我们的方法生成的合成数据优于所有真实图像基线，并且显著展现出跨不同预训练视觉模型的泛化能力——例如，我们可以使用基于DINO骨干网络蒸馏的数据集训练出表现优异的线性CLIP探针。此外，我们表明，经本方法蒸馏的数据集在细粒度分类任务中极为有效，并为模型可解释性提供了重要工具，可用于预测诸如：在理想表征假设下两个模型嵌入空间的相似程度，或判断模型是否对对抗性数据集中虚假相关性的敏感性等问题。",
        "translated_title": "预训练自监督视觉模型的数据蒸馏",
        "label": [],
        "label_reason": "聚焦模型压缩与合成数据，非图像像素级恢复",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "提出线性梯度匹配新方法，提升合成数据有效性"
    },
    {
        "title": "EvoLMM: Self-Evolving Large Multimodal Models with Continuous Rewards",
        "url": "http://arxiv.org/abs/2511.16672v1",
        "pub_date": "2025-11-20",
        "summary": "Recent advances in large multimodal models (LMMs) have enabled impressive reasoning and perception abilities, yet most existing training pipelines still depend on human-curated data or externally verified reward models, limiting their autonomy and scalability. In this work, we strive to improve LMM reasoning capabilities in a purely unsupervised fashion (without any annotated data or reward distillation). To this end, we propose a self-evolving framework, named EvoLMM, that instantiates two cooperative agents from a single backbone model: a Proposer, which generates diverse, image-grounded questions, and a Solver, which solves them through internal consistency, where learning proceeds through a continuous self-rewarding process. This dynamic feedback encourages both the generation of informative queries and the refinement of structured reasoning without relying on ground-truth or human judgments. When using the popular Qwen2.5-VL as the base model, our EvoLMM yields consistent gains upto $\\sim$3\\% on multimodal math-reasoning benchmarks, including ChartQA, MathVista, and MathVision, using only raw training images. We hope our simple yet effective approach will serve as a solid baseline easing future research in self-improving LMMs in a fully-unsupervised fashion. Our code and models are available at https://github.com/mbzuai-oryx/EvoLMM.",
        "translated": "近年来，大型多模态模型（LMMs）在推理与感知能力方面取得了显著进展，但现有大多数训练流程仍依赖人工标注数据或外部验证的奖励模型，限制了其自主性与可扩展性。在本研究中，我们致力于在完全无监督的方式下（无需任何标注数据或奖励蒸馏）提升 LMM 的推理能力。为此，我们提出了一种自演化框架 EvoLMM，该框架从单一主干模型中实例化两个协作代理：一个提议者（Proposer），负责生成多样化的、基于图像的问题；一个求解者（Solver），通过内部一致性解决这些问题，学习过程通过持续的自我奖励机制推进。该动态反馈机制鼓励生成信息量丰富的查询，并促进结构化推理的优化，而无需依赖真实标签或人工判断。当以流行的 Qwen2.5-VL 作为基座模型时，我们的 EvoLMM 在多个多模态数学推理基准（包括 ChartQA、MathVista 和 MathVision）上均实现了约 3% 的稳定提升，且仅使用原始训练图像。我们希望这一简洁而有效的方案能为未来在完全无监督模式下自改进 LMM 的研究提供坚实基线。我们的代码与模型已开源于 https://github.com/mbzuai-oryx/EvoLMM。",
        "translated_title": "EvoLMM：带连续奖励的自演进大型多模态模型",
        "label": [],
        "label_reason": "属于高阶多模态推理任务，非图像像素级处理",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出自进化框架，但无图像恢复相关创新"
    },
    {
        "title": "NoPo-Avatar: Generalizable and Animatable Avatars from Sparse Inputs without Human Poses",
        "url": "http://arxiv.org/abs/2511.16673v1",
        "pub_date": "2025-11-20",
        "summary": "We tackle the task of recovering an animatable 3D human avatar from a single or a sparse set of images. For this task, beyond a set of images, many prior state-of-the-art methods use accurate \"ground-truth\" camera poses and human poses as input to guide reconstruction at test-time. We show that pose-dependent reconstruction degrades results significantly if pose estimates are noisy. To overcome this, we introduce NoPo-Avatar, which reconstructs avatars solely from images, without any pose input. By removing the dependence of test-time reconstruction on human poses, NoPo-Avatar is not affected by noisy human pose estimates, making it more widely applicable. Experiments on challenging THuman2.0, XHuman, and HuGe100K data show that NoPo-Avatar outperforms existing baselines in practical settings (without ground-truth poses) and delivers comparable results in lab settings (with ground-truth poses).",
        "translated": "我们研究从单张或稀疏图像集合中恢复可动画化3D人体虚拟形象的任务。对于该任务，许多现有最先进的方法除了图像外，还依赖准确的“真实”相机位姿与人体位姿作为输入，在测试阶段引导重建过程。我们证明，若人体位姿估计存在噪声，基于位姿的重建会导致结果显著下降。为解决此问题，我们提出了NoPo-Avatar方法，该方法仅利用图像进行虚拟形象重建，无需任何位姿输入。通过消除测试阶段重建对人类位姿的依赖，NoPo-Avatar不受噪声人体位姿估计的影响，从而具备更广泛的适用性。在具有挑战性的THuman2.0、XHuman和HuGe100K数据集上的实验表明，NoPo-Avatar在实际场景（无真实位姿）下优于现有基线方法，并在实验室场景（有真实位姿）下达到相当的结果。",
        "translated_title": "NoPo-Avatar：无需人体姿态的稀疏输入下具备泛化能力与可动画化的虚拟形象",
        "label": [],
        "label_reason": "任务为3D人像重建，属高阶视觉任务",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出无需姿态输入的可动画化头像重建方法"
    },
    {
        "title": "Thinking-while-Generating: Interleaving Textual Reasoning throughout Visual Generation",
        "url": "http://arxiv.org/abs/2511.16671v1",
        "pub_date": "2025-11-20",
        "summary": "Recent advances in visual generation have increasingly explored the integration of reasoning capabilities. They incorporate textual reasoning, i.e., think, either before (as pre-planning) or after (as post-refinement) the generation process, yet they lack on-the-fly multimodal interaction during the generation itself. In this preliminary study, we introduce Thinking-while-Generating (TwiG), the first interleaved framework that enables co-evolving textual reasoning throughout the visual generation process. As visual content is progressively generating, textual reasoning is interleaved to both guide upcoming local regions and reflect on previously synthesized ones. This dynamic interplay produces more context-aware and semantically rich visual outputs. To unveil the potential of this framework, we investigate three candidate strategies, zero-shot prompting, supervised fine-tuning (SFT) on our curated TwiG-50K dataset, and reinforcement learning (RL) via a customized TwiG-GRPO strategy, each offering unique insights into the dynamics of interleaved reasoning. We hope this work inspires further research into interleaving textual reasoning for enhanced visual generation. Code will be released at: https://github.com/ZiyuGuo99/Thinking-while-Generating.",
        "translated": "视觉生成领域的最新进展日益探索将推理能力融入其中。它们引入了文本推理（即“思考”），要么在生成前作为预规划，要么在生成后作为后处理，但缺乏在生成过程中实时进行多模态交互的能力。在本初步研究中，我们提出了“边生成边思考”（Thinking-while-Generating, TwiG），这是首个在视觉生成过程中实现文本推理与生成内容协同演化的交错式框架。随着视觉内容逐步生成，文本推理被交错插入，既引导即将生成的局部区域，也对已合成的先前内容进行反思。这种动态交互机制生成更具上下文感知性和语义丰富性的视觉输出。为揭示该框架的潜力，我们研究了三种候选策略：零样本提示、基于我们精心构建的TwiG-50K数据集的监督微调（SFT），以及通过定制化的TwiG-GRPO策略实现的强化学习（RL），每种策略均从不同角度揭示交错推理的动态特性。我们希望本工作能激发更多关于如何交错集成文本推理以提升视觉生成效果的研究。代码将在以下地址开源：https://github.com/ZiyuGuo99/Thinking-while-Generating。",
        "translated_title": "边思考边生成：在视觉生成过程中交织文本推理",
        "label": [],
        "label_reason": "属于高阶视觉生成任务，非像素级图像处理",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "首次提出推理与生成交织框架，提升语义丰富度"
    },
    {
        "title": "Learning to Think Fast and Slow for Visual Language Models",
        "url": "http://arxiv.org/abs/2511.16670v1",
        "pub_date": "2025-11-20",
        "summary": "When confronted with complex problems, we tend to think slowly; conversely, for simple questions, we think quickly. Such a two-system thinking mechanism allows us to efficiently allocate cognitive resources, enabling quick decision-making for straightforward issues while reserving deeper analytical thinking for more intricate challenges. However, existing reasoning-oriented visual language models (VLMs), whether trained with explicit chain-of-thought annotations or rule-based RL rewards, mainly pursue lengthy, detailed reasoning chains, which often lead to excessive computational costs. In this work, we propose a simple RL approach, which enables VLMs to automatically switch between fast and slow thinking modes depending on task difficulty. The approach consists of two stages: in the first stage, we label data as either requiring fast thinking or slow thinking based on the model output length, which is inspired by the observation that pre-trained VLMs typically produce answers of varying lengths for different types of questions; in the second stage, we train the model using GRPO along with the thinking mode labels to develop dual-mode thinking. Despite its simplicity, our model, named DualMindVLM, significantly outperforms the base model and achieves performance on par with state-of-the-art visual reasoning models, while maintaining exceptionally high token efficiency.",
        "translated": "面对复杂问题时，我们倾向于慢思考；反之，对于简单问题，我们则快速决策。这种双系统思维机制使我们能够高效分配认知资源，在处理简单问题时实现快速决策，而在面对更复杂的挑战时保留深度分析能力。然而，现有的面向推理的视觉语言模型（VLMs），无论是否通过显式的链式思维标注或基于规则的强化学习奖励进行训练，主要追求冗长且详尽的推理链条，往往导致计算成本过高。在本研究中，我们提出了一种简单的强化学习方法，使 VLMs 能根据任务难度自动切换快速与慢速思考模式。该方法包含两个阶段：第一阶段，我们依据模型输出长度将数据划分为需要“快思考”或“慢思考”的类别，这一划分灵感来源于观察到预训练的 VLMs 通常会根据不同类型的提问生成不同长度的回答；第二阶段，我们结合思考模式标签，使用 GRPO 训练模型以构建双模式思考能力。尽管模型结构简洁，我们的方法——名为 DualMindVLM——显著优于基线模型，并达到与当前最先进视觉推理模型相当的性能，同时保持极高的 token 效率。",
        "translated_title": "学会快速与慢速思考以提升视觉语言模型",
        "label": [],
        "label_reason": "研究视觉语言模型推理模式，非图像像素级处理。",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出双模式推理机制，提升效率但非图像恢复任务。"
    },
    {
        "title": "Video-as-Answer: Predict and Generate Next Video Event with Joint-GRPO",
        "url": "http://arxiv.org/abs/2511.16669v1",
        "pub_date": "2025-11-20",
        "summary": "While language models have become impactful in many real-world applications, video generation remains largely confined to entertainment. Motivated by video's inherent capacity to demonstrate physical-world information that is difficult to convey through language alone (e.g., imagine teaching someone to tie a tie using only text), we identify an underutilized opportunity to extend video as a new answer modality for Next-Event Prediction (NEP), formalized as Video-Next-Event Prediction (VNEP). While the established NEP task takes a video with a procedural or predictive question as input to predict the next event in text, VNEP requires dynamic video responses. This shift from telling to showing unlocks more intuitive and customized answers for procedural learning and creative exploration. However, this task remains challenging for existing models, as it demands an understanding of multimodal input, instruction-conditioned reasoning, and the generation of video with visual and semantic consistency. To address this, we introduce VANS, a model that leverages reinforcement learning to align a Vision-Language Model (VLM) with a Video Diffusion Model (VDM) for VNEP. The core of VANS is our proposed Joint-GRPO that orchestrates the VLM and VDM to function as a unit. Driven by a shared reward on their respective output, it optimizes the VLM to produce captions that are both accurate and friendly to visualize, while guiding the VDM to generate videos that are faithful to these captions and the input visual context. To enable this learning, we craft VANS-Data-100K, a dedicated dataset for the VNEP task. Experiments on procedural and predictive benchmarks demonstrate that VANS achieves state-of-the-art performance in both video event prediction and visualization. Codes are released in https://github.com/KlingTeam/VANS.",
        "translated": "尽管语言模型已在许多现实应用中展现出强大影响力，视频生成仍主要局限于娱乐领域。受视频天然具备传达语言难以表达的物理世界信息的能力启发（例如，仅通过文字教人打领带），我们识别出一个尚未充分利用的机会：将视频扩展为下一代事件预测（NEP）的新答案模态，形式化为“视频-下一代事件预测”（VNEP）。传统的NEP任务以带有程序性或预测性问题的视频作为输入，输出下一个事件的文字描述；而VNEP则要求动态视频响应。从“讲述”转向“展示”的转变，为程序性学习与创造性探索提供了更直观、更个性化的答案。然而，该任务对现有模型而言仍具挑战性，因其需要理解多模态输入、指令条件下的推理能力，并生成视觉与语义一致的视频内容。为此，我们提出VANS模型，利用强化学习将视觉-语言模型（VLM）与视频扩散模型（VDM）对齐，用于解决VNEP任务。VANS的核心是所提出的Joint-GRPO机制，它协调VLM与VDM协同运作，作为一个整体单元。该机制基于两者各自输出的共享奖励进行优化：一方面促使VLM生成既准确又便于视觉化的文本描述；另一方面引导VDM生成忠实于这些文本描述及输入视觉上下文的视频。为支持该学习过程，我们构建了专用于VNEP任务的数据集VANS-Data-100K。在程序性与预测性基准上的实验表明，VANS在视频事件预测与可视化方面均达到当前最佳性能。代码已开源于https://github.com/KlingTeam/VANS。",
        "translated_title": "视频作为答案：利用联合GRPO预测并生成下一帧视频事件",
        "label": [],
        "label_reason": "任务属高阶视频生成与理解，非像素级图像恢复",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "提出联合强化学习框架优化多模态视频生成"
    },
    {
        "title": "V-ReasonBench: Toward Unified Reasoning Benchmark Suite for Video Generation Models",
        "url": "http://arxiv.org/abs/2511.16668v1",
        "pub_date": "2025-11-20",
        "summary": "Recent progress in generative video models, such as Veo-3, has shown surprising zero-shot reasoning abilities, creating a growing need for systematic and reliable evaluation. We introduce V-ReasonBench, a benchmark designed to assess video reasoning across four key dimensions: structured problem-solving, spatial cognition, pattern-based inference, and physical dynamics. The benchmark is built from both synthetic and real-world image sequences and provides a diverse set of answer-verifiable tasks that are reproducible, scalable, and unambiguous. Evaluations of six state-of-the-art video models reveal clear dimension-wise differences, with strong variation in structured, spatial, pattern-based, and physical reasoning. We further compare video models with strong image models, analyze common hallucination behaviors, and study how video duration affects Chain-of-Frames reasoning. Overall, V-ReasonBench offers a unified and reproducible framework for measuring video reasoning and aims to support the development of models with more reliable, human-aligned reasoning skills.",
        "translated": "近年来，生成式视频模型（如 Veo-3）展现出令人惊讶的零样本推理能力，由此催生了对系统化、可靠评估方法日益增长的需求。我们提出了 V-ReasonBench，一个旨在从四个关键维度评估视频推理能力的基准测试：结构化问题求解、空间认知、基于模式的推理以及物理动态。该基准由合成与真实世界图像序列构建而成，提供一组多样化的可验证答案任务，具备可复现性、可扩展性和无歧义性。对六种当前最先进的视频模型的评估揭示了各维度间显著差异，其在结构化推理、空间推理、模式推理及物理推理方面表现出明显异质性。我们进一步将视频模型与性能优异的图像模型进行比较，分析常见的幻觉行为，并研究视频时长如何影响帧链推理（Chain-of-Frames）。总体而言，V-ReasonBench 提供了一套统一且可复现的视频推理测量框架，旨在支持开发具备更可靠、符合人类认知的推理能力的模型。",
        "translated_title": "V-ReasonBench：面向视频生成模型的统一推理基准套件",
        "label": [],
        "label_reason": "评估视频生成模型推理能力，非图像像素级处理",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "首次统一视频推理基准，维度划分清晰"
    },
    {
        "title": "SceneDesigner: Controllable Multi-Object Image Generation with 9-DoF Pose Manipulation",
        "url": "http://arxiv.org/abs/2511.16666v1",
        "pub_date": "2025-11-20",
        "summary": "Controllable image generation has attracted increasing attention in recent years, enabling users to manipulate visual content such as identity and style. However, achieving simultaneous control over the 9D poses (location, size, and orientation) of multiple objects remains an open challenge. Despite recent progress, existing methods often suffer from limited controllability and degraded quality, falling short of comprehensive multi-object 9D pose control. To address these limitations, we propose SceneDesigner, a method for accurate and flexible multi-object 9-DoF pose manipulation. SceneDesigner incorporates a branched network to the pre-trained base model and leverages a new representation, CNOCS map, which encodes 9D pose information from the camera view. This representation exhibits strong geometric interpretation properties, leading to more efficient and stable training. To support training, we construct a new dataset, ObjectPose9D, which aggregates images from diverse sources along with 9D pose annotations. To further address data imbalance issues, particularly performance degradation on low-frequency poses, we introduce a two-stage training strategy with reinforcement learning, where the second stage fine-tunes the model using a reward-based objective on rebalanced data. At inference time, we propose Disentangled Object Sampling, a technique that mitigates insufficient object generation and concept confusion in complex multi-object scenes. Moreover, by integrating user-specific personalization weights, SceneDesigner enables customized pose control for reference subjects. Extensive qualitative and quantitative experiments demonstrate that SceneDesigner significantly outperforms existing approaches in both controllability and quality. Code is publicly available at https://github.com/FudanCVL/SceneDesigner.",
        "translated": "可控图像生成近年来备受关注，使用户能够操纵诸如身份和风格等视觉内容。然而，同时控制多个物体的九维姿态（位置、尺寸与朝向）仍是一个开放性挑战。尽管近期取得一定进展，现有方法往往存在控制能力有限、生成质量下降等问题，难以实现全面的多物体九维姿态控制。为解决上述局限，我们提出SceneDesigner，一种用于精确且灵活的多物体九自由度姿态操控的方法。SceneDesigner在预训练基础模型上引入分支网络，并利用一种新的表示形式——CNOCS图，该图从相机视角编码九维姿态信息。该表示具有强几何可解释性，从而带来更高效且稳定的训练效果。为支持训练，我们构建了一个新数据集ObjectPose9D，整合自多种来源的图像及其对应的九维姿态标注。为进一步缓解数据不平衡问题，特别是低频姿态下的性能退化现象，我们引入了一种两阶段训练策略，结合强化学习：第二阶段基于重新平衡后的数据，使用奖励机制对模型进行微调。在推理阶段，我们提出解耦对象采样（Disentangled Object Sampling）技术，有效缓解复杂多物体场景中对象生成不足及概念混淆的问题。此外，通过集成用户特定个性化权重，SceneDesigner可实现对参考主体的定制化姿态控制。大量定性与定量实验表明，SceneDesigner在可控性与质量方面显著优于现有方法。代码已开源至 https://github.com/FudanCVL/SceneDesigner。",
        "translated_title": "SceneDesigner：基于9自由度姿态操控的可控多物体图像生成",
        "label": [],
        "label_reason": "生成新图像，非像素级图像恢复任务",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "提出新控制框架与训练策略提升多物体姿态操控"
    },
    {
        "title": "TriDiff-4D: Fast 4D Generation through Diffusion-based Triplane Re-posing",
        "url": "http://arxiv.org/abs/2511.16662v1",
        "pub_date": "2025-11-20",
        "summary": "With the increasing demand for 3D animation, generating high-fidelity, controllable 4D avatars from textual descriptions remains a significant challenge. Despite notable efforts in 4D generative modeling, existing methods exhibit fundamental limitations that impede their broader applicability, including temporal and geometric inconsistencies, perceptual artifacts, motion irregularities, high computational costs, and limited control over dynamics. To address these challenges, we propose TriDiff-4D, a novel 4D generative pipeline that employs diffusion-based triplane re-posing to produce high-quality, temporally coherent 4D avatars. Our model adopts an auto-regressive strategy to generate 4D sequences of arbitrary length, synthesizing each 3D frame with a single diffusion process. By explicitly learning 3D structure and motion priors from large-scale 3D and motion datasets, TriDiff-4D enables skeleton-driven 4D generation that excels in temporal consistency, motion accuracy, computational efficiency, and visual fidelity. Specifically, TriDiff-4D first generates a canonical 3D avatar and a corresponding motion sequence from a text prompt, then uses a second diffusion model to animate the avatar according to the motion sequence, supporting arbitrarily long 4D generation. Experimental results demonstrate that TriDiff-4D significantly outperforms existing methods, reducing generation time from hours to seconds by eliminating the optimization process, while substantially improving the generation of complex motions with high-fidelity appearance and accurate 3D geometry.",
        "translated": "随着对3D动画需求的不断增加，从文本描述生成高保真、可控的4D虚拟人像仍是一项重大挑战。尽管在4D生成建模方面已取得显著进展，现有方法仍存在根本性局限，阻碍其广泛应用，包括时间与几何不一致性、感知伪影、运动异常、计算成本高昂以及对动态控制能力有限等问题。为解决上述挑战，我们提出TriDiff-4D，这是一种新颖的4D生成管线，通过基于扩散的三平面重定位技术，生成高质量且具时间一致性的4D虚拟人像。我们的模型采用自回归策略，可生成任意长度的4D序列，每个3D帧均通过单次扩散过程合成。通过从大规模3D结构与运动数据集中显式学习3D结构与运动先验，TriDiff-4D实现了由骨架驱动的4D生成，在时间一致性、运动精度、计算效率和视觉保真度方面表现卓越。具体而言，TriDiff-4D首先根据文本提示生成一个标准3D虚拟人像及其对应运动序列，随后使用第二个扩散模型依据该运动序列驱动虚拟人像动画，支持任意长度的4D生成。实验结果表明，TriDiff-4D显著优于现有方法，通过消除优化过程，将生成时间从数小时缩短至数秒，同时大幅提升了复杂运动的生成质量，兼具高保真外观与精确三维几何结构。",
        "translated_title": "TriDiff-4D：基于扩散的三平面重排实现快速四维生成",
        "label": [],
        "label_reason": "生成4D动画属高阶视觉任务，非像素级图像恢复",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "创新扩散+三平面重定位架构，提升生成效率与质量"
    },
    {
        "title": "PartUV: Part-Based UV Unwrapping of 3D Meshes",
        "url": "http://arxiv.org/abs/2511.16659v1",
        "pub_date": "2025-11-20",
        "summary": "UV unwrapping flattens 3D surfaces to 2D with minimal distortion, often requiring the complex surface to be decomposed into multiple charts. Although extensively studied, existing UV unwrapping methods frequently struggle with AI-generated meshes, which are typically noisy, bumpy, and poorly conditioned. These methods often produce highly fragmented charts and suboptimal boundaries, introducing artifacts and hindering downstream tasks. We introduce PartUV, a part-based UV unwrapping pipeline that generates significantly fewer, part-aligned charts while maintaining low distortion. Built on top of a recent learning-based part decomposition method PartField, PartUV combines high-level semantic part decomposition with novel geometric heuristics in a top-down recursive framework. It ensures each chart's distortion remains below a user-specified threshold while minimizing the total number of charts. The pipeline integrates and extends parameterization and packing algorithms, incorporates dedicated handling of non-manifold and degenerate meshes, and is extensively parallelized for efficiency. Evaluated across four diverse datasets, including man-made, CAD, AI-generated, and Common Shapes, PartUV outperforms existing tools and recent neural methods in chart count and seam length, achieves comparable distortion, exhibits high success rates on challenging meshes, and enables new applications like part-specific multi-tiles packing. Our project page is at https://www.zhaoningwang.com/PartUV.",
        "translated": "UV展开将三维表面映射为二维平面，且尽量减少形变，通常需要将复杂的表面分解为多个图表。尽管已有大量研究，现有UV展开方法在处理AI生成的网格时往往表现不佳，这类网格通常存在噪声、凹凸不平且条件差。这些方法常导致图表高度碎片化，边界次优，引入伪影并阻碍下游任务。我们提出了PartUV，一种基于部件的UV展开流程，能够生成更少、与部件对齐的图表，同时保持低形变。该方法构建于近期基于学习的部件分解方法PartField之上，结合高层语义部件分解与新颖几何启发式方法，在自顶向下的递归框架中实现协同优化。它确保每个图表的形变低于用户指定阈值，同时最小化图表总数。该流程整合并扩展了参数化与打包算法，专门处理非流形和退化网格，并全面并行化以提升效率。在包括人工制品、CAD模型、AI生成及常见形状在内的四个多样化数据集上评估，PartUV在图表数量与接缝长度方面优于现有工具与近期神经方法，形变水平相当，对复杂网格成功率高，且支持如部件特定多块打包等新应用场景。项目主页位于 https://www.zhaoningwang.com/PartUV。",
        "translated_title": "PartUV：基于部分的3D网格UV展开",
        "label": [],
        "label_reason": "处理3D网格UV展开，非图像像素级恢复任务",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "结合语义分割与几何启发式，优化图表生成"
    },
    {
        "title": "Solving Spatial Supersensing Without Spatial Supersensing",
        "url": "http://arxiv.org/abs/2511.16655v1",
        "pub_date": "2025-11-20",
        "summary": "Cambrian-S aims to take the first steps towards improving video world models with spatial supersensing by introducing (i) two benchmarks, VSI-Super-Recall (VSR) and VSI-Super-Counting (VSC), and (ii) bespoke predictive sensing inference strategies tailored to each benchmark. In this work, we conduct a critical analysis of Cambrian-S across both these fronts. First, we introduce a simple baseline, NoSense, which discards almost all temporal structure and uses only a bag-of-words SigLIP model, yet near-perfectly solves VSR, achieving 95% accuracy even on 4-hour videos. This shows benchmarks like VSR can be nearly solved without spatial cognition, world modeling or spatial supersensing. Second, we hypothesize that the tailored inference methods proposed by Cambrian-S likely exploit shortcut heuristics in the benchmark. We illustrate this with a simple sanity check on the VSC benchmark, called VSC-Repeat: We concatenate each video with itself 1-5 times, which does not change the number of unique objects. However, this simple perturbation entirely collapses the mean relative accuracy of Cambrian-S from 42% to 0%. A system that performs spatial supersensing and integrates information across experiences should recognize views of the same scene and keep object-count predictions unchanged; instead, Cambrian-S inference algorithm relies largely on a shortcut in the VSC benchmark that rooms are never revisited. Taken together, our findings suggest that (i) current VSI-Super benchmarks do not yet reliably measure spatial supersensing, and (ii) predictive-sensing inference recipes used by Cambrian-S improve performance by inadvertently exploiting shortcuts rather than from robust spatial supersensing. We include the response from the Cambrian-S authors (in Appendix A) to provide a balanced perspective alongside our claims. We release our code at: https://github.com/bethgelab/supersanity",
        "translated": "Cambrian-S 旨在通过引入（i）两个基准测试，即 VSI-Super-Recall（VSR）和 VSI-Super-Counting（VSC），以及（ii）针对每个基准量身定制的预测感知推理策略，迈出利用空间超感知改进视频世界模型的第一步。在本研究中，我们从这两个方面对 Cambrian-S 进行了关键性分析。首先，我们提出了一种简单基线方法 NoSense，该方法几乎完全丢弃时间结构，仅使用 bag-of-words SigLIP 模型，却近乎完美地解决了 VSR，即使在长达 4 小时的视频上仍达到 95% 的准确率。这一结果表明，像 VSR 这样的基准测试可能无需空间认知、世界建模或空间超感知即可被近似解决。其次，我们假设 Cambrian-S 提出的定制化推理方法很可能依赖于基准中的捷径启发式方法。我们通过对 VSC 基准进行一个简单的合理性检验——称为 VSC-Repeat——来验证这一点：我们将每个视频与其自身拼接 1 至 5 次，这不会改变场景中唯一物体的数量；然而，这种简单的扰动使 Cambrian-S 的平均相对准确率从 42% 急剧下降至 0%。一个具备空间超感知能力并能跨经验整合信息的系统应当识别同一场景的不同视图，并保持物体计数预测不变；但 Cambrian-S 的推理算法很大程度上依赖于 VSC 基准中的一个捷径，即“房间不会被重复访问”。综合来看，我们的发现表明：（i）当前的 VSI-Super 基准尚不能可靠衡量空间超感知能力；（ii）Cambrian-S 使用的预测感知推理方案所获得的性能提升，源于其无意中利用了基准中的捷径而非真正稳健的空间超感知能力。我们将在附录 A 中提供 Cambrian-S 作者的回应，以与我们的观点形成平衡视角。我们的代码已发布于：https://github.com/bethgelab/supersanity",
        "translated_title": "无需空间超采样的空间超感知",
        "label": [],
        "label_reason": "研究视频世界模型与基准测试，非图像像素级处理",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "提出基准与基线，但无图像恢复创新"
    },
    {
        "title": "Teacher-Guided One-Shot Pruning via Context-Aware Knowledge Distillation",
        "url": "http://arxiv.org/abs/2511.16653v1",
        "pub_date": "2025-11-20",
        "summary": "Unstructured pruning remains a powerful strategy for compressing deep neural networks, yet it often demands iterative train-prune-retrain cycles, resulting in significant computational overhead. To address this challenge, we introduce a novel teacher-guided pruning framework that tightly integrates Knowledge Distillation (KD) with importance score estimation. Unlike prior approaches that apply KD as a post-pruning recovery step, our method leverages gradient signals informed by the teacher during importance score calculation to identify and retain parameters most critical for both task performance and knowledge transfer. Our method facilitates a one-shot global pruning strategy that efficiently eliminates redundant weights while preserving essential representations. After pruning, we employ sparsity-aware retraining with and without KD to recover accuracy without reactivating pruned connections. Comprehensive experiments across multiple image classification benchmarks, including CIFAR-10, CIFAR-100, and TinyImageNet, demonstrate that our method consistently achieves high sparsity levels with minimal performance degradation. Notably, our approach outperforms state-of-the-art baselines such as EPG and EPSD at high sparsity levels, while offering a more computationally efficient alternative to iterative pruning schemes like COLT. The proposed framework offers a computation-efficient, performance-preserving solution well suited for deployment in resource-constrained environments.",
        "translated": "无结构化剪枝仍是压缩深度神经网络的一种强大策略，但其通常需要反复进行训练-剪枝-再训练的循环，导致显著的计算开销。为应对这一挑战，我们提出了一种新颖的教师引导剪枝框架，将知识蒸馏（KD）与重要性评分估计紧密集成。与以往将KD作为剪枝后恢复步骤的方法不同，我们的方法在计算重要性评分时利用教师模型提供的梯度信号，以识别并保留对任务性能和知识迁移均至关重要的参数。该方法支持一次性全局剪枝策略，在高效移除冗余权重的同时保留关键表征。剪枝后，我们采用稀疏感知重训练（有无KD）来恢复精度，而不重新激活被剪枝的连接。在多个图像分类基准数据集（包括CIFAR-10、CIFAR-100和TinyImageNet）上的全面实验表明，我们的方法可稳定实现高稀疏度，且性能退化极小。值得注意的是，在高稀疏度下，我们的方法优于当前最先进的基线方法（如EPG和EPSD），同时比迭代式剪枝方案（如COLT）提供更高效的计算替代方案。所提出的框架是一种计算效率高、性能保持良好的解决方案，特别适用于资源受限环境下的部署。",
        "translated_title": "基于上下文感知知识蒸馏的教师引导单次剪枝方法",
        "label": [],
        "label_reason": "属于模型压缩，非图像像素级处理",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出教师引导剪枝新框架，提升效率"
    },
    {
        "title": "Late-decoupled 3D Hierarchical Semantic Segmentation with Semantic Prototype Discrimination based Bi-branch Supervision",
        "url": "http://arxiv.org/abs/2511.16650v1",
        "pub_date": "2025-11-20",
        "summary": "3D hierarchical semantic segmentation (3DHS) is crucial for embodied intelligence applications that demand a multi-grained and multi-hierarchy understanding of 3D scenes. Despite the progress, previous 3DHS methods have overlooked following two challenges: I) multi-label learning with a parameter-sharing model can lead to multi-hierarchy conflicts in cross-hierarchy optimization, and II) the class imbalance issue is inevitable across multiple hierarchies of 3D scenes, which makes the model performance become dominated by major classes. To address these issues, we propose a novel framework with a primary 3DHS branch and an auxiliary discrimination branch. Specifically, to alleviate the multi-hierarchy conflicts, we propose a late-decoupled 3DHS framework which employs multiple decoders with the coarse-to-fine hierarchical guidance and consistency. The late-decoupled architecture can mitigate the underfitting and overfitting conflicts among multiple hierarchies and can also constrain the class imbalance problem in each individual hierarchy. Moreover, we introduce a 3DHS-oriented semantic prototype based bi-branch supervision mechanism, which additionally learns class-wise discriminative point cloud features and performs mutual supervision between the auxiliary and 3DHS branches, to enhance the class-imbalance segmentation. Extensive experiments on multiple datasets and backbones demonstrate that our approach achieves state-of-the-art 3DHS performance, and its core components can also be used as a plug-and-play enhancement to improve previous methods.",
        "translated": "三维层次语义分割（3DHS）对于需要对三维场景进行多粒度、多层次理解的具身智能应用至关重要。尽管已有进展，先前的3DHS方法仍忽视了以下两个挑战：I）采用参数共享模型的多标签学习会在跨层次优化过程中引发多层次冲突；II）三维场景在多个层次上不可避免地存在类别不平衡问题，导致模型性能被主导类别所控制。为解决这些问题，我们提出了一种新颖框架，包含主3DHS分支与辅助判别分支。具体而言，为缓解多层次冲突，我们设计了一种晚期解耦的3DHS框架，该框架采用多个解码器，并结合自粗至细的层次引导与一致性约束。该晚期解耦架构可减轻各层次间的欠拟合与过拟合冲突，同时也能约束每一独立层次内的类别不平衡问题。此外，我们引入了一种面向3DHS的语义原型双分支监督机制，该机制额外学习类别特定的点云特征，并在辅助分支与3DHS分支间实现相互监督，以增强类别不平衡场景下的分割性能。在多个数据集和骨干网络上的广泛实验表明，我们的方法实现了当前最优的3DHS性能，且其核心模块亦可作为即插即用的增强组件，用于提升现有方法的效果。",
        "translated_title": "基于语义原型判别双分支监督的后期解耦三维层次语义分割",
        "label": [],
        "label_reason": "属于高阶语义分割任务，非像素级图像处理",
        "relevance_score": 1,
        "novelty_score": 3,
        "novelty_reason": "方法为多分支监督框架，无本质创新"
    },
    {
        "title": "TRIM: Scalable 3D Gaussian Diffusion Inference with Temporal and Spatial Trimming",
        "url": "http://arxiv.org/abs/2511.16642v1",
        "pub_date": "2025-11-20",
        "summary": "Recent advances in 3D Gaussian diffusion models suffer from time-intensive denoising and post-denoising processing due to the massive number of Gaussian primitives, resulting in slow generation and limited scalability along sampling trajectories. To improve the efficiency of 3D diffusion models, we propose $\\textbf{TRIM}$ ($\\textbf{T}$rajectory $\\textbf{R}$eduction and $\\textbf{I}$nstance $\\textbf{M}$ask denoising), a post-training approach that incorporates both temporal and spatial trimming strategies, to accelerate inference without compromising output quality while supporting the inference-time scaling for Gaussian diffusion models. Instead of scaling denoising trajectories in a costly end-to-end manner, we develop a lightweight selector model to evaluate latent Gaussian primitives derived from multiple sampled noises, enabling early trajectory reduction by selecting candidates with high-quality potential. Furthermore, we introduce instance mask denoising to prune learnable Gaussian primitives by filtering out redundant background regions, reducing inference computation at each denoising step. Extensive experiments and analysis demonstrate that TRIM significantly improves both the efficiency and quality of 3D generation. Source code is available at $\\href{https://github.com/zeyuanyin/TRIM}{link}$.",
        "translated": "近年来，3D 高斯扩散模型在处理大量高斯基元时面临耗时的去噪及后处理问题，导致生成速度缓慢且沿采样轨迹扩展性受限。为提升 3D 扩散模型的效率，我们提出 $\\textbf{TRIM}$（$\\textbf{T}$rajectory $\\textbf{R}$eduction and $\\textbf{I}$nstance $\\textbf{M}$ask denoising），一种后训练方法，结合了时间与空间维度的裁剪策略，在不降低输出质量的前提下加速推理，并支持高斯扩散模型在推理阶段的可扩展性。不同于以往以高昂代价进行端到端的去噪轨迹扩展，我们设计了一个轻量级选择器模型，用于评估由多个采样噪声衍生出的潜在高斯基元，通过筛选具有高质量潜力的候选基元，实现早期轨迹缩减。此外，我们引入实例掩码去噪机制，通过过滤冗余背景区域，修剪可学习的高斯基元，从而在每一步去噪过程中减少计算开销。大量实验和分析表明，TRIM 显著提升了 3D 生成的效率与质量。源代码详见 $\\href{https://github.com/zeyuanyin/TRIM}{link}$。",
        "translated_title": "TRIM：结合时域与空域剪裁的可扩展三维高斯扩散推理方法",
        "label": [],
        "label_reason": "处理3D生成效率，非图像像素级恢复任务",
        "relevance_score": 3,
        "novelty_score": 7,
        "novelty_reason": "提出轻量选择器与实例掩码剪枝，提升推理效率"
    },
    {
        "title": "SurvAgent: Hierarchical CoT-Enhanced Case Banking and Dichotomy-Based Multi-Agent System for Multimodal Survival Prediction",
        "url": "http://arxiv.org/abs/2511.16635v1",
        "pub_date": "2025-11-20",
        "summary": "Survival analysis is critical for cancer prognosis and treatment planning, yet existing methods lack the transparency essential for clinical adoption. While recent pathology agents have demonstrated explainability in diagnostic tasks, they face three limitations for survival prediction: inability to integrate multimodal data, ineffective region-of-interest exploration, and failure to leverage experiential learning from historical cases. We introduce SurvAgent, the first hierarchical chain-of-thought (CoT)-enhanced multi-agent system for multimodal survival prediction. SurvAgent consists of two stages: (1) WSI-Gene CoT-Enhanced Case Bank Construction employs hierarchical analysis through Low-Magnification Screening, Cross-Modal Similarity-Aware Patch Mining, and Confidence-Aware Patch Mining for pathology images, while Gene-Stratified analysis processes six functional gene categories. Both generate structured reports with CoT reasoning, storing complete analytical processes for experiential learning. (2) Dichotomy-Based Multi-Expert Agent Inference retrieves similar cases via RAG and integrates multimodal reports with expert predictions through progressive interval refinement. Extensive experiments on five TCGA cohorts demonstrate SurvAgent's superority over conventional methods, proprietary MLLMs, and medical agents, establishing a new paradigm for explainable AI-driven survival prediction in precision oncology.",
        "translated": "生存分析对于癌症预后和治疗方案制定至关重要，但现有方法缺乏临床应用所必需的透明性。尽管近期病理学代理模型已在诊断任务中展现出可解释性，但在生存预测方面仍面临三大局限：无法整合多模态数据、难以有效探索感兴趣区域、未能利用历史病例的经验学习能力。我们提出 SurvAgent，这是首个采用分层链式思维（CoT）增强的多智能体系统，用于多模态生存预测。SurvAgent 包含两个阶段：（1）WSI-基因 CoT 增强病例库构建阶段，通过低倍筛查、跨模态相似性感知块采样及置信度感知块采样对病理图像进行分层分析；同时，按基因分层分析处理六个功能基因类别。二者均生成包含 CoT 推理过程的结构化报告，完整存储分析流程以支持经验学习。（2）基于二分法的多专家智能体推理阶段，通过 RAG 检索相似病例，并通过渐进区间精炼机制融合多模态报告与专家预测。在五个 TCGA 数据集中广泛实验表明，SurvAgent 在性能上显著优于传统方法、专有 MLLMs 及现有医学代理模型，为精准肿瘤学中的可解释人工智能驱动的生存预测建立了全新范式。",
        "translated_title": "SurvAgent：基于层次化思维链增强的案例库与二分多智能体系统，用于多模态生存预测",
        "label": [],
        "label_reason": "任务为生存预测，属高阶医学决策分析",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "多智能体架构创新，但非图像处理任务"
    },
    {
        "title": "SAM 3D: 3Dfy Anything in Images",
        "url": "http://arxiv.org/abs/2511.16624v1",
        "pub_date": "2025-11-20",
        "summary": "We present SAM 3D, a generative model for visually grounded 3D object reconstruction, predicting geometry, texture, and layout from a single image. SAM 3D excels in natural images, where occlusion and scene clutter are common and visual recognition cues from context play a larger role. We achieve this with a human- and model-in-the-loop pipeline for annotating object shape, texture, and pose, providing visually grounded 3D reconstruction data at unprecedented scale. We learn from this data in a modern, multi-stage training framework that combines synthetic pretraining with real-world alignment, breaking the 3D \"data barrier\". We obtain significant gains over recent work, with at least a 5:1 win rate in human preference tests on real-world objects and scenes. We will release our code and model weights, an online demo, and a new challenging benchmark for in-the-wild 3D object reconstruction.",
        "translated": "我们提出 SAM 3D，这是一种基于视觉引导的 3D 物体重建生成模型，能够从单张图像预测几何结构、纹理与布局。SAM 3D 在自然图像上表现优异，因为此类图像中遮挡与场景杂乱普遍存在，且来自上下文的视觉识别线索起着更为关键的作用。我们通过一种“人-模型协同标注”流程来标注物体形状、纹理与姿态，从而构建了迄今为止规模空前的视觉引导式 3D 重建数据集。我们在此数据基础上采用现代多阶段训练框架进行学习，该框架结合合成数据预训练与真实世界对齐，突破了 3D 领域长期存在的“数据壁垒”。我们在真实世界物体与场景的人类偏好测试中显著优于近期工作，胜率至少达到 5:1。我们将发布我们的代码与模型权重、在线演示系统，并提供一个用于野外 3D 物体重建的新挑战性基准数据集。",
        "translated_title": "SAM 3D：在图像中对任意内容进行三维化",
        "label": [],
        "label_reason": "3D重建属高阶视觉任务，非像素级图像恢复",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "提出多阶段训练框架与大规模数据集创新"
    },
    {
        "title": "SAM2S: Segment Anything in Surgical Videos via Semantic Long-term Tracking",
        "url": "http://arxiv.org/abs/2511.16618v1",
        "pub_date": "2025-11-20",
        "summary": "Surgical video segmentation is crucial for computer-assisted surgery, enabling precise localization and tracking of instruments and tissues. Interactive Video Object Segmentation (iVOS) models such as Segment Anything Model 2 (SAM2) provide prompt-based flexibility beyond methods with predefined categories, but face challenges in surgical scenarios due to the domain gap and limited long-term tracking. To address these limitations, we construct SA-SV, the largest surgical iVOS benchmark with instance-level spatio-temporal annotations (masklets) spanning eight procedure types (61k frames, 1.6k masklets), enabling comprehensive development and evaluation for long-term tracking and zero-shot generalization. Building on SA-SV, we propose SAM2S, a foundation model enhancing \\textbf{SAM2} for \\textbf{S}urgical iVOS through: (1) DiveMem, a trainable diverse memory mechanism for robust long-term tracking; (2) temporal semantic learning for instrument understanding; and (3) ambiguity-resilient learning to mitigate annotation inconsistencies across multi-source datasets. Extensive experiments demonstrate that fine-tuning on SA-SV enables substantial performance gains, with SAM2 improving by 12.99 average $\\mathcal{J}$\\&amp;$\\mathcal{F}$ over vanilla SAM2. SAM2S further advances performance to 80.42 average $\\mathcal{J}$\\&amp;$\\mathcal{F}$, surpassing vanilla and fine-tuned SAM2 by 17.10 and 4.11 points respectively, while maintaining 68 FPS real-time inference and strong zero-shot generalization. Code and dataset will be released at https://jinlab-imvr.github.io/SAM2S.",
        "translated": "手术视频分割对于计算机辅助手术至关重要，能够实现器械与组织的精确定位与跟踪。交互式视频目标分割（iVOS）模型如 Segment Anything Model 2（SAM2）相比基于预定义类别的方法提供了基于提示的灵活性，但在手术场景中仍面临领域差异和长期跟踪能力不足等挑战。为解决这些局限性，我们构建了 SA-SV，这是目前规模最大的手术 iVOS 数据集，包含八种手术类型实例级时空标注（masklets），总计 61,000 帧、1,600 个 masklets，支持对长期跟踪与零样本泛化能力的全面开发与评估。基于 SA-SV，我们提出 SAM2S，一种面向手术 iVOS 的基础模型，通过以下三点增强原始 SAM2：（1）DiveMem，一种可训练的多样化记忆机制，以提升鲁棒的长期跟踪能力；（2）时序语义学习，用于理解器械语义；（3）抗歧义学习，缓解多源数据集中标注不一致性问题。大量实验表明，在 SA-SV 上微调显著提升了性能，SAM2 的平均 $\\mathcal{J}$\\&$\\mathcal{F}$ 指标相较原始 SAM2 提升 12.99。SAM2S 进一步将性能提升至平均 80.42 $\\mathcal{J}$\\&$\\mathcal{F}$，分别超越原始 SAM2 和微调后的 SAM2 17.10 和 4.11 分数，同时保持 68 FPS 实时推理速度和强大的零样本泛化能力。代码与数据集将在 https://jinlab-imvr.github.io/SAM2S 发布。",
        "translated_title": "SAM2S：通过语义长期跟踪实现手术视频中的任意实例分割",
        "label": [],
        "label_reason": "目标为手术视频语义分割，属高阶视觉任务",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "改进SAM2的跟踪机制，非像素级图像恢复"
    },
    {
        "title": "TimeViper: A Hybrid Mamba-Transformer Vision-Language Model for Efficient Long Video Understanding",
        "url": "http://arxiv.org/abs/2511.16595v1",
        "pub_date": "2025-11-20",
        "summary": "We introduce TimeViper, a hybrid vision-language model designed to tackle challenges of long video understanding. Processing long videos demands both an efficient model architecture and an effective mechanism for handling extended temporal contexts. To this end, TimeViper adopts a hybrid Mamba-Transformer backbone that combines the efficiency of state-space models with the expressivity of attention mechanisms. Through this hybrid design, we reveal the vision-to-text information aggregation phenomenon, where information progressively flows from vision tokens to text tokens across increasing LLM depth, resulting in severe vision token redundancy. Motivated by this observation, we propose TransV, a token information transfer module that transfers and compresses vision tokens into instruction tokens while maintaining multimodal understanding capabilities. This design enables TimeViper to process hour-long videos exceeding 10,000 frames. Extensive experiments across multiple benchmarks demonstrate that TimeViper competes with state-of-the-art models while extending frame numbers. We further analyze attention behaviors of both Mamba and Transformer layers, offering new insights into hybrid model interpretability. This work represents an initial step towards developing, interpreting, and compressing hybrid Mamba-Transformer architectures.",
        "translated": "我们提出 TimeViper，一种设计用于应对长视频理解挑战的混合视觉-语言模型。处理长视频需要兼顾高效的模型架构与有效处理扩展时序上下文的机制。为此，TimeViper 采用融合状态空间模型效率与注意力机制表达能力的混合 Mamba-Transformer 主干网络。通过这一混合设计，我们揭示了视觉到文本信息聚合现象：随着大语言模型（LLM）深度增加，视觉 token 的信息逐步向文本 token 聚合，导致视觉 token 严重冗余。受此观察启发，我们提出 TransV，一种视觉 token 信息传输模块，该模块将视觉 token 转移并压缩为指令 token，同时保持多模态理解能力。该设计使 TimeViper 可处理超过 10,000 帧、长达数小时的视频。在多个基准测试上的广泛实验表明，TimeViper 在帧数扩展的同时可与当前最先进模型竞争性能。我们进一步分析了 Mamba 和 Transformer 层的注意力行为，为混合模型可解释性提供了新见解。本工作代表了开发、解释和压缩混合 Mamba-Transformer 架构的初步步骤。",
        "translated_title": "TimeViper：一种用于高效长视频理解的混合Mamba-Transformer视觉-语言模型",
        "label": [],
        "label_reason": "属于高阶视频理解任务，非像素级图像处理",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "提出混合架构与信息压缩模块，提升长视频效率"
    },
    {
        "title": "Green Resilience of Cyber-Physical Systems: Doctoral Dissertation",
        "url": "http://arxiv.org/abs/2511.16593v1",
        "pub_date": "2025-11-20",
        "summary": "Cyber-physical systems (CPS) combine computational and physical components. Online Collaborative AI System (OL-CAIS) is a type of CPS that learn online in collaboration with humans to achieve a common goal, which makes it vulnerable to disruptive events that degrade performance. Decision-makers must therefore restore performance while limiting energy impact, creating a trade-off between resilience and greenness. This research addresses how to balance these two properties in OL-CAIS. It aims to model resilience for automatic state detection, develop agent-based policies that optimize the greenness-resilience trade-off, and understand catastrophic forgetting to maintain performance consistency. We model OL-CAIS behavior through three operational states: steady, disruptive, and final. To support recovery during disruptions, we introduce the GResilience framework, which provides recovery strategies through multi-objective optimization (one-agent), game-theoretic decision-making (two-agent), and reinforcement learning (RL-agent). We also design a measurement framework to quantify resilience and greenness. Empirical evaluation uses real and simulated experiments with a collaborative robot learning object classification from human demonstrations. Results show that the resilience model captures performance transitions during disruptions, and that GResilience policies improve green recovery by shortening recovery time, stabilizing performance, and reducing human dependency. RL-agent policies achieve the strongest results, although with a marginal increase in CO2 emissions. We also observe catastrophic forgetting after repeated disruptions, while our policies help maintain steadiness. A comparison with containerized execution shows that containerization cuts CO2 emissions by half. Overall, this research provides models, metrics, and policies that ensure the green recovery of OL-CAIS.",
        "translated": "物理信息系统（CPS）融合了计算与物理组件。在线协作人工智能系统（OL-CAIS）是一种CPS，它通过与人类协同学习，在线实现共同目标，因此易受性能退化性事件的干扰。决策者必须在恢复性能的同时限制能源影响，从而在韧性与绿色性之间取得权衡。本研究旨在探讨如何在OL-CAIS中平衡这两种特性。其目标包括：建立韧性模型以实现自动状态检测；设计基于代理的策略，优化绿色性-韧性权衡；并理解灾难性遗忘现象，以维持性能一致性。我们通过三种运行状态建模OL-CAIS行为：稳定态、干扰态和终末态。为支持干扰期间的恢复，我们提出GResilience框架，该框架通过多目标优化（单代理）、博弈论决策（双代理）和强化学习（RL代理）提供恢复策略。同时，我们设计了一套度量框架，用于量化韧性和绿色性。实证评估采用真实与模拟实验，其中协作机器人从人类示范中学习物体分类任务。结果表明，韧性模型能够捕捉干扰期间性能的转变过程；GResilience策略通过缩短恢复时间、稳定性能及降低对人类依赖性，提升了绿色恢复效果。RL代理策略表现最优，但伴随轻微的二氧化碳排放增加。此外，我们在重复干扰后观察到灾难性遗忘现象，而我们的策略有助于维持系统稳定性。与容器化执行方式对比发现，容器化可使二氧化碳排放量减少一半。总体而言，本研究提供了适用于OL-CAIS绿色恢复的模型、度量标准与策略。",
        "translated_title": "网络物理系统的绿色韧性：博士论文",
        "label": [],
        "label_reason": "研究聚焦系统韧性与能效，非图像像素级处理",
        "relevance_score": 1,
        "novelty_score": 1,
        "novelty_reason": "方法针对AI系统恢复策略，无图像处理创新"
    },
    {
        "title": "Erase to Retain: Low Rank Adaptation Guided Selective Unlearning in Medical Segmentation Networks",
        "url": "http://arxiv.org/abs/2511.16574v1",
        "pub_date": "2025-11-20",
        "summary": "The ability to selectively remove knowledge from medical segmentation networks is increasingly important for privacy compliance, ethical deployment, and continual dataset revision. We introduce Erase to Retain, a controllable unlearning framework for medical image segmentation that achieves targeted forgetting without full retraining. Our method uses a teacher-student distillation paradigm with Low-Rank Adaptation (LoRA) constrained subspace updates, enabling the student network to erase lesion-specific or class-specific representations in low-rank decoder spaces while preserving global anatomical understanding. During the strong unlearning phase, LoRA modules are adversarially optimized to contradict the teacher's confident predictions on a designated forget subset, enforcing semantic removal. This is followed by a gentle restoration phase that recovers generalization on retained data through head-only supervised refinement.   For ISIC segmentation, the student reduces forget-set IoU from 0.875 to 0.509 while maintaining competitive performance on the retain and validation splits (0.647 to 0.677 IoU). On the cross-domain CHASE dataset, Erase to Retain consistently lowers forget-set IoU while preserving utility on retain and validation sets. For ISIC classification, our method decreases accuracy on the forget subset from 87.0 percent to 64.1 percent while improving retain accuracy from 83.9 percent to 90.6 percent.   These results demonstrate that LoRA-based subspace unlearning provides a practical pathway toward responsible, controllable, and reversible unlearning in medical image analysis, enabling models to forget sensitive samples or structures while preserving performance where it matters most.",
        "translated": "选择性地从医学分割网络中移除特定知识，对于隐私合规、伦理部署及持续数据集修订日益重要。我们提出了“Erase to Retain”框架，这是一种可控的模型遗忘机制，可在无需完整重训练的前提下实现目标性遗忘。该方法采用教师-学生蒸馏范式，并结合低秩自适应（LoRA）约束的子空间更新策略，使学生网络能够在低秩解码器空间中擦除与病变或类别相关的表征，同时保留全局解剖结构理解。在强遗忘阶段，LoRA模块通过对抗性优化被调整以与教师网络对指定遗忘样本集的置信预测相矛盾，从而强制语义信息的移除；随后进入温和恢复阶段，仅通过头部监督微调，即可在保留数据上恢复泛化能力。\n\n在ISIC分割任务中，学生网络将遗忘样本集的IoU从0.875降至0.509，同时在保留集和验证集上保持竞争性性能（IoU分别为0.647至0.677）。在跨域CHASE数据集上，“Erase to Retain”始终降低遗忘样本集的IoU，同时维持保留集与验证集上的实用性能。在ISIC分类任务中，本方法将遗忘样本集的准确率从87.0%降至64.1%，同时将保留集准确率从83.9%提升至90.6%。\n\n上述结果表明，基于LoRA的子空间遗忘方法为医学图像分析中的负责任、可控且可逆的模型遗忘提供了一条切实可行的路径，使模型能够遗忘敏感样本或结构，同时在关键应用场景中维持性能。",
        "translated_title": "擦除以保留：基于低秩自适应引导的医学分割网络选择性遗忘",
        "label": [],
        "label_reason": "目标为模型知识卸载，非图像像素级恢复",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "LoRA约束子空间更新提升卸载可控性"
    },
    {
        "title": "PolyMinHash: Efficient Area-Based MinHashing of Polygons for Approximate Nearest Neighbor Search",
        "url": "http://arxiv.org/abs/2511.16576v1",
        "pub_date": "2025-11-20",
        "summary": "Similarity searches are a critical task in data mining. As data sets grow larger, exact nearest neighbor searches quickly become unfeasible, leading to the adoption of approximate nearest neighbor (ANN) searches. ANN has been studied for text data, images, and trajectories. However, there has been little effort to develop ANN systems for polygons in spatial database systems and geographic information systems. We present PolyMinHash, a system for approximate polygon similarity search that adapts MinHashing into a novel 2D polygon-hashing scheme to generate short, similarity-preserving signatures of input polygons. Minhash is generated by counting the number of randomly sampled points needed before the sampled point lands within the polygon's interior area, yielding hash values that preserve area-based Jaccard similarity. We present the tradeoff between search accuracy and runtime of our PolyMinHash system. Our hashing mechanism reduces the number of candidates to be processed in the query refinement phase by up to 98% compared to the number of candidates processed by the brute-force algorithm.",
        "translated": "相似性搜索是数据挖掘中的关键任务。随着数据集规模不断扩大，精确的最近邻搜索迅速变得不可行，因此采用近似最近邻（ANN）搜索成为主流。ANN已在文本、图像和轨迹数据中得到研究，但在空间数据库系统与地理信息系统中针对多边形开发ANN系统的工作却鲜有涉及。本文提出PolyMinHash，一个面向近似多边形相似性搜索的系统，将MinHashing方法扩展为一种新颖的二维多边形哈希方案，生成输入多边形的短且保留相似性的签名。该哈希值通过统计在随机采样点落入多边形内部区域前所需的采样点数量来生成，从而保留在面积基础上的Jaccard相似性。我们分析了PolyMinHash系统在搜索精度与运行时间之间的权衡关系。我们的哈希机制相较暴力算法，在查询精炼阶段可将待处理候选数减少高达98%。",
        "translated_title": "PolyMinHash：面向多边形的基于面积的高效MinHash方法用于近似最近邻搜索",
        "label": [],
        "label_reason": "仅针对空间数据近邻搜索，无推荐系统关联",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出新颖2D多边形MinHash方案，但非为推荐设计"
    },
    {
        "title": "The Oracle and The Prism: A Decoupled and Efficient Framework for Generative Recommendation Explanation",
        "url": "http://arxiv.org/abs/2511.16543v1",
        "pub_date": "2025-11-20",
        "summary": "The integration of Large Language Models (LLMs) into explainable recommendation systems often leads to a performance-efficiency trade-off in end-to-end architectures, where joint optimization of ranking and explanation can result in suboptimal compromises. To resolve this, we propose Prism, a novel decoupled framework that rigorously separates the recommendation process into a dedicated ranking stage and an explanation generation stage.   Inspired by knowledge distillation, Prism leverages a powerful teacher LLM (e.g., FLAN-T5-XXL) as an Oracle to produce high-fidelity explanatory knowledge. A compact, fine-tuned student model (e.g., BART-Base), the Prism, then specializes in synthesizing this knowledge into personalized explanations. This decomposition ensures that each component is optimized for its specific objective, eliminating inherent conflicts in coupled models.   Extensive experiments on benchmark datasets demonstrate that our 140M-parameter Prism model significantly outperforms its 11B-parameter teacher in human evaluations of faithfulness and personalization, while achieving a 24 times speedup and a 10 times reduction in memory consumption during inference. These results validate that decoupling, coupled with targeted distillation, provides an efficient and effective pathway to high-quality explainable recommendation.",
        "translated": "将大语言模型（LLMs）集成到可解释推荐系统中时，端到端架构往往面临性能与效率之间的权衡，联合优化排序与解释会导致次优的折衷。为解决这一问题，我们提出 Prism，一种新颖的解耦框架，严格将推荐过程拆分为独立的排序阶段与解释生成阶段。受知识蒸馏启发，Prism 利用强大的教师 LLM（如 FLAN-T5-XXL）作为 Oracle 以生成高保真度的解释性知识；随后，一个轻量级、微调后的学生模型（如 BART-Base），即 Prism，专门负责将该知识合成个性化解释。这种分解确保每个组件针对其特定目标进行优化，消除了耦合模型中存在的固有冲突。在基准数据集上的大量实验表明，我们的 140M 参数 Prism 模型在人类评估的忠实度与个性化表现上显著优于其 11B 参数教师模型，同时推理阶段速度提升 24 倍，内存消耗降低 10 倍。这些结果验证了解耦结合定向蒸馏，是实现高质量可解释推荐的有效且高效途径。",
        "translated_title": "Oracle 与棱镜：一种解耦且高效的生成式推荐解释框架",
        "label": [
            "LLM生成式推荐",
            "推荐系统公平性/可解释性"
        ],
        "label_reason": "聚焦LLM生成解释，解决推荐系统可解释性问题",
        "relevance_score": 9,
        "novelty_score": 8,
        "novelty_reason": "提出解耦框架+蒸馏机制，提升效率与质量"
    },
    {
        "title": "TurkColBERT: A Benchmark of Dense and Late-Interaction Models for Turkish Information Retrieval",
        "url": "http://arxiv.org/abs/2511.16528v1",
        "pub_date": "2025-11-20",
        "summary": "Neural information retrieval systems excel in high-resource languages but remain underexplored for morphologically rich, lower-resource languages such as Turkish. Dense bi-encoders currently dominate Turkish IR, yet late-interaction models -- which retain token-level representations for fine-grained matching -- have not been systematically evaluated. We introduce TurkColBERT, the first comprehensive benchmark comparing dense encoders and late-interaction models for Turkish retrieval. Our two-stage adaptation pipeline fine-tunes English and multilingual encoders on Turkish NLI/STS tasks, then converts them into ColBERT-style retrievers using PyLate trained on MS MARCO-TR. We evaluate 10 models across five Turkish BEIR datasets covering scientific, financial, and argumentative domains. Results show strong parameter efficiency: the 1.0M-parameter colbert-hash-nano-tr is 600$\\times$ smaller than the 600M turkish-e5-large dense encoder while preserving over 71\\% of its average mAP. Late-interaction models that are 3--5$\\times$ smaller than dense encoders significantly outperform them; ColmmBERT-base-TR yields up to +13.8\\% mAP on domain-specific tasks. For production-readiness, we compare indexing algorithms: MUVERA+Rerank is 3.33$\\times$ faster than PLAID and offers +1.7\\% relative mAP gain. This enables low-latency retrieval, with ColmmBERT-base-TR achieving 0.54 ms query times under MUVERA. We release all checkpoints, configs, and evaluation scripts. Limitations include reliance on moderately sized datasets ($\\leq$50K documents) and translated benchmarks, which may not fully reflect real-world Turkish retrieval conditions; larger-scale MUVERA evaluations remain necessary.",
        "translated": "神经信息检索系统在资源丰富的语言中表现优异，但在形态丰富、资源匮乏的语言（如土耳其语）中的研究仍显不足。当前，稠密双编码器主导了土耳其语信息检索，但保留词元级表示以实现细粒度匹配的后交互模型尚未被系统性评估。我们提出了TurkColBERT，这是首个全面比较稠密编码器与后交互模型在土耳其语检索任务中的基准评测框架。我们的两阶段适配流程首先在土耳其语自然语言推理（NLI）和语义文本相似度（STS）任务上微调英文及多语言编码器，随后利用在MS MARCO-TR数据集上训练的PyLate工具将其转换为ColBERT风格的检索器。我们在五个覆盖科学、金融及论辩领域的土耳其语BEIR数据集上对10个模型进行了评估。实验结果表明，参数效率显著：仅100万参数的colbert-hash-nano-tr比6亿参数的turkish-e5-large稠密编码器小600倍，同时保持超过71%的平均mAP。相较于稠密编码器小3至5倍的后交互模型在性能上明显超越后者；ColmmBERT-base-TR在特定领域任务中最高可提升13.8%的mAP。为评估生产部署可行性，我们对比了多种索引算法：MUVERA+Rerank比PLAID快3.33倍，并带来1.7%相对mAP增益。这使得低延迟检索成为可能，ColmmBERT-base-TR在MUVERA架构下实现0.54毫秒的查询响应时间。我们开源了所有检查点、配置文件与评估脚本。局限性包括依赖中等规模数据集（≤50K文档）及翻译后的基准测试，可能无法完全反映真实土耳其语检索场景；更大规模的MUVERA评估仍有待开展。",
        "translated_title": "TurkColBERT：面向土耳其语信息检索的稠密与后交互模型基准",
        "label": [],
        "label_reason": "聚焦土耳其信息检索，非推荐系统核心问题",
        "relevance_score": 3,
        "novelty_score": 5,
        "novelty_reason": "改进编码器效率，但非推荐算法创新"
    },
    {
        "title": "Music Recommendation with Large Language Models: Challenges, Opportunities, and Evaluation",
        "url": "http://arxiv.org/abs/2511.16478v1",
        "pub_date": "2025-11-20",
        "summary": "Music Recommender Systems (MRS) have long relied on an information-retrieval framing, where progress is measured mainly through accuracy on retrieval-oriented subtasks. While effective, this reductionist paradigm struggles to address the deeper question of what makes a good recommendation, and attempts to broaden evaluation, through user studies or fairness analyses, have had limited impact. The emergence of Large Language Models (LLMs) disrupts this framework: LLMs are generative rather than ranking-based, making standard accuracy metrics questionable. They also introduce challenges such as hallucinations, knowledge cutoffs, non-determinism, and opaque training data, rendering traditional train/test protocols difficult to interpret. At the same time, LLMs create new opportunities, enabling natural-language interaction and even allowing models to act as evaluators.   This work argues that the shift toward LLM-driven MRS requires rethinking evaluation. We first review how LLMs reshape user modeling, item modeling, and natural-language recommendation in music. We then examine evaluation practices from NLP, highlighting methodologies and open challenges relevant to MRS. Finally, we synthesize insights-focusing on how LLM prompting applies to MRS, to outline a structured set of success and risk dimensions. Our goal is to provide the MRS community with an updated, pedagogical, and cross-disciplinary perspective on evaluation.",
        "translated": "音乐推荐系统（MRS）长期以来依赖于信息检索框架，其进展主要通过面向检索子任务的准确性来衡量。尽管该简化范式有效，却难以应对更深层的问题——究竟什么样的推荐才算“好”，且试图通过用户研究或公平性分析扩展评估范围的努力效果有限。大语言模型（LLMs）的出现颠覆了这一框架：LLMs是生成式而非排序式的，使得传统的准确率指标存疑；同时，它们还带来了诸如幻觉、知识截止、非确定性和训练数据不透明等新挑战，使传统的训练/测试协议难以为继。与此同时，LLMs也创造了新的机遇，支持自然语言交互，甚至可让模型充当评价者。\n\n本文认为，向以LLM驱动的MRS转型需要重新思考评估方法。我们首先回顾LLMs如何重塑音乐领域的用户建模、物料建模及自然语言推荐；随后考察自然语言处理（NLP）中的评估实践，突出与MRS相关的方法论及开放挑战；最后，我们综合这些洞见——聚焦LLM提示工程在MRS中的应用——勾勒出一套结构化的成功与风险维度。我们的目标是为MRS社区提供一个更新的、教学性的、跨学科的评估视角。",
        "translated_title": "基于大语言模型的音乐推荐：挑战、机遇与评估",
        "label": [
            "LLM生成式推荐",
            "推荐系统评估"
        ],
        "label_reason": "聚焦LLM在音乐推荐中的评估革新，属生成式推荐核心议题。",
        "relevance_score": 9,
        "novelty_score": 7,
        "novelty_reason": "提出LLM驱动下推荐评估新框架，跨领域整合方法具创新性。"
    },
    {
        "title": "ESGBench: A Benchmark for Explainable ESG Question Answering in Corporate Sustainability Reports",
        "url": "http://arxiv.org/abs/2511.16438v1",
        "pub_date": "2025-11-20",
        "summary": "We present ESGBench, a benchmark dataset and evaluation framework designed to assess explainable ESG question answering systems using corporate sustainability reports. The benchmark consists of domain-grounded questions across multiple ESG themes, paired with human-curated answers and supporting evidence to enable fine-grained evaluation of model reasoning. We analyze the performance of state-of-the-art LLMs on ESGBench, highlighting key challenges in factual consistency, traceability, and domain alignment. ESGBench aims to accelerate research in transparent and accountable ESG-focused AI systems.",
        "translated": "我们提出了ESGBench，这是一个专为评估基于企业可持续发展报告的可解释ESG问答系统而设计的基准数据集与评测框架。该基准包含多个ESG主题下的领域相关问题，并配有人工整理的答案及支持证据，以支持对模型推理过程进行细粒度评估。我们分析了当前最先进的大语言模型在ESGBench上的表现，指出了其在事实一致性、可追溯性及领域适配性方面面临的关键挑战。ESGBench旨在加速透明且负责任的ESG导向人工智能系统的相关研究。",
        "translated_title": "ESGBench：企业可持续发展报告中可解释ESG问答的基准测试",
        "label": [],
        "label_reason": "论文聚焦ESG问答系统，非推荐系统相关",
        "relevance_score": 2,
        "novelty_score": 5,
        "novelty_reason": "评估框架创新但无推荐系统应用"
    },
    {
        "title": "An Efficient LLM-based Evolutional Recommendation with Locate-Forget-Update Paradigm",
        "url": "http://arxiv.org/abs/2511.16414v1",
        "pub_date": "2025-11-20",
        "summary": "Nowadays, Large Language Models (LLMs) have shown exceptional performance in sequential recommendations, and the adoption of LLM-based recommender systems (LLMRec) is becoming increasingly widespread in existing e-commerce platforms. Despite the impressive performance, the constant high volume of new user-item interactions makes it difficult to adapt to the evolution of user preference over time, especially for LLM-based recommender systems. The challenge arises from the large number of parameters in LLMs, which makes traditional evolution methods (i.e., Re-training or Fine-tuning) impractical. Specifically, Re-training with all interactions results in prohibitively high computational costs. On the other hand, fine-tuning with only new interactions leads to preference forgetting among inactive users, ultimately compromising overall performance. To tackle this problem, we propose EvoRec, an efficient Locate-Forget-Update framework designed for LLM-based recommender systems to model the evolution of user preferences. EvoRec identifies a small set of parameters associated with preference changes and updates them precisely, thereby saving computational resources while maintaining strong recommendation performance. Notably, the modified parameters account for only 30\\% of LoRA adapter parameters, with no additional parameters introduced. Extensive experiments on two real-world datasets demonstrate that, compared to existing methods, EvoRec not only efficiently evolves LLMRec to adapt to the preferences of active users, but also preserves the interests of inactive users from being disturbed during evolution.",
        "translated": "如今，大语言模型（LLMs）在序列推荐任务中展现出卓越性能，基于LLM的推荐系统（LLMRec）已在现有电子商务平台中得到越来越广泛的部署。尽管其表现优异，但不断增长的新用户-物料交互数据使得系统难以持续适应用户偏好的时序演化，尤其是对基于LLM的推荐系统而言。这一挑战源于LLM参数规模庞大，导致传统演化方法（即重新训练或微调）难以实施。具体而言，若使用全部历史交互数据进行重新训练，将产生高昂的计算开销；而仅用新交互数据进行微调，则会导致不活跃用户的偏好被遗忘，最终损害整体推荐性能。为解决此问题，我们提出EvoRec，一种面向基于LLM推荐系统的高效“定位-遗忘-更新”框架，用于建模用户偏好的演化过程。EvoRec精准识别与偏好变化相关的少量参数并对其进行更新，从而节省计算资源同时保持强推荐性能。值得注意的是，所修改参数仅占LoRA适配器参数的30%，且未引入任何额外参数。在两个真实世界数据集上的大量实验表明，相较于现有方法，EvoRec不仅能够高效地使LLMRec适应活跃用户的偏好演化，还能有效避免不活跃用户兴趣在演化过程中受到干扰。",
        "translated_title": "一种基于大语言模型的高效演化推荐方法：定位-遗忘-更新范式",
        "label": [
            "LLM生成式推荐",
            "精排"
        ],
        "label_reason": "直接解决LLM推荐系统演化问题，优化参数更新机制。",
        "relevance_score": 10,
        "novelty_score": 9,
        "novelty_reason": "提出Locate-Forget-Update范式，高效适配用户偏好变化。"
    },
    {
        "title": "ARK: Answer-Centric Retriever Tuning via KG-augmented Curriculum Learning",
        "url": "http://arxiv.org/abs/2511.16326v1",
        "pub_date": "2025-11-20",
        "summary": "Retrieval-Augmented Generation (RAG) has emerged as a powerful framework for knowledge-intensive tasks, yet its effectiveness in long-context scenarios is often bottlenecked by the retriever's inability to distinguish sparse yet crucial evidence. Standard retrievers, optimized for query-document similarity, frequently fail to align with the downstream goal of generating a precise answer. To bridge this gap, we propose a novel fine-tuning framework that optimizes the retriever for Answer Alignment. Specifically, we first identify high-quality positive chunks by evaluating their sufficiency to generate the correct answer. We then employ a curriculum-based contrastive learning scheme to fine-tune the retriever. This curriculum leverages LLM-constructed Knowledge Graphs (KGs) to generate augmented queries, which in turn mine progressively challenging hard negatives. This process trains the retriever to distinguish the answer-sufficient positive chunks from these nuanced distractors, enhancing its generalization. Extensive experiments on 10 datasets from the Ultradomain and LongBench benchmarks demonstrate that our fine-tuned retriever achieves state-of-the-art performance, improving 14.5% over the base model without substantial architectural modifications and maintaining strong efficiency for long-context RAG. Our work presents a robust and effective methodology for building truly answer-centric retrievers.",
        "translated": "检索增强生成（RAG）已成为处理知识密集型任务的强大框架，但在长上下文场景中，其效果常因检索器难以区分稀疏却关键的证据而受限。标准检索器通过优化查询-文档相似度进行设计，往往无法与下游目标——生成精确答案——对齐。为弥合这一差距，我们提出一种新颖的微调框架，旨在将检索器优化为“答案对齐”。具体而言，我们首先通过评估各片段是否足以生成正确答案，识别出高质量正样本片段；随后采用基于课程学习的对比学习方案对检索器进行微调。该课程机制利用大语言模型构建的知识图谱（KGs）生成增强式查询，进而逐步挖掘更具挑战性的困难负样本。此过程训练检索器能够区分具备答案充分性特征的正样本片段与这些微妙干扰项，从而提升其泛化能力。在包含10个数据集的Ultradomain和LongBench基准测试中，大量实验表明，我们的微调后检索器实现了当前最优性能，在不大幅修改架构的前提下较基线模型提升14.5%，同时保持了适用于长上下文RAG的高效性。本工作提出了一种稳健且有效的构建真正以答案为中心的检索器的方法。",
        "translated_title": "ARK：基于知识图谱增强的课程学习的以答案为中心的检索器调优",
        "label": [],
        "label_reason": "聚焦RAG检索优化，非推荐系统核心问题",
        "relevance_score": 3,
        "novelty_score": 7,
        "novelty_reason": "课程学习+KG增强，提升检索精度"
    },
    {
        "title": "Incorporating Token Importance in Multi-Vector Retrieval",
        "url": "http://arxiv.org/abs/2511.16106v1",
        "pub_date": "2025-11-20",
        "summary": "ColBERT introduced a late interaction mechanism that independently encodes queries and documents using BERT, and computes similarity via fine-grained interactions over token-level vector representations. This design enables expressive matching while allowing efficient computation of scores, as the multi-vector document representations could be pre-computed offline. ColBERT models distance using a Chamfer-style function: for each query token, it selects the closest document token and sums these distances across all query tokens.   In our work, we explore enhancements to the Chamfer distance function by computing a weighted sum over query token contributions, where weights reflect the token importance. Empirically, we show that this simple extension, requiring only token-weight training while keeping the multi-vector representations fixed, further enhances the expressiveness of late interaction multi-vector mechanism. In particular, on the BEIR benchmark, our method achieves an average improvement of 1.28\\% in Recall@10 in the zero-shot setting using IDF-based weights, and 3.66\\% through few-shot fine-tuning.",
        "translated": "ColBERT 引入了一种后期交互机制，通过 BERT 独立编码查询和文档，并在词级向量表示上进行细粒度交互以计算相似度。该设计在保持高效评分计算的同时，实现了强大的匹配能力，因为多向量文档表示可在离线阶段预计算。ColBERT 使用类似 Chamfer 的距离函数衡量距离：对每个查询词元，选取其最近的文档词元，然后累加所有查询词元的距离。在本研究中，我们探索了通过计算查询词元贡献的加权和来改进 Chamfer 距离函数的方法，其中权重反映词元的重要性。实证表明，这一简单扩展——仅需训练词元权重，而保持多向量表示不变——进一步增强了后期交互多向量机制的表达能力。特别是在 BEIR 评测基准上，我们的方法在零样本设置下使用基于 IDF 的权重实现了 Recall@10 平均提升 1.28%，并通过少量样本微调实现了 3.66% 的提升。",
        "translated_title": "在多向量检索中融入Token重要性",
        "label": [
            "召回",
            "通用推荐技术"
        ],
        "label_reason": "改进多向量检索，提升召回效果",
        "relevance_score": 4,
        "novelty_score": 6,
        "novelty_reason": "简单加权策略优化，非推荐专用创新"
    },
    {
        "title": "QueryGym: A Toolkit for Reproducible LLM-Based Query Reformulation",
        "url": "http://arxiv.org/abs/2511.15996v1",
        "pub_date": "2025-11-20",
        "summary": "We present QueryGym, a lightweight, extensible Python toolkit that supports large language model (LLM)-based query reformulation. This is an important tool development since recent work on llm-based query reformulation has shown notable increase in retrieval effectiveness. However, while different authors have sporadically shared the implementation of their methods, there is no unified toolkit that provides a consistent implementation of such methods, which hinders fair comparison, rapid experimentation, consistent benchmarking and reliable deployment. QueryGym addresses this gap by providing a unified framework for implementing, executing, and comparing llm-based reformulation methods. The toolkit offers: (1) a Python API for applying diverse LLM-based methods, (2) a retrieval-agnostic interface supporting integration with backends such as Pyserini and PyTerrier, (3) a centralized prompt management system with versioning and metadata tracking, (4) built-in support for benchmarks like BEIR and MS MARCO, and (5) a completely open-source extensible implementation available to all researchers. QueryGym is publicly available at https://github.com/radinhamidi/QueryGym.",
        "translated": "我们提出了 QueryGym，一个轻量级、可扩展的 Python 工具包，支持基于大语言模型（LLM）的查询重写。由于近期关于基于 LLM 的查询重写研究已显示出显著提升的检索效果，该工具开发具有重要意义。然而，尽管不同作者偶有分享其方法的实现代码，目前尚无统一工具包提供此类方法的一致化实现，这阻碍了公平比较、快速实验、一致基准测试和可靠部署。QueryGym 通过提供一个统一框架，用于实现、执行和比较基于 LLM 的重写方法，解决了这一问题。该工具包提供以下功能：(1) 用于应用多种 LLM 基础方法的 Python API；(2) 支持与 Pyserini 和 PyTerrier 等后端集成的检索无关接口；(3) 集中的提示管理机制，支持版本控制与元数据追踪；(4) 内置对 BEIR 和 MS MARCO 等基准的支持；(5) 完全开源且可扩展的实现，向所有研究人员开放。QueryGym 可在 https://github.com/radinhamidi/QueryGym 公开获取。",
        "translated_title": "QueryGym：面向基于大语言模型的查询重写可复现性的工具包",
        "label": [
            "通用推荐技术"
        ],
        "label_reason": "支持LLM查询重写，间接提升检索效果",
        "relevance_score": 4,
        "novelty_score": 5,
        "novelty_reason": "提供统一工具链，非算法创新"
    },
    {
        "title": "Native 3D Editing with Full Attention",
        "url": "http://arxiv.org/abs/2511.17501v1",
        "pub_date": "2025-11-21",
        "summary": "Instruction-guided 3D editing is a rapidly emerging field with the potential to broaden access to 3D content creation. However, existing methods face critical limitations: optimization-based approaches are prohibitively slow, while feed-forward approaches relying on multi-view 2D editing often suffer from inconsistent geometry and degraded visual quality. To address these issues, we propose a novel native 3D editing framework that directly manipulates 3D representations in a single, efficient feed-forward pass. Specifically, we create a large-scale, multi-modal dataset for instruction-guided 3D editing, covering diverse addition, deletion, and modification tasks. This dataset is meticulously curated to ensure that edited objects faithfully adhere to the instructional changes while preserving the consistency of unedited regions with the source object. Building upon this dataset, we explore two distinct conditioning strategies for our model: a conventional cross-attention mechanism and a novel 3D token concatenation approach. Our results demonstrate that token concatenation is more parameter-efficient and achieves superior performance. Extensive evaluations show that our method outperforms existing 2D-lifting approaches, setting a new benchmark in generation quality, 3D consistency, and instruction fidelity.",
        "translated": "指令引导的3D编辑是一个迅速兴起的领域，具有拓展3D内容创作可及性的潜力。然而，现有方法面临关键限制：基于优化的方法速度过慢，而依赖多视角2D编辑的前馈方法往往导致几何不一致与视觉质量下降。为解决这些问题，我们提出了一种新颖的原生3D编辑框架，该框架可在单次高效前馈过程中直接操作3D表示。具体而言，我们构建了一个大规模、多模态的指令引导3D编辑数据集，涵盖多样化的添加、删除与修改任务。该数据集经过精心策划，确保编辑后的物体忠实遵循指令变化，同时保持未编辑区域与源物体的一致性。在此数据集基础上，我们探索了两种不同的条件化策略用于模型设计：传统交叉注意力机制与一种新颖的3D token拼接方法。实验结果表明，token拼接在参数效率上更优，并取得更佳性能。广泛评估显示，我们的方法优于现有的2D提升方法，在生成质量、3D一致性及指令保真度方面均创下新基准。",
        "translated_title": "原生三维编辑与全注意力机制",
        "label": [],
        "label_reason": "处理3D编辑，非像素级图像恢复任务",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "提出新3D条件策略，提升效率与性能"
    },
    {
        "title": "EvDiff: High Quality Video with an Event Camera",
        "url": "http://arxiv.org/abs/2511.17492v1",
        "pub_date": "2025-11-21",
        "summary": "As neuromorphic sensors, event cameras asynchronously record changes in brightness as streams of sparse events with the advantages of high temporal resolution and high dynamic range. Reconstructing intensity images from events is a highly ill-posed task due to the inherent ambiguity of absolute brightness. Early methods generally follow an end-to-end regression paradigm, directly mapping events to intensity frames in a deterministic manner. While effective to some extent, these approaches often yield perceptually inferior results and struggle to scale up in model capacity and training data. In this work, we propose EvDiff, an event-based diffusion model that follows a surrogate training framework to produce high-quality videos. To reduce the heavy computational cost of high-frame-rate video generation, we design an event-based diffusion model that performs only a single forward diffusion step, equipped with a temporally consistent EvEncoder. Furthermore, our novel Surrogate Training Framework eliminates the dependence on paired event-image datasets, allowing the model to leverage large-scale image datasets for higher capacity. The proposed EvDiff is capable of generating high-quality colorful videos solely from monochromatic event streams. Experiments on real-world datasets demonstrate that our method strikes a sweet spot between fidelity and realism, outperforming existing approaches on both pixel-level and perceptual metrics.",
        "translated": "作为神经形态传感器，事件相机异步记录亮度变化，以稀疏事件流的形式输出，具有高时间分辨率和高动态范围的优势。从事件中重建强度图像是一项高度不适定的任务，原因在于绝对亮度存在固有的模糊性。早期方法通常遵循端到端回归范式，以确定性方式直接将事件映射为强度帧。尽管在一定程度上有效，这些方法往往导致感知质量较差，并且难以扩展模型容量与训练数据规模。在本工作中，我们提出 EvDiff，一种基于事件的扩散模型，采用代理训练框架生成高质量视频。为降低高帧率视频生成带来的高昂计算开销，我们设计了一种仅执行单次前向扩散步骤的事件驱动扩散模型，并配备具备时序一致性的 EvEncoder。此外，我们的新颖代理训练框架消除了对配对事件-图像数据集的依赖，使模型能够利用大规模图像数据集实现更高容量。所提出的 EvDiff 能够仅从单色事件流中生成高质量彩色视频。在真实世界数据集上的实验表明，我们的方法在保真度与真实感之间取得了平衡，在像素级与感知指标上均优于现有方法。",
        "translated_title": "EvDiff：基于事件相机的高质量视频",
        "label": [],
        "label_reason": "重建视频非像素级图像恢复任务",
        "relevance_score": 3,
        "novelty_score": 8,
        "novelty_reason": "扩散模型新框架，但非低层图像处理"
    },
    {
        "title": "Video-R4: Reinforcing Text-Rich Video Reasoning with Visual Rumination",
        "url": "http://arxiv.org/abs/2511.17490v1",
        "pub_date": "2025-11-21",
        "summary": "Understanding text-rich videos requires reading small, transient textual cues that often demand repeated inspection. Yet most video QA models rely on single-pass perception over fixed frames, leading to hallucinations and failures on fine-grained evidence. Inspired by how humans pause, zoom, and re-read critical regions, we introduce Video-R4 (Reinforcing Text-Rich Video Reasoning with Visual Rumination), a video reasoning LMM that performs visual rumination: iteratively selecting frames, zooming into informative regions, re-encoding retrieved pixels, and updating its reasoning state. We construct two datasets with executable rumination trajectories: Video-R4-CoT-17k for supervised practice and Video-R4-RL-30k for reinforcement learning. We propose a multi-stage rumination learning framework that progressively finetunes a 7B LMM to learn atomic and mixing visual operations via SFT and GRPO-based RL. Video-R4-7B achieves state-of-the-art results on M4-ViteVQA and further generalizes to multi-page document QA, slides QA, and generic video QA, demonstrating that iterative rumination is an effective paradigm for pixel-grounded multimodal reasoning.",
        "translated": "理解富含文本的视频需要阅读微小且短暂的文本线索，这些线索往往需要反复检查。然而，大多数视频问答模型依赖于对固定帧的单次感知，导致在细粒度证据上出现幻觉和失败。受人类暂停、放大并重新阅读关键区域的方式启发，我们提出了 Video-R4（通过视觉沉思强化文本丰富视频推理），这是一种支持视觉沉思的视频推理大语言模型：它可迭代地选择帧、放大信息区域、重编码检索到的像素，并更新其推理状态。我们构建了两个包含可执行沉思轨迹的数据集：Video-R4-CoT-17k 用于监督学习训练，Video-R4-RL-30k 用于强化学习。我们提出了一种多阶段沉思学习框架，通过监督微调（SFT）与基于 GRPO 的强化学习，逐步微调一个 7B 参数的大语言模型，使其学习原子化及混合化的视觉操作。Video-R4-7B 在 M4-ViteVQA 数据集上达到当前最优性能，并进一步泛化至多页文档问答、幻灯片问答及通用视频问答任务，证明了迭代沉思是面向像素级多模态推理的有效范式。",
        "translated_title": "Video-R4：通过视觉沉思强化文本丰富的视频推理",
        "label": [],
        "label_reason": "高阶视频问答任务，非像素级图像处理",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "提出迭代视觉沉思框架，提升多模态推理能力"
    },
    {
        "title": "Downscaling Intelligence: Exploring Perception and Reasoning Bottlenecks in Small Multimodal Models",
        "url": "http://arxiv.org/abs/2511.17487v1",
        "pub_date": "2025-11-21",
        "summary": "Scaling up multimodal models has enabled remarkable advances in visual understanding and reasoning, but practical demands call for smaller, efficient systems. In this work, we conduct a principled analysis of downscaling intelligence in multimodal models, examining how reduced large language model (LLM) capacity affects multimodal capabilities. Our initial findings reveal an interesting trend: LLM downscaling disproportionately affects visual capabilities, rather than abilities inherited from the LLM. We then examine whether this drop mainly reflects the expected decline in visual reasoning or a more fundamental loss of perceptual abilities. Isolating the effect of LLM downscaling on perception, we find performance still drops sharply, often matching or exceeding the impact on reasoning. To address this bottleneck, we introduce visual extraction tuning, which explicitly trains the model to extract instruction-relevant visual details consistently across tasks. With these extracted visual details, we then apply step-by-step reasoning to generate answers. Together, these components form our Extract+Think approach, setting a new standard for efficiency and performance in this space.",
        "translated": "扩大多模态模型的规模已推动视觉理解与推理能力取得显著进展，但实际应用需求呼唤更小、更高效的系统。在本研究中，我们从原理层面分析多模态模型智能压缩的影响，考察降低大语言模型（LLM）容量如何影响其多模态能力。我们的初步发现揭示了一个有趣趋势：LLM容量缩减对视觉能力造成不成比例的负面影响，而非对其继承自LLM的能力产生主要冲击。随后，我们进一步探讨这种性能下降主要反映的是预期中的视觉推理能力衰退，还是感知能力更根本性的丧失。通过分离LLM容量压缩对感知能力的影响，我们发现即使剔除推理因素，性能仍急剧下降，往往甚至超过推理能力受损的程度。为解决这一瓶颈问题，我们提出“视觉提取调优”方法，显式训练模型在不同任务中持续稳定地提取与指令相关的关键视觉细节。借助这些提取出的视觉信息，我们再采用逐步推理生成最终答案。上述组件共同构成了我们的 Extract+Think 方法，在该领域树立了效率与性能的新标杆。",
        "translated_title": " downsizing intelligence: 探索小型多模态模型中的感知与推理瓶颈",
        "label": [],
        "label_reason": "研究多模态模型压缩，非图像像素级处理",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出视觉提取+推理新框架，提升效率"
    },
    {
        "title": "An Artificial Intelligence Framework for Measuring Human Spine Aging Using MRI",
        "url": "http://arxiv.org/abs/2511.17485v1",
        "pub_date": "2025-11-21",
        "summary": "The human spine is a complex structure composed of 33 vertebrae. It holds the body and is important for leading a healthy life. The spine is vulnerable to age-related degenerations that can be identified through magnetic resonance imaging (MRI). In this paper we propose a novel computer-vison-based deep learning method to estimate spine age using images from over 18,000 MRI series. Data are restricted to subjects with only age-related spine degeneration. Eligibility criteria are created by identifying common age-based clusters of degenerative spine conditions using uniform manifold approximation and projection (UMAP) and hierarchical density-based spatial clustering of applications with noise (HDBSCAN). Model selection is determined using a detailed ablation study on data size, loss, and the effect of different spine regions. We evaluate the clinical utility of our model by calculating the difference between actual spine age and model-predicted age, the spine age gap (SAG), and examining the association between these differences and spine degenerative conditions and lifestyle factors. We find that SAG is associated with conditions including disc bulges, disc osteophytes, spinal stenosis, and fractures, as well as lifestyle factors like smoking and physically demanding work, and thus may be a useful biomarker for measuring overall spine health.",
        "translated": "人类脊柱是由33块椎骨构成的复杂结构，支撑身体并对于维持健康生活至关重要。脊柱易受年龄相关退行性病变影响，可通过磁共振成像（MRI）识别。本文提出了一种基于计算机视觉的深度学习方法，利用超过18,000例MRI序列图像来估计脊柱年龄。研究数据仅限于仅有年龄相关脊柱退变的受试者。通过使用均匀流形逼近与投影（UMAP）和层次密度聚类算法（HDBSCAN），我们识别出退行性脊柱病变的常见年龄相关簇，从而制定入选标准。模型选择通过针对数据规模、损失函数以及不同脊柱区域影响的详细消融研究确定。我们通过计算实际脊柱年龄与模型预测年龄之间的差异——即脊柱年龄差值（SAG），并考察该差值与脊柱退行性病变及生活方式因素的相关性，评估本模型的临床实用性。研究发现，SAG与椎间盘膨出、椎体骨赘、脊髓狭窄和骨折等病变，以及吸烟和体力劳动等生活方式因素显著相关，因此可能作为衡量整体脊柱健康状况的有效生物标志物。",
        "translated_title": "一种利用磁共振成像测量人类脊柱老化的人工智能框架",
        "label": [],
        "label_reason": "任务为脊柱年龄预测，属高阶医学诊断",
        "relevance_score": 1,
        "novelty_score": 3,
        "novelty_reason": "模型复现性高，无像素级图像处理创新"
    },
    {
        "title": "Radar2Shape: 3D Shape Reconstruction from High-Frequency Radar using Multiresolution Signed Distance Functions",
        "url": "http://arxiv.org/abs/2511.17484v1",
        "pub_date": "2025-11-21",
        "summary": "Determining the shape of 3D objects from high-frequency radar signals is analytically complex but critical for commercial and aerospace applications. Previous deep learning methods have been applied to radar modeling; however, they often fail to represent arbitrary shapes or have difficulty with real-world radar signals which are collected over limited viewing angles. Existing methods in optical 3D reconstruction can generate arbitrary shapes from limited camera views, but struggle when they naively treat the radar signal as a camera view. In this work, we present Radar2Shape, a denoising diffusion model that handles a partially observable radar signal for 3D reconstruction by correlating its frequencies with multiresolution shape features. Our method consists of a two-stage approach: first, Radar2Shape learns a regularized latent space with hierarchical resolutions of shape features, and second, it diffuses into this latent space by conditioning on the frequencies of the radar signal in an analogous coarse-to-fine manner. We demonstrate that Radar2Shape can successfully reconstruct arbitrary 3D shapes even from partially-observed radar signals, and we show robust generalization to two different simulation methods and real-world data. Additionally, we release two synthetic benchmark datasets to encourage future research in the high-frequency radar domain so that models like Radar2Shape can safely be adapted into real-world radar systems.",
        "translated": "从高频雷达信号中确定三维物体的形状在分析上具有复杂性，但对商业和航空航天应用至关重要。此前，深度学习方法已被应用于雷达建模，然而它们往往难以表示任意形状，或在处理实际雷达信号时遇到困难，因为这些信号通常是在有限视角下采集的。现有的光学三维重建方法能够从有限的相机视角生成任意形状，但若直接将雷达信号视为相机视角，则会面临挑战。在本研究中，我们提出了Radar2Shape，这是一种去噪扩散模型，通过将雷达信号的频率与多分辨率形状特征相关联，以处理部分可观测的雷达信号并实现三维重建。我们的方法采用两阶段策略：首先，Radar2Shape 学习一个具有层次化形状特征分辨率的正则化潜在空间；其次，它根据雷达信号的频率，在类似由粗到细的方式下扩散至该潜在空间。我们证明了Radar2Shape即使从部分观测的雷达信号中也能成功重建任意三维形状，并展示了其在两种不同仿真方法及真实数据上的鲁棒泛化能力。此外，我们发布了两个合成基准数据集，以促进高频雷达领域的未来研究，使诸如Radar2Shape这样的模型能够安全地适配于实际雷达系统。",
        "translated_title": "Radar2Shape：利用多分辨率有符号距离函数从高频雷达重建三维形状",
        "label": [],
        "label_reason": "目标为3D重建，非像素级图像恢复任务",
        "relevance_score": 3,
        "novelty_score": 7,
        "novelty_reason": "提出扩散模型新框架处理雷达信号建模"
    },
    {
        "title": "Counterfactual World Models via Digital Twin-conditioned Video Diffusion",
        "url": "http://arxiv.org/abs/2511.17481v1",
        "pub_date": "2025-11-21",
        "summary": "World models learn to predict the temporal evolution of visual observations given a control signal, potentially enabling agents to reason about environments through forward simulation. Because of the focus on forward simulation, current world models generate predictions based on factual observations. For many emerging applications, such as comprehensive evaluations of physical AI behavior under varying conditions, the ability of world models to answer counterfactual queries, such as \"what would happen if this object was removed?\", is of increasing importance. We formalize counterfactual world models that additionally take interventions as explicit inputs, predicting temporal sequences under hypothetical modifications to observed scene properties. Traditional world models operate directly on entangled pixel-space representations where object properties and relationships cannot be selectively modified. This modeling choice prevents targeted interventions on specific scene properties. We introduce CWMDT, a framework to overcome those limitations, turning standard video diffusion models into effective counterfactual world models. First, CWMDT constructs digital twins of observed scenes to explicitly encode objects and their relationships, represented as structured text. Second, CWMDT applies large language models to reason over these representations and predict how a counterfactual intervention propagates through time to alter the observed scene. Third, CWMDT conditions a video diffusion model with the modified representation to generate counterfactual visual sequences. Evaluations on two benchmarks show that the CWMDT approach achieves state-of-the-art performance, suggesting that alternative representations of videos, such as the digital twins considered here, offer powerful control signals for video forward simulation-based world models.",
        "translated": "世界模型学习根据控制信号预测视觉观测随时间的演化，从而可能使智能体能够通过前向模拟推理环境。由于当前世界模型侧重于前向模拟，其生成的预测基于实际观测数据。然而，在许多新兴应用中（如在不同条件下对物理AI行为进行综合评估），世界模型回答反事实查询的能力——例如“如果移除该物体会发生什么？”——日益重要。我们形式化了一类反事实世界模型：这类模型将干预作为显式输入，预测在假设修改观测场景属性后的时序序列。传统世界模型直接作用于纠缠的像素空间表示，无法选择性地修改对象属性及其关系，这一建模选择限制了针对特定场景属性的定向干预。我们提出CWMDT框架以克服这些局限，将标准视频扩散模型转化为有效的反事实世界模型。首先，CWMDT构建观测场景的数字孪生体，显式编码对象及其关系，表示为结构化文本；其次，CWMDT利用大型语言模型对这些表示进行推理，预测反事实干预如何随时间传播并改变观测场景；第三，CWMDT将修改后的表示作为条件输入给视频扩散模型，生成反事实视觉序列。在两个基准测试上的评估表明，CWMDT方法达到当前最优性能，说明视频的替代表征方式（如本文所考虑的数字孪生体）可为基于视频前向模拟的世界模型提供强大的控制信号。",
        "translated_title": "通过数字孪生条件视频扩散构建反事实世界模型",
        "label": [],
        "label_reason": "聚焦视频生成与反事实推理，非像素级图像恢复任务",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "引入数字孪生+扩散模型新范式，提升控制能力"
    },
    {
        "title": "GPR-OdomNet: Difference and Similarity-Driven Odometry Estimation Network for Ground Penetrating Radar-Based Localization",
        "url": "http://arxiv.org/abs/2511.17457v1",
        "pub_date": "2025-11-21",
        "summary": "When performing robot/vehicle localization using ground penetrating radar (GPR) to handle adverse weather and environmental conditions, existing techniques often struggle to accurately estimate distances when processing B-scan images with minor distinctions. This study introduces a new neural network-based odometry method that leverages the similarity and difference features of GPR B-scan images for precise estimation of the Euclidean distances traveled between the B-scan images. The new custom neural network extracts multi-scale features from B-scan images taken at consecutive moments and then determines the Euclidean distance traveled by analyzing the similarities and differences between these features. To evaluate our method, an ablation study and comparison experiments have been conducted using the publicly available CMU-GPR dataset. The experimental results show that our method consistently outperforms state-of-the-art counterparts in all tests. Specifically, our method achieves a root mean square error (RMSE), and achieves an overall weighted RMSE of 0.449 m across all data sets, which is a 10.2\\% reduction in RMSE when compared to the best state-of-the-art method.",
        "translated": "在利用地面穿透雷达（GPR）进行机器人/车辆定位以应对恶劣天气和环境条件时，现有技术在处理B扫描图像时往往难以准确估计距离，尤其当这些图像间差异细微时。本研究提出了一种新的基于神经网络的里程计方法，该方法利用GPR B扫描图像的相似性与差异性特征，精确估计相邻B扫描图像间的欧氏距离。该新设计的神经网络从连续时刻获取的B扫描图像中提取多尺度特征，并通过分析这些特征间的相似性和差异性来确定所行进的欧氏距离。为评估本方法，我们在公开可用的CMU-GPR数据集上进行了消融实验与对比实验。实验结果表明，我们的方法在所有测试中均优于当前最先进的方法。具体而言，该方法实现了0.449米的加权均方根误差（RMSE），相比最佳的现有方法，其RMSE降低了10.2%。",
        "translated_title": "GPR-OdomNet：基于地面穿透雷达的定位差分与相似性驱动里程计估计网络",
        "label": [],
        "label_reason": "定位任务属高阶视觉，非像素级图像处理",
        "relevance_score": 3,
        "novelty_score": 7,
        "novelty_reason": "新网络结构改进距离估计，非图像恢复任务"
    },
    {
        "title": "Improving Multimodal Distillation for 3D Semantic Segmentation under Domain Shift",
        "url": "http://arxiv.org/abs/2511.17455v1",
        "pub_date": "2025-11-21",
        "summary": "Semantic segmentation networks trained under full supervision for one type of lidar fail to generalize to unseen lidars without intervention. To reduce the performance gap under domain shifts, a recent trend is to leverage vision foundation models (VFMs) providing robust features across domains. In this work, we conduct an exhaustive study to identify recipes for exploiting VFMs in unsupervised domain adaptation for semantic segmentation of lidar point clouds. Building upon unsupervised image-to-lidar knowledge distillation, our study reveals that: (1) the architecture of the lidar backbone is key to maximize the generalization performance on a target domain; (2) it is possible to pretrain a single backbone once and for all, and use it to address many domain shifts; (3) best results are obtained by keeping the pretrained backbone frozen and training an MLP head for semantic segmentation. The resulting pipeline achieves state-of-the-art results in four widely-recognized and challenging settings. The code will be available at: https://github.com/valeoai/muddos.",
        "translated": "在全监督条件下训练的针对某一类激光雷达的语义分割网络，难以直接泛化到未见过的激光雷达数据，而无需任何干预。为缩小域迁移下的性能差距，近期趋势是利用视觉基础模型（VFMs），其可提供跨域鲁棒的特征表示。在本研究中，我们进行了全面的实验，旨在探索如何在激光点云语义分割的无监督域自适应任务中有效利用VFMs。基于无监督图像到激光雷达的知识蒸馏框架，我们的研究揭示了以下三点：（1）激光雷达骨干网络的架构对在目标域上实现最大泛化性能至关重要；（2）可以仅预训练一个骨干网络，并将其用于应对多种域迁移场景；（3）最佳结果可通过冻结预训练骨干网络并训练一个MLP头进行语义分割获得。所提出的管道方法在四个广受认可且极具挑战性的设置中均取得了当前最先进的性能。代码将开源于：https://github.com/valeoai/muddos。",
        "translated_title": "提升域偏移下多模态蒸馏用于三维语义分割",
        "label": [],
        "label_reason": "任务为高阶语义分割，非像素级图像恢复",
        "relevance_score": 1,
        "novelty_score": 6,
        "novelty_reason": "微调骨干网络与MLP头，属常规架构调整"
    },
    {
        "title": "Illustrator's Depth: Monocular Layer Index Prediction for Image Decomposition",
        "url": "http://arxiv.org/abs/2511.17454v1",
        "pub_date": "2025-11-21",
        "summary": "We introduce Illustrator's Depth, a novel definition of depth that addresses a key challenge in digital content creation: decomposing flat images into editable, ordered layers. Inspired by an artist's compositional process, illustrator's depth infers a layer index to each pixel, forming an interpretable image decomposition through a discrete, globally consistent ordering of elements optimized for editability. We also propose and train a neural network using a curated dataset of layered vector graphics to predict layering directly from raster inputs. Our layer index inference unlocks a range of powerful downstream applications. In particular, it significantly outperforms state-of-the-art baselines for image vectorization while also enabling high-fidelity text-to-vector-graphics generation, automatic 3D relief generation from 2D images, and intuitive depth-aware editing. By reframing depth from a physical quantity to a creative abstraction, illustrator's depth prediction offers a new foundation for editable image decomposition.",
        "translated": "我们提出了“Illustrator’s Depth”，这是一种新颖的深度定义，旨在解决数字内容创作中的关键挑战：将平面图像分解为可编辑且有序的图层。受艺术家构图过程的启发，“Illustrator’s Depth”为每个像素推断一个图层索引，通过一种离散且全局一致的元素排序，形成可解释的图像分解，该排序特别优化了可编辑性。我们还提出并训练了一个神经网络，使用经过精心筛选的矢量图层图形数据集，直接从位图输入预测图层结构。我们的图层索引推理机制解锁了一系列强大的下游应用。特别是，它在图像矢量化任务上显著优于当前最先进的基线方法，同时支持高保真度的文本到矢量图形生成、从2D图像自动生成3D浮雕效果以及直观的深度感知编辑。通过将深度从物理量重新定义为创意抽象，“Illustrator’s Depth”的预测方法为可编辑图像分解提供了全新的基础。",
        "translated_title": "Illustrator的深度：单目图像分解中的层索引预测",
        "label": [],
        "label_reason": "目标为图像分层编辑，属高阶视觉任务",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出新深度定义并训练网络预测分层"
    },
    {
        "title": "Planning with Sketch-Guided Verification for Physics-Aware Video Generation",
        "url": "http://arxiv.org/abs/2511.17450v1",
        "pub_date": "2025-11-21",
        "summary": "Recent video generation approaches increasingly rely on planning intermediate control signals such as object trajectories to improve temporal coherence and motion fidelity. However, these methods mostly employ single-shot plans that are typically limited to simple motions, or iterative refinement which requires multiple calls to the video generator, incuring high computational cost. To overcome these limitations, we propose SketchVerify, a training-free, sketch-verification-based planning framework that improves motion planning quality with more dynamically coherent trajectories (i.e., physically plausible and instruction-consistent motions) prior to full video generation by introducing a test-time sampling and verification loop. Given a prompt and a reference image, our method predicts multiple candidate motion plans and ranks them using a vision-language verifier that jointly evaluates semantic alignment with the instruction and physical plausibility. To efficiently score candidate motion plans, we render each trajectory as a lightweight video sketch by compositing objects over a static background, which bypasses the need for expensive, repeated diffusion-based synthesis while achieving comparable performance. We iteratively refine the motion plan until a satisfactory one is identified, which is then passed to the trajectory-conditioned generator for final synthesis. Experiments on WorldModelBench and PhyWorldBench demonstrate that our method significantly improves motion quality, physical realism, and long-term consistency compared to competitive baselines while being substantially more efficient. Our ablation study further shows that scaling up the number of trajectory candidates consistently enhances overall performance.",
        "translated": "近年来，视频生成方法日益依赖于规划中间控制信号（如物体轨迹），以提升时序一致性与运动保真度。然而，这些方法大多采用单次规划策略，通常仅适用于简单运动，或采用迭代优化方式，需多次调用视频生成器，导致计算成本高昂。为克服上述局限，我们提出 SketchVerify——一种无需训练、基于草图验证的规划框架。该框架在完整视频生成前引入测试时采样与验证循环，通过动态更连贯的轨迹（即物理上合理且符合指令的运动）提高运动规划质量。给定提示词和参考图像，我们的方法预测多个候选运动计划，并利用视觉-语言验证器对其进行排序，该验证器联合评估计划与指令的语义一致性及其物理合理性。为高效评分候选运动计划，我们将每条轨迹渲染为轻量级视频草图：通过将物体叠加于静态背景之上实现，从而避免昂贵的重复扩散式合成，同时保持性能相当。我们对运动计划进行迭代优化，直至找到满意方案，再将其传递至条件轨迹生成器完成最终合成。在 WorldModelBench 和 PhyWorldBench 上的实验表明，相较于现有基线方法，我们的方法显著提升了运动质量、物理真实感与长期一致性，同时计算效率大幅提升。消融研究表明，增加轨迹候选数量可持续提升整体性能。",
        "translated_title": "面向物理感知视频生成的草图引导验证规划方法",
        "label": [],
        "label_reason": "视频生成属高阶任务，非像素级图像恢复",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "提出新规划框架提升物理一致性与效率"
    },
    {
        "title": "MMT-ARD: Multimodal Multi-Teacher Adversarial Distillation for Robust Vision-Language Models",
        "url": "http://arxiv.org/abs/2511.17448v1",
        "pub_date": "2025-11-21",
        "summary": "Vision-Language Models (VLMs) are increasingly deployed in safety-critical applications, making their adversarial robustness a crucial concern. While adversarial knowledge distillation has shown promise in transferring robustness from teacher to student models, traditional single-teacher approaches suffer from limited knowledge diversity, slow convergence, and difficulty in balancing robustness and accuracy. To address these challenges, we propose MMT-ARD: a Multimodal Multi-Teacher Adversarial Robust Distillation framework. Our key innovation is a dual-teacher knowledge fusion architecture that collaboratively optimizes clean feature preservation and robust feature enhancement. To better handle challenging adversarial examples, we introduce a dynamic weight allocation strategy based on teacher confidence, enabling adaptive focus on harder samples. Moreover, to mitigate bias among teachers, we design an adaptive sigmoid-based weighting function that balances the strength of knowledge transfer across modalities. Extensive experiments on ImageNet and zero-shot benchmarks demonstrate that MMT-ARD improves robust accuracy by +4.32% and zero-shot accuracy by +3.5% on the ViT-B-32 model, while achieving a 2.3x increase in training efficiency over traditional single-teacher methods. These results highlight the effectiveness and scalability of MMT-ARD in enhancing the adversarial robustness of multimodal large models. Our codes are available at https://github.com/itsnotacie/MMT-ARD.",
        "translated": "视觉-语言模型（VLMs）正越来越多地部署于安全关键型应用中，使其对抗鲁棒性成为一项至关重要的研究课题。尽管对抗知识蒸馏已在将教师模型的鲁棒性迁移至学生模型方面展现出潜力，但传统单教师方法仍存在知识多样性有限、收敛速度慢以及难以平衡鲁棒性与精度等问题。为应对这些挑战，我们提出MMT-ARD：一种多模态多教师对抗鲁棒蒸馏框架。我们的核心创新是一种双教师知识融合架构，该架构协同优化干净特征保留与鲁棒特征增强。为更有效地处理具有挑战性的对抗样本，我们引入一种基于教师置信度的动态权重分配策略，从而实现对困难样本的自适应聚焦。此外，为缓解教师间偏差，我们设计了一种自适应的基于Sigmoid的加权函数，以在不同模态间平衡知识迁移强度。在ImageNet及零样本基准上的大量实验表明，MMT-ARD在ViT-B-32模型上提升了4.32%的鲁棒准确率和3.5%的零样本准确率，同时相较传统单教师方法训练效率提升2.3倍。这些结果凸显了MMT-ARD在增强多模态大模型对抗鲁棒性方面的有效性与可扩展性。我们的代码已开源于 https://github.com/itsnotacie/MMT-ARD。",
        "translated_title": "MMT-ARD：面向鲁棒视觉-语言模型的多模态多教师对抗蒸馏",
        "label": [],
        "label_reason": "研究视觉-语言模型鲁棒性，非图像像素级处理",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "多教师对抗蒸馏框架，提升模型鲁棒性"
    },
    {
        "title": "REMSA: An LLM Agent for Foundation Model Selection in Remote Sensing",
        "url": "http://arxiv.org/abs/2511.17442v1",
        "pub_date": "2025-11-21",
        "summary": "Foundation Models (FMs) are increasingly used in remote sensing (RS) for tasks such as environmental monitoring, disaster assessment, and land-use mapping. These models include unimodal vision encoders trained on a single data modality and multimodal architectures trained on combinations of SAR, multispectral, hyperspectral, and image-text data. They support diverse RS tasks including semantic segmentation, image classification, change detection, and visual question answering. However, selecting an appropriate remote sensing foundation model (RSFM) remains difficult due to scattered documentation, heterogeneous formats, and varied deployment constraints. We introduce the RSFM Database (RS-FMD), a structured resource covering over 150 RSFMs spanning multiple data modalities, resolutions, and learning paradigms. Built on RS-FMD, we present REMSA, the first LLM-based agent for automated RSFM selection from natural language queries. REMSA interprets user requirements, resolves missing constraints, ranks candidate models using in-context learning, and provides transparent justifications. We also propose a benchmark of 75 expert-verified RS query scenarios, producing 900 configurations under an expert-centered evaluation protocol. REMSA outperforms several baselines, including naive agents, dense retrieval, and unstructured RAG-based LLMs. It operates entirely on publicly available metadata and does not access private or sensitive data.",
        "translated": "基础模型（FMs）在遥感（RS）领域正日益应用于环境监测、灾害评估和土地利用制图等任务。这些模型包括仅基于单一数据模态训练的单模态视觉编码器，以及在合成SAR、多光谱、高光谱与图像-文本数据上训练的多模态架构。它们支持多种遥感任务，如语义分割、图像分类、变化检测和视觉问答。然而，由于文档分散、格式异构及部署约束多样，选择合适的遥感基础模型（RSFM）仍颇具挑战。我们引入了遥感基础模型数据库（RS-FMD），该数据库系统性收集了超过150个涵盖多种数据模态、分辨率及学习范式的RSFM。基于RS-FMD，我们提出REMSA——首个基于大语言模型（LLM）的自动化RSFM选择代理系统，其可根据自然语言查询自动选取最优模型。REMSA能够解析用户需求、补全缺失约束条件、借助上下文学习对候选模型进行排序，并提供透明可解释的决策依据。同时，我们构建了一个包含75个经专家验证的遥感查询场景的基准测试集，在专家主导的评估协议下生成900种配置方案。实验表明，REMSA显著优于多项基线方法，包括朴素代理、密集检索及无结构化RAG驱动的大语言模型。整个系统仅依赖公开元数据运行，无需访问任何私有或敏感信息。",
        "translated_title": "REMSA：用于遥感领域基础模型选择的大语言模型代理",
        "label": [],
        "label_reason": "任务为模型选择，非图像像素级处理",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "首次用LLM自动选RS模型，但属高阶任务"
    },
    {
        "title": "SMILE: A Composite Lexical-Semantic Metric for Question-Answering Evaluation",
        "url": "http://arxiv.org/abs/2511.17432v1",
        "pub_date": "2025-11-21",
        "summary": "Traditional evaluation metrics for textual and visual question answering, like ROUGE, METEOR, and Exact Match (EM), focus heavily on n-gram based lexical similarity, often missing the deeper semantic understanding needed for accurate assessment. While measures like BERTScore and MoverScore leverage contextual embeddings to address this limitation, they lack flexibility in balancing sentence-level and keyword-level semantics and ignore lexical similarity, which remains important. Large Language Model (LLM) based evaluators, though powerful, come with drawbacks like high costs, bias, inconsistency, and hallucinations. To address these issues, we introduce SMILE: Semantic Metric Integrating Lexical Exactness, a novel approach that combines sentence-level semantic understanding with keyword-level semantic understanding and easy keyword matching. This composite method balances lexical precision and semantic relevance, offering a comprehensive evaluation. Extensive benchmarks across text, image, and video QA tasks show SMILE is highly correlated with human judgments and computationally lightweight, bridging the gap between lexical and semantic evaluation.",
        "translated": "传统用于文本与视觉问答的评估指标，如 ROUGE、METEOR 和精确匹配（EM），主要侧重于基于 n-gram 的词汇相似性，常忽略准确评估所需的深层语义理解。尽管 BERTScore 和 MoverScore 等度量方法借助上下文嵌入缓解了这一局限性，但它们在平衡句子级语义与关键词级语义时缺乏灵活性，并且忽略了词汇相似性，而后者仍具重要性。基于大语言模型（LLM）的评估器虽功能强大，却存在成本高昂、偏见、不一致及幻觉等缺点。为应对上述问题，我们提出 SMILE：融合词法精确性的语义度量，一种将句子级语义理解与关键词级语义理解及简易关键词匹配相结合的新型方法。该复合策略兼顾词汇精度与语义相关性，提供全面的评估能力。在文本、图像和视频问答任务上的广泛基准测试表明，SMILE 与人类判断高度相关，且计算开销轻量，弥合了词汇与语义评估之间的鸿沟。",
        "translated_title": "SMILE：一种用于问答评价的复合词法-语义度量指标",
        "label": [],
        "label_reason": "属于高阶文本评估任务，非图像处理",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "组合语义与词法评估，但非图像恢复"
    },
    {
        "title": "Self-Supervised Learning by Curvature Alignment",
        "url": "http://arxiv.org/abs/2511.17426v1",
        "pub_date": "2025-11-21",
        "summary": "Self-supervised learning (SSL) has recently advanced through non-contrastive methods that couple an invariance term with variance, covariance, or redundancy-reduction penalties. While such objectives shape first- and second-order statistics of the representation, they largely ignore the local geometry of the underlying data manifold. In this paper, we introduce CurvSSL, a curvature-regularized self-supervised learning framework, and its RKHS extension, kernel CurvSSL. Our approach retains a standard two-view encoder-projector architecture with a Barlow Twins-style redundancy-reduction loss on projected features, but augments it with a curvature-based regularizer. Each embedding is treated as a vertex whose $k$ nearest neighbors define a discrete curvature score via cosine interactions on the unit hypersphere; in the kernel variant, curvature is computed from a normalized local Gram matrix in an RKHS. These scores are aligned and decorrelated across augmentations by a Barlow-style loss on a curvature-derived matrix, encouraging both view invariance and consistency of local manifold bending. Experiments on MNIST and CIFAR-10 datasets with a ResNet-18 backbone show that curvature-regularized SSL yields competitive or improved linear evaluation performance compared to Barlow Twins and VICReg. Our results indicate that explicitly shaping local geometry is a simple and effective complement to purely statistical SSL regularizers.",
        "translated": "自监督学习（SSL）近期通过结合不变性项与方差、协方差或冗余降低惩罚的非对比方法取得了进展。尽管此类目标能够塑造表示的低阶和高阶统计特性，但它们在很大程度上忽略了底层数据流形的局部几何结构。本文提出了一种基于曲率正则化的自监督学习框架 CurvSSL 及其 RKHS 扩展形式 kernel CurvSSL。我们的方法保留了标准的双视图编码器-投影器架构，并采用类似 Barlow Twins 的冗余降低损失作用于投影特征，同时引入了基于曲率的正则化项。每个嵌入被视作超球面上的一个顶点，其 $k$ 个最近邻通过单位超球面上的余弦交互计算出离散曲率得分；在核版本中，曲率由 RKHS 中归一化的局部 Gram 矩阵计算得出。这些曲率得分通过在曲率导出矩阵上的 Barlow 风格损失实现跨增强对齐与去相关，从而鼓励两视图不变性以及局部流形弯曲的一致性。在 MNIST 和 CIFAR-10 数据集上使用 ResNet-18 主干网络的实验表明，曲率正则化的 SSL 在线性评估性能上相较 Barlow Twins 和 VICReg 具有竞争力甚至更优的表现。我们的结果表明，显式塑造局部几何结构是对纯统计型 SSL 正则化项的一种简单而有效的补充。",
        "translated_title": "曲率对齐的自监督学习",
        "label": [],
        "label_reason": "聚焦无监督学习，非图像像素级恢复任务",
        "relevance_score": 2,
        "novelty_score": 8,
        "novelty_reason": "提出曲率正则化新框架，提升表示几何一致性"
    },
    {
        "title": "Preventing Shortcut Learning in Medical Image Analysis through Intermediate Layer Knowledge Distillation from Specialist Teachers",
        "url": "http://arxiv.org/abs/2511.17421v1",
        "pub_date": "2025-11-21",
        "summary": "Deep learning models are prone to learning shortcut solutions to problems using spuriously correlated yet irrelevant features of their training data. In high-risk applications such as medical image analysis, this phenomenon may prevent models from using clinically meaningful features when making predictions, potentially leading to poor robustness and harm to patients. We demonstrate that different types of shortcuts (those that are diffuse and spread throughout the image, as well as those that are localized to specific areas) manifest distinctly across network layers and can, therefore, be more effectively targeted through mitigation strategies that target the intermediate layers. We propose a novel knowledge distillation framework that leverages a teacher network fine-tuned on a small subset of task-relevant data to mitigate shortcut learning in a student network trained on a large dataset corrupted with a bias feature. Through extensive experiments on CheXpert, ISIC 2017, and SimBA datasets using various architectures (ResNet-18, AlexNet, DenseNet-121, and 3D CNNs), we demonstrate consistent improvements over traditional Empirical Risk Minimization, augmentation-based bias-mitigation, and group-based bias-mitigation approaches. In many cases, we achieve comparable performance with a baseline model trained on bias-free data, even on out-of-distribution test data. Our results demonstrate the practical applicability of our approach to real-world medical imaging scenarios where bias annotations are limited and shortcut features are difficult to identify a priori.",
        "translated": "深度学习模型容易学习到利用训练数据中虚假相关但与任务无关的特征的捷径解决方案。在医疗图像分析等高风险应用中，这一现象可能导致模型在预测时无法使用具有临床意义的特征，从而降低模型的鲁棒性并危及患者安全。我们证明，不同类型的捷径（那些遍布图像各处的弥散型捷径，以及局限于特定区域的局部型捷径）在神经网络的不同层中表现各异，因此可通过针对中间层设计缓解策略更有效地抑制其影响。我们提出了一种新颖的知识蒸馏框架，该框架利用一个在少量任务相关数据上微调过的教师网络，以减轻学生网络在包含偏置特征的大规模数据集上训练时所学的捷径。我们在CheXpert、ISIC 2017和SimBA数据集上，采用多种架构（ResNet-18、AlexNet、DenseNet-121和3D CNNs）进行了大量实验，结果表明我们的方法相比传统经验风险最小化、基于数据增强的偏置缓解以及基于分组的偏置缓解方法均能持续获得性能提升。在许多情况下，即使在分布外测试数据上，我们的方法也能达到在无偏数据上训练的基线模型相当的性能水平。我们的结果表明，该方法在实际医学影像场景中具有实用性，尤其适用于标注偏置有限且捷径特征难以预先识别的情形。",
        "translated_title": "通过专家教师的中间层知识蒸馏防止医学图像分析中的捷径学习",
        "label": [],
        "label_reason": "属于高阶医学图像分析任务，非像素级图像处理",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出知识蒸馏框架缓解模型捷径学习，具实用价值"
    },
    {
        "title": "Sparse Mixture-of-Experts for Multi-Channel Imaging: Are All Channel Interactions Required?",
        "url": "http://arxiv.org/abs/2511.17400v1",
        "pub_date": "2025-11-21",
        "summary": "Vision Transformers ($\\text{ViTs}$) have become the backbone of vision foundation models, yet their optimization for multi-channel domains - such as cell painting or satellite imagery - remains underexplored. A key challenge in these domains is capturing interactions between channels, as each channel carries different information. While existing works have shown efficacy by treating each channel independently during tokenization, this approach naturally introduces a major computational bottleneck in the attention block - channel-wise comparisons leads to a quadratic growth in attention, resulting in excessive $\\text{FLOPs}$ and high training cost. In this work, we shift focus from efficacy to the overlooked efficiency challenge in cross-channel attention and ask: \"Is it necessary to model all channel interactions?\". Inspired by the philosophy of Sparse Mixture-of-Experts ($\\text{MoE}$), we propose MoE-ViT, a Mixture-of-Experts architecture for multi-channel images in $\\text{ViTs}$, which treats each channel as an expert and employs a lightweight router to select only the most relevant experts per patch for attention. Proof-of-concept experiments on real-world datasets - JUMP-CP and So2Sat - demonstrate that $\\text{MoE-ViT}$ achieves substantial efficiency gains without sacrificing, and in some cases enhancing, performance, making it a practical and attractive backbone for multi-channel imaging.",
        "translated": "视觉变换器（$\\text{ViTs}$）已成为视觉基础模型的骨干架构，但在多通道领域——如细胞成像或卫星图像——其优化仍处于探索初期。这些领域的关键挑战在于捕捉各通道间的交互关系，因为每个通道承载的信息各不相同。尽管现有工作通过在分词阶段独立处理各通道展现了有效性，但该方法在注意力模块中自然引入了显著的计算瓶颈：通道间比较导致注意力开销呈二次增长，从而产生过高的 $\\text{FLOPs}$ 和高昂的训练成本。本文将研究重点从有效性转向跨通道注意力中被忽视的效率问题，并提出核心疑问：“是否必须建模所有通道间的交互？”受稀疏专家混合（Sparse Mixture-of-Experts, $\\text{MoE}$）理念启发，我们提出了 MoE-ViT，一种面向多通道图像的 ViT 架构，将每个通道视为一个专家，并采用轻量级路由器仅选择每个图像块最相关的专家进行注意力计算。在真实世界数据集 JUMP-CP 和 So2Sat 上的概念验证实验表明，$\\text{MoE-ViT}$ 在不牺牲性能、甚至在某些场景下提升性能的同时，实现了显著的效率增益，使其成为多通道成像任务中实用且具吸引力的骨干网络。",
        "translated_title": "稀疏专家混合模型用于多通道成像：所有通道间的交互都必需吗？",
        "label": [],
        "label_reason": "处理多通道图像，非像素级图像恢复任务",
        "relevance_score": 3,
        "novelty_score": 7,
        "novelty_reason": "提出MoE架构提升效率，有性能改进"
    },
    {
        "title": "MCMoE: Completing Missing Modalities with Mixture of Experts for Incomplete Multimodal Action Quality Assessment",
        "url": "http://arxiv.org/abs/2511.17397v1",
        "pub_date": "2025-11-21",
        "summary": "Multimodal Action Quality Assessment (AQA) has recently emerged as a promising paradigm. By leveraging complementary information across shared contextual cues, it enhances the discriminative evaluation of subtle intra-class variations in highly similar action sequences. However, partial modalities are frequently unavailable at the inference stage in reality. The absence of any modality often renders existing multimodal models inoperable. Furthermore, it triggers catastrophic performance degradation due to interruptions in cross-modal interactions. To address this issue, we propose a novel Missing Completion Framework with Mixture of Experts (MCMoE) that unifies unimodal and joint representation learning in single-stage training. Specifically, we propose an adaptive gated modality generator that dynamically fuses available information to reconstruct missing modalities. We then design modality experts to learn unimodal knowledge and dynamically mix the knowledge of all experts to extract cross-modal joint representations. With a mixture of experts, missing modalities are further refined and complemented. Finally, in the training phase, we mine the complete multimodal features and unimodal expert knowledge to guide modality generation and generation-based joint representation extraction. Extensive experiments demonstrate that our MCMoE achieves state-of-the-art results in both complete and incomplete multimodal learning on three public AQA benchmarks. Code is available at https://github.com/XuHuangbiao/MCMoE.",
        "translated": "多模态动作质量评估（AQA）近年来已成为一个颇具前景的研究范式。通过利用共享上下文线索间的互补信息，它能够增强对高度相似动作序列中细微类内差异的判别性评估。然而，在实际应用中，推理阶段往往存在部分模态缺失的情况。任何模态的缺失通常会导致现有多模态模型失效，同时由于跨模态交互中断，还会引发灾难性的性能退化。为解决这一问题，我们提出了一种新颖的缺失补全框架——混合专家结构（MCMoE），该框架在单阶段训练中统一了单模态与联合表示的学习。具体而言，我们设计了一种自适应门控模态生成器，动态融合可用信息以重建缺失模态；随后，我们构建模态专家以学习单模态知识，并动态混合所有专家的知识以提取跨模态联合表示；借助混合专家机制，缺失模态得以进一步优化和补充。在训练阶段，我们挖掘完整的多模态特征与单模态专家知识，以指导模态生成及基于生成的联合表示提取。大量实验表明，我们的MCMoE方法在三个公开的AQA基准数据集上，无论在完整模态还是不完整模态学习任务中均取得了当前最优性能。代码开源于 https://github.com/XuHuangbiao/MCMoE。",
        "translated_title": "MCMoE：基于专家混合完成缺失模态的不完整多模态动作质量评估",
        "label": [],
        "label_reason": "处理多模态缺失，非像素级图像恢复任务。",
        "relevance_score": 1,
        "novelty_score": 6,
        "novelty_reason": "专家混合框架创新，但属高阶模态学习。"
    },
    {
        "title": "Designing and Generating Diverse, Equitable Face Image Datasets for Face Verification Tasks",
        "url": "http://arxiv.org/abs/2511.17393v1",
        "pub_date": "2025-11-21",
        "summary": "Face verification is a significant component of identity authentication in various applications including online banking and secure access to personal devices. The majority of the existing face image datasets often suffer from notable biases related to race, gender, and other demographic characteristics, limiting the effectiveness and fairness of face verification systems. In response to these challenges, we propose a comprehensive methodology that integrates advanced generative models to create varied and diverse high-quality synthetic face images. This methodology emphasizes the representation of a diverse range of facial traits, ensuring adherence to characteristics permissible in identity card photographs. Furthermore, we introduce the Diverse and Inclusive Faces for Verification (DIF-V) dataset, comprising 27,780 images of 926 unique identities, designed as a benchmark for future research in face verification. Our analysis reveals that existing verification models exhibit biases toward certain genders and races, and notably, applying identity style modifications negatively impacts model performance. By tackling the inherent inequities in existing datasets, this work not only enriches the discussion on diversity and ethics in artificial intelligence but also lays the foundation for developing more inclusive and reliable face verification technologies",
        "translated": "人脸识别是在线银行业务和个人设备安全访问等众多应用场景中身份认证的重要组成部分。现有大多数人脸图像数据集普遍存在与种族、性别及其他人口统计特征相关的显著偏差，限制了人脸验证系统的效果与公平性。为应对这些挑战，我们提出了一种综合方法，结合先进的生成模型，创建多样且高质量的合成人脸图像。该方法强调对多种面部特征的全面表征，确保符合身份证照片所允许的特性。此外，我们引入了面向验证的多样化与包容性人脸数据集（Diverse and Inclusive Faces for Verification, DIF-V），包含926个独特身份共27,780张图像，旨在作为未来人脸验证研究的基准。我们的分析表明，现有验证模型对特定性别和种族存在偏倚，并且显著地，应用身份风格修改会降低模型性能。通过解决现有数据集中固有的不平等性，本工作不仅丰富了人工智能领域关于多样性与伦理的讨论，也为构建更具包容性和可靠性的面部验证技术奠定了基础。",
        "translated_title": "为人脸识别任务设计并生成多样化、公平的面部图像数据集",
        "label": [],
        "label_reason": "高阶人脸识别任务，非图像像素级处理",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "生成多样人脸数据集，提升模型公平性"
    },
    {
        "title": "MorphSeek: Fine-grained Latent Representation-Level Policy Optimization for Deformable Image Registration",
        "url": "http://arxiv.org/abs/2511.17392v1",
        "pub_date": "2025-11-21",
        "summary": "Deformable image registration (DIR) remains a fundamental yet challenging problem in medical image analysis, largely due to the prohibitively high-dimensional deformation space of dense displacement fields and the scarcity of voxel-level supervision. Existing reinforcement learning frameworks often project this space into coarse, low-dimensional representations, limiting their ability to capture spatially variant deformations. We propose MorphSeek, a fine-grained representation-level policy optimization paradigm that reformulates DIR as a spatially continuous optimization process in the latent feature space. MorphSeek introduces a stochastic Gaussian policy head atop the encoder to model a distribution over latent features, facilitating efficient exploration and coarse-to-fine refinement. The framework integrates unsupervised warm-up with weakly supervised fine-tuning through Group Relative Policy Optimization, where multi-trajectory sampling stabilizes training and improves label efficiency. Across three 3D registration benchmarks (OASIS brain MRI, LiTS liver CT, and Abdomen MR-CT), MorphSeek achieves consistent Dice improvements over competitive baselines while maintaining high label efficiency with minimal parameter cost and low step-level latency overhead. Beyond optimizer specifics, MorphSeek advances a representation-level policy learning paradigm that achieves spatially coherent and data-efficient deformation optimization, offering a principled, backbone-agnostic, and optimizer-agnostic solution for scalable visual alignment in high-dimensional settings.",
        "translated": "变形图像配准（DIR）仍是医学图像分析中的一个基础而具有挑战性的问题，主要由于密集位移场的高维形变空间以及体素级监督数据稀缺。现有强化学习框架通常将该空间投影到粗粒度、低维表示中，限制了其捕捉空间变化形变的能力。我们提出MorphSeek，这是一种细粒度表示层面的策略优化范式，将DIR重构为在潜在特征空间中连续进行的空间优化过程。MorphSeek在编码器顶部引入了一个随机高斯策略头，用于建模潜在特征上的分布，从而促进高效探索与由粗到精的精细化优化。该框架通过组相对策略优化（Group Relative Policy Optimization），整合无监督预热与弱监督微调，其中多轨迹采样可稳定训练并提升标签效率。在三个3D配准基准测试（OASIS脑部MRI、LiTS肝部CT和腹部MR-CT）上，MorphSeek相较主流基线方法持续实现Dice值提升，同时在参数成本极低且每步延迟开销较小的前提下保持高标签效率。除优化器细节外，MorphSeek还推进了一种表示层面的策略学习范式，实现了空间一致且数据高效的形变优化，提供了一种原理严谨、架构无关、优化器无关的可扩展视觉对齐方案，适用于高维场景。",
        "translated_title": "MorphSeek：面向可变形图像配准的细粒度潜在表示层面策略优化",
        "label": [],
        "label_reason": "属医学图像配准，非像素级图像恢复任务",
        "relevance_score": 3,
        "novelty_score": 7,
        "novelty_reason": "提出新策略优化框架，提升配准效率与鲁棒性"
    },
    {
        "title": "A Little More Like This: Text-to-Image Retrieval with Vision-Language Models Using Relevance Feedback",
        "url": "http://arxiv.org/abs/2511.17255v1",
        "pub_date": "2025-11-21",
        "summary": "Large vision-language models (VLMs) enable intuitive visual search using natural language queries. However, improving their performance often requires fine-tuning and scaling to larger model variants. In this work, we propose a mechanism inspired by traditional text-based search to improve retrieval performance at inference time: relevance feedback. While relevance feedback can serve as an alternative to fine-tuning, its model-agnostic design also enables use with fine-tuned VLMs. Specifically, we introduce and evaluate four feedback strategies for VLM-based retrieval. First, we revise classical pseudo-relevance feedback (PRF), which refines query embeddings based on top-ranked results. To address its limitations, we propose generative relevance feedback (GRF), which uses synthetic captions for query refinement. Furthermore, we introduce an attentive feedback summarizer (AFS), a custom transformer-based model that integrates multimodal fine-grained features from relevant items. Finally, we simulate explicit feedback using ground-truth captions as an upper-bound baseline. Experiments on Flickr30k and COCO with the VLM backbones show that GRF, AFS, and explicit feedback improve retrieval performance by 3-5% in MRR@5 for smaller VLMs, and 1-3% for larger ones, compared to retrieval with no feedback. Moreover, AFS, similarly to explicit feedback, mitigates query drift and is more robust than GRF in iterative, multi-turn retrieval settings. Our findings demonstrate that relevance feedback can consistently enhance retrieval across VLMs and open up opportunities for interactive and adaptive visual search.",
        "translated": "大型视觉-语言模型（VLMs）能够通过自然语言查询实现直观的视觉搜索。然而，提升其性能通常需要微调并扩展至更大的模型变体。在本研究中，我们提出了一种受传统基于文本检索启发的机制，以在推理阶段提升检索性能：相关性反馈。虽然相关性反馈可作为微调的替代方案，但其模型无关的设计也使其能够与微调后的VLMs兼容使用。具体而言，我们针对基于VLM的检索任务引入并评估了四种反馈策略。首先，我们改进了经典的伪相关性反馈（PRF），该方法依据排名靠前的结果对查询嵌入进行优化。为克服其局限性，我们提出了生成式相关性反馈（GRF），利用合成标题对查询进行精细化调整。此外，我们引入了一种名为注意力反馈摘要器（AFS）的定制化Transformer模型，该模型融合了相关物料的多模态细粒度特征。最后，我们通过将真实标注标题模拟为显式反馈，构建了一个上限基线。在Flickr30k和COCO数据集上，使用不同VLM骨干网络进行实验表明：相较于无反馈检索，GRF、AFS及显式反馈分别在MRR@5指标上使小规模VLM性能提升3%-5%，大规模VLM提升1%-3%。此外，在迭代式、多轮检索场景下，AFS与显式反馈类似，能有效缓解查询漂移，并比GRF更具鲁棒性。我们的研究结果表明，相关性反馈可在各类VLM中持续提升检索性能，并为交互式与自适应视觉搜索提供了新的可能性。",
        "translated_title": "更贴近这样：利用视觉-语言模型与相关反馈的文本到图像检索",
        "label": [],
        "label_reason": "论文聚焦视觉搜索，与推荐系统无直接关联。",
        "relevance_score": 3,
        "novelty_score": 6,
        "novelty_reason": "改进检索策略，但非推荐系统专用方法。"
    },
    {
        "title": "Parametric Retrieval-Augmented Generation using Latent Routing of LoRA Adapters",
        "url": "http://arxiv.org/abs/2511.17044v1",
        "pub_date": "2025-11-21",
        "summary": "Parametric Retrieval-Augmented Generation (PRAG) is a novel RAG paradigm that integrates external knowledge directly into a Large Language Model (LLM) by parameterizing documents using LoRA adapters, demonstrating reduced inference costs compared to traditional RAG approaches. However, current PRAG approaches adopt a \\textbf{one-to-one} document encoding scheme, using a dedicated LoRA adapter for each individual document. This scheme introduces two major limitations: First, it leads to data scarcity, as the training datasets for individual LoRA adapters are limited. Second, it incurs high overhead during inference, requiring the merging of LLM weights with a new LoRA adapter for every candidate passage, which is computationally inefficient. To overcome these challenges, we propose a novel paradigm for encoding passages in PRAG that utilizes a latent routing encoding process (Poly-PRAG). During offline encoding, we treat the encoding of a set of documents as a multi-task learning process, where each passage is assigned a unique task identifier. By employing a routing function, we use a small set of latent LoRA adapters to encode the entire passage space. During online inference, this routing function selectively activates a subset of latent experts based on the input query. We conduct comprehensive evaluations of Poly-PRAG across multiple knowledge-intensive NLP tasks. Our extensive experiments demonstrate the effectiveness of the proposed method, achieving state-of-the-art results on four distinct datasets.",
        "translated": "参数化检索增强生成（Parametric Retrieval-Augmented Generation, PRAG）是一种新颖的RAG范式，通过使用LoRA适配器对文档进行参数化，将外部知识直接集成到大语言模型（Large Language Model, LLM）中，相比传统RAG方法具有更低的推理开销。然而，当前PRAG方法采用一种**一对一**的文档编码方案，为每个独立文档配备专用的LoRA适配器。该方案存在两大主要局限性：首先，导致数据稀缺，因为每个LoRA适配器的训练数据集规模有限；其次，推理阶段开销高昂，需针对每个候选段落合并LLM权重与新的LoRA适配器，计算效率低下。为克服这些挑战，我们提出了一种新型PRAG中的段落编码范式——多路路由编码机制（Poly-PRAG）。在离线编码阶段，我们将一组文档的编码视为多任务学习过程，为每段内容分配唯一任务标识符，并借助路由函数，利用少量潜在LoRA适配器来编码整个段落空间。在线推理阶段，该路由函数根据输入查询选择性激活部分潜在专家。我们在多个知识密集型自然语言处理任务上对Poly-PRAG进行全面评估。大量实验表明，所提方法有效，于四个不同数据集上均取得当前最优性能。",
        "translated_title": "基于LoRA适配器潜在路由的参数化检索增强生成",
        "label": [
            "LLM生成式推荐",
            "通用推荐技术"
        ],
        "label_reason": "PRAG方法用于LLM知识增强，间接支持推荐场景",
        "relevance_score": 4,
        "novelty_score": 7,
        "novelty_reason": "提出路由机制优化LoRA适配器使用，提升效率"
    },
    {
        "title": "CLLMRec: LLM-powered Cognitive-Aware Concept Recommendation via Semantic Alignment and Prerequisite Knowledge Distillation",
        "url": "http://arxiv.org/abs/2511.17041v1",
        "pub_date": "2025-11-21",
        "summary": "The growth of Massive Open Online Courses (MOOCs) presents significant challenges for personalized learning, where concept recommendation is crucial. Existing approaches typically rely on heterogeneous information networks or knowledge graphs to capture conceptual relationships, combined with knowledge tracing models to assess learners' cognitive states. However, these methods face significant limitations due to their dependence on high-quality structured knowledge graphs, which are often scarce in real-world educational scenarios. To address this fundamental challenge, this paper proposes CLLMRec, a novel framework that leverages Large Language Models through two synergistic technical pillars: Semantic Alignment and Prerequisite Knowledge Distillation. The Semantic Alignment component constructs a unified representation space by encoding unstructured textual descriptions of learners and concepts. The Prerequisite Knowledge Distillation paradigm employs a teacher-student architecture, where a large teacher LLM (implemented as the Prior Knowledge Aware Component) extracts conceptual prerequisite relationships from its internalized world knowledge and distills them into soft labels to train an efficient student ranker. Building upon these foundations, our framework incorporates a fine-ranking mechanism that explicitly models learners' real-time cognitive states through deep knowledge tracing, ensuring recommendations are both structurally sound and cognitively appropriate. Extensive experiments on two real-world MOOC datasets demonstrate that CLLMRec significantly outperforms existing baseline methods across multiple evaluation metrics, validating its effectiveness in generating truly cognitive-aware and personalized concept recommendations without relying on explicit structural priors.",
        "translated": "大规模开放在线课程（MOOCs）的蓬勃发展为个性化学习带来了严峻挑战，其中概念推荐至关重要。现有方法通常依赖异构信息网络或知识图谱来捕捉概念间的关系，并结合知识追踪模型以评估学习者的认知状态。然而，这些方法受限于对高质量结构化知识图谱的依赖，而此类图谱在现实教育场景中往往稀缺。为应对这一根本性挑战，本文提出CLLMRec框架，该框架通过两大协同技术支柱——语义对齐与先验知识蒸馏——充分利用大语言模型的能力。语义对齐模块通过编码学习者与概念的非结构化文本描述，在统一表征空间中构建语义关联；先验知识蒸馏机制采用教师-学生架构，其中大语言模型作为“先验知识感知组件”充当教师，从其内化的世界知识中提取概念间的先决关系，并将其转化为软标签用于训练高效的学生排序器。在此基础上，本框架进一步引入细粒度排序机制，通过深度知识追踪显式建模学习者实时认知状态，从而确保推荐结果兼具结构合理性与认知适配性。在两个真实MOOC数据集上的广泛实验表明，CLLMRec在多个评估指标上显著优于现有基线方法，验证了其无需依赖显式结构先验即可生成真正认知感知且高度个性化的概念推荐的有效性。",
        "translated_title": "CLLMRec：基于大语言模型的认知感知概念推荐方法，通过语义对齐与先验知识蒸馏实现",
        "label": [
            "LLM生成式推荐",
            "精排",
            "通用推荐技术"
        ],
        "label_reason": "基于LLM构建认知感知概念推荐，聚焦推荐系统精排环节",
        "relevance_score": 9,
        "novelty_score": 8,
        "novelty_reason": "创新融合LLM语义对齐与知识蒸馏，提升个性化推荐效果"
    },
    {
        "title": "RASTP: Representation-Aware Semantic Token Pruning for Generative Recommendation with Semantic Identifiers",
        "url": "http://arxiv.org/abs/2511.16943v1",
        "pub_date": "2025-11-21",
        "summary": "Generative recommendation systems typically leverage Semantic Identifiers (SIDs), which represent each item as a sequence of tokens that encode semantic information. However, representing item ID with multiple SIDs significantly increases input sequence length, which is a major determinant of computational complexity and memory consumption. While existing efforts primarily focus on optimizing attention computation and KV cache, we propose RASTP (Representation-Aware Semantic Token Pruning), which directly prunes less informative tokens in the input sequence. Specifically, RASTP evaluates token importance by combining semantic saliency, measured via representation magnitude, and attention centrality, derived from cumulative attention weights. Since RASTP dynamically prunes low-information or irrelevant semantic tokens, experiments on three real-world Amazon datasets show that RASTP reduces training time by 26.7\\%, while maintaining or slightly improving recommendation performance. The code has been open-sourced at https://github.com/Yuzt-zju/RASTP.",
        "translated": "生成式推荐系统通常利用语义标识符（Semantic Identifiers, SIDs），将每个物料表示为编码语义信息的词元序列。然而，使用多个SID表示物料ID会显著增加输入序列长度，而这是影响计算复杂度和内存消耗的主要因素。尽管现有研究主要聚焦于优化注意力计算与KV缓存，我们提出RASTP（Representation-Aware Semantic Token Pruning），直接从输入序列中剪枝信息量较低的词元。具体而言，RASTP通过结合语义显著性（通过嵌入向量模长衡量）与注意力中心性（由累积注意力权重推导）来评估词元的重要性。由于RASTP动态剪枝低信息量或无关语义词元，实验表明在三个真实世界Amazon数据集上，RASTP可将训练时间减少26.7%，同时保持或略微提升推荐性能。代码已开源至 https://github.com/Yuzt-zju/RASTP。",
        "translated_title": "RASTP：面向生成式推荐的语义标识符感知表示-aware语义标记剪枝",
        "label": [
            "LLM生成式推荐",
            "重排"
        ],
        "label_reason": "聚焦生成式推荐中语义Token剪枝优化",
        "relevance_score": 9,
        "novelty_score": 8,
        "novelty_reason": "提出基于表征与注意力的动态Token剪枝新方法"
    },
    {
        "title": "δ-EMG: A Monotonic Graph Index for Approximate Nearest Neighbor Search",
        "url": "http://arxiv.org/abs/2511.16921v1",
        "pub_date": "2025-11-21",
        "summary": "Approximate nearest neighbor (ANN) search in high-dimensional spaces is a foundational component of many modern retrieval and recommendation systems. Currently, almost all algorithms follow an $ε$-Recall-Bounded principle when comparing performance: they require the ANN search results to achieve a recall of more than $1-ε$ and then compare query-per-second (QPS) performance. However, this approach only accounts for the recall of true positive results and does not provide guarantees on the deviation of incorrect results. To address this limitation, we focus on an Error-Bounded ANN method, which ensures that the returned results are a $(1/δ)$-approximation of the true values. Our approach adopts a graph-based framework. To enable Error-Bounded ANN search, we propose a $δ$-EMG (Error-bounded Monotonic Graph), which, for the first time, provides a provable approximation for arbitrary queries. By enforcing a $δ$-monotonic geometric constraint during graph construction, $δ$-EMG ensures that any greedy search converges to a $(1/δ)$-approximate neighbor without backtracking. Building on this foundation, we design an error-bounded top-$k$ ANN search algorithm that adaptively controls approximation accuracy during query time. To make the framework practical at scale, we introduce $δ$-EMQG (Error-bounded Monotonic Quantized Graph), a localized and degree-balanced variant with near-linear construction complexity. We further integrate vector quantization to accelerate distance computation while preserving theoretical guarantees. Extensive experiments on the ANN-Benchmarks dataset demonstrate the effectiveness of our approach. Under a recall requirement of 0.99, our algorithm achieves 19,000 QPS on the SIFT1M dataset, outperforming other methods by more than 40\\%.",
        "translated": "高维空间中的近似最近邻（ANN）搜索是许多现代检索与推荐系统的基础组件。目前，几乎所有算法在比较性能时均遵循 $ε$-Recall-Bounded 原则：要求 ANN 搜索结果的召回率高于 $1-ε$，并在此前提下比较每秒查询数（QPS）性能。然而，该方法仅考虑真实正样本的召回率，无法对错误结果的偏差提供保证。为解决这一局限性，我们专注于一种误差有界（Error-Bounded）的 ANN 方法，确保返回结果是对真实值的 $(1/δ)$ 近似。我们的方法采用基于图的框架。为实现误差有界的 ANN 检索，我们提出了一种 $δ$-EMG（误差有界单调图），首次为任意查询提供了可证明的近似保证。通过在图构建过程中施加 $δ$-单调几何约束，$δ$-EMG 确保任何贪心搜索均可收敛至 $(1/δ)$ 近似邻居，且无需回溯。在此基础上，我们设计了一种误差有界的 Top-$k$ ANN 搜索算法，在查询阶段自适应控制近似精度。为使框架在大规模场景下具备实用性，我们引入了 $δ$-EMQG（误差有界单调量化图），其为局部化且度均衡的变体，构造复杂度接近线性。我们进一步结合向量量化技术以加速距离计算，同时保持理论保证。在 ANN-Benchmarks 数据集上的大量实验验证了本方法的有效性。在召回率为 0.99 的条件下，我们的算法在 SIFT1M 数据集上达到 19,000 QPS，优于其他方法超过 40\\%。",
        "translated_title": "δ-EMG：一种用于近似最近邻搜索的单调图索引",
        "label": [
            "通用推荐技术"
        ],
        "label_reason": "ANN搜索是推荐系统基础组件，但非专为推荐设计。",
        "relevance_score": 6,
        "novelty_score": 8,
        "novelty_reason": "首次提出误差有界图索引，理论与实践结合显著。"
    },
    {
        "title": "Multimodal Continual Learning with MLLMs from Multi-scenario Perspectives",
        "url": "http://arxiv.org/abs/2511.18507v1",
        "pub_date": "2025-11-23",
        "summary": "Continual learning in visual understanding aims to deal with catastrophic forgetting in Multimodal Large Language Models (MLLMs). MLLMs deployed on devices have to continuously adapt to dynamic scenarios in downstream tasks, such as variations in background and perspective, to effectively perform complex visual tasks. To this end, we construct a multimodal visual understanding dataset (MSVQA) encompassing four different scenarios and perspectives including high altitude, underwater, low altitude and indoor, to investigate the catastrophic forgetting in MLLMs under the dynamics of scenario shifts in real-world data streams. Furthermore, we propose mUltimodal coNtInual learning with MLLMs From multi-scenarIo pERspectives (UNIFIER) to address visual discrepancies while learning different scenarios. Specifically, it decouples the visual information from different scenarios into distinct branches within each vision block and projects them into the same feature space. A consistency constraint is imposed on the features of each branch to maintain the stability of visual representations across scenarios. Extensive experiments on the MSVQA dataset demonstrate that UNIFIER effectively alleviates forgetting of cross-scenario tasks and achieves knowledge accumulation within the same scenario.",
        "translated": "视觉理解中的持续学习旨在解决多模态大语言模型（MLLMs）中的灾难性遗忘问题。部署在设备上的MLLMs必须持续适应下游任务中动态变化的场景，例如背景与视角的变化，以有效执行复杂的视觉任务。为此，我们构建了一个多模态视觉理解数据集（MSVQA），涵盖高空、水下、低空及室内四种不同场景与视角，用以探究在真实世界数据流中场景切换所带来的MLLMs灾难性遗忘现象。此外，我们提出了mUltimodal coNtInual learning with MLLMs From multi-scenarIo pERspectives（UNIFIER），用于在学习不同场景时缓解视觉差异。具体而言，该方法将不同场景下的视觉信息解耦为每个视觉模块内的独立分支，并将其投影至同一特征空间。对各分支特征施加一致性约束，以保持跨场景视觉表征的稳定性。在MSVQA数据集上的大量实验证明，UNIFIER能有效缓解跨场景任务的遗忘现象，并实现同一场景内的知识积累。",
        "translated_title": "多模态持续学习：从多场景视角出发的多模态大模型（MLLMs）研究",
        "label": [],
        "label_reason": "研究多场景视觉理解，非像素级图像处理",
        "relevance_score": 1,
        "novelty_score": 6,
        "novelty_reason": "改进多模态持续学习框架，非图像恢复创新"
    },
    {
        "title": "Extreme Model Compression for Edge Vision-Language Models: Sparse Temporal Token Fusion and Adaptive Neural Compression",
        "url": "http://arxiv.org/abs/2511.18504v1",
        "pub_date": "2025-11-23",
        "summary": "The demand for edge AI in vision-language tasks requires models that achieve real-time performance on resource-constrained devices with limited power and memory. This paper proposes two adaptive compression techniques -- Sparse Temporal Token Fusion (STTF) and Adaptive Neural Compression (ANC) -- that integrate algorithmic innovations with hardware-aware optimizations. Unlike previous approaches relying on static pruning or uniform scaling, STTF dynamically reuses visual tokens through event-driven change detection, while ANC conditionally activates encoder branches via a learned router, enabling fine-grained adaptation to scene complexity. Our 3B-parameter TinyGPT-STTF achieves CIDEr 131.2, BLEU-4 0.38, METEOR 0.31, and ROUGE-L 0.56 on the COCO 2017 test set, surpassing LLaVA-1.5 7B by 17.6 CIDEr points while using 2.3x fewer parameters and 62x fewer on-device FLOPs. TinyGPT-ANC reaches CIDEr 128.5. On event-based vision tasks, STTF reduces average token count by 84% (from 196 to 31 tokens) while preserving 95.6% accuracy on the DVS128 Gesture dataset, and ANC cuts FLOPs by up to 90% in low-motion scenes. Compared to strong baselines, our models improve accuracy by up to 4.4% and reduce latency by up to 13x. These results enable efficient deployment of capable vision-language models on real-world edge devices.",
        "translated": "视觉-语言任务对边缘AI的需求，要求模型能够在资源受限、功耗与内存有限的设备上实现实时性能。本文提出两种自适应压缩技术——稀疏时序标记融合（Sparse Temporal Token Fusion, STTF）与自适应神经压缩（Adaptive Neural Compression, ANC），将算法创新与硬件感知优化相结合。与以往依赖静态剪枝或均匀缩放的方法不同，STTF 通过事件驱动的变更检测动态复用视觉标记，而 ANC 则通过学习到的路由器条件激活编码器分支，从而实现对场景复杂度的细粒度自适应调整。我们的30亿参数模型 TinyGPT-STTF 在 COCO 2017 测试集上取得 CIDEr 131.2、BLEU-4 0.38、METEOR 0.31 和 ROUGE-L 0.56 的表现，相较 LLaVA-1.5 7B 提升 17.6 个 CIDEr 分数，同时参数量减少 2.3 倍、设备端 FLOPs 减少 62 倍。TinyGPT-ANC 达到 CIDEr 128.5。在基于事件的视觉任务中，STTF 将平均标记数量减少 84%（从 196 降至 31 个），并在 DVS128 手势数据集上保持 95.6% 的准确率；ANC 在低运动场景下最多可降低 90% 的 FLOPs。相较于强基线模型，我们的方法最高可提升 4.4% 的准确率，并将延迟降低达 13 倍。这些结果使得具备能力的视觉-语言模型能够在现实世界边缘设备上高效部署。",
        "translated_title": "面向边缘视觉-语言模型的极端模型压缩：稀疏时序标记融合与自适应神经压缩",
        "label": [],
        "label_reason": "任务为边缘视觉语言模型压缩，属高阶任务。",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "提出动态融合与自适应压缩机制，显著提升效率。"
    },
    {
        "title": "Shape-Adapting Gated Experts: Dynamic Expert Routing for Colonoscopic Lesion Segmentation",
        "url": "http://arxiv.org/abs/2511.18493v1",
        "pub_date": "2025-11-23",
        "summary": "The substantial diversity in cell scale and form remains a primary challenge in computer-aided cancer detection on gigapixel Whole Slide Images (WSIs), attributable to cellular heterogeneity. Existing CNN-Transformer hybrids rely on static computation graphs with fixed routing, which consequently causes redundant computation and limits their adaptability to input variability. We propose Shape-Adapting Gated Experts (SAGE), an input-adaptive framework that enables dynamic expert routing in heterogeneous visual networks. SAGE reconfigures static backbones into dynamically routed expert architectures. SAGE's dual-path design features a backbone stream that preserves representation and selectively activates an expert path through hierarchical gating. This gating mechanism operates at multiple hierarchical levels, performing a two-level, hierarchical selection between shared and specialized experts to modulate model logits for Top-K activation. Our Shape-Adapting Hub (SA-Hub) harmonizes structural and semantic representations across the CNN and the Transformer module, effectively bridging diverse modules. Embodied as SAGE-UNet, our model achieves superior segmentation on three medical benchmarks: EBHI, DigestPath, and GlaS, yielding state-of-the-art Dice Scores of 95.57%, 95.16%, and 94.17%, respectively, and robustly generalizes across domains by adaptively balancing local refinement and global context. SAGE provides a scalable foundation for dynamic expert routing, enabling flexible visual reasoning.",
        "translated": "细胞尺度与形态的显著多样性仍是计算机辅助癌症检测在千兆像素全切片图像（WSIs）上的主要挑战，其根源在于细胞异质性。现有CNN-Transformer混合架构依赖静态计算图与固定路由机制，从而导致冗余计算并限制其对输入变化的适应能力。我们提出形状自适应门控专家（Shape-Adapting Gated Experts, SAGE），一种输入自适应框架，可在异构视觉网络中实现动态专家路由。SAGE将静态骨干网络重构为动态路由的专家架构。SAGE采用双路径设计：骨干流用于保留表征，通过分层门控选择性激活专家路径。该门控机制在多个层次级运作，执行共享与专用专家之间的两级分层选择，以调节模型logits，实现Top-K激活。我们的形状自适应枢纽（SA-Hub）有效协调CNN与Transformer模块间的结构与语义表征，弥合不同模块间的差异。作为SAGE-UNet模型，我们在三个医学基准数据集EBHI、DigestPath和GlaS上实现了卓越的分割性能，分别取得95.57%、95.16%和94.17%的Dice得分，并通过自适应平衡局部细化与全局上下文，在跨领域场景下展现出稳健的泛化能力。SAGE为动态专家路由提供可扩展的基础架构，支持灵活的视觉推理。",
        "translated_title": "形状自适应门控专家：用于结肠镜病变分割的动态专家路由",
        "label": [],
        "label_reason": "属于医学图像分割，非像素级图像恢复任务",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "动态专家路由机制对模型结构有改进"
    },
    {
        "title": "Uncertainty Quantification in HSI Reconstruction using Physics-Aware Diffusion Priors and Optics-Encoded Measurements",
        "url": "http://arxiv.org/abs/2511.18473v1",
        "pub_date": "2025-11-23",
        "summary": "Hyperspectral image reconstruction from a compressed measurement is a highly ill-posed inverse problem. Current data-driven methods suffer from hallucination due to the lack of spectral diversity in existing hyperspectral image datasets, particularly when they are evaluated for the metamerism phenomenon. In this work, we formulate hyperspectral image (HSI) reconstruction as a Bayesian inference problem and propose a framework, HSDiff, that utilizes an unconditionally trained, pixel-level diffusion prior and posterior diffusion sampling to generate diverse HSI samples consistent with the measurements of various hyperspectral image formation models. We propose an enhanced metameric augmentation technique using region-based metameric black and partition-of-union spectral upsampling to expand training with physically valid metameric spectra, strengthening the prior diversity and improving uncertainty calibration. We utilize HSDiff to investigate how the studied forward models shape the posterior distribution and demonstrate that guiding with effective spectral encoding provides calibrated informative uncertainty compared to non-encoded models. Through the lens of the Bayesian framework, HSDiff offers a complete, high-performance method for uncertainty-aware HSI reconstruction. Our results also reiterate the significance of effective spectral encoding in snapshot hyperspectral imaging.",
        "translated": "从压缩测量重建高光谱图像是一个高度不适定的逆问题。当前基于数据的方法由于现有高光谱图像数据集中光谱多样性不足，常在评估其对同色异谱现象的处理能力时出现幻觉。本文将高光谱图像（HSI）重建建模为贝叶斯推断问题，并提出一种框架 HSDiff，该框架利用无条件训练的像素级扩散先验与后扩散采样，生成与多种高光谱图像成像模型测量值一致且多样化的 HSI 样本。我们提出了一种增强型同色异谱增强技术，通过基于区域的同色异谱黑体及并集分割光谱上采样方法，扩展训练中物理有效的同色异谱光谱，从而强化先验多样性并提升不确定性校准能力。我们利用 HSDiff 探讨所研究的正向模型如何塑造后验分布，并证明相较于未编码光谱的模型，使用有效光谱编码可提供校准良好的信息性不确定性。在贝叶斯框架视角下，HSDiff 提供了一种完整、高性能的不确定性感知 HSI 重建方法。我们的结果也再次强调了有效光谱编码在瞬时高光谱成像中的重要性。",
        "translated_title": "基于物理感知扩散先验与光学编码测量的高光谱图像重建中的不确定性量化",
        "label": [
            "遥感图像复原"
        ],
        "label_reason": "基于物理先验的高光谱图像重建，属像素级恢复任务。",
        "relevance_score": 4,
        "novelty_score": 7,
        "novelty_reason": "引入扩散模型与物理编码提升不确定性校准，具方法改进。"
    },
    {
        "title": "Robust Posterior Diffusion-based Sampling via Adaptive Guidance Scale",
        "url": "http://arxiv.org/abs/2511.18471v1",
        "pub_date": "2025-11-23",
        "summary": "Diffusion models have recently emerged as powerful generative priors for solving inverse problems, achieving state-of-the-art results across various imaging tasks. A central challenge in this setting lies in balancing the contribution of the prior with the data fidelity term: overly aggressive likelihood updates may introduce artifacts, while conservative updates can slow convergence or yield suboptimal reconstructions. In this work, we propose an adaptive likelihood step-size strategy to guide the diffusion process for inverse-problem formulations. Specifically, we develop an observation-dependent weighting scheme based on the agreement between two different approximations of the intractable intermediate likelihood gradients, that adapts naturally to the diffusion schedule, time re-spacing, and injected stochasticity. The resulting approach, Adaptive Posterior diffusion Sampling (AdaPS), is hyperparameter-free and improves reconstruction quality across diverse imaging tasks - including super-resolution, Gaussian deblurring, and motion deblurring - on CelebA-HQ and ImageNet-256 validation sets. AdaPS consistently surpasses existing diffusion-based baselines in perceptual quality with minimal or no loss in distortion, without any task-specific tuning. Extensive ablation studies further demonstrate its robustness to the number of diffusion steps, observation noise levels, and varying stochasticity.",
        "translated": "扩散模型近期作为求解逆问题的强大生成先验，已在各类成像任务中取得了最先进的性能。在此设定中的核心挑战在于平衡先验与数据保真项的贡献：过于激进的似然更新可能引入伪影，而保守的更新则可能导致收敛缓慢或重建质量不佳。在本工作中，我们提出了一种自适应似然步长策略，用以指导逆问题形式下的扩散过程。具体而言，我们基于对不可计算的中间似然梯度两种不同近似值之间的一致性构建了观测依赖的加权方案，该方案能自然适应扩散时间表、时间重采样及注入的随机性。所提出的这一方法——自适应后验扩散采样（AdaPS）——无需超参数调整，在包括超分辨率、高斯去模糊和运动去模糊等多样成像任务上，在CelebA-HQ和ImageNet-256验证集上均提升了重建质量。AdaPS 在感知质量上持续超越现有基于扩散的基线方法，且在失真方面几乎无损失或损失极小，无需针对特定任务进行调优。大量消融实验进一步证明其对扩散步数、观测噪声水平及不同随机性具有鲁棒性。",
        "translated_title": "基于自适应引导尺度的鲁棒后验扩散采样",
        "label": [
            "超分辨率",
            "图像去模糊"
        ],
        "label_reason": "方法用于图像恢复任务，含超分与去模糊",
        "relevance_score": 9,
        "novelty_score": 8,
        "novelty_reason": "提出自适应引导策略提升扩散模型重建质量"
    },
    {
        "title": "Gaze Beyond the Frame: Forecasting Egocentric 3D Visual Span",
        "url": "http://arxiv.org/abs/2511.18470v1",
        "pub_date": "2025-11-23",
        "summary": "People continuously perceive and interact with their surroundings based on underlying intentions that drive their exploration and behaviors. While research in egocentric user and scene understanding has focused primarily on motion and contact-based interaction, forecasting human visual perception itself remains less explored despite its fundamental role in guiding human actions and its implications for AR/VR and assistive technologies. We address the challenge of egocentric 3D visual span forecasting, predicting where a person's visual perception will focus next within their three-dimensional environment. To this end, we propose EgoSpanLift, a novel method that transforms egocentric visual span forecasting from 2D image planes to 3D scenes. EgoSpanLift converts SLAM-derived keypoints into gaze-compatible geometry and extracts volumetric visual span regions. We further combine EgoSpanLift with 3D U-Net and unidirectional transformers, enabling spatio-temporal fusion to efficiently predict future visual span in the 3D grid. In addition, we curate a comprehensive benchmark from raw egocentric multisensory data, creating a testbed with 364.6K samples for 3D visual span forecasting. Our approach outperforms competitive baselines for egocentric 2D gaze anticipation and 3D localization while achieving comparable results even when projected back onto 2D image planes without additional 2D-specific training.",
        "translated": "人们基于驱动其探索与行为的内在意图，持续感知并与其周围环境互动。尽管以自我为中心的用户与场景理解研究主要聚焦于基于运动和接触的交互方式，但预测人类视觉感知本身仍较少被探索，尽管其在引导人类行为中的基础作用及其对AR/VR与辅助技术的重要意义不容忽视。我们针对以自我为中心的三维视觉范围预测挑战，旨在预测个体在其三维环境中下一刻视觉注意力将聚焦的位置。为此，我们提出EgoSpanLift，一种新颖方法，将以自我为中心的视觉范围预测从二维图像平面拓展至三维场景。EgoSpanLift将SLAM生成的关键点转换为与注视兼容的几何结构，并提取体素化的视觉范围区域。进一步地，我们将EgoSpanLift与3D U-Net及单向Transformer相结合，实现时空融合，从而高效预测三维网格中未来的视觉范围。此外，我们从原始的以自我为中心的多感官数据中构建了一个全面的基准测试集，创建了包含364.6K样本的测试平台用于三维视觉范围预测。我们的方法在以自我为中心的二维注视预测与三维定位任务上均优于现有竞争基线，在将其投影回二维图像平面时（无需额外进行二维特定训练），亦能取得相当的结果。",
        "translated_title": "帧外凝视：预测以自我为中心的三维视觉范围",
        "label": [],
        "label_reason": "任务为3D视觉预测，非像素级图像恢复",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "提出3D视觉跨度预测新框架与模型结构"
    },
    {
        "title": "SloMo-Fast: Slow-Momentum and Fast-Adaptive Teachers for Source-Free Continual Test-Time Adaptation",
        "url": "http://arxiv.org/abs/2511.18468v1",
        "pub_date": "2025-11-23",
        "summary": "Continual Test-Time Adaptation (CTTA) is crucial for deploying models in real-world applications with unseen, evolving target domains. Existing CTTA methods, however, often rely on source data or prototypes, limiting their applicability in privacy-sensitive and resource-constrained settings. Additionally, these methods suffer from long-term forgetting, which degrades performance on previously encountered domains as target domains shift. To address these challenges, we propose SloMo-Fast, a source-free, dual-teacher CTTA framework designed for enhanced adaptability and generalization. It includes two complementary teachers: the Slow-Teacher, which exhibits slow forgetting and retains long-term knowledge of previously encountered domains to ensure robust generalization, and the Fast-Teacher rapidly adapts to new domains while accumulating and integrating knowledge across them. This framework preserves knowledge of past domains and adapts efficiently to new ones. We also introduce Cyclic Test-Time Adaptation (Cyclic-TTA), a novel CTTA benchmark that simulates recurring domain shifts. Our extensive experiments demonstrate that SloMo-Fast consistently outperforms state-of-the-art methods across Cyclic-TTA, as well as ten other CTTA settings, highlighting its ability to both adapt and generalize across evolving and revisited domains.",
        "translated": "持续测试时自适应（Continual Test-Time Adaptation, CTTA）对于在真实世界应用中部署模型至关重要，特别是在面对未见过且不断演变的目标域时。然而，现有CTTA方法通常依赖源数据或原型，限制了其在隐私敏感和资源受限环境中的适用性。此外，这些方法还存在长期遗忘问题，当目标域发生转移时，会导致对先前遇到的域的性能下降。为应对这些挑战，我们提出了SloMo-Fast，一种无源、双教师架构的CTTA框架，旨在增强适应性与泛化能力。该框架包含两个互补的教师：Slow-Teacher具有缓慢遗忘特性，能够保留对先前遇到域的长期知识，从而确保稳健泛化；Fast-Teacher则快速适应新域，并在不同域间积累与整合知识。此框架既能保持对过往域的知识，又能高效适应新域。同时，我们引入了循环测试时自适应（Cyclic-TTA），一种新型CTTA基准，用于模拟周期性域迁移场景。我们的大量实验表明，SloMo-Fast在Cyclic-TTA以及另外十种CTTA设置中均持续优于当前最先进方法，凸显其在面对演化及重访域时兼具适应与泛化的能力。",
        "translated_title": "SloMo-Fast：面向无源持续测试时自适应的慢动量与快自适应教师模型",
        "label": [],
        "label_reason": "任务为持续学习，非图像像素级恢复增强",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "新框架提升适应性，但非图像处理领域"
    },
    {
        "title": "Alternating Perception-Reasoning for Hallucination-Resistant Video Understanding",
        "url": "http://arxiv.org/abs/2511.18463v1",
        "pub_date": "2025-11-23",
        "summary": "Sufficient visual perception is the foundation of video reasoning. Nevertheless, existing Video Reasoning LLMs suffer from perception shortcuts, relying on a flawed single-step perception paradigm. This paradigm describes the video and then conducts reasoning, which runs the risk of insufficient evidence and emergent hallucinations. To address these issues, we introduce a new framework that integrates a loop-based paradigm with an anti-hallucination reward. First, to address the insufficient evidence, we introduce the Perception Loop Reasoning (PLR) paradigm. Instead of describing the video at once, each loop requires the model to describe a video segment with precise timestamps, analyze this segment, and decide the next action. Second, for the risk of hallucinations, the Factual-Aware Evaluator (FAE) evaluates each perception result as a reliable anti-hallucination reward. This reward encourages the model to provide sufficient and precise video evidence. Our FAE, which performs comparably to GPT-4o, is tuned on our AnetHallu-117K, a large-scale hallucination judgment preference dataset. Extensive experiments show that our Video-PLR achieves the state-of-the-art in both 3B and 7B parameter scales and has the best data efficiency. Our code, models, and datasets are released on: https://github.com/BoweiPu/VideoPLR.",
        "translated": "充分的视觉感知是视频推理的基础。然而，现有的视频推理大语言模型（Video Reasoning LLMs）存在感知捷径问题，依赖于一种有缺陷的单步感知范式。该范式先对视频进行描述，再开展推理，容易导致证据不足和涌现式幻觉。为解决这些问题，我们提出了一种新框架，将基于循环的范式与防幻觉奖励相结合。首先，为应对证据不足的问题，我们引入了感知循环推理（Perception Loop Reasoning, PLR）范式。在每个循环中，模型需针对带精确时间戳的视频片段进行描述、分析，并决定下一步动作，而非一次性描述整个视频。其次，为降低幻觉风险，事实感知评估器（Factual-Aware Evaluator, FAE）会对每个感知结果进行评估，作为可靠的防幻觉奖励。该奖励鼓励模型提供充分且精准的视频证据。我们的FAE性能可媲美GPT-4o，在我们构建的大规模幻觉判断偏好数据集AnetHallu-117K上进行了调优。大量实验表明，我们的Video-PLR在3B和7B参数规模下均达到当前最优性能，且数据效率最高。我们的代码、模型及数据集已开源：https://github.com/BoweiPu/VideoPLR。",
        "translated_title": "交替感知-推理机制用于抗幻觉视频理解",
        "label": [],
        "label_reason": "高阶视频理解任务，非像素级图像处理",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "改进推理框架与评估机制，非图像恢复方法"
    },
    {
        "title": "Radiation-Preserving Selective Imaging for Pediatric Hip Dysplasia: A Cross-Modal Ultrasound-Xray Policy with Limited Labels",
        "url": "http://arxiv.org/abs/2511.18457v1",
        "pub_date": "2025-11-23",
        "summary": "We study an ultrasound-first, radiation-preserving policy for developmental dysplasia of the hip (DDH) that requests a radiograph only when needed.   We (i) pretrain modality-specific encoders (ResNet-18) with SimSiam on a large unlabelled registry (37186 ultrasound; 19546 radiographs), (ii) freeze the backbones and fit small, measurement-faithful heads on DDH relevant landmarks and measurements (iii) calibrate a one sided conformal deferral rule on ultrasound predictions that provides finite sample coverage guarantees under exchangeability, using a held-out calibration set. Ultrasound heads predict Graf alpha, beta, and femoral head coverage; X-ray heads predict acetabular index (AI), center-edge (CE) angle and IHDI grade. On our held out labeled evaluation set, ultrasound measurement error is modest (e.g., alpha MAE ~= 9.7 degrees, coverage MAE ~= 14.0%), while radiographic probes achieve AI and CE MAEs of ~= 7.6 degrees and ~= 8.9 degrees, respectively. The calibrated US-only policy is explored across rule families (alpha-only; alpha OR coverage; alpha AND coverage), uncertainty inflation factors, and per-utility trade-offs using decision-curve analysis. Conservative settings yield high coverage with near-zero US-only rates; permissive settings (e.g., alpha OR coverage at larger deltas) achieve non-zero US-only throughput with expected coverage tradeoffs. The result is a simple, reproducible pipeline that turns limited labels into interpretable measurements and tunable selective imaging curves suitable for clinical handoff and future external validation.",
        "translated": "我们研究了一种以超声优先、保留辐射暴露的髋发育不良（DDH）诊断策略，仅在必要时才进行X光检查。具体而言：(i) 在包含大量未标注数据（37,186例超声图像；19,546例X光图像）的大规模注册库上，使用SimSiam对模态特异性编码器（ResNet-18）进行预训练；(ii) 冻结骨干网络，并在与DDH相关的解剖标志物和测量指标上拟合小型、符合测量精度的小型头部模型；(iii) 利用保留校准集，在满足交换性假设的前提下，校准一种单边置信度退避规则，从而提供有限样本覆盖保障。超声头部模型预测Graf角α、β及股骨头覆盖率；X光头部模型预测髋臼指数（AI）、中心角（CE）及IHDI分级。在独立验证的标注数据集上，超声测量误差较小（例如，α平均绝对误差约为9.7度，覆盖率平均绝对误差约为14.0%），而X光探针分别实现AI和CE的平均绝对误差约为7.6度和8.9度。通过决策曲线分析，我们探讨了经校准的纯超声策略在不同规则族（如仅α角；α或覆盖率；α且覆盖率）、不确定性膨胀因子以及每单位效用权衡下的表现。保守设置可实现高覆盖率并接近零比例的纯超声应用；宽松设置（如α或覆盖率在更大阈值下）则可在保持期望覆盖率折衷的前提下实现非零比例的纯超声筛查率。最终结果是一种简单、可复现的流程，能够将有限标注转化为可解释的测量值，并生成可调的择期影像检测曲线，适用于临床交接和未来外部验证。",
        "translated_title": "保留辐射的儿童髋关节发育不良选择性成像：一种基于有限标注的跨模态超声- X 射线策略",
        "label": [],
        "label_reason": "任务为医学影像决策，非像素级图像处理",
        "relevance_score": 1,
        "novelty_score": 3,
        "novelty_reason": "方法复现性高，无本质创新"
    },
    {
        "title": "RegDeepLab: A Two-Stage Decoupled Framework for Interpretable Embryo Fragmentation Grading",
        "url": "http://arxiv.org/abs/2511.18454v1",
        "pub_date": "2025-11-23",
        "summary": "The degree of embryo fragmentation serves as a critical morphological indicator for assessing embryo developmental potential in In Vitro Fertilization (IVF) clinical decision-making. However, current manual grading processes are not only time-consuming but also limited by significant inter-observer variability and efficiency bottlenecks. Although deep learning has demonstrated potential in automated grading in recent years, existing solutions face a significant challenge: pure regression models lack the visual explainability required for clinical practice, while pure segmentation models struggle to directly translate pixel-level masks into precise clinical grades. This study proposes RegDeepLab, a dual-branch Multi-Task Learning (MTL) framework that integrates State-of-the-Art (SOTA) semantic segmentation (DeepLabV3+) with a multi-scale regression head. Addressing the common issues of \"Gradient Conflict\" and \"Negative Transfer\" in multi-task training, we propose a \"Two-Stage Decoupled Training Strategy.\" Experimental results demonstrate that while standard end-to-end MTL training can minimize grading error (MAE=0.046) through our designed \"Feature Injection\" mechanism, it compromises the integrity of segmentation boundaries. In contrast, our decoupled strategy successfully provides robust and high-precision grading predictions while preserving SOTA-level segmentation accuracy (Dice=0.729). Furthermore, we introduce a \"Range Loss\" to effectively utilize large-scale discrete grading data for semi-supervised learning. This study ultimately presents a dual-module clinical auxiliary solution that combines high accuracy with visual explainability.",
        "translated": "胚胎碎片化程度是体外受精（IVF）临床决策中评估胚胎发育潜能的重要形态学指标。然而，当前的手动分级过程不仅耗时，且受限于显著的观察者间差异与效率瓶颈。尽管近年来深度学习在自动分级方面展现出潜力，现有方法仍面临重大挑战：纯回归模型缺乏临床实践所需的视觉可解释性，而纯分割模型难以直接将像素级掩码转化为精确的临床分级。本文提出RegDeepLab，一种双分支多任务学习（MTL）框架，融合了最先进的语义分割模型（DeepLabV3+）与多尺度回归头。针对多任务训练中常见的“梯度冲突”与“负迁移”问题，我们提出“两阶段解耦训练策略”。实验结果表明，虽然标准端到端MTL训练通过我们设计的“特征注入”机制可最小化分级误差（MAE=0.046），但会损害分割边界的完整性；相比之下，我们的解耦策略成功实现了稳健且高精度的分级预测，同时保留了SOTA级别的分割精度（Dice=0.729）。此外，我们引入“范围损失”，以有效利用大规模离散分级数据进行半监督学习。本研究最终呈现了一种兼具高精度与视觉可解释性的双模块临床辅助解决方案。",
        "translated_title": "RegDeepLab：一种用于可解释胚胎碎片分级的两阶段解耦框架",
        "label": [],
        "label_reason": "任务为胚胎分割与分级，属高阶医学图像分析",
        "relevance_score": 3,
        "novelty_score": 6,
        "novelty_reason": "双分支MTL框架改进，但非低层图像处理创新"
    },
    {
        "title": "NAF: Zero-Shot Feature Upsampling via Neighborhood Attention Filtering",
        "url": "http://arxiv.org/abs/2511.18452v1",
        "pub_date": "2025-11-23",
        "summary": "Vision Foundation Models (VFMs) extract spatially downsampled representations, posing challenges for pixel-level tasks. Existing upsampling approaches face a fundamental trade-off: classical filters are fast and broadly applicable but rely on fixed forms, while modern upsamplers achieve superior accuracy through learnable, VFM-specific forms at the cost of retraining for each VFM. We introduce Neighborhood Attention Filtering (NAF), which bridges this gap by learning adaptive spatial-and-content weights through Cross-Scale Neighborhood Attention and Rotary Position Embeddings (RoPE), guided solely by the high-resolution input image. NAF operates zero-shot: it upsamples features from any VFM without retraining, making it the first VFM-agnostic architecture to outperform VFM-specific upsamplers and achieve state-of-the-art performance across multiple downstream tasks. It maintains high efficiency, scaling to 2K feature maps and reconstructing intermediate-resolution maps at 18 FPS. Beyond feature upsampling, NAF demonstrates strong performance on image restoration, highlighting its versatility. Code and checkpoints are available at https://github.com/valeoai/NAF.",
        "translated": "视觉基础模型（VFMs）提取空间下采样的表示，这对像素级任务构成挑战。现有上采样方法面临一个根本性权衡：传统滤波器速度快、适用范围广，但依赖固定形式；而现代上采样器通过可学习的、针对特定 VFM 的形式实现更高精度，却需要为每个 VFM 重新训练。我们提出邻域注意力滤波器（Neighborhood Attention Filtering, NAF），通过跨尺度邻域注意力与旋转位置嵌入（RoPE），仅依据高分辨率输入图像引导学习自适应的空间与内容权重，从而弥合这一差距。NAF 实现零样本操作：可对任意 VFM 输出的特征进行上采样而无需重新训练，是首个在性能上超越 VFM 特定上采样器、并在多个下游任务中达到当前最优表现的 VFM 无关架构。它保持高效性，可扩展至 2K 特征图，并以 18 FPS 的速度重建中间分辨率图。除特征上采样外，NAF 在图像恢复任务中亦展现出强大能力，凸显其通用性。代码与检查点见 https://github.com/valeoai/NAF。",
        "translated_title": "NAF：通过邻域注意力滤波实现零样本特征上采样",
        "label": [
            "超分辨率",
            "图像恢复"
        ],
        "label_reason": "提出零样本特征上采样方法，适用于图像恢复任务",
        "relevance_score": 8,
        "novelty_score": 9,
        "novelty_reason": "首次实现VFM无关的高效上采样架构并超越特定模型"
    },
    {
        "title": "EventBench: Towards Comprehensive Benchmarking of Event-based MLLMs",
        "url": "http://arxiv.org/abs/2511.18448v1",
        "pub_date": "2025-11-23",
        "summary": "Multimodal large language models (MLLMs) have made significant advancements in event-based vision, yet the comprehensive evaluation of their capabilities within a unified benchmark remains largely unexplored. In this work, we introduce EventBench, a benchmark that offers eight diverse task metrics together with a large-scale event stream dataset. EventBench differs from existing event-based benchmarks in four key aspects: (1) openness in accessibility, releasing all raw event streams and task instructions across eight evaluation metrics; (2) diversity in task coverage, spanning understanding, recognition, and spatial reasoning tasks for comprehensive capability assessment; (3) integration in spatial dimensions, pioneering the design of 3D spatial reasoning tasks for event-based MLLMs; and (4) scale in data volume, with an accompanying training set of over one million event-text pairs supporting large-scale training and evaluation. Using EventBench, we evaluate state-of-the-art closed-source models such as GPT-5 and Gemini-2.5 Pro, leading open-source models including Qwen2.5-VL and InternVL3, and event-based MLLMs such as EventGPT that directly process raw event streams. Extensive evaluation reveals that while current event-based MLLMs demonstrate strong performance in event stream understanding, they continue to struggle with fine-grained recognition and spatial reasoning.",
        "translated": "多模态大语言模型（MLLMs）在事件驱动视觉领域已取得显著进展，但其能力在统一基准上的全面评估仍基本未被探索。在本研究中，我们提出了 EventBench，一个包含八种多样化任务指标及大规模事件流数据集的基准评测平台。EventBench 与现有事件驱动基准相比，在四个方面具有显著差异：（1）开放性访问，公开所有八个评估指标所涉及的原始事件流与任务指令；（2）任务覆盖多样性，涵盖理解、识别与空间推理等任务，以实现对模型综合能力的全面评估；（3）空间维度整合，首次为事件驱动 MLLMs 设计三维空间推理任务；（4）数据规模庞大，配套训练集包含超过一百万组事件-文本配对，支持大规模训练与评估。借助 EventBench，我们评估了当前领先的闭源模型（如 GPT-5 和 Gemini-2.5 Pro）、主流开源模型（如 Qwen2.5-VL 和 InternVL3），以及可直接处理原始事件流的事件驱动 MLLMs（如 EventGPT）。广泛评估结果表明，尽管当前事件驱动 MLLMs 在事件流理解方面表现优异，但在细粒度识别与空间推理任务上仍存在明显不足。",
        "translated_title": "EventBench：面向事件驱动型多模态大模型的综合性基准评估",
        "label": [],
        "label_reason": "评估多模态模型能力，非图像像素级处理",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "引入新基准但无图像恢复创新"
    },
    {
        "title": "SineProject: Machine Unlearning for Stable Vision Language Alignment",
        "url": "http://arxiv.org/abs/2511.18444v1",
        "pub_date": "2025-11-23",
        "summary": "Multimodal Large Language Models (MLLMs) increasingly need to forget specific knowledge such as unsafe or private information without requiring full retraining. However, existing unlearning methods often disrupt vision language alignment, causing models to reject both harmful and benign queries. We trace this failure to the projector network during unlearning, its Jacobian becomes severely illconditioned, leading to unstable optimization and drift in cross modal embeddings. We introduce SineProject, a simple method that augments the frozen projector with sinusoidally modulated trainable parameters, improving the Jacobian's spectral conditioning and stabilizing alignment throughout unlearning. Across standard safety and privacy unlearning benchmarks using LLaVA v1.5 7B and 13B, SineProject reduces benign query refusals while achieving complete forgetting of targeted information, yielding state of the art forget retain trade offs with negligible computational overhead.",
        "translated": "多模态大语言模型（MLLMs）日益需要在无需完整重训练的情况下遗忘特定知识，如不安全或隐私信息。然而，现有遗忘方法常会破坏视觉-语言对齐，导致模型拒绝有害与良性查询。我们追溯此失败根源至遗忘过程中的投影器网络：其雅可比矩阵严重病态，引发优化不稳定及跨模态嵌入漂移。为此，我们提出SineProject，一种简单方法，通过为冻结的投影器增加正弦调制的可训练参数进行增强，改善雅可比矩阵的谱条件数，并在整个遗忘过程中稳定对齐效果。在使用LLaVA v1.5 7B和13B模型的标准安全与隐私遗忘基准测试中，SineProject显著降低良性查询被拒绝率，同时实现目标信息完全遗忘，在遗忘与保留权衡上达到当前最优性能，且计算开销几乎可忽略不计。",
        "translated_title": "SineProject：面向稳定视觉-语言对齐的机器遗忘",
        "label": [],
        "label_reason": "处理多模态对齐，非图像像素级恢复任务",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出SineProject稳定投影器，创新性较强"
    },
    {
        "title": "ReCoGS: Real-time ReColoring for Gaussian Splatting scenes",
        "url": "http://arxiv.org/abs/2511.18441v1",
        "pub_date": "2025-11-23",
        "summary": "Gaussian Splatting has emerged as a leading method for novel view synthesis, offering superior training efficiency and real-time inference compared to NeRF approaches, while still delivering high-quality reconstructions. Beyond view synthesis, this 3D representation has also been explored for editing tasks. Many existing methods leverage 2D diffusion models to generate multi-view datasets for training, but they often suffer from limitations such as view inconsistencies, lack of fine-grained control, and high computational demand. In this work, we focus specifically on the editing task of recoloring. We introduce a user-friendly pipeline that enables precise selection and recoloring of regions within a pre-trained Gaussian Splatting scene. To demonstrate the real-time performance of our method, we also present an interactive tool that allows users to experiment with the pipeline in practice. Code is available at https://github.com/loryruta/recogs.",
        "translated": "高斯点渲染（Gaussian Splatting）已成为新视角合成领域的领先方法，相比NeRF方法，其在训练效率和实时推理方面表现更优，同时仍能提供高质量的重建结果。除视角合成外，该三维表示也被探索用于编辑任务。许多现有方法利用二维扩散模型生成多视角数据集以进行训练，但常受限于视角不一致、缺乏精细控制以及计算开销高等问题。在本工作中，我们专注于重着色编辑任务，提出了一种用户友好的流程，使用户能够在预训练的高斯点渲染场景中精确选择并重着色特定区域。为展示方法的实时性能，我们还提供了一个交互式工具，供用户在实践中体验该流程。代码开源地址为 https://github.com/loryruta/recogs。",
        "translated_title": "ReCoGS：面向高斯点渲染场景的实时重着色",
        "label": [],
        "label_reason": "编辑任务属高阶视觉，非像素级图像恢复",
        "relevance_score": 2,
        "novelty_score": 5,
        "novelty_reason": "微调现有管线，无本质方法创新"
    },
    {
        "title": "Perceptual-Evidence Anchored Reinforced Learning for Multimodal Reasoning",
        "url": "http://arxiv.org/abs/2511.18437v1",
        "pub_date": "2025-11-23",
        "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has significantly advanced the reasoning capabilities of Large Language Models (LLMs) and is now being applied to Vision-Language Models (VLMs). However, vanilla RLVR for VLMs verifies only the final textual output, critically neglecting the foundational step of visual perception. This oversight leads to visual hallucinations and reward hacking, as reasoning built upon flawed perception is inherently unreliable. To address this, we propose PEARL (Perceptual-Evidence Anchored Reinforced Learning), a dual-branch, perception-reasoning synergistic that strengthens multimodal reasoning by explicitly anchoring it to verified visual evidence. For each reasoning-oriented QA instance, PEARL first derive a perception checklist -- a set of perception-oriented sub-questions with verifiable answers that probe the model's understanding of key visual evidence. During training, auxiliary rollouts on this checklist yield a perceptual reward that both directly reinforces the model's perception ability and acts as a fidelity gate for reasoning. If the model passes the perception check, its policy update is biased towards evidence-anchored reasoning. Otherwise, the process is halted to prevent reasoning from flawed premises. PEARL can be seamlessly integrated with popular RL methods like GRPO and DAPO. Comprehensive experiments show PEARL achieves substantial gains on multimodal reasoning benchmarks, e.g., a +9.7% improvement over the baseline and +6.6% over GRPO on MathVerse.",
        "translated": "可验证奖励的强化学习（RLVR）已显著提升了大语言模型（LLMs）的推理能力，并正逐步应用于视觉-语言模型（VLMs）。然而，针对 VLMs 的基础 RLVR 仅对最终文本输出进行验证，严重忽视了视觉感知这一核心前提步骤。这种疏忽会导致视觉幻觉与奖励欺骗，因为基于错误感知构建的推理本质上是不可靠的。为解决该问题，我们提出 PEARL（Perceptual-Evidence Anchored Reinforced Learning），一种双分支、感知-推理协同架构，通过显式锚定于经验证的视觉证据，从而增强多模态推理能力。对于每个以推理为导向的问答实例，PEARL 首先生成一个“感知清单”——一组以视觉感知为导向的子问题，其答案具备可验证性，用以探测模型对关键视觉证据的理解程度。在训练过程中，基于该清单进行辅助 rollout 可获得“感知奖励”，一方面直接强化模型的视觉感知能力，另一方面作为推理过程的保真度闸门：若模型通过感知检验，则其策略更新将偏向于证据锚定型推理；反之，若未通过，则流程被终止，以防止从错误前提出发进行推理。PEARL 可无缝集成至 GRPO 和 DAPO 等主流强化学习方法中。综合实验表明，PEARL 在多模态推理基准测试中显著优于基线及 GRPO，分别提升 9.7% 与 6.6%（如 MathVerse 数据集）。",
        "translated_title": "基于感知证据锚定的强化学习用于多模态推理",
        "label": [],
        "label_reason": "聚焦多模态推理，非图像像素级处理",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出感知锚定强化学习框架，显著提升推理可靠性"
    },
    {
        "title": "When Generative Replay Meets Evolving Deepfakes: Domain-Aware Relative Weighting for Incremental Face Forgery Detection",
        "url": "http://arxiv.org/abs/2511.18436v1",
        "pub_date": "2025-11-23",
        "summary": "The rapid advancement of face generation techniques has led to a growing variety of forgery methods. Incremental forgery detection aims to gradually update existing models with new forgery data, yet current sample replay-based methods are limited by low diversity and privacy concerns. Generative replay offers a potential solution by synthesizing past data, but its feasibility for forgery detection remains unclear. In this work, we systematically investigate generative replay and identify two scenarios: when the replay generator closely resembles the new forgery model, generated real samples blur the domain boundary, creating domain-risky samples; when the replay generator differs significantly, generated samples can be safely supervised, forming domain-safe samples. To exploit generative replay effectively, we propose a novel Domain-Aware Relative Weighting (DARW) strategy. DARW directly supervises domain-safe samples while applying a Relative Separation Loss to balance supervision and potential confusion for domain-risky samples. A Domain Confusion Score dynamically adjusts this tradeoff according to sample reliability. Extensive experiments demonstrate that DARW consistently improves incremental learning performance for forgery detection under different generative replay settings and alleviates the adverse impact of domain overlap.",
        "translated": "人脸生成技术的快速发展导致伪造方法种类日益增多。增量式伪造检测旨在利用新伪造数据逐步更新现有模型，但当前基于样本回放的方法受限于数据多样性不足及隐私问题。生成式回放通过合成历史数据提供了一种潜在解决方案，但其在伪造检测中的可行性尚不明确。本文系统研究了生成式回放，并识别出两种场景：当回放生成器与新伪造模型高度相似时，生成的真实样本会模糊域边界，形成域风险样本；当回放生成器差异显著时，生成样本可安全监督，形成域安全样本。为有效利用生成式回放，我们提出一种新颖的域感知相对加权（Domain-Aware Relative Weighting, DARW）策略。DARW直接监督域安全样本，同时引入相对分离损失以平衡对域风险样本的监督与潜在混淆。域混淆分数根据样本可靠性动态调整该权衡。大量实验表明，DARW在不同生成式回放设置下持续提升伪造检测的增量学习性能，并缓解域重叠带来的负面影响。",
        "translated_title": "当生成性重放遇见演化的深度伪造：面向域感知的增量面部伪造检测相对权重机制",
        "label": [],
        "label_reason": "高阶伪造检测任务，非像素级图像处理",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出DARW策略提升增量学习效果"
    },
    {
        "title": "DocPTBench: Benchmarking End-to-End Photographed Document Parsing and Translation",
        "url": "http://arxiv.org/abs/2511.18434v1",
        "pub_date": "2025-11-23",
        "summary": "The advent of Multimodal Large Language Models (MLLMs) has unlocked the potential for end-to-end document parsing and translation. However, prevailing benchmarks such as OmniDocBench and DITrans are dominated by pristine scanned or digital-born documents, and thus fail to adequately represent the intricate challenges of real-world capture conditions, such as geometric distortions and photometric variations. To fill this gap, we introduce DocPTBench, a comprehensive benchmark specifically designed for Photographed Document Parsing and Translation. DocPTBench comprises over 1,300 high-resolution photographed documents from multiple domains, includes eight translation scenarios, and provides meticulously human-verified annotations for both parsing and translation. Our experiments demonstrate that transitioning from digital-born to photographed documents results in a substantial performance decline: popular MLLMs exhibit an average accuracy drop of 18% in end-to-end parsing and 12% in translation, while specialized document parsing models show significant average decrease of 25%. This substantial performance gap underscores the unique challenges posed by documents captured in real-world conditions and reveals the limited robustness of existing models. Dataset and code are available at https://github.com/Topdu/DocPTBench.",
        "translated": "多模态大语言模型（MLLMs）的兴起为端到端文档解析与翻译带来了新的可能性。然而，当前主流基准数据集如OmniDocBench和DITrans主要由原始扫描或数字生成的文档构成，因而无法充分反映真实拍摄环境下复杂的挑战，例如几何畸变与光度变化。为弥补这一不足，我们提出了DocPTBench，这是一个专为照片拍摄文档解析与翻译设计的综合性基准数据集。DocPTBench包含来自多个领域的1,300余份高分辨率照片拍摄文档，涵盖八种翻译场景，并为解析与翻译任务提供经过人工精细校验的标注。我们的实验表明，从数字原生文档转向照片拍摄文档会导致性能显著下降：主流MLLMs在端到端解析中的准确率平均下降18%，翻译任务中下降12%；而专门化的文档解析模型则表现出高达25%的平均性能衰减。这一显著性能差距凸显了现实拍摄条件下文档处理所面临的独特挑战，并揭示了现有模型的鲁棒性仍显不足。数据集与代码已开源于https://github.com/Topdu/DocPTBench。",
        "translated_title": "DocPTBench：端到端拍摄文档解析与翻译的基准测试",
        "label": [],
        "label_reason": "任务为文档解析与翻译，属高阶视觉任务",
        "relevance_score": 1,
        "novelty_score": 6,
        "novelty_reason": "构建新数据集，但非图像像素级恢复方法"
    },
    {
        "title": "LungX: A Hybrid EfficientNet-Vision Transformer Architecture with Multi-Scale Attention for Accurate Pneumonia Detection",
        "url": "http://arxiv.org/abs/2511.18425v1",
        "pub_date": "2025-11-23",
        "summary": "Pneumonia remains a leading global cause of mortality where timely diagnosis is critical. We introduce LungX, a novel hybrid architecture combining EfficientNet's multi-scale features, CBAM attention mechanisms, and Vision Transformer's global context modeling for enhanced pneumonia detection. Evaluated on 20,000 curated chest X-rays from RSNA and CheXpert, LungX achieves state-of-the-art performance (86.5 percent accuracy, 0.943 AUC), representing a 6.7 percent AUC improvement over EfficientNet-B0 baselines. Visual analysis demonstrates superior lesion localization through interpretable attention maps. Future directions include multi-center validation and architectural optimizations targeting 88 percent accuracy for clinical deployment as an AI diagnostic aid.",
        "translated": "肺炎仍是全球主要致死原因之一，及时诊断至关重要。我们提出了LungX，一种新型混合架构，结合了EfficientNet的多尺度特征、CBAM注意力机制以及Vision Transformer的全局上下文建模能力，以提升肺炎检测性能。在RSNA和CheXpert提供的20,000张精选胸部X光片上评估，LungX取得了当前最优性能（准确率86.5%，AUC 0.943），相较EfficientNet-B0基线模型AUC提升了6.7%。可视化分析表明，其可解释性注意力图能更精准定位病灶。未来工作包括多中心验证及针对临床部署（目标准确率88%）的架构优化，以作为人工智能辅助诊断工具。",
        "translated_title": "LungX：一种结合高效网络与视觉Transformer的多尺度注意力架构，用于精准肺炎检测",
        "label": [],
        "label_reason": "任务为医学图像分类，属高阶视觉任务",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "改进架构提升检测性能，但非低级图像处理"
    },
    {
        "title": "CrossJEPA: Cross-Modal Joint-Embedding Predictive Architecture for Efficient 3D Representation Learning from 2D Images",
        "url": "http://arxiv.org/abs/2511.18424v1",
        "pub_date": "2025-11-23",
        "summary": "Image-to-point cross-modal learning has emerged to address the scarcity of large-scale 3D datasets in 3D representation learning. However, current methods that leverage 2D data often result in large, slow-to-train models, making them computationally expensive and difficult to deploy in resource-constrained environments. The architecture design of such models is therefore critical, determining their performance, memory footprint, and compute efficiency. The Joint-embedding Predictive Architecture (JEPA) has gained wide popularity in self-supervised learning for its simplicity and efficiency, but has been under-explored in cross-modal settings, partly due to the misconception that masking is intrinsic to JEPA. In this light, we propose CrossJEPA, a simple Cross-modal Joint Embedding Predictive Architecture that harnesses the knowledge of an image foundation model and trains a predictor to infer embeddings of specific rendered 2D views from corresponding 3D point clouds, thereby introducing a JEPA-style pretraining strategy beyond masking. By conditioning the predictor on cross-domain projection information, CrossJEPA purifies the supervision signal from semantics exclusive to the target domain. We further exploit the frozen teacher design with a one-time target embedding caching mechanism, yielding amortized efficiency. CrossJEPA achieves a new state-of-the-art in linear probing on the synthetic ModelNet40 (94.2%) and the real-world ScanObjectNN (88.3%) benchmarks, using only 14.1M pretraining parameters (8.5M in the point encoder), and about 6 pretraining hours on a standard single GPU. These results position CrossJEPA as a performant, memory-efficient, and fast-to-train framework for 3D representation learning via knowledge distillation. We analyze CrossJEPA intuitively, theoretically, and empirically, and extensively ablate our design choices. Code will be made available.",
        "translated": "图像到点的跨模态学习已兴起，旨在解决三维表示学习中大规模三维数据集稀缺的问题。然而，当前利用二维数据的方法往往导致模型庞大、训练缓慢，计算成本高昂，难以在资源受限环境中部署。此类模型的架构设计因此至关重要，直接决定了其性能、内存占用和计算效率。联合嵌入预测架构（JEPA）因其简洁性和高效性，在自监督学习领域广受欢迎，但在跨模态场景下却鲜有探索，部分原因在于人们误认为掩码机制是JEPA固有的组成部分。为此，我们提出CrossJEPA——一种简单的跨模态联合嵌入预测架构，该架构借助图像基础模型的知识，并训练一个预测器，从对应的三维点云推断出特定渲染二维视图的嵌入，从而实现超越掩码机制的JEPA式预训练策略。通过将预测器条件化于跨域投影信息，CrossJEPA可净化来自目标域特有语义的监督信号。我们进一步采用冻结教师模型并引入一次性目标嵌入缓存机制，以获得摊销效率。CrossJEPA仅使用1410万预训练参数（其中点编码器为850万），在标准单GPU上约需6小时预训练时间，便在合成数据集ModelNet40（94.2%）和真实世界数据集ScanObjectNN（88.3%）上实现了线性探针任务的新最优性能。这些结果表明，CrossJEPA是一种性能优越、内存高效且易于训练的三维表示学习框架，可通过知识蒸馏实现。我们从直观、理论及实证角度对CrossJEPA进行了分析，并对设计选择进行了广泛消融实验。代码将公开提供。",
        "translated_title": "CrossJEPA：面向从2D图像高效学习3D表示的跨模态联合嵌入预测架构",
        "label": [],
        "label_reason": "非图像像素级处理，属3D表示学习",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出跨模态JEPA架构，提升3D表征效率"
    },
    {
        "title": "NeuroVascU-Net: A Unified Multi-Scale and Cross-Domain Adaptive Feature Fusion U-Net for Precise 3D Segmentation of Brain Vessels in Contrast-Enhanced T1 MRI",
        "url": "http://arxiv.org/abs/2511.18422v1",
        "pub_date": "2025-11-23",
        "summary": "Precise 3D segmentation of cerebral vasculature from T1-weighted contrast-enhanced (T1CE) MRI is crucial for safe neurosurgical planning. Manual delineation is time-consuming and prone to inter-observer variability, while current automated methods often trade accuracy for computational cost, limiting clinical use. We present NeuroVascU-Net, the first deep learning architecture specifically designed to segment cerebrovascular structures directly from clinically standard T1CE MRI in neuro-oncology patients, addressing a gap in prior work dominated by TOF-MRA-based approaches. NeuroVascU-Net builds on a dilated U-Net and integrates two specialized modules: a Multi-Scale Contextual Feature Fusion ($MSC^2F$) module at the bottleneck and a Cross-Domain Adaptive Feature Fusion ($CDA^2F$) module at deeper hierarchical layers. $MSC^2F$ captures both local and global information via multi-scale dilated convolutions, while $CDA^2F$ dynamically integrates domain-specific features, enhancing representation while keeping computation low. The model was trained and validated on a curated dataset of T1CE scans from 137 brain tumor biopsy patients, annotated by a board-certified functional neurosurgeon. NeuroVascU-Net achieved a Dice score of 0.8609 and precision of 0.8841, accurately segmenting both major and fine vascular structures. Notably, it requires only 12.4M parameters, significantly fewer than transformer-based models such as Swin U-NetR. This balance of accuracy and efficiency positions NeuroVascU-Net as a practical solution for computer-assisted neurosurgical planning.",
        "translated": "从T1加权对比增强（T1CE）磁共振成像中对脑血管进行精确的三维分割，对于神经外科手术规划的安全至关重要。手动勾画耗时且易受观察者间差异影响，而当前的自动化方法常以牺牲精度换取计算效率，限制了其临床应用。本文提出NeuroVascU-Net，这是首个专门设计用于直接从神经肿瘤学患者临床常规T1CE MRI图像中分割脑血管结构的深度学习架构，弥补了以往研究主要依赖TOF-MRA方法的不足。NeuroVascU-Net基于扩张式U-Net构建，并集成了两个专用模块：瓶颈层处的多尺度上下文特征融合（$MSC^2F$）模块与深层层次结构中的跨域自适应特征融合（$CDA^2F$）模块。$MSC^2F$ 通过多尺度扩张卷积同时捕捉局部与全局信息，而 $CDA^2F$ 则动态整合领域特异性特征，在提升表征能力的同时保持计算量较低。该模型在由一名认证功能神经外科医生标注的137例脑肿瘤活检患者的T1CE扫描数据集上进行了训练和验证。NeuroVascU-Net实现了Dice分数0.8609与准确率0.8841，能够精准分割主要及精细血管结构。值得注意的是，该模型仅需12.4M参数，显著少于如Swin U-NetR等基于Transformer的模型。这一精度与效率的平衡使NeuroVascU-Net成为计算机辅助神经外科手术规划的实际可行方案。",
        "translated_title": "NeuroVascU-Net：一种用于对比增强 T1 MRI 中脑血管精确 3D 分割的统一多尺度与跨域自适应特征融合 U-Net",
        "label": [],
        "label_reason": "目标为3D血管分割，属高阶语义任务",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出新模块提升分割精度与效率"
    },
    {
        "title": "Revisiting Feedback Models for HyDE",
        "url": "http://arxiv.org/abs/2511.19349v1",
        "pub_date": "2025-11-24",
        "summary": "Recent approaches that leverage large language models (LLMs) for pseudo-relevance feedback (PRF) have generally not utilized well-established feedback models like Rocchio and RM3 when expanding queries for sparse retrievers like BM25. Instead, they often opt for a simple string concatenation of the query and LLM-generated expansion content. But is this optimal? To answer this question, we revisit and systematically evaluate traditional feedback models in the context of HyDE, a popular method that enriches query representations with LLM-generated hypothetical answer documents. Our experiments show that HyDE's effectiveness can be substantially improved when leveraging feedback algorithms such as Rocchio to extract and weight expansion terms, providing a simple way to further enhance the accuracy of LLM-based PRF methods.",
        "translated": "近期利用大语言模型（LLM）进行伪相关反馈（PRF）的方法，通常未在针对如BM25等稀疏检索器扩展查询时采用Rocchio和RM3等成熟反馈模型。相反，它们往往仅将原始查询与LLM生成的扩展内容简单拼接成字符串。但这种做法是否最优？为回答该问题，我们重新审视并系统评估了传统反馈模型在HyDE方法中的适用性——HyDE是一种流行方法，通过LLM生成的假设答案文档来丰富查询表示。实验表明，当结合如Rocchio等反馈算法提取并加权扩展项时，HyDE的效果可显著提升，从而为进一步增强基于LLM的PRF方法的准确性提供了一种简洁可行的途径。",
        "translated_title": "重访HyDE的反馈模型",
        "label": [
            "通用推荐技术",
            "负采样与对比学习"
        ],
        "label_reason": "HyDE用于检索增强，非推荐系统核心环节",
        "relevance_score": 4,
        "novelty_score": 6,
        "novelty_reason": "改进传统反馈模型，但未针对推荐场景优化"
    },
    {
        "title": "Generative Query Expansion with Multilingual LLMs for Cross-Lingual Information Retrieval",
        "url": "http://arxiv.org/abs/2511.19325v1",
        "pub_date": "2025-11-24",
        "summary": "Query expansion is the reformulation of a user query by adding semantically related information, and is an essential component of monolingual and cross-lingual information retrieval used to ensure that relevant documents are not missed. Recently, multilingual large language models (mLLMs) have shifted query expansion from semantic augmentation with synonyms and related words to pseudo-document generation. Pseudo-documents both introduce additional relevant terms and bridge the gap between short queries and long documents, which is particularly beneficial in dense retrieval. This study evaluates recent mLLMs and fine-tuned variants across several generative expansion strategies to identify factors that drive cross-lingual retrieval performance. Results show that query length largely determines which prompting technique is effective, and that more elaborate prompts often do not yield further gains. Substantial linguistic disparities persist: cross-lingual query expansion can produce the largest improvements for languages with the weakest baselines, yet retrieval is especially poor between languages written in different scripts. Fine-tuning is found to lead to performance gains only when the training and test data are of similar format. These outcomes underline the need for more balanced multilingual and cross-lingual training and evaluation resources.",
        "translated": "查询扩展是通过添加语义相关的信息对用户查询进行改写，是单语言和跨语言信息检索中的关键组件，旨在确保不遗漏相关文档。近年来，多语言大语言模型（mLLMs）已将查询扩展从使用同义词和相关词汇的语义增强，转变为伪文档生成。伪文档不仅引入了额外的相关术语，还弥合了短查询与长文档之间的差距，尤其在稠密检索中具有显著优势。本研究评估了多种生成式扩展策略下近期mLLMs及其微调变体的表现，以识别驱动跨语言检索性能的关键因素。结果表明，查询长度在很大程度上决定了哪种提示技术有效；而更复杂的提示往往并未带来进一步提升。语言间的显著差异依然存在：跨语言查询扩展在基线最弱的语言中能产生最大改进，但在不同书写系统的语言间，检索效果尤为不佳。研究发现，仅当训练数据与测试数据格式相似时，微调才会带来性能增益。这些结果凸显了亟需更加平衡的多语言与跨语言训练和评测资源。",
        "translated_title": "多语言大语言模型驱动的生成式查询扩展用于跨语言信息检索",
        "label": [],
        "label_reason": "聚焦跨语言信息检索，非推荐系统核心问题",
        "relevance_score": 3,
        "novelty_score": 5,
        "novelty_reason": "方法改进有限，未针对推荐场景设计"
    },
    {
        "title": "What Drives Cross-lingual Ranking? Retrieval Approaches with Multilingual Language Models",
        "url": "http://arxiv.org/abs/2511.19324v1",
        "pub_date": "2025-11-24",
        "summary": "Cross-lingual information retrieval (CLIR) enables access to multilingual knowledge but remains challenging due to disparities in resources, scripts, and weak cross-lingual semantic alignment in embedding models. Existing pipelines often rely on translation and monolingual retrieval heuristics, which add computational overhead and noise, degrading performance. This work systematically evaluates four intervention types, namely document translation, multilingual dense retrieval with pretrained encoders, contrastive learning at word, phrase, and query-document levels, and cross-encoder re-ranking, across three benchmark datasets. We find that dense retrieval models trained specifically for CLIR consistently outperform lexical matching methods and derive little benefit from document translation. Contrastive learning mitigates language biases and yields substantial improvements for encoders with weak initial alignment, and re-ranking can be effective, but depends on the quality of the cross-encoder training data. Although high-resource languages still dominate overall performance, gains over lexical and document-translated baselines are most pronounced for low-resource and cross-script pairs. These findings indicate that cross-lingual search systems should prioritise semantic multilingual embeddings and targeted learning-based alignment over translation-based pipelines, particularly for cross-script and under-resourced languages.",
        "translated": "跨语言信息检索（CLIR）能够访问多语言知识，但由于资源、字符集差异以及嵌入模型中弱的跨语言语义对齐，仍面临挑战。现有流程通常依赖于翻译和单语言检索启发式方法，这会增加计算开销并引入噪声，从而降低性能。本文系统性地评估了四种干预类型——文档翻译、基于预训练编码器的多语言稠密检索、词、短语及查询-文档层面的对比学习，以及交叉编码器重排——在三个基准数据集上的表现。我们发现，专门针对 CLIR 训练的稠密检索模型始终优于词汇匹配方法，并且从文档翻译中获益甚微。对比学习可缓解语言偏倚，显著提升初始语义对齐能力较弱的编码器性能；而重排虽有效，但其效果依赖于交叉编码器训练数据的质量。尽管高资源语言仍主导整体性能，但在低资源语言及跨字符集配对上，相对于词汇匹配和文档翻译基线方法，性能提升最为显著。这些结果表明，跨语言搜索系统应优先采用语义型多语言嵌入与基于目标学习的对齐方法，而非依赖翻译管道，尤其适用于跨字符集和资源匮乏语言。",
        "translated_title": "什么驱动跨语言排序？基于多语言语言模型的检索方法",
        "label": [
            "召回",
            "重排",
            "负采样与对比学习"
        ],
        "label_reason": "研究跨语言检索中的召回与重排方法，含对比学习优化。",
        "relevance_score": 4,
        "novelty_score": 6,
        "novelty_reason": "对比学习与重排策略改进，但非推荐系统专用创新。"
    },
    {
        "title": "From Raw Features to Effective Embeddings: A Three-Stage Approach for Multimodal Recipe Recommendation",
        "url": "http://arxiv.org/abs/2511.19176v1",
        "pub_date": "2025-11-24",
        "summary": "Recipe recommendation has become an essential task in web-based food platforms. A central challenge is effectively leveraging rich multimodal features beyond user-recipe interactions. Our analysis shows that even simple uses of multimodal signals yield competitive performance, suggesting that systematic enhancement of these signals is highly promising. We propose TESMR, a 3-stage framework for recipe recommendation that progressively refines raw multimodal features into effective embeddings through: (1) content-based enhancement using foundation models with multimodal comprehension, (2) relation-based enhancement via message propagation over user-recipe interactions, and (3) learning-based enhancement through contrastive learning with learnable embeddings. Experiments on two real-world datasets show that TESMR outperforms existing methods, achieving 7-15% higher Recall@10.",
        "translated": "食谱推荐已成为基于网络的食品平台中的关键任务。其核心挑战在于有效利用用户-食谱交互之外的丰富多模态特征。我们的分析表明，即使对多模态信号进行简单的使用也能获得具有竞争力的性能，这表明系统性增强这些信号具有巨大潜力。我们提出TESMR，一种用于食谱推荐的三阶段框架，通过以下三个步骤逐步将原始多模态特征精炼为有效的嵌入：（1）基于内容的增强，利用具备多模态理解能力的基础模型；（2）基于关系的增强，通过在用户-食谱交互图上进行消息传播实现；（3）基于学习的增强，通过对比学习与可学习嵌入完成。在两个真实世界数据集上的实验表明，TESMR优于现有方法，Recall@10提升7%至15%。",
        "translated_title": "从原始特征到有效嵌入：面向多模态食谱推荐的三阶段方法",
        "label": [
            "多模态推荐",
            "精排"
        ],
        "label_reason": "提出三阶段框架优化多模态特征用于食谱推荐",
        "relevance_score": 9,
        "novelty_score": 8,
        "novelty_reason": "创新性融合基础模型、图传播与对比学习"
    },
    {
        "title": "Heterogeneous Multi-treatment Uplift Modeling for Trade-off Optimization in Short-Video Recommendation",
        "url": "http://arxiv.org/abs/2511.18997v1",
        "pub_date": "2025-11-24",
        "summary": "The rapid proliferation of short videos on social media platforms presents unique challenges and opportunities for recommendation systems. Users exhibit diverse preferences, and the responses resulting from different strategies often conflict with one another, potentially exhibiting inverse correlations between metrics such as watch time and video view counts. Existing uplift models face limitations in handling the heterogeneous multi-treatment scenarios of short-video recommendations, often failing to effectively capture both the synergistic and individual causal effects of different strategies. Furthermore, traditional fixed-weight approaches for balancing these responses lack personalization and can result in biased decision-making. To address these issues, we propose a novel Heterogeneous Multi-treatment Uplift Modeling (HMUM) framework for trade-off optimization in short-video recommendations. HMUM comprises an Offline Hybrid Uplift Modeling (HUM) module, which captures the synergistic and individual effects of multiple strategies, and an Online Dynamic Decision-Making (DDM) module, which estimates the value weights of different user responses in real-time for personalized decision-making. Evaluated on two public datasets, an industrial dataset, and through online A/B experiments on the Kuaishou platform, our model demonstrated superior offline performance and significant improvements in key metrics. It is now fully deployed on the platform, benefiting hundreds of millions of users.",
        "translated": "社交媒体平台上短视频的迅猛增长为推荐系统带来了独特的挑战与机遇。用户表现出多样化偏好，不同策略所产生的响应往往相互冲突，相关指标（如观看时长与视频播放量）可能存在负相关关系。现有提升模型在处理短视频推荐中的异构多策略场景时存在局限，难以有效捕捉不同策略间协同效应与个体因果效应。此外，传统固定权重方法在平衡这些响应时缺乏个性化，易导致决策偏倚。为解决上述问题，我们提出了一种面向短视频推荐的异构多策略提升建模框架（Heterogeneous Multi-treatment Uplift Modeling, HMUM）。HMUM 包含两个模块：离线混合提升建模模块（Offline Hybrid Uplift Modeling, HUM），用于捕捉多种策略间的协同效应与个体效应；以及在线动态决策模块（Online Dynamic Decision-Making, DDM），用于实时估计不同用户响应的价值权重，以实现个性化决策。该模型在两个公开数据集、一个工业数据集上进行评估，并通过快手平台的线上 A/B 实验验证，展现出卓越的离线性能及关键指标的显著提升，目前已全面部署于平台，惠及数亿用户。",
        "translated_title": "异质多干预提升建模在短视频推荐中的权衡优化",
        "label": [
            "精排",
            "推荐系统评估"
        ],
        "label_reason": "聚焦短视频推荐中的多策略因果建模与优化。",
        "relevance_score": 9,
        "novelty_score": 8,
        "novelty_reason": "提出异构多处理提升模型，动态个性化权衡策略。"
    },
    {
        "title": "STORE: Semantic Tokenization, Orthogonal Rotation and Efficient Attention for Scaling Up Ranking Models",
        "url": "http://arxiv.org/abs/2511.18805v1",
        "pub_date": "2025-11-24",
        "summary": "Ranking models have become an important part of modern personalized recommendation systems. However, significant challenges persist in handling high-cardinality, heterogeneous, and sparse feature spaces, particularly regarding model scalability and efficiency. We identify two key bottlenecks: (i) Representation Bottleneck: Driven by the high cardinality and dynamic nature of features, model capacity is forced into sparse-activated embedding layers, leading to low-rank representations. This, in turn, triggers phenomena like \"One-Epoch\" and \"Interaction-Collapse,\" ultimately hindering model scalability.(ii) Computational Bottleneck: Integrating all heterogeneous features into a unified model triggers an explosion in the number of feature tokens, rendering traditional attention mechanisms computationally demanding and susceptible to attention dispersion. To dismantle these barriers, we introduce STORE, a unified and scalable token-based ranking framework built upon three core innovations: (1) Semantic Tokenization fundamentally tackles feature heterogeneity and sparsity by decomposing high-cardinality sparse features into a compact set of stable semantic tokens; and (2) Orthogonal Rotation Transformation is employed to rotate the subspace spanned by low-cardinality static features, which facilitates more efficient and effective feature interactions; and (3) Efficient attention that filters low-contributing tokens to improve computional efficiency while preserving model accuracy. Across extensive offline experiments and online A/B tests, our framework consistently improves prediction accuracy(online CTR by 2.71%, AUC by 1.195%) and training effeciency (1.84 throughput).",
        "translated": "排序模型已成为现代个性化推荐系统的重要组成部分。然而，在处理高基数、异构且稀疏的特征空间时，仍面临显著挑战，尤其体现在模型的可扩展性与效率方面。我们识别出两个关键瓶颈：(i) 表示瓶颈：由于特征具有高基数与动态特性，模型容量被迫依赖稀疏激活的嵌入层，导致低秩表示。这进一步引发“单轮训练”及“交互坍塌”等现象，最终阻碍模型的可扩展性；(ii) 计算瓶颈：将所有异构特征整合至统一模型中，会激增特征标记数量，使传统注意力机制计算开销陡增，并易受注意力分散影响。为突破上述限制，我们提出 STORE，一种基于令牌的统一且可扩展的排序框架，其构建于三项核心创新之上：(1) 语义令牌化从根本上解决特征异构性与稀疏性问题，通过将高基数稀疏特征分解为一组紧凑且稳定的语义令牌；(2) 正交旋转变换被用于旋转低基数静态特征所张成的子空间，从而更高效地促进特征交互；(3) 高效注意力机制通过过滤贡献度较低的令牌，在提升计算效率的同时保持模型精度。在广泛的离线实验和线上A/B测试中，我们的框架持续提升预测准确率（线上点击率提升2.71%，AUC提升1.195%）和训练效率（吞吐量提升1.84倍）。",
        "translated_title": "STORE：面向大规模排序模型的语义分词、正交旋转与高效注意力机制",
        "label": [
            "精排",
            "通用推荐技术"
        ],
        "label_reason": "聚焦精排模型效率与表示瓶颈，专为推荐系统优化",
        "relevance_score": 9,
        "novelty_score": 8,
        "novelty_reason": "创新性提出三模块协同提升效率与精度"
    },
    {
        "title": "Large Language Models Require Curated Context for Reliable Political Fact-Checking -- Even with Reasoning and Web Search",
        "url": "http://arxiv.org/abs/2511.18749v1",
        "pub_date": "2025-11-24",
        "summary": "Large language models (LLMs) have raised hopes for automated end-to-end fact-checking, but prior studies report mixed results. As mainstream chatbots increasingly ship with reasoning capabilities and web search tools -- and millions of users already rely on them for verification -- rigorous evaluation is urgent. We evaluate 15 recent LLMs from OpenAI, Google, Meta, and DeepSeek on more than 6,000 claims fact-checked by PolitiFact, comparing standard models with reasoning- and web-search variants. Standard models perform poorly, reasoning offers minimal benefits, and web search provides only moderate gains, despite fact-checks being available on the web. In contrast, a curated RAG system using PolitiFact summaries improved macro F1 by 233% on average across model variants. These findings suggest that giving models access to curated high-quality context is a promising path for automated fact-checking.",
        "translated": "大语言模型（LLMs）为实现端到端自动化事实核查带来了希望，但此前研究结果参差不齐。随着主流聊天机器人日益集成推理能力与网络搜索工具——且已有数百万用户依赖它们进行信息核实——亟需对其进行严谨评估。我们对来自OpenAI、Google、Meta及DeepSeek的15种最新LLMs，在由PolitiFact核查的6000余条声明上进行了评估，对比标准模型与具备推理和网络搜索功能的变体。结果显示，标准模型表现不佳，推理能力带来的提升微乎其微，而网络搜索虽能带来适度增益，但事实核查内容本身已可于网络获取。相比之下，使用PolitiFact摘要构建的精选RAG系统，在所有模型变体中平均将宏F1值提升了233%。这些发现表明，为模型提供精选高质量上下文是自动化事实核查颇具前景的发展路径。",
        "translated_title": "大语言模型需要经过筛选的上下文才能可靠地进行政治事实核查——即使具备推理与网络搜索能力",
        "label": [],
        "label_reason": "研究聚焦事实核查，非推荐系统核心问题",
        "relevance_score": 3,
        "novelty_score": 5,
        "novelty_reason": "改进RAG系统，但未应用于推荐场景"
    },
    {
        "title": "Multimodal Large Language Models with Adaptive Preference Optimization for Sequential Recommendation",
        "url": "http://arxiv.org/abs/2511.18740v1",
        "pub_date": "2025-11-24",
        "summary": "Recent advances in Large Language Models (LLMs) have opened new avenues for sequential recommendation by enabling natural language reasoning over user behavior sequences. A common approach formulates recommendation as a language modeling task, where interaction histories are transformed into prompts and user preferences are learned via supervised fine-tuning. However, these methods operate solely in the textual modality and often miss users' fine-grained interests, especially when shaped by rich visual signals such as product images or movie posters. Multimodal Large Language Models (MLLMs) offer a promising alternative by aligning text and vision in a shared semantic space. A prevalent training paradigm applies Supervised Fine-Tuning (SFT) followed by Direct Preference Optimization (DPO) to model user preferences. Yet, two core challenges remain: 1) Imbalanced sample hardness, where random negative sampling causes overfitting on easy examples and under-training on hard ones; 2) Cross-modal semantic bias, where the fixed reference model in DPO prevents the policy model from correcting modality misalignments--especially over long sequences. To address these issues, we propose a Multimodal LLM framework that integrates Hardness-aware and Noise-regularized preference optimization for Recommendation (HaNoRec). Specifically, HaNoRec dynamically adjusts optimization weights based on both the estimated hardness of each training sample and the policy model's real-time responsiveness, prioritizing harder examples. It further introduces Gaussian-perturbed distribution optimization on output logits to enhance cross-modal semantic consistency and reduce modality bias inherited from the reference model.",
        "translated": "近年来，大语言模型（LLMs）的发展为序列推荐开辟了新路径，使其能够对用户行为序列进行自然语言推理。一种常见方法将推荐任务建模为语言建模任务，其中交互历史被转化为提示（prompt），并通过监督微调学习用户偏好。然而，这些方法仅在文本模态中运作，常忽略用户的细粒度兴趣，尤其是在用户兴趣受丰富视觉信号（如商品图片或电影海报）影响时。多模态大语言模型（MLLMs）提供了一种有前景的替代方案，通过在共享语义空间中对齐文本与视觉信息。主流训练范式通常采用监督微调（SFT）后接直接偏好优化（DPO）以建模用户偏好。然而，仍存在两个核心挑战：1）样本难度不平衡问题，随机负采样导致模型在简单样本上过拟合、在困难样本上欠训练；2）跨模态语义偏差问题，DPO中固定的参考模型阻碍策略模型纠正模态错位——尤其在长序列场景下更为显著。为解决上述问题，我们提出了一种融合难度感知与噪声正则化偏好优化的推荐框架（HaNoRec）。具体而言，HaNoRec 根据每个训练样本的预估难度及策略模型的实时响应动态调整优化权重，优先处理较难样本；同时引入高斯扰动分布优化输出 logits，以增强跨模态语义一致性，并降低继承自参考模型的模态偏倚。",
        "translated_title": "面向序列推荐的多模态大语言模型与自适应偏好优化",
        "label": [
            "LLM生成式推荐",
            "序列推荐",
            "负采样与对比学习"
        ],
        "label_reason": "直接利用MLLM解决序列推荐中多模态偏好优化问题",
        "relevance_score": 10,
        "novelty_score": 9,
        "novelty_reason": "创新性提出HaNoRec框架，解决样本硬度与跨模态偏置"
    },
    {
        "title": "When and What to Recommend: Joint Modeling of Timing and Content for Active Sequential Recommendation",
        "url": "http://arxiv.org/abs/2511.18717v1",
        "pub_date": "2025-11-24",
        "summary": "Sequential recommendation models user preferences to predict the next target item. Most existing work is passive, where the system responds only when users open the application, missing chances after closure. We investigate active recommendation, which predicts the next interaction time and actively delivers items. Two challenges: accurately estimating the Time of Interest (ToI) and generating Item of Interest (IoI) conditioned on the predicted ToI. We propose PASRec, a diffusion-based framework that aligns ToI and IoI via a joint objective. Experiments on five benchmarks show superiority over eight state-of-the-art baselines under leave-one-out and temporal splits.",
        "translated": "序列推荐模型利用用户偏好预测下一个目标物品。现有大多数工作属于被动式推荐，系统仅在用户打开应用时响应，忽视了关闭应用后的潜在交互机会。我们研究主动推荐方法，即预测下一次交互时间，并主动推送物品。该方法面临两大挑战：准确估计兴趣时间（ToI），以及在预测的 ToI 条件下生成兴趣物品（IoI）。我们提出 PASRec，一种基于扩散的框架，通过联合目标对齐 ToI 与 IoI。在五个基准数据集上的实验表明，在留一验证和时间划分下，PASRec 在八种当前最先进基线方法之上表现更优。",
        "translated_title": "何时推荐何物：面向主动序列推荐的时序与内容联合建模",
        "label": [
            "序列推荐",
            "主动推荐"
        ],
        "label_reason": "提出主动时序内容联合建模框架，解决推荐时机问题",
        "relevance_score": 9,
        "novelty_score": 8,
        "novelty_reason": "首次结合扩散模型进行时间与物品联合预测"
    },
    {
        "title": "A Recommender System Based on Binary Matrix Representations for Cognitive Disorders",
        "url": "http://arxiv.org/abs/2511.18645v1",
        "pub_date": "2025-11-23",
        "summary": "Diagnosing cognitive (mental health) disorders is a delicate and complex task. Identifying the next most informative symptoms to assess, in order to distinguish between possible disorders, presents an additional challenge. This process requires comprehensive knowledge of diagnostic criteria and symptom overlap across disorders, making it difficult to navigate based on symptoms alone. This research aims to develop a recommender system for cognitive disorder diagnosis using binary matrix representations. The core algorithm utilizes a binary matrix of disorders and their symptom combinations. It filters through the rows and columns based on the patient's current symptoms to identify potential disorders and recommend the most informative next symptoms to examine. A prototype of the recommender system was implemented in Python. Using synthetic test and some real-life data, the system successfully identified plausible disorders from an initial symptom set and recommended further symptoms to refine the diagnosis. It also provided additional context on the symptom-disorder relationships. Although this is a prototype, the recommender system shows potential as a clinical support tool. A fully-developed application of this recommender system may assist mental health professionals in identifying relevant disorders more efficiently and guiding symptom-specific follow-up investigations to improve diagnostic accuracy.",
        "translated": "诊断认知（精神健康）障碍是一项精细且复杂的任务。识别用于区分可能障碍的下一个最具信息量的症状，构成了额外的挑战。这一过程要求对各类障碍的诊断标准及症状交叉重叠情况有全面了解，仅依据症状难以有效导航。本研究旨在开发一种基于二值矩阵表示的认知障碍诊断推荐系统。核心算法利用障碍及其症状组合构成的二值矩阵，通过患者当前症状筛选行与列，以识别潜在障碍，并推荐最具有信息量的下一个待评估症状。该推荐系统的原型使用Python实现。在合成测试数据和部分真实数据上的实验表明，系统能够从初始症状集合中成功识别出合理的障碍，并推荐进一步检查的症状以细化诊断，同时提供症状-障碍关系的附加上下文信息。尽管目前仍为原型系统，该推荐系统展现出作为临床辅助工具的潜力。若该推荐系统得以充分开发，或可帮助心理健康专业人员更高效地识别相关障碍，并引导针对特定症状的后续调查，从而提升诊断准确性。",
        "translated_title": "基于二元矩阵表示的认知障碍推荐系统",
        "label": [
            "通用推荐技术",
            "推荐系统评估"
        ],
        "label_reason": "基于矩阵推荐诊断，属医疗场景推荐应用",
        "relevance_score": 7,
        "novelty_score": 6,
        "novelty_reason": "方法常规但适配临床场景，有实用价值"
    },
    {
        "title": "General Agentic Memory Via Deep Research",
        "url": "http://arxiv.org/abs/2511.18423v1",
        "pub_date": "2025-11-23",
        "summary": "Memory is critical for AI agents, yet the widely-adopted static memory, aiming to create readily available memory in advance, is inevitably subject to severe information loss. To address this limitation, we propose a novel framework called \\textbf{general agentic memory (GAM)}. GAM follows the principle of \"\\textbf{just-in time (JIT) compilation}\" where it focuses on creating optimized contexts for its client at runtime while keeping only simple but useful memory during the offline stage. To this end, GAM employs a duo-design with the following components. 1) \\textbf{Memorizer}, which highlights key historical information using a lightweight memory, while maintaining complete historical information within a universal page-store. 2) \\textbf{Researcher}, which retrieves and integrates useful information from the page-store for its online request guided by the pre-constructed memory. This design allows GAM to effectively leverage the agentic capabilities and test-time scalability of frontier large language models (LLMs), while also facilitating end-to-end performance optimization through reinforcement learning. In our experimental study, we demonstrate that GAM achieves substantial improvement on various memory-grounded task completion scenarios against existing memory systems.",
        "translated": "记忆对AI代理至关重要，然而广泛采用的静态记忆机制，旨在提前构建可随时调用的记忆，不可避免地面临严重的信息丢失问题。为解决这一局限性，我们提出了一种新颖的框架，称为**通用代理记忆（GAM）**。GAM遵循“**即时编译（JIT compilation）**”原则，在运行时针对其客户端动态生成优化后的上下文，而在离线阶段仅保留简洁但有用的记忆内容。为此，GAM采用双模块设计，包含以下组件：1) **Memorizer**，利用轻量级记忆突出关键历史信息，同时在通用页存储中完整保存全部历史信息；2) **Researcher**，根据预先构建的记忆，从页存储中检索并整合对在线请求有用的信息。该设计使GAM能够有效利用前沿大语言模型（LLMs）的代理能力与测试时的可扩展性，并通过强化学习实现端到端性能优化。在实验研究中，我们证明GAM在多种基于记忆的任务完成场景下显著优于现有记忆系统。",
        "translated_title": "通用代理记忆通过深度研究",
        "label": [],
        "label_reason": "论文聚焦AI代理记忆系统，非推荐系统相关",
        "relevance_score": 3,
        "novelty_score": 7,
        "novelty_reason": "提出JIT编译式记忆框架，具创新性"
    },
    {
        "title": "Multi-Agent Collaborative Filtering: Orchestrating Users and Items for Agentic Recommendations",
        "url": "http://arxiv.org/abs/2511.18413v1",
        "pub_date": "2025-11-23",
        "summary": "Agentic recommendations cast recommenders as large language model (LLM) agents that can plan, reason, use tools, and interact with users of varying preferences in web applications. However, most existing agentic recommender systems focus on generic single-agent plan-execute workflows or multi-agent task decomposition pipelines. Without recommendation-oriented design, they often underuse the collaborative signals in the user-item interaction history, leading to unsatisfying recommendation results. To address this, we propose the Multi-Agent Collaborative Filtering (MACF) framework for agentic recommendations, drawing an analogy between traditional collaborative filtering algorithms and LLM-based multi-agent collaboration. Specifically, given a target user and query, we instantiate similar users and relevant items as LLM agents with unique profiles. Each agent is able to call retrieval tools, suggest candidate items, and interact with other agents. Different from the static preference aggregation in traditional collaborative filtering, MACF employs a central orchestrator agent to adaptively manage the collaboration between user and item agents via dynamic agent recruitment and personalized collaboration instruction. Experimental results on datasets from three different domains show the advantages of our MACF framework compared to strong agentic recommendation baselines.",
        "translated": "代理推荐将推荐系统视为能够规划、推理、调用工具并交互于具有不同偏好的用户的大型语言模型（LLM）代理。然而，现有大多数代理推荐系统聚焦于通用的单代理计划-执行工作流或多代理任务分解管道，缺乏面向推荐的设计，往往未能充分利用用户-物料交互历史中的协同信号，导致推荐效果不尽如人意。为解决这一问题，我们提出面向代理推荐的多代理协同过滤（MACF）框架，其借鉴了传统协同过滤算法与基于LLM的多代理协作之间的类比关系。具体而言，给定目标用户和查询，我们将相似用户和相关物料实例化为具备独特画像的LLM代理。每个代理均可调用检索工具、建议候选物料，并与其他代理交互。不同于传统协同过滤中静态偏好聚合的方式，MACF采用一个中央协调代理，通过动态代理招募与个性化协作指令，自适应管理用户代理与物料代理间的协作过程。在三个不同领域数据集上的实验结果表明，我们的MACF框架相较于强基线代理推荐方法具有显著优势。",
        "translated_title": "多智能体协同过滤：协调用户与物料以实现智能推荐",
        "label": [
            "LLM生成式推荐",
            "召回",
            "精排"
        ],
        "label_reason": "基于LLM的多智能体协同过滤，直接解决推荐生成与协作建模问题。",
        "relevance_score": 9,
        "novelty_score": 8,
        "novelty_reason": "首次将多智能体协作类比CF，动态管理用户-物品交互，创新性强。"
    },
    {
        "title": "A Multimodal Conversational Agent for Tabular Data Analysis",
        "url": "http://arxiv.org/abs/2511.18405v1",
        "pub_date": "2025-11-23",
        "summary": "Large language models (LLMs) can reshape information processing by handling data analysis, visualization, and interpretation in an interactive, context-aware dialogue with users, including voice interaction, while maintaining high performance. In this article, we present Talk2Data, a multimodal LLM-driven conversational agent for intuitive data exploration. The system lets users query datasets with voice or text instructions and receive answers as plots, tables, statistics, or spoken explanations. Built on LLMs, the suggested design combines OpenAI Whisper automatic speech recognition (ASR) system, Qwen-coder code generation LLM/model, custom sandboxed execution tools, and Coqui library for text-to-speech (TTS) within an agentic orchestration loop. Unlike text-only analysis tools, it adapts responses across modalities and supports multi-turn dialogues grounded in dataset context. In an evaluation of 48 tasks on three datasets, our prototype achieved 95.8% accuracy with model-only generation time under 1.7 seconds (excluding ASR and execution time). A comparison across five LLM sizes (1.5B-32B) revealed accuracy-latency-cost trade-offs, with a 7B model providing the best balance for interactive use. By routing between conversation with user and code execution, constrained to a transparent sandbox, with simultaneously grounding prompts in schema-level context, the Talk2Data agent reliably retrieves actionable insights from tables while making computations verifiable. In the article, except for the Talk2Data agent itself, we discuss implications for human-data interaction, trust in LLM-driven analytics, and future extensions toward large-scale multimodal assistants.",
        "translated": "大语言模型（LLMs）可通过与用户进行交互式、情境感知的对话——包括语音交互——在数据处理中重塑信息分析、可视化与解释过程，同时保持高性能。本文提出 Talk2Data，一种基于多模态 LLM 的对话代理，用于直观的数据探索。该系统允许用户通过语音或文本指令查询数据集，并以图表、表格、统计结果或口头解释的形式返回答案。该系统构建于 LLM 之上，其设计结合了 OpenAI Whisper 自动语音识别（ASR）系统、Qwen-coder 代码生成 LLM/模型、定制化的沙箱执行工具以及 Coqui 库中的文本转语音（TTS）功能，构成一个代理编排循环。与仅支持文本分析的工具不同，Talk2Data 能跨模态自适应响应，并支持基于数据集上下文的多轮对话。在三个数据集上的 48 个任务评估中，我们的原型系统实现了 95.8% 的准确率，且纯模型生成时间低于 1.7 秒（不包含 ASR 和执行时间）。对五种不同规模 LLM（1.5B–32B）的对比表明，在准确性、延迟与成本之间存在权衡，其中 7B 模型在交互式使用场景下提供了最佳平衡。通过在用户对话与代码执行之间路由，受限于透明沙箱环境，并将提示语同时锚定在模式级上下文中，Talk2Data 代理能够可靠地从表格中提取可操作的洞察，并确保计算过程可验证。本文除 Talk2Data 代理本身外，还讨论了人机数据交互、对 LLM 驱动分析的信任问题，以及未来向大规模多模态助手扩展的可能性。",
        "translated_title": "用于表格数据分析的多模态对话代理",
        "label": [],
        "label_reason": "论文聚焦多模态对话分析，非推荐系统核心问题。",
        "relevance_score": 3,
        "novelty_score": 6,
        "novelty_reason": "常规LLM应用改进，无推荐系统创新。"
    },
    {
        "title": "Toward an AI-Native Internet: Rethinking the Web Architecture for Semantic Retrieval",
        "url": "http://arxiv.org/abs/2511.18354v1",
        "pub_date": "2025-11-23",
        "summary": "The rise of Generative AI Search is fundamentally transforming how users and intelligent systems interact with the Internet. LLMs increasingly act as intermediaries between humans and web information. Yet the web remains optimized for human browsing rather than AI-driven semantic retrieval, resulting in wasted network bandwidth, lower information quality, and unnecessary complexity for developers. We introduce the concept of an AI-Native Internet, a web architecture in which servers expose semantically relevant information chunks rather than full documents, supported by a Web-native semantic resolver that allows AI applications to discover relevant information sources before retrieving fine-grained chunks. Through motivational experiments, we quantify the inefficiencies of current HTML-based retrieval, and outline architectural directions and open challenges for evolving today's document-centric web into an AI-oriented substrate that better supports semantic access to web content.",
        "translated": "生成式AI搜索的兴起正从根本上改变用户与智能系统如何与互联网交互的方式。大语言模型（LLM）日益成为人类与网络信息之间的中介。然而，当前的互联网仍主要为人类浏览优化，而非面向AI驱动的语义检索，导致网络带宽浪费、信息质量下降，并给开发者带来不必要的复杂性。我们提出“原生AI互联网”的概念——一种服务器仅暴露语义相关的信息片段而非完整文档的网络架构，并辅以原生支持Web语义解析的机制，使AI应用能够在细粒度片段检索前即可发现相关信息源。通过动机性实验，我们量化了当前基于HTML的检索方式所存在的效率缺陷，并勾勒出将当前以文档为中心的互联网演进为更适配AI导向的基础设施所需的关键架构方向与开放挑战，从而更好地支持对网络内容的语义访问。",
        "translated_title": "迈向原生人工智能的互联网：为语义检索重新思考网络架构",
        "label": [
            "通用推荐技术"
        ],
        "label_reason": "探讨AI驱动语义检索架构，非直接推荐算法。",
        "relevance_score": 4,
        "novelty_score": 7,
        "novelty_reason": "提出AI原生Web架构，支持语义检索优化。"
    },
    {
        "title": "Time Matters: Enhancing Sequential Recommendations with Time-Guided Graph Neural ODEs",
        "url": "http://arxiv.org/abs/2511.18347v1",
        "pub_date": "2025-11-23",
        "summary": "Sequential recommendation (SR) is widely deployed in e-commerce platforms, streaming services, etc., revealing significant potential to enhance user experience. However, existing methods often overlook two critical factors: irregular user interests between interactions and highly uneven item distributions over time. The former factor implies that actual user preferences are not always continuous, and long-term historical interactions may not be relevant to current purchasing behavior. Therefore, relying only on these historical interactions for recommendations may result in a lack of user interest at the target time. The latter factor, characterized by peaks and valleys in interaction frequency, may result from seasonal trends, special events, or promotions. These externally driven distributions may not align with individual user interests, leading to inaccurate recommendations. To address these deficiencies, we propose TGODE to both enhance and capture the long-term historical interactions. Specifically, we first construct a user time graph and item evolution graph, which utilize user personalized preferences and global item distribution information, respectively. To tackle the temporal sparsity caused by irregular user interactions, we design a time-guided diffusion generator to automatically obtain an augmented time-aware user graph. Additionally, we devise a user interest truncation factor to efficiently identify sparse time intervals and achieve balanced preference inference. After that, the augmented user graph and item graph are fed into a generalized graph neural ordinary differential equation (ODE) to align with the evolution of user preferences and item distributions. This allows two patterns of information evolution to be matched over time. Experimental results demonstrate that TGODE outperforms baseline methods across five datasets, with improvements ranging from 10% to 46%.",
        "translated": "序列推荐（SR）广泛部署于电子商务平台、流媒体服务等场景，展现出显著提升用户体验的潜力。然而，现有方法常忽视两个关键因素：用户交互间兴趣的不连续性与物品随时间分布的高度不均衡。前者表明用户实际偏好并非始终连续，长期历史交互可能与当前购买行为无关，因此仅依赖历史交互进行推荐可能导致目标时刻用户兴趣缺失。后者表现为交互频率的峰谷波动，可能源于季节趋势、特殊事件或促销活动等外部驱动因素，其分布模式未必契合个体用户兴趣，从而导致推荐结果不准确。为解决上述缺陷，我们提出TGODE模型以增强并捕捉长期历史交互。具体而言，我们首先构建用户时序图与物品演化图，分别利用用户个性化偏好与全局物品分布信息。为应对因用户交互不规律导致的时间稀疏性，我们设计了时间引导扩散生成器，自动生成扩展的时间感知用户图。此外，我们引入用户兴趣截断因子，高效识别稀疏时间段，并实现偏好推理的平衡。随后，将扩展后的用户图与物品图输入广义图神经ODE模型，使其与用户偏好及物品分布随时间演化的动态保持一致，从而实现两类信息演化在时间维度上的匹配。实验结果表明，TGODE在五个数据集上均优于基线方法，性能提升幅度介于10%至46%之间。",
        "translated_title": "时间至关重要：基于时间引导的图神经网络微分方程提升序列推荐",
        "label": [
            "序列推荐",
            "图神经网络推荐"
        ],
        "label_reason": "聚焦序列推荐中时序建模与图神经网络结合",
        "relevance_score": 9,
        "novelty_score": 8,
        "novelty_reason": "创新性引入时间引导ODE框架处理用户兴趣演化"
    },
    {
        "title": "UFO: Unfair-to-Fair Evolving Mitigates Unfairness in LLM-based Recommender Systems via Self-Play Fine-tuning",
        "url": "http://arxiv.org/abs/2511.18342v1",
        "pub_date": "2025-11-23",
        "summary": "Large language model-based Recommender Systems (LRSs) have demonstrated superior recommendation performance by integrating pre-training with Supervised Fine-Tuning (SFT). However, this approach introduces item-side unfairness. Existing studies primarily attribute this issue to the absence of fairness constraints during SFT and attempt to mitigate unfairness via re-weighting and re-ranking methods. In this paper, we find that unfairness arises not only from SFT but also from pre-training, where inherent biases are further amplified during SFT. This finding underscores the failure of current methods to address the root causes of unfairness. Moreover, current methods struggle to preserve satisfactory recommendation performance. To tackle these issues, we propose an Unfair-to-Fair evOlving (UFO) framework using a self-play mechanism, formulating unfairness mitigation as a two-player game. UFO alternates between two player roles: the \\textit{judger}, which identifies unfairness from both pre-training and SFT, and the \\textit{corrector}, which adjusts the LRS to address identified unfairness while preserving recommendation performance. Iterative optimization between these roles enables UFO to completely resolve unfairness. Extensive experiments demonstrate that UFO effectively mitigates unfairness while improving recommendation performance.",
        "translated": "基于大语言模型的推荐系统（LRSs）通过结合预训练与监督微调（SFT）展现了优越的推荐性能。然而，该方法引入了物品侧的不公平性。现有研究主要将此问题归因于SFT阶段缺乏公平性约束，并尝试通过重加权与重排序方法缓解不公平性。本文发现，不公平性不仅源于SFT阶段，也源自预训练阶段，在SFT过程中固有偏见进一步被放大。这一发现表明当前方法未能触及不公平性的根本成因。此外，现有方法难以在缓解不公平性的同时保持满意的推荐性能。为解决上述问题，我们提出一种“从不公平到公平”的自演进框架（Unfair-to-Fair evOlving, UFO），采用自对弈机制，将不公平性缓解建模为双人博弈。UFO交替扮演两个角色：\\textit{判别者}，用于识别预训练与SFT阶段产生的不公平性；以及\\textit{修正者}，通过调整LRS以应对已识别的不公平性，同时保留推荐性能。这两个角色间的迭代优化使UFO能够彻底消除不公平性。大量实验表明，UFO有效缓解了不公平性并提升了推荐性能。",
        "translated_title": "UFO：通过自对弈微调缓解基于大语言模型推荐系统中的不公平性，实现从不公平到公平的演化",
        "label": [
            "LLM生成式推荐",
            "推荐系统公平性/可解释性"
        ],
        "label_reason": "聚焦LLM推荐中的公平性问题，提出自博弈优化框架。",
        "relevance_score": 9,
        "novelty_score": 8,
        "novelty_reason": "首创自博弈机制解决推荐系统不公平性与性能平衡。"
    },
    {
        "title": "Path-Constrained Retrieval: A Structural Approach to Reliable LLM Agent Reasoning Through Graph-Scoped Semantic Search",
        "url": "http://arxiv.org/abs/2511.18313v1",
        "pub_date": "2025-11-23",
        "summary": "Large Language Model agents often retrieve context from knowledge bases that lack structural consistency with the agent's current reasoning state, leading to incoherent reasoning chains. We introduce Path-Constrained Retrieval (PCR), a retrieval method that combines structural graph constraints with semantic search to ensure retrieved information maintains logical relationships within a knowledge graph. PCR restricts the search space to nodes reachable from an anchor node, preventing retrieval of structurally disconnected information that may lead to inconsistent reasoning. We evaluate PCR on PathRAG-6, a benchmark spanning six domains with 180 nodes and 360 edges. Our results show that PCR achieves full structural consistency compared to 24-32 percent in baseline methods, while maintaining strong relevance scores. On the technology domain, PCR obtains full relevance at rank 10 with full structural consistency, significantly outperforming vector search and hybrid retrieval. PCR reduces the average graph distance of retrieved context by 78 percent compared to baselines, demonstrating retrieval of more structurally consistent information. These findings suggest that path-constrained retrieval is an effective approach for improving the reliability and coherence of LLM agent reasoning systems.",
        "translated": "大语言模型代理常常从缺乏与当前推理状态结构一致性的知识库中检索上下文，从而导致推理链不连贯。我们引入了路径约束检索（Path-Constrained Retrieval, PCR），一种结合结构图约束与语义搜索的检索方法，以确保检索到的信息在知识图谱内保持逻辑关联性。PCR将搜索空间限制在从锚点节点可达的节点范围内，避免检索那些可能导致推理不一致的结构上断开的信息。我们在涵盖六个领域、包含180个节点和360条边的PathRAG-6基准数据集上评估PCR。结果表明，相比基线方法24%-32%的结构一致性，PCR实现了完全的结构一致性，同时保持了较高的相关性得分。在技术领域，PCR在排名前10时实现完全相关性且具备完整结构一致性，显著优于向量搜索和混合检索方法。与基线方法相比，PCR使检索上下文的平均图距离降低了78%，表明其能更有效地检索结构一致的信息。这些发现表明，路径约束检索是提升大语言模型代理推理系统可靠性和连贯性的有效方法。",
        "translated_title": "路径约束检索：通过图域语义搜索实现大语言模型代理可靠推理的结构化方法",
        "label": [
            "LLM生成式推荐",
            "通用推荐技术"
        ],
        "label_reason": "方法用于LLM推理，与推荐系统间接相关",
        "relevance_score": 4,
        "novelty_score": 7,
        "novelty_reason": "提出图结构约束检索，提升LLM推理一致性"
    },
    {
        "title": "Large Language Model Enhanced Graph Invariant Contrastive Learning for Out-of-Distribution Recommendation",
        "url": "http://arxiv.org/abs/2511.18282v1",
        "pub_date": "2025-11-23",
        "summary": "Out-of-distribution (OOD) generalization has emerged as a significant challenge in graph recommender systems. Traditional graph neural network algorithms often fail because they learn spurious environmental correlations instead of stable causal relationships, leading to substantial performance degradation under distribution shifts. While recent advancements in Large Language Models (LLMs) offer a promising avenue due to their vast world knowledge and reasoning capabilities, effectively integrating this knowledge with the fine-grained topology of specific graphs to solve the OOD problem remains a significant challenge. To address these issues, we propose {$\\textbf{Inv}$ariant $\\textbf{G}$raph $\\textbf{C}$ontrastive Learning with $\\textbf{LLM}$s for Out-of-Distribution Recommendation (InvGCLLM)}, an innovative causal learning framework that synergistically integrates the strengths of data-driven models and knowledge-driven LLMs. Our framework first employs a data-driven invariant learning model to generate causal confidence scores for each user-item interaction. These scores then guide an LLM to perform targeted graph refinement, leveraging its world knowledge to prune spurious connections and augment missing causal links. Finally, the structurally purified graphs provide robust supervision for a causality-guided contrastive learning objective, enabling the model to learn representations that are resilient to spurious correlations. Experiments conducted on four public datasets demonstrate that InvGCLLM achieves significant improvements in out-of-distribution recommendation, consistently outperforming state-of-the-art baselines.",
        "translated": "分布外（OOD）泛化已成为图推荐系统中的重要挑战。传统图神经网络算法往往失效，因为它们学习的是环境中的虚假相关性而非稳定的因果关系，在分布偏移下会导致显著性能下降。尽管近期大语言模型（LLMs）的发展因其广泛的世界知识与推理能力展现出巨大潜力，但如何有效融合这一知识与特定图结构的细粒度拓扑以解决OOD问题，仍是一个重大挑战。为应对这些问题，我们提出了一种创新性的因果学习框架——基于大语言模型的不变图对比学习方法用于分布外推荐（InvGCLLM），该框架协同整合数据驱动模型与知识驱动型LLMs的优势。我们的框架首先使用一个数据驱动的不变性学习模型，为每个用户-物料交互生成因果置信得分；随后，这些得分引导LLM执行有针对性的图结构精炼，利用其世界知识剪枝虚假连接并增强缺失的因果关联；最后，结构净化后的图提供稳健监督，支持因果引导的对比学习目标，从而使模型能够学习到对虚假相关性具有鲁棒性的表示。在四个公开数据集上的实验表明，InvGCLLM在分布外推荐任务中取得了显著提升，并持续优于现有最先进基线方法。",
        "translated_title": "大语言模型增强的图不变对比学习用于分布外推荐",
        "label": [
            "召回",
            "精排",
            "图神经网络推荐",
            "负采样与对比学习"
        ],
        "label_reason": "结合LLM与图结构解决OOD推荐，属推荐系统核心问题",
        "relevance_score": 9,
        "novelty_score": 8,
        "novelty_reason": "创新性融合LLM与因果对比学习框架"
    },
    {
        "title": "Democratic Recommendation with User and Item Representatives Produced by Graph Condensation",
        "url": "http://arxiv.org/abs/2511.18279v1",
        "pub_date": "2025-11-23",
        "summary": "The challenges associated with large-scale user-item interaction graphs have attracted increasing attention in graph-based recommendation systems, primarily due to computational inefficiencies and inadequate information propagation. Existing methods provide partial solutions but suffer from notable limitations: model-centric approaches, such as sampling and aggregation, often struggle with generalization, while data-centric techniques, including graph sparsification and coarsening, lead to information loss and ineffective handling of bipartite graph structures. Recent advances in graph condensation offer a promising direction by reducing graph size while preserving essential information, presenting a novel approach to mitigating these challenges. Inspired by the principles of democracy, we propose \\textbf{DemoRec}, a framework that leverages graph condensation to generate user and item representatives for recommendation tasks. By constructing a compact interaction graph and clustering nodes with shared characteristics from the original graph, DemoRec significantly reduces graph size and computational complexity. Furthermore, it mitigates the over-reliance on high-order information, a critical challenge in large-scale bipartite graphs. Extensive experiments conducted on four public datasets demonstrate the effectiveness of DemoRec, showcasing substantial improvements in recommendation performance, computational efficiency, and robustness compared to SOTA methods.",
        "translated": "大规模用户-物料交互图所引发的挑战在基于图的推荐系统中受到越来越多的关注，主要源于计算效率低下和信息传播不充分的问题。现有方法虽提供部分解决方案，但仍存在显著局限：以模型为中心的方法（如采样与聚合）往往难以泛化；而以数据为中心的技术（包括图稀疏化与粗化）则导致信息丢失，并无法有效处理二分图结构。近期图压缩领域的进展为缓解上述挑战提供了有前景的新方向——通过减少图规模同时保留关键信息。受民主原则启发，我们提出**DemoRec**框架，利用图压缩技术为推荐任务生成用户与物料代表。该框架通过构建紧凑的交互图，并对原图中具有相似特征的节点进行聚类，大幅降低图规模与计算复杂度，同时缓解了在大规模二分图中过度依赖高阶信息这一关键难题。在四个公开数据集上进行的广泛实验表明，DemoRec在推荐性能、计算效率及鲁棒性方面均显著优于当前最先进方法。",
        "translated_title": "基于图凝聚生成的用户与物料代表的民主推荐",
        "label": [
            "召回",
            "图神经网络推荐"
        ],
        "label_reason": "基于图压缩的用户/物品代表生成，用于推荐召回环节。",
        "relevance_score": 9,
        "novelty_score": 8,
        "novelty_reason": "首次将民主思想与图压缩结合用于推荐系统召回。"
    },
    {
        "title": "LLM Reasoning for Cold-Start Item Recommendation",
        "url": "http://arxiv.org/abs/2511.18261v1",
        "pub_date": "2025-11-23",
        "summary": "Large Language Models (LLMs) have shown significant potential for improving recommendation systems through their inherent reasoning capabilities and extensive knowledge base. Yet, existing studies predominantly address warm-start scenarios with abundant user-item interaction data, leaving the more challenging cold-start scenarios, where sparse interactions hinder traditional collaborative filtering methods, underexplored. To address this limitation, we propose novel reasoning strategies designed for cold-start item recommendations within the Netflix domain. Our method utilizes the advanced reasoning capabilities of LLMs to effectively infer user preferences, particularly for newly introduced or rarely interacted items. We systematically evaluate supervised fine-tuning, reinforcement learning-based fine-tuning, and hybrid approaches that combine both methods to optimize recommendation performance. Extensive experiments on real-world data demonstrate significant improvements in both methodological efficacy and practical performance in cold-start recommendation contexts. Remarkably, our reasoning-based fine-tuned models outperform Netflix's production ranking model by up to 8% in certain cases.",
        "translated": "大语言模型（LLMs）凭借其固有的推理能力和广泛的知识库，在提升推荐系统性能方面展现出巨大潜力。然而，现有研究主要聚焦于拥有丰富用户-物料交互数据的暖启动场景，而对更具有挑战性的冷启动场景关注不足——在该场景下，稀疏的交互数据阻碍了传统协同过滤方法的有效应用。为解决这一局限，我们提出了一种面向Netflix领域冷启动物料推荐的新型推理策略。我们的方法利用LLMs强大的推理能力，有效推断用户偏好，尤其适用于新引入或极少被交互的物料。我们系统性地评估了监督微调、基于强化学习的微调以及融合二者优势的混合方法，以优化推荐性能。在真实世界数据上的大量实验表明，我们的方法在冷启动推荐场景中显著提升了方法有效性与实际表现。尤为突出的是，我们的基于推理的微调模型在某些情况下相较Netflix生产级排序模型性能高出高达8%。",
        "translated_title": "大语言模型推理用于冷启动物料推荐",
        "label": [
            "LLM生成式推荐",
            "召回"
        ],
        "label_reason": "直接利用LLM解决冷启动推荐问题，属核心推荐环节",
        "relevance_score": 10,
        "novelty_score": 9,
        "novelty_reason": "创新性结合LLM推理与冷启动策略，效果显著提升"
    },
    {
        "title": "LumiTex: Towards High-Fidelity PBR Texture Generation with Illumination Context",
        "url": "http://arxiv.org/abs/2511.19437v1",
        "pub_date": "2025-11-24",
        "summary": "Physically-based rendering (PBR) provides a principled standard for realistic material-lighting interactions in computer graphics. Despite recent advances in generating PBR textures, existing methods fail to address two fundamental challenges: 1) materials decomposition from image prompts under limited illumination cues, and 2) seamless and view-consistent texture completion. To this end, we propose LumiTex, an end-to-end framework that comprises three key components: (1) a multi-branch generation scheme that disentangles albedo and metallic-roughness under shared illumination priors for robust material understanding, (2) a lighting-aware material attention mechanism that injects illumination context into the decoding process for physically grounded generation of albedo, metallic, and roughness maps, and (3) a geometry-guided inpainting module based on a large view synthesis model that enriches texture coverage and ensures seamless, view-consistent UV completion. Extensive experiments demonstrate that LumiTex achieves state-of-the-art performance in texture quality, surpassing both existing open-source and commercial methods.",
        "translated": "基于物理的渲染（PBR）为计算机图形学中真实材料-光照交互提供了一种原理性的标准。尽管近年来在生成 PBR 纹理方面取得了显著进展，现有方法仍未能有效解决两个根本性挑战：1）在光照线索有限的情况下，从图像提示中分解材料属性；2）实现无缝且视角一致的纹理补全。为此，我们提出 LumiTex，一个端到端框架，包含三个关键组件：（1）一种多分支生成方案，在共享光照先验约束下分离基础色（albedo）与金属度-粗糙度（metallic-roughness），以增强对材料的理解；（2）一种光照感知的材料注意力机制，将光照上下文注入解码过程，从而实现基础色、金属度及粗糙度图的物理一致性生成；（3）一个基于大规模视角合成模型的几何引导修复模块，用于扩充纹理覆盖范围，并确保无缝、视角一致的 UV 补全。大量实验表明，LumiTex 在纹理质量上达到当前最优性能，超越了现有的开源和商业方法。",
        "translated_title": "LumiTex：面向光照上下文的高保真PBR纹理生成",
        "label": [],
        "label_reason": "生成PBR纹理属高阶合成任务，非像素级图像恢复。",
        "relevance_score": 2,
        "novelty_score": 8,
        "novelty_reason": "提出新架构提升材质与光照建模，但非图像复原任务。"
    },
    {
        "title": "VDC-Agent: When Video Detailed Captioners Evolve Themselves via Agentic Self-Reflection",
        "url": "http://arxiv.org/abs/2511.19436v1",
        "pub_date": "2025-11-24",
        "summary": "We present VDC-Agent, a self-evolving framework for Video Detailed Captioning that requires neither human annotations nor larger teacher models. The agent forms a closed loop of caption generation, principle-guided scoring (score and textual suggestions), and prompt refinement. When caption quality regresses, a self-reflection path leverages the previous chain-of-thought to amend the update. Running this process on unlabeled videos produces trajectories of (caption, score) pairs. We convert the trajectories into preference tuples and filter out samples with JSON parsing errors, resulting in VDC-Agent-19K, which contains 18,886 automatically constructed pairs. We then fine-tune the base MLLM on this dataset using an easy-to-hard curriculum direct preference optimization. Built on Qwen2.5-VL-7B-Instruct, our VDC-Agent-7B attains state-of-the-art performance on the VDC benchmark with 49.08% average accuracy and 2.50 score, surpassing specialized video captioners and improving over the base model by +5.13% accuracy and +0.27 score at similar inference cost.",
        "translated": "我们提出 VDC-Agent，这是一种无需人工标注且不依赖大型教师模型的自演化视频详细字幕生成框架。该代理形成一个闭环：字幕生成、基于原理指导的评分（得分与文本建议）以及提示优化。当字幕质量下降时，自反思路径将利用先前的思维链来修正更新过程。在未标注视频上运行该流程可生成（字幕, 得分）对的轨迹。我们将这些轨迹转换为偏好元组，并过滤掉因 JSON 解析错误导致的样本，最终得到包含 18,886 对自动构建数据的 VDC-Agent-19K 数据集。随后，我们在该数据集上采用由易到难的课程式直接偏好优化方法微调基础多模态大语言模型（MLLM）。基于 Qwen2.5-VL-7B-Instruct 构建的 VDC-Agent-7B 在 VDC 基准测试中取得当前最优性能：平均准确率 49.08%，得分 2.50，超越专用视频字幕生成器，并在相似推理开销下较基线模型分别提升 +5.13% 准确率和 +0.27 分数。",
        "translated_title": "VDC-Agent：当视频详细描述器通过自主反思实现自我进化时",
        "label": [],
        "label_reason": "视频字幕生成属高阶视觉任务，非像素级图像处理。",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "创新自演化框架，但非图像恢复或增强领域。"
    },
    {
        "title": "Are Image-to-Video Models Good Zero-Shot Image Editors?",
        "url": "http://arxiv.org/abs/2511.19435v1",
        "pub_date": "2025-11-24",
        "summary": "Large-scale video diffusion models show strong world simulation and temporal reasoning abilities, but their use as zero-shot image editors remains underexplored. We introduce IF-Edit, a tuning-free framework that repurposes pretrained image-to-video diffusion models for instruction-driven image editing. IF-Edit addresses three key challenges: prompt misalignment, redundant temporal latents, and blurry late-stage frames. It includes (1) a chain-of-thought prompt enhancement module that transforms static editing instructions into temporally grounded reasoning prompts; (2) a temporal latent dropout strategy that compresses frame latents after the expert-switch point, accelerating denoising while preserving semantic and temporal coherence; and (3) a self-consistent post-refinement step that sharpens late-stage frames using a short still-video trajectory. Experiments on four public benchmarks, covering non-rigid editing, physical and temporal reasoning, and general instruction edits, show that IF-Edit performs strongly on reasoning-centric tasks while remaining competitive on general-purpose edits. Our study provides a systematic view of video diffusion models as image editors and highlights a simple recipe for unified video-image generative reasoning.",
        "translated": "大规模视频扩散模型展现出强大的世界模拟与时间推理能力，但其作为零样本图像编辑器的应用仍处于探索初期。我们提出了 IF-Edit，一种无需微调的框架，通过重新利用预训练的图像到视频扩散模型实现指令驱动的图像编辑。IF-Edit 解决了三个关键挑战：提示对齐偏差、冗余的时间潜变量以及后期帧模糊问题。具体包括：(1) 一个链式思维提示增强模块，将静态编辑指令转化为具有时间锚定的推理提示；(2) 一种时间潜变量裁剪策略，在专家切换点之后压缩帧潜变量，加速去噪过程同时保持语义和时间一致性；(3) 一个自洽后精炼步骤，利用短时静止视频轨迹锐化后期帧。在四个公开基准数据集上的实验涵盖了非刚性编辑、物理与时间推理以及通用指令编辑任务，结果显示 IF-Edit 在以推理为核心的任务上表现强劲，并在通用编辑任务中保持竞争力。本研究系统性地探讨了视频扩散模型作为图像编辑器的应用潜力，并提出了一种统一视频-图像生成推理的简洁范式。",
        "translated_title": "图像到视频的模型是否是优秀的零样本图像编辑器？",
        "label": [],
        "label_reason": "使用视频扩散模型做图像编辑属高阶生成任务",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出新框架提升零样本图像编辑效果"
    },
    {
        "title": "Breaking the Likelihood-Quality Trade-off in Diffusion Models by Merging Pretrained Experts",
        "url": "http://arxiv.org/abs/2511.19434v1",
        "pub_date": "2025-11-24",
        "summary": "Diffusion models for image generation often exhibit a trade-off between perceptual sample quality and data likelihood: training objectives emphasizing high-noise denoising steps yield realistic images but poor likelihoods, whereas likelihood-oriented training overweights low-noise steps and harms visual fidelity. We introduce a simple plug-and-play sampling method that combines two pretrained diffusion experts by switching between them along the denoising trajectory. Specifically, we apply an image-quality expert at high noise levels to shape global structure, then switch to a likelihood expert at low noise levels to refine pixel statistics. The approach requires no retraining or fine-tuning -- only the choice of an intermediate switching step. On CIFAR-10 and ImageNet32, the merged model consistently matches or outperforms its base components, improving or preserving both likelihood and sample quality relative to each expert alone. These results demonstrate that expert switching across noise levels is an effective way to break the likelihood-quality trade-off in image diffusion models.",
        "translated": "图像生成中的扩散模型常表现出感知样本质量与数据似然值之间的权衡：强调高噪声去噪步骤的训练目标可生成逼真的图像，但似然值较低；而以似然为导向的训练则过度关注低噪声步骤，损害视觉保真度。我们提出一种简单易用的采样方法，通过在去噪轨迹中切换两个预训练的扩散专家模型进行融合。具体而言，在高噪声阶段应用图像质量专家以塑造全局结构，随后在低噪声阶段切换至似然专家以优化像素统计特性。该方法无需重新训练或微调，仅需选择一个中间切换步骤即可。在 CIFAR-10 和 ImageNet32 数据集上，融合后的模型始终与基线组件相当或优于后者，在保持或提升似然值的同时，也显著改善了样本质量。这些结果表明，跨噪声水平切换专家模型是一种有效打破图像扩散模型中似然-质量权衡的方法。",
        "translated_title": "通过融合预训练专家打破扩散模型中的似然-质量权衡",
        "label": [],
        "label_reason": "生成模型非图像恢复任务，属high-level",
        "relevance_score": 3,
        "novelty_score": 8,
        "novelty_reason": "创新采样策略，但非低层图像处理"
    },
    {
        "title": "Mixture of Horizons in Action Chunking",
        "url": "http://arxiv.org/abs/2511.19433v1",
        "pub_date": "2025-11-24",
        "summary": "Vision-language-action (VLA) models have shown remarkable capabilities in robotic manipulation, but their performance is sensitive to the $\\textbf{action chunk length}$ used during training, termed $\\textbf{horizon}$. Our empirical study reveals an inherent trade-off: longer horizons provide stronger global foresight but degrade fine-grained accuracy, while shorter ones sharpen local control yet struggle on long-term tasks, implying fixed choice of single horizons being suboptimal. To mitigate the trade-off, we propose a $\\textbf{mixture of horizons (MoH)}$ strategy. MoH rearranges the action chunk into several segments with different horizons, processes them in parallel with a shared action transformer, and fuses outputs with a light linear gate. It has three appealing benefits. 1) MoH exploits long-term foresight and short-term precision jointly within a single model, improving both performance and generalizability to complex tasks. 2) MoH is plug-and-play for full-attention action modules with minimal training or inference overhead. 3) MoH enables dynamic inference with adaptive horizons, which selects stable actions through cross-horizon consensus, achieving 2.5$\\times$ higher throughput than baselines while preserving superior performance. Extensive experiments over flow-based policies $π_0$, $π_{0.5}$, and one-step regression policy $π_{\\text{reg}}$ demonstrate that MoH yields consistent and significant gains on both simulations and real-world tasks. Notably, under mixed-task setting, $π_{0.5}$ with MoH reaches a new state-of-the-art with 99$\\%$ average success rate on LIBERO after only $30k$ training iterations. Project page: https://github.com/Timsty1/MixtureOfHorizons",
        "translated": "视觉-语言-动作（VLA）模型在机器人操作任务中展现了显著的能力，但其性能对训练过程中所使用的**动作片段长度**（称为**时间跨度**或 **horizon**）高度敏感。我们的实证研究表明，存在一种固有的权衡关系：较长的时间跨度提供更强的全局前瞻能力，但会降低细粒度精度；而较短的时间跨度则增强局部控制能力，但在长期任务上表现不佳，这表明固定单一时间跨度的选择是次优的。为缓解这一权衡，我们提出了一种**多时间跨度混合策略（Mixture of Horizons, MoH）**。MoH将动作片段划分为若干具有不同时间跨度的子段，在共享的动作Transformer中并行处理，并通过一个轻量线性门控机制融合输出结果。该方法具备三大优势：1）MoH 在单个模型内同时利用长时前瞻与短时精度，从而提升性能并增强对复杂任务的泛化能力；2）MoH 可无缝集成至全注意力动作模块，且训练或推理开销极低；3）MoH 支持动态推理，通过跨时间跨度的一致性选择稳定动作，相较基线方法实现 2.5 倍更高的吞吐量，同时保持更优性能。在基于流的策略 $π_0$、$π_{0.5}$ 和一步回归策略 $π_{\\text{reg}}$ 上的广泛实验表明，MoH 在仿真环境和真实世界任务中均带来一致且显著的性能提升。特别地，在混合任务设置下，仅经过 30k 轮训练迭代，配备 MoH 的 $π_{0.5}$ 在 LIBERO 平台上的平均成功率便达到 99%，创下新 SOTA。项目主页：https://github.com/Timsty1/MixtureOfHorizons",
        "translated_title": "动作分块中的多视角融合",
        "label": [],
        "label_reason": "研究动作序列划分，非图像像素级处理",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "提出MoH策略提升模型鲁棒性与效率"
    },
    {
        "title": "Cloud4D",
        "url": "http://arxiv.org/abs/2511.19431v1",
        "pub_date": "2025-11-24",
        "summary": "There has been great progress in improving numerical weather prediction and climate models using machine learning. However, most global models act at a kilometer-scale, making it challenging to model individual clouds and factors such as extreme precipitation, wind gusts, turbulence, and surface irradiance. Therefore, there is a need to move towards higher-resolution models, which in turn require high-resolution real-world observations that current instruments struggle to obtain. We present Cloud4D, the first learning-based framework that reconstructs a physically consistent, four-dimensional cloud state using only synchronized ground-based cameras. Leveraging a homography-guided 2D-to-3D transformer, Cloud4D infers the full 3D distribution of liquid water content at 25 m spatial and 5 s temporal resolution. By tracking the 3D liquid water content retrievals over time, Cloud4D additionally estimates horizontal wind vectors. Across a two-month deployment comprising six skyward cameras, our system delivers an order-of-magnitude improvement in space-time resolution relative to state-of-the-art satellite measurements, while retaining single-digit relative error ($&lt;10\\%$) against collocated radar measurements. Code and data are available on our project page https://cloud4d.jacob-lin.com/.",
        "translated": "在利用机器学习改进数值天气预报和气候模型方面已取得显著进展。然而，大多数全球模型的分辨率仅为千米级，难以精确模拟单个云团及极端降水、阵风、湍流和地表辐照度等细节因素。因此，亟需转向更高分辨率的模型，而这反过来又要求获得高分辨率的真实世界观测数据——而当前仪器设备尚难以获取此类数据。我们提出了 Cloud4D，这是首个仅依赖同步地面相机即可重建物理一致的四维云状态的学习型框架。Cloud4D 利用基于同态变换引导的二维至三维 Transformer，推断出空间分辨率为 25 米、时间分辨率为 5 秒的完整三维液态水含量分布。通过追踪随时间变化的三维液态水含量反演结果，Cloud4D 还可进一步估算水平风向量。在为期两个月、包含六台朝天相机的部署实验中，我们的系统相较现有最先进卫星观测手段，在时空分辨率上提升了数量级，同时与同址雷达观测相比仍保持低于 10% 的相对误差。代码与数据可在项目页面 https://cloud4d.jacob-lin.com/ 获取。",
        "translated_title": "Cloud4D",
        "label": [],
        "label_reason": "处理云场重建，属高维气象建模，非像素级图像恢复",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "首次用学习框架重建4D云态，方法具新颖性"
    },
    {
        "title": "Cook and Clean Together: Teaching Embodied Agents for Parallel Task Execution",
        "url": "http://arxiv.org/abs/2511.19430v1",
        "pub_date": "2025-11-24",
        "summary": "Task scheduling is critical for embodied AI, enabling agents to follow natural language instructions and execute actions efficiently in 3D physical worlds. However, existing datasets often simplify task planning by ignoring operations research (OR) knowledge and 3D spatial grounding. In this work, we propose Operations Research knowledge-based 3D Grounded Task Scheduling (ORS3D), a new task that requires the synergy of language understanding, 3D grounding, and efficiency optimization. Unlike prior settings, ORS3D demands that agents minimize total completion time by leveraging parallelizable subtasks, e.g., cleaning the sink while the microwave operates. To facilitate research on ORS3D, we construct ORS3D-60K, a large-scale dataset comprising 60K composite tasks across 4K real-world scenes. Furthermore, we propose GRANT, an embodied multi-modal large language model equipped with a simple yet effective scheduling token mechanism to generate efficient task schedules and grounded actions. Extensive experiments on ORS3D-60K validate the effectiveness of GRANT across language understanding, 3D grounding, and scheduling efficiency. The code is available at https://github.com/H-EmbodVis/GRANT",
        "translated": "任务调度对于具身人工智能至关重要，它使智能体能够在三维物理世界中遵循自然语言指令并高效执行动作。然而，现有数据集往往通过忽略运筹学（OR）知识与三维空间定位，简化了任务规划过程。在本研究中，我们提出一种基于运筹学知识的三维接地任务调度（ORS3D），该任务需要语言理解、三维定位与效率优化三者的协同作用。与以往设置不同的是，ORS3D要求智能体利用可并行化的子任务以最小化总完成时间，例如在微波炉运行的同时清洗水槽。为促进对ORS3D的研究，我们构建了ORS3D-60K数据集，该数据集包含4K个真实场景中的6万个复合任务。此外，我们提出了GRANT模型，这是一款具身多模态大语言模型，配备了一种简洁而有效的调度标记机制，用于生成高效的任务调度方案与接地动作。在ORS3D-60K上的大量实验验证了GRANT在语言理解、三维定位及调度效率方面的有效性。代码已开源，详见 https://github.com/H-EmbodVis/GRANT",
        "translated_title": "烹饪与清洁并行：训练具身智能体执行并行任务",
        "label": [],
        "label_reason": "任务调度属高阶AI规划，非像素级图像处理。",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出新调度机制，但非图像恢复领域创新。"
    },
    {
        "title": "Flow Map Distillation Without Data",
        "url": "http://arxiv.org/abs/2511.19428v1",
        "pub_date": "2025-11-24",
        "summary": "State-of-the-art flow models achieve remarkable quality but require slow, iterative sampling. To accelerate this, flow maps can be distilled from pre-trained teachers, a procedure that conventionally requires sampling from an external dataset. We argue that this data-dependency introduces a fundamental risk of Teacher-Data Mismatch, as a static dataset may provide an incomplete or even misaligned representation of the teacher's full generative capabilities. This leads us to question whether this reliance on data is truly necessary for successful flow map distillation. In this work, we explore a data-free alternative that samples only from the prior distribution, a distribution the teacher is guaranteed to follow by construction, thereby circumventing the mismatch risk entirely. To demonstrate the practical viability of this philosophy, we introduce a principled framework that learns to predict the teacher's sampling path while actively correcting for its own compounding errors to ensure high fidelity. Our approach surpasses all data-based counterparts and establishes a new state-of-the-art by a significant margin. Specifically, distilling from SiT-XL/2+REPA, our method reaches an impressive FID of 1.45 on ImageNet 256x256, and 1.49 on ImageNet 512x512, both with only 1 sampling step. We hope our work establishes a more robust paradigm for accelerating generative models and motivates the broader adoption of flow map distillation without data.",
        "translated": "当前最先进的流模型虽能实现卓越的质量，但其推理过程需依赖缓慢的迭代采样。为加速该过程，可通过从预训练教师模型中蒸馏流图，而传统方法通常需要借助外部数据集进行采样。我们指出，这种对数据的依赖性会引入一种根本性的风险——教师-数据不匹配问题：静态数据集可能无法完整或准确反映教师模型全部生成能力的分布。由此，我们质疑：在成功蒸馏流图的过程中，是否真的必须依赖外部数据？在本工作中，我们探索了一种无需数据的替代方案，仅从教师模型所遵循的先验分布中进行采样（该分布由构造保证），从而彻底规避了不匹配的风险。为验证该理念的实际可行性，我们提出一个严谨的框架，该框架学习预测教师的采样路径，并主动校正自身累积误差，以确保高保真度。我们的方法显著超越所有基于数据的方法，并以较大优势建立新的性能基准。具体而言，从 SiT-XL/2+REPA 教师模型蒸馏所得，我们的方法在 ImageNet 256x256 上达到令人印象深刻的 FID 值 1.45，在 ImageNet 512x512 上达到 1.49，且均仅需 1 步采样。我们希望本工作能确立一种更稳健的范式，用于加速生成式模型，并推动无数据流图蒸馏技术在更广泛场景中的采用。",
        "translated_title": "无数据的流图蒸馏",
        "label": [],
        "label_reason": "生成模型加速，非图像像素级恢复任务",
        "relevance_score": 1,
        "novelty_score": 9,
        "novelty_reason": "首次提出无数据流图蒸馏框架，显著提升效率"
    },
    {
        "title": "Ref-SAM3D: Bridging SAM3D with Text for Reference 3D Reconstruction",
        "url": "http://arxiv.org/abs/2511.19426v1",
        "pub_date": "2025-11-24",
        "summary": "SAM3D has garnered widespread attention for its strong 3D object reconstruction capabilities. However, a key limitation remains: SAM3D cannot reconstruct specific objects referred to by textual descriptions, a capability that is essential for practical applications such as 3D editing, game development, and virtual environments. To address this gap, we introduce Ref-SAM3D, a simple yet effective extension to SAM3D that incorporates textual descriptions as a high-level prior, enabling text-guided 3D reconstruction from a single RGB image. Through extensive qualitative experiments, we show that Ref-SAM3D, guided only by natural language and a single 2D view, delivers competitive and high-fidelity zero-shot reconstruction performance. Our results demonstrate that Ref-SAM3D effectively bridges the gap between 2D visual cues and 3D geometric understanding, offering a more flexible and accessible paradigm for reference-guided 3D reconstruction. Code is available at: https://github.com/FudanCVL/Ref-SAM3D.",
        "translated": "SAM3D 因其强大的 3D 物体重建能力而广受关注。然而，其一个关键局限性仍存在：SAM3D 无法根据文本描述重建特定物体，而这一能力对于实际应用（如 3D 编辑、游戏开发和虚拟环境）至关重要。为解决此问题，我们提出了 Ref-SAM3D，这是对 SAM3D 的一个简洁而有效的扩展，通过将文本描述作为高层先验信息，实现从单张 RGB 图像中进行文本引导的 3D 重建。通过大量定性实验，我们表明 Ref-SAM3D 仅依赖自然语言和单张二维视图，即可实现具有竞争力且高保真的零样本重建效果。我们的结果证明，Ref-SAM3D 有效弥合了二维视觉线索与三维几何理解之间的鸿沟，为基于参考的 3D 重建提供了一种更灵活且易用的方法。代码已开源于：https://github.com/FudanCVL/Ref-SAM3D。",
        "translated_title": "Ref-SAM3D：将SAM3D与文本结合用于参考三维重建",
        "label": [],
        "label_reason": "属3D重建，非低层图像处理任务",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "文本引导3D重建，提升实用性与灵活性"
    },
    {
        "title": "SAM3-Adapter: Efficient Adaptation of Segment Anything 3 for Camouflage Object Segmentation, Shadow Detection, and Medical Image Segmentation",
        "url": "http://arxiv.org/abs/2511.19425v1",
        "pub_date": "2025-11-24",
        "summary": "The rapid rise of large-scale foundation models has reshaped the landscape of image segmentation, with models such as Segment Anything achieving unprecedented versatility across diverse vision tasks. However, previous generations-including SAM and its successor-still struggle with fine-grained, low-level segmentation challenges such as camouflaged object detection, medical image segmentation, cell image segmentation, and shadow detection. To address these limitations, we originally proposed SAM-Adapter in 2023, demonstrating substantial gains on these difficult scenarios. With the emergence of Segment Anything 3 (SAM3)-a more efficient and higher-performing evolution with a redesigned architecture and improved training pipeline-we revisit these long-standing challenges. In this work, we present SAM3-Adapter, the first adapter framework tailored for SAM3 that unlocks its full segmentation capability. SAM3-Adapter not only reduces computational overhead but also consistently surpasses both SAM and SAM2-based solutions, establishing new state-of-the-art results across multiple downstream tasks, including medical imaging, camouflaged (concealed) object segmentation, and shadow detection. Built upon the modular and composable design philosophy of the original SAM-Adapter, SAM3-Adapter provides stronger generalizability, richer task adaptability, and significantly improved segmentation precision. Extensive experiments confirm that integrating SAM3 with our adapter yields superior accuracy, robustness, and efficiency compared to all prior SAM-based adaptations. We hope SAM3-Adapter can serve as a foundation for future research and practical segmentation applications. Code, pre-trained models, and data processing pipelines are available.",
        "translated": "大规模基础模型的迅速兴起已重塑图像分割领域格局，如 Segment Anything 模型在各类视觉任务中展现出前所未有的通用性。然而，此前版本（包括 SAM 及其后续改进版）仍难以应对细粒度、低层次的分割挑战，例如伪装目标检测、医学图像分割、细胞图像分割和阴影检测。为解决这些局限性，我们于2023年首次提出 SAM-Adapter，在上述困难场景中取得了显著性能提升。随着 Segment Anything 3（SAM3）——一个架构重新设计、训练流程优化、效率更高且性能更强的演进版本——的出现，我们重新审视了这些长期存在的挑战。本文提出 SAM3-Adapter，这是首个专为 SAM3 设计的适配器框架，充分释放其完整的分割能力。SAM3-Adapter 不仅降低了计算开销，还在多个下游任务（包括医学影像、伪装/隐蔽目标分割及阴影检测）上持续超越基于 SAM 和 SAM2 的方法，建立了多项新的最先进性能基准。依托原始 SAM-Adapter 所倡导的模块化与可组合设计思想，SAM3-Adapter 具备更强的泛化能力、更丰富的任务适应性，并显著提升了分割精度。大量实验表明，将 SAM3 与我们的适配器结合，在准确率、鲁棒性和效率方面均优于所有先前的 SAM 基础适配方案。我们希望 SAM3-Adapter 能成为未来研究及实际分割应用的基础。相关代码、预训练模型及数据处理流程均已公开。",
        "translated_title": "SAM3-Adapter：Segment Anything 3 在迷彩目标分割、阴影检测与医学图像分割中的高效适配",
        "label": [],
        "label_reason": "任务为高阶语义分割，非像素级图像恢复",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "适配框架创新，但属高阶任务优化"
    },
    {
        "title": "Chain-of-Visual-Thought: Teaching VLMs to See and Think Better with Continuous Visual Tokens",
        "url": "http://arxiv.org/abs/2511.19418v1",
        "pub_date": "2025-11-24",
        "summary": "Vision-Language Models (VLMs) excel at reasoning in linguistic space but struggle with perceptual understanding that requires dense visual perception, e.g., spatial reasoning and geometric awareness. This limitation stems from the fact that current VLMs have limited mechanisms to capture dense visual information across spatial dimensions. We introduce Chain-of-Visual-Thought (COVT), a framework that enables VLMs to reason not only in words but also through continuous visual tokens-compact latent representations that encode rich perceptual cues. Within a small budget of roughly 20 tokens, COVT distills knowledge from lightweight vision experts, capturing complementary properties such as 2D appearance, 3D geometry, spatial layout, and edge structure. During training, the VLM with COVT autoregressively predicts these visual tokens to reconstruct dense supervision signals (e.g., depth, segmentation, edges, and DINO features). At inference, the model reasons directly in the continuous visual token space, preserving efficiency while optionally decoding dense predictions for interpretability. Evaluated across more than ten diverse perception benchmarks, including CV-Bench, MMVP, RealWorldQA, MMStar, WorldMedQA, and HRBench, integrating COVT into strong VLMs such as Qwen2.5-VL and LLaVA consistently improves performance by 3% to 16% and demonstrates that compact continuous visual thinking enables more precise, grounded, and interpretable multimodal intelligence.",
        "translated": "视觉-语言模型（VLMs）在语言空间中的推理能力表现优异，但在需要密集视觉感知的感知理解任务中存在不足，例如空间推理与几何感知。这一局限性源于当前 VLMs 缺乏有效机制以捕捉跨空间维度的密集视觉信息。我们提出 Chain-of-Visual-Thought（COVT），一种使 VLMs 能不仅通过文字，还能通过连续视觉标记进行推理的框架——这些视觉标记是紧凑的潜在表示，编码了丰富的感知线索。在仅约 20 个标记的小预算内，COVT 从轻量级视觉专家中蒸馏知识，捕获互补属性，如二维外观、三维几何、空间布局和边缘结构。训练过程中，配备 COVT 的 VLM 自回归预测这些视觉标记，以重建密集监督信号（例如深度、分割、边缘和 DINO 特征）。推理阶段，模型直接在连续视觉标记空间中进行推理，保持效率的同时，可选地解码密集预测以提升可解释性。在超过十个多样化的感知基准测试中评估，包括 CV-Bench、MMVP、RealWorldQA、MMStar、WorldMedQA 和 HRBench，将 COVT 集成至强大的 VLMs（如 Qwen2.5-VL 和 LLaVA）后，性能稳定提升 3% 至 16%，表明紧凑的连续视觉思维能够实现更精确、更具 grounding 且更易解释的多模态智能。",
        "translated_title": "视觉思维链：通过连续视觉标记训练视觉语言模型以更优地观察与思考",
        "label": [],
        "label_reason": "目标为多模态推理，非像素级图像处理",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "提出视觉token新范式，提升VLM感知能力"
    },
    {
        "title": "UniGame: Turning a Unified Multimodal Model Into Its Own Adversary",
        "url": "http://arxiv.org/abs/2511.19413v1",
        "pub_date": "2025-11-24",
        "summary": "Unified Multimodal Models (UMMs) have shown impressive performance in both understanding and generation with a single architecture. However, UMMs still exhibit a fundamental inconsistency: understanding favors compact embeddings, whereas generation favors reconstruction-rich representations. This structural trade-off produces misaligned decision boundaries, degraded cross-modal coherence, and heightened vulnerability under distributional and adversarial shifts. In this paper, we present UniGame, a self-adversarial post-training framework that directly targets the inconsistencies. By applying a lightweight perturber at the shared token interface, UniGame enables the generation branch to actively seek and challenge fragile understanding, turning the model itself into its own adversary. Experiments demonstrate that UniGame significantly improves the consistency (+4.6%). Moreover, it also achieves substantial improvements in understanding (+3.6%), generation (+0.02), out-of-distribution and adversarial robustness (+4.8% and +6.2% on NaturalBench and AdVQA). The framework is architecture-agnostic, introduces less than 1% additional parameters, and is complementary to existing post-training methods. These results position adversarial self-play as a general and effective principle for enhancing the coherence, stability, and unified competence of future multimodal foundation models. The official code is available at: https://github.com/AIFrontierLab/UniGame",
        "translated": "统一多模态模型（UMMs）在理解和生成任务中，仅通过单一架构便展现出令人印象深刻的性能。然而，UMMs 仍存在一个根本性不一致：理解任务偏好紧凑的嵌入表示，而生成任务则偏好富含重建信息的表征。这种结构上的权衡导致决策边界错位、跨模态一致性下降，并在分布偏移和对抗扰动下表现出更高的脆弱性。本文提出 UniGame，一种面向上述不一致性的自对抗后训练框架。通过在共享 token 接口处引入轻量级扰动器，UniGame 促使生成分支主动寻找并挑战脆弱的理解能力，从而将模型自身转化为自身的对手。实验表明，UniGame 显著提升了模型的一致性（+4.6%），同时在理解能力（+3.6%）、生成能力（+0.02）、分布外鲁棒性（NaturalBench 上 +4.8%）与对抗鲁棒性（AdVQA 上 +6.2%）方面均取得显著提升。该框架架构无关，额外参数少于 1%，且可与现有后训练方法互补。这些结果确立了自对抗对弈作为增强未来多模态基础模型一致性、稳定性及综合能力的通用有效原则。官方代码见：https://github.com/AIFrontierLab/UniGame",
        "translated_title": "UniGame：将统一多模态模型转化为自身的对抗者",
        "label": [],
        "label_reason": "属于高阶多模态对齐任务，非低层图像处理",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "提出自对抗框架提升模型一致性与鲁棒性"
    },
    {
        "title": "In-Video Instructions: Visual Signals as Generative Control",
        "url": "http://arxiv.org/abs/2511.19401v1",
        "pub_date": "2025-11-24",
        "summary": "Large-scale video generative models have recently demonstrated strong visual capabilities, enabling the prediction of future frames that adhere to the logical and physical cues in the current observation. In this work, we investigate whether such capabilities can be harnessed for controllable image-to-video generation by interpreting visual signals embedded within the frames as instructions, a paradigm we term In-Video Instruction. In contrast to prompt-based control, which provides textual descriptions that are inherently global and coarse, In-Video Instruction encodes user guidance directly into the visual domain through elements such as overlaid text, arrows, or trajectories. This enables explicit, spatial-aware, and unambiguous correspondences between visual subjects and their intended actions by assigning distinct instructions to different objects. Extensive experiments on three state-of-the-art generators, including Veo 3.1, Kling 2.5, and Wan 2.2, show that video models can reliably interpret and execute such visually embedded instructions, particularly in complex multi-object scenarios.",
        "translated": "近年来，大规模视频生成模型已展现出强大的视觉能力，能够根据当前观测中的逻辑与物理线索预测未来帧。在本工作中，我们探讨此类能力是否可被用于可控的图像到视频生成，即通过将帧内嵌入的视觉信号解释为指令，我们称之为“视频内指令”（In-Video Instruction）。与基于提示（prompt-based）的控制方式不同，后者提供本质上全局且粗略的文本描述；而“视频内指令”则通过叠加文字、箭头或轨迹等视觉元素，直接将用户引导编码至视觉域中。这使得不同对象可被赋予明确、空间感知且无歧义的指令，从而实现视觉主体与其预期动作之间的显式对应关系。在包括 Veo 3.1、Kling 2.5 和 Wan 2.2 在内的三类前沿生成器上进行的大量实验表明，视频模型能够可靠地解析并执行此类嵌入于视觉中的指令，尤其是在复杂的多目标场景中表现尤为突出。",
        "translated_title": "视频内指令：视觉信号作为生成控制",
        "label": [],
        "label_reason": "生成可控视频，非像素级图像恢复任务",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出视觉指令新范式，提升控制精度"
    },
    {
        "title": "Real-Time Object Tracking with On-Device Deep Learning for Adaptive Beamforming in Dynamic Acoustic Environments",
        "url": "http://arxiv.org/abs/2511.19396v1",
        "pub_date": "2025-11-24",
        "summary": "Advances in object tracking and acoustic beamforming are driving new capabilities in surveillance, human-computer interaction, and robotics. This work presents an embedded system that integrates deep learning-based tracking with beamforming to achieve precise sound source localization and directional audio capture in dynamic environments. The approach combines single-camera depth estimation and stereo vision to enable accurate 3D localization of moving objects. A planar concentric circular microphone array constructed with MEMS microphones provides a compact, energy-efficient platform supporting 2D beam steering across azimuth and elevation. Real-time tracking outputs continuously adapt the array's focus, synchronizing the acoustic response with the target's position. By uniting learned spatial awareness with dynamic steering, the system maintains robust performance in the presence of multiple or moving sources. Experimental evaluation demonstrates significant gains in signal-to-interference ratio, making the design well-suited for teleconferencing, smart home devices, and assistive technologies.",
        "translated": "物体跟踪与声学波束形成技术的进步正在推动安防、人机交互和机器人领域的新能力。本文提出了一种嵌入式系统，将基于深度学习的跟踪与波束形成相结合，以在动态环境中实现精确的声音源定位与定向音频采集。该方法结合单目深度估计与立体视觉，从而实现对移动目标的精准三维定位。系统采用由MEMS麦克风构建的平面同心圆形麦克风阵列，提供紧凑且功耗低的平台，支持方位角与仰角两个维度的波束赋形。实时跟踪输出持续调整阵列的聚焦方向，使声学响应与目标位置保持同步。通过融合学习到的空间感知能力与动态波束赋形，系统在面对多个或移动声源时仍能保持鲁棒性能。实验评估表明，该设计显著提升了信干比，适用于视频会议、智能家居设备及辅助技术等应用场景。",
        "translated_title": "面向动态声学环境的自适应波束成形中基于设备端深度学习的实时目标跟踪",
        "label": [],
        "label_reason": "属于高阶目标跟踪与音频处理任务",
        "relevance_score": 1,
        "novelty_score": 3,
        "novelty_reason": "方法集成现有技术，无像素级图像恢复创新"
    },
    {
        "title": "BackSplit: The Importance of Sub-dividing the Background in Biomedical Lesion Segmentation",
        "url": "http://arxiv.org/abs/2511.19394v1",
        "pub_date": "2025-11-24",
        "summary": "Segmenting small lesions in medical images remains notoriously difficult. Most prior work tackles this challenge by either designing better architectures, loss functions, or data augmentation schemes; and collecting more labeled data. We take a different view, arguing that part of the problem lies in how the background is modeled. Common lesion segmentation collapses all non-lesion pixels into a single \"background\" class, ignoring the rich anatomical context in which lesions appear. In reality, the background is highly heterogeneous-composed of tissues, organs, and other structures that can now be labeled manually or inferred automatically using existing segmentation models.   In this paper, we argue that training with fine-grained labels that sub-divide the background class, which we call BackSplit, is a simple yet powerful paradigm that can offer a significant performance boost without increasing inference costs. From an information theoretic standpoint, we prove that BackSplit increases the expected Fisher Information relative to conventional binary training, leading to tighter asymptotic bounds and more stable optimization. With extensive experiments across multiple datasets and architectures, we empirically show that BackSplit consistently boosts small-lesion segmentation performance, even when auxiliary labels are generated automatically using pretrained segmentation models. Additionally, we demonstrate that auxiliary labels derived from interactive segmentation frameworks exhibit the same beneficial effect, demonstrating its robustness, simplicity, and broad applicability.",
        "translated": "医学图像中微小病灶的分割仍极具挑战性。大多数现有工作通过设计更优的网络架构、损失函数或数据增强策略，以及收集更多标注数据来应对这一问题。我们则持不同观点，认为部分困难源于背景建模方式的局限。传统病灶分割方法将所有非病灶像素统一归为单一“背景”类别，忽视了病灶所处的丰富解剖学上下文。实际上，真实背景高度异质——由多种组织、器官及其他结构构成，这些结构如今可借助人工标注或利用现有分割模型自动推断。\n\n在本文中，我们主张采用一种称为 BackSplit 的细粒度标签训练范式：即将背景类别进一步细分。该方法简单而有效，在不增加推理开销的前提下显著提升性能。从信息论角度出发，我们证明 BackSplit 相较于传统二分类训练能提升预期 Fisher 信息量，从而获得更紧致的渐近界和更稳定的优化过程。我们对多个数据集与多种架构进行了广泛实验，实证表明 BackSplit 能持续提升微小病灶分割性能，即使辅助标签通过预训练分割模型自动生成亦然。此外，我们还验证了交互式分割框架生成的辅助标签同样具备相同增益效果，凸显其鲁棒性、简洁性及广泛适用性。",
        "translated_title": "BackSplit：背景分割在生物医学病灶分割中的重要性",
        "label": [],
        "label_reason": "任务为医学图像分割，属高阶视觉任务",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出背景细分新范式，提升小病灶分割性能"
    },
    {
        "title": "UISearch: Graph-Based Embeddings for Multimodal Enterprise UI Screenshots Retrieval",
        "url": "http://arxiv.org/abs/2511.19380v1",
        "pub_date": "2025-11-24",
        "summary": "Enterprise software companies maintain thousands of user interface screens across products and versions, creating critical challenges for design consistency, pattern discovery, and compliance check. Existing approaches rely on visual similarity or text semantics, lacking explicit modeling of structural properties fundamental to user interface (UI) composition. We present a novel graph-based representation that converts UI screenshots into attributed graphs encoding hierarchical relationships and spatial arrangements, potentially generalizable to document layouts, architectural diagrams, and other structured visual domains. A contrastive graph autoencoder learns embeddings preserving multi-level similarity across visual, structural, and semantic properties. The comprehensive analysis demonstrates that our structural embeddings achieve better discriminative power than state-of-the-art Vision Encoders, representing a fundamental advance in the expressiveness of the UI representation. We implement this representation in UISearch, a multi-modal search framework that combines structural embeddings with semantic search through a composable query language. On 20,396 financial software UIs, UISearch achieves 0.92 Top-5 accuracy with 47.5ms median latency (P95: 124ms), scaling to 20,000+ screens. The hybrid indexing architecture enables complex queries and supports fine-grained UI distinction impossible with vision-only approaches.",
        "translated": "企业软件公司在其多个产品及版本中维护成千上万的用户界面屏幕，这给设计一致性、模式发现与合规性检查带来了重大挑战。现有方法依赖于视觉相似性或文本语义，未能显式建模构成用户界面（UI）所必需的基本结构属性。我们提出了一种新颖的基于图的表示方法，将 UI 截图转化为带有属性的图结构，编码层级关系和空间布局，该方法有望推广至文档排版、建筑图表及其他结构化视觉领域。一种对比图自编码器学习嵌入表示，以保留视觉、结构与语义层面的多级相似性。全面分析表明，我们的结构化嵌入在判别能力上优于当前最先进的视觉编码器，标志着 UI 表示表达能力的根本性提升。我们将此表示应用于 UISearch，一个多模态搜索框架，通过可组合查询语言结合结构嵌入与语义搜索功能。在 20,396 个金融软件 UI 上，UISearch 实现了 0.92 的 Top-5 准确率，平均延迟为 47.5ms（P95：124ms），并可扩展至 20,000+ 个屏幕。混合索引架构支持复杂查询，并实现仅凭视觉方法无法达成的细粒度 UI 区分能力。",
        "translated_title": "UISearch：基于图的嵌入方法用于多模态企业界面截图检索",
        "label": [],
        "label_reason": "任务为UI检索，属高阶视觉应用",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出图结构嵌入，提升UI表示能力"
    },
    {
        "title": "An Anatomy Aware Hybrid Deep Learning Framework for Lung Cancer Tumor Stage Classification",
        "url": "http://arxiv.org/abs/2511.19367v1",
        "pub_date": "2025-11-24",
        "summary": "Accurate lung cancer tumor staging is crucial for prognosis and treatment planning. However, it remains challenging for end-to-end deep learning approaches, as such approaches often overlook spatial and anatomical information that are central to the tumor-node-metastasis system. The tumor stage depends on multiple quantitative criteria, including the tumor size and its proximity to the nearest anatomical structures, and small variations can alter the staging outcome. We propose a medically grounded hybrid pipeline that performs staging by explicitly measuring the tumor's size and distance properties rather than treating it as a pure image classification task. Our method employs specialized encoder-decoder networks to precisely segment the lung and adjacent anatomy, including the lobes, tumor, mediastinum, and diaphragm. Subsequently, we extract the necessary tumor properties, i.e. measure the largest tumor dimension and calculate the distance between the tumor and neighboring anatomical structures by a quantitative analysis of the segmentation masks. Finally, we apply rule-based tumor staging aligned with the medical guidelines. This novel framework has been evaluated on the Lung-PET-CT-Dx dataset, demonstrating superior performance compared to traditional deep learning models, achieving an overall classification accuracy of 91.36%. We report the per-stage F1-scores of 0.93 (T1), 0.89 (T2), 0.96 (T3), and 0.90 (T4), a critical evaluation aspect often omitted in prior literature. To our knowledge, this is the first study that embeds explicit clinical context into tumor stage classification. Unlike standard convolutional neural networks that operate in an uninterpretable \"black box\" manner, our method offers both state-of-the-art performance and transparent decision support.",
        "translated": "准确的肺癌肿瘤分期对于预后评估和治疗规划至关重要。然而，端到端深度学习方法在该任务中仍面临挑战，因为此类方法往往忽略肿瘤-淋巴结-转移（TNM）系统所依赖的空间与解剖学信息。肿瘤分期取决于多个定量指标，包括肿瘤大小及其与最近解剖结构的距离，微小的变化即可改变分期结果。我们提出了一种基于医学知识的混合流程框架，通过显式测量肿瘤尺寸与距离属性进行分期，而非将其视为纯图像分类任务。本方法采用专门设计的编码器-解码器网络，精准分割肺部及邻近解剖结构，包括肺叶、肿瘤、纵隔和膈肌。随后，我们提取所需肿瘤属性：即通过分割掩膜的定量分析，测量最大肿瘤维度并计算肿瘤与邻近解剖结构间的距离。最后，我们依据临床指南应用基于规则的肿瘤分期算法。该新颖框架已在Lung-PET-CT-Dx数据集上进行评估，相较传统深度学习模型表现更优，整体分类准确率达91.36%。我们报告各分期F1分数分别为0.93（T1）、0.89（T2）、0.96（T3）和0.90（T4），这是前人研究常被忽略的关键评估指标。据我们所知，本研究是首个将显式临床语境嵌入肿瘤分期分类任务的工作。与标准卷积神经网络以“黑箱”方式运行不同，我们的方法兼具最先进性能与透明决策支持能力。",
        "translated_title": "一种结合解剖学知识的混合深度学习框架用于肺癌肿瘤分期分类",
        "label": [],
        "label_reason": "任务为高阶医学分类，非像素级图像处理",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "引入解剖结构测量与规则化分期，提升临床可解释性"
    },
    {
        "title": "DeCo: Frequency-Decoupled Pixel Diffusion for End-to-End Image Generation",
        "url": "http://arxiv.org/abs/2511.19365v1",
        "pub_date": "2025-11-24",
        "summary": "Pixel diffusion aims to generate images directly in pixel space in an end-to-end fashion. This approach avoids the limitations of VAE in the two-stage latent diffusion, offering higher model capacity. Existing pixel diffusion models suffer from slow training and inference, as they usually model both high-frequency signals and low-frequency semantics within a single diffusion transformer (DiT). To pursue a more efficient pixel diffusion paradigm, we propose the frequency-DeCoupled pixel diffusion framework. With the intuition to decouple the generation of high and low frequency components, we leverage a lightweight pixel decoder to generate high-frequency details conditioned on semantic guidance from the DiT. This thus frees the DiT to specialize in modeling low-frequency semantics. In addition, we introduce a frequency-aware flow-matching loss that emphasizes visually salient frequencies while suppressing insignificant ones. Extensive experiments show that DeCo achieves superior performance among pixel diffusion models, attaining FID of 1.62 (256x256) and 2.22 (512x512) on ImageNet, closing the gap with latent diffusion methods. Furthermore, our pretrained text-to-image model achieves a leading overall score of 0.86 on GenEval in system-level comparison. Codes are publicly available at https://github.com/Zehong-Ma/DeCo.",
        "translated": "像素扩散旨在以端到端的方式直接在像素空间中生成图像。该方法规避了两阶段潜在扩散中变分自编码器（VAE）的局限性，从而提供更高的模型容量。现有的像素扩散模型因通常在单一扩散变换器（DiT）中同时建模高频信号与低频语义，导致训练和推理速度缓慢。为追求更高效的像素扩散范式，我们提出频率解耦像素扩散框架（Frequency-DeCoupled pixel diffusion framework）。基于将高频与低频成分生成过程分离的直觉，我们利用一个轻量级像素解码器，在DiT提供的语义引导下生成高频细节。这使得DiT得以专注于建模低频语义。此外，我们引入一种频率感知的流匹配损失函数，强调视觉显著频率的同时抑制无关紧要的频率成分。大量实验表明，DeCo在所有像素扩散模型中性能最优，在ImageNet上分别实现256x256分辨率下的FID=1.62和512x512分辨率下的FID=2.22，基本逼近潜在扩散方法的性能。此外，我们的预训练文本到图像模型在系统级评估GenEval上取得了0.86的领先综合得分。代码已公开于https://github.com/Zehong-Ma/DeCo。",
        "translated_title": "DeCo：频域解耦的像素扩散模型用于端到端图像生成",
        "label": [],
        "label_reason": "生成新图像，非图像像素级恢复",
        "relevance_score": 2,
        "novelty_score": 9,
        "novelty_reason": "提出频域解耦架构与损失函数显著改进"
    },
    {
        "title": "Growing with the Generator: Self-paced GRPO for Video Generation",
        "url": "http://arxiv.org/abs/2511.19356v1",
        "pub_date": "2025-11-24",
        "summary": "Group Relative Policy Optimization (GRPO) has emerged as a powerful reinforcement learning paradigm for post-training video generation models. However, existing GRPO pipelines rely on static, fixed-capacity reward models whose evaluation behavior is frozen during training. Such rigid rewards introduce distributional bias, saturate quickly as the generator improves, and ultimately limit the stability and effectiveness of reinforcement-based alignment. We propose Self-Paced GRPO, a competence-aware GRPO framework in which reward feedback co-evolves with the generator. Our method introduces a progressive reward mechanism that automatically shifts its emphasis from coarse visual fidelity to temporal coherence and fine-grained text-video semantic alignment as generation quality increases. This self-paced curriculum alleviates reward-policy mismatch, mitigates reward exploitation, and yields more stable optimization. Experiments on VBench across multiple video generation backbones demonstrate consistent improvements in both visual quality and semantic alignment over GRPO baselines with static rewards, validating the effectiveness and generality of Self-Paced GRPO.",
        "translated": "组相对策略优化（GRPO）已成为用于训练后视频生成模型的强大强化学习范式。然而，现有 GRPO 流程依赖于静态、固定容量的奖励模型，其评估行为在训练过程中保持冻结不变。此类刚性奖励引入分布偏差，在生成器性能提升时迅速饱和，最终限制了基于强化学习的对齐稳定性与有效性。我们提出了一种名为自适应节奏 GRPO（Self-Paced GRPO）的具备能力感知的 GRPO 框架，其中奖励反馈与生成器同步演化。我们的方法引入一种渐进式奖励机制，该机制会随着生成质量提升而自动调整重点，从粗粒度视觉保真度逐步转向时序一致性及细粒度图文语义对齐。这种自适应节奏课程缓解了奖励-策略不匹配问题，抑制了奖励利用现象，并实现了更稳定的优化效果。在多个视频生成骨干网络上，基于 VBench 的实验表明，相较于使用静态奖励的 GRPO 基线方法，Self-Paced GRPO 在视觉质量和语义对齐方面均取得一致性的提升，验证了其有效性与通用性。",
        "translated_title": "与生成器共同成长：用于视频生成的自适应GRPO",
        "label": [],
        "label_reason": "视频生成属高阶任务，非像素级图像恢复",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "提出自适应奖励机制提升生成稳定性"
    },
    {
        "title": "CellFMCount: A Fluorescence Microscopy Dataset, Benchmark, and Methods for Cell Counting",
        "url": "http://arxiv.org/abs/2511.19351v1",
        "pub_date": "2025-11-24",
        "summary": "Accurate cell counting is essential in various biomedical research and clinical applications, including cancer diagnosis, stem cell research, and immunology. Manual counting is labor-intensive and error-prone, motivating automation through deep learning techniques. However, training reliable deep learning models requires large amounts of high-quality annotated data, which is difficult and time-consuming to produce manually. Consequently, existing cell-counting datasets are often limited, frequently containing fewer than $500$ images. In this work, we introduce a large-scale annotated dataset comprising $3{,}023$ images from immunocytochemistry experiments related to cellular differentiation, containing over $430{,}000$ manually annotated cell locations. The dataset presents significant challenges: high cell density, overlapping and morphologically diverse cells, a long-tailed distribution of cell count per image, and variation in staining protocols. We benchmark three categories of existing methods: regression-based, crowd-counting, and cell-counting techniques on a test set with cell counts ranging from $10$ to $2{,}126$ cells per image. We also evaluate how the Segment Anything Model (SAM) can be adapted for microscopy cell counting using only dot-annotated datasets. As a case study, we implement a density-map-based adaptation of SAM (SAM-Counter) and report a mean absolute error (MAE) of $22.12$, which outperforms existing approaches (second-best MAE of $27.46$). Our results underscore the value of the dataset and the benchmarking framework for driving progress in automated cell counting and provide a robust foundation for future research and development.",
        "translated": "准确的细胞计数在多种生物医学研究和临床应用中至关重要，包括癌症诊断、干细胞研究和免疫学。手动计数费时且易出错，因此亟需借助深度学习技术实现自动化。然而，训练可靠的深度学习模型需要大量高质量标注数据，而这类数据手工生成既困难又耗时。因此，现有的细胞计数数据集往往规模有限，通常包含少于500张图像。在本工作中，我们引入了一个大规模标注数据集，该数据集包含3,023张来自与细胞分化相关的免疫细胞化学实验的图像，其中人工标注的细胞位置超过430,000个。该数据集具有显著挑战性：细胞密度高、细胞相互重叠且形态多样、每张图像的细胞数量呈现长尾分布，以及染色方案存在差异。我们在一个测试集上对三类现有方法——基于回归的方法、人群计数方法和专用细胞计数技术——进行了基准评估，该测试集中每张图像的细胞数量范围为10至2,126个。此外，我们还评估了Segment Anything Model（SAM）如何仅使用点标注数据集适配用于显微镜下细胞计数。作为案例研究，我们实现了基于密度图的SAM适配版本（SAM-Counter），并报告其平均绝对误差（MAE）为22.12，优于现有方法（次优方法MAE为27.46）。我们的结果凸显了该数据集及基准框架在推动自动化细胞计数发展中的价值，并为未来的研究与开发提供了坚实基础。",
        "translated_title": "CellFMCount：用于细胞计数的荧光显微镜数据集、基准与方法",
        "label": [],
        "label_reason": "任务为细胞计数，属高阶目标检测/分割",
        "relevance_score": 1,
        "novelty_score": 3,
        "novelty_reason": "方法复用SAM，创新度低"
    },
    {
        "title": "VibraVerse: A Large-Scale Geometry-Acoustics Alignment Dataset for Physically-Consistent Multimodal Learning",
        "url": "http://arxiv.org/abs/2511.20422v1",
        "pub_date": "2025-11-25",
        "summary": "Understanding the physical world requires perceptual models grounded in physical laws rather than mere statistical correlations. However, existing multimodal learning frameworks, focused on vision and language, lack physical consistency and overlook the intrinsic causal relationships among an object's geometry, material, vibration modes, and the sounds it produces. We introduce VibraVerse, a large-scale geometry-acoustics alignment dataset that explicitly bridges the causal chain from 3D geometry -&gt; physical attributes -&gt; modal parameters -&gt; acoustic signals. Each 3D model has explicit physical properties (density, Young's modulus, Poisson's ratio) and volumetric geometry, from which modal eigenfrequencies and eigenvectors are computed for impact sound synthesis under controlled excitations. To establish this coherence, we introduce CLASP, a contrastive learning framework for cross-modal alignment that preserves the causal correspondence between an object's physical structure and its acoustic response. This framework enforces physically consistent alignment across modalities, ensuring that every sample is coherent, traceable to the governing equations, and embedded within a unified representation space spanning shape, image, and sound. Built upon VibraVerse, we define a suite of benchmark tasks for geometry-to-sound prediction, sound-guided shape reconstruction, and cross-modal representation learning. Extensive validations on these tasks demonstrate that models trained on VibraVerse exhibit superior accuracy, interpretability, and generalization across modalities. These results establish VibraVerse as a benchmark for physically consistent and causally interpretable multimodal learning, providing a foundation for sound-guided embodied perception and a deeper understanding of the physical world. The dataset will be open-sourced.",
        "translated": "理解物理世界需要建立在物理定律基础上的感知模型，而非仅依赖统计相关性。然而，现有的多模态学习框架主要聚焦于视觉与语言，缺乏物理一致性，并忽视了物体几何结构、材料属性、振动模态与其所产生声音之间的内在因果关系。我们引入VibraVerse，这是一个大规模的几何-声学对齐数据集，明确构建从三维几何 → 物理属性 → 模态参数 → 声学信号的因果链路。每个3D模型均具备明确的物理属性（密度、杨氏模量、泊松比）及体素级几何结构，由此可计算其模态固有频率和特征向量，从而在受控激励条件下合成敲击声。为建立这种一致性，我们提出CLASP，一种用于跨模态对齐的对比学习框架，旨在保留物体物理结构与其声学响应间的因果对应关系。该框架强制各模态间实现物理一致的对齐，确保每个样本具有连贯性、可追溯至控制方程，并嵌入统一的表征空间中，涵盖形状、图像与声音三者。基于VibraVerse，我们定义了一系列基准任务，包括几何到声音预测、声音引导的形状重建以及跨模态表征学习。在这些任务上的广泛验证表明，基于VibraVerse训练的模型展现出更优的准确性、可解释性及跨模态泛化能力。这些结果确立了VibraVerse作为物理一致且因果可解释的多模态学习基准的地位，为声音引导的具身感知提供基础，并促进对物理世界的深入理解。该数据集将开源发布。",
        "translated_title": "VibraVerse：一个大规模几何-声学对齐数据集，用于物理一致的多模态学习",
        "label": [],
        "label_reason": "任务为多模态物理建模，非图像像素级处理",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "提出新跨模态对齐框架CLASP与物理一致性学习"
    },
    {
        "title": "StableTrack: Stabilizing Multi-Object Tracking on Low-Frequency Detections",
        "url": "http://arxiv.org/abs/2511.20418v1",
        "pub_date": "2025-11-25",
        "summary": "Multi-object tracking (MOT) is one of the most challenging tasks in computer vision, where it is important to correctly detect objects and associate these detections across frames. Current approaches mainly focus on tracking objects in each frame of a video stream, making it almost impossible to run the model under conditions of limited computing resources. To address this issue, we propose StableTrack, a novel approach that stabilizes the quality of tracking on low-frequency detections. Our method introduces a new two-stage matching strategy to improve the cross-frame association between low-frequency detections. We propose a novel Bbox-Based Distance instead of the conventional Mahalanobis distance, which allows us to effectively match objects using the Re-ID model. Furthermore, we integrate visual tracking into the Kalman Filter and the overall tracking pipeline. Our method outperforms current state-of-the-art trackers in the case of low-frequency detections, achieving $\\textit{11.6%}$ HOTA improvement at $\\textit{1}$ Hz on MOT17-val, while keeping up with the best approaches on the standard MOT17, MOT20, and DanceTrack benchmarks with full-frequency detections.",
        "translated": "多目标跟踪（MOT）是计算机视觉中最具有挑战性的任务之一，其关键在于准确检测目标并跨帧关联这些检测结果。当前方法主要聚焦于逐帧视频流中的目标跟踪，这使得模型在计算资源受限的条件下几乎无法运行。为解决此问题，我们提出了StableTrack，一种新颖的方法，旨在稳定低频检测下的跟踪质量。我们的方法引入了一种新的两阶段匹配策略，以提升低频检测帧之间的跨帧关联性能。我们提出了一种基于边界框的新距离度量方法，替代传统的马氏距离，从而能够更有效地利用Re-ID模型实现目标匹配。此外，我们将视觉跟踪集成到卡尔曼滤波器及整体跟踪流程中。在低频检测场景下，我们的方法显著优于现有最先进跟踪器，在MOT17-val数据集上以1 Hz帧率实现了11.6%的HOTA提升；同时，在标准MOT17、MOT20和DanceTrack基准测试（全频检测）中，其性能与最优方法持平。",
        "translated_title": "StableTrack：在低频检测条件下稳定多目标跟踪",
        "label": [],
        "label_reason": "属于高阶目标跟踪任务，非图像像素级处理",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出新匹配策略与视觉跟踪集成，提升低频检测稳定性"
    },
    {
        "title": "MajutsuCity: Language-driven Aesthetic-adaptive City Generation with Controllable 3D Assets and Layouts",
        "url": "http://arxiv.org/abs/2511.20415v1",
        "pub_date": "2025-11-25",
        "summary": "Generating realistic 3D cities is fundamental to world models, virtual reality, and game development, where an ideal urban scene must satisfy both stylistic diversity, fine-grained, and controllability. However, existing methods struggle to balance the creative flexibility offered by text-based generation with the object-level editability enabled by explicit structural representations. We introduce MajutsuCity, a natural language-driven and aesthetically adaptive framework for synthesizing structurally consistent and stylistically diverse 3D urban scenes. MajutsuCity represents a city as a composition of controllable layouts, assets, and materials, and operates through a four-stage pipeline. To extend controllability beyond initial generation, we further integrate MajutsuAgent, an interactive language-grounded editing agent} that supports five object-level operations. To support photorealistic and customizable scene synthesis, we also construct MajutsuDataset, a high-quality multimodal dataset} containing 2D semantic layouts and height maps, diverse 3D building assets, and curated PBR materials and skyboxes, each accompanied by detailed annotations. Meanwhile, we develop a practical set of evaluation metrics, covering key dimensions such as structural consistency, scene complexity, material fidelity, and lighting atmosphere. Extensive experiments demonstrate MajutsuCity reduces layout FID by 83.7% compared with CityDreamer and by 20.1% over CityCraft. Our method ranks first across all AQS and RDR scores, outperforming existing methods by a clear margin. These results confirm MajutsuCity as a new state-of-the-art in geometric fidelity, stylistic adaptability, and semantic controllability for 3D city generation. We expect our framework can inspire new avenues of research in 3D city generation. Our dataset and code will be released at https://github.com/LongHZ140516/MajutsuCity.",
        "translated": "生成逼真的三维城市是世界模型、虚拟现实和游戏开发的基础，其中理想的都市场景必须同时满足风格多样性、细粒度控制与可编辑性。然而，现有方法难以在基于文本的生成所赋予的创意灵活性与通过显式结构表示实现的对象级可编辑性之间取得平衡。我们提出了MajutsuCity，这是一个由自然语言驱动并具备美学自适应能力的框架，用于合成结构一致且风格多样的三维城市场景。MajutsuCity将城市建模为可控布局、资产与材质的组合，并通过四阶段流程运行。为扩展初始生成后的可控性，我们进一步引入了MajutsuAgent——一个支持五类对象级操作的交互式语言引导编辑代理。为支持逼真且可定制化的场景合成，我们还构建了MajutsuDataset，一个高质量的多模态数据集，包含二维语义布局图与高度图、多样化的三维建筑资产、精选的PBR材质与天空盒，每项素材均配有详细标注。同时，我们开发了一套实用的评估指标体系，涵盖结构一致性、场景复杂度、材质保真度及光照氛围等关键维度。大量实验表明，相较于CityDreamer，MajutsuCity将布局FID降低了83.7%，相较CityCraft降低20.1%；我们的方法在所有AQS与RDR评分中均排名第一，显著优于现有方法。这些结果验证了MajutsuCity在三维城市生成中的几何保真度、风格自适应性与语义可控性方面达到新的技术前沿。我们期待该框架能激发三维城市生成领域的研究新方向。我们的数据集与代码将在 https://github.com/LongHZ140516/MajutsuCity 上公开。",
        "translated_title": "MajutsuCity：基于语言驱动、自适应美学的城市生成方法，支持可控的3D资产与布局",
        "label": [],
        "label_reason": "生成3D城市属高阶场景合成任务，非像素级图像恢复。",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "提出新框架与交互式编辑机制，提升可控性与多样性。"
    },
    {
        "title": "Image-Free Timestep Distillation via Continuous-Time Consistency with Trajectory-Sampled Pairs",
        "url": "http://arxiv.org/abs/2511.20410v1",
        "pub_date": "2025-11-25",
        "summary": "Timestep distillation is an effective approach for improving the generation efficiency of diffusion models. The Consistency Model (CM), as a trajectory-based framework, demonstrates significant potential due to its strong theoretical foundation and high-quality few-step generation. Nevertheless, current continuous-time consistency distillation methods still rely heavily on training data and computational resources, hindering their deployment in resource-constrained scenarios and limiting their scalability to diverse domains. To address this issue, we propose Trajectory-Backward Consistency Model (TBCM), which eliminates the dependence on external training data by extracting latent representations directly from the teacher model's generation trajectory. Unlike conventional methods that require VAE encoding and large-scale datasets, our self-contained distillation paradigm significantly improves both efficiency and simplicity. Moreover, the trajectory-extracted samples naturally bridge the distribution gap between training and inference, thereby enabling more effective knowledge transfer. Empirically, TBCM achieves 6.52 FID and 28.08 CLIP scores on MJHQ-30k under one-step generation, while reducing training time by approximately 40% compared to Sana-Sprint and saving a substantial amount of GPU memory, demonstrating superior efficiency without sacrificing quality. We further reveal the diffusion-generation space discrepancy in continuous-time consistency distillation and analyze how sampling strategies affect distillation performance, offering insights for future distillation research. GitHub Link: https://github.com/hustvl/TBCM.",
        "translated": "时间步蒸馏是一种提升扩散模型生成效率的有效方法。一致性模型（CM）作为一种基于轨迹的框架，因其坚实的理论基础和高质量的少量步骤生成能力而展现出巨大潜力。然而，当前的连续时间一致性蒸馏方法仍高度依赖训练数据与计算资源，限制了其在资源受限场景中的部署，并制约了其在多领域中的可扩展性。为解决这一问题，我们提出轨迹反向一致性模型（Trajectory-Backward Consistency Model, TBCM），该方法通过直接从教师模型的生成轨迹中提取潜在表示，从而消除对外部训练数据的依赖。与传统方法需依赖变分自编码器（VAE）编码及大规模数据集不同，我们的自包含蒸馏范式显著提升了效率与简洁性。此外，轨迹提取的样本天然弥合了训练与推理阶段分布之间的鸿沟，从而实现更有效的知识迁移。实验证明，在 MJHQ-30k 数据集上，TBCM 在单步生成下取得 6.52 的 FID 分数和 28.08 的 CLIP 分数，相较 Sana-Sprint 方法训练时间减少约 40%，并大幅节省 GPU 显存，实现了高效性与质量的兼得。我们进一步揭示了连续时间一致性蒸馏中扩散生成空间的差异性，并分析采样策略对蒸馏性能的影响，为未来蒸馏研究提供有益启示。GitHub 链接：https://github.com/hustvl/TBCM。",
        "translated_title": "基于连续时间一致性与轨迹采样对的无图像时步蒸馏",
        "label": [],
        "label_reason": "聚焦扩散模型蒸馏，非图像像素级恢复任务",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "提出轨迹反向一致性模型，提升蒸馏效率与质量"
    },
    {
        "title": "A Training-Free Approach for Multi-ID Customization via Attention Adjustment and Spatial Control",
        "url": "http://arxiv.org/abs/2511.20401v1",
        "pub_date": "2025-11-25",
        "summary": "Multi-ID customization is an interesting topic in computer vision and attracts considerable attention recently. Given the ID images of multiple individuals, its purpose is to generate a customized image that seamlessly integrates them while preserving their respective identities. Compared to single-ID customization, multi-ID customization is much more difficult and poses two major challenges. First, since the multi-ID customization model is trained to reconstruct an image from the cropped person regions, it often encounters the copy-paste issue during inference, leading to lower quality. Second, the model also suffers from inferior text controllability. The generated result simply combines multiple persons into one image, regardless of whether it is aligned with the input text. In this work, we propose MultiID to tackle this challenging task in a training-free manner. Since the existing single-ID customization models have less copy-paste issue, our key idea is to adapt these models to achieve multi-ID customization. To this end, we present an ID-decoupled cross-attention mechanism, injecting distinct ID embeddings into the corresponding image regions and thus generating multi-ID outputs. To enhance the generation controllability, we introduce three critical strategies, namely the local prompt, depth-guided spatial control, and extended self-attention, making the results more consistent with the text prompts and ID images. We also carefully build a benchmark, called IDBench, for evaluation. The extensive qualitative and quantitative results demonstrate the effectiveness of MultiID in solving the aforementioned two challenges. Its performance is comparable or even better than the training-based multi-ID customization methods.",
        "translated": "多身份定制（Multi-ID customization）是计算机视觉领域一个有趣且近年来备受关注的研究课题。给定多个个体的身份图像，其目标是生成一张无缝融合这些身份、同时保留各自身份特征的定制化图像。相比单身份定制，多身份定制难度显著更高，并面临两大主要挑战。首先，由于多身份定制模型通常训练为从裁剪出的人物区域重建图像，推理过程中常出现“复制粘贴”问题，导致生成图像质量较低。其次，该模型也存在较差的文本可控性：生成结果仅简单地将多个个体合并至同一图像中，而未考虑是否与输入文本语义相符。在本文中，我们提出 MultiID——一种无需训练即可解决此挑战性任务的方法。由于现有的单身份定制模型较少出现复制粘贴问题，我们的核心思想是适配这些模型以实现多身份定制。为此，我们设计了一种身份解耦的跨注意力机制，将不同的身份嵌入注入对应图像区域，从而生成多身份输出。为进一步提升生成可控性，我们引入三种关键策略：局部提示、深度引导的空间控制以及扩展自注意力机制，使生成结果更贴近文本提示与身份图像。此外，我们还精心构建了一个评估基准——IDBench。大量定性和定量实验结果表明，MultiID 在有效应对上述两大挑战方面表现优异，其性能可媲美甚至超越基于训练的多身份定制方法。",
        "translated_title": "一种无需训练的多ID自定义方法，通过注意力调整与空间控制实现",
        "label": [],
        "label_reason": "生成定制图像属高阶视觉任务，非像素级恢复",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出ID解耦注意力机制，提升多ID可控性"
    },
    {
        "title": "FREE: Uncertainty-Aware Autoregression for Parallel Diffusion Transformers",
        "url": "http://arxiv.org/abs/2511.20390v1",
        "pub_date": "2025-11-25",
        "summary": "Diffusion Transformers (DiTs) achieve state-of-the-art generation quality but require long sequential denoising trajectories, leading to high inference latency. Recent speculative inference methods enable lossless parallel sampling in U-Net-based diffusion models via a drafter-verifier scheme, but their acceleration is limited on DiTs due to insufficient draft accuracy during verification. To address this limitation, we analyze the DiTs' feature dynamics and find the features of the final transformer layer (top-block) exhibit strong temporal consistency and rich semantic abstraction. Based on this insight, we propose FREE, a novel framework that employs a lightweight drafter to perform feature-level autoregression with parallel verification, guaranteeing lossless acceleration with theoretical and empirical support. Meanwhile, prediction variance (uncertainty) of DiTs naturally increases in later denoising steps, reducing acceptance rates under speculative sampling. To mitigate this effect, we further introduce an uncertainty-guided relaxation strategy, forming FREE (relax), which dynamically adjusts the acceptance probability in response to uncertainty levels. Experiments on ImageNet-$512^2$ show that FREE achieves up to $1.86 \\times$ acceleration, and FREE (relax) further reaches $2.25 \\times$ speedup while maintaining high perceptual and quantitative fidelity in generation quality.",
        "translated": "扩散Transformer（DiTs）在生成质量上达到最先进水平，但其需要较长的序列式去噪轨迹，导致推理延迟较高。近期提出的推测推理方法通过“草稿者-验证者”机制，在基于U-Net的扩散模型中实现了无损并行采样，但在DiTs上加速效果受限，因验证阶段草稿精度不足。为解决这一局限性，我们分析了DiTs的特征动态变化，发现最后一层Transformer模块（top-block）的特征表现出强时间一致性与丰富的语义抽象能力。基于此洞察，我们提出FREE框架，该框架采用轻量级草稿器在特征层面实现自回归，并结合并行验证机制，确保加速过程无损，且具备理论与实证支持。同时，DiTs在后期去噪步骤中预测方差（不确定性）天然上升，导致推测采样下的接受率下降。为缓解这一问题，我们进一步引入不确定性引导的松弛策略，形成FREE（relax），该策略根据不确定性水平动态调整接受概率。在ImageNet-$512^2$上的实验表明，FREE可实现最高达$1.86 \\times$的加速，而FREE（relax）进一步提升至$2.25 \\times$的速度增益，同时保持高感知与定量生成质量保真度。",
        "translated_title": "FREE：面向并行扩散变换器的不确定性感知自回归模型",
        "label": [],
        "label_reason": "生成新图像，属 high-level 任务",
        "relevance_score": 1,
        "novelty_score": 9,
        "novelty_reason": "提出新颖的并行采样框架与不确定性引导机制"
    },
    {
        "title": "VGGTFace: Topologically Consistent Facial Geometry Reconstruction in the Wild",
        "url": "http://arxiv.org/abs/2511.20366v1",
        "pub_date": "2025-11-25",
        "summary": "Reconstructing topologically consistent facial geometry is crucial for the digital avatar creation pipelines. Existing methods either require tedious manual efforts, lack generalization to in-the-wild data, or are constrained by the limited expressiveness of 3D Morphable Models. To address these limitations, we propose VGGTFace, an automatic approach that innovatively applies the 3D foundation model, \\emph{i.e.} VGGT, for topologically consistent facial geometry reconstruction from in-the-wild multi-view images captured by everyday users. Our key insight is that, by leveraging VGGT, our method naturally inherits strong generalization ability and expressive power from its large-scale training and point map representation. However, it is unclear how to reconstruct a topologically consistent mesh from VGGT, as the topology information is missing in its prediction. To this end, we augment VGGT with Pixel3DMM for injecting topology information via pixel-aligned UV values. In this manner, we convert the pixel-aligned point map of VGGT to a point cloud with topology. Tailored to this point cloud with known topology, we propose a novel Topology-Aware Bundle Adjustment strategy to fuse them, where we construct a Laplacian energy for the Bundle Adjustment objective. Our method achieves high-quality reconstruction in 10 seconds for 16 views on a single NVIDIA RTX 4090. Experiments demonstrate state-of-the-art results on benchmarks and impressive generalization to in-the-wild data. Code is available at https://github.com/grignarder/vggtface.",
        "translated": "重建拓扑一致的面部几何结构对于数字虚拟形象生成管线至关重要。现有方法要么需要繁琐的手动操作，要么缺乏对真实场景数据的泛化能力，或受限于3D可变形模型表达能力不足。为解决上述局限性，我们提出VGGTFace，一种自动化的创新方法，利用3D基础模型（即VGGT），从普通用户拍摄的真实场景多视角图像中重建拓扑一致的面部几何结构。我们的核心思路是：通过利用VGGT，该方法自然继承其大规模训练及点图表示所带来的强泛化能力和表达能力。然而，如何从VGGT重建拓扑一致的网格仍不明确，因为其预测结果中缺失拓扑信息。为此，我们引入Pixel3DMM扩展VGGT，通过像素对齐的UV值注入拓扑信息。在此基础上，我们将VGGT的像素对齐点图转换为带拓扑结构的点云。针对具有已知拓扑结构的此类点云，我们提出了一种新颖的“拓扑感知束调整”策略进行融合，其中构建了用于束调整目标函数的Laplacian能量项。本方法在单张NVIDIA RTX 4090显卡上以16个视角输入，在10秒内即可实现高质量重建。实验表明，本方法在基准测试中达到当前最优性能，并在真实场景数据上展现出卓越的泛化能力。代码开源于 https://github.com/grignarder/vggtface。",
        "translated_title": "VGGTFace：野外环境下拓扑一致的面部几何重建",
        "label": [],
        "label_reason": "任务为3D人脸重建，属高阶视觉任务",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出拓扑感知束调整策略，显著提升重建质量"
    },
    {
        "title": "From Passive Perception to Active Memory: A Weakly Supervised Image Manipulation Localization Framework Driven by Coarse-Grained Annotations",
        "url": "http://arxiv.org/abs/2511.20359v1",
        "pub_date": "2025-11-25",
        "summary": "Image manipulation localization (IML) faces a fundamental trade-off between minimizing annotation cost and achieving fine-grained localization accuracy. Existing fully-supervised IML methods depend heavily on dense pixel-level mask annotations, which limits scalability to large datasets or real-world deployment.In contrast, the majority of existing weakly-supervised IML approaches are based on image-level labels, which greatly reduce annotation effort but typically lack precise spatial localization. To address this dilemma, we propose BoxPromptIML, a novel weakly-supervised IML framework that effectively balances annotation cost and localization performance. Specifically, we propose a coarse region annotation strategy, which can generate relatively accurate manipulation masks at lower cost. To improve model efficiency and facilitate deployment, we further design an efficient lightweight student model, which learns to perform fine-grained localization through knowledge distillation from a fixed teacher model based on the Segment Anything Model (SAM). Moreover, inspired by the human subconscious memory mechanism, our feature fusion module employs a dual-guidance strategy that actively contextualizes recalled prototypical patterns with real-time observational cues derived from the input. Instead of passive feature extraction, this strategy enables a dynamic process of knowledge recollection, where long-term memory is adapted to the specific context of the current image, significantly enhancing localization accuracy and robustness. Extensive experiments across both in-distribution and out-of-distribution datasets show that BoxPromptIML outperforms or rivals fully-supervised models, while maintaining strong generalization, low annotation cost, and efficient deployment characteristics.",
        "translated": "图像操作定位（IML）面临一个根本性的权衡：在最小化标注成本与实现细粒度定位精度之间。现有的全监督IML方法严重依赖密集的像素级掩码标注，这限制了其在大规模数据集或真实场景部署中的可扩展性。相比之下，大多数现有弱监督IML方法基于图像级标签，虽然大幅降低了标注开销，但通常缺乏精确的空间定位能力。为解决这一困境，我们提出BoxPromptIML，一种新颖的弱监督IML框架，能够有效平衡标注成本与定位性能。具体而言，我们提出了一种粗区域标注策略，可在较低成本下生成相对准确的操作掩码。为进一步提升模型效率并便于部署，我们设计了一个轻量高效的学生模型，该模型通过从基于Segment Anything Model（SAM）的固定教师模型中进行知识蒸馏，学习执行细粒度定位任务。此外，受人类潜意识记忆机制启发，我们的特征融合模块采用双引导策略，主动将召回的原型模式与由输入图像实时获取的观测线索相结合。这种策略取代被动特征提取，实现动态的知识回忆过程，使长期记忆能适配当前图像的特定上下文，从而显著提升定位精度与鲁棒性。在分布内和分布外大量数据集上的广泛实验表明，BoxPromptIML优于或媲美全监督模型，同时保持强泛化能力、低标注成本与高效部署特性。",
        "translated_title": "从被动感知到主动记忆：一种由粗粒度标注驱动的弱监督图像操作定位框架",
        "label": [],
        "label_reason": "任务为弱监督图像篡改定位，属高阶视觉分析",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出基于粗区域标注与知识蒸馏的轻量框架"
    },
    {
        "title": "GS-Checker: Tampering Localization for 3D Gaussian Splatting",
        "url": "http://arxiv.org/abs/2511.20354v1",
        "pub_date": "2025-11-25",
        "summary": "Recent advances in editing technologies for 3D Gaussian Splatting (3DGS) have made it simple to manipulate 3D scenes. However, these technologies raise concerns about potential malicious manipulation of 3D content. To avoid such malicious applications, localizing tampered regions becomes crucial. In this paper, we propose GS-Checker, a novel method for locating tampered areas in 3DGS models. Our approach integrates a 3D tampering attribute into the 3D Gaussian parameters to indicate whether the Gaussian has been tampered. Additionally, we design a 3D contrastive mechanism by comparing the similarity of key attributes between 3D Gaussians to seek tampering cues at 3D level. Furthermore, we introduce a cyclic optimization strategy to refine the 3D tampering attribute, enabling more accurate tampering localization. Notably, our approach does not require expensive 3D labels for supervision. Extensive experimental results demonstrate the effectiveness of our proposed method to locate the tampered 3DGS area.",
        "translated": "近期在3D高斯点绘制（3D Gaussian Splatting, 3DGS）编辑技术方面的进展，使其操纵3D场景变得简单。然而，这些技术也引发了对3D内容可能遭受恶意篡改的担忧。为避免此类恶意应用，定位被篡改区域至关重要。本文提出GS-Checker，一种用于定位3DGS模型中篡改区域的新方法。我们的方法将3D篡改属性整合至3D高斯参数中，以指示该高斯是否已被篡改。此外，我们设计了一种3D对比机制，通过比较3D高斯间关键属性的相似性，在3D层面寻找篡改线索。进一步地，我们引入循环优化策略以精炼3D篡改属性，从而实现更精确的篡改区域定位。值得注意的是，本方法无需昂贵的3D标注进行监督。大量实验结果验证了所提方法在定位3DGS篡改区域方面的有效性。",
        "translated_title": "GS-Checker：针对三维高斯点绘的篡改定位",
        "label": [],
        "label_reason": "目标为3D模型篡改定位，属高阶内容验证任务",
        "relevance_score": 1,
        "novelty_score": 6,
        "novelty_reason": "引入循环优化与对比机制，但非图像像素级恢复"
    },
    {
        "title": "Thinking in 360°: Humanoid Visual Search in the Wild",
        "url": "http://arxiv.org/abs/2511.20351v1",
        "pub_date": "2025-11-25",
        "summary": "Humans rely on the synergistic control of head (cephalomotor) and eye (oculomotor) to efficiently search for visual information in 360°. However, prior approaches to visual search are limited to a static image, neglecting the physical embodiment and its interaction with the 3D world. How can we develop embodied visual search agents as efficient as humans while bypassing the constraints imposed by real-world hardware? To this end, we propose humanoid visual search where a humanoid agent actively rotates its head to search for objects or paths in an immersive world represented by a 360° panoramic image. To study visual search in visually-crowded real-world scenarios, we build H* Bench, a new benchmark that moves beyond household scenes to challenging in-the-wild scenes that necessitate advanced visual-spatial reasoning capabilities, such as transportation hubs, large-scale retail spaces, urban streets, and public institutions. Our experiments first reveal that even top-tier proprietary models falter, achieving only ~30% success in object and path search. We then use post-training techniques to enhance the open-source Qwen2.5-VL, increasing its success rate by over threefold for both object search (14.83% to 47.38%) and path search (6.44% to 24.94%). Notably, the lower ceiling of path search reveals its inherent difficulty, which we attribute to the demand for sophisticated spatial commonsense. Our results not only show a promising path forward but also quantify the immense challenge that remains in building MLLM agents that can be seamlessly integrated into everyday human life.",
        "translated": "人类依赖头颈部（cephalomotor）与眼部（oculomotor）的协同控制，在360°环境中高效搜索视觉信息。然而，以往的视觉搜索方法局限于静态图像，忽视了物理具身性及其与三维世界的交互作用。我们如何开发出如同人类般高效的具身视觉搜索代理，同时绕过现实硬件所施加的限制？为此，我们提出“类人视觉搜索”框架：其中，一个类人智能体主动转动头部，在由360°全景图像所呈现的沉浸式世界中搜寻目标物体或路径。为研究真实复杂场景下的视觉搜索问题，我们构建了H* Bench这一全新基准数据集——其突破传统家庭场景限制，涵盖对先进视觉-空间推理能力要求更高的野外真实环境，如交通枢纽、大型零售空间、城市街道及公共机构等。实验表明，即便是顶尖的专有模型也表现不佳，在物体和路径搜索任务中的成功率仅约30%。随后，我们通过后训练技术提升开源模型Qwen2.5-VL，使其在物体搜索（从14.83%提升至47.38%）与路径搜索（从6.44%提升至24.94%）的成功率均实现三倍以上增长。值得注意的是，路径搜索任务的较低上限揭示了其固有的难度，我们认为这主要源于对高阶空间常识推理的需求。我们的研究成果不仅指明了一条可行的发展道路，同时也量化了构建可无缝融入人类日常生活的多模态大语言模型（MLLM）代理仍面临的巨大挑战。",
        "translated_title": "360°视角思考：野外的人形视觉搜索",
        "label": [],
        "label_reason": "研究视觉搜索与空间推理，属高阶视觉任务。",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "改进开源模型性能，非像素级图像处理创新。"
    },
    {
        "title": "Material-informed Gaussian Splatting for 3D World Reconstruction in a Digital Twin",
        "url": "http://arxiv.org/abs/2511.20348v1",
        "pub_date": "2025-11-25",
        "summary": "3D reconstruction for Digital Twins often relies on LiDAR-based methods, which provide accurate geometry but lack the semantics and textures naturally captured by cameras. Traditional LiDAR-camera fusion approaches require complex calibration and still struggle with certain materials like glass, which are visible in images but poorly represented in point clouds. We propose a camera-only pipeline that reconstructs scenes using 3D Gaussian Splatting from multi-view images, extracts semantic material masks via vision models, converts Gaussian representations to mesh surfaces with projected material labels, and assigns physics-based material properties for accurate sensor simulation in modern graphics engines and simulators. This approach combines photorealistic reconstruction with physics-based material assignment, providing sensor simulation fidelity comparable to LiDAR-camera fusion while eliminating hardware complexity and calibration requirements. We validate our camera-only method using an internal dataset from an instrumented test vehicle, leveraging LiDAR as ground truth for reflectivity validation alongside image similarity metrics.",
        "translated": "数字孪生中的三维重建通常依赖基于激光雷达的方法，这些方法能够提供精确的几何结构，但缺乏相机自然捕获的语义信息与纹理。传统的激光雷达-相机融合方法需要复杂的标定过程，且在处理某些材料（如玻璃）时仍存在困难——这类材料在图像中可见，但在点云中表现不佳。我们提出一种仅使用相机的流程：通过多视角图像利用三维高斯溅射技术重建场景，借助视觉模型提取语义材质掩码，将高斯表示转换为带有投影材质标签的网格表面，并分配基于物理的材质属性，从而在现代图形引擎和仿真器中实现精准传感器模拟。该方法结合了逼真重建与基于物理的材质分配，其传感器仿真精度可媲美激光雷达-相机融合方案，同时消除了硬件复杂性和标定需求。我们在一辆配备传感器的测试车辆内部数据集上验证了该仅相机方法，利用激光雷达作为反射率验证的地面真值，并结合图像相似性度量进行评估。",
        "translated_title": "面向数字孪生的基于材质信息的高斯点云三维世界重建",
        "label": [],
        "label_reason": "目标为3D重建与材质模拟，属高阶视觉任务",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "融合视觉模型与物理材质分配，创新性中等"
    },
    {
        "title": "AMB3R: Accurate Feed-forward Metric-scale 3D Reconstruction with Backend",
        "url": "http://arxiv.org/abs/2511.20343v1",
        "pub_date": "2025-11-25",
        "summary": "We present AMB3R, a multi-view feed-forward model for dense 3D reconstruction on a metric-scale that addresses diverse 3D vision tasks. The key idea is to leverage a sparse, yet compact, volumetric scene representation as our backend, enabling geometric reasoning with spatial compactness. Although trained solely for multi-view reconstruction, we demonstrate that AMB3R can be seamlessly extended to uncalibrated visual odometry (online) or large-scale structure from motion without the need for task-specific fine-tuning or test-time optimization. Compared to prior pointmap-based models, our approach achieves state-of-the-art performance in camera pose, depth, and metric-scale estimation, 3D reconstruction, and even surpasses optimization-based SLAM and SfM methods with dense reconstruction priors on common benchmarks.",
        "translated": "我们提出了AMB3R，这是一种面向度量尺度的多视角前馈模型，用于稠密三维重建，并可支持多种三维视觉任务。其核心思想是利用一种稀疏但紧凑的体素场景表示作为后端，从而实现具有空间紧凑性的几何推理。尽管该模型仅针对多视角重建进行训练，我们证明AMB3R可无缝扩展至未经标定的视觉里程计（在线）或大规模结构从运动问题，而无需特定任务的微调或测试时优化。与先前基于点图的模型相比，我们的方法在相机位姿、深度及度量尺度估计、三维重建等任务上均达到当前最优性能，甚至在常见基准数据集上，在具备稠密重建先验的情况下，超越了基于优化的SLAM与SfM方法。",
        "translated_title": "AMB3R：基于后端的准确前馈度量级三维重建",
        "label": [],
        "label_reason": "3D重建属高阶视觉任务，非像素级图像恢复。",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "提出紧凑体积表示，显著提升多视角重建性能。"
    },
    {
        "title": "ShelfRectNet: Single View Shelf Image Rectification with Homography Estimation",
        "url": "http://arxiv.org/abs/2511.20335v1",
        "pub_date": "2025-11-25",
        "summary": "Estimating homography from a single image remains a challenging yet practically valuable task, particularly in domains like retail, where only one viewpoint is typically available for shelf monitoring and product alignment. In this paper, we present a deep learning framework that predicts a 4-point parameterized homography matrix to rectify shelf images captured from arbitrary angles. Our model leverages a ConvNeXt-based backbone for enhanced feature representation and adopts normalized coordinate regression for improved stability. To address data scarcity and promote generalization, we introduce a novel augmentation strategy by modeling and sampling synthetic homographies. Our method achieves a mean corner error of 1.298 pixels on the test set. When compared with both classical computer vision and deep learning-based approaches, our method demonstrates competitive performance in both accuracy and inference speed. Together, these results establish our approach as a robust and efficient solution for realworld single-view rectification. To encourage further research in this domain, we will make our dataset, ShelfRectSet, and code publicly available",
        "translated": "从单张图像估计本质矩阵仍是一项具有挑战性但实际价值显著的任务，尤其是在零售领域，货架监控与产品对齐通常仅能获取单一视角。本文提出一种深度学习框架，用于预测参数化为四点的单应矩阵，以校正任意角度拍摄的货架图像。我们的模型采用 ConvNeXt 架构作为骨干网络以增强特征表示，并引入归一化坐标回归以提升稳定性。为应对数据稀缺问题并促进泛化能力，我们提出一种新颖的数据增强策略——通过建模和采样合成单应矩阵。在测试集上，我们的方法实现了 1.298 像素的平均角点误差。与经典计算机视觉方法及基于深度学习的方法相比，本方法在精度和推理速度方面均表现出竞争力。综合来看，这些结果确立了我们的方法作为一种稳健且高效的单视角图像校正方案，适用于真实场景。为鼓励该领域的进一步研究，我们将公开我们的数据集 ShelfRectSet 及相关代码。",
        "translated_title": "ShelfRectNet：基于单目视角的货架图像校正与透视变换估计",
        "label": [],
        "label_reason": "单视角图像透视校正属几何变换，非像素级图像恢复。",
        "relevance_score": 3,
        "novelty_score": 6,
        "novelty_reason": "改进网络结构与数据增强，但无低级视觉任务创新。"
    },
    {
        "title": "3D Motion Perception of Binocular Vision Target with PID-CNN",
        "url": "http://arxiv.org/abs/2511.20332v1",
        "pub_date": "2025-11-25",
        "summary": "This article trained a network for perceiving three-dimensional motion information of binocular vision target, which can provide real-time three-dimensional coordinate, velocity, and acceleration, and has a basic spatiotemporal perception capability. Understood the ability of neural networks to fit nonlinear problems from the perspective of PID. Considered a single-layer neural network as using a second-order difference equation and a nonlinearity to describe a local problem. Multilayer networks gradually transform the raw representation to the desired representation through multiple such combinations. Analysed some reference principles for designing neural networks. Designed a relatively small PID convolutional neural network, with a total of 17 layers and 413 thousand parameters. Implemented a simple but practical feature reuse method by concatenation and pooling. The network was trained and tested using the simulated randomly moving ball datasets, and the experimental results showed that the prediction accuracy was close to the upper limit that the input image resolution can represent. Analysed the experimental results and errors, as well as the existing shortcomings and possible directions for improvement. Finally, discussed the advantages of high-dimensional convolution in improving computational efficiency and feature space utilization. As well as the potential advantages of using PID information to implement memory and attention mechanisms.",
        "translated": "本文训练了一种网络，用于感知双目视觉目标的三维运动信息，可提供实时的三维坐标、速度与加速度，并具备基础的空间-时间感知能力。从PID的角度理解神经网络拟合非线性问题的能力，将单层神经网络视为利用二阶差分方程与非线性函数描述局部问题；多层网络通过多次此类组合，逐步将原始表示转化为所需表示。分析了若干设计神经网络的参考原则，设计了一个规模相对较小的PID卷积神经网络，共17层，参数量约41.3万个。通过拼接与池化实现了简单但实用的特征复用方法。该网络使用模拟随机运动球数据集进行训练与测试，实验结果表明预测精度接近输入图像分辨率所能表征的上限。分析了实验结果与误差，以及当前存在的不足及可能的改进方向。最后，讨论了高维卷积在提升计算效率与特征空间利用率方面的优势，以及利用PID信息实现记忆与注意力机制的潜在优势。",
        "translated_title": "双目视觉目标的三维运动感知（基于PID-CNN）",
        "label": [],
        "label_reason": "目标为3D运动感知，属高阶视觉任务",
        "relevance_score": 2,
        "novelty_score": 4,
        "novelty_reason": "PID-CNN结构创新有限，未解决像素级图像处理"
    },
    {
        "title": "ArtiBench and ArtiBrain: Benchmarking Generalizable Vision-Language Articulated Object Manipulation",
        "url": "http://arxiv.org/abs/2511.20330v1",
        "pub_date": "2025-11-25",
        "summary": "Interactive articulated manipulation requires long-horizon, multi-step interactions with appliances while maintaining physical consistency. Existing vision-language and diffusion-based policies struggle to generalize across parts, instances, and categories. We first introduce ArtiBench, a five-level benchmark covering kitchen, storage, office, and tool environments. ArtiBench enables structured evaluation from cross-part and cross-instance variation to long-horizon multi-object tasks, revealing the core generalization challenges of articulated object manipulation. Building on this benchmark, we propose ArtiBrain, a modular framework that unifies high-level reasoning with adaptive low-level control. ArtiBrain uses a VLM-based Task Reasoner (GPT-4.1) to decompose and validate subgoals, and employs a Hybrid Controller that combines geometry-aware keyframe execution with affordance-guided diffusion for precise and interpretable manipulation. An Affordance Memory Bank continually accumulates successful execution episodes and propagates part-level actionable affordances to unseen articulated parts and configurations. Extensive experiments on ArtiBench show that our ArtiBrain significantly outperforms state-of-the-art multimodal and diffusion-based methods in robustness and generalization. Code and dataset will be released upon acceptance.",
        "translated": "交互式分段操纵需要与电器进行长时程、多步骤的交互，同时保持物理一致性。现有基于视觉-语言和扩散模型的策略难以在部件、实例及类别间泛化。我们首先提出 ArtiBench，这是一个涵盖厨房、储物、办公及工具环境的五级基准测试平台。ArtiBench 支持从跨部件与跨实例变化到长时程多物体任务的结构化评估，揭示了分段物体操纵的核心泛化挑战。在此基准基础上，我们提出 ArtiBrain，一个统一高层推理与自适应低层控制的模块化框架。ArtiBrain 采用基于视觉-语言模型（VLM）的任务规划器（GPT-4.1）对子目标进行分解与验证，并结合几何感知关键帧执行与功能引导扩散机制，实现精准且可解释的操纵控制。一个功能记忆库持续积累成功执行的会话，并将部件级可操作功能传播至未见的分段部件与配置。在 ArtiBench 上的大量实验表明，我们的 ArtiBrain 在鲁棒性与泛化能力方面显著优于当前最先进的多模态与扩散类方法。代码与数据集将在被接受后公开。",
        "translated_title": "ArtiBench 与 ArtiBrain：通用视觉-语言 articulated 物体操作的基准评估",
        "label": [],
        "label_reason": "任务属高阶视觉-语言操控，非像素级图像处理",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "提出新框架ArtiBrain提升多模态操控泛化能力"
    },
    {
        "title": "AD-R1: Closed-Loop Reinforcement Learning for End-to-End Autonomous Driving with Impartial World Models",
        "url": "http://arxiv.org/abs/2511.20325v1",
        "pub_date": "2025-11-25",
        "summary": "End-to-end models for autonomous driving hold the promise of learning complex behaviors directly from sensor data, but face critical challenges in safety and handling long-tail events. Reinforcement Learning (RL) offers a promising path to overcome these limitations, yet its success in autonomous driving has been elusive. We identify a fundamental flaw hindering this progress: a deep seated optimistic bias in the world models used for RL. To address this, we introduce a framework for post-training policy refinement built around an Impartial World Model. Our primary contribution is to teach this model to be honest about danger. We achieve this with a novel data synthesis pipeline, Counterfactual Synthesis, which systematically generates a rich curriculum of plausible collisions and off-road events. This transforms the model from a passive scene completer into a veridical forecaster that remains faithful to the causal link between actions and outcomes. We then integrate this Impartial World Model into our closed-loop RL framework, where it serves as an internal critic. During refinement, the agent queries the critic to ``dream\" of the outcomes for candidate actions. We demonstrate through extensive experiments, including on a new Risk Foreseeing Benchmark, that our model significantly outperforms baselines in predicting failures. Consequently, when used as a critic, it enables a substantial reduction in safety violations in challenging simulations, proving that teaching a model to dream of danger is a critical step towards building truly safe and intelligent autonomous agents.",
        "translated": "面向自动驾驶的端到端模型有望直接从传感器数据中学习复杂行为，但在安全性和长尾事件处理方面面临关键挑战。强化学习（RL）为克服这些限制提供了一条有前景的路径，但其在自动驾驶领域的成功却难以实现。我们识别出阻碍这一进展的根本缺陷：用于RL的世界模型中存在深层次的乐观偏置。为解决这一问题，我们提出一种基于“公正世界模型”的后训练策略框架。我们的主要贡献在于训练该模型诚实评估危险。我们通过一种新颖的数据合成流程——反事实合成（Counterfactual Synthesis），系统性地生成大量合理的碰撞与离道路事件训练样本，从而将模型从被动场景补全者转变为忠实预测未来结果的因果推断器。随后，我们将此“公正世界模型”集成至闭环强化学习框架中，作为内部批评者角色。在策略优化阶段，智能体向批评者查询候选动作可能引发的后果，以“想象”潜在结果。我们在包括新提出的“风险预见基准测试”在内的大量实验中证明，相较于基线方法，本模型在预测失败事件方面显著更优。因此，当用作批评者时，它能够大幅降低复杂仿真环境中的安全违规事件，表明教会模型“想象危险”是构建真正安全且智能的自动驾驶代理的关键一步。",
        "translated_title": "AD-R1：用于端到端自动驾驶的闭环强化学习与无偏世界模型",
        "label": [],
        "label_reason": "高阶自动驾驶决策任务，非像素级图像处理",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "提出新RL框架与反事实合成方法"
    },
    {
        "title": "IrisNet: Infrared Image Status Awareness Meta Decoder for Infrared Small Targets Detection",
        "url": "http://arxiv.org/abs/2511.20319v1",
        "pub_date": "2025-11-25",
        "summary": "Infrared Small Target Detection (IRSTD) faces significant challenges due to low signal-to-noise ratios, complex backgrounds, and the absence of discernible target features. While deep learning-based encoder-decoder frameworks have advanced the field, their static pattern learning suffers from pattern drift across diverse scenarios (\\emph{e.g.}, day/night variations, sky/maritime/ground domains), limiting robustness. To address this, we propose IrisNet, a novel meta-learned framework that dynamically adapts detection strategies to the input infrared image status. Our approach establishes a dynamic mapping between infrared image features and entire decoder parameters via an image-to-decoder transformer. More concretely, we represent the parameterized decoder as a structured 2D tensor preserving hierarchical layer correlations and enable the transformer to model inter-layer dependencies through self-attention while generating adaptive decoding patterns via cross-attention. To further enhance the perception ability of infrared images, we integrate high-frequency components to supplement target-position and scene-edge information. Experiments on NUDT-SIRST, NUAA-SIRST, and IRSTD-1K datasets demonstrate the superiority of our IrisNet, achieving state-of-the-art performance.",
        "translated": "红外小目标检测（IRSTD）因信噪比低、背景复杂及缺乏可辨识的目标特征而面临严峻挑战。尽管基于深度学习的编码器-解码器框架已推动该领域发展，但其静态模式学习在多样场景（如昼夜变化、天空/海洋/地面域）下易出现模式漂移，限制了模型的鲁棒性。为此，我们提出IrisNet，一种新颖的元学习框架，能够根据输入红外图像的状态动态调整检测策略。我们的方法通过图像到解码器的Transformer建立红外图像特征与整个解码器参数之间的动态映射。更具体而言，我们将参数化解码器表示为保持层级层间关联的结构化二维张量，并借助自注意力机制建模层间依赖关系，同时通过交叉注意力生成自适应解码模式。为进一步增强红外图像的感知能力，我们引入高频成分以补充目标位置和场景边缘信息。在NUDT-SIRST、NUAA-SIRST及IRSTD-1K数据集上的实验表明，IrisNet性能优越，达到当前最先进水平。",
        "translated_title": "IrisNet：面向红外小目标检测的红外图像状态感知元解码器",
        "label": [],
        "label_reason": "目标检测属高阶视觉任务，非像素级图像恢复。",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出元学习框架改进检测策略，具结构创新性。"
    },
    {
        "title": "TReFT: Taming Rectified Flow Models For One-Step Image Translation",
        "url": "http://arxiv.org/abs/2511.20307v1",
        "pub_date": "2025-11-25",
        "summary": "Rectified Flow (RF) models have advanced high-quality image and video synthesis via optimal transport theory. However, when applied to image-to-image translation, they still depend on costly multi-step denoising, hindering real-time applications. Although the recent adversarial training paradigm, CycleGAN-Turbo, works in pretrained diffusion models for one-step image translation, we find that directly applying it to RF models leads to severe convergence issues. In this paper, we analyze these challenges and propose TReFT, a novel method to Tame Rectified Flow models for one-step image Translation. Unlike previous works, TReFT directly uses the velocity predicted by pretrained DiT or UNet as output-a simple yet effective design that tackles the convergence issues under adversarial training with one-step inference. This design is mainly motivated by a novel observation that, near the end of the denoising process, the velocity predicted by pretrained RF models converges to the vector from origin to the final clean image, a property we further justify through theoretical analysis. When applying TReFT to large pretrained RF models such as SD3.5 and FLUX, we introduce memory-efficient latent cycle-consistency and identity losses during training, as well as lightweight architectural simplifications for faster inference. Pretrained RF models finetuned with TReFT achieve performance comparable to sota methods across multiple image translation datasets while enabling real-time inference.",
        "translated": "基于校正流（Rectified Flow, RF）的模型通过最优传输理论在高质量图像与视频合成方面取得了显著进展。然而，当应用于图像到图像翻译任务时，它们仍依赖于代价高昂的多步去噪过程，限制了其在实时应用中的可行性。尽管近期提出的对抗训练范式 CycleGAN-Turbo 在预训练扩散模型中实现了单步图像翻译，但我们发现直接将其应用于 RF 模型会导致严重的收敛问题。本文分析了这些挑战，并提出了 TReFT，一种用于驯化校正流模型实现单步图像翻译的新方法。与以往工作不同，TReFT 直接将预训练 DiT 或 UNet 预测的速度作为输出——这是一种简洁而有效的设计，在对抗训练条件下通过单步推理解决了收敛难题。该设计主要基于一个新颖观察：在去噪过程接近尾声时，预训练 RF 模型预测的速度收敛于从原点指向最终干净图像的向量，我们进一步通过理论分析对该特性进行了验证。当将 TReFT 应用于大型预训练 RF 模型（如 SD3.5 和 FLUX）时，我们在训练过程中引入了内存高效的潜在域循环一致性损失和身份损失，并对架构进行了轻量化简化以加速推理。经过 TReFT 微调的预训练 RF 模型在多个图像翻译数据集上实现了与当前最先进方法相当的性能，同时支持实时推理。",
        "translated_title": "TReFT：驯化校正流模型用于一步图像翻译",
        "label": [],
        "label_reason": "目标为图像翻译，属高阶视觉任务",
        "relevance_score": 2,
        "novelty_score": 8,
        "novelty_reason": "提出新架构解决收敛问题，提升推理效率"
    },
    {
        "title": "TaCo: Capturing Spatio-Temporal Semantic Consistency in Remote Sensing Change Detection",
        "url": "http://arxiv.org/abs/2511.20306v1",
        "pub_date": "2025-11-25",
        "summary": "Remote sensing change detection (RSCD) aims to identify surface changes across bi-temporal satellite images. Most previous methods rely solely on mask supervision, which effectively guides spatial localization but provides limited constraints on the temporal semantic transitions. Consequently, they often produce spatially coherent predictions while still suffering from unresolved semantic inconsistencies. To address this limitation, we propose TaCo, a spatio-temporal semantic consistent network, which enriches the existing mask-supervised framework with a spatio-temporal semantic joint constraint. TaCo conceptualizes change as a semantic transition between bi-temporal states, in which one temporal feature representation can be derived from the other via dedicated transition features. To realize this, we introduce a Text-guided Transition Generator that integrates textual semantics with bi-temporal visual features to construct the cross-temporal transition features. In addition, we propose a spatio-temporal semantic joint constraint consisting of bi-temporal reconstruct constraints and a transition constraint: the former enforces alignment between reconstructed and original features, while the latter enhances discrimination for changes. This design can yield substantial performance gains without introducing any additional computational overhead during inference. Extensive experiments on six public datasets, spanning both binary and semantic change detection tasks, demonstrate that TaCo consistently achieves SOTA performance.",
        "translated": "遥感变化检测（RSCD）旨在识别双时相卫星图像间的地表变化。大多数现有方法仅依赖掩码监督，虽能有效引导空间定位，但对时间语义过渡的约束有限。因此，这些方法常产生空间一致的预测结果，但仍存在未解决的语义不一致性问题。为解决此局限，我们提出 TaCo，一种时空语义一致网络，在现有掩码监督框架基础上引入时空语义联合约束。TaCo 将变化概念化为双时相状态间的语义过渡，其中某一时刻的特征表示可通过特定的过渡特征从另一时刻推导而来。为实现这一目标，我们引入了文本引导的过渡生成器，该模块将文本语义与双时相视觉特征结合，构建跨时相过渡特征。此外，我们提出一种时空语义联合约束机制，包含双时相重构约束与过渡约束：前者强制重建特征与原始特征对齐，后者增强对变化区域的判别能力。该设计在推理阶段无需引入任何额外计算开销，即可显著提升性能。我们在六个公开数据集上的广泛实验表明，TaCo 在二元及语义变化检测任务中均持续取得当前最优效果（SOTA）。",
        "translated_title": "TaCo：在遥感变化检测中捕捉时空语义一致性",
        "label": [],
        "label_reason": "任务为遥感变化检测，属高阶语义理解",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "引入文本引导过渡机制，改进语义一致性"
    },
    {
        "title": "CrossEarth-Gate: Fisher-Guided Adaptive Tuning Engine for Efficient Adaptation of Cross-Domain Remote Sensing Semantic Segmentation",
        "url": "http://arxiv.org/abs/2511.20302v1",
        "pub_date": "2025-11-25",
        "summary": "In Remote Sensing (RS), Parameter-Efficient Fine-Tuning (PEFT) has emerged as a key approach to activate the generalizable representation ability of foundation models for downstream tasks. However, existing specialized PEFT methods often fail when applied to large-scale Earth observation tasks, as they are unable to fully handle the multifaceted and unpredictable domain gaps (\\eg, spatial, semantic, and frequency shifts) inherent in RS data. To overcome this, we propose CrossEarth-Gate, which introduces two primary contributions. First, we establish a comprehensive RS module toolbox to address multifaceted domain gaps, comprising spatial, semantic, and frequency modules. Second, we develop a Fisher-guided adaptive selection mechanism that operates on this toolbox. This selection is guided by Fisher Information to quantify each module's importance by measuring its contribution to the task-specific gradient flow. It dynamically activates only the most critical modules at the appropriate layers, guiding the gradient flow to maximize adaptation effectiveness and efficiency. Comprehensive experiments validate the efficacy and generalizability of our method, where CrossEarth-Gate achieves state-of-the-art performance across 16 cross-domain benchmarks for RS semantic segmentation. The code of the work will be released.",
        "translated": "在遥感（RS）领域，参数高效微调（PEFT）已成为激活基础模型通用表征能力以服务于下游任务的关键方法。然而，现有的专用PEFT方法在应用于大规模地球观测任务时往往失效，因其难以充分应对遥感数据固有的多维度、不可预测的域间差异（例如空间、语义和频域偏移）。为解决这一问题，我们提出CrossEarth-Gate，其包含两项主要贡献：第一，构建了一个全面的遥感模块工具箱，涵盖空间、语义与频域模块，用以应对多维度域间差异；第二，设计了一种基于Fisher信息引导的自适应选择机制，该机制作用于上述工具箱。通过衡量各模块对任务特定梯度流的贡献，Fisher信息量化了各模块的重要性，并动态激活最关键模块于适当层位，引导梯度流以最大化适应效果与效率。全面实验验证了本方法的有效性与泛化能力，在16个跨域遥感语义分割基准测试中，CrossEarth-Gate取得了当前最优性能。本工作代码将公开发布。",
        "translated_title": "CrossEarth-Gate：基于Fisher引导的自适应调优引擎，用于跨域遥感语义分割的高效适配",
        "label": [],
        "label_reason": "任务为语义分割，属高阶视觉任务。",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "提出Fisher引导模块选择机制，显著提升适应效率。"
    },
    {
        "title": "Kleinkram: Open Robotic Data Management",
        "url": "http://arxiv.org/abs/2511.20492v1",
        "pub_date": "2025-11-25",
        "summary": "We introduce Kleinkram, a free and open-source system designed to solve the challenge of managing massive, unstructured robotic datasets. Designed as a modular, on-premises cloud solution, Kleinkram enables scalable storage, indexing, and sharing of datasets, ranging from individual experiments to large-scale research collections. Kleinkram natively integrates with standard formats such as ROS bags and MCAP and utilises S3-compatible storage for flexibility. Beyond storage, Kleinkram features an integrated \"Action Runner\" that executes customizable Docker-based workflows for data validation, curation, and benchmarking. Kleinkram has successfully managed over 30 TB of data from diverse robotic systems, streamlining the research lifecycle through a modern web interface and a robust Command Line Interface (CLI).",
        "translated": "我们引入了 Kleinkram，这是一个免费且开源的系统，旨在解决管理海量、非结构化机器人数据集的挑战。Kleinkram 设计为模块化的本地部署云解决方案，支持从单个实验到大规模研究集合的数据集可扩展存储、索引与共享。Kleinkram 原生支持 ROS bags 和 MCAP 等标准格式，并采用兼容 S3 的存储方案以提升灵活性。除存储功能外，Kleinkram 还集成了一套“动作执行器”（Action Runner），可执行基于 Docker 的自定义工作流，用于数据验证、整理及基准测试。Kleinkram 已成功管理来自多种机器人系统的超过 30 TB 数据，通过现代化的 Web 界面和强大的命令行接口（CLI），有效简化了研究全流程。",
        "translated_title": "Kleinkram：开放机器人数据管理",
        "label": [],
        "label_reason": "论文聚焦机器人数据管理，与推荐系统无关",
        "relevance_score": 1,
        "novelty_score": 1,
        "novelty_reason": "无推荐系统创新，属通用数据管理工具"
    },
    {
        "title": "HHFT: Hierarchical Heterogeneous Feature Transformer for Recommendation Systems",
        "url": "http://arxiv.org/abs/2511.20235v1",
        "pub_date": "2025-11-25",
        "summary": "We propose HHFT (Hierarchical Heterogeneous Feature Transformer), a Transformer-based architecture tailored for industrial CTR prediction. HHFT addresses the limitations of DNN through three key designs: (1) Semantic Feature Partitioning: Grouping heterogeneous features (e.g. user profile, item information, behaviour sequennce) into semantically coherent blocks to preserve domain-specific information; (2) Heterogeneous Transformer Encoder: Adopting block-specific QKV projections and FFNs to avoid semantic confusion between distinct feature types; (3) Hiformer Layer: Capturing high-order interactions across features. Our findings reveal that Transformers significantly outperform DNN baselines, achieving a +0.4% improvement in CTR AUC at scale. We have successfully deployed the model on Taobao's production platform, observing a significant uplift in key business metrics, including a +0.6% increase in Gross Merchandise Value (GMV).",
        "translated": "我们提出 HHFT（分层异构特征变换器），这是一种面向工业级点击率预测的 Transformer 架构。HHFT 通过三项关键设计克服了深度神经网络（DNN）的局限性：(1) 语义特征分区：将异构特征（如用户画像、物品信息、行为序列）划分为语义连贯的模块，以保留领域特定信息；(2) 异构 Transformer 编码器：采用针对不同模块的独立 QKV 投影与前馈网络（FFN），避免不同特征类型间的语义混淆；(3) HiFormer 层：捕捉跨特征的高阶交互关系。我们的实验结果表明，Transformer 显著优于 DNN 基线模型，在大规模场景下实现了 CTR AUC 提升 +0.4%。该模型已在淘宝生产平台成功部署，显著提升了关键业务指标，包括商品交易总额（GMV）提升 +0.6%。",
        "translated_title": "HHFT：面向推荐系统的层次化异构特征变换器",
        "label": [
            "精排",
            "通用推荐技术"
        ],
        "label_reason": "提出Transformer架构用于CTR预测，属精排核心方法",
        "relevance_score": 9,
        "novelty_score": 7,
        "novelty_reason": "改进Transformer结构适配推荐场景，有显著性能提升"
    },
    {
        "title": "HKRAG: Holistic Knowledge Retrieval-Augmented Generation Over Visually-Rich Documents",
        "url": "http://arxiv.org/abs/2511.20227v1",
        "pub_date": "2025-11-25",
        "summary": "Existing multimodal Retrieval-Augmented Generation (RAG) methods for visually rich documents (VRD) are often biased towards retrieving salient knowledge(e.g., prominent text and visual elements), while largely neglecting the critical fine-print knowledge(e.g., small text, contextual details). This limitation leads to incomplete retrieval and compromises the generator's ability to produce accurate and comprehensive answers. To bridge this gap, we propose HKRAG, a new holistic RAG framework designed to explicitly capture and integrate both knowledge types. Our framework features two key components: (1) a Hybrid Masking-based Holistic Retriever that employs explicit masking strategies to separately model salient and fine-print knowledge, ensuring a query-relevant holistic information retrieval; and (2) an Uncertainty-guided Agentic Generator that dynamically assesses the uncertainty of initial answers and actively decides how to integrate the two distinct knowledge streams for optimal response generation. Extensive experiments on open-domain visual question answering benchmarks show that HKRAG consistently outperforms existing methods in both zero-shot and supervised settings, demonstrating the critical importance of holistic knowledge retrieval for VRD understanding.",
        "translated": "现有针对视觉丰富文档（VRD）的多模态检索增强生成（RAG）方法往往倾向于检索显著知识（如醒目文本与视觉元素），而大量忽略关键的细粒度信息（如小字、上下文细节）。这一局限导致检索不完整，削弱了生成器输出准确且全面答案的能力。为弥合此差距，我们提出HKRAG，一种旨在显式捕获并融合两类知识的全新整体型RAG框架。该框架包含两个核心组件：(1) 基于混合掩码的整体检索器，采用显式掩码策略分别建模显著知识与细粒度知识，确保查询相关性的全局信息检索；(2) 基于不确定性引导的代理生成器，动态评估初始答案的不确定性，并主动决定如何整合两类不同知识流以实现最优响应生成。在开放域视觉问答基准上的广泛实验表明，HKRAG在零样本与监督设置下均持续优于现有方法，验证了整体知识检索对VRD理解的关键重要性。",
        "translated_title": "HKRAG：面向视觉丰富文档的综合知识检索增强生成",
        "label": [],
        "label_reason": "聚焦多模态检索生成，非推荐系统核心环节",
        "relevance_score": 3,
        "novelty_score": 7,
        "novelty_reason": "创新性改进RAG框架，但非推荐算法"
    },
    {
        "title": "Enhancing Sequential Recommendation with World Knowledge from Large Language Models",
        "url": "http://arxiv.org/abs/2511.20177v1",
        "pub_date": "2025-11-25",
        "summary": "Sequential Recommendation System~(SRS) has become pivotal in modern society, which predicts subsequent actions based on the user's historical behavior. However, traditional collaborative filtering-based sequential recommendation models often lead to suboptimal performance due to the limited information of their collaborative signals. With the rapid development of LLMs, an increasing number of works have incorporated LLMs' world knowledge into sequential recommendation. Although they achieve considerable gains, these approaches typically assume the correctness of LLM-generated results and remain susceptible to noise induced by LLM hallucinations. To overcome these limitations, we propose GRASP (Generation Augmented Retrieval with Holistic Attention for Sequential Prediction), a flexible framework that integrates generation augmented retrieval for descriptive synthesis and similarity retrieval, and holistic attention enhancement which employs multi-level attention to effectively employ LLM's world knowledge even with hallucinations and better capture users' dynamic interests. The retrieved similar users/items serve as auxiliary contextual information for the later holistic attention enhancement module, effectively mitigating the noisy guidance of supervision-based methods. Comprehensive evaluations on two public benchmarks and one industrial dataset reveal that GRASP consistently achieves state-of-the-art performance when integrated with diverse backbones. The code is available at: https://anonymous.4open.science/r/GRASP-SRS.",
        "translated": "序列推荐系统（SRS）已成为现代社会中的关键组成部分，其基于用户的历史行为预测后续动作。然而，传统的基于协同过滤的序列推荐模型往往因协同信号信息有限而导致性能次优。随着大语言模型（LLM）的快速发展，越来越多的研究工作开始将LLM的世界知识融入序列推荐中。尽管这些方法取得了显著提升，但它们通常假设LLM生成结果的准确性，并仍易受由LLM幻觉引发的噪声影响。为克服上述局限性，我们提出GRASP（Generation Augmented Retrieval with Holistic Attention for Sequential Prediction），一个灵活的框架，该框架融合了增强生成的检索机制，用于描述性合成与相似度检索，并结合全局注意力增强模块，通过多级注意力机制有效利用LLM世界知识，即使在存在幻觉的情况下亦能更好地捕捉用户的动态兴趣。所检索到的相似用户/物料作为辅助上下文信息供给后续的全局注意力增强模块，有效缓解了基于监督信号的方法所面临的噪声引导问题。在两个公开基准数据集和一个工业数据集上的全面评估表明，当与多种骨干网络集成时，GRASP始终实现最先进性能。代码可访问：https://anonymous.4open.science/r/GRASP-SRS。",
        "translated_title": "利用大语言模型的世界知识增强序列推荐",
        "label": [
            "序列推荐",
            "LLM生成式推荐"
        ],
        "label_reason": "结合LLM增强序列推荐，解决幻觉问题。",
        "relevance_score": 9,
        "novelty_score": 8,
        "novelty_reason": "创新框架融合生成与检索，提升鲁棒性。"
    },
    {
        "title": "SEDA: A Self-Adapted Entity-Centric Data Augmentation for Boosting Gird-based Discontinuous NER Models",
        "url": "http://arxiv.org/abs/2511.20143v1",
        "pub_date": "2025-11-25",
        "summary": "Named Entity Recognition (NER) is a critical task in natural language processing, yet it remains particularly challenging for discontinuous entities. The primary difficulty lies in text segmentation, as traditional methods often missegment or entirely miss cross-sentence discontinuous entities, significantly affecting recognition accuracy. Therefore, we aim to address the segmentation and omission issues associated with such entities. Recent studies have shown that grid-tagging methods are effective for information extraction due to their flexible tagging schemes and robust architectures. Building on this, we integrate image data augmentation techniques, such as cropping, scaling, and padding, into grid-based models to enhance their ability to recognize discontinuous entities and handle segmentation challenges. Experimental results demonstrate that traditional segmentation methods often fail to capture cross-sentence discontinuous entities, leading to decreased performance. In contrast, our augmented grid models achieve notable improvements. Evaluations on the CADEC, ShARe13, and ShARe14 datasets show F1 score gains of 1-2.5% overall and 3.7-8.4% for discontinuous entities, confirming the effectiveness of our approach.",
        "translated": "命名实体识别（NER）是自然语言处理中的关键任务，但针对非连续实体仍极具挑战性。其主要难点在于文本切分：传统方法常错误切分或完全遗漏跨句非连续实体，显著影响识别准确率。因此，我们旨在解决此类实体所面临的切分与遗漏问题。近期研究表明，网格标注方法因其灵活的标注方案和稳健的架构，在信息抽取任务中表现优异。在此基础上，我们将图像数据增强技术（如裁剪、缩放与填充）引入基于网格的模型，以增强其对非连续实体的识别能力并缓解切分难题。实验结果表明，传统切分方法常无法捕捉跨句非连续实体，导致性能下降；相比之下，我们的增强型网格模型取得了显著提升。在CADEC、ShARe13及ShARe14数据集上的评估显示，整体F1分数提升1%-2.5%，针对非连续实体的提升幅度达3.7%-8.4%，验证了本方法的有效性。",
        "translated_title": "SEDA：一种自适应实体为中心的数据增强方法，用于提升基于网格的不连续命名实体识别模型",
        "label": [],
        "label_reason": "论文聚焦NER任务，与推荐系统无关。",
        "relevance_score": 1,
        "novelty_score": 3,
        "novelty_reason": "方法改进有限，未引入新范式。"
    },
    {
        "title": "Towards A Tri-View Diffusion Framework for Recommendation",
        "url": "http://arxiv.org/abs/2511.20122v1",
        "pub_date": "2025-11-25",
        "summary": "Diffusion models (DMs) have recently gained significant interest for their exceptional potential in recommendation tasks. This stems primarily from their prominent capability in distilling, modeling, and generating comprehensive user preferences. However, previous work fails to examine DMs in recommendation tasks through a rigorous lens. In this paper, we first experimentally investigate the completeness of recommender models from a thermodynamic view. We reveal that existing DM-based recommender models operate by maximizing the energy, while classic recommender models operate by reducing the entropy. Based on this finding, we propose a minimalistic diffusion framework that incorporates both factors via the maximization of Helmholtz free energy. Meanwhile, to foster the optimization, our reverse process is armed with a well-designed denoiser to maintain the inherent anisotropy, which measures the user-item cross-correlation in the context of bipartite graphs. Finally, we adopt an Acceptance-Rejection Gumbel Sampling Process (AR-GSP) to prioritize the far-outnumbered unobserved interactions for model robustness. AR-GSP integrates an acceptance-rejection sampling to ensure high-quality hard negative samples for general recommendation tasks, and a timestep-dependent Gumbel Softmax to handle an adaptive sampling strategy for diffusion models. Theoretical analyses and extensive experiments demonstrate that our proposed framework has distinct superiority over baselines in terms of accuracy and efficiency.",
        "translated": "扩散模型（DMs）近年来因其在推荐任务中的卓越潜力而受到广泛关注。这主要源于其在提炼、建模和生成全面用户偏好方面的突出能力。然而，以往研究未能从严格的角度考察扩散模型在推荐任务中的表现。本文首先从热力学视角实验性地探究推荐模型的完备性。我们发现，现有的基于扩散模型的推荐模型通过最大化能量进行运作，而经典推荐模型则通过降低熵值实现优化。基于这一发现，我们提出一种极简化的扩散框架，通过最大化亥姆霍兹自由能同时融合上述两个因素。与此同时，为促进优化，我们的逆过程配备了一种精心设计的去噪器，以保持固有的各向异性，该各向异性衡量的是二部图背景下用户-物料间的交叉相关性。最后，我们采用接受-拒绝Gumbel采样过程（AR-GSP），以提升模型对数量远超观测交互的未观测交互的鲁棒性。AR-GSP结合了接受-拒绝采样机制，以确保通用推荐任务中高质量硬负样本的获取，并引入时步依赖的Gumbel Softmax，用于处理扩散模型所需的自适应采样策略。理论分析与大量实验表明，所提出的框架在准确性和效率方面均显著优于基线方法。",
        "translated_title": "面向推荐系统的三视角扩散框架",
        "label": [
            "精排",
            "LLM生成式推荐"
        ],
        "label_reason": "提出基于扩散模型的推荐框架，聚焦生成式推荐与排序优化。",
        "relevance_score": 9,
        "novelty_score": 8,
        "novelty_reason": "首次将热力学视角与扩散模型结合用于推荐系统建模。"
    },
    {
        "title": "Invisible in Search? Auditing Aesthetic Bias in the Visual Representation of Holocaust Victims on Google",
        "url": "http://arxiv.org/abs/2511.20036v1",
        "pub_date": "2025-11-25",
        "summary": "Information retrieval systems, such as search engines, increasingly shape the representation of the past and present states of social reality. Despite their importance, these systems face challenges in dealing with the ethical aspects of representation due to various forms of bias, including aesthetic bias that perpetuates hegemonic patterns of representation. While most research on aesthetic bias has examined it in the context of current societal issues, it is also crucial for historical representation, particularly of sensitive subjects such as historical atrocities. To address this gap, we conduct a comparative audit of the visual representation of Holocaust victims on Google. We find that Google tends to propagate a male-dominated representation of Holocaust victims with an emphasis on atrocity context, risking rendering invisible gender-specific suffering and decreasing potential for nurturing empathy. We also observe a variation in representation across geographic locations, suggesting that search algorithms may produce their own aesthetic of victimhood.",
        "translated": "信息检索系统，如搜索引擎，日益塑造着社会现实过去与现在状态的表征。尽管这些系统至关重要，但由于存在多种形式的偏见（包括强化霸权表征模式的审美偏见），它们在处理表征的伦理方面面临挑战。虽然大多数关于审美偏见的研究聚焦于当前社会议题，但其对历史表征——特别是涉及敏感主题如历史暴行的历史再现——同样具有关键意义。为填补这一空白，我们对谷歌上大屠杀受害者的视觉表征进行了比较审计。研究发现，谷歌倾向于传播以男性为主导、强调暴行语境的大屠杀受害者表征，这可能导致性别特定苦难被忽视，并削弱培养共情的可能性。此外，我们还观察到不同地理区域间表征呈现差异，表明搜索算法可能自身生成一种独特的“受害者美学”。",
        "translated_title": "搜索中看不见的？审计谷歌上大屠杀受害者视觉表征中的审美偏见",
        "label": [],
        "label_reason": "研究搜索偏见，非推荐系统核心问题",
        "relevance_score": 3,
        "novelty_score": 5,
        "novelty_reason": "审计方法常规，未提出新推荐技术"
    },
    {
        "title": "Adaptive Knowledge Transfer for Cross-Disciplinary Cold-Start Knowledge Tracing",
        "url": "http://arxiv.org/abs/2511.20009v1",
        "pub_date": "2025-11-25",
        "summary": "Cross-Disciplinary Cold-start Knowledge Tracing (CDCKT) faces a critical challenge: insufficient student interaction data in the target discipline prevents effective knowledge state modeling and performance prediction. Existing cross-disciplinary methods rely on overlapping entities between disciplines for knowledge transfer through simple mapping functions, but suffer from two key limitations: (1) overlapping entities are scarce in real-world scenarios, and (2) simple mappings inadequately capture cross-disciplinary knowledge complexity. To overcome these challenges, we propose Mixed of Experts and Adversarial Generative Network-based Cross-disciplinary Cold-start Knowledge Tracing Framework. Our approach consists of three key components: First, we pre-train a source discipline model and cluster student knowledge states into K categories. Second, these cluster attributes guide a mixture-of-experts network through a gating mechanism, serving as a cross-domain mapping bridge. Third, an adversarial discriminator enforces feature separation by pulling same-attribute student features closer while pushing different-attribute features apart, effectively mitigating small-sample limitations. We validate our method's effectiveness across 20 extreme cross-disciplinary cold-start scenarios.",
        "translated": "跨学科冷启动知识追踪（CDCKT）面临一个关键挑战：目标学科中学生交互数据不足，难以有效建模知识状态并进行性能预测。现有跨学科方法依赖学科间重叠实体，通过简单映射函数实现知识迁移，但存在两大核心局限：（1）现实场景中重叠实体稀缺；（2）简单映射无法充分捕捉跨学科知识的复杂性。为克服上述挑战，我们提出基于混合专家与对抗生成网络的跨学科冷启动知识追踪框架。该方法包含三个关键组件：首先，预训练源学科模型，并将学生知识状态聚类为 K 个类别；其次，利用这些聚类属性通过门控机制引导混合专家网络，作为跨域映射桥梁；第三，引入对抗判别器，通过拉近同属性学生特征、推离异属性特征，强制实现特征分离，从而有效缓解小样本限制。我们在 20 个极端跨学科冷启动场景中验证了该方法的有效性。",
        "translated_title": "跨学科冷启动下的自适应知识迁移",
        "label": [
            "跨域/联邦推荐",
            "负采样与对比学习"
        ],
        "label_reason": "解决跨学科冷启动知识追踪，适配推荐系统用户建模场景。",
        "relevance_score": 6,
        "novelty_score": 7,
        "novelty_reason": "提出MoE+对抗网络新框架，提升小样本跨域迁移能力。"
    },
    {
        "title": "Popularity Bias Alignment Estimates",
        "url": "http://arxiv.org/abs/2511.19999v1",
        "pub_date": "2025-11-25",
        "summary": "We are extending Popularity Bias Memorization theorem from arXiv:archive/2404.12008 in several directions. We extend it to arbitrary degree distributions and also prove both upper and lower estimates for the alignment with top-k singular hyperspace.",
        "translated": "我们正在将 arXiv:archive/2404.12008 中提出的流行度偏差记忆定理扩展至若干方向。我们将其推广至任意度分布情形，并同时证明了其与前 k 个奇异超空间对齐的上下界估计。",
        "translated_title": "流行度偏差对齐估计",
        "label": [],
        "label_reason": "论文聚焦流行度偏差理论，与推荐系统无直接关联。",
        "relevance_score": 3,
        "novelty_score": 5,
        "novelty_reason": "扩展流行度偏置定理，但非推荐系统专用创新。"
    },
    {
        "title": "REWA: Witness-Overlap Theory -- Foundations for Composable Binary Similarity Systems",
        "url": "http://arxiv.org/abs/2511.19998v1",
        "pub_date": "2025-11-25",
        "summary": "REWA introduces a general theory of similarity based on witness-overlap structures. We show that whenever similarity between concepts can be expressed as monotone witness overlap -- whether arising from graph neighborhoods, causal relations, temporal structure, topological features, symbolic patterns, or embedding-based neighborhoods -- it admits a reduction to compact encodings with provable ranking preservation guarantees. REWA systems consist of: (1) finite witness sets $W(v)$, (2) semi-random bit assignments generated from each witness, and (3) monotonicity of expected similarity in the overlap $Δ(u, v) = |W(u) \\cap W(v)|$. We prove that under an overlap-gap condition on the final witness sets -- independent of how they were constructed -- top-$k$ rankings are preserved using $m = O(\\log(|V|/δ))$ bits. The witness-set formulation is compositional: any sequence of structural, temporal, causal, topological, information-theoretic, or learned transformations can be combined into pipelines that terminate in discrete witness sets. The theory applies to the final witness overlap, enabling modular construction of similarity systems from reusable primitives. This yields a vast design space: millions of composable similarity definitions inherit logarithmic encoding complexity. REWA subsumes and unifies Bloom filters, minhash, LSH bitmaps, random projections, sketches, and hierarchical filters as special cases. It provides a principled foundation for similarity systems whose behavior is governed by witness overlap rather than hash-function engineering. This manuscript presents the axioms, the main reducibility theorem, complete proofs with explicit constants, and a detailed discussion of compositional design, limitations, and future extensions including multi-bit encodings, weighted witnesses, and non-set representations.",
        "translated": "REWA 基于见证重叠结构提出了一种通用的相似性理论。我们证明，只要概念之间的相似性可表示为单调见证重叠——无论该重叠源于图邻域、因果关系、时序结构、拓扑特征、符号模式或基于嵌入的邻域——则其均可被缩减为具有可证排序保持保证的紧凑编码。REWA 系统由以下三部分组成：（1）有限的见证集 $W(v)$；（2）从每个见证生成的半随机位赋值；（3）期望相似度在重叠 $Δ(u, v) = |W(u) \\cap W(v)|$ 中的单调性。我们在最终见证集满足重叠-间隙条件的前提下（与这些集合如何构造无关），证明仅需 $m = O(\\log(|V|/δ))$ 位即可保持 top-$k$ 排名。见证集形式具备组合性：任意序列的结构、时序、因果、拓扑、信息论或学习变换均可组合成终止于离散见证集的管线。该理论适用于最终的见证重叠，从而允许从可复用的基本构件模块化构建相似性系统。这开辟了广阔的设计空间：数百万种可组合的相似性定义继承对数级编码复杂度。REWA 包含并统一了 Bloom 过滤器、MinHash、LSH 位图、随机投影、小样本估计及分层过滤器作为特例。它为一种行为由见证重叠而非哈希函数工程所主导的相似性系统提供了严谨的基础。本文呈现了公理、主要可约性定理、带显式常数的完整证明，以及对组合式设计、局限性与未来扩展（包括多比特编码、加权见证及非集合表示）的详细讨论。",
        "translated_title": "REWA：见证重叠理论——可组合二元相似性系统的理论基础",
        "label": [],
        "label_reason": "与推荐系统无直接关联，属通用相似性理论",
        "relevance_score": 3,
        "novelty_score": 7,
        "novelty_reason": "提出可组合相似性框架，但非专为推荐设计"
    },
    {
        "title": "$\\text{R}^2\\text{R}$: A Route-to-Rerank Post-Training Framework for Multi-Domain Decoder-Only Rerankers",
        "url": "http://arxiv.org/abs/2511.19987v1",
        "pub_date": "2025-11-25",
        "summary": "Decoder-only rerankers are central to Retrieval-Augmented Generation (RAG). However, generalist models miss domain-specific nuances in high-stakes fields like finance and law, and naive fine-tuning causes surface-form overfitting and catastrophic forgetting. To address this challenge, we introduce R2R, a domain-aware framework that combines dynamic expert routing with a two-stage training strategy, Entity Abstraction for Generalization (EAG). EAG introduces a counter-shortcut mechanism by masking the most predictive surface cues, forcing the reranker to learn domain-invariant relevance patterns rather than memorizing dataset-specific entities. To efficiently activate domain experts, R2R employs a lightweight Latent Semantic Router that probes internal representations from the frozen backbone decoder to select the optimal LoRA expert per query. Extensive experiments across different reranker backbones and diverse domains (legal, medical, and financial) demonstrate that R2R consistently surpasses generalist and single-domain fine-tuned baselines. Our results confirm that R2R is a model-agnostic and modular approach to domain specialization with strong cross-domain robustness.",
        "translated": "仅解码器结构的重排模型是检索增强生成（RAG）的核心。然而，通用模型在金融、法律等高风险领域中缺乏特定领域的细微差别，而简单的微调会导致表面形式过拟合与灾难性遗忘。为应对这一挑战，我们提出了 R2R，一个具有领域感知能力的框架，结合动态专家路由与两阶段训练策略——实体抽象以促进泛化（EAG）。EAG 通过屏蔽最具预测性的表面线索引入反捷径机制，迫使重排模型学习领域无关的相关性模式，而非记忆特定数据集中的实体。为高效激活领域专家，R2R 使用轻量级潜在语义路由器，探测冻结主干解码器的内部表示，为每个查询选择最优 LoRA 专家。在不同重排模型主干及多样领域（法律、医学与金融）上的广泛实验表明，R2R 一致优于通用模型和单一领域微调基线。我们的结果证实，R2R 是一种模型无关且模块化的领域专业化方法，具备强大的跨领域鲁棒性。",
        "translated_title": "R²R：面向多域解码器-only重排器的路由-重排后训练框架",
        "label": [
            "重排",
            "LLM生成式推荐"
        ],
        "label_reason": "针对RAG中重排模块的多领域适配问题",
        "relevance_score": 8,
        "novelty_score": 7,
        "novelty_reason": "提出路由+抽象机制解决领域泛化问题"
    },
    {
        "title": "The 2nd Workshop on Human-Centered Recommender Systems",
        "url": "http://arxiv.org/abs/2511.19979v1",
        "pub_date": "2025-11-25",
        "summary": "Recommender systems shape how people discover information, form opinions, and connect with society. Yet, as their influence grows, traditional metrics, e.g., accuracy, clicks, and engagement, no longer capture what truly matters to humans. The workshop on Human-Centered Recommender Systems (HCRS) calls for a paradigm shift from optimizing engagement toward designing systems that truly understand, involve, and benefit people. It brings together researchers in recommender systems, human-computer interaction, AI safety, and social computing to explore how human values, e.g., trust, safety, fairness, transparency, and well-being, can be integrated into recommendation processes. Centered around three thematic axes-Human Understanding, Human Involvement, and Human Impact-HCRS features keynotes, panels, and papers covering topics from LLM-based interactive recommenders to societal welfare optimization. By fostering interdisciplinary collaboration, HCRS aims to shape the next decade of responsible and human-aligned recommendation research.",
        "translated": "推荐系统塑造着人们如何发现信息、形成观点以及与社会建立联系。然而，随着其影响力日益扩大，传统的评价指标（如准确率、点击量和参与度）已无法真正反映对人类而言至关重要的价值。人类中心化推荐系统研讨会（HCRS）呼吁从优化用户参与转向设计真正理解、融入并惠及人类的系统。该研讨会汇聚了推荐系统、人机交互、人工智能安全及社会计算等领域的研究人员，共同探讨如何将人类价值观——例如信任、安全、公平、透明性与福祉——整合进推荐过程。围绕“人类理解”、“人类参与”与“人类影响”三大主题轴心，HCRS设置了主旨演讲、专题讨论与论文评审，涵盖基于大语言模型的互动式推荐器到社会福祉优化等议题。通过促进跨学科协作，HCRS致力于塑造未来十年负责任且以人为本的推荐研究方向。",
        "translated_title": "第二届以人为中心的推荐系统研讨会",
        "label": [
            "推荐系统公平性/可解释性",
            "LLM生成式推荐",
            "通用推荐技术"
        ],
        "label_reason": "聚焦人本推荐，整合价值观与LLM交互",
        "relevance_score": 8,
        "novelty_score": 7,
        "novelty_reason": "推动推荐系统向伦理与人类价值对齐"
    },
    {
        "title": "LLM-EDT: Large Language Model Enhanced Cross-domain Sequential Recommendation with Dual-phase Training",
        "url": "http://arxiv.org/abs/2511.19931v1",
        "pub_date": "2025-11-25",
        "summary": "Cross-domain Sequential Recommendation (CDSR) has been proposed to enrich user-item interactions by incorporating information from various domains. Despite current progress, the imbalance issue and transition issue hinder further development of CDSR. The former one presents a phenomenon that the interactions in one domain dominate the entire behavior, leading to difficulty in capturing the domain-specific features in the other domain. The latter points to the difficulty in capturing users' cross-domain preferences within the mixed interaction sequence, resulting in poor next-item prediction performance for specific domains. With world knowledge and powerful reasoning ability, Large Language Models (LLMs) partially alleviate the above issues by performing as a generator and an encoder. However, current LLMs-enhanced CDSR methods are still under exploration, which fail to recognize the irrelevant noise and rough profiling problems. Thus, to make peace with the aforementioned challenges, we proposed an LLMs Enhanced Cross-domain Sequential Recommendation with Dual-phase Training ({LLM-EDT}). To address the imbalance issue while introducing less irrelevant noise, we first propose the transferable item augmenter to adaptively generate possible cross-domain behaviors for users. Then, to alleviate the transition issue, we introduce a dual-phase training strategy to empower the domain-specific thread with a domain-shared background. As for the rough profiling problem, we devise a domain-aware profiling module to summarize the user's preference in each domain and adaptively aggregate them to generate comprehensive user profiles. The experiments on three public datasets validate the effectiveness of our proposed LLM-EDT. To ease reproducibility, we have released the detailed code online at {https://anonymous.4open.science/r/LLM-EDT-583F}.",
        "translated": "跨域序列推荐（CDSR）通过引入多领域信息来丰富用户-物料交互。尽管当前研究已取得一定进展，但领域不平衡问题与领域过渡问题仍制约着CDSR的进一步发展。前者表现为某一领域中的交互行为主导了整体行为模式，导致难以捕捉其他领域的特定特征；后者则指向在混合交互序列中难以有效捕捉用户的跨域偏好，从而造成对特定领域下一物料预测性能下降。凭借世界知识和强大的推理能力，大语言模型（LLMs）可作为生成器与编码器部分缓解上述问题。然而，当前基于LLMs增强的CDSR方法尚未充分探索，未能有效识别无关噪声及粗粒度画像问题。为此，我们提出了一种名为“LLM增强跨域序列推荐双阶段训练框架”（LLM-EDT）的方法，以应对上述挑战。为解决领域不平衡问题并减少无关噪声干扰，我们首先提出一种可迁移的物料增强器，用于自适应生成用户可能的跨域行为；随后，为缓解领域过渡问题，我们引入双阶段训练策略，赋予各领域专属模块以共享背景知识的支持；针对粗粒度画像问题，我们设计了一种领域感知画像模块，分别总结用户在各个领域的偏好，并自适应聚合以生成全面的用户画像。我们在三个公开数据集上的实验验证了所提方法LLM-EDT的有效性。为便于复现，我们已在{https://anonymous.4open.science/r/LLM-EDT-583F}公开详细代码。",
        "translated_title": "LLM-EDT：基于大语言模型增强的跨域序列推荐与双阶段训练",
        "label": [
            "跨域/联邦推荐",
            "序列推荐",
            "LLM生成式推荐"
        ],
        "label_reason": "针对跨域序列推荐，融合LLM解决不平衡与过渡问题。",
        "relevance_score": 9,
        "novelty_score": 8,
        "novelty_reason": "双阶段训练+领域感知模块，创新性提升跨域建模能力。"
    },
    {
        "title": "RubricRL: Simple Generalizable Rewards for Text-to-Image Generation",
        "url": "http://arxiv.org/abs/2511.20651v1",
        "pub_date": "2025-11-25",
        "summary": "Reinforcement learning (RL) has recently emerged as a promising approach for aligning text-to-image generative models with human preferences. A key challenge, however, lies in designing effective and interpretable rewards. Existing methods often rely on either composite metrics (e.g., CLIP, OCR, and realism scores) with fixed weights or a single scalar reward distilled from human preference models, which can limit interpretability and flexibility. We propose RubricRL, a simple and general framework for rubric-based reward design that offers greater interpretability, composability, and user control. Instead of using a black-box scalar signal, RubricRL dynamically constructs a structured rubric for each prompt--a decomposable checklist of fine-grained visual criteria such as object correctness, attribute accuracy, OCR fidelity, and realism--tailored to the input text. Each criterion is independently evaluated by a multimodal judge (e.g., o4-mini), and a prompt-adaptive weighting mechanism emphasizes the most relevant dimensions. This design not only produces interpretable and modular supervision signals for policy optimization (e.g., GRPO or PPO), but also enables users to directly adjust which aspects to reward or penalize. Experiments with an autoregressive text-to-image model demonstrate that RubricRL improves prompt faithfulness, visual detail, and generalizability, while offering a flexible and extensible foundation for interpretable RL alignment across text-to-image architectures.",
        "translated": "强化学习（RL）最近作为一种有前景的方法，被用于将文本到图像生成模型与人类偏好对齐。然而，一个关键挑战在于设计有效且可解释的奖励机制。现有方法通常依赖于复合指标（例如 CLIP、OCR 和真实感评分）并采用固定权重，或从人类偏好模型蒸馏出单一标量奖励，这可能限制其可解释性和灵活性。我们提出 RubricRL，这是一种简单通用的基于评分标准的奖励设计框架，能够提供更高的可解释性、组合性与用户可控性。RubricRL 不再使用黑盒标量信号，而是为每个提示动态构建结构化的评分标准——即针对输入文本定制的、可分解的细粒度视觉准则清单，如物体正确性、属性准确性、OCR 保真度和真实感等。每个准则由多模态评判器（例如 o4-mini）独立评估，并通过提示自适应加权机制突出最相关的维度。该设计不仅为策略优化（如 GRPO 或 PPO）生成可解释且模块化的监督信号，还允许用户直接调整应奖励或惩罚的具体方面。在自回归文本到图像模型上的实验表明，RubricRL 可提升提示忠实度、视觉细节及泛化能力，同时为跨文本到图像架构的可解释 RL 对齐提供了灵活且可扩展的基础。",
        "translated_title": "RubricRL：用于文本到图像生成的简单通用奖励机制",
        "label": [],
        "label_reason": "属于图像生成，非像素级图像恢复任务",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出可调评分框架，但属高阶对齐优化"
    },
    {
        "title": "MedROV: Towards Real-Time Open-Vocabulary Detection Across Diverse Medical Imaging Modalities",
        "url": "http://arxiv.org/abs/2511.20650v1",
        "pub_date": "2025-11-25",
        "summary": "Traditional object detection models in medical imaging operate within a closed-set paradigm, limiting their ability to detect objects of novel labels. Open-vocabulary object detection (OVOD) addresses this limitation but remains underexplored in medical imaging due to dataset scarcity and weak text-image alignment. To bridge this gap, we introduce MedROV, the first Real-time Open Vocabulary detection model for medical imaging. To enable open-vocabulary learning, we curate a large-scale dataset, Omnis, with 600K detection samples across nine imaging modalities and introduce a pseudo-labeling strategy to handle missing annotations from multi-source datasets. Additionally, we enhance generalization by incorporating knowledge from a large pre-trained foundation model. By leveraging contrastive learning and cross-modal representations, MedROV effectively detects both known and novel structures. Experimental results demonstrate that MedROV outperforms the previous state-of-the-art foundation model for medical image detection with an average absolute improvement of 40 mAP50, and surpasses closed-set detectors by more than 3 mAP50, while running at 70 FPS, setting a new benchmark in medical detection. Our source code, dataset, and trained model are available at https://github.com/toobatehreem/MedROV.",
        "translated": "医学影像中的传统目标检测模型基于封闭集范式运行，限制了其对新类别对象的检测能力。开放词汇目标检测（OVOD）可缓解这一局限，但在医学影像领域尚处于探索阶段，主要受限于数据集稀缺与文本-图像对齐度弱的问题。为弥合此差距，我们提出了MedROV——首个面向医学影像的实时开放词汇检测模型。为支持开放词汇学习，我们构建了一个大规模数据集Omnis，涵盖九种成像模态、共计60万例检测样本，并引入伪标签策略以应对多源数据集中标注缺失的问题。此外，我们通过融合大型预训练基础模型的知识增强模型泛化能力。借助对比学习与跨模态表征，MedROV能够有效检测已知和未知结构。实验结果表明，MedROV在医学图像检测任务中超越此前最先进的基础模型，平均绝对mAP50提升40点，并相较封闭集检测器高出3 mAP50以上，同时保持70 FPS的实时推理速度，确立了医学检测领域的全新基准。我们的源代码、数据集及训练模型已开源于 https://github.com/toobatehreem/MedROV。",
        "translated_title": "MedROV：面向多样医学影像模态的实时开放词汇检测",
        "label": [],
        "label_reason": "目标为医学图像检测，属高阶视觉任务",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "首次实现医学领域开放词汇检测，性能提升显著"
    },
    {
        "title": "Infinity-RoPE: Action-Controllable Infinite Video Generation Emerges From Autoregressive Self-Rollout",
        "url": "http://arxiv.org/abs/2511.20649v1",
        "pub_date": "2025-11-25",
        "summary": "Current autoregressive video diffusion models are constrained by three core bottlenecks: (i) the finite temporal horizon imposed by the base model's 3D Rotary Positional Embedding (3D-RoPE), (ii) slow prompt responsiveness in maintaining fine-grained action control during long-form rollouts, and (iii) the inability to realize discontinuous cinematic transitions within a single generation stream. We introduce $\\infty$-RoPE, a unified inference-time framework that addresses all three limitations through three interconnected components: Block-Relativistic RoPE, KV Flush, and RoPE Cut. Block-Relativistic RoPE reformulates temporal encoding as a moving local reference frame, where each newly generated latent block is rotated relative to the base model's maximum frame horizon while earlier blocks are rotated backward to preserve relative temporal geometry. This relativistic formulation eliminates fixed temporal positions, enabling continuous video generation far beyond the base positional limits. To obtain fine-grained action control without re-encoding, KV Flush renews the KV cache by retaining only two latent frames, the global sink and the last generated latent frame, thereby ensuring immediate prompt responsiveness. Finally, RoPE Cut introduces controlled discontinuities in temporal RoPE coordinates, enabling multi-cut scene transitions within a single continuous rollout. Together, these components establish $\\infty$-RoPE as a training-free foundation for infinite-horizon, controllable, and cinematic video diffusion. Comprehensive experiments show that $\\infty$-RoPE consistently surpasses previous autoregressive models in overall VBench scores.",
        "translated": "当前的自回归视频扩散模型受限于三个核心瓶颈：(i) 基础模型中三维旋转位置嵌入（3D-RoPE）所设定的有限时间跨度；(ii) 在长序列生成过程中维持细粒度动作控制时响应缓慢；(iii) 无法在单次生成流内实现非连续性的电影级转场。我们提出了 $\\infty$-RoPE，一个统一的推理时框架，通过三个相互关联的组件解决了上述所有限制：块相对旋转 RoPE、KV 清除与 RoPE 切割。块相对旋转 RoPE 将时间编码重构为移动的局部参考系——每个新生成的潜在块相对于基础模型的最大帧范围进行旋转，而较早的块则向后旋转以保持相对时间几何结构。这种相对论式建模消除了固定的时间位置，从而实现了远超基础位置限制的连续视频生成。为了在不重新编码的前提下获得细粒度动作控制，KV 清除仅保留两个潜在帧——全局接收端和最后一个生成的潜在帧，由此确保即时提示响应能力。最后，RoPE 切割在时间 RoPE 坐标中引入可控的非连续性，使单次连续生成流内可实现多处场景切换。这三部分共同构成了 $\\infty$-RoPE，作为无需训练的无限时域、可控且具电影风格的视频扩散的基础架构。全面实验表明，$\\infty$-RoPE 在整体 VBench 评分上持续超越以往的自回归模型。",
        "translated_title": "Infinity-RoPE：自回归自展开机制催生可控制动作的无限视频生成",
        "label": [],
        "label_reason": "生成视频属高阶任务，非像素级图像处理",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "新框架提升生成能力，但非图像恢复类创新"
    },
    {
        "title": "Diverse Video Generation with Determinantal Point Process-Guided Policy Optimization",
        "url": "http://arxiv.org/abs/2511.20647v1",
        "pub_date": "2025-11-25",
        "summary": "While recent text-to-video (T2V) diffusion models have achieved impressive quality and prompt alignment, they often produce low-diversity outputs when sampling multiple videos from a single text prompt. We tackle this challenge by formulating it as a set-level policy optimization problem, with the goal of training a policy that can cover the diverse range of plausible outcomes for a given prompt. To address this, we introduce DPP-GRPO, a novel framework for diverse video generation that combines Determinantal Point Processes (DPPs) and Group Relative Policy Optimization (GRPO) theories to enforce explicit reward on diverse generations. Our objective turns diversity into an explicit signal by imposing diminishing returns on redundant samples (via DPP) while supplies groupwise feedback over candidate sets (via GRPO). Our framework is plug-and-play and model-agnostic, and encourages diverse generations across visual appearance, camera motions, and scene structure without sacrificing prompt fidelity or perceptual quality. We implement our method on WAN and CogVideoX, and show that our method consistently improves video diversity on state-of-the-art benchmarks such as VBench, VideoScore, and human preference studies. Moreover, we release our code and a new benchmark dataset of 30,000 diverse prompts to support future research.",
        "translated": "尽管近期基于文本到视频（T2V）的扩散模型在生成质量与提示对齐方面取得了显著进展，但在从单个文本提示采样多个视频时，其输出往往缺乏多样性。我们通过将其建模为集合级策略优化问题来应对这一挑战，目标是训练一个能够覆盖给定提示下多种合理结果范围的策略。为此，我们提出了 DPP-GRPO，一种新颖的多样化视频生成框架，结合了行列式点过程（DPPs）与群体相对策略优化（GRPO）理论，以显式奖励多样化的生成结果。我们的目标将多样性转化为明确信号：通过 DPP 对冗余样本施加递减回报，同时借助 GRPO 在候选集上提供群体反馈。该框架具有即插即用、模型无关的特点，并能在不牺牲提示保真度或感知质量的前提下，鼓励在视觉外观、摄像机运动与场景结构等方面产生多样化生成。我们在 WAN 和 CogVideoX 上实现了本方法，并表明其在 VBench、VideoScore 等主流基准测试以及人类偏好研究中持续提升视频多样性。此外，我们公开了代码及一个包含 30,000 个多样化提示的新基准数据集，以支持未来研究。",
        "translated_title": "基于行列式点过程引导的策略优化的多样化视频生成",
        "label": [],
        "label_reason": "生成视频属 high-level 任务，非像素级图像恢复",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "提出 DPP-GRPO 新框架提升多样性，非图像处理创新"
    },
    {
        "title": "LocateAnything3D: Vision-Language 3D Detection with Chain-of-Sight",
        "url": "http://arxiv.org/abs/2511.20648v1",
        "pub_date": "2025-11-25",
        "summary": "To act in the world, a model must name what it sees and know where it is in 3D. Today's vision-language models (VLMs) excel at open-ended 2D description and grounding, yet multi-object 3D detection remains largely missing from the VLM toolbox. We present LocateAnything3D, a VLM-native recipe that casts 3D detection as a next-token prediction problem. The key is a short, explicit Chain-of-Sight (CoS) sequence that mirrors how human reason from images: find an object in 2D, then infer its distance, size, and pose. The decoder first emits 2D detections as a visual chain-of-thought, then predicts 3D boxes under an easy-to-hard curriculum: across objects, a near-to-far order reduces early ambiguity and matches ego-centric utility; within each object, a center-from-camera, dimensions, and rotation factorization ranks information by stability and learnability. This VLM-native interface preserves open-vocabulary and visual-prompting capability without specialized heads. On the challenging Omni3D benchmark, our model achieves state-of-the-art results, with 49.89 AP_3D, surpassing the previous best by +15.51 absolute improvement even when the baseline is given ground-truth 2D boxes. It also generalizes zero-shot to held-out categories with strong robustness. By turning 3D detection into a disciplined next-token problem, LocateAnything3D offers a practical foundation for models to perceive in 3D.",
        "translated": "为在真实世界中行动，模型必须识别其所见之物，并知晓自身在三维空间中的位置。当前的视觉-语言模型（VLMs）在开放式二维描述与定位任务上表现优异，但多目标三维检测仍基本缺失于其工具箱中。我们提出 LocateAnything3D，一种原生适配 VLM 的方法，将三维检测转化为下一个 token 的预测问题。其核心在于一个简短、明确的“视线链”（Chain-of-Sight, CoS）序列，该序列模拟人类从图像推理的过程：首先在二维空间中定位物体，再推断其距离、尺寸和姿态。解码器首先以视觉形式输出二维检测结果作为“思维链”，随后在由易至难的学习课程下预测三维框：跨物体时，按近至远顺序进行，可降低早期歧义并契合以自我为中心视角的实用需求；对单个物体而言，则通过“从相机中心出发的位置”、“尺寸”及“旋转”的因子化排序，依据信息的稳定性和可学习性进行组织。该方法保留了开放词汇和视觉提示能力，无需专用头结构。在具有挑战性的 Omni3D 数据集上，我们的模型取得了当前最优性能，达到 49.89 AP_3D，在基线模型已获真实二维框标注的情况下，绝对提升达 +15.51。同时，模型亦能零样本泛化至未见过的类别，并展现出强鲁棒性。通过将三维检测转化为有纪律规范的下一个 token 预测问题，LocateAnything3D 为模型实现三维感知提供了切实可行的基础。",
        "translated_title": "LocateAnything3D：基于视觉-语言的链式视域三维检测",
        "label": [],
        "label_reason": "属于3D目标检测，非图像像素级处理",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "提出CoS范式提升3D检测性能"
    },
    {
        "title": "3D-Aware Multi-Task Learning with Cross-View Correlations for Dense Scene Understanding",
        "url": "http://arxiv.org/abs/2511.20646v1",
        "pub_date": "2025-11-25",
        "summary": "This paper addresses the challenge of training a single network to jointly perform multiple dense prediction tasks, such as segmentation and depth estimation, i.e., multi-task learning (MTL). Current approaches mainly capture cross-task relations in the 2D image space, often leading to unstructured features lacking 3D-awareness. We argue that 3D-awareness is vital for modeling cross-task correlations essential for comprehensive scene understanding. We propose to address this problem by integrating correlations across views, i.e., cost volume, as geometric consistency in the MTL network. Specifically, we introduce a lightweight Cross-view Module (CvM), shared across tasks, to exchange information across views and capture cross-view correlations, integrated with a feature from MTL encoder for multi-task predictions. This module is architecture-agnostic and can be applied to both single and multi-view data. Extensive results on NYUv2 and PASCAL-Context demonstrate that our method effectively injects geometric consistency into existing MTL methods to improve performance.",
        "translated": "本文探讨了训练单一网络联合执行多个密集预测任务（如分割与深度估计）的挑战，即多任务学习（MTL）。当前方法主要在2D图像空间中捕捉跨任务关系，常导致缺乏3D感知能力的无结构特征。我们认为，3D感知对于建模实现全面场景理解所必需的跨任务相关性至关重要。为此，我们提出通过整合视图间相关性——即代价体（cost volume）——作为几何一致性引入到MTL网络中以解决该问题。具体而言，我们设计了一个轻量级的跨视图模块（Cross-view Module, CvM），该模块在各任务间共享，用于在不同视图间交换信息并捕捉跨视图相关性，并将其与MTL编码器输出的特征相结合以支持多任务预测。该模块架构无关，可应用于单视角和多视角数据。在NYUv2和PASCAL-Context上的大量实验表明，我们的方法能有效将几何一致性注入现有MTL方法中，从而提升性能。",
        "translated_title": "面向密集场景理解的3D感知多任务学习与跨视角关联",
        "label": [],
        "label_reason": "任务属高阶场景理解，非像素级图像恢复",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出跨视图模块提升多任务一致性，架构通用"
    },
    {
        "title": "PixelDiT: Pixel Diffusion Transformers for Image Generation",
        "url": "http://arxiv.org/abs/2511.20645v1",
        "pub_date": "2025-11-25",
        "summary": "Latent-space modeling has been the standard for Diffusion Transformers (DiTs). However, it relies on a two-stage pipeline where the pretrained autoencoder introduces lossy reconstruction, leading to error accumulation while hindering joint optimization. To address these issues, we propose PixelDiT, a single-stage, end-to-end model that eliminates the need for the autoencoder and learns the diffusion process directly in the pixel space. PixelDiT adopts a fully transformer-based architecture shaped by a dual-level design: a patch-level DiT that captures global semantics and a pixel-level DiT that refines texture details, enabling efficient training of a pixel-space diffusion model while preserving fine details. Our analysis reveals that effective pixel-level token modeling is essential to the success of pixel diffusion. PixelDiT achieves 1.61 FID on ImageNet 256x256, surpassing existing pixel generative models by a large margin. We further extend PixelDiT to text-to-image generation and pretrain it at the 1024x1024 resolution in pixel space. It achieves 0.74 on GenEval and 83.5 on DPG-bench, approaching the best latent diffusion models.",
        "translated": "潜在空间建模一直是扩散Transformer（DiTs）的标准方法。然而，该方法依赖于两阶段pipeline，其中预训练的自动编码器引入有损重建，导致误差累积并阻碍联合优化。为解决这些问题，我们提出PixelDiT，这是一种单阶段、端到端模型，无需自动编码器，直接在像素空间学习扩散过程。PixelDiT采用全Transformer架构，结合双层级设计：一个基于patch级别的DiT用于捕捉全局语义，另一个基于像素级别的DiT用于细化纹理细节，从而在保留精细细节的同时高效训练像素空间扩散模型。我们的分析表明，有效的像素级token建模对于像素扩散的成功至关重要。PixelDiT在ImageNet 256x256上实现了1.61的FID得分，显著超越现有像素生成模型。我们进一步将PixelDiT扩展至文本到图像生成任务，并在像素空间中以1024x1024分辨率进行预训练，其在GenEval上达到0.74，在DPG-bench上达到83.5，接近当前最佳的潜在扩散模型性能。",
        "translated_title": "PixelDiT：用于图像生成的像素扩散Transformer",
        "label": [],
        "label_reason": "生成新图像，非像素级恢复任务",
        "relevance_score": 1,
        "novelty_score": 9,
        "novelty_reason": "首次端到端像素空间扩散模型架构创新"
    },
    {
        "title": "Vision-Language Memory for Spatial Reasoning",
        "url": "http://arxiv.org/abs/2511.20644v1",
        "pub_date": "2025-11-25",
        "summary": "Spatial reasoning is a critical capability for intelligent robots, yet current vision-language models (VLMs) still fall short of human-level performance in video-based spatial reasoning. This gap mainly stems from two challenges: a semantic-geometric misalignment that prevents consistent 3D understanding, and the absence of persistent memory to retain 3D representation and understanding over time. To address these limitations, we present VLM$^2$, a Vision-Language Model with persistent Memory for spatial reasoning with a view-consistent, 3D-aware representation purely from 2D video. Specifically, to enhance long-horizon reasoning, we incorporate a dual-memory module, consisting of a working memory that operates as a sliding window to focus on immediate context, and an episodic memory that consolidates and stores critical long-term information. This design enables efficient and long-horizon spatial reasoning with a fixed computational cost. Extensive experiments on multiple benchmarks show that VLM$^2$ achieves state-of-the-art performance among video-only models, significantly advancing the frontier of visual-spatial intelligence.",
        "translated": "空间推理是智能机器人的一项关键能力，但当前的视觉-语言模型（VLMs）在基于视频的空间推理任务中仍远未达到人类水平。这一差距主要源于两个挑战：语义与几何信息的错位，导致难以建立一致的3D理解；以及缺乏持久记忆机制，无法在时间维度上保留和利用3D表示与认知。为解决上述局限性，我们提出了 VLM$^2$，一种面向空间推理、具备持久记忆机制的视觉-语言模型，其通过纯2D视频输入即可获得具有一致视角、3D感知的表征。具体而言，为增强长程推理能力，我们引入了双记忆模块：工作记忆作为滑动窗口，聚焦于即时上下文；而情节记忆则负责整合并存储关键的长期信息。该设计使得系统能够在固定计算开销下实现高效且支持长程的空间推理。在多个基准测试中的大量实验表明，VLM$^2$ 在仅使用视频数据的模型中取得了最先进性能，显著推动了视觉-空间智能的发展前沿。",
        "translated_title": "用于空间推理的视觉-语言记忆",
        "label": [],
        "label_reason": "任务属高阶视觉理解，非像素级图像处理",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "引入双记忆模块提升长程空间推理能力"
    },
    {
        "title": "Concept-Aware Batch Sampling Improves Language-Image Pretraining",
        "url": "http://arxiv.org/abs/2511.20643v1",
        "pub_date": "2025-11-25",
        "summary": "What data should a vision-language model be trained on? To answer this question, many data curation efforts center on the quality of a dataset. However, most of these existing methods are (i) offline, i.e. they produce a static dataset from a set of predetermined filtering criteria, and (ii) concept-agnostic, i.e. they use model-based filters which induce additional data biases. In this work, we go beyond such offline, concept-agnostic methods and advocate for more flexible, task-adaptive online concept-based curation. Our first contribution is DataConcept, a collection of 128M web-crawled image-text pairs annotated with fine-grained details about their concept composition. Building on DataConcept, we introduce Concept-Aware Batch Sampling (CABS), a simple yet effective batch sampling framework that flexibly constructs batches on-the-fly based on specific target distributions. We propose two variants: (i) Diversity Maximization (CABS-DM) to curate batches with a broad coverage of available concepts, and (ii) Frequency Maximization (CABS-FM) to curate batches with high object multiplicity. Through extensive evaluations across 28 benchmarks, we demonstrate that our CABS method significantly benefits CLIP/SigLIP model classes and yields highly performant models. Overall, CABS represents a strong open-source alternative to proprietary online data curation algorithms, enabling practitioners to define custom concept distributions that optimize for specific downstream tasks.",
        "translated": "视觉-语言模型应训练在何种数据上？为回答这一问题，许多数据整理工作聚焦于数据集的质量。然而，现有大多数方法存在以下两个主要缺陷：(i) 离线式（offline），即基于预定的过滤标准生成静态数据集；(ii) 概念无感知（concept-agnostic），即使用基于模型的过滤器，从而引入额外的数据偏差。在本工作中，我们超越了这类离线且概念无感知的方法，倡导更灵活、任务自适应的在线概念驱动型数据整理策略。我们的第一项贡献是 DataConcept，一个包含 1.28 亿个网络爬取图像-文本对的数据集，每个样本均标注了其概念构成的细粒度细节。在此基础上，我们提出了概念感知批采样框架（Concept-Aware Batch Sampling, CABS），这是一种简单而有效的批处理构建方案，可根据特定目标分布动态地实时构造批次。我们提出两种变体：(i) 多样性最大化（CABS-DM）用于构建涵盖广泛可用概念的批次；(ii) 频率最大化（CABS-FM）用于构建高物体重复率的批次。通过在 28 个基准测试上的广泛评估，我们证明了 CABS 方法显著提升了 CLIP/SigLIP 模型类别的性能，并生成了高度表现优异的模型。总体而言，CABS 提供了一个强有力的开源替代方案，可取代专有在线数据整理算法，使从业者能够定义自定义的概念分布，以针对特定下游任务进行优化。",
        "translated_title": "概念感知的批量采样提升语言-图像预训练",
        "label": [],
        "label_reason": "任务为数据采样优化，非图像像素级处理",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出概念感知采样框架，提升模型性能"
    },
    {
        "title": "Unleashing the Power of Vision-Language Models for Long-Tailed Multi-Label Visual Recognition",
        "url": "http://arxiv.org/abs/2511.20641v1",
        "pub_date": "2025-11-25",
        "summary": "Long-tailed multi-label visual recognition poses a significant challenge, as images typically contain multiple labels with highly imbalanced class distributions, leading to biased models that favor head classes while underperforming on tail classes. Recent efforts have leveraged pre-trained vision-language models, such as CLIP, alongside long-tailed learning techniques to exploit rich visual-textual priors for improved performance. However, existing methods often derive semantic inter-class relationships directly from imbalanced datasets, resulting in unreliable correlations for tail classes due to data scarcity. Moreover, CLIP's zero-shot paradigm is optimized for single-label image-text matching, making it suboptimal for multi-label tasks. To address these issues, we propose the correlation adaptation prompt network (CAPNET), a novel end-to-end framework that explicitly models label correlations from CLIP's textual encoder. The framework incorporates a graph convolutional network for label-aware propagation and learnable soft prompts for refined embeddings. It utilizes a distribution-balanced Focal loss with class-aware re-weighting for optimized training under imbalance. Moreover, it improves generalization through test-time ensembling and realigns visual-textual modalities using parameter-efficient fine-tuning to avert overfitting on tail classes without compromising head class performance. Extensive experiments and ablation studies on benchmarks including VOC-LT, COCO-LT, and NUS-WIDE demonstrate that CAPNET achieves substantial improvements over state-of-the-art methods, validating its effectiveness for real-world long-tailed multi-label visual recognition.",
        "translated": "长尾多标签视觉识别面临重大挑战，因为图像通常包含多个标签，且类别分布高度不平衡，导致模型倾向于偏向头部类别，而在尾部类别的表现不佳。近期研究利用预训练的视觉-语言模型（如 CLIP）结合长尾学习技术，以挖掘丰富的视觉-文本先验知识以提升性能。然而，现有方法常直接从不平衡数据集中推导语义类间关系，由于尾部类数据稀缺，导致其相关性不可靠。此外，CLIP 的零样本范式针对单标签图像-文本匹配进行优化，难以适用于多标签任务。为解决上述问题，我们提出关联自适应提示网络（CAPNET），一种新颖的端到端框架，显式地从 CLIP 的文本编码器中建模标签间的关联关系。该框架结合图卷积网络实现标签感知传播，并引入可学习的软提示以优化嵌入表示。它采用分布均衡的焦点损失配合类别感知重加权机制，在不平衡条件下实现优化训练。此外，通过测试时集成策略提升泛化能力，并借助参数高效微调对齐视觉与文本模态，从而避免在尾部类别上过拟合，同时不损害头部类别的性能。在 VOC-LT、COCO-LT 和 NUS-WIDE 等基准数据集上的大量实验及消融研究表明，CAPNET 相较于当前最先进方法取得显著性能提升，验证了其在现实场景中应对长尾多标签视觉识别任务的有效性。",
        "translated_title": "释放视觉-语言模型在长尾多标签视觉识别中的潜力",
        "label": [],
        "label_reason": "任务为多标签分类，属高阶视觉识别",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "改进CLIP与长尾学习框架，非像素级图像处理"
    },
    {
        "title": "MotionV2V: Editing Motion in a Video",
        "url": "http://arxiv.org/abs/2511.20640v1",
        "pub_date": "2025-11-25",
        "summary": "While generative video models have achieved remarkable fidelity and consistency, applying these capabilities to video editing remains a complex challenge. Recent research has explored motion controllability as a means to enhance text-to-video generation or image animation; however, we identify precise motion control as a promising yet under-explored paradigm for editing existing videos. In this work, we propose modifying video motion by directly editing sparse trajectories extracted from the input. We term the deviation between input and output trajectories a \"motion edit\" and demonstrate that this representation, when coupled with a generative backbone, enables powerful video editing capabilities. To achieve this, we introduce a pipeline for generating \"motion counterfactuals\", video pairs that share identical content but distinct motion, and we fine-tune a motion-conditioned video diffusion architecture on this dataset. Our approach allows for edits that start at any timestamp and propagate naturally. In a four-way head-to-head user study, our model achieves over 65 percent preference against prior work. Please see our project page: https://ryanndagreat.github.io/MotionV2V",
        "translated": "尽管生成式视频模型已在保真度与一致性方面取得了显著成果，但将这些能力应用于视频编辑仍是一项复杂挑战。近期研究已探索通过运动可控性增强文本到视频生成或图像动画的效果；然而，我们认为精确的运动控制是编辑现有视频的一种极具前景却尚未充分挖掘的范式。在本工作中，我们提出通过直接编辑从输入视频中提取的稀疏轨迹来修改视频运动。我们将输入与输出轨迹之间的偏差称为“运动编辑”，并证明当该表示与生成式主干网络结合时，可实现强大的视频编辑能力。为实现此目标，我们引入了一种生成“运动反事实”（motion counterfactuals）的流程——即内容相同但运动不同的视频对，并在此数据集上对基于运动条件的视频扩散架构进行微调。我们的方法允许从任意时间戳开始编辑，并自然传播至后续帧。在一项四组头对头用户评测中，我们的模型相较此前工作获得超过65%的偏好率。请访问我们的项目页面：https://ryanndagreat.github.io/MotionV2V",
        "translated_title": "MotionV2V：视频中运动的编辑",
        "label": [],
        "label_reason": "视频编辑属高阶视觉任务，非像素级图像恢复",
        "relevance_score": 2,
        "novelty_score": 8,
        "novelty_reason": "提出运动轨迹编辑新范式，显著提升编辑能力"
    },
    {
        "title": "iMontage: Unified, Versatile, Highly Dynamic Many-to-many Image Generation",
        "url": "http://arxiv.org/abs/2511.20635v1",
        "pub_date": "2025-11-25",
        "summary": "Pre-trained video models learn powerful priors for generating high-quality, temporally coherent content. While these models excel at temporal coherence, their dynamics are often constrained by the continuous nature of their training data. We hypothesize that by injecting the rich and unconstrained content diversity from image data into this coherent temporal framework, we can generate image sets that feature both natural transitions and a far more expansive dynamic range. To this end, we introduce iMontage, a unified framework designed to repurpose a powerful video model into an all-in-one image generator. The framework consumes and produces variable-length image sets, unifying a wide array of image generation and editing tasks. To achieve this, we propose an elegant and minimally invasive adaptation strategy, complemented by a tailored data curation process and training paradigm. This approach allows the model to acquire broad image manipulation capabilities without corrupting its invaluable original motion priors. iMontage excels across several mainstream many-in-many-out tasks, not only maintaining strong cross-image contextual consistency but also generating scenes with extraordinary dynamics that surpass conventional scopes. Find our homepage at: https://kr1sjfu.github.io/iMontage-web/.",
        "translated": "预训练的视频模型能够学习到生成高质量且具有时间一致性的内容的强大先验知识。尽管这些模型在时间一致性方面表现卓越，其动态特性往往受限于训练数据本身的连续性。我们假设，通过将图像数据中丰富且不受约束的内容多样性注入这一连贯的时间框架中，可以生成既具备自然过渡又拥有更广阔动态范围的图像集合。为此，我们提出了iMontage，一个统一的框架，旨在将强大的视频模型重新转化为多功能图像生成器。该框架可接收并输出可变长度的图像集，从而统一多种图像生成与编辑任务。为实现这一目标，我们提出了一种优雅且影响最小的适配策略，并辅以量身定制的数据整理流程和训练范式。这种方法使模型能够在不破坏其原始运动先验价值的前提下，获得广泛的图像操作能力。iMontage在多个主流的多输入-多输出任务中表现出色，不仅保持了跨图像上下文的一致性，还能生成超出传统范畴的富有非凡动态效果的场景。请访问我们的主页：https://kr1sjfu.github.io/iMontage-web/。",
        "translated_title": "iMontage：统一、多功能、高度动态的多对多图像生成",
        "label": [],
        "label_reason": "生成图像集合，非像素级恢复或增强",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "新框架统一多图像生成，创新训练策略"
    },
    {
        "title": "MapReduce LoRA: Advancing the Pareto Front in Multi-Preference Optimization for Generative Models",
        "url": "http://arxiv.org/abs/2511.20629v1",
        "pub_date": "2025-11-25",
        "summary": "Reinforcement learning from human feedback (RLHF) with reward models has advanced alignment of generative models to human aesthetic and perceptual preferences. However, jointly optimizing multiple rewards often incurs an alignment tax, improving one dimension while degrading others. To address this, we introduce two complementary methods: MapReduce LoRA and Reward-aware Token Embedding (RaTE). MapReduce LoRA trains preference-specific LoRA experts in parallel and iteratively merges them to refine a shared base model; RaTE learns reward-specific token embeddings that compose at inference for flexible preference control. Experiments on Text-to-Image generation (Stable Diffusion 3.5 Medium and FLUX.1-dev) show improvements of 36.1%, 4.6%, and 55.7%, and 32.7%, 4.3%, and 67.1% on GenEval, PickScore, and OCR, respectively. On Text-to-Video generation (HunyuanVideo), visual and motion quality improve by 48.1% and 90.0%, respectively. On the language task, Helpful Assistant, with Llama-2 7B, helpful and harmless improve by 43.4% and 136.7%, respectively. Our framework sets a new state-of-the-art multi-preference alignment recipe across modalities.",
        "translated": "基于人类反馈的强化学习（RLHF）结合奖励模型，已推动生成式模型与人类审美与感知偏好对齐。然而，联合优化多个奖励常导致“对齐开销”，即在提升某一维度的同时损害其他维度。为解决该问题，我们提出两种互补方法：MapReduce LoRA 与 奖励感知词嵌入（RaTE）。MapReduce LoRA 并行训练针对不同偏好的 LoRA 专家，并通过迭代融合精炼共享基础模型；RaTE 学习针对不同奖励的词嵌入，在推理阶段组合以实现灵活的偏好控制。在文本到图像生成任务（Stable Diffusion 3.5 Medium 和 FLUX.1-dev）上的实验显示，GenEval、PickScore 和 OCR 三项指标分别提升 36.1%、4.6%、55.7%，以及 32.7%、4.3%、67.1%。在文本到视频生成任务（HunyuanVideo）上，视觉质量与运动质量分别提升 48.1% 与 90.0%。在语言任务 Helpful Assistant 上，使用 Llama-2 7B 模型，有益性与无害性分别提升 43.4% 与 136.7%。我们的框架为跨模态多偏好对齐树立了新的最先进方案。",
        "translated_title": "MapReduce LoRA：推进生成模型多偏好优化的帕累托前沿",
        "label": [],
        "label_reason": "属高阶生成模型偏好优化，非像素级图像处理",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "提出MapReduce LoRA与RaTE新架构提升多偏好对齐"
    },
    {
        "title": "ShapeGen: Towards High-Quality 3D Shape Synthesis",
        "url": "http://arxiv.org/abs/2511.20624v1",
        "pub_date": "2025-11-25",
        "summary": "Inspired by generative paradigms in image and video, 3D shape generation has made notable progress, enabling the rapid synthesis of high-fidelity 3D assets from a single image. However, current methods still face challenges, including the lack of intricate details, overly smoothed surfaces, and fragmented thin-shell structures. These limitations leave the generated 3D assets still one step short of meeting the standards favored by artists. In this paper, we present ShapeGen, which achieves high-quality image-to-3D shape generation through 3D representation and supervision improvements, resolution scaling up, and the advantages of linear transformers. These advancements allow the generated assets to be seamlessly integrated into 3D pipelines, facilitating their widespread adoption across various applications. Through extensive experiments, we validate the impact of these improvements on overall performance. Ultimately, thanks to the synergistic effects of these enhancements, ShapeGen achieves a significant leap in image-to-3D generation, establishing a new state-of-the-art performance.",
        "translated": "受图像与视频生成范式的启发，3D形状生成已取得显著进展，能够从单张图像快速合成高保真度的3D资产。然而，当前方法仍面临若干挑战，包括细节不够丰富、表面过度平滑以及薄壳结构碎片化等问题。这些局限性使得生成的3D资产尚未完全满足艺术家所青睐的标准。在本文中，我们提出了ShapeGen，通过改进3D表示与监督机制、提升分辨率、并利用线性Transformer的优势，实现了高质量的图像到3D形状生成。这些改进使生成的资产可无缝融入3D工作流程，并推动其在各类应用场景中的广泛应用。通过大量实验，我们验证了上述改进对整体性能的积极影响。最终，得益于各项增强措施的协同效应，ShapeGen在图像到3D生成任务上实现了显著突破，确立了新的最先进性能水平。",
        "translated_title": "ShapeGen：迈向高质量3D形状生成",
        "label": [],
        "label_reason": "生成3D模型属高阶任务，非像素级图像处理",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "改进架构提升质量，但非低层图像恢复创新"
    },
    {
        "title": "Wanderland: Geometrically Grounded Simulation for Open-World Embodied AI",
        "url": "http://arxiv.org/abs/2511.20620v1",
        "pub_date": "2025-11-25",
        "summary": "Reproducible closed-loop evaluation remains a major bottleneck in Embodied AI such as visual navigation. A promising path forward is high-fidelity simulation that combines photorealistic sensor rendering with geometrically grounded interaction in complex, open-world urban environments. Although recent video-3DGS methods ease open-world scene capturing, they are still unsuitable for benchmarking due to large visual and geometric sim-to-real gaps. To address these challenges, we introduce Wanderland, a real-to-sim framework that features multi-sensor capture, reliable reconstruction, accurate geometry, and robust view synthesis. Using this pipeline, we curate a diverse dataset of indoor-outdoor urban scenes and systematically demonstrate how image-only pipelines scale poorly, how geometry quality impacts novel view synthesis, and how all of these adversely affect navigation policy learning and evaluation reliability. Beyond serving as a trusted testbed for embodied navigation, Wanderland's rich raw sensor data further allows benchmarking of 3D reconstruction and novel view synthesis models. Our work establishes a new foundation for reproducible research in open-world embodied AI. Project website is at https://ai4ce.github.io/wanderland/.",
        "translated": "在具身人工智能（如视觉导航）领域，可复现的闭环评估仍是一个主要瓶颈。一个有前景的发展方向是高保真仿真，即将逼真的传感器渲染与在复杂开放世界城市环境中基于几何的交互相结合。尽管近期的视频-3DGS方法简化了开放世界场景的采集，但它们仍因存在显著的视觉与几何仿真到真实世界的差距而不适合用于基准评测。为应对这些挑战，我们提出了Wanderland，一个从真实世界到仿真的框架，具备多传感器采集、可靠重建、精确几何和鲁棒视图合成能力。通过该流程，我们构建了一个涵盖室内外城市场景的多样化数据集，并系统性地展示了仅依赖图像的管线如何难以扩展、几何质量如何影响新视图合成，以及这些因素如何对导航策略的学习与评估可靠性产生负面影响。除了作为具身导航的可信测试平台外，Wanderland丰富的原始传感器数据还支持对3D重建与新视图合成模型的基准评测。我们的工作为开放世界具身AI中的可复现研究奠定了新的基础。项目网站见 https://ai4ce.github.io/wanderland/。",
        "translated_title": "Wanderland：面向开放世界具身人工智能的几何基础仿真",
        "label": [],
        "label_reason": "研究聚焦于仿真与导航，非图像像素级处理",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "提出新仿真框架，提升开放世界AI可复现性"
    },
    {
        "title": "Evaluating the Performance of Deep Learning Models in Whole-body Dynamic 3D Posture Prediction During Load-reaching Activities",
        "url": "http://arxiv.org/abs/2511.20615v1",
        "pub_date": "2025-11-25",
        "summary": "This study aimed to explore the application of deep neural networks for whole-body human posture prediction during dynamic load-reaching activities. Two time-series models were trained using bidirectional long short-term memory (BLSTM) and transformer architectures. The dataset consisted of 3D full-body plug-in gait dynamic coordinates from 20 normal-weight healthy male individuals each performing 204 load-reaching tasks from different load positions while adapting various lifting and handling techniques. The model inputs consisted of the 3D position of the hand-load position, lifting (stoop, full-squat and semi-squat) and handling (one- and two-handed) techniques, body weight and height, and the 3D coordinate data of the body posture from the first 25% of the task duration. These inputs were used by the models to predict body coordinates during the remaining 75% of the task period. Moreover, a novel method was proposed to improve the accuracy of the previous and present posture prediction networks by enforcing constant body segment lengths through the optimization of a new cost function. The results indicated that the new cost function decreased the prediction error of the models by approximately 8% and 21% for the arm and leg models, respectively. We indicated that utilizing the transformer architecture, with a root-mean-square-error of 47.0 mm, exhibited ~58% more accurate long-term performance than the BLSTM-based model. This study merits the use of neural networks that capture time series dependencies in 3D motion frames, providing a unique approach for understanding and predict motion dynamics during manual material handling activities.",
        "translated": "本研究旨在探索深度神经网络在动态取物活动中全身人体姿态预测中的应用。采用双向长短时记忆网络（BLSTM）和Transformer架构分别训练了两种时序模型。数据集包含20名正常体重健康男性个体在执行204种不同负载位置的取物任务时采集的三维全身插值步态动态坐标，每位受试者均采用多种举升与操作技术完成任务。模型输入包括手-负载位置的三维坐标、举升方式（弯腰、全蹲、半蹲）及操作方式（单手、双手）、身体重量与身高，以及任务前25%时间段内的人体姿态三维坐标数据。这些输入被用于预测剩余75%任务时段内的身体坐标。此外，提出了一种新颖方法，通过优化新的代价函数来强制保持身体各段长度恒定，从而提升前后姿态预测网络的精度。结果表明，新代价函数使手臂模型和腿部模型的预测误差分别降低了约8%和21%。研究还显示，基于Transformer架构的模型（均方根误差为47.0 mm）相比BLSTM模型，在长时序性能上准确率高出约58%。本研究表明，利用能够捕捉三维运动帧中时序依赖关系的神经网络，可为理解并预测手动物料搬运活动中的运动动力学提供独特方法。",
        "translated_title": "评估深度学习模型在负重取物活动中全身动态三维姿态预测中的性能",
        "label": [],
        "label_reason": "任务为动作预测，非图像像素级恢复",
        "relevance_score": 1,
        "novelty_score": 3,
        "novelty_reason": "改进成本函数，但属运动建模非图像处理"
    },
    {
        "title": "The Consistency Critic: Correcting Inconsistencies in Generated Images via Reference-Guided Attentive Alignment",
        "url": "http://arxiv.org/abs/2511.20614v1",
        "pub_date": "2025-11-25",
        "summary": "Previous works have explored various customized generation tasks given a reference image, but they still face limitations in generating consistent fine-grained details. In this paper, our aim is to solve the inconsistency problem of generated images by applying a reference-guided post-editing approach and present our ImageCritic. We first construct a dataset of reference-degraded-target triplets obtained via VLM-based selection and explicit degradation, which effectively simulates the common inaccuracies or inconsistencies observed in existing generation models. Furthermore, building on a thorough examination of the model's attention mechanisms and intrinsic representations, we accordingly devise an attention alignment loss and a detail encoder to precisely rectify inconsistencies. ImageCritic can be integrated into an agent framework to automatically detect inconsistencies and correct them with multi-round and local editing in complex scenarios. Extensive experiments demonstrate that ImageCritic can effectively resolve detail-related issues in various customized generation scenarios, providing significant improvements over existing methods.",
        "translated": "先前的工作已探索了在给定参考图像条件下执行各种定制化生成任务的方法，但它们在生成一致的细粒度细节方面仍存在局限。本文旨在通过引入基于参考引导的后编辑方法来解决生成图像的一致性问题，并提出我们的 ImageCritic 框架。我们首先构建了一个由 VLM 基于选择与显式退化生成的参考-退化-目标三元组数据集，该数据集能有效模拟现有生成模型中常见的不准确或不一致性现象。此外，基于对模型注意力机制及内在表征的深入分析，我们相应设计了一种注意力对齐损失函数和一个细节编码器，以精确修正生成中的不一致性。ImageCritic 可集成至代理框架中，在复杂场景下自动检测不一致并借助多轮局部编辑进行修正。大量实验表明，ImageCritic 能够有效解决各类定制化生成任务中的细节相关问题，并显著优于现有方法。",
        "translated_title": "一致性批评者：通过参考引导的注意力对齐修正生成图像中的不一致性",
        "label": [],
        "label_reason": "处理生成图像一致性，非像素级图像恢复任务",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出注意力对齐损失修正细节不一致，提升生成质量"
    },
    {
        "title": "Optimization of Sums of Bivariate Functions: An Introduction to Relaxation-Based Methods for the Case of Finite Domains",
        "url": "http://arxiv.org/abs/2511.20607v1",
        "pub_date": "2025-11-25",
        "summary": "We study the optimization of functions with $n&gt;2$ arguments that have a representation as a sum of several functions that have only $2$ of the $n$ arguments each, termed sums of bivariates, on finite domains. The complexity of optimizing sums of bivariates is shown to be NP-equivalent and it is shown that there exists free lunch in the optimization of sums of bivariates. Based on measure-valued extensions of the objective function, so-called relaxations, $\\ell^2$-approximation, and entropy-regularization, we derive several tractable problem formulations solvable with linear programming, coordinate ascent as well as with closed-form solutions. The limits of applying tractable versions of such relaxations to sums of bivariates are investigated using general results for reconstructing measures from their bivariate marginals. Experiments in which the derived algorithms are applied to random functions, vertex coloring, and signal reconstruction problems provide insights into qualitatively different function classes that can be modeled as sums of bivariates.",
        "translated": "我们研究具有 $n>2$ 个自变量的函数的优化问题，这些函数可表示为若干仅依赖于其中两个自变量的函数之和，称为双变量之和，在有限域上进行分析。研究表明，双变量之和函数的优化复杂度属于 NP 等价类，并且在该类函数的优化中存在“免费午餐”现象。基于目标函数的测度值扩展（称为松弛方法）、$\\ell^2$ 近似以及熵正则化，我们推导出若干可通过线性规划、坐标上升法或闭式解求解的可行问题形式。通过利用重构测度从其二元边缘分布的一般理论，我们研究了此类松弛方法的可行版本在双变量之和上的应用极限。实验将所推导的算法应用于随机函数、顶点着色及信号重建问题，揭示了可建模为双变量之和的不同函数类别的定性差异。",
        "translated_title": "双变量函数和的优化：有限域情形下基于松弛方法的入门",
        "label": [],
        "label_reason": "非图像处理任务，属数学优化理论",
        "relevance_score": 1,
        "novelty_score": 3,
        "novelty_reason": "方法通用，无图像恢复创新"
    },
    {
        "title": "Latent Diffusion Inversion Requires Understanding the Latent Space",
        "url": "http://arxiv.org/abs/2511.20592v1",
        "pub_date": "2025-11-25",
        "summary": "The recovery of training data from generative models (``model inversion'') has been extensively studied for diffusion models in the data domain. The encoder/decoder pair and corresponding latent codes have largely been ignored by inversion techniques applied to latent space generative models, e.g., Latent Diffusion models (LDMs). In this work we describe two key findings: (1) The diffusion model exhibits non-uniform memorization across latent codes, tending to overfit samples located in high-distortion regions of the decoder pullback metric. (2) Even within a single latent code, different dimensions contribute unequally to memorization. We introduce a principled method to rank latent dimensions by their per-dimensional contribution to the decoder pullback metric, identifying those most responsible for memorization. Empirically, removing less-memorizing dimensions when computing attack statistics for score-based membership inference attacker significantly improves performance, with average AUROC gains of 2.7\\% and substantial increases in TPR@1\\%FPR (6.42\\%) across diverse datasets including CIFAR-10, CelebA, ImageNet-1K, Pokémon, MS-COCO, and Flickr. This indicates stronger confidence in identifying members under extremely low false-positive tolerance. Our results highlight the overlooked influence of the auto-encoder geometry on LDM memorization and provide a new perspective for analyzing privacy risks in diffusion-based generative models.",
        "translated": "扩散模型在数据域中针对生成模型的训练数据恢复（“模型反演”）已得到广泛研究。然而，应用于潜在空间生成模型（如潜在扩散模型 LDMs）的反演技术，大多忽略了编码器/解码器对及其对应的潜在码。本文提出两项关键发现：（1）扩散模型在潜在码上呈现出非均匀的记忆行为，倾向于过拟合位于解码器拉回度量高失真区域的样本。（2）即使在单个潜在码内，各维度对记忆贡献亦不均衡。我们提出一种基于维度贡献度排序的原理性方法，以量化每个维度对解码器拉回度量的贡献，并识别出最负责记忆存储的维度。实证表明，当为基于分数的成员推断攻击者计算攻击统计时，移除贡献较小的维度可显著提升性能，在包括 CIFAR-10、CelebA、ImageNet-1K、Pokémon、MS-COCO 和 Flickr 在内的多个数据集上，平均 AUROC 提升 2.7%，TPR@1%FPR 增幅达 6.42%。这表明在极低假阳性容忍度下，系统更可靠地识别成员。我们的结果揭示了自编码器几何结构对 LDM 记忆行为被长期忽视的影响，并为分析基于扩散模型的生成式隐私风险提供了全新视角。",
        "translated_title": "潜在扩散逆向需要理解潜在空间",
        "label": [],
        "label_reason": "研究模型逆向与隐私风险，非图像像素级恢复任务",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出维度排序方法提升攻击性能，属理论创新"
    },
    {
        "title": "VQ-VA World: Towards High-Quality Visual Question-Visual Answering",
        "url": "http://arxiv.org/abs/2511.20573v1",
        "pub_date": "2025-11-25",
        "summary": "This paper studies Visual Question-Visual Answering (VQ-VA): generating an image, rather than text, in response to a visual question -- an ability that has recently emerged in proprietary systems such as NanoBanana and GPT-Image. To also bring this capability to open-source models, we introduce VQ-VA World, a data-centric framework built around an agentic pipeline for large-scale, targeted data construction. Leveraging web-scale deployment, this pipeline crawls a massive amount of ~1.8M high-quality, interleaved image-text samples for model training. For evaluation, we further release IntelligentBench, a human-curated benchmark that systematically assesses VQ-VA along the aspects of world knowledge, design knowledge, and reasoning. Training with VQ-VA World data yields strong empirical gains: it helps LightFusion attain 53.06 on IntelligentBench, substantially surpassing the best prior open-source baselines (i.e., 7.78 from vanilla LightFusion; 1.94 from UniWorld-V1), and significantly narrowing the gap toward leading proprietary systems (e.g., 81.67 from NanoBanana; 82.64 from GPT-Image). By releasing the full suite of model weights, datasets, and pipelines, we hope to stimulate future research on VQ-VA.",
        "translated": "本文研究视觉问答-视觉回答（VQ-VA）：针对视觉问题生成图像而非文本——这一能力近期已在NanoBanana和GPT-Image等专有系统中出现。为将此能力扩展至开源模型，我们提出了VQ-VA World，一个以代理流水线为核心、面向大规模定向数据构建的数据驱动型框架。该流水线依托网络级部署，爬取了约180万条高质量的图像-文本交错样本，用于模型训练。在评估方面，我们进一步发布了IntelligentBench，一个由人类精心编纂的基准测试集，系统性地从世界知识、设计知识与推理能力三个维度评估VQ-VA性能。使用VQ-VA World数据进行训练可获得显著的实证收益：它使LightFusion在IntelligentBench上达到53.06分，大幅超越此前最佳开源基线（即原始LightFusion的7.78分；UniWorld-V1的1.94分），并显著缩小与领先专有系统之间的差距（例如NanoBanana的81.67分；GPT-Image的82.64分）。通过发布完整的模型权重、数据集及流水线，我们期望激发未来对VQ-VA领域的研究。",
        "translated_title": "VQ-VA World：迈向高质量视觉问答-视觉回答",
        "label": [],
        "label_reason": "属于高阶视觉问答任务，非像素级图像处理",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出新数据框架提升开放模型VQ-VA性能"
    },
    {
        "title": "Deep Learning-Based Multiclass Classification of Oral Lesions with Stratified Augmentation",
        "url": "http://arxiv.org/abs/2511.21582v1",
        "pub_date": "2025-11-26",
        "summary": "Oral cancer is highly common across the globe and is mostly diagnosed during the later stages due to the close visual similarity to benign, precancerous, and malignant lesions in the oral cavity. Implementing computer aided diagnosis systems early on has the potential to greatly improve clinical outcomes. This research intends to use deep learning to build a multiclass classifier for sixteen different oral lesions. To overcome the challenges of limited and imbalanced datasets, the proposed technique combines stratified data splitting and advanced data augmentation and oversampling to perform the classification. The experimental results, which achieved 83.33 percent accuracy, 89.12 percent precision, and 77.31 percent recall, demonstrate the superiority of the suggested model over state of the art methods now in use. The suggested model effectively conveys the effectiveness of oversampling and augmentation strategies in situations where the minority class classification performance is noteworthy. As a first step toward trustworthy computer aided diagnostic systems for the early detection of oral cancer in clinical settings, the suggested framework shows promise.",
        "translated": "口腔癌在全球范围内发病率极高，但由于其在口腔内与良性、癌前及恶性病变在视觉上高度相似，多数病例往往在晚期才被诊断。早期引入计算机辅助诊断系统有望显著改善临床疗效。本研究旨在利用深度学习构建一个针对十六种不同口腔病变的多类别分类器。为应对数据集有限且分布不均衡的挑战，所提出的方法结合了分层数据划分、高级数据增强及过采样技术以实现有效分类。实验结果表明，该模型达到了83.33%的准确率、89.12%的精确率和77.31%的召回率，优于当前主流方法。该模型充分验证了过采样与增强策略在少数类别分类性能突出时的有效性。作为迈向临床环境中可靠口腔癌早期检测的计算机辅助诊断系统的初步步骤，本框架展现出广阔前景。",
        "translated_title": "基于深度学习的口腔病变多类别分类与分层增强",
        "label": [],
        "label_reason": "任务为高阶分类，非图像像素级恢复",
        "relevance_score": 1,
        "novelty_score": 2,
        "novelty_reason": "方法常规，无低层图像处理创新"
    },
    {
        "title": "Harmony: Harmonizing Audio and Video Generation through Cross-Task Synergy",
        "url": "http://arxiv.org/abs/2511.21579v1",
        "pub_date": "2025-11-26",
        "summary": "The synthesis of synchronized audio-visual content is a key challenge in generative AI, with open-source models facing challenges in robust audio-video alignment. Our analysis reveals that this issue is rooted in three fundamental challenges of the joint diffusion process: (1) Correspondence Drift, where concurrently evolving noisy latents impede stable learning of alignment; (2) inefficient global attention mechanisms that fail to capture fine-grained temporal cues; and (3) the intra-modal bias of conventional Classifier-Free Guidance (CFG), which enhances conditionality but not cross-modal synchronization. To overcome these challenges, we introduce Harmony, a novel framework that mechanistically enforces audio-visual synchronization. We first propose a Cross-Task Synergy training paradigm to mitigate drift by leveraging strong supervisory signals from audio-driven video and video-driven audio generation tasks. Then, we design a Global-Local Decoupled Interaction Module for efficient and precise temporal-style alignment. Finally, we present a novel Synchronization-Enhanced CFG (SyncCFG) that explicitly isolates and amplifies the alignment signal during inference. Extensive experiments demonstrate that Harmony establishes a new state-of-the-art, significantly outperforming existing methods in both generation fidelity and, critically, in achieving fine-grained audio-visual synchronization.",
        "translated": "同步音频-视觉内容的合成是生成式人工智能的核心挑战，开源模型在鲁棒的音视频对齐方面面临困难。我们的分析表明，这一问题源于联合扩散过程中的三个根本性挑战：（1）对应漂移（Correspondence Drift），即同时演化的噪声潜在变量阻碍了对齐的稳定学习；（2）低效的全局注意力机制，无法捕捉细粒度的时间线索；以及（3）传统无条件分类器引导（Classifier-Free Guidance, CFG）的模态内偏倚，其虽增强了条件性，却未能促进跨模态同步。为克服这些挑战，我们提出Harmony，一种新框架，从机制层面强制实现音频-视觉同步。首先，我们提出一种跨任务协同训练范式，通过利用音频驱动视频与视频驱动音频生成任务中强监督信号来缓解漂移。接着，我们设计了一种全局-局部解耦交互模块，以实现高效且精确的时间风格对齐。最后，我们提出了新型同步增强型CFG（SyncCFG），在推理阶段显式分离并放大对齐信号。大量实验表明，Harmony建立了新的性能基准，在生成保真度方面显著优于现有方法，并尤为关键的是，在实现细粒度音视频同步方面表现突出。",
        "translated_title": "Harmony：通过跨任务协同实现音频与视频生成的和谐统一",
        "label": [],
        "label_reason": "生成音频视频同步内容属高阶任务，非像素级图像处理",
        "relevance_score": 1,
        "novelty_score": 9,
        "novelty_reason": "提出新框架解决跨模态对齐问题，创新显著"
    },
    {
        "title": "Enhanced Landmark Detection Model in Pelvic Fluoroscopy using 2D/3D Registration Loss",
        "url": "http://arxiv.org/abs/2511.21575v1",
        "pub_date": "2025-11-26",
        "summary": "Automated landmark detection offers an efficient approach for medical professionals to understand patient anatomic structure and positioning using intra-operative imaging. While current detection methods for pelvic fluoroscopy demonstrate promising accuracy, most assume a fixed Antero-Posterior view of the pelvis. However, orientation often deviates from this standard view, either due to repositioning of the imaging unit or of the target structure itself. To address this limitation, we propose a novel framework that incorporates 2D/3D landmark registration into the training of a U-Net landmark prediction model. We analyze the performance difference by comparing landmark detection accuracy between the baseline U-Net, U-Net trained with Pose Estimation Loss, and U-Net fine-tuned with Pose Estimation Loss under realistic intra-operative conditions where patient pose is variable.",
        "translated": "自动化地标检测为医学专业人士提供了一种高效的方法，使其能够利用术中影像理解患者的解剖结构及体位。尽管当前针对骨盆透视成像的检测方法已展现出良好的准确性，但多数方法均假设骨盆处于标准的前后位视角。然而，在实际应用中，患者体位常偏离该标准视角，或因成像设备重新定位、目标结构自身移动所致。为解决这一局限性，我们提出一种新颖框架，将2D/3D地标配准融入U-Net地标预测模型的训练过程。我们通过比较基准U-Net、采用姿态估计损失训练的U-Net以及在真实术中条件下（患者体位可变）进行姿态估计损失微调后的U-Net三者之间的地标检测精度，分析其性能差异。",
        "translated_title": "基于2D/3D配准损失的骨盆荧光透视图像增强地标检测模型",
        "label": [],
        "label_reason": "目标为医学图像分割，非像素级图像恢复",
        "relevance_score": 3,
        "novelty_score": 5,
        "novelty_reason": "改进训练策略，无新架构或理论创新"
    },
    {
        "title": "Multimodal Robust Prompt Distillation for 3D Point Cloud Models",
        "url": "http://arxiv.org/abs/2511.21574v1",
        "pub_date": "2025-11-26",
        "summary": "Adversarial attacks pose a significant threat to learning-based 3D point cloud models, critically undermining their reliability in security-sensitive applications. Existing defense methods often suffer from (1) high computational overhead and (2) poor generalization ability across diverse attack types. To bridge these gaps, we propose a novel yet efficient teacher-student framework, namely Multimodal Robust Prompt Distillation (MRPD) for distilling robust 3D point cloud model. It learns lightweight prompts by aligning student point cloud model's features with robust embeddings from three distinct teachers: a vision model processing depth projections, a high-performance 3D model, and a text encoder. To ensure a reliable knowledge transfer, this distillation is guided by a confidence-gated mechanism which dynamically balances the contribution of all input modalities. Notably, since the distillation is all during the training stage, there is no additional computational cost at inference. Extensive experiments demonstrate that MRPD substantially outperforms state-of-the-art defense methods against a wide range of white-box and black-box attacks, while even achieving better performance on clean data. Our work presents a new, practical paradigm for building robust 3D vision systems by efficiently harnessing multimodal knowledge.",
        "translated": "对抗攻击对基于学习的3D点云模型构成重大威胁，严重削弱其在安全敏感应用中的可靠性。现有防御方法通常存在以下两个问题：(1) 计算开销高，(2) 对多种攻击类型的泛化能力差。为弥合这些差距，我们提出了一种新颖且高效的教师-学生框架——多模态鲁棒提示蒸馏（Multimodal Robust Prompt Distillation, MRPD），用于蒸馏鲁棒的3D点云模型。该框架通过将学生点云模型的特征与来自三种不同教师模型的鲁棒嵌入对齐，学习轻量级提示：一种处理深度投影的视觉模型、一个高性能的3D模型，以及一个文本编码器。为确保可靠的知识迁移，该蒸馏过程由置信度门控机制引导，动态平衡所有输入模态的贡献。值得注意的是，由于蒸馏全过程均发生在训练阶段，推理阶段无需额外计算开销。大量实验表明，MRPD在面对广泛的白盒和黑盒攻击时显著优于现有最先进防御方法，甚至在干净数据上也取得了更优性能。我们的工作提出了一种新的、实用的方法论，通过高效利用多模态知识构建鲁棒的3D视觉系统。",
        "translated_title": "多模态鲁棒提示蒸馏用于3D点云模型",
        "label": [],
        "label_reason": "处理3D点云防御，非图像像素级恢复任务",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "多模态知识蒸馏框架创新，提升模型鲁棒性"
    },
    {
        "title": "UAVLight: A Benchmark for Illumination-Robust 3D Reconstruction in Unmanned Aerial Vehicle (UAV) Scenes",
        "url": "http://arxiv.org/abs/2511.21565v1",
        "pub_date": "2025-11-26",
        "summary": "Illumination inconsistency is a fundamental challenge in multi-view 3D reconstruction. Variations in sunlight direction, cloud cover, and shadows break the constant-lighting assumption underlying both classical multi-view stereo (MVS) and structure from motion (SfM) pipelines and recent neural rendering methods, leading to geometry drift, color inconsistency, and shadow imprinting. This issue is especially critical in UAV-based reconstruction, where long flight durations and outdoor environments make lighting changes unavoidable. However, existing datasets either restrict capture to short time windows, thus lacking meaningful illumination diversity, or span months and seasons, where geometric and semantic changes confound the isolated study of lighting robustness. We introduce UAVLight, a controlled-yet-real benchmark for illumination-robust 3D reconstruction. Each scene is captured along repeatable, geo-referenced flight paths at multiple fixed times of day, producing natural lighting variation under consistent geometry, calibration, and viewpoints. With standardized evaluation protocols across lighting conditions, UAVLight provides a reliable foundation for developing and benchmarking reconstruction methods that are consistent, faithful, and relightable in real outdoor environments.",
        "translated": "光照不一致性是多视角三维重建中的根本性挑战。太阳光方向、云层覆盖和阴影的变化破坏了传统多视角立体视觉（MVS）与运动恢复结构（SfM）流程以及近期神经渲染方法所依赖的恒定光照假设，导致几何漂移、色彩不一致及阴影残留等问题。这一问题在基于无人机的重建中尤为关键，因为长时间飞行与户外环境使得光照变化不可避免。然而，现有数据集要么仅限于短时间窗口采集，因而缺乏有意义的光照多样性，要么跨越数月甚至季节，此时几何与语义变化干扰了对光照鲁棒性的独立研究。我们提出UAVLight，这是一个可控却真实的光照鲁棒三维重建基准测试集。每个场景均沿可重复、地理参考的飞行路径，在一天中多个固定时段进行拍摄，从而在保持几何结构、校准参数与视角一致的前提下产生自然光照变化。通过跨不同光照条件的标准评估协议，UAVLight为开发和验证在真实户外环境中具备一致性、保真度与重光照能力的重建方法提供了可靠基础。",
        "translated_title": "UAVLight：用于无人机场景中光照鲁棒三维重建的基准数据集",
        "label": [],
        "label_reason": "聚焦3D重建，非像素级图像处理任务",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出新基准数据集，助力光照鲁棒性研究"
    },
    {
        "title": "Video Generation Models Are Good Latent Reward Models",
        "url": "http://arxiv.org/abs/2511.21541v1",
        "pub_date": "2025-11-26",
        "summary": "Reward feedback learning (ReFL) has proven effective for aligning image generation with human preferences. However, its extension to video generation faces significant challenges. Existing video reward models rely on vision-language models designed for pixel-space inputs, confining ReFL optimization to near-complete denoising steps after computationally expensive VAE decoding. This pixel-space approach incurs substantial memory overhead and increased training time, and its late-stage optimization lacks early-stage supervision, refining only visual quality rather than fundamental motion dynamics and structural coherence. In this work, we show that pre-trained video generation models are naturally suited for reward modeling in the noisy latent space, as they are explicitly designed to process noisy latent representations at arbitrary timesteps and inherently preserve temporal information through their sequential modeling capabilities. Accordingly, we propose Process Reward Feedback Learning~(PRFL), a framework that conducts preference optimization entirely in latent space, enabling efficient gradient backpropagation throughout the full denoising chain without VAE decoding. Extensive experiments demonstrate that PRFL significantly improves alignment with human preferences, while achieving substantial reductions in memory consumption and training time compared to RGB ReFL.",
        "translated": "奖励反馈学习（ReFL）已被证明能有效对齐图像生成与人类偏好。然而，将其扩展至视频生成面临显著挑战。现有视频奖励模型依赖于为像素空间输入设计的视觉-语言模型，致使 ReFL 优化被限制在计算开销高昂的 VAE 解码后近乎完全去噪的阶段。这种像素空间方法带来了巨大的内存开销和训练时间增加，且其后期优化缺乏早期监督，仅提升视觉质量而未能有效优化基础运动动态与结构一致性。在本工作中，我们表明预训练的视频生成模型天然适用于噪声潜变量空间中的奖励建模，因为它们明确设计用于处理任意时刻点的噪声潜表示，并通过其序列建模能力内在保留时序信息。据此，我们提出过程奖励反馈学习（PRFL），一种完全在潜空间中进行偏好优化的框架，可实现无需 VAE 解码的全去噪链路高效梯度反传。大量实验表明，PRFL 显著提升了与人类偏好的对齐程度，同时相较 RGB ReFL 实现了内存消耗和训练时间的大幅降低。",
        "translated_title": "视频生成模型是优良的潜在奖励模型",
        "label": [],
        "label_reason": "研究视频生成模型用于奖励建模，非图像像素级处理",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "提出PRFL框架，在潜空间优化提升效率与对齐"
    },
    {
        "title": "Bangla Sign Language Translation: Dataset Creation Challenges, Benchmarking and Prospects",
        "url": "http://arxiv.org/abs/2511.21533v1",
        "pub_date": "2025-11-26",
        "summary": "Bangla Sign Language Translation (BdSLT) has been severely constrained so far as the language itself is very low resource. Standard sentence level dataset creation for BdSLT is of immense importance for developing AI based assistive tools for deaf and hard of hearing people of Bangla speaking community. In this paper, we present a dataset, IsharaKhobor , and two subset of it for enabling research. We also present the challenges towards developing the dataset and present some way forward by benchmarking with landmark based raw and RQE embedding. We do some ablation on vocabulary restriction and canonicalization of the same within the dataset, which resulted in two more datasets, IsharaKhobor_small and IsharaKhobor_canonical_small. The dataset is publicly available at: www.kaggle.com/datasets/hasanssl/isharakhobor [1].",
        "translated": "目前，孟加拉语手语翻译（BdSLT）的发展严重受限，因为该语言本身资源极其匮乏。为聋哑及听力障碍的孟加拉语使用者开发基于人工智能的辅助工具，亟需构建标准化的句子级数据集。本文提出一个名为 IsharaKhobor 的数据集，并提供其两个子集以促进相关研究。我们还阐述了构建该数据集所面临的挑战，并通过基于地标特征的原始数据与 RQE 嵌入进行基准测试，提出若干可行方向。我们在数据集中对词汇限制和词形规范化进行了消融实验，由此衍生出两个新数据集：IsharaKhobor_small 和 IsharaKhobor_canonical_small。该数据集已公开发布于：www.kaggle.com/datasets/hasanssl/isharakhobor [1]。",
        "translated_title": "孟加拉语手语翻译：数据集构建挑战、基准评测与前景",
        "label": [],
        "label_reason": "研究手语翻译，属高阶自然语言处理任务。",
        "relevance_score": 1,
        "novelty_score": 3,
        "novelty_reason": "数据集构建与基准测试，无图像像素级处理创新。"
    },
    {
        "title": "The Age-specific Alzheimer 's Disease Prediction with Characteristic Constraints in Nonuniform Time Span",
        "url": "http://arxiv.org/abs/2511.21530v1",
        "pub_date": "2025-11-26",
        "summary": "Alzheimer's disease is a debilitating disorder marked by a decline in cognitive function. Timely identification of the disease is essential for the development of personalized treatment strategies that aim to mitigate its progression. The application of generated images for the prediction of Alzheimer's disease poses challenges, particularly in accurately representing the disease's characteristics when input sequences are captured at irregular time intervals. This study presents an innovative methodology for sequential image generation, guided by quantitative metrics, to maintain the essential features indicative of disease progression. Furthermore, an age-scaling factor is integrated into the process to produce age-specific MRI images, facilitating the prediction of advanced stages of the disease. The results obtained from the ablation study suggest that the inclusion of quantitative metrics significantly improves the accuracy of MRI image synthesis. Furthermore, the application of age-scaled pixel loss contributed to the enhanced iterative generation of MRI images. In terms of long-term disease prognosis, the Structural Similarity Index reached a peak value of 0.882, indicating a substantial degree of similarity in the synthesized images.",
        "translated": "阿尔茨海默病是一种导致认知功能衰退的严重疾病。及时识别该疾病对于制定旨在延缓其进展的个性化治疗策略至关重要。使用生成图像预测阿尔茨海默病的应用面临挑战，特别是在输入序列以非规则时间间隔采集时，难以准确反映该疾病的特征。本研究提出了一种基于定量指标引导的序列图像生成创新方法，以保持反映疾病进展的关键特征。此外，该方法还引入了年龄缩放因子，用于生成特定年龄的MRI图像，从而有助于预测疾病晚期阶段。消融实验结果表明，引入定量指标显著提升了MRI图像合成的准确性；同时，应用年龄缩放像素损失也促进了MRI图像的迭代生成优化。在长期疾病预后评估方面，结构相似性指数（SSIM）达到峰值0.882，表明合成图像与真实图像具有高度相似性。",
        "translated_title": "非均匀时间跨度下基于特征约束的年龄特异性阿尔茨海默病预测",
        "label": [],
        "label_reason": "预测阿尔茨海默病属高阶任务，非图像像素级恢复",
        "relevance_score": 2,
        "novelty_score": 4,
        "novelty_reason": "引入年龄因子与量化指标，但非图像处理创新"
    },
    {
        "title": "EoS-FM: Can an Ensemble of Specialist Models act as a Generalist Feature Extractor?",
        "url": "http://arxiv.org/abs/2511.21523v1",
        "pub_date": "2025-11-26",
        "summary": "Recent advances in foundation models have shown great promise in domains such as natural language processing and computer vision, and similar efforts are now emerging in the Earth Observation community. These models aim to generalize across tasks with limited supervision, reducing the need for training separate models for each task. However, current strategies, which largely focus on scaling model size and dataset volume, require prohibitive computational and data resources, limiting accessibility to only a few large institutions. Moreover, this paradigm of ever-larger models stands in stark contrast with the principles of sustainable and environmentally responsible AI, as it leads to immense carbon footprints and resource inefficiency. In this work, we present a novel and efficient alternative: an Ensemble-of-Specialists framework for building Remote Sensing Foundation Models (RSFMs). Our method decomposes the training process into lightweight, task-specific ConvNeXtV2 specialists that can be frozen and reused. This modular approach offers strong advantages in efficiency, interpretability, and extensibility. Moreover, it naturally supports federated training, pruning, and continuous specialist integration, making it particularly well-suited for collaborative and resource-constrained settings. Our framework sets a new direction for building scalable and efficient RSFMs.",
        "translated": "近年来，基础模型在自然语言处理和计算机视觉等领域的进展展现出巨大潜力，类似的努力如今也正在地球观测领域兴起。这些模型旨在通过有限的监督信号实现跨任务泛化，从而减少为每个任务单独训练模型的需求。然而，当前主流策略主要聚焦于扩展模型规模与数据集体量，这需要难以承受的计算资源与数据资源，使该技术仅限于少数大型机构使用。此外，这种“模型不断变大”的范式与可持续、环境友好的人工智能原则背道而驰，因其导致巨大的碳足迹与资源浪费。在本工作中，我们提出了一种新颖且高效的替代方案：一种面向遥感基础模型（RSFMs）构建的“专家集成”框架（Ensemble-of-Specialists）。我们的方法将训练过程分解为轻量级、面向特定任务的 ConvNeXtV2 专家模型，这些模型可被冻结并复用。该模块化设计在效率、可解释性及可扩展性方面具有显著优势。此外，它天然支持联邦训练、模型剪枝以及持续引入新专家，使其特别适用于协作型及资源受限的场景。本框架为构建可扩展且高效的 RSFMs 标志着新的方向。",
        "translated_title": "EoS-FM：多个专业模型的集成能否作为通用特征提取器？",
        "label": [],
        "label_reason": "研究遥感任务，但非图像像素级恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "提出模块化专家模型框架，提升效率与可扩展性。"
    },
    {
        "title": "Self-Paced Learning for Images of Antinuclear Antibodies",
        "url": "http://arxiv.org/abs/2511.21519v1",
        "pub_date": "2025-11-26",
        "summary": "Antinuclear antibody (ANA) testing is a crucial method for diagnosing autoimmune disorders, including lupus, Sjögren's syndrome, and scleroderma. Despite its importance, manual ANA detection is slow, labor-intensive, and demands years of training. ANA detection is complicated by over 100 coexisting antibody types, resulting in vast fluorescent pattern combinations. Although machine learning and deep learning have enabled automation, ANA detection in real-world clinical settings presents unique challenges as it involves multi-instance, multi-label (MIML) learning. In this paper, a novel framework for ANA detection is proposed that handles the complexities of MIML tasks using unaltered microscope images without manual preprocessing. Inspired by human labeling logic, it identifies consistent ANA sub-regions and assigns aggregated labels accordingly. These steps are implemented using three task-specific components: an instance sampler, a probabilistic pseudo-label dispatcher, and self-paced weight learning rate coefficients. The instance sampler suppresses low-confidence instances by modeling pattern confidence, while the dispatcher adaptively assigns labels based on instance distinguishability. Self-paced learning adjusts training according to empirical label observations. Our framework overcomes limitations of traditional MIML methods and supports end-to-end optimization. Extensive experiments on one ANA dataset and three public medical MIML benchmarks demonstrate the superiority of our framework. On the ANA dataset, our model achieves up to +7.0% F1-Macro and +12.6% mAP gains over the best prior method, setting new state-of-the-art results. It also ranks top-2 across all key metrics on public datasets, reducing Hamming loss and one-error by up to 18.2% and 26.9%, respectively. The source code can be accessed at https://github.com/fletcherjiang/ANA-SelfPacedLearning.",
        "translated": "抗核抗体（ANA）检测是诊断自身免疫性疾病（如系统性红斑狼疮、干燥综合征和硬皮病）的关键方法。尽管其重要性显著，但人工ANA检测过程缓慢、劳动密集，且需数年专业培训。由于存在超过100种共存的抗体类型，ANA检测面临大量荧光模式组合的复杂性。虽然机器学习与深度学习已实现检测自动化，但在现实临床环境中，ANA检测仍涉及多实例、多标签（MIML）学习任务，具有独特挑战。本文提出一种新颖的ANA检测框架，无需手动预处理，直接利用未经修改的显微镜图像完成MIML任务。该框架受人类标注逻辑启发，识别一致的ANA子区域，并据此分配聚合标签。这一流程通过三个任务特定组件实现：实例采样器、概率伪标签分发器以及自适应学习率系数的自我节奏学习模块。实例采样器通过建模模式置信度抑制低置信度样本；分发器根据实例区分度自适应分配标签；自我节奏学习则依据经验标签观测动态调整训练策略。本框架突破传统MIML方法的局限，支持端到端优化。在单一ANA数据集及三大公开医学MIML基准数据集上的大量实验表明，本框架性能优越。在ANA数据集上，模型相较最优先前方法，在F1-Macro指标上提升高达+7.0%，mAP指标提升达+12.6%，创当前最先进结果。同时，该模型在公开数据集所有关键指标上均位列前两名，将Hamming损失与one-error分别降低最高达18.2%与26.9%。源代码可访问 https://github.com/fletcherjiang/ANA-SelfPacedLearning。",
        "translated_title": "抗核抗体图像的自适应学习",
        "label": [],
        "label_reason": "任务为医学多标签分类，非像素级图像处理",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出自适应标签分配与学习率机制改进MIML"
    },
    {
        "title": "Generalized Design Choices for Deepfake Detectors",
        "url": "http://arxiv.org/abs/2511.21507v1",
        "pub_date": "2025-11-26",
        "summary": "The effectiveness of deepfake detection methods often depends less on their core design and more on implementation details such as data preprocessing, augmentation strategies, and optimization techniques. These factors make it difficult to fairly compare detectors and to understand which factors truly contribute to their performance. To address this, we systematically investigate how different design choices influence the accuracy and generalization capabilities of deepfake detection models, focusing on aspects related to training, inference, and incremental updates. By isolating the impact of individual factors, we aim to establish robust, architecture-agnostic best practices for the design and development of future deepfake detection systems. Our experiments identify a set of design choices that consistently improve deepfake detection and enable state-of-the-art performance on the AI-GenBench benchmark.",
        "translated": "深度伪造检测方法的有效性往往与其核心设计关系不大，而更多取决于数据预处理、增强策略和优化技术等实现细节。这些因素使得公平比较不同检测器变得困难，并难以明确哪些因素真正对其性能产生贡献。为解决这一问题，我们系统性地研究了不同设计选择对深度伪造检测模型准确率与泛化能力的影响，重点关注与训练、推理及增量更新相关的方面。通过分离各个因素的独立影响，我们旨在为未来深度伪造检测系统的开发建立稳健且与架构无关的最佳实践。我们的实验识别出一组能够持续提升检测性能的设计选择，并在AI-GenBench基准上实现了当前最优水平。",
        "translated_title": "深度伪造检测器的通用设计选择",
        "label": [],
        "label_reason": "属于高阶伪造检测任务，非像素级图像处理",
        "relevance_score": 1,
        "novelty_score": 3,
        "novelty_reason": "复现与调参为主，无新架构或理论创新"
    },
    {
        "title": "CanKD: Cross-Attention-based Non-local operation for Feature-based Knowledge Distillation",
        "url": "http://arxiv.org/abs/2511.21503v1",
        "pub_date": "2025-11-26",
        "summary": "We propose Cross-Attention-based Non-local Knowledge Distillation (CanKD), a novel feature-based knowledge distillation framework that leverages cross-attention mechanisms to enhance the knowledge transfer process. Unlike traditional self-attention-based distillation methods that align teacher and student feature maps independently, CanKD enables each pixel in the student feature map to dynamically consider all pixels in the teacher feature map. This non-local knowledge transfer more thoroughly captures pixel-wise relationships, improving feature representation learning. Our method introduces only an additional loss function to achieve superior performance compared with existing attention-guided distillation methods. Extensive experiments on object detection and image segmentation tasks demonstrate that CanKD outperforms state-of-the-art feature and hybrid distillation methods. These experimental results highlight CanKD's potential as a new paradigm for attention-guided distillation in computer vision tasks. Code is available at https://github.com/tori-hotaru/CanKD",
        "translated": "我们提出了一种基于交叉注意力的非局部知识蒸馏方法（CanKD），这是一种新颖的特征导向知识蒸馏框架，利用交叉注意力机制增强知识迁移过程。与传统的基于自注意力的知识蒸馏方法不同，后者独立对齐教师与学生特征图，CanKD 使学生特征图中的每个像素能够动态地考虑教师特征图中的所有像素。这种非局部知识迁移更全面地捕捉像素级关系，从而提升特征表示学习效果。我们的方法仅引入一个额外的损失函数，即可在性能上超越现有基于注意力引导的知识蒸馏方法。在目标检测和图像分割任务上的大量实验表明，CanKD 超越了当前最先进的特征蒸馏与混合蒸馏方法。这些实验结果凸显了 CanKD 作为计算机视觉任务中注意力引导蒸馏新范式所具有的潜力。代码可访问 https://github.com/tori-hotaru/CanKD",
        "translated_title": "CanKD：基于交叉注意力的非局部操作用于特征驱动的知识蒸馏",
        "label": [],
        "label_reason": "专注知识蒸馏，非图像像素级恢复任务",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出跨注意力机制提升蒸馏效果"
    },
    {
        "title": "Merge and Bound: Direct Manipulations on Weights for Class Incremental Learning",
        "url": "http://arxiv.org/abs/2511.21490v1",
        "pub_date": "2025-11-26",
        "summary": "We present a novel training approach, named Merge-and-Bound (M&amp;B) for Class Incremental Learning (CIL), which directly manipulates model weights in the parameter space for optimization. Our algorithm involves two types of weight merging: inter-task weight merging and intra-task weight merging. Inter-task weight merging unifies previous models by averaging the weights of models from all previous stages. On the other hand, intra-task weight merging facilitates the learning of current task by combining the model parameters within current stage. For reliable weight merging, we also propose a bounded update technique that aims to optimize the target model with minimal cumulative updates and preserve knowledge from previous tasks; this strategy reveals that it is possible to effectively obtain new models near old ones, reducing catastrophic forgetting. M&amp;B is seamlessly integrated into existing CIL methods without modifying architecture components or revising learning objectives. We extensively evaluate our algorithm on standard CIL benchmarks and demonstrate superior performance compared to state-of-the-art methods.",
        "translated": "我们提出了一种新颖的训练方法，称为 Merge-and-Bound（M&B），用于类增量学习（CIL），该方法直接在参数空间中操作模型权重以实现优化。我们的算法包含两种类型的权重合并：跨任务权重合并与内任务权重合并。跨任务权重合并通过平均所有先前阶段模型的权重来统一历史模型；另一方面，内任务权重合并则通过结合当前阶段内的模型参数，促进当前任务的学习。为确保权重合并的可靠性，我们还提出了一种有界更新技术，旨在以最小累积更新量优化目标模型并保留先前任务的知识；该策略表明，可以有效获得接近旧模型的新模型，从而减轻灾难性遗忘。M&B 可无缝集成到现有 CIL 方法中，无需修改网络架构或调整学习目标。我们在标准 CIL 数据集上对算法进行了广泛评估，结果表明其性能优于当前最先进的方法。",
        "translated_title": "合并与约束：面向类增量学习的权重直接操作",
        "label": [],
        "label_reason": "属于高阶任务，非图像处理",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "改进CIL权重更新机制，非图像恢复"
    },
    {
        "title": "Frequency-Aware Token Reduction for Efficient Vision Transformer",
        "url": "http://arxiv.org/abs/2511.21477v1",
        "pub_date": "2025-11-26",
        "summary": "Vision Transformers have demonstrated exceptional performance across various computer vision tasks, yet their quadratic computational complexity concerning token length remains a significant challenge. To address this, token reduction methods have been widely explored. However, existing approaches often overlook the frequency characteristics of self-attention, such as rank collapsing and over-smoothing phenomenon. In this paper, we propose a frequency-aware token reduction strategy that improves computational efficiency while preserving performance by mitigating rank collapsing. Our method partitions tokens into high-frequency tokens and low-frequency tokens. high-frequency tokens are selectively preserved, while low-frequency tokens are aggregated into a compact direct current token to retain essential low-frequency components. Through extensive experiments and analysis, we demonstrate that our approach significantly improves accuracy while reducing computational overhead and mitigating rank collapsing and over smoothing. Furthermore, we analyze the previous methods, shedding light on their implicit frequency characteristics and limitations.",
        "translated": "视觉Transformer在各类计算机视觉任务中展现了卓越的性能，但其计算复杂度随token长度呈二次增长，仍是一个重大挑战。为应对这一问题，Token缩减方法已被广泛研究。然而，现有方法常忽略自注意力机制中的频率特性，如秩坍塌与过度平滑现象。本文提出一种频率感知的Token缩减策略，在缓解秩坍塌的同时提升计算效率并保持性能。该方法将Token划分为高频Token与低频Token：高频Token被选择性保留，而低频Token则聚合为一个紧凑的直流Token以保留关键的低频成分。通过大量实验与分析，我们证明该方法显著提升了精度、降低了计算开销，并有效缓解了秩坍塌与过度平滑问题。此外，我们对先前方法进行了分析，揭示了其隐含的频率特性及局限性。",
        "translated_title": "频域感知的Token缩减用于高效视觉Transformer",
        "label": [],
        "label_reason": "聚焦Transformer效率优化，非图像像素级恢复任务",
        "relevance_score": 3,
        "novelty_score": 7,
        "novelty_reason": "提出频率感知令牌压缩机制，显著提升计算效率"
    },
    {
        "title": "MobileI2V: Fast and High-Resolution Image-to-Video on Mobile Devices",
        "url": "http://arxiv.org/abs/2511.21475v1",
        "pub_date": "2025-11-26",
        "summary": "Recently, video generation has witnessed rapid advancements, drawing increasing attention to image-to-video (I2V) synthesis on mobile devices. However, the substantial computational complexity and slow generation speed of diffusion models pose significant challenges for real-time, high-resolution video generation on resource-constrained mobile devices. In this work, we propose MobileI2V, a 270M lightweight diffusion model for real-time image-to-video generation on mobile devices. The core lies in: (1) We analyzed the performance of linear attention modules and softmax attention modules on mobile devices, and proposed a linear hybrid architecture denoiser that balances generation efficiency and quality. (2) We design a time-step distillation strategy that compresses the I2V sampling steps from more than 20 to only two without significant quality loss, resulting in a 10-fold increase in generation speed. (3) We apply mobile-specific attention optimizations that yield a 2-fold speed-up for attention operations during on-device inference. MobileI2V enables, for the first time, fast 720p image-to-video generation on mobile devices, with quality comparable to existing models. Under one-step conditions, the generation speed of each frame of 720p video is less than 100 ms. Our code is available at: https://github.com/hustvl/MobileI2V.",
        "translated": "近年来，视频生成领域取得了快速进展，图像到视频（I2V）合成在移动设备上的应用正受到越来越多的关注。然而，扩散模型较高的计算复杂度与缓慢的生成速度，对资源受限的移动设备上实现实时、高分辨率视频生成构成了显著挑战。本文提出 MobileI2V，一种面向移动端的轻量级扩散模型（参数量 270M），可实现图像到视频的实时生成。其核心贡献包括：（1）我们分析了线性注意力模块与 softmax 注意力模块在移动设备上的性能表现，并设计了一种线性混合架构去噪器，在保证生成质量的同时提升生成效率。（2）我们设计了一种时间步蒸馏策略，将 I2V 采样步骤从超过 20 步压缩至仅 2 步，且未造成明显质量损失，从而使生成速度提升十倍。（3）我们引入针对移动端优化的注意力机制，使设备端推理时注意力操作加速两倍。MobileI2V 首次实现了在移动设备上快速生成 720p 分辨率的图像到视频内容，其质量可媲美现有模型。在单步条件下，每帧 720p 视频的生成耗时低于 100 毫秒。我们的代码已开源于：https://github.com/hustvl/MobileI2V。",
        "translated_title": "MobileI2V：移动端快速高分辨率图像到视频生成",
        "label": [],
        "label_reason": "生成视频属高阶任务，非像素级图像恢复",
        "relevance_score": 3,
        "novelty_score": 7,
        "novelty_reason": "轻量化扩散模型优化，效率提升显著"
    },
    {
        "title": "EvRainDrop: HyperGraph-guided Completion for Effective Frame and Event Stream Aggregation",
        "url": "http://arxiv.org/abs/2511.21439v1",
        "pub_date": "2025-11-26",
        "summary": "Event cameras produce asynchronous event streams that are spatially sparse yet temporally dense. Mainstream event representation learning algorithms typically use event frames, voxels, or tensors as input. Although these approaches have achieved notable progress, they struggle to address the undersampling problem caused by spatial sparsity. In this paper, we propose a novel hypergraph-guided spatio-temporal event stream completion mechanism, which connects event tokens across different times and spatial locations via hypergraphs and leverages contextual information message passing to complete these sparse events. The proposed method can flexibly incorporate RGB tokens as nodes in the hypergraph within this completion framework, enabling multi-modal hypergraph-based information completion. Subsequently, we aggregate hypergraph node information across different time steps through self-attention, enabling effective learning and fusion of multi-modal features. Extensive experiments on both single- and multi-label event classification tasks fully validated the effectiveness of our proposed framework. The source code of this paper will be released on https://github.com/Event-AHU/EvRainDrop.",
        "translated": "事件相机产生异步事件流，其在空间上稀疏但在时间上密集。主流的事件表示学习算法通常将事件帧、体素或张量作为输入。尽管这些方法已取得显著进展，但仍难以有效应对因空间稀疏性所导致的欠采样问题。本文提出了一种新颖的超图引导时空事件流补全机制，该机制通过超图连接不同时空位置的事件令牌，并利用上下文信息消息传递来完成这些稀疏事件。所提方法可灵活地将RGB令牌作为超图中的节点纳入此补全框架，从而实现多模态超图信息补全。随后，我们通过自注意力机制对不同时间步的超图节点信息进行聚合，以实现多模态特征的有效学习与融合。在单标签和多标签事件分类任务上的大量实验充分验证了所提框架的有效性。本文源代码将在 https://github.com/Event-AHU/EvRainDrop 上开源。",
        "translated_title": "EvRainDrop：基于超图引导的帧与事件流聚合方法",
        "label": [],
        "label_reason": "聚焦事件流分类，非图像像素级恢复任务",
        "relevance_score": 3,
        "novelty_score": 6,
        "novelty_reason": "多模态超图融合框架，但无低层图像修复目标"
    },
    {
        "title": "From Observation to Action: Latent Action-based Primitive Segmentation for VLA Pre-training in Industrial Settings",
        "url": "http://arxiv.org/abs/2511.21428v1",
        "pub_date": "2025-11-26",
        "summary": "We present a novel unsupervised framework to unlock vast unlabeled human demonstration data from continuous industrial video streams for Vision-Language-Action (VLA) model pre-training. Our method first trains a lightweight motion tokenizer to encode motion dynamics, then employs an unsupervised action segmenter leveraging a novel \"Latent Action Energy\" metric to discover and segment semantically coherent action primitives. The pipeline outputs both segmented video clips and their corresponding latent action sequences, providing structured data directly suitable for VLA pre-training. Evaluations on public benchmarks and a proprietary electric motor assembly dataset demonstrate effective segmentation of key tasks performed by humans at workstations. Further clustering and quantitative assessment via a Vision-Language Model confirm the semantic coherence of the discovered action primitives. To our knowledge, this is the first fully automated end-to-end system for extracting and organizing VLA pre-training data from unstructured industrial videos, offering a scalable solution for embodied AI integration in manufacturing.",
        "translated": "我们提出了一种新颖的无监督框架，旨在从连续的工业视频流中挖掘大量未标注的人类演示数据，用于 Vision-Language-Action（VLA）模型的预训练。该方法首先训练一个轻量化的运动 tokenizer 以编码运动动态，随后采用一种基于新型“潜在动作能量”度量的无监督动作分割器，以发现并分割语义连贯的动作原型。该流程输出分段的视频片段及其对应的潜在动作序列，直接提供适用于 VLA 预训练的结构化数据。在公开基准数据集和自有电动机装配数据集上的评估表明，该方法能够有效分割人类在工作台执行的关键任务。进一步通过 Vision-Language 模型进行聚类与定量评估，证实了所发现的动作原型具备良好的语义一致性。据我们所知，这是首个实现端到端全自动提取与组织 VLA 预训练数据的系统，为制造领域具身 AI 的集成提供了可扩展的解决方案。",
        "translated_title": "从观测到行动：面向工业场景VLA预训练的潜在动作驱动基础分割方法",
        "label": [],
        "label_reason": "任务为视频动作分割，属高阶视觉与机器人学习",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "首次自动化提取工业视频动作片段用于VLA预训练"
    },
    {
        "title": "E-M3RF: An Equivariant Multimodal 3D Re-assembly Framework",
        "url": "http://arxiv.org/abs/2511.21422v1",
        "pub_date": "2025-11-26",
        "summary": "3D reassembly is a fundamental geometric problem, and in recent years it has increasingly been challenged by deep learning methods rather than classical optimization. While learning approaches have shown promising results, most still rely primarily on geometric features to assemble a whole from its parts. As a result, methods struggle when geometry alone is insufficient or ambiguous, for example, for small, eroded, or symmetric fragments. Additionally, solutions do not impose physical constraints that explicitly prevent overlapping assemblies. To address these limitations, we introduce E-M3RF, an equivariant multimodal 3D reassembly framework that takes as input the point clouds, containing both point positions and colors of fractured fragments, and predicts the transformations required to reassemble them using SE(3) flow matching. Each fragment is represented by both geometric and color features: i) 3D point positions are encoded as rotationconsistent geometric features using a rotation-equivariant encoder, ii) the colors at each 3D point are encoded with a transformer. The two feature sets are then combined to form a multimodal representation. We experimented on four datasets: two synthetic datasets, Breaking Bad and Fantastic Breaks, and two real-world cultural heritage datasets, RePAIR and Presious, demonstrating that E-M3RF on the RePAIR dataset reduces rotation error by 23.1% and translation error by 13.2%, while Chamfer Distance decreases by 18.4% compared to competing methods.",
        "translated": "三维重装配是一个基础的几何问题，近年来，其研究逐渐由经典优化方法转向深度学习方法。尽管学习型方法已展现出良好的效果，但多数仍主要依赖几何特征将碎片拼合成整体。因此，当仅靠几何信息不足以或存在歧义时（例如小尺寸、侵蚀性或对称性的碎片），现有方法往往表现不佳。此外，现有方案未能显式引入物理约束以防止重叠组装。为解决上述局限，我们提出 E-M3RF，一种具有等变性的多模态三维重装配框架。该框架输入包含破碎碎片点云数据（含点位置与颜色信息），并通过 SE(3) 流匹配预测所需变换以实现重装配。每个碎片同时编码几何与颜色特征：i）三维点位置通过旋转等变编码器被转换为旋转一致的几何特征；ii）各三维点的颜色则由 Transformer 编码。随后，两类特征组合形成多模态表示。我们在四个数据集上进行实验：两个合成数据集 Breaking Bad 与 Fantastic Breaks，以及两个真实文化遗产数据集 RePAIR 与 Presious。实验表明，在 RePAIR 数据集上，E-M3RF 相较于现有方法可降低旋转误差 23.1%，平移误差 13.2%，且 Chamfer 距离下降 18.4%。",
        "translated_title": "E-M3RF：一种等变多模态三维重组装框架",
        "label": [],
        "label_reason": "任务属3D几何重构，非图像像素级处理",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "提出旋转等变多模态框架，显著提升重建精度"
    },
    {
        "title": "SAM Guided Semantic and Motion Changed Region Mining for Remote Sensing Change Captioning",
        "url": "http://arxiv.org/abs/2511.21420v1",
        "pub_date": "2025-11-26",
        "summary": "Remote sensing change captioning is an emerging and popular research task that aims to describe, in natural language, the content of interest that has changed between two remote sensing images captured at different times. Existing methods typically employ CNNs/Transformers to extract visual representations from the given images or incorporate auxiliary tasks to enhance the final results, with weak region awareness and limited temporal alignment. To address these issues, this paper explores the use of the SAM (Segment Anything Model) foundation model to extract region-level representations and inject region-of-interest knowledge into the captioning framework. Specifically, we employ a CNN/Transformer model to extract global-level vision features, leverage the SAM foundation model to delineate semantic- and motion-level change regions, and utilize a specially constructed knowledge graph to provide information about objects of interest. These heterogeneous sources of information are then fused via cross-attention, and a Transformer decoder is used to generate the final natural language description of the observed changes. Extensive experimental results demonstrate that our method achieves state-of-the-art performance across multiple widely used benchmark datasets. The source code of this paper will be released on https://github.com/Event-AHU/SAM_ChangeCaptioning",
        "translated": "遥感变化描述是一种新兴且热门的研究任务，旨在用自然语言描述在不同时刻获取的两张遥感图像之间发生变化的内容。现有方法通常采用卷积神经网络（CNN）或Transformer提取给定图像的视觉表征，或引入辅助任务以提升最终效果，但存在区域感知能力弱和时间对齐有限的问题。为解决这些问题，本文探索利用SAM（Segment Anything Model）基础模型提取区域级表征，并将感兴趣区域知识注入描述框架中。具体而言，我们使用CNN/Transformer模型提取全局视觉特征，借助SAM基础模型勾勒语义与运动级别的变化区域，并通过专门构建的知识图谱提供感兴趣对象的信息。这些异构信息源随后通过交叉注意力机制进行融合，再由Transformer解码器生成所观测变化的最终自然语言描述。大量实验结果表明，我们的方法在多个常用基准数据集上均达到当前最优性能。本文代码将在https://github.com/Event-AHU/SAM_ChangeCaptioning发布。",
        "translated_title": "基于SAM引导的语义与运动变化区域挖掘用于遥感变化描述",
        "label": [],
        "label_reason": "任务为遥感图像变化描述，属高阶视觉理解",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "融合SAM与知识图谱提升区域感知，方法有改进"
    },
    {
        "title": "DiverseVAR: Balancing Diversity and Quality of Next-Scale Visual Autoregressive Models",
        "url": "http://arxiv.org/abs/2511.21415v1",
        "pub_date": "2025-11-26",
        "summary": "We introduce DiverseVAR, a framework that enhances the diversity of text-conditioned visual autoregressive models (VAR) at test time without requiring retraining, fine-tuning, or substantial computational overhead. While VAR models have recently emerged as strong competitors to diffusion and flow models for image generation, they suffer from a critical limitation in diversity, often producing nearly identical images even for simple prompts. This issue has largely gone unnoticed amid the predominant focus on image quality. We address this limitation at test time in two stages. First, inspired by diversity enhancement techniques in diffusion models, we propose injecting noise into the text embedding. This introduces a trade-off between diversity and image quality: as diversity increases, the image quality sharply declines. To preserve quality, we propose scale-travel: a novel latent refinement technique inspired by time-travel strategies in diffusion models. Specifically, we use a multi-scale autoencoder to extract coarse-scale tokens that enable us to resume generation at intermediate stages. Extensive experiments show that combining text-embedding noise injection with our scale-travel refinement significantly enhances diversity while minimizing image-quality degradation, achieving a new Pareto frontier in the diversity-quality trade-off.",
        "translated": "我们提出 DiverseVAR，这是一种在测试阶段无需重新训练、微调或增加大量计算开销即可提升文本条件视觉自回归模型（VAR）多样性的框架。尽管 VAR 模型近期已成为图像生成领域与扩散模型和流模型竞争的有力候选者，但其在多样性方面存在关键限制：即使对于简单的提示，也常生成几乎相同的图像。这一问题在当前主要关注图像质量的研究背景下往往被忽视。我们在测试阶段分两步解决此局限性。首先，受扩散模型中多样性增强技术的启发，我们提出向文本嵌入注入噪声。这引入了多样性与图像质量之间的权衡：随着多样性提升，图像质量急剧下降。为保持图像质量，我们提出“尺度旅行”（scale-travel）——一种受扩散模型中时间旅行策略启发的新颖潜在空间精炼技术。具体而言，我们使用多尺度自动编码器提取粗尺度标记，从而能够在生成过程的中间阶段恢复继续生成。大量实验表明，结合文本嵌入噪声注入与我们的尺度旅行精炼方法，可显著提升多样性，同时最小化图像质量的退化，在多样性-质量权衡上实现了新的帕累托前沿。",
        "translated_title": "DiverseVAR：平衡下一尺度视觉自回归模型的多样性与质量",
        "label": [],
        "label_reason": "聚焦图像生成多样性，非像素级图像恢复任务",
        "relevance_score": 2,
        "novelty_score": 8,
        "novelty_reason": "提出新噪声注入与多尺度精炼策略提升生成多样性"
    },
    {
        "title": "RIA: A Ranking-Infused Approach for Optimized listwise CTR Prediction",
        "url": "http://arxiv.org/abs/2511.21394v1",
        "pub_date": "2025-11-26",
        "summary": "Reranking improves recommendation quality by modeling item interactions. However, existing methods often decouple ranking and reranking, leading to weak listwise evaluation models that suffer from combinatorial sparsity and limited representational power under strict latency constraints. In this paper, we propose RIA (Ranking-Infused Architecture), a unified, end-to-end framework that seamlessly integrates pointwise and listwise evaluation. RIA introduces four key components: (1) the User and Candidate DualTransformer (UCDT) for fine-grained user-item-context modeling; (2) the Context-aware User History and Target (CUHT) module for position-sensitive preference learning; (3) the Listwise Multi-HSTU (LMH) module to capture hierarchical item dependencies; and (4) the Embedding Cache (EC) module to bridge efficiency and effectiveness during inference. By sharing representations across ranking and reranking, RIA enables rich contextual knowledge transfer while maintaining low latency. Extensive experiments show that RIA outperforms state-of-the-art models on both public and industrial datasets, achieving significant gains in AUC and LogLoss. Deployed in Meituan advertising system, RIA yields a +1.69% improvement in Click-Through Rate (CTR) and a +4.54% increase in Cost Per Mille (CPM) in online A/B tests.",
        "translated": "重排通过建模物料交互来提升推荐质量。然而，现有方法往往将排序与重排解耦，导致其列表级评估模型能力薄弱，在严格的延迟约束下易受组合稀疏性影响且表征能力有限。本文提出RIA（Ranking-Infused Architecture），一种统一的端到端框架，无缝融合点级与列表级评估。RIA包含四个关键组件：（1）用户与候选物料双Transformer（UCDT），用于细粒度的用户-物料-上下文建模；（2）上下文感知用户历史与目标模块（CUHT），用于位置敏感偏好学习；（3）列表级多层HSTU模块（LMH），用于捕捉物料间的层次依赖关系；（4）嵌入缓存模块（EC），用于推理阶段平衡效率与效果。通过在排序与重排间共享表示，RIA实现丰富上下文知识迁移的同时维持低延迟。大量实验表明，RIA在公开及工业数据集上均优于当前最先进模型，在AUC和LogLoss指标上取得显著提升。部署于美团广告系统后，RIA在在线A/B测试中使点击率（CTR）提升1.69%，千次展示成本（CPM）增长4.54%。",
        "translated_title": "RIA：一种融合排序机制的列表式点击率预测优化方法",
        "label": [
            "精排",
            "重排",
            "通用推荐技术"
        ],
        "label_reason": "提出统一精排与重排框架，提升列表级CTR预测",
        "relevance_score": 9,
        "novelty_score": 8,
        "novelty_reason": "创新性融合点/列表评估，高效知识迁移"
    },
    {
        "title": "FITRep: Attention-Guided Item Representation via MLLMs",
        "url": "http://arxiv.org/abs/2511.21389v1",
        "pub_date": "2025-11-26",
        "summary": "Online platforms usually suffer from user experience degradation due to near-duplicate items with similar visuals and text. While Multimodal Large Language Models (MLLMs) enable multimodal embedding, existing methods treat representations as black boxes, ignoring structural relationships (e.g., primary vs. auxiliary elements), leading to local structural collapse problem. To address this, inspired by Feature Integration Theory (FIT), we propose FITRep, the first attention-guided, white-box item representation framework for fine-grained item deduplication. FITRep consists of: (1) Concept Hierarchical Information Extraction (CHIE), using MLLMs to extract hierarchical semantic concepts; (2) Structure-Preserving Dimensionality Reduction (SPDR), an adaptive UMAP-based method for efficient information compression; and (3) FAISS-Based Clustering (FBC), a FAISS-based clustering that assigns each item a unique cluster id using FAISS. Deployed on Meituan's advertising system, FITRep achieves +3.60% CTR and +4.25% CPM gains in online A/B tests, demonstrating both effectiveness and real-world impact.",
        "translated": "在线平台通常因视觉与文本相似的近重复物料导致用户体验下降。尽管多模态大语言模型（MLLMs）能够生成多模态嵌入，现有方法将这些表示视为黑箱，忽视了结构关系（如主元素与辅助元素），从而引发局部结构坍塌问题。为解决该问题，受特征整合理论（FIT）启发，我们提出 FITRep——首个面向细粒度物料去重的、基于注意力引导的白盒物料表征框架。FITRep 包含三部分：(1) 概念层次信息提取（CHIE），利用 MLLMs 提取分层语义概念；(2) 结构保持降维（SPDR），一种基于自适应 UMAP 的高效信息压缩方法；以及 (3) 基于 FAISS 的聚类（FBC），通过 FAISS 为每件物料分配唯一聚类 ID。在美团广告系统中部署后，FITRep 在线上 A/B 测试中分别实现 +3.60% 点击率（CTR）和 +4.25% CPM 提升，验证了其有效性与实际应用价值。",
        "translated_title": "FITRep：基于大语言模型的注意力引导物料表示",
        "label": [
            "多模态推荐",
            "召回"
        ],
        "label_reason": "解决推荐中去重问题，优化召回阶段表示",
        "relevance_score": 7,
        "novelty_score": 8,
        "novelty_reason": "首次引入FIT理论指导多模态物品表征"
    },
    {
        "title": "Beyond Patch Aggregation: 3-Pass Pyramid Indexing for Vision-Enhanced Document Retrieval",
        "url": "http://arxiv.org/abs/2511.21121v1",
        "pub_date": "2025-11-26",
        "summary": "Document centric RAG pipelines usually begin with OCR, followed by brittle heuristics for chunking, table parsing, and layout reconstruction. These text first workflows are costly to maintain, sensitive to small layout shifts, and often lose the spatial cues that contain the answer. Vision first retrieval has emerged as a strong alternative. By operating directly on page images, systems like ColPali and ColQwen preserve structure and reduce pipeline complexity while achieving strong benchmark performance. However, these late interaction models tie retrieval to a specific vision backbone and require storing hundreds of patch embeddings per page, creating high memory overhead and complicating large scale deployment.   We introduce VisionRAG, a multimodal retrieval system that is OCR free and model agnostic. VisionRAG indexes documents directly as images, preserving layout, tables, and spatial cues, and builds semantic vectors without committing to a specific extraction. Our three pass pyramid indexing framework creates vectors using global page summaries, section headers, visual hotspots, and fact level cues. These summaries act as lightweight retrieval surrogates. At query time, VisionRAG retrieves the most relevant pages using the pyramid index, then forwards the raw page image encoded as base64 to a multimodal LLM for final question answering. During retrieval, reciprocal rank fusion integrates signals across the pyramid to produce robust ranking.   VisionRAG stores only 17 to 27 vectors per page, matching the efficiency of patch based methods while staying flexible across multimodal encoders. On financial document benchmarks, it achieves 0.8051 accuracy at 10 on FinanceBench and 0.9629 recall at 100 on TAT DQA. These results show that OCR free, summary guided multimodal retrieval is a practical and scalable alternative to traditional text extraction pipelines.",
        "translated": "以文档为中心的RAG管道通常始于OCR，随后依赖脆弱的启发式方法进行分块、表格解析和版面重建。这些“先文本”的工作流维护成本高昂，对微小的版面变化敏感，并常常丢失包含答案的空间线索。视觉优先的检索方法已逐渐成为强有力的替代方案。通过直接在页面图像上操作，如ColPali和ColQwen等系统能够保留结构信息，在降低管道复杂度的同时实现优异的基准性能。然而，这类晚期交互模型将检索与特定视觉骨干网络绑定，并需为每页存储数百个patch嵌入，导致内存开销巨大，且不利于大规模部署。\n\n我们引入VisionRAG，一种无需OCR且模型无关的多模态检索系统。VisionRAG直接将文档索引为图像，保留布局、表格及空间线索，并在不承诺特定提取方式的前提下构建语义向量。我们的三阶段金字塔索引框架利用全局页面摘要、节标题、视觉热点及事实级线索生成向量。这些摘要充当轻量级检索代理。查询时，VisionRAG首先使用金字塔索引检索最相关的页面，再将原始页面图像编码为base64格式传递给多模态大语言模型（LLM）进行最终问答。在检索过程中，互逆排名融合技术整合金字塔各层级信号，生成稳健排序结果。\n\nVisionRAG每页仅存储17至27个向量，兼顾了基于patch的方法效率，同时保持对多种多模态编码器的高度兼容性。在金融文档基准测试中，其在FinanceBench 10位准确率上达到0.8051，在TAT DQA 100位召回率上达0.9629。这些结果表明，无需OCR、由摘要引导的多模态检索是一种切实可行且可扩展的传统文本提取管道的有效替代方案。",
        "translated_title": "超越补丁聚合：面向视觉增强文档检索的三阶段金字塔索引",
        "label": [
            "多模态推荐",
            "通用推荐技术"
        ],
        "label_reason": "提出视觉增强检索框架，适配RAG场景",
        "relevance_score": 4,
        "novelty_score": 7,
        "novelty_reason": "三阶段金字塔索引创新，提升效率与灵活性"
    },
    {
        "title": "ICPO: Intrinsic Confidence-Driven Group Relative Preference Optimization for Efficient Reinforcement Learning",
        "url": "http://arxiv.org/abs/2511.21005v1",
        "pub_date": "2025-11-26",
        "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) demonstrates significant potential in enhancing the reasoning capabilities of Large Language Models (LLMs). However, existing RLVR methods are often constrained by issues such as coarse-grained rewards, reward noise, and inefficient exploration, which lead to unstable training and entropy collapse. To address this challenge, we propose the Intrinsic Confidence-Driven Group Relative Preference Optimization method (ICPO). The intuition behind it lies in the fact that the probabilities of an LLM generating different responses can inherently and directly reflect its self-assessment of the reasoning process. Inspired by the idea of preference modeling, ICPO calculates a preference advantage score for each response by comparing the relative generation probabilities of multiple responses under the same input prompt, and integrates this score with verifiable rewards to guide the exploration process. We have discovered that the preference advantage score not only alleviates the issues of coarse-grained rewards and reward noise but also effectively curbs overconfident errors, enhances the relative superiority of undervalued high-quality responses, and prevents the model from overfitting to specific strategies, thereby facilitating more thorough exploration. Comprehensive experiments across four general-domain benchmarks and three mathematical benchmarks demonstrate that ICPO steadily boosts reasoning compared to GRPO.",
        "translated": "基于可验证奖励的强化学习（RLVR）在增强大语言模型（LLMs）推理能力方面展现出显著潜力。然而，现有RLVR方法常受粗粒度奖励、奖励噪声及探索效率低下等问题制约，导致训练不稳定与熵坍塌。为应对这一挑战，我们提出内在置信驱动的群体相对偏好优化方法（ICPO）。其核心思想在于：大语言模型生成不同响应的概率本身即可直接反映其对推理过程的自我评估。受偏好建模理念启发，ICPO通过比较同一输入提示下多个响应的相对生成概率，为每个响应计算偏好优势得分，并将该得分与可验证奖励相结合，以引导探索过程。我们发现，偏好优势得分不仅缓解了粗粒度奖励与奖励噪声问题，还能有效抑制过度自信错误，提升被低估的高质量响应的相对优势，并防止模型过度拟合特定策略，从而促进更全面的探索。在四个通用领域基准和三个数学领域基准上的综合实验表明，ICPO相较GRPO持续提升了推理性能。",
        "translated_title": "ICPO：基于内在置信度驱动的组相对偏好优化，用于高效强化学习",
        "label": [],
        "label_reason": "论文聚焦LLM强化学习，非推荐系统相关",
        "relevance_score": 3,
        "novelty_score": 7,
        "novelty_reason": "改进RLVR方法，提升推理能力，非推荐场景"
    },
    {
        "title": "Generating Querying Code from Text for Multi-Modal Electronic Health Record",
        "url": "http://arxiv.org/abs/2511.20904v1",
        "pub_date": "2025-11-25",
        "summary": "Electronic health records (EHR) contain extensive structured and unstructured data, including tabular information and free-text clinical notes. Querying relevant patient information often requires complex database operations, increasing the workload for clinicians. However, complex table relationships and professional terminology in EHRs limit the query accuracy. In this work, we construct a publicly available dataset, TQGen, that integrates both \\textbf{T}ables and clinical \\textbf{T}ext for natural language-to-query \\textbf{Gen}eration. To address the challenges posed by complex medical terminology and diverse types of questions in EHRs, we propose TQGen-EHRQuery, a framework comprising a medical knowledge module and a questions template matching module. For processing medical text, we introduced the concept of a toolset, which encapsulates the text processing module as a callable tool, thereby improving processing efficiency and flexibility. We conducted extensive experiments to assess the effectiveness of our dataset and workflow, demonstrating their potential to enhance information querying in EHR systems.",
        "translated": "电子健康记录（EHR）包含大量结构化与非结构化数据，包括表格信息和自由文本临床笔记。查询相关患者信息通常需要复杂的数据库操作，增加了临床医生的工作负担。然而，EHR中的复杂表关系和专业术语限制了查询的准确性。在本工作中，我们构建了一个公开可用的数据集 TQGen，该数据集整合了 \\textbf{T}ables 和临床 \\textbf{T}ext，用于自然语言到查询的 \\textbf{Gen}eration。为应对 EHR 中复杂的医学术语和多样化的提问类型带来的挑战，我们提出了 TQGen-EHRQuery 框架，该框架包含一个医学知识模块和一个问题模板匹配模块。针对医学文本处理，我们引入了“工具集”概念，将文本处理模块封装为可调用工具，从而提升处理效率与灵活性。我们进行了广泛的实验以评估数据集及工作流的有效性，验证了其增强 EHR 系统中信息查询能力的潜力。",
        "translated_title": "从文本生成查询代码用于多模态电子健康记录",
        "label": [],
        "label_reason": "论文聚焦医疗文本到SQL生成，非推荐系统相关",
        "relevance_score": 2,
        "novelty_score": 5,
        "novelty_reason": "方法属通用NLP任务，无推荐系统创新"
    },
    {
        "title": "E-GEO: A Testbed for Generative Engine Optimization in E-Commerce",
        "url": "http://arxiv.org/abs/2511.20867v1",
        "pub_date": "2025-11-25",
        "summary": "With the rise of large language models (LLMs), generative engines are becoming powerful alternatives to traditional search, reshaping retrieval tasks. In e-commerce, for instance, conversational shopping agents now guide consumers to relevant products. This shift has created the need for generative engine optimization (GEO)--improving content visibility and relevance for generative engines. Yet despite its growing importance, current GEO practices are ad hoc, and their impacts remain poorly understood, especially in e-commerce. We address this gap by introducing E-GEO, the first benchmark built specifically for e-commerce GEO. E-GEO contains over 7,000 realistic, multi-sentence consumer product queries paired with relevant listings, capturing rich intent, constraints, preferences, and shopping contexts that existing datasets largely miss. Using this benchmark, we conduct the first large-scale empirical study of e-commerce GEO, evaluating 15 common rewriting heuristics and comparing their empirical performance. To move beyond heuristics, we further formulate GEO as a tractable optimization problem and develop a lightweight iterative prompt-optimization algorithm that can significantly outperform these baselines. Surprisingly, the optimized prompts reveal a stable, domain-agnostic pattern--suggesting the existence of a \"universally effective\" GEO strategy. Our data and code are publicly available at https://github.com/psbagga17/E-GEO.",
        "translated": "随着大语言模型（LLM）的兴起，生成式引擎正逐渐成为传统搜索的强大替代方案，重塑了检索任务。例如，在电子商务领域，对话式购物代理如今引导消费者找到相关商品。这一转变催生了生成式引擎优化（GEO）的需求——即提升内容在生成式引擎中的可见性与相关性。然而，尽管GEO的重要性日益凸显，当前的实践仍属临时性、非系统化，其实际效果也尚不明确，尤其是在电子商务场景中。为填补这一空白，我们提出了E-GEO，这是首个专为电子商务GEO设计的基准数据集。E-GEO包含超过7000条真实、多句式的消费者产品查询及其对应的匹配商品列表，涵盖了现有数据集普遍缺失的丰富意图、约束条件、偏好和购物情境。基于此基准，我们首次开展了大规模实证研究，评估了15种常见的重写启发式方法，并比较了它们的实际表现。为进一步超越启发式方法，我们还将GEO形式化为一个可计算的优化问题，并开发了一种轻量级的迭代提示优化算法，该算法显著优于现有基线方法。令人惊讶的是，优化后的提示展现出一种稳定且跨领域的通用模式——暗示着存在一种“普适有效的”GEO策略。我们的数据与代码已公开发布于 https://github.com/psbagga17/E-GEO。",
        "translated_title": "E-GEO：电子商务生成式引擎优化的测试平台",
        "label": [
            "LLM生成式推荐",
            "通用推荐技术"
        ],
        "label_reason": "聚焦生成式推荐优化，适配电商场景",
        "relevance_score": 8,
        "novelty_score": 7,
        "novelty_reason": "首次构建电商GEO基准并提出迭代提示优化算法"
    },
    {
        "title": "Canvas-to-Image: Compositional Image Generation with Multimodal Controls",
        "url": "http://arxiv.org/abs/2511.21691v1",
        "pub_date": "2025-11-26",
        "summary": "While modern diffusion models excel at generating high-quality and diverse images, they still struggle with high-fidelity compositional and multimodal control, particularly when users simultaneously specify text prompts, subject references, spatial arrangements, pose constraints, and layout annotations. We introduce Canvas-to-Image, a unified framework that consolidates these heterogeneous controls into a single canvas interface, enabling users to generate images that faithfully reflect their intent. Our key idea is to encode diverse control signals into a single composite canvas image that the model can directly interpret for integrated visual-spatial reasoning. We further curate a suite of multi-task datasets and propose a Multi-Task Canvas Training strategy that optimizes the diffusion model to jointly understand and integrate heterogeneous controls into text-to-image generation within a unified learning paradigm. This joint training enables Canvas-to-Image to reason across multiple control modalities rather than relying on task-specific heuristics, and it generalizes well to multi-control scenarios during inference. Extensive experiments show that Canvas-to-Image significantly outperforms state-of-the-art methods in identity preservation and control adherence across challenging benchmarks, including multi-person composition, pose-controlled composition, layout-constrained generation, and multi-control generation.",
        "translated": "尽管现代扩散模型在生成高质量且多样化的图像方面表现出色，但在高保真度的组合控制与多模态控制方面仍存在困难，尤其是在用户同时指定文本提示、主体参考图、空间布局、姿态约束及布局标注等多类控制信号时。我们提出 Canvas-to-Image，一个统一框架，将这些异构控制信号整合至单一画布界面，使用户能够生成忠于其意图的图像。我们的核心思想是将多种控制信号编码为一幅复合画布图像，模型可直接解析该图像以实现一体化的视觉-空间推理。我们进一步构建了一组多任务数据集，并提出了 Multi-Task Canvas Training 训练策略，旨在通过统一学习范式优化扩散模型，使其协同理解并融合异构控制信号，用于文本到图像生成。这种联合训练使 Canvas-to-Image 能在多个控制模态间进行跨模态推理，而非依赖任务特定启发式方法，并在推理阶段对多控制场景具有良好的泛化能力。大量实验表明，Canvas-to-Image 在包括多人组合、姿态控制组合、布局约束生成及多控制生成等具有挑战性的基准测试中，显著优于当前最先进方法，在身份保持与控制遵循方面表现优异。",
        "translated_title": "画布到图像：基于多模态控制的组合式图像生成",
        "label": [],
        "label_reason": "生成新图像，非像素级恢复或增强",
        "relevance_score": 2,
        "novelty_score": 8,
        "novelty_reason": "多模态控制新框架，提升生成一致性"
    },
    {
        "title": "TraceGen: World Modeling in 3D Trace Space Enables Learning from Cross-Embodiment Videos",
        "url": "http://arxiv.org/abs/2511.21690v1",
        "pub_date": "2025-11-26",
        "summary": "Learning new robot tasks on new platforms and in new scenes from only a handful of demonstrations remains challenging. While videos of other embodiments - humans and different robots - are abundant, differences in embodiment, camera, and environment hinder their direct use. We address the small-data problem by introducing a unifying, symbolic representation - a compact 3D \"trace-space\" of scene-level trajectories - that enables learning from cross-embodiment, cross-environment, and cross-task videos. We present TraceGen, a world model that predicts future motion in trace-space rather than pixel space, abstracting away appearance while retaining the geometric structure needed for manipulation. To train TraceGen at scale, we develop TraceForge, a data pipeline that transforms heterogeneous human and robot videos into consistent 3D traces, yielding a corpus of 123K videos and 1.8M observation-trace-language triplets. Pretraining on this corpus produces a transferable 3D motion prior that adapts efficiently: with just five target robot videos, TraceGen attains 80% success across four tasks while offering 50-600x faster inference than state-of-the-art video-based world models. In the more challenging case where only five uncalibrated human demonstration videos captured on a handheld phone are available, it still reaches 67.5% success on a real robot, highlighting TraceGen's ability to adapt across embodiments without relying on object detectors or heavy pixel-space generation.",
        "translated": "仅凭少量示范便在新平台和新场景中学习新的机器人任务仍具挑战性。尽管人类和其他机器人实体的视频资源丰富，但因载体、摄像机与环境差异，这些视频难以直接复用。我们通过引入一种统一的符号化表示——即紧凑型三维“轨迹空间”，来应对小样本问题，该表示支持跨载体、跨环境及跨任务视频的学习。我们提出了 TraceGen，一种世界模型，其预测未来运动轨迹时采用的是轨迹空间而非像素空间，从而抽象掉外观信息，同时保留执行操作所需的几何结构。为大规模训练 TraceGen，我们开发了数据管道 TraceForge，可将异构的人类与机器人视频转化为一致的三维轨迹，最终构建了一个包含 12.3 万条视频和 180 万组观测-轨迹-语言三元组的数据集。在此数据集上预训练后，TraceGen 获得了一种可迁移的三维运动先验，并能高效适应新任务：仅需五段目标机器人视频，TraceGen 即可在四项任务中实现 80% 的成功率，且推理速度比当前最先进的基于视频的世界模型快 50 至 600 倍。在更具挑战性的场景下，若仅有五段未经标定的手持手机拍摄的人类示范视频，则 TraceGen 在真实机器人上的成功率仍可达 67.5%，凸显其无需依赖物体检测器或复杂的像素空间生成即可实现跨载体自适应的能力。",
        "translated_title": "TraceGen：在三维轨迹空间中的世界建模使跨具身视频学习成为可能",
        "label": [],
        "label_reason": "任务为跨具身视频迁移学习，非像素级图像处理",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "提出3D轨迹空间抽象建模，显著提升迁移效率"
    },
    {
        "title": "G$^2$VLM: Geometry Grounded Vision Language Model with Unified 3D Reconstruction and Spatial Reasoning",
        "url": "http://arxiv.org/abs/2511.21688v1",
        "pub_date": "2025-11-26",
        "summary": "Vision-Language Models (VLMs) still lack robustness in spatial intelligence, demonstrating poor performance on spatial understanding and reasoning tasks. We attribute this gap to the absence of a visual geometry learning process capable of reconstructing 3D space from 2D images. We present G$^2$VLM, a geometry grounded vision-language model that bridges two fundamental aspects of spatial intelligence: spatial 3D reconstruction and spatial understanding. G$^2$VLM natively leverages learned 3D visual geometry features to directly predict 3D attributes and enhance spatial reasoning tasks via in-context learning and interleaved reasoning. Our unified design is highly scalable for spatial understanding: it trains on abundant multi-view image and video data, while simultaneously leveraging the benefits of 3D visual priors that are typically only derived from hard-to-collect annotations. Experimental results demonstrate G$^2$VLM is proficient in both tasks, achieving comparable results to state-of-the-art feed-forward 3D reconstruction models and achieving better or competitive results across spatial understanding and reasoning tasks. By unifying a semantically strong VLM with low-level 3D vision tasks, we hope G$^2$VLM can serve as a strong baseline for the community and unlock more future applications, such as 3D scene editing.",
        "translated": "视觉-语言模型（VLMs）在空间智能方面仍缺乏鲁棒性，在空间理解与推理任务中表现不佳。我们归因于其缺少一种能够从二维图像重建三维空间的视觉几何学习过程。本文提出 G$^2$VLM，一种基于几何的视觉-语言模型，旨在弥合空间智能两个核心要素：三维空间重建与空间理解。G$^2$VLM 原生利用所学得的三维视觉几何特征，直接预测三维属性，并通过上下文学习与交错推理增强空间推理能力。我们的统一设计高度可扩展于空间理解任务：其训练数据涵盖大量多视角图像与视频数据，同时充分利用通常仅能从难以采集标注中获得的三维视觉先验知识。实验结果表明，G$^2$VLM 在两项任务中均表现出色，其三维重建性能可媲美当前最先进的前馈式三维重建模型，在空间理解与推理任务中也取得了更优或具有竞争力的结果。通过将语义强健的 VLM 与低层三维视觉任务统一起来，我们希望 G$^2$VLM 能成为社区的一个强大基线，并解锁更多未来应用，例如三维场景编辑。",
        "translated_title": "G²VLM：基于几何约束的视觉语言模型，支持统一的3D重建与空间推理",
        "label": [],
        "label_reason": "聚焦3D重建与空间推理，属高阶视觉任务",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "统一VLM与3D重建架构，提升空间推理能力"
    },
    {
        "title": "Seeing without Pixels: Perception from Camera Trajectories",
        "url": "http://arxiv.org/abs/2511.21681v1",
        "pub_date": "2025-11-26",
        "summary": "Can one perceive a video's content without seeing its pixels, just from the camera trajectory-the path it carves through space? This paper is the first to systematically investigate this seemingly implausible question. Towards this end, we propose a contrastive learning framework to train CamFormer, a dedicated encoder that projects camera pose trajectories into a joint embedding space, aligning them with natural language. We find that, contrary to its apparent simplicity, the camera trajectory is a remarkably informative signal to uncover video content. In other words, \"how you move\" can indeed reveal \"what you are doing\" (egocentric) or \"observing\" (exocentric). We demonstrate the versatility of our learned CamFormer embeddings on a diverse suite of downstream tasks, ranging from cross-modal alignment to classification and temporal analysis. Importantly, our representations are robust across diverse camera pose estimation methods, including both high-fidelity multi-sensored and standard RGB-only estimators. Our findings establish camera trajectory as a lightweight, robust, and versatile modality for perceiving video content.",
        "translated": "能否仅通过相机的运动轨迹——其在空间中划出的路径——就感知到视频的内容？本文首次系统性地探讨了这一看似不切实际的问题。为此，我们提出了一种对比学习框架，用于训练 CamFormer，这是一种专门设计的编码器，可将相机位姿轨迹投影至联合嵌入空间，并与自然语言对齐。我们发现，尽管其表面形式简单，相机轨迹却是一个极具信息量的信号，能够揭示视频内容。换言之，“如何移动”确实可以反映“正在做什么”（第一人称视角）或“正在观察什么”（第三人称视角）。我们在一系列多样化的下游任务中展示了所学得的 CamFormer 嵌入的通用性，涵盖跨模态对齐、分类及时间序列分析等任务。重要的是，我们的表示方法在多种相机位姿估计方法下均表现出鲁棒性，包括高保真多传感器估计器和标准 RGB 估计器。我们的研究结果确立了相机轨迹作为一种轻量级、鲁棒且多功能的模态，可用于感知视频内容。",
        "translated_title": "像素之外的视觉：从相机轨迹中感知",
        "label": [],
        "label_reason": "研究相机轨迹与视频内容关系，非像素级图像处理",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "提出基于轨迹的对比学习框架，跨模态对齐创新"
    },
    {
        "title": "Revolutionizing Glioma Segmentation &amp; Grading Using 3D MRI - Guided Hybrid Deep Learning Models",
        "url": "http://arxiv.org/abs/2511.21673v1",
        "pub_date": "2025-11-26",
        "summary": "Gliomas are brain tumor types that have a high mortality rate which means early and accurate diagnosis is important for therapeutic intervention for the tumors. To address this difficulty, the proposed research will develop a hybrid deep learning model which integrates U-Net based segmentation and a hybrid DenseNet-VGG classification network with multihead attention and spatial-channel attention capabilities. The segmentation model will precisely demarcate the tumors in a 3D volume of MRI data guided by spatial and contextual information. The classification network which combines a branch of both DenseNet and VGG, will incorporate the demarcated tumor on which features with attention mechanisms would be focused on clinically relevant features. High-dimensional 3D MRI data could successfully be utilized in the model through preprocessing steps which are normalization, resampling, and data augmentation. Through a variety of measures the framework is evaluated: measures of performance in segmentation are Dice coefficient and Mean Intersection over Union (IoU) and measures of performance in classification are accuracy precision, recall, and F1-score. The hybrid framework that has been proposed has demonstrated through physical testing that it has the capability of obtaining a Dice coefficient of 98% in tumor segmentation, and 99% on classification accuracy, outperforming traditional CNN models and attention-free methods. Utilizing multi-head attention mechanisms enhances notions of priority in aspects of the tumor that are clinically significant, and enhances interpretability and accuracy. The results suggest a great potential of the framework in facilitating the timely and reliable diagnosis and grading of glioma by clinicians is promising, allowing for better planning of patient treatment.",
        "translated": "胶质瘤是脑部肿瘤的一种，具有高致死率，因此早期且准确的诊断对于肿瘤的治疗干预至关重要。为应对这一挑战，本研究将开发一种混合深度学习模型，该模型整合了基于 U-Net 的分割网络与融合 DenseNet 与 VGG 的分类网络，并具备多头注意力机制和空间-通道注意力能力。分割模型将在 MRI 三维数据中，借助空间与上下文信息精确勾勒肿瘤边界。分类网络结合 DenseNet 与 VGG 两个分支，将聚焦于已勾勒出的肿瘤区域，并通过注意力机制突出临床相关的特征。通过标准化、重采样及数据增强等预处理步骤，高维度的三维 MRI 数据可被有效输入模型。该框架通过多种评估指标进行性能测试：分割任务采用 Dice 系数与平均交并比（IoU）作为评价指标；分类任务则使用准确率、精确率、召回率及 F1 分数。经物理实验验证，所提出的混合框架在肿瘤分割任务中可达到 98% 的 Dice 系数，在分类任务中达到 99% 的准确率，显著优于传统 CNN 模型及无注意力机制的方法。利用多头注意力机制增强了对临床重要肿瘤区域的关注度，同时提升了模型的可解释性与准确性。结果表明，该框架在辅助临床医生及时、可靠地诊断与分级胶质瘤方面具有巨大潜力，有助于优化患者的治疗规划。",
        "translated_title": "利用3D MRI引导的混合深度学习模型革新胶质瘤分割与分级",
        "label": [],
        "label_reason": "属于医学图像分割，非像素级图像恢复任务",
        "relevance_score": 1,
        "novelty_score": 3,
        "novelty_reason": "模型组合常规，无本质创新突破"
    },
    {
        "title": "Uncertainty Quantification for Visual Object Pose Estimation",
        "url": "http://arxiv.org/abs/2511.21666v1",
        "pub_date": "2025-11-26",
        "summary": "Quantifying the uncertainty of an object's pose estimate is essential for robust control and planning. Although pose estimation is a well-studied robotics problem, attaching statistically rigorous uncertainty is not well understood without strict distributional assumptions. We develop distribution-free pose uncertainty bounds about a given pose estimate in the monocular setting. Our pose uncertainty only requires high probability noise bounds on pixel detections of 2D semantic keypoints on a known object. This noise model induces an implicit, non-convex set of pose uncertainty constraints. Our key contribution is SLUE (S-Lemma Uncertainty Estimation), a convex program to reduce this set to a single ellipsoidal uncertainty bound that is guaranteed to contain the true object pose with high probability. SLUE solves a relaxation of the minimum volume bounding ellipsoid problem inspired by the celebrated S-lemma. It requires no initial guess of the bound's shape or size and is guaranteed to contain the true object pose with high probability. For tighter uncertainty bounds at the same confidence, we extend SLUE to a sum-of-squares relaxation hierarchy which is guaranteed to converge to the minimum volume ellipsoidal uncertainty bound for a given set of keypoint constraints. We show this pose uncertainty bound can easily be projected to independent translation and axis-angle orientation bounds. We evaluate SLUE on two pose estimation datasets and a real-world drone tracking scenario. Compared to prior work, SLUE generates substantially smaller translation bounds and competitive orientation bounds. We release code at https://github.com/MIT-SPARK/PoseUncertaintySets.",
        "translated": "量化物体位姿估计的不确定性对于鲁棒控制与规划至关重要。尽管位姿估计是机器人学中研究较为成熟的领域，但在缺乏严格分布假设的情况下，为其赋予统计严谨的不确定性仍不甚清晰。我们针对单目视觉场景，提出一种无需依赖特定概率分布的位姿不确定性上界方法。该方法仅需对已知物体上2D语义关键点像素检测所引入噪声的概率高置信度界限即可。此类噪声模型隐式诱导出一个非凸的位姿不确定性约束集合。本文的核心贡献是SLUE（S-Lemma不确定性估计），它是一个凸规划问题，可将上述非凸集合压缩为一个单一椭球形不确定边界，并保证该边界以高概率覆盖真实物体位姿。SLUE基于著名的S-引理思想，求解最小体积包围椭球问题的松弛形式；其无需预设边界形状或尺度，且保证以高概率包含真实位姿。为进一步在相同置信度下获得更紧致的不确定性边界，我们将SLUE推广至平方和松弛层次结构，该结构被证明能收敛到给定关键点约束下的最小体积椭球形不确定性边界。我们展示了该位姿不确定性边界可轻松投影至独立的平移和轴角方向边界。我们在两个位姿估计数据集及一个现实无人机跟踪场景中评估了SLUE的效果。相较于现有方法，SLUE生成的平移边界显著更小，方向边界表现也具有竞争力。相关代码已开源于 https://github.com/MIT-SPARK/PoseUncertaintySets。",
        "translated_title": "视觉目标姿态估计中的不确定性量化",
        "label": [],
        "label_reason": "任务为姿态估计，属高阶视觉任务",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "提出SLUE新方法优化不确定性边界"
    },
    {
        "title": "Attention-Guided Patch-Wise Sparse Adversarial Attacks on Vision-Language-Action Models",
        "url": "http://arxiv.org/abs/2511.21663v1",
        "pub_date": "2025-11-26",
        "summary": "In recent years, Vision-Language-Action (VLA) models in embodied intelligence have developed rapidly. However, existing adversarial attack methods require costly end-to-end training and often generate noticeable perturbation patches. To address these limitations, we propose ADVLA, a framework that directly applies adversarial perturbations on features projected from the visual encoder into the textual feature space. ADVLA efficiently disrupts downstream action predictions under low-amplitude constraints, and attention guidance allows the perturbations to be both focused and sparse. We introduce three strategies that enhance sensitivity, enforce sparsity, and concentrate perturbations. Experiments demonstrate that under an $L_{\\infty}=4/255$ constraint, ADVLA combined with Top-K masking modifies less than 10% of the patches while achieving an attack success rate of nearly 100%. The perturbations are concentrated on critical regions, remain almost imperceptible in the overall image, and a single-step iteration takes only about 0.06 seconds, significantly outperforming conventional patch-based attacks. In summary, ADVLA effectively weakens downstream action predictions of VLA models under low-amplitude and locally sparse conditions, avoiding the high training costs and conspicuous perturbations of traditional patch attacks, and demonstrates unique effectiveness and practical value for attacking VLA feature spaces.",
        "translated": "近年来，具身智能中的视觉-语言-动作（VLA）模型发展迅速。然而，现有对抗攻击方法通常需要高昂的端到端训练成本，并常生成明显可见的扰动块。为解决上述局限性，我们提出了ADVLA框架，该框架直接在视觉编码器投影至文本特征空间的特征上施加对抗扰动。ADVLA能够在低幅值约束下高效破坏下游动作预测，而注意力引导机制使扰动兼具聚焦性和稀疏性。我们引入三种策略以增强敏感性、强制稀疏性并集中扰动分布。实验表明，在 $L_{\\infty}=4/255$ 约束条件下，ADVLA结合Top-K掩码仅修改少于10%的块，即可实现近100%的攻击成功率。扰动集中在关键区域，整体图像几乎不可察觉，且单步迭代仅需约0.06秒，显著优于传统基于块的攻击方法。综上，ADVLA在低幅值与局部稀疏条件下有效削弱VLA模型的下游动作预测能力，避免了传统块攻击的高训练开销和明显扰动问题，展现出对VLA特征空间攻击的独特有效性与实用价值。",
        "translated_title": "面向视觉-语言-动作模型的注意力引导型分块稀疏对抗攻击",
        "label": [],
        "label_reason": "攻击VLA模型，非图像像素级恢复任务",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "新框架提升攻击效率与稀疏性"
    },
    {
        "title": "Multi-Crit: Benchmarking Multimodal Judges on Pluralistic Criteria-Following",
        "url": "http://arxiv.org/abs/2511.21662v1",
        "pub_date": "2025-11-26",
        "summary": "Large multimodal models (LMMs) are increasingly adopted as judges in multimodal evaluation systems due to their strong instruction following and consistency with human preferences. However, their ability to follow diverse, fine-grained evaluation criteria remains underexplored. We develop Multi-Crit, a benchmark for evaluating multimodal judges on their capacity to follow pluralistic criteria and produce reliable criterion-level judgments. Covering both open-ended generation and verifiable reasoning tasks, Multi-Crit is built through a rigorous data curation pipeline that gathers challenging response pairs with multi-criterion human annotations. It further introduces three novel metrics for systematically assessing pluralistic adherence, criterion-switching flexibility, and the ability to recognize criterion-level preference conflicts. Comprehensive analysis of 25 LMMs reveals that 1) proprietary models still struggle to maintain consistent adherence to pluralistic criteria--especially in open-ended evaluation; 2) open-source models lag further behind in flexibly following diverse criteria; and 3) critic fine-tuning with holistic judgment signals enhances visual grounding but fails to generalize to pluralistic criterion-level judgment. Additional analyses on reasoning fine-tuning, test-time scaling, and boundary consistency between open-source and proprietary models further probe the limits of current multimodal judges. As a pioneering study, Multi-Crit lays the foundation for building reliable and steerable multimodal AI evaluation.",
        "translated": "大型多模态模型（LMMs）因其强大的指令遵循能力及与人类偏好高度一致，正日益被应用于多模态评估系统中的评判角色。然而，其对多样、细粒度评估标准的遵循能力尚未得到充分探索。我们构建了 Multi-Crit，一个专门用于评估多模态评判者在遵循多元标准并生成可靠的标准级判断方面能力的基准测试集。该基准涵盖开放式生成与可验证推理两类任务，通过严谨的数据采集流程构建而成，收集了具有多重标准人工标注的挑战性响应配对。此外，Multi-Crit 还引入了三项新颖指标，以系统评估评判者对多元标准的遵循程度、标准切换的灵活性，以及识别标准级偏好冲突的能力。对25个 LMM 的全面分析表明：1）专有模型仍难以保持对多元标准的一致遵循——尤其在开放式评估中表现突出；2）开源模型在灵活遵循多样化标准方面差距更大；3）使用整体判断信号进行批评微调虽能增强视觉接地能力，但无法推广到多元标准级判断。进一步针对推理微调、测试时扩展性以及开源与专有模型间边界一致性展开的分析，进一步揭示了当前多模态评判者的局限性。作为一项开创性研究，Multi-Crit 为构建可靠且可控的多模态 AI 评估体系奠定了基础。",
        "translated_title": "多准则：在多元标准遵循下的多模态评判者基准测试",
        "label": [],
        "label_reason": "研究对象为多模态评价系统，非图像像素级处理",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出新评测基准与指标，但非图像恢复任务"
    },
    {
        "title": "CaFlow: Enhancing Long-Term Action Quality Assessment with Causal Counterfactual Flow",
        "url": "http://arxiv.org/abs/2511.21653v1",
        "pub_date": "2025-11-26",
        "summary": "Action Quality Assessment (AQA) predicts fine-grained execution scores from action videos and is widely applied in sports, rehabilitation, and skill evaluation. Long-term AQA, as in figure skating or rhythmic gymnastics, is especially challenging since it requires modeling extended temporal dynamics while remaining robust to contextual confounders. Existing approaches either depend on costly annotations or rely on unidirectional temporal modeling, making them vulnerable to spurious correlations and unstable long-term representations. To this end, we propose CaFlow, a unified framework that integrates counterfactual de-confounding with bidirectional time-conditioned flow. The Causal Counterfactual Regularization (CCR) module disentangles causal and confounding features in a self-supervised manner and enforces causal robustness through counterfactual interventions, while the BiT-Flow module models forward and backward dynamics with a cycle-consistency constraint to produce smoother and more coherent representations. Extensive experiments on multiple long-term AQA benchmarks demonstrate that CaFlow achieves state-of-the-art performance. Code is available at https://github.com/Harrison21/CaFlow",
        "translated": "动作质量评估（AQA）从动作视频中预测细粒度的执行评分，并广泛应用于体育、康复和技能评价等领域。长期 AQA，如花样滑冰或艺术体操等场景，尤其具有挑战性，因其需要建模延展的时间动态，并同时对上下文干扰因素保持鲁棒性。现有方法要么依赖昂贵的人工标注，要么仅依赖单向时间建模，因而易受虚假相关性影响并导致长期表征不稳定。为此，我们提出 CaFlow，一个统一框架，结合反事实去混杂与双向时间条件流。因果反事实正则化（CCR）模块以自监督方式解耦因果特征与混杂特征，并通过反事实干预强制实现因果鲁棒性；而 BiT-Flow 模块在循环一致性约束下建模前向与后向动态，生成更平滑且连贯的表征。在多个长期 AQA 数据集上的大量实验表明，CaFlow 达到当前最优性能。代码已开源于 https://github.com/Harrison21/CaFlow",
        "translated_title": "CaFlow：利用因果反事实流提升长期动作质量评估",
        "label": [],
        "label_reason": "任务为动作质量评估，属高阶视频理解",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "创新性改进因果建模与双向时序流框架"
    },
    {
        "title": "Continual Error Correction on Low-Resource Devices",
        "url": "http://arxiv.org/abs/2511.21652v1",
        "pub_date": "2025-11-26",
        "summary": "The proliferation of AI models in everyday devices has highlighted a critical challenge: prediction errors that degrade user experience. While existing solutions focus on error detection, they rarely provide efficient correction mechanisms, especially for resource-constrained devices. We present a novel system enabling users to correct AI misclassifications through few-shot learning, requiring minimal computational resources and storage. Our approach combines server-side foundation model training with on-device prototype-based classification, enabling efficient error correction through prototype updates rather than model retraining. The system consists of two key components: (1) a server-side pipeline that leverages knowledge distillation to transfer robust feature representations from foundation models to device-compatible architectures, and (2) a device-side mechanism that enables ultra-efficient error correction through prototype adaptation. We demonstrate our system's effectiveness on both image classification and object detection tasks, achieving over 50% error correction in one-shot scenarios on Food-101 and Flowers-102 datasets while maintaining minimal forgetting (less than 0.02%) and negligible computational overhead. Our implementation, validated through an Android demonstration app, proves the system's practicality in real-world scenarios.",
        "translated": "人工智能模型在日常设备中的广泛应用凸显了一个关键挑战：预测误差会显著降低用户体验。尽管现有解决方案侧重于错误检测，但它们极少提供高效的修正机制，尤其在资源受限的设备上表现不足。我们提出了一种新颖系统，使用户可通过少样本学习纠正AI误分类，该系统仅需极低的计算资源与存储开销。我们的方法结合了服务端基础模型训练与设备端原型分类机制，通过原型更新而非模型重训练实现高效纠错。该系统包含两个核心组件：（1）服务端流水线，利用知识蒸馏技术将基础模型中鲁棒的特征表示迁移至适配设备架构的轻量结构；（2）设备端机制，通过原型自适应实现超高效错误修正。我们在图像分类与目标检测任务中验证了系统的有效性，在Food-101和Flowers-102数据集的一次性场景下实现超过50%的错误修正率，同时保持极低遗忘率（小于0.02%）与几乎可忽略的计算开销。经Android演示应用验证，我们的实现方案证明了该系统在现实场景中的实用性。",
        "translated_title": "低资源设备上的持续错误校正",
        "label": [],
        "label_reason": "聚焦AI误分类修正，非图像像素级处理",
        "relevance_score": 1,
        "novelty_score": 6,
        "novelty_reason": "原型更新机制创新，但属高阶任务"
    },
    {
        "title": "Mechanisms of Non-Monotonic Scaling in Vision Transformers",
        "url": "http://arxiv.org/abs/2511.21635v1",
        "pub_date": "2025-11-26",
        "summary": "Deeper Vision Transformers often perform worse than shallower ones, which challenges common scaling assumptions. Through a systematic empirical analysis of ViT-S, ViT-B, and ViT-L on ImageNet, we identify a consistent three-phase Cliff-Plateau-Climb pattern that governs how representations evolve with depth. We observe that better performance is associated with progressive marginalization of the [CLS] token, originally designed as a global aggregation hub, in favor of distributed consensus among patch tokens. We quantify patterns of information mixing with an Information Scrambling Index, and show that in ViT-L the information-task tradeoff emerges roughly 10 layers later than in ViT-B, and that these additional layers correlate with increased information diffusion rather than improved task performance. Taken together, these results suggest that transformer architectures in this regime may benefit more from carefully calibrated depth that executes clean phase transitions than from simply increasing parameter count. The Information Scrambling Index provides a useful diagnostic for existing models and suggests a potential design target for future architectures. All code is available at: https://github.com/AnanthaPadmanaban-KrishnaKumar/Cliff-Plateau-Climb.",
        "translated": "深层视觉 Transformer 通常表现劣于浅层模型，这挑战了常见的缩放假设。通过对 ImageNet 上 ViT-S、ViT-B 和 ViT-L 的系统性实证分析，我们发现其表征随深度演化的规律呈现一致的三阶段模式：悬崖-平台-攀升。我们观察到，性能提升与 [CLS] 标记（最初设计为全局聚合中心）的逐步边缘化相关联，取而代之的是补丁标记间的分布式共识形成。我们通过信息混杂指数（Information Scrambling Index）量化信息混合的模式，并表明在 ViT-L 中，信息-任务权衡约比 ViT-B 晚出现 10 层，且这些额外层数与信息扩散增强相关，而非任务性能提升。综合来看，这些结果表明，在此范式下，Transformer 架构更受益于经过精心校准的深度结构，使其能执行干净的相变，而非单纯增加参数量。信息混杂指数可作为现有模型的有效诊断工具，并为未来架构的设计提供潜在目标。所有代码见：https://github.com/AnanthaPadmanaban-KrishnaKumar/Cliff-Plateau-Climb。",
        "translated_title": "视觉Transformer中的非单调缩放机制",
        "label": [],
        "label_reason": "研究Transformer深度影响，非图像像素级处理",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "提出信息混杂指数，诊断模型深度优化"
    },
    {
        "title": "Qwen3-VL Technical Report",
        "url": "http://arxiv.org/abs/2511.21631v1",
        "pub_date": "2025-11-26",
        "summary": "We introduce Qwen3-VL, the most capable vision-language model in the Qwen series to date, achieving superior performance across a broad range of multimodal benchmarks. It natively supports interleaved contexts of up to 256K tokens, seamlessly integrating text, images, and video. The model family includes both dense (2B/4B/8B/32B) and mixture-of-experts (30B-A3B/235B-A22B) variants to accommodate diverse latency-quality trade-offs. Qwen3-VL delivers three core pillars: (i) markedly stronger pure-text understanding, surpassing comparable text-only backbones in several cases; (ii) robust long-context comprehension with a native 256K-token window for both text and interleaved multimodal inputs, enabling faithful retention, retrieval, and cross-referencing across long documents and videos; and (iii) advanced multimodal reasoning across single-image, multi-image, and video tasks, demonstrating leading performance on comprehensive evaluations such as MMMU and visual-math benchmarks (e.g., MathVista and MathVision). Architecturally, we introduce three key upgrades: (i) an enhanced interleaved-MRoPE for stronger spatial-temporal modeling across images and video; (ii) DeepStack integration, which effectively leverages multi-level ViT features to tighten vision-language alignment; and (iii) text-based time alignment for video, evolving from T-RoPE to explicit textual timestamp alignment for more precise temporal grounding. Under comparable token budgets and latency constraints, Qwen3-VL achieves superior performance in both dense and Mixture-of-Experts (MoE) architectures. We envision Qwen3-VL serving as a foundational engine for image-grounded reasoning, agentic decision-making, and multimodal code intelligence in real-world workflows.",
        "translated": "我们推出了 Qwen3-VL，这是截至目前 Qwen 系列中能力最强的视觉-语言模型，在广泛的多模态基准测试中均取得卓越性能。该模型原生支持高达 256K tokens 的交错上下文，可无缝融合文本、图像与视频。模型家族包含密集型（2B/4B/8B/32B）和专家混合型（30B-A3B/235B-A22B）两种架构，以适应多样化的延迟-质量权衡需求。Qwen3-VL 提供三大核心能力支柱：（i）显著增强的纯文本理解能力，在多项任务中超越同类纯文本骨干模型；（ii）强大的长上下文理解能力，原生支持 256K tokens 的文本及交错多模态输入窗口，可实现对长文档与视频内容的忠实保留、检索与跨引用；（iii）在单图、多图及视频任务中的高级多模态推理能力，在 MMMU 和视觉数学等综合性评测（如 MathVista 和 MathVision）中表现领先。从架构层面，我们引入三项关键升级：（i）增强型交错-MRoPE，强化图像与视频的空间-时间建模能力；（ii）DeepStack 集成，有效利用多层级 ViT 特征提升视觉-语言对齐效果；（iii）基于文本的时间对齐机制，由 T-RoPE 进化而来，通过显式文本时间戳对齐实现更精准的时序定位。在同等 token 预算与延迟约束下，Qwen3-VL 在密集型与专家混合型（MoE）架构中均展现出更优性能。我们期望 Qwen3-VL 成为真实工作流中图像引导推理、自主决策及多模态代码智能的基础引擎。",
        "translated_title": "Qwen3-VL 技术报告",
        "label": [],
        "label_reason": "属于高阶多模态视觉语言模型，非像素级图像处理",
        "relevance_score": 1,
        "novelty_score": 1,
        "novelty_reason": "复现式架构升级，无低层图像处理创新"
    },
    {
        "title": "Active Learning for GCN-based Action Recognition",
        "url": "http://arxiv.org/abs/2511.21625v1",
        "pub_date": "2025-11-26",
        "summary": "Despite the notable success of graph convolutional networks (GCNs) in skeleton-based action recognition, their performance often depends on large volumes of labeled data, which are frequently scarce in practical settings. To address this limitation, we propose a novel label-efficient GCN model. Our work makes two primary contributions. First, we develop a novel acquisition function that employs an adversarial strategy to identify a compact set of informative exemplars for labeling. This selection process balances representativeness, diversity, and uncertainty. Second, we introduce bidirectional and stable GCN architectures. These enhanced networks facilitate a more effective mapping between the ambient and latent data spaces, enabling a better understanding of the learned exemplar distribution. Extensive evaluations on two challenging skeleton-based action recognition benchmarks reveal significant improvements achieved by our label-efficient GCNs compared to prior work.",
        "translated": "尽管图卷积网络（GCNs）在基于骨架的动作识别任务中取得了显著成功，但其性能通常依赖于大量标注数据，而这些数据在实际场景中往往稀缺。为解决这一局限性，我们提出了一种新颖的低标注需求GCN模型。本工作有两个主要贡献。首先，我们设计了一种新型采样函数，采用对抗策略来识别一组紧凑且具有信息量的标注示例。该选择过程兼顾代表性、多样性和不确定性。其次，我们引入了双向且稳定的GCN架构。这些增强型网络有助于更有效地建立环境空间与潜在空间之间的映射关系，从而更好地理解所学习到的示例分布。在两个具有挑战性的基于骨架的动作识别基准数据集上的广泛评估表明，相较于先前方法，我们的低标注需求GCNs取得了显著性能提升。",
        "translated_title": "基于图卷积网络的动作识别中的主动学习",
        "label": [],
        "label_reason": "属于高阶动作识别任务，非图像像素级处理",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "改进GCN结构与采样策略，但非低层图像处理"
    },
    {
        "title": "ReSAM: Refine, Requery, and Reinforce: Self-Prompting Point-Supervised Segmentation for Remote Sensing Images",
        "url": "http://arxiv.org/abs/2511.21606v1",
        "pub_date": "2025-11-26",
        "summary": "Interactive segmentation models such as the Segment Anything Model (SAM) have demonstrated remarkable generalization on natural images, but perform suboptimally on remote sensing imagery (RSI) due to severe domain shift and the scarcity of dense annotations. To address this, we propose a self-prompting, point-supervised framework that adapts SAM to RSIs using only sparse point annotations. Our method employs a Refine-Requery-Reinforce loop, where coarse pseudo-masks are generated from initial points (Refine), improved with self-constructed box prompts (Requery), and embeddings are aligned across iterations to reduce confirmation bias (Reinforce). Without relying on full-mask supervision, our approach progressively enhances SAM's segmentation quality and domain robustness through self-guided prompt adaptation . We evaluate our proposed method on three RSI benchmark datasets, including WHU, HRSID, and NWPU VHR-10, showing that our method consistently surpasses pretrained SAM and recent point-supervised segmentation methods. Our results demonstrate that self-prompting and semantic alignment provide an efficient path towards scalable, point-level adaptation of foundation segmentation models for remote sensing applications.",
        "translated": "诸如 Segment Anything Model (SAM) 等交互式分割模型在自然图像上展现了卓越的泛化能力，但在遥感影像（RSI）上表现欠佳，主要由于领域偏移严重且密集标注稀缺。为解决该问题，我们提出一种自提示、点监督框架，仅利用稀疏点标注即可将 SAM 适配至遥感影像。我们的方法采用 Refine-Requery-Reinforce 循环机制：首先基于初始点生成粗略伪掩码（Refine），继而通过自构造的框提示进行优化（Requery），并在各轮迭代间对嵌入进行语义对齐以减轻确认偏倚（Reinforce）。无需依赖全掩码监督，我们的方法通过自引导的提示自适应策略逐步提升 SAM 的分割质量与领域鲁棒性。我们在三个遥感影像基准数据集（WHU、HRSID 和 NWPU VHR-10）上评估了所提方法，结果表明其持续优于预训练 SAM 及近期的点监督分割方法。实验结果证明，自提示与语义对齐为实现面向遥感应用的基础分割模型可扩展的点级自适应提供了一条高效路径。",
        "translated_title": "ReSAM：细化、重查询与强化：面向遥感图像的自提示点监督分割",
        "label": [],
        "label_reason": "任务为语义分割，属 high-level 视觉任务",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出自提示框架改进SAM，但非像素级图像恢复"
    },
    {
        "title": "MoGAN: Improving Motion Quality in Video Diffusion via Few-Step Motion Adversarial Post-Training",
        "url": "http://arxiv.org/abs/2511.21592v1",
        "pub_date": "2025-11-26",
        "summary": "Video diffusion models achieve strong frame-level fidelity but still struggle with motion coherence, dynamics and realism, often producing jitter, ghosting, or implausible dynamics. A key limitation is that the standard denoising MSE objective provides no direct supervision on temporal consistency, allowing models to achieve low loss while still generating poor motion. We propose MoGAN, a motion-centric post-training framework that improves motion realism without reward models or human preference data. Built atop a 3-step distilled video diffusion model, we train a DiT-based optical-flow discriminator to differentiate real from generated motion, combined with a distribution-matching regularizer to preserve visual fidelity. With experiments on Wan2.1-T2V-1.3B, MoGAN substantially improves motion quality across benchmarks. On VBench, MoGAN boosts motion score by +7.3% over the 50-step teacher and +13.3% over the 3-step DMD model. On VideoJAM-Bench, MoGAN improves motion score by +7.4% over the teacher and +8.8% over DMD, while maintaining comparable or even better aesthetic and image-quality scores. A human study further confirms that MoGAN is preferred for motion quality (52% vs. 38% for the teacher; 56% vs. 29% for DMD). Overall, MoGAN delivers significantly more realistic motion without sacrificing visual fidelity or efficiency, offering a practical path toward fast, high-quality video generation. Project webpage is: https://xavihart.github.io/mogan.",
        "translated": "视频扩散模型在帧级保真度上表现优异，但仍难以实现运动连贯性、动态真实性与整体真实感，常产生抖动、鬼影或不合逻辑的动态效果。其关键限制在于，标准去噪MSE损失目标未对时间一致性提供直接监督，导致模型可在损失较低的同时仍输出质量不佳的运动序列。我们提出MoGAN，一种以运动为中心的后训练框架，无需奖励模型或人类偏好数据即可提升运动真实感。该框架构建于一个三步蒸馏视频扩散模型之上，通过训练基于DiT的光流判别器以区分真实与生成运动，并结合分布匹配正则化项以保留视觉保真度。在Wan2.1-T2V-1.3B模型上的实验表明，MoGAN显著提升了各类基准测试中的运动质量：在VBench上，MoGAN相较50步教师模型提升运动得分7.3%，相较3步DMD模型提升13.3%；在VideoJAM-Bench上，MoGAN相较教师模型提升7.4%，相较DMD模型提升8.8%，同时保持相当甚至更优的美学与图像质量评分。一项人类评测进一步证实，MoGAN在运动质量方面更受青睐（教师模型为38%，MoGAN为52%；DMD模型为29%，MoGAN为56%）。总体而言，MoGAN在不牺牲视觉保真度或效率的前提下，显著提升了运动的真实性，为快速高质量视频生成提供了一条切实可行的路径。项目网页地址为：https://xavihart.github.io/mogan。",
        "translated_title": "MoGAN：通过少量步数的运动对抗后训练提升视频扩散中的运动质量",
        "label": [],
        "label_reason": "聚焦视频生成运动质量，非像素级图像恢复任务",
        "relevance_score": 3,
        "novelty_score": 7,
        "novelty_reason": "提出基于光流判别器的动态度量改进框架"
    },
    {
        "title": "Video-R2: Reinforcing Consistent and Grounded Reasoning in Multimodal Language Models",
        "url": "http://arxiv.org/abs/2511.23478v1",
        "pub_date": "2025-11-28",
        "summary": "Reasoning over dynamic visual content remains a central challenge for multimodal large language models. Recent thinking models generate explicit reasoning traces for interpretability; however, their reasoning often appears convincing while being logically inconsistent or weakly grounded in visual evidence. We identify and formalize these issues through two diagnostic metrics: Think Answer Consistency (TAC), which measures the alignment between reasoning and answers, and Video Attention Score (VAS), which captures the extent to which reasoning depends on visual versus textual cues. Analysis across 11 video reasoning benchmarks shows that current models rely heavily on linguistic priors rather than visual content. To address this, we propose a reinforcement learning approach that enhances both temporal precision and reasoning consistency. Our approach combines timestamp aware supervised fine tuning with Group Relative Policy Optimization (GRPO) guided by a novel Temporal Alignment Reward (TAR). This dual step post training stage encourages temporally aligned and causally coherent video reasoning. The resulting model, Video R2, achieves consistently higher TAC, VAS, and accuracy across multiple benchmarks, demonstrating that improvements in temporal alignment and reasoning coherence lead to more accurate and trustworthy video understanding. Our code, dataset, and model will be open sourced.",
        "translated": "对动态视觉内容的推理仍是多模态大语言模型的核心挑战。近期的思维模型为增强可解释性生成显式推理轨迹；然而，其推理过程往往看似具有说服力，但逻辑上不一致或与视觉证据关联薄弱。我们通过两个诊断指标识别并形式化了上述问题：Think Answer Consistency（TAC），用于衡量推理与答案的一致性；Video Attention Score（VAS），用于量化推理依赖视觉线索还是文本线索的程度。在11个视频推理基准上的分析表明，当前模型严重依赖语言先验，而非视觉内容。为解决此问题，我们提出一种强化学习方法，以提升时序精度和推理一致性。该方法结合时间戳感知监督微调与基于新颖Temporal Alignment Reward（TAR）引导的Group Relative Policy Optimization（GRPO）。这一双阶段后训练机制鼓励视频推理在时间维度上对齐且因果关系连贯。所得模型Video R2在多个基准测试中持续取得更高的TAC、VAS及准确率，证明时序对齐度与推理连贯性的提升可带来更精确、更可靠的视频理解能力。我们的代码、数据集与模型将开源。",
        "translated_title": "Video-R2：强化多模态语言模型中一致且 grounded 的推理",
        "label": [],
        "label_reason": "高阶视频理解任务，非像素级图像处理",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "强化学习优化视频推理一致性，方法新颖"
    },
    {
        "title": "Video-CoM: Interactive Video Reasoning via Chain of Manipulations",
        "url": "http://arxiv.org/abs/2511.23477v1",
        "pub_date": "2025-11-28",
        "summary": "Recent multimodal large language models (MLLMs) have advanced video understanding, yet most still \"think about videos\" ie once a video is encoded, reasoning unfolds entirely in text, treating visual input as a static context. This passive paradigm creates a semantic bottleneck: models cannot rewatch, refocus, or verify evidence, leading to shallow visual reasoning on tasks requiring fine grained spatio temporal understanding. In this work, we introduce Interactive Video Reasoning, a new paradigm that transforms video into an active cognitive workspace, enabling models to \"think with videos\". Our model, Video CoM, reasons through a Chain of Manipulations (CoM), performing iterative visual actions to gather and refine evidence. To support this behavior, we construct Video CoM Instruct, an 18K instruction tuning dataset curated for multi step manipulation reasoning. Beyond supervised learning, we further optimize the manipulation policy via reinforcement learning with reasoning aware Group Relative Policy Optimization (GRPO). Unlike prior work that relies solely on sparse answer rewards, our method introduces step level reasoning rewards, guiding the model toward grounded and consistent reasoning. Video CoM achieves strong results across nine video reasoning benchmarks, improving average performance by 3.6 percent over recent state of the art models, while training on only 25K SFT and 3K GRPO video samples, significantly fewer than comparable large scale models. Ablation studies demonstrate that reasoning aware rewards improve both accuracy and interpretability. Code: https://github.com/mbzuai-oryx/Video-CoM",
        "translated": "近期的多模态大语言模型（MLLMs）虽已推动视频理解的发展，但大多数仍“以文本思考视频”，即一旦视频被编码，推理过程完全发生在文本层面，将视觉输入视为静态上下文。这种被动范式造成语义瓶颈：模型无法回看、重新聚焦或验证证据，导致在需要精细时空理解的任务中视觉推理流于表面。在本文中，我们提出交互式视频推理（Interactive Video Reasoning），一种将视频转化为主动认知工作空间的新范式，使模型能够“以视频思考”。我们的模型 Video CoM 通过操作链（Chain of Manipulations, CoM）进行推理，执行迭代视觉操作以收集并优化证据。为支持该行为，我们构建了 Video CoM Instruct 数据集，包含18K条针对多步操作推理的指令微调样本。除监督学习外，我们进一步通过强化学习优化操作策略，采用具有推理感知能力的组相对策略优化（Group Relative Policy Optimization, GRPO）。与以往仅依赖稀疏答案奖励的方法不同，我们的方法引入步骤级推理奖励，引导模型实现接地且一致的推理。Video CoM 在九个视频推理基准测试中取得优异表现，相较最新最先进模型平均性能提升3.6%，且仅需25K条SFT和3K条GRPO视频样本进行训练，远少于可比规模的大模型。消融研究表明，推理感知奖励同时提升了准确率与可解释性。代码：https://github.com/mbzuai-oryx/Video-CoM",
        "translated_title": "Video-CoM：通过操作链实现的交互式视频推理",
        "label": [],
        "label_reason": "属于高阶视频理解任务，非像素级图像处理",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "引入交互式推理与强化学习奖励机制，显著提升性能"
    },
    {
        "title": "AnyTalker: Scaling Multi-Person Talking Video Generation with Interactivity Refinement",
        "url": "http://arxiv.org/abs/2511.23475v1",
        "pub_date": "2025-11-28",
        "summary": "Recently, multi-person video generation has started to gain prominence. While a few preliminary works have explored audio-driven multi-person talking video generation, they often face challenges due to the high costs of diverse multi-person data collection and the difficulty of driving multiple identities with coherent interactivity. To address these challenges, we propose AnyTalker, a multi-person generation framework that features an extensible multi-stream processing architecture. Specifically, we extend Diffusion Transformer's attention block with a novel identity-aware attention mechanism that iteratively processes identity-audio pairs, allowing arbitrary scaling of drivable identities. Besides, training multi-person generative models demands massive multi-person data. Our proposed training pipeline depends solely on single-person videos to learn multi-person speaking patterns and refines interactivity with only a few real multi-person clips. Furthermore, we contribute a targeted metric and dataset designed to evaluate the naturalness and interactivity of the generated multi-person videos. Extensive experiments demonstrate that AnyTalker achieves remarkable lip synchronization, visual quality, and natural interactivity, striking a favorable balance between data costs and identity scalability.",
        "translated": "近年来，多人物视频生成开始受到广泛关注。尽管已有少数初步研究探索了音频驱动的多人物对话视频生成，但它们往往面临两大挑战：多样化的多人物数据采集成本高昂，以及难以通过音频驱动多个身份并保持交互的一致性。为解决上述问题，我们提出了AnyTalker，一个支持多人物生成的框架，其核心特征是具备可扩展的多流处理架构。具体而言，我们对Diffusion Transformer的注意力模块进行了扩展，引入了一种新颖的身份感知注意力机制，该机制能够迭代处理身份-音频配对，从而支持任意数量可驱动的身份。此外，训练多人物生成模型需要海量多人物数据。我们的训练流程仅依赖单人视频以学习多人物说话模式，并仅需少量真实的多人物片段即可优化交互一致性。此外，我们贡献了一个专门设计的评估指标和数据集，用于衡量所生成多人物视频的自然度与交互性。大量实验表明，AnyTalker在唇形同步、视觉质量和自然交互方面均表现出色，在数据成本与身份可扩展性之间取得了良好的平衡。",
        "translated_title": "AnyTalker：通过交互优化扩展多人物说话视频生成",
        "label": [],
        "label_reason": "生成多人物视频属高阶视觉任务",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "提出交互优化框架与训练策略显著提升效果"
    },
    {
        "title": "Visual Generation Tuning",
        "url": "http://arxiv.org/abs/2511.23469v1",
        "pub_date": "2025-11-28",
        "summary": "Large Vision Language Models (VLMs) effectively bridge the modality gap through extensive pretraining, acquiring sophisticated visual representations aligned with language. However, it remains underexplored whether these representations, optimized for multimodal understanding tasks, harbor an inherent potential for visual generation. In this paper, we propose VGT, Visual Generation Tuning, a novel paradigm designed to stimulate the underlying capabilities of visual generation within any vision language models. By performing efficient visual generation tuning on well-pretrained VLMs, we significantly mitigate the alignment costs and accelerate the convergence of autoregressive modeling in the continuous space (20x speedup). Specifically, we dismiss the entangled pixel-level VAEs designed for diffusion transformers and formulate VGT-AE through aligning the semantic encoders from pretrained VLMs with the latent representations of pixel decoders. In image reconstruction tasks, we achieve 26.67 PSNR and 0.50 rFID at a 28x compression ratio, outperforming specialized VAEs; in visual generation tasks, we achieve state-of-the-art outcomes among autoregressive models, 0.77 on GenEval and 78.73 on DPG-Bench. Furthermore, our proposed VGT showcases significant scaling promise and is versatile for endowing any VLMs trained for multimodal understanding with the capabilities of visual generation, which paves the new avenue to explore next-generation unified multimodal foundation models. Models and codes are available at https://github.com/hustvl/VGT.",
        "translated": "大型视觉语言模型（VLMs）通过大规模预训练有效弥合了模态间的鸿沟，习得与语言对齐的复杂视觉表征。然而，这些为多模态理解任务优化的表征是否内在蕴含视觉生成潜力，仍鲜有探索。本文提出 VGT（Visual Generation Tuning），一种旨在激发任意视觉语言模型中潜在视觉生成能力的新范式。通过对已充分预训练的 VLMs 进行高效的视觉生成调优，我们显著降低了对齐成本，并加速了连续空间中的自回归建模收敛速度（提速达 20 倍）。具体而言，我们摒弃为扩散 Transformer 设计的纠缠式像素级 VAEs，转而通过将预训练 VLMs 的语义编码器与像素解码器的潜在表示进行对齐，构建了 VGT-AE 模型。在图像重建任务中，我们在 28 倍压缩率下取得了 26.67 PSNR 和 0.50 rFID 的性能，超越专用 VAE；在视觉生成任务中，我们在自回归模型中达到最先进水平：GenEval 上达 0.77，DPG-Bench 上达 78.73。此外，所提出的 VGT 展现出显著的可扩展潜力，且具备通用性——可赋予任何用于多模态理解训练的 VLMs 视觉生成能力，从而开辟探索下一代统一多模态基础模型的新路径。模型与代码开源于 https://github.com/hustvl/VGT。",
        "translated_title": "视觉生成调优",
        "label": [],
        "label_reason": "核心任务为视觉生成，属高阶多模态建模",
        "relevance_score": 3,
        "novelty_score": 8,
        "novelty_reason": "提出新调优范式提升生成效率与质量"
    },
    {
        "title": "Object-Centric Data Synthesis for Category-level Object Detection",
        "url": "http://arxiv.org/abs/2511.23450v1",
        "pub_date": "2025-11-28",
        "summary": "Deep learning approaches to object detection have achieved reliable detection of specific object classes in images. However, extending a model's detection capability to new object classes requires large amounts of annotated training data, which is costly and time-consuming to acquire, especially for long-tailed classes with insufficient representation in existing datasets. Here, we introduce the object-centric data setting, when limited data is available in the form of object-centric data (multi-view images or 3D models), and systematically evaluate the performance of four different data synthesis methods to finetune object detection models on novel object categories in this setting. The approaches are based on simple image processing techniques, 3D rendering, and image diffusion models, and use object-centric data to synthesize realistic, cluttered images with varying contextual coherence and complexity. We assess how these methods enable models to achieve category-level generalization in real-world data, and demonstrate significant performance boosts within this data-constrained experimental setting.",
        "translated": "深度学习方法在图像中特定目标类别的检测方面已取得可靠成果。然而，将模型的检测能力扩展到新目标类别时，需要大量标注训练数据，而获取此类数据成本高昂且耗时，尤其对于现有数据集中代表性不足的长尾类别而言。为此，我们引入了以对象为中心的数据设置——当可用数据形式为以对象为中心的数据（如多视角图像或3D模型）且数量有限时，并系统评估了四种不同的数据合成方法在该设置下对新型目标类别进行模型微调的效果。这些方法基于简单的图像处理技术、3D渲染和图像扩散模型，利用以对象为中心的数据合成具有不同上下文连贯性和复杂度的真实、杂乱图像。我们评估了这些方法如何使模型在真实世界数据中实现类别级泛化，并证明了在此数据受限的实验环境下性能显著提升。",
        "translated_title": "面向类别级目标检测的以对象为中心的数据合成",
        "label": [],
        "label_reason": "目标检测属高阶视觉任务，非像素级图像处理",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出数据合成方法提升检测泛化能力"
    },
    {
        "title": "Physics-Informed Neural Networks for Thermophysical Property Retrieval",
        "url": "http://arxiv.org/abs/2511.23449v1",
        "pub_date": "2025-11-28",
        "summary": "Inverse heat problems refer to the estimation of material thermophysical properties given observed or known heat diffusion behaviour. Inverse heat problems have wide-ranging uses, but a critical application lies in quantifying how building facade renovation reduces thermal transmittance, a key determinant of building energy efficiency. However, solving inverse heat problems with non-invasive data collected in situ is error-prone due to environmental variability or deviations from theoretically assumed conditions. Hence, current methods for measuring thermal conductivity are either invasive, require lengthy observation periods, or are sensitive to environmental and experimental conditions. Here, we present a PINN-based iterative framework to estimate the thermal conductivity k of a wall from a set of thermographs; our framework alternates between estimating the forward heat problem with a PINN for a fixed k, and optimizing k by comparing the thermographs and surface temperatures predicted by the PINN, repeating until the estimated k's convergence. Using both environmental data captured by a weather station and data generated from Finite-Volume-Method software simulations, we accurately predict k across different environmental conditions and data collection sampling times, given the temperature profile of the wall at dawn is close to steady state. Although violating the steady-state assumption impacts the accuracy of k's estimation, we show that our proposed framework still only exhibits a maximum MAE of 4.0851. Our work demonstrates the potential of PINN-based methods for reliable estimation of material properties in situ and under realistic conditions, without lengthy measurement campaigns. Given the lack of research on using machine learning, and more specifically on PINNs, for solving in-situ inverse problems, we expect our work to be a starting point for more research on the topic.",
        "translated": "逆热问题是指在已知或观测到的热扩散行为条件下，估计材料的热物理性质。逆热问题具有广泛的应用场景，但其关键应用之一在于量化建筑立面改造对热传递系数（thermal transmittance）的影响，而热传递系数是决定建筑能效的关键参数。然而，在现场采集非侵入式数据以求解逆热问题时，由于环境变量波动或偏离理论假设条件，往往导致结果误差较大。因此，当前用于测量导热系数的方法要么需要侵入性操作，要么需长时间观测周期，或者对环境和实验条件敏感。在此，我们提出了一种基于物理信息神经网络（PINN）的迭代框架，通过一组热成像图来估算墙体的导热系数 k；该框架交替进行两步：首先固定 k 值，利用 PINN 求解正向热传导问题；然后通过比较 PINN 预测的热成像图与表面温度与实际观测值之间的差异，优化 k 值，并重复此过程直至估算的 k 值收敛。我们在气象站采集的环境数据和有限体积法软件仿真生成的数据上进行测试，当墙体在黎明时段的温度分布接近稳态时，本方法可在不同环境条件及采样时间下准确预测 k 值。尽管违反稳态假设会影响 k 的估计精度，但我们证明所提出的框架仍仅表现出最大平均绝对误差（MAE）为 4.0851。本研究展示了基于 PINN 方法在真实环境下无需漫长测量即可可靠估算材料属性的巨大潜力。鉴于目前尚缺乏针对利用机器学习（尤其是 PINN）解决现场逆问题的研究，我们期望本文将成为该领域进一步研究的起点。",
        "translated_title": "基于物理信息的神经网络用于热物理性质反演",
        "label": [],
        "label_reason": "非图像处理任务，属物理参数反演建模",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出PINN迭代框架解决热传导反问题"
    },
    {
        "title": "Hunyuan-GameCraft-2: Instruction-following Interactive Game World Model",
        "url": "http://arxiv.org/abs/2511.23429v1",
        "pub_date": "2025-11-28",
        "summary": "Recent advances in generative world models have enabled remarkable progress in creating open-ended game environments, evolving from static scene synthesis toward dynamic, interactive simulation. However, current approaches remain limited by rigid action schemas and high annotation costs, restricting their ability to model diverse in-game interactions and player-driven dynamics. To address these challenges, we introduce Hunyuan-GameCraft-2, a new paradigm of instruction-driven interaction for generative game world modeling. Instead of relying on fixed keyboard inputs, our model allows users to control game video contents through natural language prompts, keyboard, or mouse signals, enabling flexible and semantically rich interaction within generated worlds. We formally defined the concept of interactive video data and developed an automated process to transform large-scale, unstructured text-video pairs into causally aligned interactive datasets. Built upon a 14B image-to-video Mixture-of-Experts(MoE) foundation model, our model incorporates a text-driven interaction injection mechanism for fine-grained control over camera motion, character behavior, and environment dynamics. We introduce an interaction-focused benchmark, InterBench, to evaluate interaction performance comprehensively. Extensive experiments demonstrate that our model generates temporally coherent and causally grounded interactive game videos that faithfully respond to diverse and free-form user instructions such as \"open the door\", \"draw a torch\", or \"trigger an explosion\".",
        "translated": "近年来，生成式世界模型的进展显著推动了开放世界游戏环境的构建，其研究范式已从静态场景合成逐步演进为动态、交互式的模拟。然而，当前方法仍受限于刚性的动作框架和高昂的标注成本，难以有效建模游戏中多样化的交互行为与玩家驱动的动力学过程。为此，我们提出Hunyuan-GameCraft-2，一种面向指令驱动交互的生成式游戏世界建模新范式。该模型不再依赖固定的键盘输入，而是允许用户通过自然语言提示、键盘或鼠标信号来控制游戏视频内容，从而在生成的世界中实现灵活且语义丰富的交互。我们正式定义了“交互式视频数据”的概念，并开发了一套自动化流程，将大规模非结构化的文本-视频对转化为因果对齐的交互数据集。本模型基于一个140亿参数的图像到视频混合专家（Mixture-of-Experts, MoE）基础模型，结合文本驱动的交互注入机制，以实现对摄像机运动、角色行为及环境动态的细粒度控制。我们进一步构建了一个聚焦交互性能的基准评测集InterBench，用于全面评估模型能力。大量实验表明，我们的模型能够生成在时间上连贯、因果关系合理的交互式游戏视频，精准响应诸如“开门”、“画一根火把”或“触发爆炸”等多样化、自由形式的用户指令。",
        "translated_title": "Hunyuan-GameCraft-2：指令跟随式交互游戏世界模型",
        "label": [],
        "label_reason": "属于高阶游戏世界生成与交互任务",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "提出指令驱动交互新范式，提升视频生成可控性"
    },
    {
        "title": "DisMo: Disentangled Motion Representations for Open-World Motion Transfer",
        "url": "http://arxiv.org/abs/2511.23428v1",
        "pub_date": "2025-11-28",
        "summary": "Recent advances in text-to-video (T2V) and image-to-video (I2V) models, have enabled the creation of visually compelling and dynamic videos from simple textual descriptions or initial frames. However, these models often fail to provide an explicit representation of motion separate from content, limiting their applicability for content creators. To address this gap, we propose DisMo, a novel paradigm for learning abstract motion representations directly from raw video data via an image-space reconstruction objective. Our representation is generic and independent of static information such as appearance, object identity, or pose. This enables open-world motion transfer, allowing motion to be transferred across semantically unrelated entities without requiring object correspondences, even between vastly different categories. Unlike prior methods, which trade off motion fidelity and prompt adherence, are overfitting to source structure or drifting from the described action, our approach disentangles motion semantics from appearance, enabling accurate transfer and faithful conditioning. Furthermore, our motion representation can be combined with any existing video generator via lightweight adapters, allowing us to effortlessly benefit from future advancements in video models. We demonstrate the effectiveness of our method through a diverse set of motion transfer tasks. Finally, we show that the learned representations are well-suited for downstream motion understanding tasks, consistently outperforming state-of-the-art video representation models such as V-JEPA in zero-shot action classification on benchmarks including Something-Something v2 and Jester. Project page: https://compvis.github.io/DisMo",
        "translated": "近年来，文本到视频（T2V）和图像到视频（I2V）模型的进展，使得从简单的文本描述或初始帧中生成视觉上引人入胜且动态的视频成为可能。然而，这些模型通常无法提供一种与内容分离的显式运动表示，限制了其在内容创作者中的适用性。为解决这一问题，我们提出 DisMo，一种全新的范式，通过图像空间重建目标，直接从原始视频数据中学习抽象的运动表示。我们的表示是通用的，且独立于外观、物体身份或姿态等静态信息。这使得运动能够实现开放世界迁移，在无需物体对应关系的情况下，甚至在语义上完全无关的不同类别之间进行运动迁移。与以往方法相比，后者往往在运动保真度与提示遵从性之间做出权衡，或者过度拟合源结构、偏离所描述的动作，而我们的方法将运动语义与外观解耦，从而实现了精确的迁移与忠实的条件控制。此外，我们的运动表示可通过轻量级适配器与任何现有视频生成器相结合，使我们能够轻松受益于未来视频模型的进步。我们通过一系列多样化的运动迁移任务展示了该方法的有效性。最后，我们证明所学习的表示适用于下游运动理解任务，在 Something-Something v2 和 Jester 等基准测试中的零样本动作分类任务上，持续优于当前最先进的视频表征模型（如 V-JEPA）。项目主页：https://compvis.github.io/DisMo",
        "translated_title": "DisMo：用于开放世界运动迁移的解耦运动表示",
        "label": [],
        "label_reason": "处理视频动作迁移，非像素级图像恢复任务",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "提出解耦运动表征新范式，提升跨域迁移能力"
    },
    {
        "title": "MANTA: Physics-Informed Generalized Underwater Object Tracking",
        "url": "http://arxiv.org/abs/2511.23405v1",
        "pub_date": "2025-11-28",
        "summary": "Underwater object tracking is challenging due to wavelength dependent attenuation and scattering, which severely distort appearance across depths and water conditions. Existing trackers trained on terrestrial data fail to generalize to these physics-driven degradations. We present MANTA, a physics-informed framework integrating representation learning with tracking design for underwater scenarios. We propose a dual-positive contrastive learning strategy coupling temporal consistency with Beer-Lambert augmentations to yield features robust to both temporal and underwater distortions. We further introduce a multi-stage pipeline augmenting motion-based tracking with a physics-informed secondary association algorithm that integrates geometric consistency and appearance similarity for re-identification under occlusion and drift. To complement standard IoU metrics, we propose Center-Scale Consistency (CSC) and Geometric Alignment Score (GAS) to assess geometric fidelity. Experiments on four underwater benchmarks (WebUOT-1M, UOT32, UTB180, UWCOT220) show that MANTA achieves state-of-the-art performance, improving Success AUC by up to 6 percent, while ensuring stable long-term generalized underwater tracking and efficient runtime.",
        "translated": "水下目标跟踪因波长依赖的衰减与散射效应而极具挑战性，这些物理驱动的退化严重扭曲了不同深度和水质条件下的目标外观。现有在陆地数据上训练的跟踪器难以泛化至此类环境。本文提出MANTA，一种融合表征学习与跟踪设计的物理信息引导框架，专用于水下场景。我们提出一种双正样本对比学习策略，结合时序一致性与Beer-Lambert增强，以生成对时序扰动与水下退化均鲁棒的特征。进一步地，我们引入多阶段处理流程，在基于运动的跟踪基础上叠加物理信息引导的二次关联算法，该算法综合几何一致性与外观相似性，在遮挡与漂移条件下实现重识别。为补充标准IoU指标，我们提出中心-尺度一致性（CSC）与几何对齐得分（GAS），用于评估几何保真度。在四个水下基准数据集（WebUOT-1M、UOT32、UTB180、UWCOT220）上的实验表明，MANTA达到当前最优性能，在Success AUC上提升最高达6%，同时确保长期稳定泛化的水下跟踪能力及高效的运行时性能。",
        "translated_title": "MANTA：物理引导的通用水下目标跟踪",
        "label": [],
        "label_reason": "跟踪任务属高阶视觉，非像素级图像恢复",
        "relevance_score": 3,
        "novelty_score": 7,
        "novelty_reason": "引入物理模型与对比学习提升跟踪鲁棒性"
    },
    {
        "title": "VQRAE: Representation Quantization Autoencoders for Multimodal Understanding, Generation and Reconstruction",
        "url": "http://arxiv.org/abs/2511.23386v1",
        "pub_date": "2025-11-28",
        "summary": "Unifying multimodal understanding, generation and reconstruction representation in a single tokenizer remains a key challenge in building unified models. Previous research predominantly attempts to address this in a dual encoder paradigm, e.g., utilizing the separate encoders for understanding and generation respectively or balancing semantic representations and low-level features with contrastive loss. In this paper, we propose VQRAE, a Vector Quantization version of Representation AutoEncoders, which pioneers the first exploration in unified representation to produce Continuous semantic features for image understanding and Discrete tokens for visual generation within a unified tokenizer. Specifically, we build upon pretrained vision foundation models with a symmetric ViT decoder and adopt a two-stage training strategy: first, it freezes the encoder and learns a high-dimensional semantic VQ codebook with pixel reconstruction objective; then jointly optimizes the encoder with self-distillation constraints. This design enables negligible semantic information for maintaining the ability of multimodal understanding, discrete tokens that are compatible for generation and fine-grained reconstruction. Besides, we identify the intriguing property in quantizing semantic encoders that rely on high-dimensional codebook in contrast to the previous common practice of low-dimensional codebook in image reconstruction. The semantic VQ codebook can achieve a 100% utilization ratio at a dimension of 1536. VQRAE presents competitive performance on several benchmarks of visual understanding, generation and reconstruction with promising scaling property in the autoregressive paradigm for its discrete merits.",
        "translated": "在单一词元器中统一多模态理解、生成与重建的表示仍是在构建统一模型时面临的关键挑战。以往研究主要尝试在双编码器范式下解决该问题，例如分别使用独立编码器进行理解与生成，或通过对比损失平衡语义表示与低层特征。本文提出VQRAE（Vector Quantization版本的表示自编码器），首次在统一表示框架内探索连续语义特征用于图像理解，离散词元用于视觉生成，并在同一词元器中实现协调。具体而言，我们基于预训练视觉基础模型，采用对称ViT解码器，并设计两阶段训练策略：首先冻结编码器，以像素重建目标学习高维语义VQ码本；随后联合优化编码器并引入自蒸馏约束。该设计使模型既能保持多模态理解所需的极小语义信息量，又能输出兼容生成与精细重建的离散词元。此外，我们发现，在语义编码器量化过程中，若采用高维码本而非传统图像重建中常用的低维码本，将展现出引人注目的特性。语义VQ码本在维度为1536时即可实现100%利用率。VQRAE在多个视觉理解、生成与重建基准测试中展现出竞争力，并因离散特性而在自回归范式中具备良好的可扩展性潜力。",
        "translated_title": "VQRAE：用于多模态理解、生成与重建的表示量化自编码器",
        "label": [],
        "label_reason": "核心任务为多模态生成与理解，非图像像素级恢复",
        "relevance_score": 2,
        "novelty_score": 8,
        "novelty_reason": "首次统一语义连续特征与离散生成令牌的编码器设计"
    },
    {
        "title": "DEAL-300K: Diffusion-based Editing Area Localization with a 300K-Scale Dataset and Frequency-Prompted Baseline",
        "url": "http://arxiv.org/abs/2511.23377v1",
        "pub_date": "2025-11-28",
        "summary": "Diffusion-based image editing has made semantic level image manipulation easy for general users, but it also enables realistic local forgeries that are hard to localize. Existing benchmarks mainly focus on the binary detection of generated images or the localization of manually edited regions and do not reflect the properties of diffusion-based edits, which often blend smoothly into the original content. We present Diffusion-Based Image Editing Area Localization Dataset (DEAL-300K), a large scale dataset for diffusion-based image manipulation localization (DIML) with more than 300,000 annotated images. We build DEAL-300K by using a multi-modal large language model to generate editing instructions, a mask-free diffusion editor to produce manipulated images, and an active-learning change detection pipeline to obtain pixel-level annotations. On top of this dataset, we propose a localization framework that uses a frozen Visual Foundation Model (VFM) together with Multi Frequency Prompt Tuning (MFPT) to capture both semantic and frequency-domain cues of edited regions. Trained on DEAL-300K, our method reaches a pixel-level F1 score of 82.56% on our test split and 80.97% on the external CoCoGlide benchmark, providing strong baselines and a practical foundation for future DIML research.The dataset can be accessed via https://github.com/ymhzyj/DEAL-300K.",
        "translated": "基于扩散模型的图像编辑已使普通用户能够轻松实现语义级别的图像操作，但同时也使得难以定位的真实局部伪造内容成为可能。现有基准数据集主要关注生成图像的二值检测或人工编辑区域的定位，无法反映基于扩散模型的编辑特性，后者常与原始内容平滑融合。我们提出Diffusion-Based Image Editing Area Localization Dataset (DEAL-300K)，这是一个大规模用于扩散式图像操作定位（DIML）的数据集，包含超过30万张标注图像。我们通过多模态大语言模型生成编辑指令、使用无掩码扩散编辑器生成操作后的图像，并借助主动学习变化检测流程获取像素级标注，构建了DEAL-300K数据集。在该数据集基础上，我们提出一种定位框架，结合冻结的视觉基础模型（VFM）与多频提示微调（MFPT），以同时捕捉编辑区域的语义信息和频域特征。在DEAL-300K上训练后，我们的方法在自建测试集上达到82.56%的像素级F1分数，在外部CoCoGlide基准上达到80.97%，为未来DIML研究提供了强有力的基线和实用基础。该数据集可通过https://github.com/ymhzyj/DEAL-300K访问。",
        "translated_title": "DEAL-300K：基于扩散模型的编辑区域定位方法，结合30万样本规模数据集与频域引导基线",
        "label": [],
        "label_reason": "任务为图像编辑定位，非像素级恢复增强",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出新数据集与频率提示框架，提升定位精度"
    },
    {
        "title": "Optimizing Multimodal Language Models through Attention-based Interpretability",
        "url": "http://arxiv.org/abs/2511.23375v1",
        "pub_date": "2025-11-28",
        "summary": "Modern large language models become multimodal, analyzing various data formats like text and images. While fine-tuning is effective for adapting these multimodal language models (MLMs) to downstream tasks, full fine-tuning is computationally expensive. Parameter-Efficient Fine-Tuning (PEFT) methods address this by training only a small portion of model weights. However, MLMs are difficult to interpret, making it challenging to identify which components are most effective for training to balance efficiency and performance. We propose an attention-based interpretability method for MLMs by analyzing attention scores relative to image tokens. The core idea is to identify attention heads that focus on image key objects. We utilize this information to select optimal model components for PEFT in multimodal models. Our contributions include a method for identifying attention heads associated with image key objects, its application to PEFT for image captioning, and the creation of a new dataset containing images, key object masks, and their textual descriptions. We conducted experiments on MLMs with 2-3 billion parameters to validate the method's effectiveness. By calculating Head Impact (HI) scores we quantify an attention head's focus on key objects, indicating its significance in image understanding. Our fine-tuning experiments demonstrate that adapting layers with the highest HI scores leads to the most significant shifts in metrics compared to pre-trained, randomly selected, or lowest-HI-score layers. This indicates that fine-tuning a small percentage (around 0.01%) of parameters in these crucial layers can substantially influence image understanding capabilities.",
        "translated": "现代大型语言模型已发展为多模态模型，能够分析包括文本和图像在内的多种数据格式。尽管微调对适配这些多模态语言模型（MLMs）至下游任务非常有效，但全参数微调计算成本高昂。参数高效微调（PEFT）方法通过仅训练模型权重的一小部分来应对这一问题。然而，由于多模态语言模型难以解释，识别哪些组件在训练中最具有效性以平衡效率与性能变得困难。我们提出一种基于注意力机制的可解释性方法，通过对图像标记相对注意力分数进行分析来实现。核心思想是识别那些聚焦于图像关键对象的注意力头。我们利用该信息，在多模态模型中选择最合适的模型组件用于PEFT。我们的贡献包括：一种用于识别与图像关键对象相关联注意力头的方法；将其应用于图像字幕生成任务中的PEFT；以及构建一个新数据集，包含图像、关键对象掩码及其文本描述。我们在参数规模为20亿至30亿的多模态语言模型上进行了实验，验证该方法的有效性。通过计算“头部影响”（Head Impact, HI）得分，我们量化了各注意力头对关键对象的关注程度，从而反映其在图像理解中的重要性。我们的微调实验表明，对HI得分最高的层进行微调，相比预训练模型、随机选取层或HI得分最低的层，能带来最大的性能指标提升。这表明，仅对这些关键层中极小比例（约0.01%）的参数进行微调，即可显著影响图像理解能力。",
        "translated_title": "通过基于注意力的可解释性优化多模态语言模型",
        "label": [],
        "label_reason": "属于高阶多模态理解任务，非像素级图像处理",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "方法聚焦注意力机制解释，非图像恢复创新"
    },
    {
        "title": "SimScale: Learning to Drive via Real-World Simulation at Scale",
        "url": "http://arxiv.org/abs/2511.23369v1",
        "pub_date": "2025-11-28",
        "summary": "Achieving fully autonomous driving systems requires learning rational decisions in a wide span of scenarios, including safety-critical and out-of-distribution ones. However, such cases are underrepresented in real-world corpus collected by human experts. To complement for the lack of data diversity, we introduce a novel and scalable simulation framework capable of synthesizing massive unseen states upon existing driving logs. Our pipeline utilizes advanced neural rendering with a reactive environment to generate high-fidelity multi-view observations controlled by the perturbed ego trajectory. Furthermore, we develop a pseudo-expert trajectory generation mechanism for these newly simulated states to provide action supervision. Upon the synthesized data, we find that a simple co-training strategy on both real-world and simulated samples can lead to significant improvements in both robustness and generalization for various planning methods on challenging real-world benchmarks, up to +6.8 EPDMS on navhard and +2.9 on navtest. More importantly, such policy improvement scales smoothly by increasing simulation data only, even without extra real-world data streaming in. We further reveal several crucial findings of such a sim-real learning system, which we term SimScale, including the design of pseudo-experts and the scaling properties for different policy architectures. Our simulation data and code would be released.",
        "translated": "实现完全自动驾驶系统需要在广泛场景下学习理性的决策，包括安全关键和分布外的场景。然而，这些场景在人类专家收集的真实世界数据集中代表性不足。为弥补数据多样性的缺失，我们引入了一种新颖且可扩展的仿真框架，能够在现有驾驶日志基础上合成大量未见过的状态。我们的流程利用先进的神经渲染技术与反应式环境，生成由扰动的自车轨迹控制的高保真多视角观测。此外，我们为这些新合成状态开发了伪专家轨迹生成机制，以提供动作监督。在合成数据的基础上，我们发现仅对真实世界与仿真样本进行简单的协同训练策略，即可显著提升多种规划方法在具有挑战性的现实基准测试中的鲁棒性与泛化能力，分别在 navhard 上提升 +6.8 EPDMS，在 navtest 上提升 +2.9。更重要的是，此类策略改进可通过单纯增加仿真数据规模平滑地扩展，即使无需额外获取真实世界数据流。我们进一步揭示了该模拟-真实学习系统若干关键发现，称之为 SimScale，包括伪专家设计及不同策略架构的扩展特性。我们的仿真数据与代码将予以公开。",
        "translated_title": "SimScale：通过大规模真实世界模拟学习驾驶",
        "label": [],
        "label_reason": "任务为自动驾驶决策，属高阶视觉与规划",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "提出可扩展仿真框架与伪专家机制提升泛化"
    },
    {
        "title": "A Hierarchical Computer Vision Pipeline for Physiological Data Extraction from Bedside Monitors",
        "url": "http://arxiv.org/abs/2511.23355v1",
        "pub_date": "2025-11-28",
        "summary": "In many low-resource healthcare settings, bedside monitors remain standalone legacy devices without network connectivity, creating a persistent interoperability gap that prevents seamless integration of physiological data into electronic health record (EHR) systems. To address this challenge without requiring costly hardware replacement, we present a computer vision-based pipeline for the automated capture and digitisation of vital sign data directly from bedside monitor screens. Our method employs a hierarchical detection framework combining YOLOv11 for accurate monitor and region of interest (ROI) localisation with PaddleOCR for robust text extraction. To enhance reliability across variable camera angles and lighting conditions, a geometric rectification module standardizes the screen perspective before character recognition. We evaluated the system on a dataset of 6,498 images collected from open-source corpora and real-world intensive care units in Vietnam. The model achieved a mean Average Precision (mAP@50-95) of 99.5% for monitor detection and 91.5% for vital sign ROI localisation. The end-to-end extraction accuracy exceeded 98.9% for core physiological parameters, including heart rate, oxygen saturation SpO2, and arterial blood pressure. These results demonstrate that a lightweight, camera-based approach can reliably transform unstructured information from screen captures into structured digital data, providing a practical and scalable pathway to improve information accessibility and clinical documentation in low-resource settings.",
        "translated": "在许多医疗资源匮乏的环境中，床边监护仪仍为独立的老旧设备，缺乏网络连接能力，导致生理数据难以与电子健康记录（EHR）系统无缝集成，从而形成持续存在的互操作性鸿沟。为应对这一挑战且无需更换昂贵硬件，我们提出了一种基于计算机视觉的自动化流程，可直接从床边监护仪屏幕捕获并数字化生命体征数据。该方法采用分层检测框架，结合 YOLOv11 实现对监护仪及感兴趣区域（ROI）的精准定位，并利用 PaddleOCR 提取鲁棒的文本信息。为增强系统在不同摄像头角度和光照条件下的可靠性，引入几何校正模块，在字符识别前统一标准化屏幕视角。我们在一个包含 6,498 张图像的数据集上评估了本系统，该数据集源自开源语料库及越南现实世界重症监护病房的实际采集。模型在监护仪检测任务上的平均精度（mAP@50-95）达到 99.5%，在生命体征 ROI 定位任务中为 91.5%。端到端提取的准确率超过 98.9%，涵盖了心率、血氧饱和度 SpO2 和动脉血压等核心生理参数。实验结果表明，一种轻量级、基于摄像头的方法能够可靠地将屏幕截图中的非结构化信息转化为结构化数字数据，为改善低资源环境下临床信息可及性与病历文档质量提供了一条切实可行且可扩展的路径。",
        "translated_title": "用于从床旁监护仪中提取生理数据的分层计算机视觉流水线",
        "label": [],
        "label_reason": "任务为医学数据提取，非图像像素级恢复",
        "relevance_score": 2,
        "novelty_score": 4,
        "novelty_reason": "组合现有模型，无新范式或架构创新"
    },
    {
        "title": "Flow Straighter and Faster: Efficient One-Step Generative Modeling via MeanFlow on Rectified Trajectories",
        "url": "http://arxiv.org/abs/2511.23342v1",
        "pub_date": "2025-11-28",
        "summary": "Flow-based generative models have recently demonstrated strong performance, yet sampling typically relies on expensive numerical integration of ordinary differential equations (ODEs). Rectified Flow enables one-step sampling by learning nearly straight probability paths, but achieving such straightness requires multiple computationally intensive reflow iterations. MeanFlow achieves one-step generation by directly modeling the average velocity over time; however, when trained on highly curved flows, it suffers from slow convergence and noisy supervision. To address these limitations, we propose Rectified MeanFlow, a framework that models the mean velocity field along the rectified trajectory using only a single reflow step. This eliminates the need for perfectly straightened trajectories while enabling efficient training. Furthermore, we introduce a simple yet effective truncation heuristic that aims to reduce residual curvature and further improve performance. Extensive experiments on ImageNet at 64, 256, and 512 resolutions show that Re-MeanFlow consistently outperforms prior one-step flow distillation and Rectified Flow methods in both sample quality and training efficiency. Code is available at https://github.com/Xinxi-Zhang/Re-MeanFlow.",
        "translated": "基于流的生成模型近年来表现出强大性能，但采样通常依赖于普通微分方程（ODEs）的昂贵数值积分。修正流通过学习近似直线的概率路径实现一步采样，但要达到这种直线性需进行多次计算密集型的重流迭代。MeanFlow通过直接建模时间上的平均速度实现一步生成；然而，当在高度弯曲的流上训练时，其收敛缓慢且监督信号噪声较大。为解决上述局限性，我们提出Rectified MeanFlow框架，该框架仅使用单次重流步骤，即可建模沿修正轨迹的平均速度场。这消除了对完全直线化轨迹的需求，同时实现了高效训练。此外，我们引入一种简单而有效的截断启发式方法，旨在减少残余曲率并进一步提升性能。在ImageNet数据集上于64、256和512分辨率的广泛实验表明，Re-MeanFlow在样本质量与训练效率方面均显著优于先前的一步流蒸馏及修正流方法。代码已开源，详见 https://github.com/Xinxi-Zhang/Re-MeanFlow。",
        "translated_title": "流更直更快：通过校正轨迹上的MeanFlow实现高效的单步生成建模",
        "label": [],
        "label_reason": "生成模型非图像像素级恢复任务",
        "relevance_score": 3,
        "novelty_score": 8,
        "novelty_reason": "改进采样效率，但非低层图像处理"
    },
    {
        "title": "Markovian Scale Prediction: A New Era of Visual Autoregressive Generation",
        "url": "http://arxiv.org/abs/2511.23334v1",
        "pub_date": "2025-11-28",
        "summary": "Visual AutoRegressive modeling (VAR) based on next-scale prediction has revitalized autoregressive visual generation. Although its full-context dependency, i.e., modeling all previous scales for next-scale prediction, facilitates more stable and comprehensive representation learning by leveraging complete information flow, the resulting computational inefficiency and substantial overhead severely hinder VAR's practicality and scalability. This motivates us to develop a new VAR model with better performance and efficiency without full-context dependency. To address this, we reformulate VAR as a non-full-context Markov process, proposing Markov-VAR. It is achieved via Markovian Scale Prediction: we treat each scale as a Markov state and introduce a sliding window that compresses certain previous scales into a compact history vector to compensate for historical information loss owing to non-full-context dependency. Integrating the history vector with the Markov state yields a representative dynamic state that evolves under a Markov process. Extensive experiments demonstrate that Markov-VAR is extremely simple yet highly effective: Compared to VAR on ImageNet, Markov-VAR reduces FID by 10.5% (256 $\\times$ 256) and decreases peak memory consumption by 83.8% (1024 $\\times$ 1024). We believe that Markov-VAR can serve as a foundation for future research on visual autoregressive generation and other downstream tasks.",
        "translated": "基于下一尺度预测的视觉自回归建模（VAR）已重新激活了自回归视觉生成的研究。尽管其全上下文依赖性——即为下一尺度预测建模所有先前尺度——通过利用完整的信息流，有助于学习更稳定、更全面的表示，但由此产生的计算效率低下和显著的开销严重制约了 VAR 的实用性和可扩展性。这促使我们开发一种无需全上下文依赖却具有良好性能与效率的新 VAR 模型。为此，我们将 VAR 重新表述为非全上下文马尔可夫过程，提出 Markov-VAR。该方法通过马尔可夫尺度预测实现：将每个尺度视为一个马尔可夫状态，并引入滑动窗口，将部分先前尺度压缩为一个紧凑的历史向量，以弥补因非全上下文依赖导致的历史信息损失。将历史向量与马尔可夫状态结合，得到一个代表性的动态状态，该状态在马尔可夫过程中演化。大量实验表明，Markov-VAR 极其简洁却高度有效：相比 ImageNet 上的 VAR，Markov-VAR 在 256×256 分辨率下 FID 下降 10.5%，在 1024×1024 分辨率下峰值内存消耗降低 83.8%。我们认为，Markov-VAR 可作为未来视觉自回归生成及其他下游任务研究的基础。",
        "translated_title": "马尔可夫尺度预测：视觉自回归生成的新纪元",
        "label": [],
        "label_reason": "生成新图像，非像素级恢复或增强",
        "relevance_score": 1,
        "novelty_score": 9,
        "novelty_reason": "提出Markovian Scale Prediction新范式，显著提升效率"
    },
    {
        "title": "UniGeoSeg: Towards Unified Open-World Segmentation for Geospatial Scenes",
        "url": "http://arxiv.org/abs/2511.23332v1",
        "pub_date": "2025-11-28",
        "summary": "Instruction-driven segmentation in remote sensing generates masks from guidance, offering great potential for accessible and generalizable applications. However, existing methods suffer from fragmented task formulations and limited instruction data, hindering effective understanding and generalization. To address these issues, we introduce GeoSeg-1M, the first million-scale dataset for remote sensing instruction-driven segmentation, constructed via an automatic mask filtering and instruction generation pipeline that synthesizes referring, interactive, and reasoning segmentation instructions from multiple public datasets. GeoSeg-1M contains 590K images, 117 categories, and 1.1M image-mask-instruction triplets. Building upon this foundation, we further curate GeoSeg-Bench, a challenging benchmark designed to evaluate contextual understanding and reasoning capabilities across diverse instruction-driven tasks and complex geospatial scenes. Furthermore, we present UniGeoSeg, a unified framework that serves as a strong baseline, incorporating task-aware text enhancement, latent knowledge memory, and a progressive training strategy to facilitate multi-task learning. Extensive experiments demonstrate the state-of-the-art performance of UniGeoSeg across GeoSeg-Bench and diverse public benchmarks, while exhibiting strong zero-shot generalization. Datasets and source code were released at https://github.com/MiliLab/UniGeoSeg.",
        "translated": "遥感图像中的指令驱动分割方法根据引导信息生成掩码，为可访问性和通用化应用提供了巨大潜力。然而，现有方法受限于任务表述碎片化及指令数据匮乏，阻碍了其有效理解与泛化能力。为解决上述问题，我们提出了GeoSeg-1M，这是首个百万级规模的遥感指令驱动分割数据集。该数据集通过自动掩码筛选与指令生成流程构建，从多个公开数据集中合成指代式、交互式和推理式分割指令。GeoSeg-1M包含59万张图像、117个类别，以及110万组图像-掩码-指令三元组。在此基础上，我们进一步构建了GeoSeg-Bench基准测试集，旨在评估模型在多样化的指令驱动任务及复杂地理空间场景中对上下文理解和推理能力的表现。此外，我们提出UniGeoSeg统一框架，作为强基线方法，融合任务感知文本增强、隐式知识记忆模块与渐进式训练策略，以促进多任务学习。大量实验表明，UniGeoSeg在GeoSeg-Bench及多个公开基准上均达到当前最优性能，并展现出强大的零样本泛化能力。相关数据集与源代码已开源至https://github.com/MiliLab/UniGeoSeg。",
        "translated_title": "UniGeoSeg：面向地理场景的统一开放世界分割",
        "label": [],
        "label_reason": "任务为语义分割，属高阶视觉理解",
        "relevance_score": 2,
        "novelty_score": 5,
        "novelty_reason": "框架整合文本增强与多任务学习，创新度中等"
    },
    {
        "title": "A Perceptually Inspired Variational Framework for Color Enhancement",
        "url": "http://arxiv.org/abs/2511.23329v1",
        "pub_date": "2025-11-28",
        "summary": "Basic phenomenology of human color vision has been widely taken as an inspiration to devise explicit color correction algorithms. The behavior of these models in terms of significative image features (such as contrast and dispersion) can be difficult to characterize. To cope with this, we propose to use a variational formulation of color contrast enhancement that is inspired by the basic phenomenology of color perception. In particular, we devise a set of basic requirements to be fulfilled by an energy to be considered as `perceptually inspired', showing that there is an explicit class of functionals satisfying all of them. We single out three explicit functionals that we consider of basic interest, showing similarities and differences with existing models. The minima of such functionals is computed using a gradient descent approach. We also present a general methodology to reduce the computational cost of the algorithms under analysis from ${\\cal O}(N^2)$ to ${\\cal O}(N\\log N)$, being $N$ the number of input pixels.",
        "translated": "人类色觉的基本现象已被广泛用作设计显式色彩校正算法的灵感来源。这些模型在显著图像特征（如对比度与分散性）方面的表现往往难以准确刻画。为应对这一挑战，我们提出一种受色觉基本现象启发的变分色彩对比增强框架。具体而言，我们制定了若干基本要求，以判定某一能量函数是否可视为“感知驱动型”，并证明存在一个明确的功能类满足所有这些条件。我们进一步识别出三个具有基础意义的具体功能形式，并分析它们与现有模型之间的相似性与差异。上述功能的最小值采用梯度下降法进行计算。此外，我们还提出了一种通用方法，将所分析算法的计算复杂度从 ${\\cal O}(N^2)$ 降低至 ${\\cal O}(N\\log N)$，其中 $N$ 为输入像素数量。",
        "translated_title": "一种受感知启发的变分框架用于色彩增强",
        "label": [
            "对比度增强",
            "图像增强"
        ],
        "label_reason": "基于人眼感知的对比度增强，属低层图像处理",
        "relevance_score": 7,
        "novelty_score": 6,
        "novelty_reason": "提出三类感知启发函数，优化计算效率"
    },
    {
        "title": "Toward Automatic Safe Driving Instruction: A Large-Scale Vision Language Model Approach",
        "url": "http://arxiv.org/abs/2511.23311v1",
        "pub_date": "2025-11-28",
        "summary": "Large-scale Vision Language Models (LVLMs) exhibit advanced capabilities in tasks that require visual information, including object detection. These capabilities have promising applications in various industrial domains, such as autonomous driving. For example, LVLMs can generate safety-oriented descriptions of videos captured by road-facing cameras. However, ensuring comprehensive safety requires monitoring driver-facing views as well to detect risky events, such as the use of mobiles while driving. Thus, the ability to process synchronized inputs is necessary from both driver-facing and road-facing cameras. In this study, we develop models and investigate the capabilities of LVLMs by constructing a dataset and evaluating their performance on this dataset. Our experimental results demonstrate that while pre-trained LVLMs have limited effectiveness, fine-tuned LVLMs can generate accurate and safety-aware driving instructions. Nonetheless, several challenges remain, particularly in detecting subtle or complex events in the video. Our findings and error analysis provide valuable insights that can contribute to the improvement of LVLM-based systems in this domain.",
        "translated": "大规模视觉语言模型（LVLMs）在需要视觉信息的任务中展现出先进能力，例如目标检测。这些能力在多个工业领域具有广阔的应用前景，如自动驾驶。例如，LVLMs 可以生成面向安全的视频描述，该视频由面向道路的摄像头捕获。然而，确保全面的安全性还需要监控驾驶员视角，以检测如驾驶时使用手机等风险事件。因此，必须能够处理来自驾驶员视角和道路视角摄像头的同步输入。在本研究中，我们构建了一个数据集，并通过评估其性能来开发模型并探究 LVLMs 的能力。实验结果表明，虽然预训练的 LVLMs 效果有限，但经过微调的 LVLMs 能够生成准确且具备安全意识的驾驶指令。尽管如此，仍存在若干挑战，特别是在检测视频中的细微或复杂事件方面。我们的发现及错误分析为提升此类领域基于 LVLM 的系统性能提供了宝贵见解。",
        "translated_title": "迈向自动驾驶安全指令的自动化：一种大规模视觉语言模型方法",
        "label": [],
        "label_reason": "聚焦高阶视觉语言任务，非像素级图像处理",
        "relevance_score": 1,
        "novelty_score": 3,
        "novelty_reason": "复现与微调为主，创新度低"
    },
    {
        "title": "FACT-GS: Frequency-Aligned Complexity-Aware Texture Reparameterization for 2D Gaussian Splatting",
        "url": "http://arxiv.org/abs/2511.23292v1",
        "pub_date": "2025-11-28",
        "summary": "Realistic scene appearance modeling has advanced rapidly with Gaussian Splatting, which enables real-time, high-quality rendering. Recent advances introduced per-primitive textures that incorporate spatial color variations within each Gaussian, improving their expressiveness. However, texture-based Gaussians parameterize appearance with a uniform per-Gaussian sampling grid, allocating equal sampling density regardless of local visual complexity. This leads to inefficient texture space utilization, where high-frequency regions are under-sampled and smooth regions waste capacity, causing blurred appearance and loss of fine structural detail. We introduce FACT-GS, a Frequency-Aligned Complexity-aware Texture Gaussian Splatting framework that allocates texture sampling density according to local visual frequency. Grounded in adaptive sampling theory, FACT-GS reformulates texture parameterization as a differentiable sampling-density allocation problem, replacing the uniform textures with a learnable frequency-aware allocation strategy implemented via a deformation field whose Jacobian modulates local sampling density. Built on 2D Gaussian Splatting, FACT-GS performs non-uniform sampling on fixed-resolution texture grids, preserving real-time performance while recovering sharper high-frequency details under the same parameter budget.",
        "translated": "随着高斯点渲染（Gaussian Splatting）的发展，真实场景外观建模已实现快速进步，支持实时、高质量的渲染。近期研究引入了每个高斯点内的逐点纹理，将空间色彩变化纳入其中，从而提升了其表达能力。然而，基于纹理的高斯点以均匀采样网格参数化外观，在每个高斯点内分配相等的采样密度，与局部视觉复杂度无关。这导致纹理空间利用效率低下：高频区域采样不足，平滑区域则浪费容量，从而造成外观模糊并丢失精细结构细节。我们提出 FACT-GS（Frequency-Aligned Complexity-aware Texture Gaussian Splatting），一种基于频率感知和局部复杂度自适应的纹理高斯点渲染框架，根据局部视觉频率动态调整纹理采样密度。该方法根植于自适应采样理论，将纹理参数化重构为一个可微分的采样密度分配问题，用一个可学习的频率感知分配策略替代原有均匀纹理，该策略通过变形场的雅可比矩阵调控局部采样密度。FACT-GS 基于二维高斯点渲染构建，在固定分辨率的纹理网格上执行非均匀采样，保持实时性能的同时，在相同参数预算下恢复更锐利的高频细节。",
        "translated_title": "FACT-GS：面向2D高斯光栅化的频域对齐与复杂度感知纹理重参数化",
        "label": [],
        "label_reason": "非图像恢复任务，聚焦渲染优化",
        "relevance_score": 3,
        "novelty_score": 8,
        "novelty_reason": "提出频率感知采样策略提升细节表现"
    },
    {
        "title": "Do LLM-judges Align with Human Relevance in Cranfield-style Recommender Evaluation?",
        "url": "http://arxiv.org/abs/2511.23312v1",
        "pub_date": "2025-11-28",
        "summary": "Evaluating recommender systems remains a long-standing challenge, as offline methods based on historical user interactions and train-test splits often yield unstable and inconsistent results due to exposure bias, popularity bias, sampled evaluations, and missing-not-at-random patterns. In contrast, textual document retrieval benefits from robust, standardized evaluation via Cranfield-style test collections, which combine pooled relevance judgments with controlled setups. While recent work shows that adapting this methodology to recommender systems is feasible, constructing such collections remains costly due to the need for manual relevance judgments, thus limiting scalability. This paper investigates whether Large Language Models (LLMs) can serve as reliable automatic judges to address these scalability challenges. Using the ML-32M-ext Cranfield-style movie recommendation collection, we first examine the limitations of existing evaluation methodologies. Then we explore the alignment and the recommender systems ranking agreement between the LLM-judge and human provided relevance labels. We find that incorporating richer item metadata and longer user histories improves alignment, and that LLM-judge yields high agreement with human-based rankings (Kendall's tau = 0.87). Finally, an industrial case study in the podcast recommendation domain demonstrates the practical value of LLM-judge for model selection. Overall, our results show that LLM-judge is a viable and scalable approach for evaluating recommender systems.",
        "translated": "评估推荐系统仍是一个长期存在的挑战，因为基于历史用户交互和训练-测试划分的离线方法，常因曝光偏差、流行度偏差、采样评估及缺失非随机模式而产生不稳定且不一致的结果。相比之下，文本文档检索得益于Cranfield式测试集所支持的稳健且标准化的评估体系，该体系结合了聚合的相关性判断与受控实验设置。尽管近期研究已表明将此方法论适配至推荐系统是可行的，但由于需要人工标注相关性，构建此类集合成本高昂，从而限制了其可扩展性。本文探讨大型语言模型（LLMs）是否能够作为可靠的自动判别器，以应对上述可扩展性挑战。我们使用ML-32M-ext Cranfield式电影推荐数据集，首先检验现有评估方法的局限性；随后探究LLM判别器与人工标注相关性标签在排序一致性上的对齐程度。结果表明，引入更丰富的物料元数据与更长的用户历史序列可提升对齐效果，且LLM判别器与人工排名高度一致（Kendall's tau = 0.87）。最后，在播客推荐领域的工业案例研究中，验证了LLM判别器在模型选择中的实际价值。总体而言，我们的结果表明，LLM判别器是一种可行且可扩展的推荐系统评估方法。",
        "translated_title": "大语言模型判别器在 Cranfield 风格推荐系统评估中是否与人类相关性判断一致？",
        "label": [
            "推荐系统评估",
            "LLM生成式推荐"
        ],
        "label_reason": "利用LLM自动评估推荐系统排序效果，解决标定成本问题。",
        "relevance_score": 9,
        "novelty_score": 7,
        "novelty_reason": "提出LLM作为自动评估器，提升评估效率与一致性。"
    },
    {
        "title": "Masked Diffusion for Generative Recommendation",
        "url": "http://arxiv.org/abs/2511.23021v1",
        "pub_date": "2025-11-28",
        "summary": "Generative recommendation (GR) with semantic IDs (SIDs) has emerged as a promising alternative to traditional recommendation approaches due to its performance gains, capitalization on semantic information provided through language model embeddings, and inference and storage efficiency. Existing GR with SIDs works frame the probability of a sequence of SIDs corresponding to a user's interaction history using autoregressive modeling. While this has led to impressive next item prediction performances in certain settings, these autoregressive GR with SIDs models suffer from expensive inference due to sequential token-wise decoding, potentially inefficient use of training data and bias towards learning short-context relationships among tokens. Inspired by recent breakthroughs in NLP, we propose to instead model and learn the probability of a user's sequence of SIDs using masked diffusion. Masked diffusion employs discrete masking noise to facilitate learning the sequence distribution, and models the probability of masked tokens as conditionally independent given the unmasked tokens, allowing for parallel decoding of the masked tokens. We demonstrate through thorough experiments that our proposed method consistently outperforms autoregressive modeling. This performance gap is especially pronounced in data-constrained settings and in terms of coarse-grained recall, consistent with our intuitions. Moreover, our approach allows the flexibility of predicting multiple SIDs in parallel during inference while maintaining superior performance to autoregressive modeling.",
        "translated": "基于语义ID（SIDs）的生成式推荐（GR）因其性能提升、利用语言模型嵌入提供的语义信息，以及推理与存储效率高，已逐渐成为传统推荐方法的有前景替代方案。现有基于SIDs的生成式推荐工作通常采用自回归建模框架，以刻画与用户交互历史对应的SIDs序列的概率分布。尽管该方法在特定场景下实现了令人印象深刻的下一个物品预测性能，但这些基于自回归的SIDs生成式推荐模型存在推理成本高昂的问题——源于逐token顺序解码；此外，其训练数据利用率可能较低，且倾向于学习短上下文关系中的token关联。受自然语言处理领域近期突破的启发，我们提出改用掩码扩散模型来建模并学习用户SIDs序列的概率分布。掩码扩散通过离散掩码噪声促进序列分布的学习，并将掩码token的概率建模为在未掩码token条件下的独立事件，从而支持掩码token的并行解码。通过详尽实验，我们证明所提方法在多个指标上持续优于自回归建模方法。该性能优势尤其在数据受限场景及粗粒度召回任务中表现显著，符合我们的直觉预期。此外，我们的方法在推理阶段允许并行预测多个SIDs，同时保持较自回归建模更优的性能水平。",
        "translated_title": "掩码扩散生成式推荐",
        "label": [
            "LLM生成式推荐",
            "重排"
        ],
        "label_reason": "提出扩散模型用于生成式推荐，优化推理效率与召回",
        "relevance_score": 10,
        "novelty_score": 9,
        "novelty_reason": "首次将掩码扩散应用于生成式推荐，突破序列解码瓶颈"
    },
    {
        "title": "CNN-Based Framework for Pedestrian Age and Gender Classification Using Far-View Surveillance in Mixed-Traffic Intersections",
        "url": "http://arxiv.org/abs/2511.22873v1",
        "pub_date": "2025-11-28",
        "summary": "Pedestrian safety remains a pressing concern in congested urban intersections, particularly in low- and middle-income countries where traffic is multimodal, and infrastructure often lacks formal control. Demographic factors like age and gender significantly influence pedestrian vulnerability, yet real-time monitoring systems rarely capture this information. To address this gap, this study proposes a deep learning framework that classifies pedestrian age group and gender from far-view intersection footage using convolutional neural networks (CNNs), without relying on facial recognition or high-resolution imagery. The classification is structured as a unified six-class problem, distinguishing adult, teenager, and child pedestrians for both males and females, based on full-body visual cues. Video data was collected from three high-risk intersections in Dhaka, Bangladesh. Two CNN architectures were implemented: ResNet50, a deep convolutional neural network pretrained on ImageNet, and a custom lightweight CNN optimized for computational efficiency. Eight model variants explored combinations of pooling strategies and optimizers. ResNet50 with Max Pooling and SGD achieved the highest accuracy (86.19%), while the custom CNN performed comparably (84.15%) with fewer parameters and faster training. The model's efficient design enables real-time inference on standard surveillance feeds. For practitioners, this system provides a scalable, cost-effective tool to monitor pedestrian demographics at intersections using existing camera infrastructure. Its outputs can shape intersection design, optimize signal timing, and enable targeted safety interventions for vulnerable groups such as children or the elderly. By offering demographic insights often missing in conventional traffic data, the framework supports more inclusive, data-driven planning in mixed-traffic environments.",
        "translated": "行人安全在拥挤的城市交叉路口仍是一个紧迫问题，特别是在交通模式多样、基础设施常缺乏正式管控的低收入和中等收入国家。年龄与性别等人口统计因素显著影响行人的脆弱性，但现有的实时监控系统极少捕捉此类信息。为弥补这一不足，本研究提出一种深度学习框架，利用卷积神经网络（CNNs），从远距离视角的交叉路口视频中对行人年龄组别和性别进行分类，且不依赖人脸识别或高分辨率图像。该分类被构建成一个统一的六类问题，根据全身视觉线索区分男性和女性的成人、青少年及儿童行人。视频数据采集自孟加拉国达卡市三个高风险交叉路口。实验实现了两种CNN架构：在ImageNet上预训练的深度卷积神经网络ResNet50，以及专为计算效率优化设计的轻量级自定义CNN。八种模型变体探索了不同池化策略与优化器的组合。其中，采用最大池化与SGD优化器的ResNet50取得最高准确率（86.19%），而自定义CNN表现相当（84.15%），参数更少、训练速度更快。该模型高效的设计使其可在标准监控视频流上实现实时推理。对实践者而言，该系统提供了一种可扩展、低成本的工具，利用现有摄像头基础设施监测交叉路口的行人人口特征。其输出可用于指导交叉口设计、优化信号配时，并针对儿童或老年人等脆弱群体实施精准安全干预。通过补充传统交通数据中常缺失的人口统计洞察，该框架支持混合交通环境中更具包容性的数据驱动规划。",
        "translated_title": "基于卷积神经网络的混合交通交叉口远距离监控下行人年龄与性别分类框架",
        "label": [],
        "label_reason": "论文聚焦行人年龄性别分类，与推荐系统无关。",
        "relevance_score": 2,
        "novelty_score": 4,
        "novelty_reason": "常规CNN应用，无推荐系统创新。"
    },
    {
        "title": "FedAU2: Attribute Unlearning for User-Level Federated Recommender Systems with Adaptive and Robust Adversarial Training",
        "url": "http://arxiv.org/abs/2511.22872v1",
        "pub_date": "2025-11-28",
        "summary": "Federated Recommender Systems (FedRecs) leverage federated learning to protect user privacy by retaining data locally. However, user embeddings in FedRecs often encode sensitive attribute information, rendering them vulnerable to attribute inference attacks. Attribute unlearning has emerged as a promising approach to mitigate this issue. In this paper, we focus on user-level FedRecs, which is a more practical yet challenging setting compared to group-level FedRecs. Adversarial training emerges as the most feasible approach within this context. We identify two key challenges in implementing adversarial training-based attribute unlearning for user-level FedRecs: i) mitigating training instability caused by user data heterogeneity, and ii) preventing attribute information leakage through gradients. To address these challenges, we propose FedAU2, an attribute unlearning method for user-level FedRecs. For CH1, we propose an adaptive adversarial training strategy, where the training dynamics are adjusted in response to local optimization behavior. For CH2, we propose a dual-stochastic variational autoencoder to perturb the adversarial model, effectively preventing gradient-based information leakage. Extensive experiments on three real-world datasets demonstrate that our proposed FedAU2 achieves superior performance in unlearning effectiveness and recommendation performance compared to existing baselines.",
        "translated": "联邦推荐系统（FedRecs）利用联邦学习在本地保留数据，从而保护用户隐私。然而，FedRecs 中的用户嵌入通常编码了敏感属性信息，使其易受属性推断攻击。属性遗忘作为一种有前景的方法，可缓解此问题。本文聚焦于用户级 FedRecs，相较于群体级 FedRecs，该设定更具实用性但也更具挑战性。对抗训练在此背景下成为最可行的方案。我们识别出在用户级 FedRecs 中应用基于对抗训练的属性遗忘面临两个关键挑战：i）缓解因用户数据异质性导致的训练不稳定问题；ii）防止通过梯度泄露属性信息。为应对上述挑战，我们提出 FedAU2，一种面向用户级 FedRecs 的属性遗忘方法。针对 CH1，我们设计了一种自适应对抗训练策略，根据局部优化行为动态调整训练过程；针对 CH2，我们提出一种双随机变分自编码器，通过扰动对抗模型有效防止基于梯度的信息泄露。在三个真实世界数据集上的大量实验表明，所提出的 FedAU2 在遗忘效果和推荐性能方面均显著优于现有基线方法。",
        "translated_title": "FedAU2：面向用户级联邦推荐系统的属性遗忘方法，结合自适应与鲁棒对抗训练",
        "label": [
            "跨域/联邦推荐",
            "推荐系统公平性/可解释性"
        ],
        "label_reason": "聚焦联邦推荐中的属性遗忘，属隐私保护相关",
        "relevance_score": 8,
        "novelty_score": 7,
        "novelty_reason": "提出自适应对抗训练与双随机变分自编码器组合方案"
    },
    {
        "title": "RAG System for Supporting Japanese Litigation Procedures: Faithful Response Generation Complying with Legal Norms",
        "url": "http://arxiv.org/abs/2511.22858v1",
        "pub_date": "2025-11-28",
        "summary": "This study discusses the essential components that a Retrieval-Augmented Generation (RAG)-based LLM system should possess in order to support Japanese medical litigation procedures complying with legal norms. In litigation, expert commissioners, such as physicians, architects, accountants, and engineers, provide specialized knowledge to help judges clarify points of dispute. When considering the substitution of these expert roles with a RAG-based LLM system, the constraint of strict adherence to legal norms is imposed. Specifically, three requirements arise: (1) the retrieval module must retrieve appropriate external knowledge relevant to the disputed issues in accordance with the principle prohibiting the use of private knowledge, (2) the responses generated must originate from the context provided by the RAG and remain faithful to that context, and (3) the retrieval module must reference external knowledge with appropriate timestamps corresponding to the issues at hand. This paper discusses the design of a RAG-based LLM system that satisfies these requirements.",
        "translated": "本研究探讨了基于检索增强生成（RAG）的大语言模型（LLM）系统为支持符合法律规范的日本医疗诉讼程序所应具备的核心组件。在诉讼过程中，专家委员（如医师、建筑师、会计师和工程师）提供专业知识以协助法官厘清争议焦点。当考虑用基于 RAG 的 LLM 系统替代这些专家角色时，必须遵循严格的法律规范约束。具体而言，提出三项要求：（1）检索模块必须依据禁止使用私人知识的原则，检索与争议事项相关的适当外部知识；（2）生成的响应必须源自 RAG 提供的上下文，并忠实于该上下文；（3）检索模块必须引用与当前争议事项相匹配的恰当时间戳对应的外部知识。本文讨论了一种满足上述要求的基于 RAG 的 LLM 系统的设计。",
        "translated_title": "支持日本诉讼程序的RAG系统：符合法律规范的忠实响应生成",
        "label": [],
        "label_reason": "论文聚焦法律RAG系统，非推荐系统相关",
        "relevance_score": 3,
        "novelty_score": 4,
        "novelty_reason": "改进RAG在法律场景下的约束设计"
    },
    {
        "title": "Two-Stage Distributionally Robust Optimization Framework for Secure Communications in Aerial-RIS Systems",
        "url": "http://arxiv.org/abs/2511.22855v1",
        "pub_date": "2025-11-28",
        "summary": "This letter proposes a two-stage distributionally robust optimization (DRO) framework for secure deployment and beamforming in an aerial reconfigurable intelligent surface (A-RIS) assisted millimeter-wave system. To account for multi-timescale uncertainties arising from user mobility, imperfect channel state information (CSI), and hardware impairments, our approach decouples the long-term unmanned aerial vehicle (UAV) placement from the per-slot beamforming design. By employing the conditional value-at-risk (CVaR) as a distribution-free risk metric, a low-complexity algorithm is developed, which combines a surrogate model for efficient deployment with an alternating optimization (AO) scheme for robust real-time beamforming. Simulation results validate that the proposed DRO-CVaR framework significantly enhances the tail-end secrecy spectral efficiency and maintains a lower outage probability compared to benchmark schemes, especially under severe uncertainty conditions.",
        "translated": "本通信提出了一种两级分布鲁棒优化（DRO）框架，用于在由空中可重构智能表面（A-RIS）辅助的毫米波系统中实现安全部署与波束赋形。为应对由用户移动性、不完美的信道状态信息（CSI）和硬件缺陷所引发的多时间尺度不确定性，我们的方法将长期无人机（UAV）部署与每时隙波束赋形设计解耦。通过采用条件值-at-风险（CVaR）作为无分布假设的风险度量，我们开发了一种低复杂度算法，该算法结合了用于高效部署的代理模型与交替优化（AO）方案，以实现鲁棒的实时波束赋形。仿真结果验证了所提出的DRO-CVaR框架在显著提升尾部安全性频谱效率的同时，相较于基准方案在更低的中断概率下表现更优，尤其在严重不确定性条件下。",
        "translated_title": "面向空天地一体化RIS系统的两阶段分布鲁棒优化框架",
        "label": [],
        "label_reason": "论文聚焦无线通信安全，与推荐系统无关。",
        "relevance_score": 1,
        "novelty_score": 3,
        "novelty_reason": "方法属优化算法，无推荐系统创新。"
    },
    {
        "title": "CoFiRec: Coarse-to-Fine Tokenization for Generative Recommendation",
        "url": "http://arxiv.org/abs/2511.22707v1",
        "pub_date": "2025-11-27",
        "summary": "In web environments, user preferences are often refined progressively as users move from browsing broad categories to exploring specific items. However, existing generative recommenders overlook this natural refinement process. Generative recommendation formulates next-item prediction as autoregressive generation over tokenized user histories, where each item is represented as a sequence of discrete tokens. Prior models typically fuse heterogeneous attributes such as ID, category, title, and description into a single embedding before quantization, which flattens the inherent semantic hierarchy of items and fails to capture the gradual evolution of user intent during web interactions. To address this limitation, we propose CoFiRec, a novel generative recommendation framework that explicitly incorporates the Coarse-to-Fine nature of item semantics into the tokenization process. Instead of compressing all attributes into a single latent space, CoFiRec decomposes item information into multiple semantic levels, ranging from high-level categories to detailed descriptions and collaborative filtering signals. Based on this design, we introduce the CoFiRec Tokenizer, which tokenizes each level independently while preserving structural order. During autoregressive decoding, the language model is instructed to generate item tokens from coarse to fine, progressively modeling user intent from general interests to specific item-level interests. Experiments across multiple public benchmarks and backbones demonstrate that CoFiRec outperforms existing methods, offering a new perspective for generative recommendation. Theoretically, we prove that structured tokenization leads to lower dissimilarity between generated and ground truth items, supporting its effectiveness in generative recommendation. Our code is available at https://github.com/YennNing/CoFiRec.",
        "translated": "在Web环境中，用户偏好通常随着其从浏览宽泛类别逐步深入到探索具体物品而不断精细化。然而，现有的生成式推荐系统忽视了这一自然的渐进式细化过程。生成式推荐将下一物品预测建模为对分词后的用户历史序列的自回归生成，其中每个物品被表示为离散token组成的序列。先前模型通常在量化前将异构属性（如ID、类别、标题和描述）融合为单一嵌入向量，这导致物品固有的语义层次结构被扁平化，并无法捕捉用户在Web交互过程中意图的渐进演化。为解决这一局限性，我们提出CoFiRec，一种全新的生成式推荐框架，该框架明确将物品语义的“粗到细”特性融入分词过程。与压缩所有属性至单个潜在空间不同，CoFiRec将物品信息分解为多个语义层级，涵盖从高层类别到详细描述及协同过滤信号。基于此设计，我们引入CoFiRec Tokenizer，该分词器在保持结构顺序的前提下，对各层级独立进行分词。在自回归解码过程中，语言模型被引导按“粗到细”顺序生成物品token，从而逐步建模用户从一般兴趣到具体物品级兴趣的意图演化。在多个公开基准数据集及不同骨干网络上的实验表明，CoFiRec优于现有方法，为生成式推荐提供了全新视角。理论上，我们证明结构化分词可降低生成物品与真实物品之间的差异度，支持其在生成式推荐中的有效性。我们的代码开源于 https://github.com/YennNing/CoFiRec。",
        "translated_title": "CoFiRec：用于生成式推荐的粗到细分词方法",
        "label": [
            "LLM生成式推荐",
            "通用推荐技术"
        ],
        "label_reason": "直接提出生成式推荐新框架，解决语义层次建模问题。",
        "relevance_score": 10,
        "novelty_score": 9,
        "novelty_reason": "创新性结构化分层token化，显著提升生成质量。"
    },
    {
        "title": "SciPostGen: Bridging the Gap between Scientific Papers and Poster Layouts",
        "url": "http://arxiv.org/abs/2511.22490v1",
        "pub_date": "2025-11-27",
        "summary": "As the number of scientific papers continues to grow, there is a demand for approaches that can effectively convey research findings, with posters serving as a key medium for presenting paper contents. Poster layouts determine how effectively research is communicated and understood, highlighting their growing importance. In particular, a gap remains in understanding how papers correspond to the layouts that present them, which calls for datasets with paired annotations at scale. To bridge this gap, we introduce SciPostGen, a large-scale dataset for understanding and generating poster layouts from scientific papers. Our analyses based on SciPostGen show that paper structures are associated with the number of layout elements in posters. Based on this insight, we explore a framework, Retrieval-Augmented Poster Layout Generation, which retrieves layouts consistent with a given paper and uses them as guidance for layout generation. We conducted experiments under two conditions: with and without layout constraints typically specified by poster creators. The results show that the retriever estimates layouts aligned with paper structures, and our framework generates layouts that also satisfy given constraints.",
        "translated": "随着科学论文数量持续增长，亟需能够有效传达研究成果的方法，海报作为呈现论文内容的关键媒介，其布局决定着研究信息传递与理解的效率，因而其重要性日益凸显。尤其在当前，尚缺乏对论文如何对应其展示布局的系统认知，这呼唤大规模配对标注数据集的出现。为弥合这一差距，我们提出 SciPostGen，这是一个面向从科学论文生成和理解海报布局的大规模数据集。基于 SciPostGen 的分析表明，论文结构与海报中布局元素的数量存在关联。基于此洞察，我们探索了一种框架——检索增强型海报布局生成（Retrieval-Augmented Poster Layout Generation），该框架首先检索与给定论文一致的布局，并以此作为布局生成的引导。我们在两种条件下进行了实验：一种是包含通常由海报创作者指定的布局约束条件，另一种则无此类约束。结果表明，检索器能够估计出与论文结构相符的布局，而我们的框架生成的布局亦能满足给定约束条件。",
        "translated_title": "SciPostGen：弥合科学论文与海报布局之间的鸿沟",
        "label": [],
        "label_reason": "论文聚焦学术海报布局生成，与推荐系统无直接关联。",
        "relevance_score": 3,
        "novelty_score": 5,
        "novelty_reason": "方法基于检索增强生成，但非为推荐任务设计。"
    },
    {
        "title": "An Efficient Embedding Based Ad Retrieval with GPU-Powered Feature Interaction",
        "url": "http://arxiv.org/abs/2511.22460v1",
        "pub_date": "2025-11-27",
        "summary": "In large-scale advertising recommendation systems, retrieval serves as a critical component, aiming to efficiently select a subset of candidate ads relevant to user behaviors from a massive ad inventory for subsequent ranking and recommendation. The Embedding-Based Retrieval (EBR) methods modeled by the dual-tower network are widely used in the industry to maintain both retrieval efficiency and accuracy. However, the dual-tower model has significant limitations: the embeddings of users and ads interact only at the final inner product computation, resulting in insufficient feature interaction capabilities. Although DNN-based models with both user and ad as input features, allowing for early-stage interaction between these features, are introduced in the ranking stage to mitigate this issue, they are computationally infeasible for the retrieval stage. To bridge this gap, this paper proposes an efficient GPU-based feature interaction for the dual-tower network to significantly improve retrieval accuracy while substantially reducing computational costs. Specifically, we introduce a novel compressed inverted list designed for GPU acceleration, enabling efficient feature interaction computation at scale. To the best of our knowledge, this is the first framework in the industry to successfully implement Wide and Deep in a retrieval system. We apply this model to the real-world business scenarios in Tencent Advertising, and experimental results demonstrate that our method outperforms existing approaches in offline evaluation and has been successfully deployed to Tencent's advertising recommendation system, delivering significant online performance gains. This improvement not only validates the effectiveness of the proposed method, but also provides new practical guidance for optimizing large-scale ad retrieval systems.",
        "translated": "在大规模广告推荐系统中，召回作为关键一环，旨在从海量广告库存中高效筛选出与用户行为相关的候选广告子集，供后续排序与推荐使用。基于嵌入的召回（Embedding-Based Retrieval, EBR）方法通过双塔网络建模，在业界广泛应用，以兼顾召回效率与准确性。然而，双塔模型存在显著局限性：用户与广告的嵌入仅在最终内积计算阶段发生交互，导致特征交互能力不足。尽管在排序阶段引入了以用户和广告为输入特征的深度神经网络（DNN）模型，可在早期阶段实现特征交互以缓解该问题，但此类模型在召回阶段因计算开销过大而不可行。为弥合这一差距，本文提出一种面向双塔网络的高效GPU加速特征交互机制，在显著提升召回准确率的同时大幅降低计算成本。具体而言，我们设计了一种专用于GPU加速的压缩倒排列表，可支持大规模下高效特征交互计算。据我们所知，这是业界首个成功在召回系统中实现Wide and Deep框架的方案。我们将该模型应用于腾讯广告的实际业务场景，实验结果表明，本方法在离线评估中优于现有方法，并已成功部署至腾讯广告推荐系统，带来显著的线上性能提升。该改进不仅验证了所提方法的有效性，也为优化大规模广告召回系统提供了新的实用指导。",
        "translated_title": "一种基于高效嵌入的广告召回系统，利用GPU加速特征交互",
        "label": [
            "召回",
            "通用推荐技术"
        ],
        "label_reason": "聚焦广告召回阶段特征交互优化，提升效率与精度",
        "relevance_score": 9,
        "novelty_score": 7,
        "novelty_reason": "首次在召回系统中实现Wide and Deep架构，具实用创新"
    },
    {
        "title": "Structured Extraction from Business Process Diagrams Using Vision-Language Models",
        "url": "http://arxiv.org/abs/2511.22448v1",
        "pub_date": "2025-11-27",
        "summary": "Business Process Model and Notation (BPMN) is a widely adopted standard for representing complex business workflows. While BPMN diagrams are often exchanged as visual images, existing methods primarily rely on XML representations for computational analysis. In this work, we present a pipeline that leverages Vision-Language Models (VLMs) to extract structured JSON representations of BPMN diagrams directly from images, without requiring source model files or textual annotations. We also incorporate optical character recognition (OCR) for textual enrichment and evaluate the generated element lists against ground truth data derived from the source XML files. Our approach enables robust component extraction in scenarios where original source files are unavailable. We benchmark multiple VLMs and observe performance improvements in several models when OCR is used for text enrichment. In addition, we conducted extensive statistical analyses of OCR-based enrichment methods and prompt ablation studies, providing a clearer understanding of their impact on model performance.",
        "translated": "业务流程模型与符号（BPMN）是表示复杂业务工作流广泛采用的标准。尽管BPMN图通常以视觉图像形式进行交换，现有方法主要依赖XML表示来进行计算分析。在本研究中，我们提出了一种利用视觉-语言模型（VLMs）直接从图像中提取BPMN图结构化JSON表示的管道，无需依赖源模型文件或文本标注。我们同时引入光学字符识别（OCR）进行文本增强，并将生成的元素列表与从源XML文件推导出的真实数据进行对比评估。该方法能够在无原始源文件的情况下实现稳健的组件提取。我们对多种VLMs进行了基准测试，观察到当使用OCR进行文本增强时，多个模型的性能得到提升。此外，我们还对基于OCR的增强方法进行了广泛的统计分析和提示消融实验，进一步明确了其对模型性能的影响。",
        "translated_title": "基于视觉-语言模型的业务流程图结构化提取",
        "label": [],
        "label_reason": "论文聚焦视觉-语言模型在BPMN图提取，与推荐系统无关。",
        "relevance_score": 1,
        "novelty_score": 4,
        "novelty_reason": "OCR增强VLM方法常规改进，非推荐系统创新。"
    },
    {
        "title": "Efficiency and Effectiveness of SPLADE Models on Billion-Scale Web Document Title",
        "url": "http://arxiv.org/abs/2511.22263v1",
        "pub_date": "2025-11-27",
        "summary": "This paper presents a comprehensive comparison of BM25, SPLADE, and Expanded-SPLADE models in the context of large-scale web document retrieval. We evaluate the effectiveness and efficiency of these models on datasets spanning from tens of millions to billions of web document titles. SPLADE and Expanded-SPLADE, which utilize sparse lexical representations, demonstrate superior retrieval performance compared to BM25, especially for complex queries. However, these models incur higher computational costs. We introduce pruning strategies, including document-centric pruning and top-k query term selection, boolean query with term threshold to mitigate these costs and improve the models' efficiency without significantly sacrificing retrieval performance. The results show that Expanded-SPLADE strikes the best balance between effectiveness and efficiency, particularly when handling large datasets. Our findings offer valuable insights for deploying sparse retrieval models in large-scale search engines.",
        "translated": "本文在大规模网络文档检索背景下，对 BM25、SPLADE 和 Expanded-SPLADE 三种模型进行了全面比较。我们在涵盖数千万至数十亿网页标题的数据集上评估了这些模型的有效性与效率。SPLADE 和 Expanded-SPLADE 利用稀疏词汇表示，在复杂查询场景下相较于 BM25 表现出更优的检索性能。然而，这些模型带来了更高的计算开销。为此，我们引入了剪枝策略，包括以文档为中心的剪枝、前 k 个查询词选取以及布尔查询结合词项阈值，以缓解计算成本并提升模型效率，而不会显著牺牲检索性能。实验结果表明，Expanded-SPLADE 在处理大规模数据集时，在效果与效率之间取得了最佳平衡。我们的研究为在大规模搜索引擎中部署稀疏检索模型提供了有价值的参考。",
        "translated_title": "SPLADE模型在十亿级网页文档标题上的效率与效果",
        "label": [],
        "label_reason": "研究聚焦大规模文档检索，非推荐系统核心问题",
        "relevance_score": 3,
        "novelty_score": 5,
        "novelty_reason": "改进稀疏模型效率，但无推荐系统创新"
    },
    {
        "title": "UNION: A Lightweight Target Representation for Efficient Zero-Shot Image-Guided Retrieval with Optional Textual Queries",
        "url": "http://arxiv.org/abs/2511.22253v1",
        "pub_date": "2025-11-27",
        "summary": "Image-Guided Retrieval with Optional Text (IGROT) is a general retrieval setting where a query consists of an anchor image, with or without accompanying text, aiming to retrieve semantically relevant target images. This formulation unifies two major tasks: Composed Image Retrieval (CIR) and Sketch-Based Image Retrieval (SBIR). In this work, we address IGROT under low-data supervision by introducing UNION, a lightweight and generalisable target representation that fuses the image embedding with a null-text prompt. Unlike traditional approaches that rely on fixed target features, UNION enhances semantic alignment with multimodal queries while requiring no architectural modifications to pretrained vision-language models. With only 5,000 training samples - from LlavaSCo for CIR and Training-Sketchy for SBIR - our method achieves competitive results across benchmarks, including CIRCO mAP@50 of 38.5 and Sketchy mAP@200 of 82.7, surpassing many heavily supervised baselines. This demonstrates the robustness and efficiency of UNION in bridging vision and language across diverse query types.",
        "translated": "图像引导的可选文本检索（IGROT）是一种通用的检索设置，其中查询由一个锚图像组成，可附带或不附带文本，旨在检索语义相关的目标图像。该设定统一了两个主要任务：组合图像检索（CIR）与基于草图的图像检索（SBIR）。在本工作中，我们针对低监督数据场景下的 IGROT 问题，提出 UNION，一种轻量且具泛化能力的目标表示方法，将图像嵌入与空文本提示融合。与依赖固定目标特征的传统方法不同，UNION 在无需修改预训练视觉-语言模型架构的前提下，增强了与多模态查询之间的语义对齐能力。仅使用 5,000 个训练样本——来自 LlavaSCo 的 CIR 数据集和 Training-Sketchy 的 SBIR 数据集——我们的方法在多个基准测试中取得了具有竞争力的结果，包括 CIRCO 的 mAP@50 达到 38.5、Sketchy 的 mAP@200 达到 82.7，超越了许多高度监督的基线方法。这证明了 UNION 在跨越多样化的查询类型时，有效连接视觉与语言的能力，兼具鲁棒性与高效性。",
        "translated_title": "UNION：一种轻量级目标表示，用于高效零样本图像引导检索并支持可选文本查询",
        "label": [
            "通用推荐技术"
        ],
        "label_reason": "方法适用于图像检索，可适配推荐系统中视觉内容召回",
        "relevance_score": 4,
        "novelty_score": 7,
        "novelty_reason": "提出轻量级目标表示，无需架构修改提升多模态对齐"
    },
    {
        "title": "FIGROTD: A Friendly-to-Handle Dataset for Image Guided Retrieval with Optional Text",
        "url": "http://arxiv.org/abs/2511.22247v1",
        "pub_date": "2025-11-27",
        "summary": "Image-Guided Retrieval with Optional Text (IGROT) unifies visual retrieval (without text) and composed retrieval (with text). Despite its relevance in applications like Google Image and Bing, progress has been limited by the lack of an accessible benchmark and methods that balance performance across subtasks. Large-scale datasets such as MagicLens are comprehensive but computationally prohibitive, while existing models often favor either visual or compositional queries. We introduce FIGROTD, a lightweight yet high-quality IGROT dataset with 16,474 training triplets and 1,262 test triplets across CIR, SBIR, and CSTBIR. To reduce redundancy, we propose the Variance Guided Feature Mask (VaGFeM), which selectively enhances discriminative dimensions based on variance statistics. We further adopt a dual-loss design (InfoNCE + Triplet) to improve compositional reasoning. Trained on FIGROTD, VaGFeM achieves competitive results on nine benchmarks, reaching 34.8 mAP@10 on CIRCO and 75.7 mAP@200 on Sketchy, outperforming stronger baselines despite fewer triplets.",
        "translated": "图像引导的可选文本检索（IGROT）统一了无文本的视觉检索与带文本的组合式检索。尽管其在如 Google 图像和 Bing 等应用中具有重要意义，但进展受限于缺乏公开可用的基准数据集以及能够平衡子任务性能的方法。大规模数据集如 MagicLens 虽然全面，但计算开销过高；而现有模型往往偏向视觉或组合查询。我们提出了 FIGROTD，这是一个轻量级且高质量的 IGROT 数据集，包含来自 CIR、SBIR 和 CSTBIR 三类任务的 16,474 个训练三元组和 1,262 个测试三元组。为降低冗余，我们提出方差引导特征掩码（VaGFeM），基于方差统计选择性增强判别维度。此外，我们采用双损失设计（InfoNCE + Triplet）以提升组合推理能力。在 FIGROTD 上训练后，VaGFeM 在九个基准数据集上表现优异，在 CIRCO 上达到 34.8 mAP@10，在 Sketchy 上达到 75.7 mAP@200，优于参数更少的基线模型。",
        "translated_title": "FIGROTD：一个便于处理的图像引导检索数据集（支持可选文本）",
        "label": [
            "通用推荐技术"
        ],
        "label_reason": "聚焦图像检索基准，非推荐系统核心环节",
        "relevance_score": 4,
        "novelty_score": 6,
        "novelty_reason": "改进特征掩码与损失设计，但非推荐专用"
    },
    {
        "title": "Evaluating Embedding Models and Pipeline Optimization for AI Search Quality",
        "url": "http://arxiv.org/abs/2511.22240v1",
        "pub_date": "2025-11-27",
        "summary": "We evaluate the performance of various text embedding models and pipeline configurations for AI-driven search systems. We compare sentence-transformer and generative embedding models (e.g., All-MPNet, BGE, GTE, and Qwen) at different dimensions, indexing methods (Milvus HNSW/IVF), and chunking strategies. A custom evaluation dataset of 11,975 query-chunk pairs was synthesized from US City Council meeting transcripts using a local large language model (LLM). The data pipeline includes preprocessing, automated question generation per chunk, manual validation, and continuous integration/continuous deployment (CI/CD) integration. We measure retrieval accuracy using reference-based metrics: Top-K Accuracy and Normalized Discounted Cumulative Gain (NDCG). Our results demonstrate that higher-dimensional embeddings significantly boost search quality (e.g., Qwen3-Embedding-8B/4096 achieves Top-3 accuracy about 0.571 versus 0.412 for GTE-large/1024), and that neural re-rankers (e.g., a BGE cross-encoder) further improve ranking accuracy (Top-3 up to 0.527). Finer-grained chunking (512 characters versus 2000 characters) also improves accuracy. We discuss the impact of these factors and outline future directions for pipeline automation and evaluation.",
        "translated": "我们评估了多种文本嵌入模型及用于AI驱动搜索系统的管道配置的性能。我们对比了不同维度、索引方法（Milvus HNSW/IVF）和分块策略下的句子变换器与生成式嵌入模型（例如 All-MPNet、BGE、GTE 和 Qwen）。该自定义评估数据集由11,975组查询-分块对组成，通过本地大语言模型（LLM）从美国市议会会议记录中合成。数据管道包括预处理、按分块自动生成问题、人工验证，以及持续集成/持续部署（CI/CD）集成。我们采用基于参考的标准指标评估检索准确性：Top-K 准确率与归一化折扣累积增益（NDCG）。实验结果表明，高维嵌入显著提升搜索质量（例如，Qwen3-Embedding-8B/4096 在 Top-3 准确率上达到 0.571，相较 GTE-large/1024 的 0.412 显著提高），而神经重排序器（如 BGE 跨编码器）进一步提升了排序准确率（Top-3 达至 0.527）。更精细的分块粒度（512 字符 vs 2000 字符）亦可提升准确率。我们讨论了这些因素的影响，并概述了未来在管道自动化与评估方面的研究方向。",
        "translated_title": "评估嵌入模型与管道优化对AI搜索质量的影响",
        "label": [
            "通用推荐技术",
            "重排（Re-ranking）"
        ],
        "label_reason": "研究搜索召回与重排，非专为推荐设计",
        "relevance_score": 4,
        "novelty_score": 5,
        "novelty_reason": "改进嵌入与重排策略，但非推荐系统原创"
    },
    {
        "title": "From Topology to Retrieval: Decoding Embedding Spaces with Unified Signatures",
        "url": "http://arxiv.org/abs/2511.22150v1",
        "pub_date": "2025-11-27",
        "summary": "Studying how embeddings are organized in space not only enhances model interpretability but also uncovers factors that drive downstream task performance. In this paper, we present a comprehensive analysis of topological and geometric measures across a wide set of text embedding models and datasets. We find a high degree of redundancy among these measures and observe that individual metrics often fail to sufficiently differentiate embedding spaces. Building on these insights, we introduce Unified Topological Signatures (UTS), a holistic framework for characterizing embedding spaces. We show that UTS can predict model-specific properties and reveal similarities driven by model architecture. Further, we demonstrate the utility of our method by linking topological structure to ranking effectiveness and accurately predicting document retrievability. We find that a holistic, multi-attribute perspective is essential to understanding and leveraging the geometry of text embeddings.",
        "translated": "研究嵌入在空间中的组织方式不仅有助于提升模型的可解释性，还能揭示驱动下游任务性能的关键因素。本文对一系列文本嵌入模型与数据集中的拓扑与几何度量进行了全面分析。我们发现这些度量之间存在高度冗余，并观察到单个指标往往不足以充分区分不同的嵌入空间。基于上述洞察，我们提出了统一拓扑签名（Unified Topological Signatures, UTS），一个用于刻画嵌入空间的综合框架。我们表明，UTS能够预测特定模型的属性，并揭示由模型架构所驱动的相似性。进一步地，我们通过将拓扑结构与排序效果关联起来，并准确预测文档的检索能力，展示了该方法的实用性。我们发现，要理解和利用文本嵌入的空间几何特性，必须采取全面、多属性的视角。",
        "translated_title": "从拓扑到检索：通过统一签名解码嵌入空间",
        "label": [
            "通用推荐技术",
            "推荐系统评估"
        ],
        "label_reason": "分析嵌入空间结构，间接支持排序与检索效果评估",
        "relevance_score": 4,
        "novelty_score": 6,
        "novelty_reason": "提出统一拓扑签名框架，但非专为推荐设计"
    },
    {
        "title": "Real-Time Procedural Learning From Experience for AI Agents",
        "url": "http://arxiv.org/abs/2511.22074v1",
        "pub_date": "2025-11-27",
        "summary": "Learning how to do things from trial and error in real time is a hallmark of biological intelligence, yet most LLM-based agents lack mechanisms to acquire procedural knowledge after deployment. We propose Procedural Recall for Agents with eXperiences Indexed by State (PRAXIS), a lightweight post-training learning mechanism that stores the consequences of actions and retrieves them by jointly matching environmental and internal states of past episodes to the current state. PRAXIS augments agentic action selection with retrieved state-action-result exemplars that are generated in real time. When evaluated on the REAL web browsing benchmark, PRAXIS improves task completion accuracy, reliability, and cost efficiency across different foundation model backbones, and shows preliminary generalization to unseen tasks in similar environments. These results demonstrate that PRAXIS enables the practical adoption of AI agents in fast-evolving stateful environments by helping them learn new procedures effectively.",
        "translated": "从试错中实时学习如何完成任务，是生物智能的显著特征，但大多数基于大语言模型（LLM）的智能体在部署后缺乏获取程序性知识的机制。我们提出了一种名为“基于状态索引经验的智能体程序性召回”（PRAXIS）的轻量级后训练学习机制。该机制通过将过往回合中的环境状态与内部状态共同匹配当前状态，存储并检索行动后果。PRAXIS 在智能体的动作选择过程中，动态生成并引入实时检索到的状态-动作-结果示例。在 REAL 网页浏览基准测试中，PRAXIS 在不同基础模型架构上均提升了任务完成准确率、可靠性及成本效率，并初步展现出对相似环境中未见过任务的泛化能力。这些结果表明，PRAXIS 能够帮助 AI 智能体有效学习新程序，从而实现在快速演化的状态化环境中实际部署应用。",
        "translated_title": "AI代理从经验中实时过程式学习",
        "label": [
            "LLM生成式推荐",
            "通用推荐技术"
        ],
        "label_reason": "论文聚焦LLM代理实时学习，与推荐系统间接相关。",
        "relevance_score": 4,
        "novelty_score": 7,
        "novelty_reason": "提出PRAXIS机制，实现实时经验回溯，具实用价值。"
    },
    {
        "title": "Selecting User Histories to Generate LLM Users for Cold-Start Item Recommendation",
        "url": "http://arxiv.org/abs/2511.21989v1",
        "pub_date": "2025-11-27",
        "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in reasoning, generalization, and simulating human-like behavior across a wide range of tasks. These strengths present new opportunities to enhance traditional recommendation systems (RS), especially in the cold-start item scenario where newly introduced items lack interactions. Existing works have used LLMs to address cold-start issues in traditional RS through data augmentation, but they have limitations. One recent work directly addresses this issue by prompting LLMs to generate augmented interaction data between randomly sampled users and cold-start items. Then, they train the traditional RS with augmented data, incorporating collaborative signals for cold-start items. Although they use LLMs to provide cold-start items with feedback, they use partial user histories, which does not allow the LLM to fully emulate the user. Furthermore, randomly selecting users is not optimal for augmentation. To address these challenges, we leverage the LLM as a user and develop a reinforcement learning (RL) framework that trains a policy to select users for augmentation, optimizing for cold-start item performance after augmented training. The policy model learns to select users for cold-start item data augmentation based on their behavioral features and histories. To optimize user selection for cold-start item performance, we employ a policy gradient method that updates the policy in the direction of actions that lead to high rewards. Experiments on Amazon Product Review datasets show substantial gains in cold-start item recall, demonstrating the effectiveness of our method as a scalable, serving-efficient augmentation strategy for modern RS.",
        "translated": "大语言模型（LLMs）在多种任务中展现出卓越的推理、泛化及模拟人类行为的能力。这些优势为增强传统推荐系统（RS）带来了新的机遇，尤其是在冷启动物料场景下——新引入的物料缺乏用户交互记录。现有研究已利用LLMs通过数据增强解决传统RS中的冷启动问题，但其方法仍存在局限性。近期一项工作直接针对该问题，通过提示LLMs生成随机采样用户与冷启动物料之间的增强交互数据，随后使用该增强数据训练传统推荐系统，并融入协同信号以提升冷启动物料表现。尽管其利用LLMs为冷启动物料提供反馈，但仅使用部分用户历史行为，未能使LLM充分模拟完整用户。此外，随机选取用户作为增强对象并非最优策略。为应对上述挑战，我们以LLM作为用户代理，构建了一种强化学习（RL）框架，用于训练策略以选择最合适的用户进行物料增强，从而在增强训练后最大化冷启动物料的性能。该策略模型根据用户的特征与历史行为，学习选择用于冷启动物料数据增强的用户。为优化用户选择以提升冷启动物料效果，我们采用策略梯度方法，在高奖励方向更新策略。在Amazon Product Review数据集上的实验表明，本方法显著提升了冷启动物料召回率，验证了其作为一种可扩展且服务高效的现代推荐系统数据增强策略的有效性。",
        "translated_title": "选择用户历史以生成大语言模型用户用于冷启动物品推荐",
        "label": [
            "LLM生成式推荐",
            "召回",
            "负采样与对比学习"
        ],
        "label_reason": "利用LLM生成用户行为数据增强冷启动召回",
        "relevance_score": 9,
        "novelty_score": 8,
        "novelty_reason": "RL驱动的用户选择策略创新，提升冷启动效果"
    },
    {
        "title": "Estimation of Kinematic Motion from Dashcam Footage",
        "url": "http://arxiv.org/abs/2512.01104v1",
        "pub_date": "2025-11-30",
        "summary": "The goal of this paper is to explore the accuracy of dashcam footage to predict the actual kinematic motion of a car-like vehicle. Our approach uses ground truth information from the vehicle's on-board data stream, through the controller area network, and a time-synchronized dashboard camera, mounted to a consumer-grade vehicle, for 18 hours of footage and driving. The contributions of the paper include neural network models that allow us to quantify the accuracy of predicting the vehicle speed and yaw, as well as the presence of a lead vehicle, and its relative distance and speed. In addition, the paper describes how other researchers can gather their own data to perform similar experiments, using open-source tools and off-the-shelf technology.",
        "translated": "本文旨在探索行车记录仪视频在预测类似汽车的车辆实际运动学参数方面的准确性。我们的方法利用车载数据流（通过控制器局域网络）和与之时间同步的仪表盘摄像头所采集的真实值信息，该摄像头安装于一辆消费级车辆上，采集了18小时的行驶视频数据。本文的主要贡献包括若干神经网络模型，可用于量化预测车辆速度与航向角、前方车辆是否存在及其相对距离和速度的准确性。此外，本文还描述了其他研究人员如何利用开源工具和现成技术收集自己的数据，以开展类似实验。",
        "translated_title": "从行车记录仪画面中估计运动学运动",
        "label": [],
        "label_reason": "研究车辆运动估计，非图像像素级处理",
        "relevance_score": 1,
        "novelty_score": 1,
        "novelty_reason": "仅建模运动预测，无图像恢复创新"
    },
    {
        "title": "Learning Eigenstructures of Unstructured Data Manifolds",
        "url": "http://arxiv.org/abs/2512.01103v1",
        "pub_date": "2025-11-30",
        "summary": "We introduce a novel framework that directly learns a spectral basis for shape and manifold analysis from unstructured data, eliminating the need for traditional operator selection, discretization, and eigensolvers. Grounded in optimal-approximation theory, we train a network to decompose an implicit approximation operator by minimizing the reconstruction error in the learned basis over a chosen distribution of probe functions. For suitable distributions, they can be seen as an approximation of the Laplacian operator and its eigendecomposition, which are fundamental in geometry processing. Furthermore, our method recovers in a unified manner not only the spectral basis, but also the implicit metric's sampling density and the eigenvalues of the underlying operator. Notably, our unsupervised method makes no assumption on the data manifold, such as meshing or manifold dimensionality, allowing it to scale to arbitrary datasets of any dimension. On point clouds lying on surfaces in 3D and high-dimensional image manifolds, our approach yields meaningful spectral bases, that can resemble those of the Laplacian, without explicit construction of an operator. By replacing the traditional operator selection, construction, and eigendecomposition with a learning-based approach, our framework offers a principled, data-driven alternative to conventional pipelines. This opens new possibilities in geometry processing for unstructured data, particularly in high-dimensional spaces.",
        "translated": "我们提出了一种新颖的框架，该框架直接从无结构数据中学习形状与流形分析所需的谱基，从而无需传统算子选择、离散化及特征值求解器。基于最优逼近理论，我们训练一个网络，通过最小化在选定探针函数分布上的重构误差，分解一个隐式逼近算子。对于合适的分布，这些探针函数可视为拉普拉斯算子及其特征分解的近似，而后者是几何处理中的基础工具。此外，我们的方法统一恢复了谱基，同时还能获得隐式度量的采样密度以及底层算子的特征值。值得注意的是，我们的无监督方法对数据流形不作任何先验假设（如网格划分或流形维度），使其能够扩展到任意维度的数据集。在位于三维空间表面的点云和高维图像流形上，我们的方法能够生成有意义的谱基，其形式可类似于拉普拉斯算子的谱，却无需显式构造算子。通过将传统的算子选择、构造与特征分解替换为基于学习的方法，我们的框架提供了一种以数据驱动为核心、具有理论依据的替代方案，为无结构数据的几何处理——特别是高维空间——开辟了新的可能性。",
        "translated_title": "学习无结构数据流形的特征结构",
        "label": [],
        "label_reason": "处理高维无结构数据，非图像像素级恢复任务",
        "relevance_score": 3,
        "novelty_score": 8,
        "novelty_reason": "提出学习谱基的新框架，提升几何处理效率"
    },
    {
        "title": "CycliST: A Video Language Model Benchmark for Reasoning on Cyclical State Transitions",
        "url": "http://arxiv.org/abs/2512.01095v1",
        "pub_date": "2025-11-30",
        "summary": "We present CycliST, a novel benchmark dataset designed to evaluate Video Language Models (VLM) on their ability for textual reasoning over cyclical state transitions. CycliST captures fundamental aspects of real-world processes by generating synthetic, richly structured video sequences featuring periodic patterns in object motion and visual attributes. CycliST employs a tiered evaluation system that progressively increases difficulty through variations in the number of cyclic objects, scene clutter, and lighting conditions, challenging state-of-the-art models on their spatio-temporal cognition. We conduct extensive experiments with current state-of-the-art VLMs, both open-source and proprietary, and reveal their limitations in generalizing to cyclical dynamics such as linear and orbital motion, as well as time-dependent changes in visual attributes like color and scale. Our results demonstrate that present-day VLMs struggle to reliably detect and exploit cyclic patterns, lack a notion of temporal understanding, and are unable to extract quantitative insights from scenes, such as the number of objects in motion, highlighting a significant technical gap that needs to be addressed. More specifically, we find no single model consistently leads in performance: neither size nor architecture correlates strongly with outcomes, and no model succeeds equally well across all tasks. By providing a targeted challenge and a comprehensive evaluation framework, CycliST paves the way for visual reasoning models that surpass the state-of-the-art in understanding periodic patterns.",
        "translated": "我们提出了CycliST，一个专为评估视频语言模型（VLM）在循环状态转换上的文本推理能力而设计的新基准数据集。CycliST通过生成具有周期性物体运动与视觉属性变化的合成、结构丰富的视频序列，捕捉现实世界过程的基本特征。该数据集采用分层评估体系，通过增加循环物体数量、场景杂乱程度和光照条件的变化逐步提升难度，从而对当前最先进模型的空间-时间认知能力提出挑战。我们对当前最先进的开源及专有VLM进行了大量实验，揭示其在泛化到循环动态（如线性运动与轨道运动）以及随时间变化的视觉属性（如颜色与尺度）方面存在明显局限。我们的结果表明，现有VLM难以可靠地检测并利用循环模式，缺乏对时间的理解，也无力从场景中提取定量信息（如处于运动中的物体数量），凸显了亟待弥补的重要技术差距。更具体而言，我们发现没有任何单一模型能始终领先于其他模型：模型规模或架构并不显著影响性能，且无一模型能在所有任务上均表现优异。通过提供有针对性的挑战和全面的评估框架，CycliST为超越现有技术水平、深入理解周期性模式的视觉推理模型开辟了道路。",
        "translated_title": "CycliST：面向循环状态转换推理的视频语言模型基准",
        "label": [],
        "label_reason": "研究视频语言模型推理，非图像像素级处理",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "提出新基准评测周期性状态推理能力"
    },
    {
        "title": "Accelerating Inference of Masked Image Generators via Reinforcement Learning",
        "url": "http://arxiv.org/abs/2512.01094v1",
        "pub_date": "2025-11-30",
        "summary": "Masked Generative Models (MGM)s demonstrate strong capabilities in generating high-fidelity images. However, they need many sampling steps to create high-quality generations, resulting in slow inference speed. In this work, we propose Speed-RL, a novel paradigm for accelerating a pretrained MGMs to generate high-quality images in fewer steps. Unlike conventional distillation methods which formulate the acceleration problem as a distribution matching problem, where a few-step student model is trained to match the distribution generated by a many-step teacher model, we consider this problem as a reinforcement learning problem. Since the goal of acceleration is to generate high quality images in fewer steps, we can combine a quality reward with a speed reward and finetune the base model using reinforcement learning with the combined reward as the optimization target. Through extensive experiments, we show that the proposed method was able to accelerate the base model by a factor of 3x while maintaining comparable image quality.",
        "translated": "掩码生成模型（Masked Generative Models, MGMs）在生成高保真图像方面展现出强大能力。然而，它们需要大量采样步骤才能生成高质量图像，导致推理速度缓慢。在本文中，我们提出 Speed-RL，一种新颖范式，用于加速预训练的 MGMs 在更少步骤内生成高质量图像。与传统蒸馏方法将加速问题建模为分布匹配问题不同——即用少量步骤的学生模型训练以匹配多步骤教师模型所生成的分布——我们将该问题视为强化学习问题。由于加速的目标是在更少步骤内生成高质量图像，我们可以结合质量奖励与速度奖励，并使用强化学习对基础模型进行微调，以复合奖励作为优化目标。通过大量实验，我们证明所提方法可将基础模型的推理速度提升约3倍，同时保持相当的图像质量。",
        "translated_title": "通过强化学习加速掩码图像生成器的推理",
        "label": [],
        "label_reason": "目标为加速图像生成，非像素级恢复或增强",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "用RL优化采样步数，提升效率有创新"
    },
    {
        "title": "Generalized Medical Phrase Grounding",
        "url": "http://arxiv.org/abs/2512.01085v1",
        "pub_date": "2025-11-30",
        "summary": "Medical phrase grounding (MPG) maps textual descriptions of radiological findings to corresponding image regions. These grounded reports are easier to interpret, especially for non-experts. Existing MPG systems mostly follow the referring expression comprehension (REC) paradigm and return exactly one bounding box per phrase. Real reports often violate this assumption. They contain multi-region findings, non-diagnostic text, and non-groundable phrases, such as negations or descriptions of normal anatomy. Motivated by this, we reformulate the task as generalised medical phrase grounding (GMPG), where each sentence is mapped to zero, one, or multiple scored regions. To realise this formulation, we introduce the first GMPG model: MedGrounder. We adopted a two-stage training regime: pre-training on report sentence--anatomy box alignment datasets and fine-tuning on report sentence--human annotated box datasets. Experiments on PadChest-GR and MS-CXR show that MedGrounder achieves strong zero-shot transfer and outperforms REC-style and grounded report generation baselines on multi-region and non-groundable phrases, while using far fewer human box annotations. Finally, we show that MedGrounder can be composed with existing report generators to produce grounded reports without retraining the generator.",
        "translated": "医学短语定位（Medical phrase grounding, MPG）将放射学发现的文本描述映射到对应的图像区域。这些定位后的报告更易于理解，尤其对非专业人士而言。现有MPG系统大多遵循指代表达理解（Referring Expression Comprehension, REC）范式，为每个短语返回恰好一个边界框。然而，真实报告常违背这一假设：它们包含多区域发现、非诊断性文本以及不可定位短语，例如否定词或正常解剖结构的描述。受此启发，我们将该任务重新定义为广义医学短语定位（Generalised Medical Phrase Grounding, GMPG），其中每句话可映射至零个、一个或多个评分区域。为实现该形式化，我们提出首个GMPG模型——MedGrounder。该模型采用两阶段训练策略：首先在报告句子—解剖结构框对齐数据集上进行预训练，随后在报告句子—人工标注框数据集上进行微调。在PadChest-GR与MS-CXR数据集上的实验表明，MedGrounder在零样本迁移方面表现优异，并在多区域及不可定位短语任务上超越REC风格与定位报告生成基线方法，同时大幅减少对人工标注框的依赖。最后，我们证明MedGrounder可与现有报告生成器组合使用，无需重新训练生成器即可输出定位报告。",
        "translated_title": "广义医学短语定位",
        "label": [],
        "label_reason": "任务为医学文本-图像区域匹配，属高阶视觉理解",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出GMPG新框架及MedGrounder模型，显著提升多区域处理能力"
    },
    {
        "title": "Opening the Sim-to-Real Door for Humanoid Pixel-to-Action Policy Transfer",
        "url": "http://arxiv.org/abs/2512.01061v1",
        "pub_date": "2025-11-30",
        "summary": "Recent progress in GPU-accelerated, photorealistic simulation has opened a scalable data-generation path for robot learning, where massive physics and visual randomization allow policies to generalize beyond curated environments. Building on these advances, we develop a teacher-student-bootstrap learning framework for vision-based humanoid loco-manipulation, using articulated-object interaction as a representative high-difficulty benchmark. Our approach introduces a staged-reset exploration strategy that stabilizes long-horizon privileged-policy training, and a GRPO-based fine-tuning procedure that mitigates partial observability and improves closed-loop consistency in sim-to-real RL. Trained entirely on simulation data, the resulting policy achieves robust zero-shot performance across diverse door types and outperforms human teleoperators by up to 31.7% in task completion time under the same whole-body control stack. This represents the first humanoid sim-to-real policy capable of diverse articulated loco-manipulation using pure RGB perception.",
        "translated": "近年来，基于GPU加速的逼真物理仿真技术的发展为机器人学习开辟了一条可扩展的数据生成路径，在该路径中，大规模的物理与视觉随机化使策略能够超越精心设计的环境并实现泛化。在此基础上，我们构建了一个基于教师-学生自引导学习框架，用于视觉驱动的人形机器人运动-操作任务，以刚体对象交互作为代表性高难度基准。我们的方法引入了分阶段重置探索策略，以稳定长时域特权策略训练；并采用基于GRPO的微调过程，缓解部分可观测性问题，提升仿真到真实环境中闭环控制的一致性。所训练的策略完全依赖仿真数据，能够在多种门类场景下实现鲁棒的零样本性能，并在相同全身控制栈条件下，相较人类遥操作者任务完成时间快达31.7%。这标志着首个仅利用纯RGB感知即可实现多样化关节式运动-操作能力的人形机器人仿真到真实迁移策略。",
        "translated_title": "开启从仿真到现实的人形像素到动作策略迁移之门",
        "label": [],
        "label_reason": "属于高阶机器人控制任务，非像素级图像处理",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "提出新颖sim-to-real框架与GRPO微调策略"
    },
    {
        "title": "Parameter Reduction Improves Vision Transformers: A Comparative Study of Sharing and Width Reduction",
        "url": "http://arxiv.org/abs/2512.01059v1",
        "pub_date": "2025-11-30",
        "summary": "Although scaling laws and many empirical results suggest that increasing the size of Vision Transformers often improves performance, model accuracy and training behavior are not always monotonically increasing with scale. Focusing on ViT-B/16 trained on ImageNet-1K, we study two simple parameter-reduction strategies applied to the MLP blocks, each removing 32.7\\% of the baseline parameters. Our \\emph{GroupedMLP} variant shares MLP weights between adjacent transformer blocks and achieves 81.47\\% top-1 accuracy while maintaining the baseline computational cost. Our \\emph{ShallowMLP} variant halves the MLP hidden dimension and reaches 81.25\\% top-1 accuracy with a 38\\% increase in inference throughput. Both models outperform the 86.6M-parameter baseline (81.05\\%) and exhibit substantially improved training stability, reducing peak-to-final accuracy degradation from 0.47\\% to the range 0.03\\% to 0.06\\%. These results suggest that, for ViT-B/16 on ImageNet-1K with a standard training recipe, the model operates in an overparameterized regime in which MLP capacity can be reduced without harming performance and can even slightly improve it. More broadly, our findings suggest that architectural constraints such as parameter sharing and reduced width may act as useful inductive biases, and highlight the importance of how parameters are allocated when designing Vision Transformers. All code is available at: https://github.com/AnanthaPadmanaban-KrishnaKumar/parameter-efficient-vit-mlps.",
        "translated": "尽管缩放定律和许多实证结果表明，增大视觉Transformer（ViT）的规模通常能提升性能，但模型精度与训练行为并不总是随规模单调递增。我们聚焦于在ImageNet-1K上训练的ViT-B/16模型，研究了两种应用于MLP模块的简单参数缩减策略，每种策略均削减了基准模型32.7%的参数量。我们的\\emph{GroupedMLP}变体在相邻Transformer块间共享MLP权重，在保持基准计算开销的同时实现了81.47%的top-1准确率。我们的\\emph{ShallowMLP}变体将MLP隐藏维度减半，在推理吞吐量提升38%的情况下达到了81.25%的top-1准确率。两个模型均优于参数量为86.6M的基准模型（81.05%），且表现出显著改进的训练稳定性，峰值至最终准确率退化比率从0.47%降低至0.03%至0.06%区间。这些结果表明，对于采用标准训练方案的ViT-B/16模型在ImageNet-1K上的表现，其处于过参数化状态——即MLP容量可在不损害性能的前提下被缩减，甚至略微提升性能。更广泛而言，我们的发现表明，如参数共享与宽度缩减等架构约束可能起到有益的归纳偏置作用，并强调在设计视觉Transformer时参数分配方式的重要性。所有代码可访问：https://github.com/AnanthaPadmanaban-KrishnaKumar/parameter-efficient-vit-mlps。",
        "translated_title": "参数缩减提升视觉Transformer性能：共享与宽度缩减的对比研究",
        "label": [],
        "label_reason": "研究模型压缩，非图像像素级处理",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出参数缩减策略提升ViT效率"
    },
    {
        "title": "TRoVe: Discovering Error-Inducing Static Feature Biases in Temporal Vision-Language Models",
        "url": "http://arxiv.org/abs/2512.01048v1",
        "pub_date": "2025-11-30",
        "summary": "Vision-language models (VLMs) have made great strides in addressing temporal understanding tasks, which involve characterizing visual changes across a sequence of images. However, recent works have suggested that when making predictions, VLMs may rely on static feature biases, such as background or object features, rather than dynamic visual changes. Static feature biases are a type of shortcut and can contribute to systematic prediction errors on downstream tasks; as a result, identifying and characterizing error-inducing static feature biases is critical prior to real-world model deployment. In this work, we introduce TRoVe, an automated approach for discovering error-inducing static feature biases learned by temporal VLMs. Given a trained VLM and an annotated validation dataset associated with a downstream classification task, TRoVe extracts candidate static features from the dataset and scores each feature by (i) the effect of the feature on classification errors as well as (ii) the extent to which the VLM relies on the feature when making predictions. In order to quantitatively evaluate TRoVe, we introduce an evaluation framework consisting of 101 trained temporal VLMs paired with ground-truth annotations for learned static feature biases. We use this framework to demonstrate that TRoVe can accurately identify error-inducing static feature biases in VLMs, achieving a 28.6% improvement over the closest baseline. Finally, we apply TRoVe to 7 off-the-shelf VLMs and 2 temporal understanding tasks, surfacing previously-unknown static feature biases and demonstrating that knowledge of learned biases can aid in improving model performance at test time. Our code is available at https://github.com/Stanford-AIMI/TRoVe.",
        "translated": "视觉-语言模型（VLMs）在处理涉及序列图像中视觉变化表征的时序理解任务方面取得了显著进展。然而，近期研究表明，当进行预测时，VLMs 可能依赖于静态特征偏置（如背景或物体特征），而非动态视觉变化。这类静态特征偏置属于一种捷径，会导致下游任务中系统性的预测误差；因此，在真实世界模型部署前，识别并刻画引发误差的静态特征偏置至关重要。本文提出 TRoVe，一种自动化方法，用于发现时序 VLMs 学习到的引发错误的静态特征偏置。给定一个训练好的 VLM 以及与下游分类任务相关的标注验证数据集，TRoVe 从数据集中提取候选静态特征，并通过以下两个维度对每个特征进行评分：(i) 特征对分类误差的影响程度，以及 (ii) VLM 在预测过程中对该特征的依赖程度。为定量评估 TRoVe，我们构建了一个包含 101 个训练好的时序 VLMs 以及其学习到的静态特征偏置真实标注的评估框架。我们利用该框架证明 TRoVe 能够准确识别 VLMs 中引发误差的静态特征偏置，相较最相近基线方法性能提升 28.6%。最后，我们将 TRoVe 应用于 7 个现成 VLMs 和 2 个时序理解任务，揭示了此前未知的静态特征偏置，并表明对所学偏置的认知有助于在测试阶段提升模型性能。我们的代码开源于 https://github.com/Stanford-AIMI/TRoVe。",
        "translated_title": "TRoVe：发现时序视觉-语言模型中的错误诱导静态特征偏差",
        "label": [],
        "label_reason": "研究视觉语言模型中的静态特征偏差，非图像像素级处理",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出自动化检测静态特征偏差的新方法，提升模型鲁棒性"
    },
    {
        "title": "Lotus-2: Advancing Geometric Dense Prediction with Powerful Image Generative Model",
        "url": "http://arxiv.org/abs/2512.01030v1",
        "pub_date": "2025-11-30",
        "summary": "Recovering pixel-wise geometric properties from a single image is fundamentally ill-posed due to appearance ambiguity and non-injective mappings between 2D observations and 3D structures. While discriminative regression models achieve strong performance through large-scale supervision, their success is bounded by the scale, quality and diversity of available data and limited physical reasoning. Recent diffusion models exhibit powerful world priors that encode geometry and semantics learned from massive image-text data, yet directly reusing their stochastic generative formulation is suboptimal for deterministic geometric inference: the former is optimized for diverse and high-fidelity image generation, whereas the latter requires stable and accurate predictions. In this work, we propose Lotus-2, a two-stage deterministic framework for stable, accurate and fine-grained geometric dense prediction, aiming to provide an optimal adaption protocol to fully exploit the pre-trained generative priors. Specifically, in the first stage, the core predictor employs a single-step deterministic formulation with a clean-data objective and a lightweight local continuity module (LCM) to generate globally coherent structures without grid artifacts. In the second stage, the detail sharpener performs a constrained multi-step rectified-flow refinement within the manifold defined by the core predictor, enhancing fine-grained geometry through noise-free deterministic flow matching. Using only 59K training samples, less than 1% of existing large-scale datasets, Lotus-2 establishes new state-of-the-art results in monocular depth estimation and highly competitive surface normal prediction. These results demonstrate that diffusion models can serve as deterministic world priors, enabling high-quality geometric reasoning beyond traditional discriminative and generative paradigms.",
        "translated": "从单张图像恢复像素级的几何属性本质上是病态问题，由于外观歧义以及二维观测与三维结构之间的非单射映射关系。虽然判别式回归模型在大规模监督下取得了优异性能，但其成功受限于可用数据的规模、质量和多样性，且物理推理能力有限。近期扩散模型展现出强大的世界先验知识，这些先验编码了从海量图文数据中学习到的几何与语义信息；然而，直接复用其随机生成框架对于确定性几何推断而言并不理想：前者针对多样化和高保真度图像生成进行优化，而后者则要求稳定且精确的预测。在本文中，我们提出Lotus-2，一种两阶段确定性框架，旨在实现稳定、准确且精细的几何密集预测，并提供最优适配协议以充分挖掘预训练生成先验。具体而言，在第一阶段，核心预测器采用单步确定性公式，结合干净数据目标与轻量局部连续模块（LCM），生成全局一致的结构而不产生网格伪影；在第二阶段，细节增强器在核心预测器定义的流形内执行约束多步修正流精炼，通过无噪声确定性流匹配提升几何细节精度。仅使用59K训练样本——远低于现有大规模数据集的1%——Lotus-2在单目深度估计任务上建立新SOTA结果，并在表面法线预测任务中表现极具竞争力。这些结果表明，扩散模型可作为确定性世界先验，使几何推理超越传统判别式与生成式范式，实现更高品质的几何理解。",
        "translated_title": "Lotus-2：利用强大的图像生成模型推进几何密集预测",
        "label": [],
        "label_reason": "目标为几何预测，非像素级图像质量恢复",
        "relevance_score": 3,
        "novelty_score": 7,
        "novelty_reason": "提出两阶段框架优化扩散模型用于几何推理"
    },
    {
        "title": "FOM-Nav: Frontier-Object Maps for Object Goal Navigation",
        "url": "http://arxiv.org/abs/2512.01009v1",
        "pub_date": "2025-11-30",
        "summary": "This paper addresses the Object Goal Navigation problem, where a robot must efficiently find a target object in an unknown environment. Existing implicit memory-based methods struggle with long-term memory retention and planning, while explicit map-based approaches lack rich semantic information. To address these challenges, we propose FOM-Nav, a modular framework that enhances exploration efficiency through Frontier-Object Maps and vision-language models. Our Frontier-Object Maps are built online and jointly encode spatial frontiers and fine-grained object information. Using this representation, a vision-language model performs multimodal scene understanding and high-level goal prediction, which is executed by a low-level planner for efficient trajectory generation. To train FOM-Nav, we automatically construct large-scale navigation datasets from real-world scanned environments. Extensive experiments validate the effectiveness of our model design and constructed dataset. FOM-Nav achieves state-of-the-art performance on the MP3D and HM3D benchmarks, particularly in navigation efficiency metric SPL, and yields promising results on a real robot.",
        "translated": "本文研究对象目标导航问题，其中机器人需在未知环境中高效定位目标物体。现有基于隐式记忆的方法在长期记忆保持与规划方面表现不佳，而基于显式地图的方法则缺乏丰富的语义信息。为应对这些挑战，我们提出FOM-Nav，一个模块化框架，通过前沿-物体地图（Frontier-Object Maps）与视觉-语言模型协同提升探索效率。我们的前沿-物体地图在线构建，联合编码空间前沿与细粒度物体信息。基于该表示，视觉-语言模型执行多模态场景理解及高层目标预测，由低层规划器生成高效轨迹。为训练FOM-Nav，我们从真实扫描环境自动构建大规模导航数据集。大量实验验证了所提模型设计与数据集的有效性。FOM-Nav在MP3D和HM3D基准测试中达到当前最优性能，特别是在导航效率指标SPL上，并在真实机器人平台上取得令人鼓舞的结果。",
        "translated_title": "FOM-Nav：面向目标导航的前沿物体地图",
        "label": [],
        "label_reason": "任务为机器人导航，非图像像素级处理",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "提出新框架与语义地图结合，但非图像恢复"
    },
    {
        "title": "LISA-3D: Lifting Language-Image Segmentation to 3D via Multi-View Consistency",
        "url": "http://arxiv.org/abs/2512.01008v1",
        "pub_date": "2025-11-30",
        "summary": "Text-driven 3D reconstruction demands a mask generator that simultaneously understands open-vocabulary instructions and remains consistent across viewpoints. We present LISA-3D, a two-stage framework that lifts language-image segmentation into 3D by retrofitting the instruction-following model LISA with geometry-aware Low-Rank Adaptation (LoRA) layers and reusing a frozen SAM-3D reconstructor. During training we exploit off-the-shelf RGB-D sequences and their camera poses to build a differentiable reprojection loss that enforces cross-view agreement without requiring any additional 3D-text supervision. The resulting masks are concatenated with RGB images to form RGBA prompts for SAM-3D, which outputs Gaussian splats or textured meshes without retraining. Across ScanRefer and Nr3D, LISA-3D improves language-to-3D accuracy by up to +15.6 points over single-view baselines while adapting only 11.6M parameters. The system is modular, data-efficient, and supports zero-shot deployment on unseen categories, providing a practical recipe for language-guided 3D content creation. Our code will be available at https://github.com/binisalegend/LISA-3D.",
        "translated": "文本驱动的三维重建需要一个能够同时理解开放词汇指令并在不同视角间保持一致性的掩码生成器。我们提出了LISA-3D，一个两阶段框架，通过为遵循指令的模型LISA添加几何感知的低秩适配（LoRA）层，并复用一个冻结的SAM-3D重构器，将语言-图像分割提升至三维空间。在训练过程中，我们利用现成的RGB-D序列及其相机位姿构建可微分的重投影损失函数，在无需任何额外三维-文本监督的情况下强制实现跨视角一致性。生成的掩码与RGB图像拼接形成RGBA提示输入给SAM-3D，该模型在无需再训练的情况下即可输出高斯点云或纹理化网格。在ScanRefer和Nr3D数据集上，LISA-3D相较单视角基线方法在语言到三维的准确性上最高提升15.6个点，仅需适应11.6M参数。该系统具有模块化、数据高效的特点，且支持在未见类别上的零样本部署，为语言引导的三维内容创作提供了一种实用方案。我们的代码将在https://github.com/binisalegend/LISA-3D公开。",
        "translated_title": "LISA-3D：通过多视角一致性将语言-图像分割提升至三维",
        "label": [],
        "label_reason": "属于3D重建与语言引导，非像素级图像处理",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "提出多视角一致性框架提升3D重建准确率"
    },
    {
        "title": "Provenance-Driven Reliable Semantic Medical Image Vector Reconstruction via Lightweight Blockchain-Verified Latent Fingerprints",
        "url": "http://arxiv.org/abs/2512.00999v1",
        "pub_date": "2025-11-30",
        "summary": "Medical imaging is essential for clinical diagnosis, yet real-world data frequently suffers from corruption, noise, and potential tampering, challenging the reliability of AI-assisted interpretation. Conventional reconstruction techniques prioritize pixel-level recovery and may produce visually plausible outputs while compromising anatomical fidelity, an issue that can directly impact clinical outcomes. We propose a semantic-aware medical image reconstruction framework that integrates high-level latent embeddings with a hybrid U-Net architecture to preserve clinically relevant structures during restoration. To ensure trust and accountability, we incorporate a lightweight blockchain-based provenance layer using scale-free graph design, enabling verifiable recording of each reconstruction event without imposing significant overhead. Extensive evaluation across multiple datasets and corruption types demonstrates improved structural consistency, restoration accuracy, and provenance integrity compared with existing approaches. By uniting semantic-guided reconstruction with secure traceability, our solution advances dependable AI for medical imaging, enhancing both diagnostic confidence and regulatory compliance in healthcare environments.",
        "translated": "医学成像对临床诊断至关重要，但现实世界中的数据常遭受损坏、噪声干扰及潜在篡改，这严重挑战了AI辅助解读的可靠性。传统重建技术侧重像素级恢复，虽可生成视觉上合理的输出，却可能牺牲解剖结构的真实性，这一问题会直接导致临床后果。我们提出一种语义感知的医学图像重建框架，通过将高层潜在嵌入与混合U-Net架构相结合，在恢复过程中保留临床相关的结构。为确保可信度与责任追溯，我们引入基于轻量级区块链的溯源层，采用无标度图设计，可在不显著增加开销的前提下实现每次重建事件的可验证记录。在多个数据集和多种退化类型上的广泛评估表明，相较现有方法，本方案在结构一致性、恢复精度及溯源完整性方面均有提升。通过融合语义引导重建与安全可追溯机制，我们的解决方案推动了医学影像领域可靠人工智能的发展，增强了医疗环境中诊断信心与合规性。",
        "translated_title": "基于溯源驱动的可靠语义医学图像向量重建方法：通过轻量级区块链验证的潜在指纹",
        "label": [],
        "label_reason": "聚焦医学图像语义重建与溯源，非像素级恢复任务",
        "relevance_score": 3,
        "novelty_score": 6,
        "novelty_reason": "结合区块链与语义重建，创新性中等偏弱"
    },
    {
        "title": "S2AM3D: Scale-controllable Part Segmentation of 3D Point Cloud",
        "url": "http://arxiv.org/abs/2512.00995v1",
        "pub_date": "2025-11-30",
        "summary": "Part-level point cloud segmentation has recently attracted significant attention in 3D computer vision. Nevertheless, existing research is constrained by two major challenges: native 3D models lack generalization due to data scarcity, while introducing 2D pre-trained knowledge often leads to inconsistent segmentation results across different views. To address these challenges, we propose S2AM3D, which incorporates 2D segmentation priors with 3D consistent supervision. We design a point-consistent part encoder that aggregates multi-view 2D features through native 3D contrastive learning, producing globally consistent point features. A scale-aware prompt decoder is then proposed to enable real-time adjustment of segmentation granularity via continuous scale signals. Simultaneously, we introduce a large-scale, high-quality part-level point cloud dataset with more than 100k samples, providing ample supervision signals for model training. Extensive experiments demonstrate that S2AM3D achieves leading performance across multiple evaluation settings, exhibiting exceptional robustness and controllability when handling complex structures and parts with significant size variations.",
        "translated": "点级点云分割近年来在三维计算机视觉领域受到了广泛关注。然而，现有研究受限于两大主要挑战：原生三维模型因数据稀缺而缺乏泛化能力；而引入二维预训练知识往往导致不同视角下分割结果不一致。为解决上述问题，我们提出S2AM3D方法，将二维分割先验与三维一致性监督相结合。我们设计了一种点一致的部件编码器，通过原生三维对比学习聚合多视角二维特征，生成全局一致的点特征。随后，提出一种尺度感知提示解码器，以连续尺度信号实现对分割粒度的实时调整。同时，我们构建了一个大规模、高质量的点级点云数据集，包含超过10万样本，为模型训练提供充足的监督信号。大量实验表明，S2AM3D在多种评估设置下均取得领先性能，在处理结构复杂且部件尺寸差异显著的情形时表现出卓越的鲁棒性与可控性。",
        "translated_title": "S2AM3D：尺度可控的三维点云部分分割",
        "label": [],
        "label_reason": "任务为3D点云分割，属高阶视觉任务",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "引入尺度可控解码器与对比学习提升性能"
    },
    {
        "title": "PhotoFramer: Multi-modal Image Composition Instruction",
        "url": "http://arxiv.org/abs/2512.00993v1",
        "pub_date": "2025-11-30",
        "summary": "Composition matters during the photo-taking process, yet many casual users struggle to frame well-composed images. To provide composition guidance, we introduce PhotoFramer, a multi-modal composition instruction framework. Given a poorly composed image, PhotoFramer first describes how to improve the composition in natural language and then generates a well-composed example image. To train such a model, we curate a large-scale dataset. Inspired by how humans take photos, we organize composition guidance into a hierarchy of sub-tasks: shift, zoom-in, and view-change tasks. Shift and zoom-in data are sampled from existing cropping datasets, while view-change data are obtained via a two-stage pipeline. First, we sample pairs with varying viewpoints from multi-view datasets, and train a degradation model to transform well-composed photos into poorly composed ones. Second, we apply this degradation model to expert-taken photos to synthesize poor images to form training pairs. Using this dataset, we finetune a model that jointly processes and generates both text and images, enabling actionable textual guidance with illustrative examples. Extensive experiments demonstrate that textual instructions effectively steer image composition, and coupling them with exemplars yields consistent improvements over exemplar-only baselines. PhotoFramer offers a practical step toward composition assistants that make expert photographic priors accessible to everyday users. Codes, model weights, and datasets have been released in https://zhiyuanyou.github.io/photoframer.",
        "translated": "构图在摄影过程中至关重要，然而许多普通用户难以拍摄出构图良好的照片。为提供构图指导，我们提出了 PhotoFramer，一种多模态构图指令框架。给定一张构图不佳的照片，PhotoFramer 首先用自然语言描述如何改进构图，随后生成一张构图优良的示例图像。为训练此类模型，我们构建了一个大规模数据集。受人类拍照方式启发，我们将构图指导组织为一系列子任务的层级结构：平移、放大和视角变换任务。平移与放大数据采样自现有裁剪数据集，而视角变换数据则通过两阶段流程获得。首先，我们在多视角数据集中采样具有不同视角的图像对，并训练一个退化模型，将构图良好的照片转化为构图不佳的照片；其次，我们将该退化模型应用于专业摄影师拍摄的照片，合成构图不佳的图像以形成训练对。利用该数据集，我们微调了一个联合处理并生成文本与图像的模型，从而实现兼具可操作性文本指导和具象示例的效果。大量实验表明，文本指令能有效引导图像构图，且结合示例相较仅使用示例的基线方法始终带来显著提升。PhotoFramer 为构图辅助工具提供了切实可行的一步，使专业摄影先验知识得以惠及普通用户。代码、模型权重及数据集已发布于 https://zhiyuanyou.github.io/photoframer。",
        "translated_title": "PhotoFramer：多模态图像合成指令",
        "label": [],
        "label_reason": "提供构图指导，非像素级图像恢复任务",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "组合文本与图像生成，属高阶视觉任务"
    },
    {
        "title": "MM-ACT: Learn from Multimodal Parallel Generation to Act",
        "url": "http://arxiv.org/abs/2512.00975v1",
        "pub_date": "2025-11-30",
        "summary": "A generalist robotic policy needs both semantic understanding for task planning and the ability to interact with the environment through predictive capabilities. To tackle this, we present MM-ACT, a unified Vision-Language-Action (VLA) model that integrates text, image, and action in shared token space and performs generation across all three modalities. MM-ACT adopts a re-mask parallel decoding strategy for text and image generation, and employs a one-step parallel decoding strategy for action generation to improve efficiency. We introduce Context-Shared Multimodal Learning, a unified training paradigm that supervises generation in all three modalities from a shared context, enhancing action generation through cross-modal learning. Experiments were conducted on the LIBERO simulation and Franka real-robot setups as well as RoboTwin2.0 to assess in-domain and out-of-domain performances respectively. Our approach achieves a success rate of 96.3% on LIBERO, 72.0% across three tasks of real Franka, and 52.38% across eight bimanual tasks of RoboTwin2.0 with an additional gain of 9.25% from cross-modal learning. We release our codes, models and data at https://github.com/HHYHRHY/MM-ACT.",
        "translated": "一个通用的机器人策略需要兼具任务规划所需的语义理解能力，以及通过预测能力与环境交互的能力。为解决这一问题，我们提出 MM-ACT，这是一个统一的视觉-语言-动作（VLA）模型，它在共享的 token 空间中整合文本、图像和动作，并能在三模态间进行生成。MM-ACT 采用重掩码并行解码策略分别处理文本和图像生成，同时采用一步并行解码策略处理动作生成，以提升效率。我们引入上下文共享多模态学习，这是一种统一的训练范式，从共享上下文对三模态的生成进行监督，并通过跨模态学习增强动作生成能力。我们在 LIBERO 模拟环境、Franka 真实机器人平台以及 RoboTwin2.0 上开展实验，分别评估其域内与域外性能。我们的方法在 LIBERO 上达到 96.3% 的成功率，在真实 Franka 的三项任务上实现 72.0% 的成功率，在 RoboTwin2.0 的八项双臂任务中取得 52.38% 的成功率，且跨模态学习额外带来 9.25% 的性能增益。我们将在 https://github.com/HHYHRHY/MM-ACT 释放代码、模型和数据。",
        "translated_title": "MM-ACT：从多模态并行生成中学习以行动",
        "label": [],
        "label_reason": "任务为多模态机器人控制，非图像像素级处理",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "提出跨模态生成新范式，提升动作规划效率"
    },
    {
        "title": "Efficient and Scalable Monocular Human-Object Interaction Motion Reconstruction",
        "url": "http://arxiv.org/abs/2512.00960v1",
        "pub_date": "2025-11-30",
        "summary": "Generalized robots must learn from diverse, large-scale human-object interactions (HOI) to operate robustly in the real world. Monocular internet videos offer a nearly limitless and readily available source of data, capturing an unparalleled diversity of human activities, objects, and environments. However, accurately and scalably extracting 4D interaction data from these in-the-wild videos remains a significant and unsolved challenge. Thus, in this work, we introduce 4DHOISolver, a novel and efficient optimization framework that constrains the ill-posed 4D HOI reconstruction problem by leveraging sparse, human-in-the-loop contact point annotations, while maintaining high spatio-temporal coherence and physical plausibility. Leveraging this framework, we introduce Open4DHOI, a new large-scale 4D HOI dataset featuring a diverse catalog of 144 object types and 103 actions. Furthermore, we demonstrate the effectiveness of our reconstructions by enabling an RL-based agent to imitate the recovered motions. However, a comprehensive benchmark of existing 3D foundation models indicates that automatically predicting precise human-object contact correspondences remains an unsolved problem, underscoring the immediate necessity of our human-in-the-loop strategy while posing an open challenge to the community. Data and code will be publicly available at https://wenboran2002.github.io/open4dhoi/",
        "translated": "通用机器人必须从多样且大规模的人-物体交互（HOI）数据中学习，以在真实世界中稳健运行。单目互联网视频提供了近乎无限且易于获取的数据源，能够捕捉前所未有的人类活动、物体及环境多样性。然而，从这些野外视频中准确且可扩展地提取4D交互数据仍是一个重大且未解决的挑战。为此，本文提出了4DHOISolver，这是一种新颖高效的优化框架，通过利用稀疏的人类参与式接触点标注来约束病态的4D HOI重建问题，同时保持高时空连贯性与物理合理性。基于此框架，我们引入了Open4DHOI，这是一个全新的大规模4D HOI数据集，包含144种物体类型和103种动作的多样化目录。此外，我们通过使强化学习（RL）代理模仿重建后的运动，验证了我们的重建效果。然而，对现有3D基础模型的全面基准测试表明，自动精确预测人-物体接触对应关系仍是一个未解决的问题，这凸显了我们人类参与式策略的迫切必要性，同时也为社区提出了一个开放性挑战。数据与代码将公开于 https://wenboran2002.github.io/open4dhoi/",
        "translated_title": "高效可扩展的单目人-物体交互运动重建",
        "label": [],
        "label_reason": "目标为4D运动重建，属高阶视觉任务",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "提出高效优化框架与新数据集，具显著改进"
    },
    {
        "title": "Adaptive Evidential Learning for Temporal-Semantic Robustness in Moment Retrieval",
        "url": "http://arxiv.org/abs/2512.00953v1",
        "pub_date": "2025-11-30",
        "summary": "In the domain of moment retrieval, accurately identifying temporal segments within videos based on natural language queries remains challenging. Traditional methods often employ pre-trained models that struggle with fine-grained information and deterministic reasoning, leading to difficulties in aligning with complex or ambiguous moments. To overcome these limitations, we explore Deep Evidential Regression (DER) to construct a vanilla Evidential baseline. However, this approach encounters two major issues: the inability to effectively handle modality imbalance and the structural differences in DER's heuristic uncertainty regularizer, which adversely affect uncertainty estimation. This misalignment results in high uncertainty being incorrectly associated with accurate samples rather than challenging ones. Our observations indicate that existing methods lack the adaptability required for complex video scenarios. In response, we propose Debiased Evidential Learning for Moment Retrieval (DEMR), a novel framework that incorporates a Reflective Flipped Fusion (RFF) block for cross-modal alignment and a query reconstruction task to enhance text sensitivity, thereby reducing bias in uncertainty estimation. Additionally, we introduce a Geom-regularizer to refine uncertainty predictions, enabling adaptive alignment with difficult moments and improving retrieval accuracy. Extensive testing on standard datasets and debiased datasets ActivityNet-CD and Charades-CD demonstrates significant enhancements in effectiveness, robustness, and interpretability, positioning our approach as a promising solution for temporal-semantic robustness in moment retrieval. The code is publicly available at https://github.com/KaijingOfficial/DEMR.",
        "translated": "在时刻检索领域，基于自然语言查询准确识别视频中的时间片段仍具挑战性。传统方法通常采用预训练模型，其难以处理细粒度信息及确定性推理，导致与复杂或模糊时刻的对齐困难。为克服这些局限，我们探索深度证据回归（Deep Evidential Regression, DER），构建一个基础的证据学基线模型。然而，该方法存在两大主要问题：无法有效应对模态不平衡，以及DER中启发式不确定性正则器的结构差异，从而损害了不确定性估计的效果。这种错位导致高不确定性被错误地关联到准确样本，而非真正困难的样本。我们的观察表明，现有方法缺乏应对复杂视频场景所需的适应能力。为此，我们提出了一种面向时刻检索的去偏证据学习框架（Debiased Evidential Learning for Moment Retrieval, DEMR），该框架引入反射翻转融合（Reflective Flipped Fusion, RFF）模块以实现跨模态对齐，并通过查询重构任务提升文本敏感性，从而降低不确定性估计中的偏差。此外，我们引入几何正则化器（Geom-regularizer）以优化不确定性预测，使其能够自适应地与困难时刻对齐，进而提高检索精度。在标准数据集及去偏数据集ActivityNet-CD和Charades-CD上的广泛实验表明，本方法在有效性、鲁棒性和可解释性方面均显著提升，展现出在时刻语义鲁棒性任务中的巨大潜力。代码已公开于 https://github.com/KaijingOfficial/DEMR。",
        "translated_title": "面向时刻检索的自适应证据学习以实现时序-语义鲁棒性",
        "label": [],
        "label_reason": "任务为视频时刻检索，属高阶语义理解",
        "relevance_score": 1,
        "novelty_score": 6,
        "novelty_reason": "改进证据回归框架，但非图像像素级处理"
    },
    {
        "title": "Binary-Gaussian: Compact and Progressive Representation for 3D Gaussian Segmentation",
        "url": "http://arxiv.org/abs/2512.00944v1",
        "pub_date": "2025-11-30",
        "summary": "3D Gaussian Splatting (3D-GS) has emerged as an efficient 3D representation and a promising foundation for semantic tasks like segmentation. However, existing 3D-GS-based segmentation methods typically rely on high-dimensional category features, which introduce substantial memory overhead. Moreover, fine-grained segmentation remains challenging due to label space congestion and the lack of stable multi-granularity control mechanisms. To address these limitations, we propose a coarse-to-fine binary encoding scheme for per-Gaussian category representation, which compresses each feature into a single integer via the binary-to-decimal mapping, drastically reducing memory usage. We further design a progressive training strategy that decomposes panoptic segmentation into a series of independent sub-tasks, reducing inter-class conflicts and thereby enhancing fine-grained segmentation capability. Additionally, we fine-tune opacity during segmentation training to address the incompatibility between photometric rendering and semantic segmentation, which often leads to foreground-background confusion. Extensive experiments on multiple benchmarks demonstrate that our method achieves state-of-the-art segmentation performance while significantly reducing memory consumption and accelerating inference.",
        "translated": "三维高斯点云（3D-GS）已成为一种高效的三维表示方法，并有望作为语义任务（如分割）的基础。然而，现有的基于3D-GS的分割方法通常依赖于高维类别特征，这会引入显著的内存开销。此外，由于标签空间拥挤以及缺乏稳定的多粒度控制机制，细粒度分割仍面临挑战。为解决上述问题，我们提出了一种针对每个高斯点类别的粗到精二进制编码方案，通过二进制到十进制映射将每个特征压缩为单个整数，从而大幅降低内存占用。我们进一步设计了一种渐进式训练策略，将全景分割分解为一系列独立的子任务，减少类别间冲突，从而增强细粒度分割能力。此外，我们在分割训练过程中微调不透明度，以缓解光度渲染与语义分割之间的不兼容性，该不兼容性常导致前景-背景混淆。在多个基准数据集上的大量实验表明，我们的方法在实现最先进分割性能的同时，显著降低了内存消耗并加速了推理过程。",
        "translated_title": "Binary-Gaussian：用于三维高斯分割的紧凑与渐进式表示",
        "label": [],
        "label_reason": "3D分割属高阶语义任务，非像素级图像处理",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出二进制编码与渐进训练策略提升分割性能"
    },
    {
        "title": "SceneProp: Combining Neural Network and Markov Random Field for Scene-Graph Grounding",
        "url": "http://arxiv.org/abs/2512.00936v1",
        "pub_date": "2025-11-30",
        "summary": "Grounding complex, compositional visual queries with multiple objects and relationships is a fundamental challenge for vision-language models. While standard phrase grounding methods excel at localizing single objects, they lack the structural inductive bias to parse intricate relational descriptions, often failing as queries become more descriptive. To address this structural deficit, we focus on scene-graph grounding, a powerful but less-explored formulation where the query is an explicit graph of objects and their relationships. However, existing methods for this task also struggle, paradoxically showing decreased performance as the query graph grows -- failing to leverage the very information that should make grounding easier. We introduce SceneProp, a novel method that resolves this issue by reformulating scene-graph grounding as a Maximum a Posteriori (MAP) inference problem in a Markov Random Field (MRF). By performing global inference over the entire query graph, SceneProp finds the optimal assignment of image regions to nodes that jointly satisfies all constraints. This is achieved within an end-to-end framework via a differentiable implementation of the Belief Propagation algorithm. Experiments on four benchmarks show that our dedicated focus on the scene-graph grounding formulation allows SceneProp to significantly outperform prior work. Critically, its accuracy consistently improves with the size and complexity of the query graph, demonstrating for the first time that more relational context can, and should, lead to better grounding. Codes are available at https://github.com/keitaotani/SceneProp.",
        "translated": " grounding 复杂且具有组合结构的视觉查询（涉及多个物体及其关系）是视觉-语言模型面临的一项根本性挑战。尽管标准短语 grounding 方法在定位单个物体方面表现优异，但它们缺乏解析复杂关系描述所需的结构化归纳偏置，当查询语句变得更具描述性时往往失效。为解决这一结构性缺陷，我们聚焦于场景图 grounding，这是一种强大但尚未充分探索的表述方式——其中查询被明确定义为一个包含物体及其关系的显式图结构。然而，现有方法同样存在困难，甚至呈现出悖论性的现象：随着查询图规模增大，其性能反而下降——未能利用本应使 grounding 更易实现的关键信息。为此，我们提出了 SceneProp，一种新颖的方法，通过将场景图 grounding 重新建模为马尔可夫随机场（MRF）中的最大后验概率（MAP）推理问题来解决此问题。SceneProp 在整个查询图上执行全局推理，寻找最优的图像区域到节点的分配方案，以同时满足所有约束条件。该方法通过可微分实现的信念传播算法，在端到端框架内完成上述过程。在四个基准数据集上的实验表明，由于我们的方法专门针对场景图 grounding 表述进行优化，SceneProp 显著优于当前工作。尤为重要的是，其准确率随着查询图规模和复杂度的增加而持续提升，首次证明了更丰富的关系上下文不仅能够、而且应当有助于更精准的 grounding。代码已开源于 https://github.com/keitaotani/SceneProp。",
        "translated_title": "SceneProp：结合神经网络与马尔可夫随机场的场景图定位",
        "label": [],
        "label_reason": "任务为高阶视觉场景理解，非像素级图像处理",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "首次将MRF与BP用于场景图接地，结构创新显著"
    },
    {
        "title": "LAHNet: Local Attentive Hashing Network for Point Cloud Registration",
        "url": "http://arxiv.org/abs/2512.00927v1",
        "pub_date": "2025-11-30",
        "summary": "Most existing learning-based point cloud descriptors for point cloud registration focus on perceiving local information of point clouds to generate distinctive features. However, a reasonable and broader receptive field is essential for enhancing feature distinctiveness. In this paper, we propose a Local Attentive Hashing Network for point cloud registration, called LAHNet, which introduces a local attention mechanism with the inductive bias of locality of convolution-like operators into point cloud descriptors. Specifically, a Group Transformer is designed to capture reasonable long-range context between points. This employs a linear neighborhood search strategy, Locality-Sensitive Hashing, enabling uniformly partitioning point clouds into non-overlapping windows. Meanwhile, an efficient cross-window strategy is adopted to further expand the reasonable feature receptive field. Furthermore, building on this effective windowing strategy, we propose an Interaction Transformer to enhance the feature interactions of the overlap regions within point cloud pairs. This computes an overlap matrix to match overlap regions between point cloud pairs by representing each window as a global signal. Extensive results demonstrate that LAHNet can learn robust and distinctive features, achieving significant registration results on real-world indoor and outdoor benchmarks.",
        "translated": "现有基于学习的点云描述子主要聚焦于感知点云的局部信息以生成具有区分性的特征。然而，为了增强特征的区分性，一个合理且更广的感受野至关重要。本文提出了一种用于点云配准的局部注意力哈希网络（LAHNet），该网络将卷积类算子所具有的局部归纳偏置引入点云描述子中，通过局部注意力机制实现这一目标。具体而言，设计了一种组变换器，用以捕捉点之间的合理长程上下文；该变换器采用线性邻域搜索策略——局部敏感哈希（Locality-Sensitive Hashing），从而将点云均匀划分为无重叠的窗口。同时，采用一种高效的跨窗口策略，进一步扩展合理的特征感受野。在此基础上，我们提出了一种交互变换器，用于增强点云对之间重叠区域的特征交互：该方法通过将每个窗口表示为全局信号，并计算重叠矩阵，匹配点云对间的重叠区域。大量实验结果表明，LAHNet 能够学习到鲁棒且具有区分性的特征，在真实世界室内与室外基准数据集上实现了显著的配准效果。",
        "translated_title": "LAHNet：用于点云配准的局部注意力哈希网络",
        "label": [],
        "label_reason": "处理点云配准，非图像像素级恢复任务",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "引入局部注意力机制提升特征区分度"
    },
    {
        "title": "MMAG: Mixed Memory-Augmented Generation for Large Language Models Applications",
        "url": "http://arxiv.org/abs/2512.01710v1",
        "pub_date": "2025-12-01",
        "summary": "Large Language Models (LLMs) excel at generating coherent text within a single prompt but fall short in sustaining relevance, personalization, and continuity across extended interactions. Human communication, however, relies on multiple forms of memory, from recalling past conversations to adapting to personal traits and situational context. This paper introduces the Mixed Memory-Augmented Generation (MMAG) pattern, a framework that organizes memory for LLM-based agents into five interacting layers: conversational, long-term user, episodic and event-linked, sensory and context-aware, and short-term working memory. Drawing inspiration from cognitive psychology, we map these layers to technical components and outline strategies for coordination, prioritization, and conflict resolution. We demonstrate the approach through its implementation in the Heero conversational agent, where encrypted long-term bios and conversational history already improve engagement and retention. We further discuss implementation concerns around storage, retrieval, privacy, and latency, and highlight open challenges. MMAG provides a foundation for building memory-rich language agents that are more coherent, proactive, and aligned with human needs.",
        "translated": "大语言模型（LLMs）在单次提示下擅长生成连贯文本，但在持续交互中难以维持相关性、个性化与连续性。然而，人类交流依赖多种记忆形式，包括回忆过往对话、适应个人特质和情境背景。本文提出混合记忆增强生成（MMAG）范式，为基于LLM的智能体组织记忆，将其划分为五个相互作用层级：对话记忆层、长期用户记忆层、事件关联型情景记忆层、感官与情境感知记忆层以及短期工作记忆层。受认知心理学启发，我们将这些记忆层级映射至技术组件，并概述协调机制、优先级排序策略及冲突解决方法。我们通过在Heero对话智能体中的实现验证该框架，其中加密的长期用户档案与对话历史已显著提升用户参与度与留存率。此外，我们进一步讨论存储、检索、隐私与延迟等实施考量，并指出当前尚未解决的关键挑战。MMAG为构建具备丰富记忆能力、更加连贯、主动且契合人类需求的语言智能体奠定了基础。",
        "translated_title": "MMAG：面向大语言模型应用的混合记忆增强生成",
        "label": [],
        "label_reason": "论文聚焦LLM记忆机制，非推荐系统核心环节",
        "relevance_score": 3,
        "novelty_score": 6,
        "novelty_reason": "引入多层记忆框架，但未用于推荐场景"
    },
    {
        "title": "Structured Spectral Reasoning for Frequency-Adaptive Multimodal Recommendation",
        "url": "http://arxiv.org/abs/2512.01372v1",
        "pub_date": "2025-12-01",
        "summary": "Multimodal recommendation aims to integrate collaborative signals with heterogeneous content such as visual and textual information, but remains challenged by modality-specific noise, semantic inconsistency, and unstable propagation over user-item graphs. These issues are often exacerbated by naive fusion or shallow modeling strategies, leading to degraded generalization and poor robustness. While recent work has explored the frequency domain as a lens to separate stable from noisy signals, most methods rely on static filtering or reweighting, lacking the ability to reason over spectral structure or adapt to modality-specific reliability. To address these challenges, we propose a Structured Spectral Reasoning (SSR) framework for frequency-aware multimodal recommendation. Our method follows a four-stage pipeline: (i) Decompose graph-based multimodal signals into spectral bands via graph-guided transformations to isolate semantic granularity; (ii) Modulate band-level reliability with spectral band masking, a training-time masking with a prediction-consistency objective that suppresses brittle frequency components; (iii) Fuse complementary frequency cues using hyperspectral reasoning with low-rank cross-band interaction; and (iv) Align modality-specific spectral features via contrastive regularization to promote semantic and structural consistency. Experiments on three real-world benchmarks show consistent gains over strong baselines, particularly under sparse and cold-start settings. Additional analyses indicate that structured spectral modeling improves robustness and provides clearer diagnostics of how different bands contribute to performance.",
        "translated": "多模态推荐旨在融合协同信号与视觉、文本等异构内容，但面临模态特定噪声、语义不一致以及在用户-物品图上传播不稳定等挑战。这些问题常因朴素的融合或浅层建模策略而加剧，导致泛化能力下降和鲁棒性不足。尽管近期工作已尝试以频域视角区分稳定与噪声信号，多数方法依赖静态滤波或重加权，缺乏对频谱结构推理的能力，亦无法适应不同模态的可靠性特性。为应对上述挑战，我们提出一种面向频域感知的多模态推荐框架——结构化频谱推理（Structured Spectral Reasoning, SSR）。该方法遵循四阶段流程：(i) 通过图引导变换将基于图的多模态信号分解为频谱带，以分离语义粒度；(ii) 利用频谱带掩码机制调节各频带可靠性，该机制在训练时引入预测一致性目标，抑制脆弱频段成分；(iii) 采用高光谱推理结合低秩跨频带交互，融合互补频域线索；(iv) 通过对比正则化对齐各模态的频谱特征，促进语义与结构一致性。在三个真实世界基准数据集上的实验表明，SSR 相较于强基线模型持续取得性能提升，尤其在稀疏与冷启动场景下表现突出。附加分析显示，结构化频谱建模可增强鲁棒性，并提供更清晰的诊断，揭示不同频带对性能贡献的具体方式。",
        "translated_title": "面向频域自适应多模态推荐的结构化谱推理",
        "label": [
            "多模态推荐",
            "重排",
            "负采样与对比学习"
        ],
        "label_reason": "聚焦多模态推荐中的频域建模与结构化推理",
        "relevance_score": 9,
        "novelty_score": 8,
        "novelty_reason": "提出频域结构化推理框架，结合对比正则化提升鲁棒性"
    },
    {
        "title": "Toward a benchmark for CTR prediction in online advertising: datasets, evaluation protocols and perspectives",
        "url": "http://arxiv.org/abs/2512.01179v1",
        "pub_date": "2025-12-01",
        "summary": "This research designs a unified architecture of CTR prediction benchmark (Bench-CTR) platform that offers flexible interfaces with datasets and components of a wide range of CTR prediction models. Moreover, we construct a comprehensive system of evaluation protocols encompassing real-world and synthetic datasets, a taxonomy of metrics, standardized procedures and experimental guidelines for calibrating the performance of CTR prediction models. Furthermore, we implement the proposed benchmark platform and conduct a comparative study to evaluate a wide range of state-of-the-art models from traditional multivariate statistical to modern large language model (LLM)-based approaches on three public datasets and two synthetic datasets. Experimental results reveal that, (1) high-order models largely outperform low-order models, though such advantage varies in terms of metrics and on different datasets; (2) LLM-based models demonstrate a remarkable data efficiency, i.e., achieving the comparable performance to other models while using only 2% of the training data; (3) the performance of CTR prediction models has achieved significant improvements from 2015 to 2016, then reached a stage with slow progress, which is consistent across various datasets. This benchmark is expected to facilitate model development and evaluation and enhance practitioners' understanding of the underlying mechanisms of models in the area of CTR prediction. Code is available at https://github.com/NuriaNinja/Bench-CTR.",
        "translated": "本研究设计了一个统一的点击率（CTR）预测基准平台（Bench-CTR），该平台提供灵活的接口，支持多种CTR预测模型的数据集与组件。此外，我们构建了一套全面的评估体系，涵盖真实世界与合成数据集、指标分类体系、标准化实验流程及性能校准指南，以系统性地评估CTR预测模型的表现。进一步地，我们实现了所提出的基准平台，并在三个公开数据集和两个合成数据集上开展对比实验，评测从传统多元统计模型到现代基于大语言模型（LLM）方法的一系列前沿模型。实验结果表明：（1）高阶模型在多数情况下显著优于低阶模型，但其优势因评估指标和数据集不同而有所差异；（2）基于LLM的模型展现出卓越的数据效率——仅使用训练数据的2%即可达到与其他模型相当的性能；（3）CTR预测模型的整体性能自2015年至2016年大幅提升后，进入进展缓慢的平台期，这一趋势在多个数据集上均保持一致。该基准平台有望促进模型开发与评估，加深从业者对CTR预测模型内在机制的理解。代码已开源，详见 https://github.com/NuriaNinja/Bench-CTR。",
        "translated_title": "面向在线广告点击率预测的基准：数据集、评估协议与研究视角",
        "label": [
            "精排（Ranking）",
            "通用推荐技术（General Recommendation Techniques）"
        ],
        "label_reason": "聚焦CTR预测，属推荐系统精排核心任务",
        "relevance_score": 8,
        "novelty_score": 7,
        "novelty_reason": "构建统一评测平台，推动模型比较与评估标准化"
    },
    {
        "title": "Conversion rate prediction in online advertising: modeling techniques, performance evaluation and future directions",
        "url": "http://arxiv.org/abs/2512.01171v1",
        "pub_date": "2025-12-01",
        "summary": "Conversion and conversion rate (CVR) prediction play a critical role in efficient advertising decision-making. In past decades, although researchers have developed plenty of models for CVR prediction, the methodological evolution and relationships between different techniques have been precluded. In this paper, we conduct a comprehensive literature review on CVR prediction in online advertising, and classify state-of-the-art CVR prediction models into six categories with respect to the underlying techniques and elaborate on connections between these techniques. For each category of models, we present the framework of underlying techniques, their advantages and disadvantages, and discuss how they are utilized for CVR prediction. Moreover, we summarize the performance of various CVR prediction models on public and proprietary datasets. Finally, we identify research trends, major challenges, and promising future directions. We observe that results of performance evaluation reported in prior studies are not unanimous; semantics-enriched, attribution-enhanced, debiased CVR prediction and jointly modeling CTR and CVR prediction would be promising directions to explore in the future. This review is expected to provide valuable references and insights for future researchers and practitioners in this area.",
        "translated": "转化率（CVR）预测与转化预测在高效广告决策中发挥着关键作用。过去几十年来，尽管研究人员已开发出大量用于CVR预测的模型，但不同技术之间的方法论演化及其相互关系尚未被系统梳理。本文对在线广告中的CVR预测进行了全面文献综述，并依据底层技术将当前最先进的CVR预测模型划分为六大类，详细阐述了各类技术间的关联性。对于每一类模型，我们介绍了其底层技术框架、优缺点，并探讨了它们如何应用于CVR预测。此外，我们总结了各类CVR预测模型在公开及自有数据集上的性能表现。最后，我们指出了研究趋势、主要挑战及未来有前景的研究方向。我们观察到，以往研究中报告的性能评估结果并不统一；语义增强、归因增强、去偏的CVR预测以及联合建模点击率（CTR）与CVR预测将是未来值得探索的重要方向。本综述有望为该领域的未来研究者和从业者提供有价值的参考与洞见。",
        "translated_title": "在线广告中的转化率预测：建模技术、性能评估与未来方向",
        "label": [],
        "label_reason": "聚焦广告转化率预测，非推荐系统核心问题",
        "relevance_score": 3,
        "novelty_score": 3,
        "novelty_reason": "为方法综述，无新模型或架构提出"
    },
    {
        "title": "Optimizing Generative Ranking Relevance via Reinforcement Learning in Xiaohongshu Search",
        "url": "http://arxiv.org/abs/2512.00968v1",
        "pub_date": "2025-11-30",
        "summary": "Ranking relevance is a fundamental task in search engines, aiming to identify the items most relevant to a given user query. Traditional relevance models typically produce scalar scores or directly predict relevance labels, limiting both interpretability and the modeling of complex relevance signals. Inspired by recent advances in Chain-of-Thought (CoT) reasoning for complex tasks, we investigate whether explicit reasoning can enhance both interpretability and performance in relevance modeling. However, existing reasoning-based Generative Relevance Models (GRMs) primarily rely on supervised fine-tuning on large amounts of human-annotated or synthetic CoT data, which often leads to limited generalization. Moreover, domain-agnostic, free-form reasoning tends to be overly generic and insufficiently grounded, limiting its potential to handle the diverse and ambiguous cases prevalent in open-domain search. In this work, we formulate relevance modeling in Xiaohongshu search as a reasoning task and introduce a Reinforcement Learning (RL)-based training framework to enhance the grounded reasoning capabilities of GRMs. Specifically, we incorporate practical business-specific relevance criteria into the multi-step reasoning prompt design and propose Stepwise Advantage Masking (SAM), a lightweight process-supervision strategy which facilitates effective learning of these criteria through improved credit assignment. To enable industrial deployment, we further distill the large-scale RL-tuned model to a lightweight version suitable for real-world search systems. Extensive experiments on industrial datasets, along with online A/B tests, demonstrate the effectiveness of our approach.",
        "translated": "排序相关性是搜索引擎中的基础任务，旨在识别与给定用户查询最相关的物料。传统相关性模型通常输出标量分数或直接预测相关性标签，这限制了其可解释性，并难以建模复杂的相关信号。受近期针对复杂任务的链式思维（Chain-of-Thought, CoT）推理方法进展启发，我们探究显式推理是否能够提升相关性建模的可解释性与性能。然而，现有的基于推理的生成式相关性模型（Generative Relevance Models, GRMs）主要依赖于大规模人工标注或合成的CoT数据进行监督微调，往往导致泛化能力有限。此外，领域无关、自由形式的推理常过于通用且缺乏具体依据，难以有效应对开放域搜索中普遍存在的多样化和模糊性场景。在本工作中，我们将小红书搜索中的相关性建模形式化为一个推理任务，并引入基于强化学习（Reinforcement Learning, RL）的训练框架以增强GRMs的具身推理能力。具体而言，我们在多步推理提示设计中融入实际业务相关的相关性标准，并提出逐步优势掩码（Stepwise Advantage Masking, SAM），这是一种轻量级过程监督策略，通过改进奖励归因机制，促进对这些标准的有效学习。为进一步实现工业部署，我们进一步将大规模RL微调后的模型蒸馏至适合现实搜索系统使用的轻量化版本。在工业数据集上的大量实验以及线上A/B测试均验证了本方法的有效性。",
        "translated_title": "小红书搜索中的生成式排序相关性优化研究",
        "label": [
            "精排",
            "LLM生成式推荐"
        ],
        "label_reason": "基于RL优化生成式排序，适配推荐系统精排环节",
        "relevance_score": 9,
        "novelty_score": 8,
        "novelty_reason": "引入RL+SAM策略提升推理接地性与可部署性"
    },
    {
        "title": "SHRAG: AFrameworkfor Combining Human-Inspired Search with RAG",
        "url": "http://arxiv.org/abs/2512.00772v1",
        "pub_date": "2025-11-30",
        "summary": "Retrieval-Augmented Generation (RAG) is gaining recognition as one of the key technological axes for next generation information retrieval, owing to its ability to mitigate the hallucination phenomenon in Large Language   Models (LLMs)and effectively incorporate up-to-date information. However, specialized expertise is necessary to   construct ahigh-quality retrieval system independently; moreover, RAGdemonstratesrelativelyslowerprocessing   speeds compared to conventional pure retrieval systems because it involves both retrieval and generation stages.   Accordingly, this study proposes SHRAG, a novel framework designed to facilitate the seamless integration of   Information Retrieval and RAG while simultaneously securing precise retrieval performance. SHRAG utilizes a   Large Language Model as a Query Strategist to automatically transform unstructured natural language queries   into logically structured search queries, subsequently performing Boolean retrieval to emulate the search process   of an expert human searcher. Furthermore, it incorporates multilingual query expansion and a multilingual   embedding model, enabling it to perform efficient cross-lingual question answering within the multilingual   dataset environment of the ScienceON Challenge. Experimental results demonstrate that the proposed method,   combining logical retrieval capabilities and generative reasoning, can significantly enhance the accuracy and   reliability of RAG systems. Furthermore, SHRAG movesbeyondconventionaldocument-centric retrieval methods,   presenting the potential for a new search paradigm capable of providing direct and reliable responses to queries.",
        "translated": "检索增强生成（RAG）因其能够缓解大语言模型（LLMs）中的幻觉现象，并有效整合最新信息，正逐渐被视为下一代信息检索的关键技术之一。然而，构建一个高质量的独立检索系统需要专门的专业知识；此外，由于RAG需经历检索与生成两个阶段，其处理速度相较于传统的纯检索系统相对较慢。为此，本文提出SHRAG框架，旨在实现信息检索与RAG的无缝集成，同时保障精确的检索性能。SHRAG利用大语言模型作为查询策略师，自动将非结构化的自然语言查询转化为逻辑结构化的搜索查询，随后执行布尔检索以模拟专家人工检索者的搜索过程。此外，该框架还结合了多语言查询扩展及多语言嵌入模型，使其能够在ScienceON挑战赛的多语言数据集环境中高效完成跨语言问答任务。实验结果表明，该方法融合了逻辑检索能力与生成式推理，能显著提升RAG系统的准确性与可靠性。此外，SHRAG超越了传统以文档为中心的检索方法，展现出一种可提供直接且可靠回答的新检索范式的潜力。",
        "translated_title": "SHRAG：一种结合人类启发式搜索与RAG的框架",
        "label": [],
        "label_reason": "聚焦信息检索与RAG，非推荐系统核心问题",
        "relevance_score": 3,
        "novelty_score": 6,
        "novelty_reason": "改进检索策略，但未针对推荐场景设计"
    },
    {
        "title": "Text Mining Analysis of Symptom Patterns in Medical Chatbot Conversations",
        "url": "http://arxiv.org/abs/2512.00768v1",
        "pub_date": "2025-11-30",
        "summary": "The fast growth of digital health systems has led to a need to better comprehend how they interpret and represent patient-reported symptoms. Chatbots have been used in healthcare to provide clinical support and enhance the user experience, making it possible to provide meaningful clinical patterns from text-based data through chatbots. The proposed research utilises several different natural language processing methods to study the occurrences of symptom descriptions in medicine as well as analyse the patterns that emerge through these conversations within medical bots. Through the use of the Medical Conversations to Disease Dataset which contains 960 multi-turn dialogues divided into 24 Clinical Conditions, a standardised representation of conversations between patient and bot is created for further analysis by computational means. The multi-method approach uses a variety of tools, including Latent Dirichlet Allocation (LDA) to identify latent symptom themes, K-Means to group symptom descriptions by similarity, Transformer-based Named Entity Recognition (NER) to extract medical concepts, and the Apriori algorithm to discover frequent symptom pairs. Findings from the analysis indicate a coherent structure of clinically relevant topics, moderate levels of clustering cohesiveness and several high confidence rates on the relationships between symptoms like fever headache and rash itchiness. The results support the notion that conversational medical data can be a valuable diagnostic signal for early symptom interpretation, assist in strengthening decision support and improve how users interact with tele-health technology. By demonstrating a method for converting unstructured free-flowing dialogue into actionable knowledge regarding symptoms this work provides an extensible framework to further enhance future performance, dependability and clinical utility of selecting medical chatbots.",
        "translated": "数字健康系统的快速发展促使人们亟需更深入地理解其如何解释和表征患者报告的症状。聊天机器人已在医疗领域被用于提供临床支持并提升用户体验，从而通过基于文本的对话数据为医疗聊天机器人提取有意义的临床模式。本研究运用多种自然语言处理方法，探究医学文献中症状描述的出现情况，并分析医疗机器人对话中所呈现的模式。研究基于“Medical Conversations to Disease Dataset”数据集（包含24种临床病症划分的960个多轮对话），构建标准化的患者与机器人对话表示形式，以供计算手段进一步分析。该多方法路径结合了多种工具：包括使用潜在狄利克雷分配（LDA）识别潜在症状主题、K-Means聚类相似症状描述、基于Transformer的命名实体识别（NER）抽取医学概念，以及Apriori算法挖掘高频症状对。分析结果表明，临床相关主题结构清晰，聚类内凝聚性适中，并在如发热头痛与皮疹瘙痒等症状关系上展现出较高的置信度。这些结果支持了以下观点：对话式医疗数据可作为早期症状解读的有价值诊断信号，有助于强化决策支持并改善用户与远程健康技术的交互体验。通过展示将非结构化自由对话转化为关于症状的可操作知识的方法，本研究提供了可扩展的框架，有望进一步提升未来医疗聊天机器人选择的性能、可靠性及临床实用性。",
        "translated_title": "医疗聊天机器人对话中的症状模式文本挖掘分析",
        "label": [],
        "label_reason": "论文聚焦医疗对话文本挖掘，与推荐系统无直接关联。",
        "relevance_score": 3,
        "novelty_score": 4,
        "novelty_reason": "常规NLP方法用于症状模式分析，无推荐系统创新。"
    },
    {
        "title": "Upcycled and Merged MoE Reward Model for Mitigating Reward Hacking",
        "url": "http://arxiv.org/abs/2512.00724v1",
        "pub_date": "2025-11-30",
        "summary": "Reward models play a critical role in Reinforcement Learning from Human Feedback (RLHF) by assessing the consistency between generated outputs and human preferences. However, conventional reward models are prone to reward hacking or over-optimization, where the policy exploits shortcut patterns to obtain high reward scores that do not reflect true human preference. Although Mixture-of-Experts (MoE)-based reward models can enhance discriminative capability, they typically introduce substantial computational overhead. To address these challenges, we propose an upcycle and merge MoE reward modeling approach. We first upcycle a dense reward model into a MoE architecture, where a shared expert captures general knowledge, while normal experts specialize in instruction-specific patterns. We then apply routing-weight normalization and merge experts back into a dense model through a learnable weight-averaging mechanism, preserving performance gains while significantly reducing inference cost. Experimental results demonstrate that our method effectively mitigates reward hacking across various model scales. Our work highlights the potential of upcycle and merge MoE structures for improving both robustness and efficiency of RLHF reward models.",
        "translated": "奖励模型在基于人类反馈的强化学习（RLHF）中扮演着关键角色，用于评估生成输出与人类偏好之间的一致性。然而，传统奖励模型容易出现奖励黑客行为或过度优化问题，即策略通过利用捷径模式获取高奖励分数，而这些分数并不能反映真实的人类偏好。尽管基于专家混合（MoE）的奖励模型能够增强判别能力，但通常会引入显著的计算开销。为应对上述挑战，我们提出一种“上采样并融合”MoE奖励建模方法：首先将稠密奖励模型上采样为MoE架构，其中共享专家负责捕捉通用知识，而普通专家则专注于特定指令模式；随后通过路由权重归一化，并借助可学习的加权平均机制将专家重新融合回稠密模型，在保留性能提升的同时大幅降低推理成本。实验结果表明，该方法能有效缓解不同规模模型下的奖励黑客现象。本工作凸显了“上采样并融合”MoE结构在提升RLHF奖励模型鲁棒性与效率方面的潜力。",
        "translated_title": "循环复用与融合的MoE奖励模型以缓解奖励黑客行为",
        "label": [],
        "label_reason": "论文聚焦RLHF奖励模型优化，非推荐系统范畴",
        "relevance_score": 3,
        "novelty_score": 7,
        "novelty_reason": "MoE结构创新用于奖励建模，提升效率与鲁棒性"
    },
    {
        "title": "Cross-Domain Federated Semantic Communication with Global Representation Alignment and Domain-Aware Aggregation",
        "url": "http://arxiv.org/abs/2512.00711v1",
        "pub_date": "2025-11-30",
        "summary": "Semantic communication can significantly improve bandwidth utilization in wireless systems by exploiting the meaning behind raw data. However, the advancements achieved through semantic communication are closely dependent on the development of deep learning (DL) models for joint source-channel coding (JSCC) encoder/decoder techniques, which require a large amount of data for training. To address this data-intensive nature of DL models, federated learning (FL) has been proposed to train a model in a distributed manner, where the server broadcasts the DL model to clients in the network for training with their local data. However, the conventional FL approaches suffer from catastrophic degradation when client data are from different domains. In contrast, in this paper, a novel FL framework is proposed to address this domain shift by constructing the global representation, which aligns with the local features of the clients to preserve the semantics of different data domains. In addition, the dominance problem of client domains with a large number of samples is identified and, then, addressed with a domain-aware aggregation approach. This work is the first to consider the domain shift in training the semantic communication system for the image reconstruction task. Finally, simulation results demonstrate that the proposed approach outperforms the model-contrastive FL (MOON) framework by 0.5 for PSNR values under three domains at an SNR of 1 dB, and this gap continues to widen as the channel quality improves.",
        "translated": "语义通信可通过利用原始数据背后的语义信息，显著提升无线系统中的带宽利用率。然而，语义通信所取得的进展高度依赖于深度学习（DL）模型在联合信源信道编码（JSCC）编解码技术中的发展，而这些模型的训练需要大量数据。为应对深度学习模型对数据密集性的要求，联邦学习（FL）被提出，以分布式方式训练模型：服务器将DL模型广播至网络中的客户端，由其使用本地数据进行训练。然而，传统的FL方法在客户端数据来自不同域时会遭受灾难性性能退化。相比之下，本文提出了一种新颖的FL框架，通过构建全局表征来解决该域迁移问题，该表征与各客户端的局部特征对齐，从而保留不同数据域的语义信息。此外，本文识别并解决了样本数量较多的客户端域主导性问题，采用了域感知聚合方法予以应对。本工作首次考虑了在图像重建任务中训练语义通信系统的域迁移问题。仿真结果表明，在信噪比为1 dB、三个不同域条件下，所提方法相较模型对比式FL（MOON）框架的PSNR值高出0.5，且随着信道质量改善，这一差距持续扩大。",
        "translated_title": "跨域联邦语义通信：全局表示对齐与领域感知聚合",
        "label": [],
        "label_reason": "论文聚焦无线通信领域，与推荐系统无关。",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "改进联邦学习框架，但非推荐系统创新。"
    },
    {
        "title": "ProEx: A Unified Framework Leveraging Large Language Model with Profile Extrapolation for Recommendation",
        "url": "http://arxiv.org/abs/2512.00679v1",
        "pub_date": "2025-11-30",
        "summary": "The powerful text understanding and generation capabilities of large language models (LLMs) have brought new vitality to general recommendation with implicit feedback. One possible strategy involves generating a unique user (or item) profile from historical interaction data, which is then mapped to a semantic representation in the language space. However, a single-instance profile may be insufficient to comprehensively capture the complex intentions behind a user's interacted items. Moreover, due to the inherent instability of LLMs, a biased or misinterpreted profile could even undermine the original recommendation performance. Consequently, an intuitive solution is to generate multiple profiles for each user (or item), each reflecting a distinct aspect of their characteristics. In light of this, we propose a unified recommendation framework with multi-faceted profile extrapolation (ProEx) in this paper. By leveraging chain-of-thought reasoning, we construct multiple distinct profiles for each user and item. These new profiles are subsequently mapped into semantic vectors, extrapolating from the position of the original profile to explore a broader region of the language space. Subsequently, we introduce the concept of environments, where each environment represents a possible linear combination of all profiles. The differences across environments are minimized to reveal the inherent invariance of user preferences. We apply ProEx to three discriminative methods and three generative methods, and conduct extensive experiments on three datasets. The experimental results demonstrate that ProEx significantly enhances the performance of these base recommendation models.",
        "translated": "大语言模型（LLM）强大的文本理解与生成能力为基于隐式反馈的通用推荐系统注入了新的活力。一种可行策略是根据用户的历史交互数据生成独特的用户（或物品）画像，并将其映射到语言空间中的语义表示。然而，单一画像可能不足以全面捕捉用户所交互物品背后的复杂意图；此外，由于大语言模型固有的不稳定性，偏倚或误释的画像甚至可能损害原始推荐性能。因此，一个直观的解决方案是为每个用户（或物品）生成多个画像，每个画像反映其特征的不同侧面。针对这一思路，本文提出了一种统一的推荐框架——多维画像外推（ProEx）。通过链式思维推理机制，我们为每个用户和物品构建多个不同的画像。随后，这些新画像被映射至语义向量空间，并从原始画像位置出发进行外推，以探索更广阔的语言空间区域。接着，我们引入“环境”概念，其中每个环境代表所有画像的一种可能线性组合。通过最小化不同环境间的差异，揭示用户偏好内在不变性。我们将 ProEx 应用于三种判别式方法和三种生成式方法，并在三个数据集上进行了广泛实验。实验结果表明，ProEx 显著提升了这些基础推荐模型的性能。",
        "translated_title": "ProEx：一种结合大语言模型与画像外推的统一推荐框架",
        "label": [
            "LLM生成式推荐",
            "通用推荐技术"
        ],
        "label_reason": "利用LLM多视角用户画像提升推荐性能",
        "relevance_score": 10,
        "novelty_score": 9,
        "novelty_reason": "创新性多视角profile生成与环境建模"
    },
    {
        "title": "DLRREC: Denoising Latent Representations via Multi-Modal Knowledge Fusion in Deep Recommender Systems",
        "url": "http://arxiv.org/abs/2512.00596v1",
        "pub_date": "2025-11-29",
        "summary": "Modern recommender systems struggle to effectively utilize the rich, yet high-dimensional and noisy, multi-modal features generated by Large Language Models (LLMs). Treating these features as static inputs decouples them from the core recommendation task. We address this limitation with a novel framework built on a key insight: deeply fusing multi-modal and collaborative knowledge for representation denoising. Our unified architecture introduces two primary technical innovations. First, we integrate dimensionality reduction directly into the recommendation model, enabling end-to-end co-training that makes the reduction process aware of the final ranking objective. Second, we introduce a contrastive learning objective that explicitly incorporates the collaborative filtering signal into the latent space. This synergistic process refines raw LLM embeddings, filtering noise while amplifying task-relevant signals. Extensive experiments confirm our method's superior discriminative power, proving that this integrated fusion and denoising strategy is critical for achieving state-of-the-art performance. Our work provides a foundational paradigm for effectively harnessing LLMs in recommender systems.",
        "translated": "现代推荐系统难以有效利用大型语言模型（LLMs）生成的丰富但高维且噪声较多的多模态特征。将这些特征视为静态输入，使其与核心推荐任务脱节。我们通过一个新颖框架解决这一局限，其核心思想是：深度融合多模态与协同知识以实现表征去噪。我们的统一架构引入两项关键技术革新。首先，我们将降维直接集成到推荐模型中，实现端到端联合训练，使降维过程感知最终排序目标。其次，我们引入一种对比学习目标，显式将协同过滤信号融入潜在空间。这一协同过程精炼原始LLM嵌入，滤除噪声并增强与任务相关信号。大量实验验证了该方法优越的判别能力，证明这种集成融合与去噪策略对于实现业界领先性能至关重要。本工作为有效利用LLMs于推荐系统提供了基础范式。",
        "translated_title": "DLRREC：通过多模态知识融合在深度推荐系统中进行潜在表示去噪",
        "label": [
            "精排",
            "LLM生成式推荐",
            "负采样与对比学习"
        ],
        "label_reason": "融合多模态与协同信号优化推荐表示",
        "relevance_score": 9,
        "novelty_score": 8,
        "novelty_reason": "首创对比学习+维度压缩联合优化框架"
    },
    {
        "title": "PEOAT: Personalization-Guided Evolutionary Question Assembly for One-Shot Adaptive Testing",
        "url": "http://arxiv.org/abs/2512.00439v1",
        "pub_date": "2025-11-29",
        "summary": "With the rapid advancement of intelligent education, Computerized Adaptive Testing (CAT) has attracted increasing attention by integrating educational psychology with deep learning technologies. Unlike traditional paper-and-pencil testing, CAT aims to efficiently and accurately assess examinee abilities by adaptively selecting the most suitable items during the assessment process. However, its real-time and sequential nature presents limitations in practical scenarios, particularly in large-scale assessments where interaction costs are high, or in sensitive domains such as psychological evaluations where minimizing noise and interference is essential. These challenges constrain the applicability of conventional CAT methods in time-sensitive or resourceconstrained environments. To this end, we first introduce a novel task called one-shot adaptive testing (OAT), which aims to select a fixed set of optimal items for each test-taker in a one-time selection. Meanwhile, we propose PEOAT, a Personalization-guided Evolutionary question assembly framework for One-shot Adaptive Testing from the perspective of combinatorial optimization. Specifically, we began by designing a personalization-aware initialization strategy that integrates differences between examinee ability and exercise difficulty, using multi-strategy sampling to construct a diverse and informative initial population. Building on this, we proposed a cognitive-enhanced evolutionary framework incorporating schema-preserving crossover and cognitively guided mutation to enable efficient exploration through informative signals. To maintain diversity without compromising fitness, we further introduced a diversity-aware environmental selection mechanism. The effectiveness of PEOAT is validated through extensive experiments on two datasets, complemented by case studies that uncovered valuable insights.",
        "translated": "随着智能教育的快速发展，计算机自适应测试（CAT）通过融合教育心理学与深度学习技术，日益受到关注。与传统的纸笔测试不同，CAT旨在通过在测评过程中动态选择最合适的题目，高效且精准地评估考生能力。然而，其实时性和序列性特点在实际应用场景中存在局限，特别是在大规模测评场景下交互成本高昂，或在心理评估等敏感领域需最大限度减少噪声与干扰的情况下尤为突出。这些挑战限制了传统CAT方法在时间敏感或资源受限环境中的适用性。为此，我们首先提出了一项新型任务——单次自适应测试（OAT），旨在为每位考生一次性选定一组最优题目。同时，我们提出了PEOAT框架，即面向单次自适应测试的个性化引导进化题库组装框架，从组合优化视角出发构建该框架。具体而言，我们首先设计了一种个性化感知的初始化策略，结合考生能力与题目难度差异，采用多策略采样方法构建多样且信息丰富的初始种群；在此基础上，我们进一步提出一种认知增强型进化框架，集成保持结构的交叉操作与认知引导变异机制，以有效利用信息信号实现高效探索；为在不牺牲适应度的前提下维持种群多样性，我们还引入了一种多样性感知的环境选择机制。PEOAT的有效性已在两个数据集上通过大量实验验证，并辅以案例研究揭示了有价值的洞见。",
        "translated_title": "PEOAT：面向单次自适应测试的个性化引导演化问题组合",
        "label": [],
        "label_reason": "论文聚焦自适应测试，非推荐系统相关",
        "relevance_score": 3,
        "novelty_score": 5,
        "novelty_reason": "改进进化算法，但无推荐系统创新"
    },
    {
        "title": "Mitigating the Threshold Priming Effect in Large Language Model-Based Relevance Judgments via Personality Infusing",
        "url": "http://arxiv.org/abs/2512.00390v1",
        "pub_date": "2025-11-29",
        "summary": "Recent research has explored LLMs as scalable tools for relevance labeling, but studies indicate they are susceptible to priming effects, where prior relevance judgments influence later ones. Although psychological theories link personality traits to such biases, it is unclear whether simulated personalities in LLMs exhibit similar effects. We investigate how Big Five personality profiles in LLMs influence priming in relevance labeling, using multiple LLMs on TREC 2021 and 2022 Deep Learning Track datasets. Our results show that certain profiles, such as High Openness and Low Neuroticism, consistently reduce priming susceptibility. Additionally, the most effective personality in mitigating priming may vary across models and task types. Based on these findings, we propose personality prompting as a method to mitigate threshold priming, connecting psychological evidence with LLM-based evaluation practices.",
        "translated": "近期研究已探索将大语言模型（LLM）作为可扩展的关联性标注工具，但研究表明其易受启动效应影响，即先前的相关性判断会干扰后续判断。虽然心理学理论将人格特质与此类偏见相联系，但尚不明确大语言模型中模拟的人格是否表现出类似效应。我们通过在TREC 2021和2022深度学习赛道数据集上使用多个大语言模型，探究模型内“大五人格”特征如何影响相关性标注中的启动效应。实验结果表明，某些人格特征（如高开放性和低神经质）能持续降低启动效应的敏感度。此外，最有效的缓解启动效应的人格可能因模型及任务类型而异。基于上述发现，我们提出“人格提示法”以缓解阈值启动效应，并将心理学实证与基于大语言模型的评估实践相结合。",
        "translated_title": "通过人格注入缓解大语言模型相关性判断中的阈值启动效应",
        "label": [
            "LLM生成式推荐",
            "推荐系统评估"
        ],
        "label_reason": "研究LLM在相关性标注中的偏倚，属评估环节改进",
        "relevance_score": 4,
        "novelty_score": 6,
        "novelty_reason": "提出人格提示法缓解偏倚，属方法优化"
    },
    {
        "title": "The Information Theory of Similarity",
        "url": "http://arxiv.org/abs/2512.00378v1",
        "pub_date": "2025-11-29",
        "summary": "We establish a precise mathematical equivalence between witness-based similarity systems (REWA) and Shannon's information theory. We prove that witness overlap is mutual information, that REWA bit complexity bounds arise from channel capacity limitations, and that ranking-preserving encodings obey rate-distortion constraints. This unification reveals that fifty years of similarity search research -- from Bloom filters to locality-sensitive hashing to neural retrieval -- implicitly developed information theory for relational data. We derive fundamental lower bounds showing that REWA's $O(Δ^{-2} \\log N)$ complexity is optimal: no encoding scheme can preserve similarity rankings with fewer bits. The framework establishes that semantic similarity has physical units (bits of mutual information), search is communication (query transmission over a noisy channel), and retrieval systems face fundamental capacity limits analogous to Shannon's channel coding theorem.",
        "translated": "我们建立了基于见证的相似性系统（REWA）与香农信息论之间的精确数学等价关系。我们证明：见证重叠即互信息；REWA 的比特复杂度界限源于信道容量限制；保持排序的编码需满足率失真约束。这一统一揭示，过去五十年间从布隆过滤器到局部敏感哈希再到神经检索的相似性搜索研究，实际上隐式发展了面向关系数据的信息理论。我们推导出基本下界，表明 REWA 的 $O(Δ^{-2} \\log N)$ 复杂度是最优的：不存在任何编码方案能够在更少的比特内保持相似性排序。该框架确立了语义相似性具有物理单位（互信息比特）、搜索是通信（在噪声信道上传输查询），且检索系统面临类似于香农信道编码定理的基本容量限制。",
        "translated_title": "相似性的信息论",
        "label": [],
        "label_reason": "论文聚焦信息论与相似性搜索，非推荐系统核心环节。",
        "relevance_score": 3,
        "novelty_score": 8,
        "novelty_reason": "构建信息论框架解释相似性搜索，理论创新显著。"
    },
    {
        "title": "Breaking It Down: Domain-Aware Semantic Segmentation for Retrieval Augmented Generation",
        "url": "http://arxiv.org/abs/2512.00367v1",
        "pub_date": "2025-11-29",
        "summary": "Document chunking is a crucial component of Retrieval-Augmented Generation (RAG), as it directly affects the retrieval of relevant and precise context. Conventional fixed-length and recursive splitters often produce arbitrary, incoherent segments that fail to preserve semantic structure. Although semantic chunking has gained traction, its influence on generation quality remains underexplored. This paper introduces two efficient semantic chunking methods, Projected Similarity Chunking (PSC) and Metric Fusion Chunking (MFC), trained on PubMed data using three different embedding models. We further present an evaluation framework that measures the effect of chunking on both retrieval and generation by augmenting PubMedQA with full-text PubMed Central articles. Our results show substantial retrieval improvements (24x with PSC) in MRR and higher Hits@k on PubMedQA. We provide a comprehensive analysis, including statistical significance and response-time comparisons with common chunking libraries. Despite being trained on a single domain, PSC and MFC also generalize well, achieving strong out-of-domain generation performance across multiple datasets. Overall, our findings confirm that our semantic chunkers, especially PSC, consistently deliver superior performance.",
        "translated": "文档分块是检索增强生成（RAG）的关键组件，因其直接影响相关且精确上下文的召回效果。传统的固定长度与递归式分块器往往生成任意性高、语义不连贯的片段，难以保留语义结构。尽管语义分块已逐渐受到关注，其对生成质量的影响仍鲜有深入研究。本文提出两种高效的语义分块方法：投影相似度分块（Projected Similarity Chunking, PSC）与度量融合分块（Metric Fusion Chunking, MFC），并基于 PubMed 数据集结合三种不同嵌入模型进行训练。我们进一步构建评估框架，通过将 PubMedQA 与全文 PubMed Central 文章结合，衡量分块对召回与生成性能的影响。实验结果表明，在 PubMedQA 上，PSC 实现了高达 24 倍的 MRR 提升及更高的 Hits@k 指标。我们提供了全面分析，包括统计显著性检验以及与主流分块库在响应时间上的对比。尽管两类方法均在单一领域数据上训练，但 PSC 与 MFC 在多个数据集上展现出良好的泛化能力，实现了强跨域生成表现。总体而言，我们的研究证实，所提出的语义分块器——尤其是 PSC——持续展现出卓越性能。",
        "translated_title": "拆解分析：面向检索增强生成的领域感知语义分割",
        "label": [],
        "label_reason": "聚焦RAG中的分块，非推荐系统核心环节",
        "relevance_score": 3,
        "novelty_score": 6,
        "novelty_reason": "改进分块方法，但无推荐系统创新"
    },
    {
        "title": "Evidence-Guided Schema Normalization for Temporal Tabular Reasoning",
        "url": "http://arxiv.org/abs/2512.00329v1",
        "pub_date": "2025-11-29",
        "summary": "Temporal reasoning over evolving semi-structured tables poses a challenge to current QA systems. We propose a SQL-based approach that involves (1) generating a 3NF schema from Wikipedia infoboxes, (2) generating SQL queries, and (3) query execution. Our central finding challenges model scaling assumptions: the quality of schema design has a greater impact on QA precision than model capacity. We establish three evidence-based principles: normalization that preserves context, semantic naming that reduces ambiguity, and consistent temporal anchoring. Our best configuration (Gemini 2.5 Flash schema + Gemini-2.0-Flash queries) achieves 80.39 EM, a 16.8\\% improvement over the baseline (68.89 EM).",
        "translated": "对不断演变的半结构化表格进行时序推理，给当前问答系统带来了挑战。我们提出一种基于SQL的方法，包括（1）从维基百科信息框生成3NF模式，（2）生成SQL查询，以及（3）执行查询。我们的核心发现质疑了模型扩展性的假设：模式设计的质量对问答精度的影响大于模型容量。我们确立了三个基于实证的原则：保留上下文的规范化、减少歧义的语义命名，以及一致的时序锚定。我们的最佳配置（Gemini 2.5 Flash schema + Gemini-2.0-Flash queries）取得80.39 EM，相较基线（68.89 EM）提升16.8%。",
        "translated_title": "基于证据引导的模式规范化用于时序表格推理",
        "label": [],
        "label_reason": "论文聚焦表格推理与SQL生成，非推荐系统相关",
        "relevance_score": 2,
        "novelty_score": 5,
        "novelty_reason": "改进schema设计，但无推荐系统创新"
    },
    {
        "title": "Evolving Paradigms in Task-Based Search and Learning: A Comparative Analysis of Traditional Search Engine with LLM-Enhanced Conversational Search System",
        "url": "http://arxiv.org/abs/2512.00313v1",
        "pub_date": "2025-11-29",
        "summary": "Large Language Models (LLMs) are rapidly reshaping information retrieval by enabling interactive, generative, and inference-driven search. While traditional keyword-based search remains central to web and academic information access, it often struggles to support multi-step reasoning and exploratory learning tasks. LLM-powered search interfaces, such as ChatGPT and Claude, introduce new capabilities that may influence how users formulate queries, navigate information, and construct knowledge. However, empirical understanding of these effects is still limited. This study compares search behavior and learning outcomes in two environments: a standard search engine and an LLM-powered search system. We investigate (1) how search strategies, query formulation, and evaluation behaviors differ across systems, and (2) how LLM use affects comprehension, knowledge integration, and critical thinking during search-based learning tasks. Findings offer insight into how generative AI shapes information-seeking processes and contribute to ongoing discussions in information retrieval, human-AI interaction, and technology-supported learning.",
        "translated": "大语言模型（LLMs）正迅速重塑信息检索，通过支持交互式、生成式和推理驱动的搜索方式。尽管传统的基于关键词的搜索在网页与学术信息获取中仍居核心地位，但其往往难以有效支持多步骤推理与探索性学习任务。由大语言模型驱动的搜索界面（如ChatGPT与Claude）引入了新的能力，可能影响用户构建查询、导航信息及建构知识的方式。然而，目前对其实际影响的实证理解仍显不足。本研究比较了两种环境下的搜索行为与学习成果：标准搜索引擎与大语言模型驱动的搜索系统。我们探究了两点：（1）不同系统间搜索策略、查询构建与评估行为的差异；（2）大语言模型使用如何影响用户在基于搜索的学习任务中的理解力、知识整合能力与批判性思维。研究结果揭示了生成式人工智能如何塑造信息寻求过程，并为信息检索、人机交互及技术支持学习等领域的持续讨论提供贡献。",
        "translated_title": "任务驱动搜索与学习中的演进范式：传统搜索引擎与大语言模型增强的对话式搜索系统的比较分析",
        "label": [],
        "label_reason": "研究聚焦信息检索与人机交互，非推荐系统核心问题",
        "relevance_score": 3,
        "novelty_score": 4,
        "novelty_reason": "对比LLM搜索系统，无推荐算法创新"
    },
    {
        "title": "EfficientFlow: Efficient Equivariant Flow Policy Learning for Embodied AI",
        "url": "http://arxiv.org/abs/2512.02020v1",
        "pub_date": "2025-12-01",
        "summary": "Generative modeling has recently shown remarkable promise for visuomotor policy learning, enabling flexible and expressive control across diverse embodied AI tasks. However, existing generative policies often struggle with data inefficiency, requiring large-scale demonstrations, and sampling inefficiency, incurring slow action generation during inference. We introduce EfficientFlow, a unified framework for efficient embodied AI with flow-based policy learning. To enhance data efficiency, we bring equivariance into flow matching. We theoretically prove that when using an isotropic Gaussian prior and an equivariant velocity prediction network, the resulting action distribution remains equivariant, leading to improved generalization and substantially reduced data demands. To accelerate sampling, we propose a novel acceleration regularization strategy. As direct computation of acceleration is intractable for marginal flow trajectories, we derive a novel surrogate loss that enables stable and scalable training using only conditional trajectories. Across a wide range of robotic manipulation benchmarks, the proposed algorithm achieves competitive or superior performance under limited data while offering dramatically faster inference. These results highlight EfficientFlow as a powerful and efficient paradigm for high-performance embodied AI.",
        "translated": "生成式建模近期在视觉-运动策略学习中展现出显著潜力，能够为多样化的具身人工智能任务提供灵活且表达力强的控制能力。然而，现有生成式策略往往面临数据效率低下（需大量演示样本）和采样效率低下（推理过程中动作生成缓慢）的问题。本文提出 EfficientFlow，一种基于流式策略学习的统一框架，旨在提升具身人工智能的效率。为增强数据效率，我们引入等变性于流匹配过程；理论证明：当采用各向同性高斯先验及等变速度预测网络时，所得动作分布保持等变性，从而实现更优泛化能力并大幅降低对训练数据的需求。为加速采样，我们提出一种新颖的加速度正则化策略：由于边际流轨迹的直接加速度计算不可行，我们推导出一种新型代理损失函数，在仅使用条件轨迹的情况下即可实现稳定、可扩展的训练。在多个机器人操作基准测试中，该算法在数据受限条件下仍能取得竞争力甚至优越的性能，并显著提升推理速度。上述结果表明，EfficientFlow 是一种强大而高效的高表现力具身人工智能范式。",
        "translated_title": "EfficientFlow：面向具身人工智能的高效等变流策略学习",
        "label": [],
        "label_reason": "属于高阶视觉任务，非图像像素级处理",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "提出新流基策略学习框架，提升效率与泛化"
    },
    {
        "title": "Data-Centric Visual Development for Self-Driving Labs",
        "url": "http://arxiv.org/abs/2512.02018v1",
        "pub_date": "2025-12-01",
        "summary": "Self-driving laboratories offer a promising path toward reducing the labor-intensive, time-consuming, and often irreproducible workflows in the biological sciences. Yet their stringent precision requirements demand highly robust models whose training relies on large amounts of annotated data. However, this kind of data is difficult to obtain in routine practice, especially negative samples. In this work, we focus on pipetting, the most critical and precision sensitive action in SDLs. To overcome the scarcity of training data, we build a hybrid pipeline that fuses real and virtual data generation. The real track adopts a human-in-the-loop scheme that couples automated acquisition with selective human verification to maximize accuracy with minimal effort. The virtual track augments the real data using reference-conditioned, prompt-guided image generation, which is further screened and validated for reliability. Together, these two tracks yield a class-balanced dataset that enables robust bubble detection training. On a held-out real test set, a model trained entirely on automatically acquired real images reaches 99.6% accuracy, and mixing real and generated data during training sustains 99.4% accuracy while reducing collection and review load. Our approach offers a scalable and cost-effective strategy for supplying visual feedback data to SDL workflows and provides a practical solution to data scarcity in rare event detection and broader vision tasks.",
        "translated": "自动驾驶实验室为减少生物科学领域中劳动密集型、耗时且往往不可复现的工作流程提供了一条有前景的路径。然而，其严苛的精度要求需要高度鲁棒的模型，而这类模型的训练依赖于大量标注数据。但此类数据在常规实践中难以获取，尤其是负样本。在本研究中，我们聚焦于移液——这是自动驾驶实验室中最关键且对精度最敏感的操作。为克服训练数据稀缺的问题，我们构建了一个融合真实数据与虚拟数据生成的混合管道。真实数据路径采用“人机协同”方案，将自动化采集与选择性人工验证相结合，在最小化人力投入的前提下最大化准确性；虚拟数据路径则通过参考条件引导的提示式图像生成技术对真实数据进行扩充，并进一步筛选和验证以确保可靠性。这两条路径共同构建了一个类别平衡的数据集，从而支持鲁棒的气泡检测训练。在未参与训练的真实测试集上，仅使用全自动采集的真实图像训练的模型准确率达到99.6%；而在训练过程中混合使用真实数据与生成数据时，模型仍保持99.4%的准确率，同时显著降低了数据收集与审核的负担。我们的方法为向自动驾驶实验室工作流提供视觉反馈数据提供了可扩展且成本效益高的策略，并为稀有事件检测及更广泛视觉任务中的数据稀缺问题提供了切实可行的解决方案。",
        "translated_title": "以数据为中心的自动驾驶实验室视觉开发",
        "label": [],
        "label_reason": "任务为数据生成与模型训练，非图像像素级恢复",
        "relevance_score": 2,
        "novelty_score": 4,
        "novelty_reason": "数据增强方案常规，无新范式或架构"
    },
    {
        "title": "Visual Sync: Multi-Camera Synchronization via Cross-View Object Motion",
        "url": "http://arxiv.org/abs/2512.02017v1",
        "pub_date": "2025-12-01",
        "summary": "Today, people can easily record memorable moments, ranging from concerts, sports events, lectures, family gatherings, and birthday parties with multiple consumer cameras. However, synchronizing these cross-camera streams remains challenging. Existing methods assume controlled settings, specific targets, manual correction, or costly hardware. We present VisualSync, an optimization framework based on multi-view dynamics that aligns unposed, unsynchronized videos at millisecond accuracy. Our key insight is that any moving 3D point, when co-visible in two cameras, obeys epipolar constraints once properly synchronized. To exploit this, VisualSync leverages off-the-shelf 3D reconstruction, feature matching, and dense tracking to extract tracklets, relative poses, and cross-view correspondences. It then jointly minimizes the epipolar error to estimate each camera's time offset. Experiments on four diverse, challenging datasets show that VisualSync outperforms baseline methods, achieving an median synchronization error below 50 ms.",
        "translated": "如今，人们可以轻松地使用多种消费级摄像机记录难忘的瞬间，例如音乐会、体育赛事、讲座、家庭聚会和生日派对。然而，将这些跨摄像机视频流进行同步仍具挑战性。现有方法通常假设受控环境、特定目标、人工校正或昂贵硬件。我们提出 VisualSync，这是一个基于多视角动态的优化框架，能够在毫秒级精度下对未摆拍且不同步的视频进行对齐。我们的核心洞察是：当两个摄像机同时观测到某个移动的 3D 点时，一旦时间对齐得当，该点便满足极线约束。为利用这一点，VisualSync 利用现成的三维重建、特征匹配与稠密跟踪技术，提取轨迹片段、相对位姿及跨视角对应关系；随后，通过联合最小化极线误差来估计各摄像机的时间偏移量。在四个多样且具有挑战性的数据集上的实验表明，VisualSync 超越了基线方法，实现了中位同步误差低于 50 毫秒。",
        "translated_title": "视觉同步：通过跨视角目标运动实现多相机同步",
        "label": [],
        "label_reason": "视频同步属高阶任务，非像素级图像处理",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "首次用多视角运动实现毫秒级视频同步"
    },
    {
        "title": "Objects in Generated Videos Are Slower Than They Appear: Models Suffer Sub-Earth Gravity and Don't Know Galileo's Principle...for now",
        "url": "http://arxiv.org/abs/2512.02016v1",
        "pub_date": "2025-12-01",
        "summary": "Video generators are increasingly evaluated as potential world models, which requires them to encode and understand physical laws. We investigate their representation of a fundamental law: gravity. Out-of-the-box video generators consistently generate objects falling at an effectively slower acceleration. However, these physical tests are often confounded by ambiguous metric scale. We first investigate if observed physical errors are artifacts of these ambiguities (e.g., incorrect frame rate assumptions). We find that even temporal rescaling cannot correct the high-variance gravity artifacts. To rigorously isolate the underlying physical representation from these confounds, we introduce a unit-free, two-object protocol that tests the timing ratio $t_1^2/t_2^2 = h_1/h_2$, a relationship independent of $g$, focal length, and scale. This relative test reveals violations of Galileo's equivalence principle. We then demonstrate that this physical gap can be partially mitigated with targeted specialization. A lightweight low-rank adaptor fine-tuned on only 100 single-ball clips raises $g_{\\mathrm{eff}}$ from $1.81\\,\\mathrm{m/s^2}$ to $6.43\\,\\mathrm{m/s^2}$ (reaching $65\\%$ of terrestrial gravity). This specialist adaptor also generalizes zero-shot to two-ball drops and inclined planes, offering initial evidence that specific physical laws can be corrected with minimal data.",
        "translated": "视频生成器正日益被评估为潜在的世界模型，这要求它们能够编码并理解物理定律。我们研究了其对一条基本物理定律——重力——的表征能力。未经调整的视频生成器始终倾向于生成物体以有效更慢的加速度下落。然而，这些物理测试往往受制于模糊的度量尺度问题。我们首先探究所观察到的物理误差是否源于此类模糊性（例如，错误假设帧率）。我们发现，即使进行时间重缩放，也无法修正高方差的重力伪影。为严格分离底层物理表征与这些干扰因素，我们引入了一种无量纲、双物体协议，通过检验时间比 $t_1^2/t_2^2 = h_1/h_2$ 来验证该关系独立于重力加速度 $g$、焦距及尺度。这一相对测试揭示了违反伽利略等效原理的现象。随后我们证明，这种物理缺陷可通过定向专业化部分缓解。仅在100个单球体下落片段上微调的一个轻量级低秩适配器，可将有效重力加速度 $g_{\\mathrm{eff}}$ 从 $1.81\\,\\mathrm{m/s^2}$ 提升至 $6.43\\,\\mathrm{m/s^2}$（达到地表重力的65%）。该专用适配器还实现了零样本泛化至双球体下落和斜面场景，初步表明特定物理定律可通过极少量数据得到校正。",
        "translated_title": "生成视频中的物体运动速度低于其视觉呈现：模型受小于地心引力的影响，并尚未理解伽利略原理……目前如此",
        "label": [],
        "label_reason": "研究视频生成器物理建模，非图像像素级处理",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出新测试协议并微调适配器提升物理准确性"
    },
    {
        "title": "Generative Video Motion Editing with 3D Point Tracks",
        "url": "http://arxiv.org/abs/2512.02015v1",
        "pub_date": "2025-12-01",
        "summary": "Camera and object motions are central to a video's narrative. However, precisely editing these captured motions remains a significant challenge, especially under complex object movements. Current motion-controlled image-to-video (I2V) approaches often lack full-scene context for consistent video editing, while video-to-video (V2V) methods provide viewpoint changes or basic object translation, but offer limited control over fine-grained object motion. We present a track-conditioned V2V framework that enables joint editing of camera and object motion. We achieve this by conditioning a video generation model on a source video and paired 3D point tracks representing source and target motions. These 3D tracks establish sparse correspondences that transfer rich context from the source video to new motions while preserving spatiotemporal coherence. Crucially, compared to 2D tracks, 3D tracks provide explicit depth cues, allowing the model to resolve depth order and handle occlusions for precise motion editing. Trained in two stages on synthetic and real data, our model supports diverse motion edits, including joint camera/object manipulation, motion transfer, and non-rigid deformation, unlocking new creative potential in video editing.",
        "translated": "相机与物体运动是视频叙事的核心。然而，对这些捕捉到的运动进行精确编辑仍是一个重大挑战，尤其是在复杂物体运动的情况下。当前的基于运动控制的图像到视频（I2V）方法通常缺乏完整的场景上下文以实现一致的视频编辑；而视频到视频（V2V）方法虽然能提供视角变化或基本物体平移，但对细粒度物体运动的控制能力有限。我们提出了一种基于轨迹条件的 V2V 框架，能够联合编辑相机与物体运动。我们通过将视频生成模型的条件输入设定为源视频及表示源目标运动配对的 3D 点轨迹来实现该目标。这些 3D 轨迹建立了稀疏对应关系，在将源视频中的丰富上下文迁移至新运动的同时保持时空一致性。关键的是，相较于 2D 轨迹，3D 轨迹提供了显式的深度线索，使模型能够解析深度顺序并处理遮挡，从而实现精准的运动编辑。该模型在合成数据与真实数据上分两阶段训练，支持多种运动编辑操作，包括相机与物体的联合操控、运动迁移以及非刚性形变，为视频编辑开辟了新的创作可能性。",
        "translated_title": "基于3D点轨迹的生成式视频运动编辑",
        "label": [],
        "label_reason": "视频编辑属高阶任务，非像素级图像恢复",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "提出3D点轨迹引导的V2V框架，具创新性"
    },
    {
        "title": "TUNA: Taming Unified Visual Representations for Native Unified Multimodal Models",
        "url": "http://arxiv.org/abs/2512.02014v1",
        "pub_date": "2025-12-01",
        "summary": "Unified multimodal models (UMMs) aim to jointly perform multimodal understanding and generation within a single framework. We present TUNA, a native UMM that builds a unified continuous visual representation by cascading a VAE encoder with a representation encoder. This unified representation space allows end-to-end processing of images and videos for both understanding and generation tasks. Compared to prior UMMs with decoupled representations, TUNA's unified visual space avoids representation format mismatches introduced by separate encoders, outperforming decoupled alternatives in both understanding and generation. Moreover, we observe that stronger pretrained representation encoders consistently yield better performance across all multimodal tasks, highlighting the importance of the representation encoder. Finally, in this unified setting, jointly training on both understanding and generation data allows the two tasks to benefit from each other rather than interfere. Our extensive experiments on multimodal understanding and generation benchmarks show that TUNA achieves state-of-the-art results in image and video understanding, image and video generation, and image editing, demonstrating the effectiveness and scalability of its unified representation design.",
        "translated": "统一多模态模型（UMMs）旨在在一个统一框架内联合执行多模态理解和生成任务。我们提出了TUNA，一种原生的UMM，通过将变分自编码器（VAE）编码器与表示编码器级联，构建统一的连续视觉表征空间。该统一表征空间支持图像与视频在理解与生成任务中的端到端处理。相较于先前采用分离式表征的UMM，TUNA的统一视觉空间避免了因独立编码器所引入的表征格式不匹配问题，在理解与生成两个方面均优于分离式方案。此外，我们观察到更强的预训练表示编码器在所有多模态任务中均能持续带来更优性能，凸显了表示编码器的重要性。最后，在该统一架构下，同时在理解与生成数据上进行联合训练，使两项任务能够相互促进而非彼此干扰。我们在多个多模态理解与生成基准上的广泛实验表明，TUNA在图像与视频理解、图像与视频生成及图像编辑任务中均取得了最先进结果，验证了其统一表征设计的有效性与可扩展性。",
        "translated_title": "TUNA：驯化统一视觉表征以支持原生统一多模态模型",
        "label": [],
        "label_reason": "属于多模态生成与理解的高阶任务，非像素级图像恢复。",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出统一视觉表征架构，提升多模态性能但非低层图像处理创新。"
    },
    {
        "title": "Improved Mean Flows: On the Challenges of Fastforward Generative Models",
        "url": "http://arxiv.org/abs/2512.02012v1",
        "pub_date": "2025-12-01",
        "summary": "MeanFlow (MF) has recently been established as a framework for one-step generative modeling. However, its ``fastforward'' nature introduces key challenges in both the training objective and the guidance mechanism. First, the original MF's training target depends not only on the underlying ground-truth fields but also on the network itself. To address this issue, we recast the objective as a loss on the instantaneous velocity $v$, re-parameterized by a network that predicts the average velocity $u$. Our reformulation yields a more standard regression problem and improves the training stability. Second, the original MF fixes the classifier-free guidance scale during training, which sacrifices flexibility. We tackle this issue by formulating guidance as explicit conditioning variables, thereby retaining flexibility at test time. The diverse conditions are processed through in-context conditioning, which reduces model size and benefits performance. Overall, our $\\textbf{improved MeanFlow}$ ($\\textbf{iMF}$) method, trained entirely from scratch, achieves $\\textbf{1.72}$ FID with a single function evaluation (1-NFE) on ImageNet 256$\\times$256. iMF substantially outperforms prior methods of this kind and closes the gap with multi-step methods while using no distillation. We hope our work will further advance fastforward generative modeling as a stand-alone paradigm.",
        "translated": "MeanFlow (MF) 近期已被确立为一种一步生成建模的框架。然而，其“快进”特性在训练目标与引导机制上引入了关键挑战。首先，原始 MF 的训练目标不仅依赖于底层真实场，还依赖于网络本身。为解决此问题，我们将目标重新表述为对瞬时速度 $v$ 的损失，该速度由预测平均速度 $u$ 的网络进行重参数化。这一重构形成一个更标准的回归问题，提升了训练稳定性。其次，原始 MF 在训练期间固定分类器自由引导比例，牺牲了灵活性。我们通过将引导形式化为显式的条件变量来应对这一问题，从而在测试阶段保持灵活性。多样化的条件通过上下文条件处理，既减小了模型规模，又提升了性能。总体而言，我们提出的改进版 MeanFlow（$\\textbf{iMF}$）方法完全从头训练，在 ImageNet 256$\\times$256 数据集上单次函数评估（1-NFE）下达到 $\\textbf{1.72}$ FID。iMF 显著超越同类先前方法，并在无需蒸馏的前提下缩小了与多步方法之间的性能差距。我们希望本工作能进一步推动“快进式”生成建模作为独立范式的进步。",
        "translated_title": "改进的均值流：关于快速前向生成模型的挑战",
        "label": [],
        "label_reason": "生成模型属高阶视觉任务，非像素级图像恢复",
        "relevance_score": 1,
        "novelty_score": 9,
        "novelty_reason": "提出改进框架iMF，显著提升生成性能"
    },
    {
        "title": "AirSim360: A Panoramic Simulation Platform within Drone View",
        "url": "http://arxiv.org/abs/2512.02009v1",
        "pub_date": "2025-12-01",
        "summary": "The field of 360-degree omnidirectional understanding has been receiving increasing attention for advancing spatial intelligence. However, the lack of large-scale and diverse data remains a major limitation. In this work, we propose AirSim360, a simulation platform for omnidirectional data from aerial viewpoints, enabling wide-ranging scene sampling with drones. Specifically, AirSim360 focuses on three key aspects: a render-aligned data and labeling paradigm for pixel-level geometric, semantic, and entity-level understanding; an interactive pedestrian-aware system for modeling human behavior; and an automated trajectory generation paradigm to support navigation tasks. Furthermore, we collect more than 60K panoramic samples and conduct extensive experiments across various tasks to demonstrate the effectiveness of our simulator. Unlike existing simulators, our work is the first to systematically model the 4D real world under an omnidirectional setting. The entire platform, including the toolkit, plugins, and collected datasets, will be made publicly available at https://insta360-research-team.github.io/AirSim360-website.",
        "translated": "360度全景理解领域正日益受到关注，以推动空间智能的发展。然而，大规模且多样化的数据仍是一个主要限制因素。在本研究中，我们提出了AirSim360，这是一个面向空中视角的全景数据仿真平台，支持通过无人机进行广泛场景采样。具体而言，AirSim360聚焦于三个关键方面：一种与渲染对齐的数据和标注范式，用于实现像素级的几何、语义及实体级理解；一个交互式的行人感知系统，用于建模人类行为；以及一种自动化轨迹生成范式，以支持导航任务。此外，我们收集了超过6万张全景样本，并在多种任务上进行了广泛的实验，以验证仿真器的有效性。与现有仿真器不同，我们的工作首次系统性地在全景设置下建模四维真实世界。整个平台（包括工具包、插件及所收集的数据集）将公开发布于 https://insta360-research-team.github.io/AirSim360-website。",
        "translated_title": "AirSim360：无人机视角下的全景仿真平台",
        "label": [],
        "label_reason": "研究聚焦仿真平台与场景建模，非图像像素级恢复",
        "relevance_score": 1,
        "novelty_score": 6,
        "novelty_reason": "首次系统建模4D全景世界，但属高阶仿真任务"
    },
    {
        "title": "MV-TAP: Tracking Any Point in Multi-View Videos",
        "url": "http://arxiv.org/abs/2512.02006v1",
        "pub_date": "2025-12-01",
        "summary": "Multi-view camera systems enable rich observations of complex real-world scenes, and understanding dynamic objects in multi-view settings has become central to various applications. In this work, we present MV-TAP, a novel point tracker that tracks points across multi-view videos of dynamic scenes by leveraging cross-view information. MV-TAP utilizes camera geometry and a cross-view attention mechanism to aggregate spatio-temporal information across views, enabling more complete and reliable trajectory estimation in multi-view videos. To support this task, we construct a large-scale synthetic training dataset and real-world evaluation sets tailored for multi-view tracking. Extensive experiments demonstrate that MV-TAP outperforms existing point-tracking methods on challenging benchmarks, establishing an effective baseline for advancing research in multi-view point tracking.",
        "translated": "多视图相机系统能够对复杂的现实世界场景进行丰富的观测，理解多视图设置下的动态物体已成为多种应用的核心。在本研究中，我们提出了MV-TAP，这是一种新颖的点跟踪器，通过利用跨视图信息，在动态场景的多视图视频中实现点的跨视图跟踪。MV-TAP 利用相机几何结构和跨视图注意力机制，聚合各视图间的时空信息，从而在多视图视频中实现更完整、更可靠的轨迹估计。为支持该任务，我们构建了一个大规模的合成训练数据集和针对多视图跟踪任务定制的真实世界评估数据集。大量实验表明，MV-TAP 在具有挑战性的基准测试上优于现有点跟踪方法，为推进多视图点跟踪研究建立了有效的基线。",
        "translated_title": "MV-TAP：多视角视频中任意点的跟踪",
        "label": [],
        "label_reason": "任务为多视角点跟踪，属高阶视觉任务",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "提出新跨视图注意力机制提升跟踪性能"
    },
    {
        "title": "Learning Visual Affordance from Audio",
        "url": "http://arxiv.org/abs/2512.02005v1",
        "pub_date": "2025-12-01",
        "summary": "We introduce Audio-Visual Affordance Grounding (AV-AG), a new task that segments object interaction regions from action sounds. Unlike existing approaches that rely on textual instructions or demonstration videos, which often limited by ambiguity or occlusion, audio provides real-time, semantically rich, and visually independent cues for affordance grounding, enabling more intuitive understanding of interaction regions. To support this task, we construct the first AV-AG dataset, comprising a large collection of action sounds, object images, and pixel-level affordance annotations. The dataset also includes an unseen subset to evaluate zero-shot generalization. Furthermore, we propose AVAGFormer, a model equipped with a semantic-conditioned cross-modal mixer and a dual-head decoder that effectively fuses audio and visual signals for mask prediction. Experiments show that AVAGFormer achieves state-of-the-art performance on AV-AG, surpassing baselines from related tasks. Comprehensive analyses highlight the distinctions between AV-AG and AVS, the benefits of end-to-end modeling, and the contribution of each component. Code and dataset have been released on https://jscslld.github.io/AVAGFormer/.",
        "translated": "我们提出了音频-视觉可操作性定位（Audio-Visual Affordance Grounding, AV-AG），这是一个从动作声音中分割物体交互区域的新任务。与依赖文本指令或演示视频的现有方法不同，后者常受限于语义模糊或遮挡问题，音频提供了实时、语义丰富且视觉独立的线索，用于可操作性定位，从而实现对交互区域更直观的理解。为支持该任务，我们构建了首个AV-AG数据集，包含大量动作声音、物体图像及像素级可操作性标注。该数据集还包含一个未见子集，以评估零样本泛化能力。此外，我们提出AVAGFormer模型，其配备语义条件交叉模态混合器与双头解码器，能有效融合音频与视觉信号以进行掩码预测。实验表明，AVAGFormer在AV-AG任务上达到当前最优性能，超越相关任务基线模型。全面分析突出了AV-AG与AVS任务的区别、端到端建模的优势以及各组件的贡献。代码与数据集已发布于 https://jscslld.github.io/AVAGFormer/。",
        "translated_title": "从音频中学习视觉可操作性",
        "label": [],
        "label_reason": "任务为多模态交互区域分割，非图像像素级恢复",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "首次端到端音频-视觉 affordance 融合模型"
    },
    {
        "title": "RoaD: Rollouts as Demonstrations for Closed-Loop Supervised Fine-Tuning of Autonomous Driving Policies",
        "url": "http://arxiv.org/abs/2512.01993v1",
        "pub_date": "2025-12-01",
        "summary": "Autonomous driving policies are typically trained via open-loop behavior cloning of human demonstrations. However, such policies suffer from covariate shift when deployed in closed loop, leading to compounding errors. We introduce Rollouts as Demonstrations (RoaD), a simple and efficient method to mitigate covariate shift by leveraging the policy's own closed-loop rollouts as additional training data. During rollout generation, RoaD incorporates expert guidance to bias trajectories toward high-quality behavior, producing informative yet realistic demonstrations for fine-tuning. This approach enables robust closed-loop adaptation with orders of magnitude less data than reinforcement learning, and avoids restrictive assumptions of prior closed-loop supervised fine-tuning (CL-SFT) methods, allowing broader applications domains including end-to-end driving. We demonstrate the effectiveness of RoaD on WOSAC, a large-scale traffic simulation benchmark, where it performs similar or better than the prior CL-SFT method; and in AlpaSim, a high-fidelity neural reconstruction-based simulator for end-to-end driving, where it improves driving score by 41\\% and reduces collisions by 54\\%.",
        "translated": "自动驾驶策略通常通过人类演示的开环行为克隆进行训练。然而，当这些策略在闭环环境下部署时，会因协变量偏移而产生累积性误差。我们提出“Rollouts as Demonstrations”（RoaD），一种简单且高效的方法，通过利用策略自身的闭环 rollout 作为额外训练数据，以缓解协变量偏移问题。在 rollout 生成过程中，RoaD 引入专家引导，使轨迹偏向高质量行为，从而生成具有信息量且真实可靠的演示样本用于微调。该方法可在比强化学习少几个数量级的数据下实现稳健的闭环自适应，并避免了传统闭环监督微调（CL-SFT）方法所依赖的诸多限制性假设，因此可广泛应用于包括端到端驾驶在内的多个领域。我们在 WOSAC（大规模交通仿真基准）上验证了 RoaD 的有效性，其性能与现有 CL-SFT 方法相当或更优；并在 AlpaSim（基于神经重建的高保真端到端驾驶模拟器）中，使其驾驶评分提升 41%，碰撞次数减少 54%。",
        "translated_title": "RoaD：作为闭环监督微调自动驾驶策略的演示轨迹 rollout",
        "label": [],
        "label_reason": "属于自动驾驶策略训练，非图像处理任务。",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "提出新方法优化闭环策略微调，效率显著提升。"
    },
    {
        "title": "PAI-Bench: A Comprehensive Benchmark For Physical AI",
        "url": "http://arxiv.org/abs/2512.01989v1",
        "pub_date": "2025-12-01",
        "summary": "Physical AI aims to develop models that can perceive and predict real-world dynamics; yet, the extent to which current multi-modal large language models and video generative models support these abilities is insufficiently understood. We introduce Physical AI Bench (PAI-Bench), a unified and comprehensive benchmark that evaluates perception and prediction capabilities across video generation, conditional video generation, and video understanding, comprising 2,808 real-world cases with task-aligned metrics designed to capture physical plausibility and domain-specific reasoning. Our study provides a systematic assessment of recent models and shows that video generative models, despite strong visual fidelity, often struggle to maintain physically coherent dynamics, while multi-modal large language models exhibit limited performance in forecasting and causal interpretation. These observations suggest that current systems are still at an early stage in handling the perceptual and predictive demands of Physical AI. In summary, PAI-Bench establishes a realistic foundation for evaluating Physical AI and highlights key gaps that future systems must address.",
        "translated": "物理人工智能旨在开发能够感知并预测现实世界动态的模型；然而，当前多模态大语言模型与视频生成模型在支持这些能力方面的程度尚缺乏充分理解。我们引入了物理人工智能基准（PAI-Bench），这是一个统一且全面的评估基准，用于衡量视频生成、条件视频生成和视频理解中的感知与预测能力，涵盖2,808个真实世界案例，并配备与任务对齐的评估指标，以捕捉物理合理性及领域特定推理能力。本研究系统性评估了近期模型的表现，发现尽管视频生成模型在视觉保真度方面表现优异，但往往难以维持物理上一致的动力学行为；而多模态大语言模型在预测与因果解释方面则显示出有限性能。这些观察表明，当前系统在处理物理人工智能所需的感知与预测需求方面仍处于早期阶段。总之，PAI-Bench为物理人工智能的评估奠定了现实基础，并指出了未来系统亟需弥补的关键差距。",
        "translated_title": "PAI-Bench：物理AI的综合性基准测试集",
        "label": [],
        "label_reason": "评估物理AI能力，非图像像素级处理",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "提出新基准，但非图像恢复类创新"
    },
    {
        "title": "Artemis: Structured Visual Reasoning for Perception Policy Learning",
        "url": "http://arxiv.org/abs/2512.01988v1",
        "pub_date": "2025-12-01",
        "summary": "Recent reinforcement-learning frameworks for visual perception policy have begun to incorporate intermediate reasoning chains expressed in natural language. Empirical observations indicate that such purely linguistic intermediate reasoning often reduces performance on perception tasks. We argue that the core issue lies not in reasoning per se but in the form of reasoning: while these chains perform semantic reasoning in an unstructured linguistic space, visual perception requires reasoning in a spatial and object-centric space. In response, we introduce Artemis, a perception-policy learning framework that performs structured proposal-based reasoning, where each intermediate step is represented as a (label, bounding-box) pair capturing a verifiable visual state. This design enables explicit tracking of intermediate states, direct supervision for proposal quality, and avoids ambiguity introduced by language-based reasoning. Artemis is built on Qwen2.5-VL-3B, achieves strong performance on grounding and detection task and exhibits substantial generalization to counting and geometric-perception tasks. The consistent improvements across these diverse settings confirm that aligning reasoning with spatial representations enhances perception-policy learning. Owing to its strengthened visual reasoning, Artemis also achieves competitive performance on general MLLM benchmarks, illustrating that spatially grounded reasoning provides a principled route toward scalable and general perception policies.",
        "translated": "近年来，用于视觉感知策略的强化学习框架开始引入以自然语言表达的中间推理链。实证观察表明，此类纯语言形式的中间推理往往降低感知任务的性能。我们认为，问题的核心并非推理本身，而是推理的形式：这些推理链在无结构的语言空间中进行语义推理，而视觉感知则需要在空间与物体为中心的空间中进行推理。为此，我们提出了Artemis，一种基于结构化提案的感知策略学习框架，其中每个中间步骤均表示为（标签，边界框）对，用以捕捉可验证的视觉状态。该设计支持中间状态的显式跟踪、提案质量的直接监督，并避免了基于语言推理所引入的歧义。Artemis基于Qwen2.5-VL-3B构建，在定位与检测任务上表现优异，并在计数和几何感知任务中展现出强大的泛化能力。在这些多样化场景中的持续性能提升证实，将推理与空间表示对齐能够增强感知策略的学习效果。得益于其强化的视觉推理能力，Artemis在通用多模态大模型（MLLM）基准测试中亦取得竞争力的表现，表明空间锚定推理为实现可扩展且通用的感知策略提供了一种原理性的路径。",
        "translated_title": "Artemis：面向感知策略学习的结构化视觉推理",
        "label": [],
        "label_reason": "任务属高阶视觉决策，非像素级图像处理",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "结构化视觉推理框架创新，但非图像恢复"
    },
    {
        "title": "Chain-of-Ground: Improving GUI Grounding via Iterative Reasoning and Reference Feedback",
        "url": "http://arxiv.org/abs/2512.01979v1",
        "pub_date": "2025-12-01",
        "summary": "GUI grounding aims to align natural language instructions with precise regions in complex user interfaces. Advanced multimodal large language models show strong ability in visual GUI grounding but still struggle with small or visually similar targets and ambiguity in real world layouts. These limitations arise from limited grounding capacity and from underuse of existing reasoning potential. We present Chain of Ground CoG a training free multi step grounding framework that uses multimodal large language models for iterative visual reasoning and refinement. Instead of direct prediction the model progressively reflects and adjusts its hypotheses leading to more accurate and interpretable localization. Our approach achieves 68.4 accuracy on the ScreenSpot Pro benchmark an improvement of 4.8 points. To measure real world generalization we introduce TPanel UI a dataset of 420 labeled industrial control panels with visual distortions such as blur and masking. On TPanel UI Chain of Ground improves over the strong baseline Qwen3 VL 235B by 6.9 points showing the effectiveness of multi step training free grounding across real world and digital interfaces. These results highlight a direction for unlocking grounding potential through structured iterative refinement instead of additional training.",
        "translated": "GUI grounding旨在将自然语言指令与复杂用户界面中的精确区域对齐。先进的多模态大语言模型在视觉GUI grounding任务中展现出强大能力，但在处理小尺寸或视觉相似的目标以及现实布局中的歧义时仍存在困难。这些局限性源于其接地能力有限，以及对现有推理潜力的利用不足。我们提出了Chain of Ground（CoG），一种无需训练的多步骤接地框架，该框架利用多模态大语言模型进行迭代视觉推理与优化。模型不再直接预测，而是逐步反思并调整自身假设，从而实现更准确且可解释的定位。我们的方法在ScreenSpot Pro基准测试中取得了68.4%的准确率，相比此前最佳结果提升了4.8个百分点。为评估真实世界泛化能力，我们引入了TPanel UI数据集——包含420个标注工业控制面板，其图像存在模糊、遮挡等视觉畸变。在TPanel UI上，Chain of Ground相较强基线模型Qwen3 VL 235B提升了6.9个百分点，充分验证了无需训练的多步骤接地方法在真实世界与数字界面中的有效性。这些结果凸显了一种新的方向：通过结构化的迭代优化释放接地潜力，而非依赖额外训练。",
        "translated_title": "Chain-of-Ground：通过迭代推理与参考反馈提升 GUI 地址定位性能",
        "label": [],
        "label_reason": "任务为GUI定位，属高阶视觉理解",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "迭代推理框架提升定位精度，非像素级图像处理"
    },
    {
        "title": "SGDiff: Scene Graph Guided Diffusion Model for Image Collaborative SegCaptioning",
        "url": "http://arxiv.org/abs/2512.01975v1",
        "pub_date": "2025-12-01",
        "summary": "Controllable image semantic understanding tasks, such as captioning or segmentation, necessitate users to input a prompt (e.g., text or bounding boxes) to predict a unique outcome, presenting challenges such as high-cost prompt input or limited information output. This paper introduces a new task ``Image Collaborative Segmentation and Captioning'' (SegCaptioning), which aims to translate a straightforward prompt, like a bounding box around an object, into diverse semantic interpretations represented by (caption, masks) pairs, allowing flexible result selection by users. This task poses significant challenges, including accurately capturing a user's intention from a minimal prompt while simultaneously predicting multiple semantically aligned caption words and masks. Technically, we propose a novel Scene Graph Guided Diffusion Model that leverages structured scene graph features for correlated mask-caption prediction. Initially, we introduce a Prompt-Centric Scene Graph Adaptor to map a user's prompt to a scene graph, effectively capturing his intention. Subsequently, we employ a diffusion process incorporating a Scene Graph Guided Bimodal Transformer to predict correlated caption-mask pairs by uncovering intricate correlations between them. To ensure accurate alignment, we design a Multi-Entities Contrastive Learning loss to explicitly align visual and textual entities by considering inter-modal similarity, resulting in well-aligned caption-mask pairs. Extensive experiments conducted on two datasets demonstrate that SGDiff achieves superior performance in SegCaptioning, yielding promising results for both captioning and segmentation tasks with minimal prompt input.",
        "translated": "可控的图像语义理解任务（如图文生成或分割）要求用户输入提示（如文本或边界框），以预测唯一结果，这带来了诸如高成本提示输入或输出信息有限等挑战。本文提出一项新任务“图像协同分割与图文生成”（SegCaptioning），旨在将简单的提示（如围绕物体的边界框）转化为由（图文、掩码）对表示的多样化语义解释，从而允许用户灵活选择结果。该任务具有显著挑战：需从极简提示中精准捕捉用户意图，同时预测多个语义一致的图文词与掩码对。技术上，我们提出一种新颖的场景图引导扩散模型（SGDiff），利用结构化场景图特征实现相关掩码-图文对的联合预测。首先，我们引入“提示中心场景图适配器”，将用户的提示映射为场景图，有效捕捉其意图；随后，我们采用融合场景图引导双模态变换器的扩散过程，通过揭示图文与掩码间的复杂关联，预测相关联的图文-掩码对。为确保精确对齐，我们设计了多实体对比学习损失函数，通过考虑跨模态相似性显式对齐视觉与文本实体，最终生成高度对齐的图文-掩码对。在两个数据集上的大量实验表明，SGDiff在SegCaptioning任务中表现卓越，在极简提示输入下，其图文生成与分割性能均取得令人满意的结果。",
        "translated_title": "SGDiff：基于场景图引导的扩散模型用于图像协同分割与标题生成",
        "label": [],
        "label_reason": "任务为高阶语义理解，非像素级图像恢复",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "引入场景图引导扩散模型提升多模态对齐"
    },
    {
        "title": "SpriteHand: Real-Time Versatile Hand-Object Interaction with Autoregressive Video Generation",
        "url": "http://arxiv.org/abs/2512.01960v1",
        "pub_date": "2025-12-01",
        "summary": "Modeling and synthesizing complex hand-object interactions remains a significant challenge, even for state-of-the-art physics engines. Conventional simulation-based approaches rely on explicitly defined rigid object models and pre-scripted hand gestures, making them inadequate for capturing dynamic interactions with non-rigid or articulated entities such as deformable fabrics, elastic materials, hinge-based structures, furry surfaces, or even living creatures. In this paper, we present SpriteHand, an autoregressive video generation framework for real-time synthesis of versatile hand-object interaction videos across a wide range of object types and motion patterns. SpriteHand takes as input a static object image and a video stream in which the hands are imagined to interact with the virtual object embedded in a real-world scene, and generates corresponding hand-object interaction effects in real time. Our model employs a causal inference architecture for autoregressive generation and leverages a hybrid post-training approach to enhance visual realism and temporal coherence. Our 1.3B model supports real-time streaming generation at around 18 FPS and 640x368 resolution, with an approximate 150 ms latency on a single NVIDIA RTX 5090 GPU, and more than a minute of continuous output. Experiments demonstrate superior visual quality, physical plausibility, and interaction fidelity compared to both generative and engine-based baselines.",
        "translated": "建模与合成复杂的双手-物体交互仍是一项重大挑战，即使对于最先进的物理引擎而言亦然。传统的基于仿真方法依赖于明确定义的刚体对象模型和预设的手部动作脚本，难以有效捕捉与非刚性或铰接式实体（如可变形织物、弹性材料、铰链结构、毛绒表面，甚至生物体）之间的动态交互。在本文中，我们提出了 SpriteHand，一种用于实时生成多样手-物体交互视频的自回归视频生成框架，支持广泛类型物体及运动模式。SpriteHand 接收一张静态物体图像与一段视频流，其中手被假定为与嵌入现实场景中的虚拟物体进行交互，并实时生成相应的手-物体交互效果。我们的模型采用因果推理架构实现自回归生成，并结合混合后训练策略以增强视觉真实感与时序一致性。1.3B 参数规模的模型可在单张 NVIDIA RTX 5090 GPU 上实现约 18 FPS 和 640x368 分辨率的实时流式生成，延迟约为 150 毫秒，并支持超过一分钟的连续输出。实验表明，相较于现有生成式及引擎类基线方法，我们的方法在视觉质量、物理合理性及交互保真度方面均表现更优。",
        "translated_title": "SpriteHand: 基于自回归视频生成的实时通用手-物体交互",
        "label": [],
        "label_reason": "生成视频交互，非图像像素级恢复",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "新架构提升实时生成质量与物理一致性"
    },
    {
        "title": "GrndCtrl: Grounding World Models via Self-Supervised Reward Alignment",
        "url": "http://arxiv.org/abs/2512.01952v1",
        "pub_date": "2025-12-01",
        "summary": "Recent advances in video world modeling have enabled large-scale generative models to simulate embodied environments with high visual fidelity, providing strong priors for prediction, planning, and control. Yet, despite their realism, these models often lack geometric grounding, limiting their use in navigation tasks that require spatial coherence and long-horizon stability. We introduce Reinforcement Learning with World Grounding (RLWG), a self-supervised post-training framework that aligns pretrained world models with a physically verifiable structure through geometric and perceptual rewards. Analogous to reinforcement learning from verifiable feedback (RLVR) in language models, RLWG can use multiple rewards that measure pose cycle-consistency, depth reprojection, and temporal coherence. We instantiate this framework with GrndCtrl, a reward-aligned adaptation method based on Group Relative Policy Optimization (GRPO), yielding world models that maintain stable trajectories, consistent geometry, and reliable rollouts for embodied navigation. Like post-training alignment in large language models, GrndCtrl leverages verifiable rewards to bridge generative pretraining and grounded behavior, achieving superior spatial coherence and navigation stability over supervised fine-tuning in outdoor environments.",
        "translated": "近年来视频世界建模的进展使大规模生成模型能够以高视觉保真度模拟具身环境，为预测、规划与控制任务提供了强大的先验知识。然而，尽管这些模型具有高度真实感，其往往缺乏几何 grounding，限制了其在需要空间一致性与长时稳定性的导航任务中的应用。我们提出一种名为 Reinforcement Learning with World Grounding（RLWG）的自监督后训练框架，该框架通过几何与感知奖励，将预训练的世界模型对齐至可物理验证的结构。这类似于语言模型中基于可验证反馈的强化学习（RLVR），RLWG 可利用多种奖励机制，分别衡量姿态循环一致性、深度重投影与时间一致性。我们以 GrndCtrl 作为该框架的具体实现，GrndCtrl 是一种基于 Group Relative Policy Optimization（GRPO）的奖励对齐适配方法，由此获得的世界模型能够在具身导航中保持稳定轨迹、一致几何结构与可靠的 rollout 能力。类似大语言模型中的后训练对齐策略，GrndCtrl 利用可验证奖励弥合生成式预训练与 grounded 行为之间的鸿沟，在户外环境中实现了优于监督微调的空间一致性与导航稳定性。",
        "translated_title": "GrndCtrl：通过自监督奖励对齐构建世界模型",
        "label": [],
        "label_reason": "任务属高阶导航控制，非像素级图像处理",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "提出新奖励对齐框架，但非图像恢复任务"
    },
    {
        "title": "Script: Graph-Structured and Query-Conditioned Semantic Token Pruning for Multimodal Large Language Models",
        "url": "http://arxiv.org/abs/2512.01949v1",
        "pub_date": "2025-12-01",
        "summary": "The rapid growth of visual tokens in multimodal large language models (MLLMs) leads to excessive memory consumption and inference latency, especially when handling high-resolution images and videos. Token pruning is a technique used to mitigate this issue by removing redundancy, but existing methods often ignore relevance to the user query or suffer from the limitations of attention mechanisms, reducing their adaptability and effectiveness. To address these challenges, we propose Script, a plug-and-play pruning method that requires no retraining and generalizes across diverse MLLMs. Script comprises two modules: a graph-structured pruning module that removes visually redundant tokens, and a query-conditioned semantic pruning module that preserves query-relevant visual information. Together, they enhance performance on multimodal tasks. Experiments on fourteen benchmarks across image and video understanding tasks show that Script consistently achieves higher model efficiency and predictive accuracy compared to existing pruning methods. On LLaVA-NeXT-7B, it achieves up to 6.8x prefill speedup and 10x FLOP reduction, while retaining 96.88% of the original performance.",
        "translated": "多模态大语言模型（MLLMs）中视觉令牌的快速增长导致内存消耗过高和推理延迟显著增加，尤其是在处理高分辨率图像和视频时。令牌剪枝是一种通过去除冗余信息来缓解该问题的技术，但现有方法常忽略与用户查询的相关性，或受限于注意力机制，从而降低了其适应性和有效性。为应对这些挑战，我们提出Script，一种即插即用的剪枝方法，无需重新训练且能泛化至多种MLLMs。Script由两个模块组成：一个基于图结构的剪枝模块，用于移除视觉上冗余的令牌；以及一个基于查询条件的语义剪枝模块，用于保留与查询相关的视觉信息。二者协同作用，提升了在多模态任务上的性能表现。实验在十四项涵盖图像与视频理解任务的基准数据集上表明，Script相较现有剪枝方法始终实现更高的模型效率与预测精度。在LLaVA-NeXT-7B上，它可实现高达6.8倍的预填充速度提升及10倍的FLOP降低，同时保持96.88%的原始性能。",
        "translated_title": "脚本：面向多模态大语言模型的图结构与查询引导语义标记裁剪",
        "label": [],
        "label_reason": "处理模型效率优化，非图像像素级恢复任务。",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出图结构与查询条件剪枝方法，提升多模态模型效率。"
    },
    {
        "title": "Guardian: Detecting Robotic Planning and Execution Errors with Vision-Language Models",
        "url": "http://arxiv.org/abs/2512.01946v1",
        "pub_date": "2025-12-01",
        "summary": "Robust robotic manipulation requires reliable failure detection and recovery. Although current Vision-Language Models (VLMs) show promise, their accuracy and generalization are limited by the scarcity of failure data. To address this data gap, we propose an automatic robot failure synthesis approach that procedurally perturbs successful trajectories to generate diverse planning and execution failures. This method produces not only binary classification labels but also fine-grained failure categories and step-by-step reasoning traces in both simulation and the real world. With it, we construct three new failure detection benchmarks: RLBench-Fail, BridgeDataV2-Fail, and UR5-Fail, substantially expanding the diversity and scale of existing failure datasets. We then train Guardian, a VLM with multi-view images for detailed failure reasoning and detection. Guardian achieves state-of-the-art performance on both existing and newly introduced benchmarks. It also effectively improves task success rates when integrated into a state-of-the-art manipulation system in simulation and real robots, demonstrating the impact of our generated failure data.",
        "translated": "可靠的机器人操作需要可靠的故障检测与恢复能力。尽管当前的视觉-语言模型（VLMs）展现出潜力，但其准确性和泛化能力受限于故障数据的稀缺性。为解决这一数据缺口，我们提出一种自动机器人故障合成方法，通过程序化扰动成功轨迹来生成多样化的规划与执行故障。该方法不仅能输出二分类标签，还能在仿真和真实世界中生成细粒度的故障类别及逐步推理轨迹。基于此，我们构建了三个新的故障检测基准：RLBench-Fail、BridgeDataV2-Fail 和 UR5-Fail，显著扩展了现有故障数据集的多样性与规模。随后，我们训练了 Guardian——一个支持多视角图像的 VLM，用于进行详细的故障推理与检测。Guardian 在现有及新引入的基准上均实现了最先进性能，并且当集成到最先进的操作系统中时，在仿真与真实机器人上均有效提升了任务成功率，验证了所生成故障数据的价值。",
        "translated_title": "Guardian：利用视觉-语言模型检测机器人规划与执行错误",
        "label": [],
        "label_reason": "任务为机器人故障检测，属高阶视觉应用",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出新合成失败数据方法并构建基准"
    },
    {
        "title": "Physical ID-Transfer Attacks against Multi-Object Tracking via Adversarial Trajectory",
        "url": "http://arxiv.org/abs/2512.01934v1",
        "pub_date": "2025-12-01",
        "summary": "Multi-Object Tracking (MOT) is a critical task in computer vision, with applications ranging from surveillance systems to autonomous driving. However, threats to MOT algorithms have yet been widely studied. In particular, incorrect association between the tracked objects and their assigned IDs can lead to severe consequences, such as wrong trajectory predictions. Previous attacks against MOT either focused on hijacking the trackers of individual objects, or manipulating the tracker IDs in MOT by attacking the integrated object detection (OD) module in the digital domain, which are model-specific, non-robust, and only able to affect specific samples in offline datasets. In this paper, we present AdvTraj, the first online and physical ID-manipulation attack against tracking-by-detection MOT, in which an attacker uses adversarial trajectories to transfer its ID to a targeted object to confuse the tracking system, without attacking OD. Our simulation results in CARLA show that AdvTraj can fool ID assignments with 100% success rate in various scenarios for white-box attacks against SORT, which also have high attack transferability (up to 93% attack success rate) against state-of-the-art (SOTA) MOT algorithms due to their common design principles. We characterize the patterns of trajectories generated by AdvTraj and propose two universal adversarial maneuvers that can be performed by a human walker/driver in daily scenarios. Our work reveals under-explored weaknesses in the object association phase of SOTA MOT systems, and provides insights into enhancing the robustness of such systems.",
        "translated": "多目标跟踪（MOT）是计算机视觉中的关键任务，其应用场景涵盖安防监控到自动驾驶等多个领域。然而，针对MOT算法的攻击研究尚未得到充分展开。特别是，被跟踪目标与其分配ID之间的错误关联可能导致严重后果，例如轨迹预测错误。以往针对MOT的攻击要么专注于劫持单个目标的跟踪器，要么通过在数字域中攻击集成的目标检测（OD）模块来篡改MOT中的跟踪器ID，这些方法具有模型特异性、非鲁棒性，并且仅能影响离线数据集中特定样本。本文提出AdvTraj，这是首个针对基于检测的MOT系统的在线式、物理层面ID操纵攻击。攻击者利用对抗性轨迹将自身ID转移至目标对象，从而迷惑跟踪系统，且无需攻击目标检测模块。我们在CARLA仿真环境中验证表明，AdvTraj可在针对SORT的白盒攻击场景中以100%成功率误导ID分配；同时，由于主流SOTA MOT算法的设计原理具有共性，该攻击具备高度迁移能力（最高可达93%的成功率）。我们分析了AdvTraj生成的轨迹模式，并提出了两种可由人类步行者/驾驶员在日常场景中执行的通用对抗性操作。本工作揭示了当前SOTA MOT系统在目标关联阶段尚未被充分探索的脆弱性，并为提升此类系统的鲁棒性提供了启示。",
        "translated_title": "基于对抗性轨迹的多目标跟踪中的物理ID传输攻击",
        "label": [],
        "label_reason": "攻击目标为轨迹ID篡改，属高阶系统鲁棒性研究。",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "提出首个物理可执行的ID转移攻击方法，具泛化性。"
    },
    {
        "title": "LORE: A Large Generative Model for Search Relevance",
        "url": "http://arxiv.org/abs/2512.03025v1",
        "pub_date": "2025-12-02",
        "summary": "Achievement. We introduce LORE, a systematic framework for Large Generative Model-based relevance in e-commerce search. Deployed and iterated over three years, LORE achieves a cumulative +27\\% improvement in online GoodRate metrics. This report shares the valuable experience gained throughout its development lifecycle, spanning data, features, training, evaluation, and deployment. Insight. While existing works apply Chain-of-Thought (CoT) to enhance relevance, they often hit a performance ceiling. We argue this stems from treating relevance as a monolithic task, lacking principled deconstruction. Our key insight is that relevance comprises distinct capabilities: knowledge and reasoning, multi-modal matching, and rule adherence. We contend that a qualitative-driven decomposition is essential for breaking through current performance bottlenecks. Contributions. LORE provides a complete blueprint for the LLM relevance lifecycle. Key contributions include: (1) A two-stage training paradigm combining progressive CoT synthesis via SFT with human preference alignment via RL. (2) A comprehensive benchmark, RAIR, designed to evaluate these core capabilities. (3) A query frequency-stratified deployment strategy that efficiently transfers offline LLM capabilities to the online system. LORE serves as both a practical solution and a methodological reference for other vertical domains.",
        "translated": "成果。我们提出了LORE，一个基于大生成式模型的电商搜索相关性系统化框架。该框架在三年部署与迭代过程中，累计实现线上GoodRate指标提升27%。本报告分享其全生命周期开发过程中的宝贵经验，涵盖数据、特征、训练、评估与部署等环节。\n\n洞察。尽管现有工作通过思维链（Chain-of-Thought, CoT）提升相关性，往往受限于性能瓶颈。我们认为这源于将相关性视为单一任务，缺乏原则性的分解。我们的核心洞见是：相关性包含三项独立能力——知识与推理、多模态匹配、规则遵循。我们主张，唯有基于定性驱动的分解，方可突破当前性能瓶颈。\n\n贡献。LORE为大语言模型（LLM）相关性生命周期提供了完整蓝图。主要贡献包括：（1）一种两阶段训练范式，结合通过监督微调（SFT）进行渐进式CoT合成与通过强化学习（RL）对齐人类偏好；（2）一套综合性基准RAIR，用于评估上述核心能力；（3）一种按查询频率分层的部署策略，高效将离线LLM能力迁移至在线系统。LORE既是实际解决方案，亦可作为其他垂直领域方法论的参考。",
        "translated_title": "LORE：用于搜索相关性的大型生成模型",
        "label": [
            "LLM生成式推荐",
            "重排",
            "通用推荐技术"
        ],
        "label_reason": "基于LLM的搜索相关性优化，适配推荐系统重排环节。",
        "relevance_score": 7,
        "novelty_score": 8,
        "novelty_reason": "创新两阶段训练与能力分解框架，突破性能瓶颈。"
    },
    {
        "title": "GraphMatch: Fusing Language and Graph Representations in a Dynamic Two-Sided Work Marketplace",
        "url": "http://arxiv.org/abs/2512.02849v1",
        "pub_date": "2025-12-02",
        "summary": "Recommending matches in a text-rich, dynamic two-sided marketplace presents unique challenges due to evolving content and interaction graphs. We introduce GraphMatch, a new large-scale recommendation framework that fuses pre-trained language models with graph neural networks to overcome these challenges. Unlike prior approaches centered on standalone models, GraphMatch is a comprehensive recipe built on powerful text encoders and GNNs working in tandem. It employs adversarial negative sampling alongside point-in-time subgraph training to learn representations that capture both the fine-grained semantics of evolving text and the time-sensitive structure of the graph. We evaluated extensively on interaction data from Upwork, a leading labor marketplace, at large scale, and discuss our approach towards low-latency inference suitable for real-time use. In our experiments, GraphMatch outperforms language-only and graph-only baselines on matching tasks while being efficient at runtime. These results demonstrate that unifying language and graph representations yields a highly effective solution to text-rich, dynamic two-sided recommendations, bridging the gap between powerful pretrained LMs and large-scale graphs in practice.",
        "translated": "在内容丰富且动态的双边市场中推荐匹配项面临独特挑战，因其内容与交互图结构持续演化。我们提出 GraphMatch，一种全新的大规模推荐框架，融合预训练语言模型与图神经网络以应对上述挑战。与以往聚焦于独立模型的方法不同，GraphMatch 是一套基于强大文本编码器与 GNN 协同工作的综合性方案。其采用对抗负采样与时间点子图训练相结合的方式，学习到同时捕捉动态文本细粒度语义与时序敏感图结构的表征。我们在 Upwork（全球领先劳动力市场）的大规模交互数据上进行了全面评估，并探讨了面向实时应用的低延迟推理方法。实验表明，GraphMatch 在匹配任务上优于仅依赖语言模型或仅依赖图结构的基线方法，同时具备高效的运行时性能。这些结果证明，统一语言与图表示可为文本丰富、动态双向推荐提供高度有效的解决方案，在实践中弥合了强大预训练大语言模型与大规模图结构之间的鸿沟。",
        "translated_title": "GraphMatch：在动态双边工作市场中融合语言与图表示",
        "label": [
            "多模态推荐",
            "图神经网络推荐",
            "LLM生成式推荐"
        ],
        "label_reason": "融合语言模型与图结构，解决动态双边市场推荐问题。",
        "relevance_score": 9,
        "novelty_score": 8,
        "novelty_reason": "首创联合LM与GNN架构，提升语义与结构建模能力。"
    },
    {
        "title": "Towards Unification of Hallucination Detection and Fact Verification for Large Language Models",
        "url": "http://arxiv.org/abs/2512.02772v1",
        "pub_date": "2025-12-02",
        "summary": "Large Language Models (LLMs) frequently exhibit hallucinations, generating content that appears fluent and coherent but is factually incorrect. Such errors undermine trust and hinder their adoption in real-world applications. To address this challenge, two distinct research paradigms have emerged: model-centric Hallucination Detection (HD) and text-centric Fact Verification (FV). Despite sharing the same goal, these paradigms have evolved in isolation, using distinct assumptions, datasets, and evaluation protocols. This separation has created a research schism that hinders their collective progress. In this work, we take a decisive step toward bridging this divide. We introduce UniFact, a unified evaluation framework that enables direct, instance-level comparison between FV and HD by dynamically generating model outputs and corresponding factuality labels. Through large-scale experiments across multiple LLM families and detection methods, we reveal three key findings: (1) No paradigm is universally superior; (2) HD and FV capture complementary facets of factual errors; and (3) hybrid approaches that integrate both methods consistently achieve state-of-the-art performance. Beyond benchmarking, we provide the first in-depth analysis of why FV and HD diverged, as well as empirical evidence supporting the need for their unification. The comprehensive experimental results call for a new, integrated research agenda toward unifying Hallucination Detection and Fact Verification in LLMs.   We have open-sourced all the code, data, and baseline implementation at: https://github.com/oneal2000/UniFact/",
        "translated": "大语言模型（LLMs）经常表现出幻觉现象，生成的内容虽流畅连贯，但事实错误。此类错误会削弱用户信任，并阻碍其在实际应用中的落地。为应对这一挑战，研究界已形成两种截然不同的研究范式：以模型为中心的幻觉检测（HD）与以文本为中心的事实验证（FV）。尽管二者目标一致，却各自独立发展，采用不同的假设、数据集和评估协议。这种割裂导致了研究领域的分裂，严重制约了双方协同进步。本文采取关键举措弥合此鸿沟。我们提出UniFact，一个统一的评估框架，通过动态生成模型输出及其对应的事实性标签，实现FV与HD在实例级别上的直接对比。通过对多个LLM家族及检测方法的大规模实验，我们揭示三项核心发现：（1）不存在普遍优于他者的单一范式；（2）HD与FV分别捕捉事实错误的不同侧面，具有互补性；（3）融合二者方法的混合策略始终取得当前最优性能。除基准测试外，我们首次深入剖析了FV与HD为何走向分野，并提供实证证据支持其统一化的必要性。全面的实验结果呼吁建立新的整合性研究议程，推动LLMs中幻觉检测与事实验证的统一化发展。我们已开源全部代码、数据及基线实现，详见：https://github.com/oneal2000/UniFact/",
        "translated_title": "面向大语言模型的幻觉检测与事实验证的统一化研究",
        "label": [],
        "label_reason": "论文聚焦LLM事实验证，与推荐系统无直接关联",
        "relevance_score": 3,
        "novelty_score": 7,
        "novelty_reason": "提出统一框架对比HD与FV，方法具创新性但非推荐领域"
    },
    {
        "title": "FGC-Comp: Adaptive Neighbor-Grouped Attribute Completion for Graph-based Anomaly Detection",
        "url": "http://arxiv.org/abs/2512.02705v1",
        "pub_date": "2025-12-02",
        "summary": "Graph-based Anomaly Detection models have gained widespread adoption in recent years, identifying suspicious nodes by aggregating neighborhood information. However, most existing studies overlook the pervasive issues of missing and adversarially obscured node attributes, which can undermine aggregation stability and prediction reliability. To mitigate this, we propose FGC-Comp, a lightweight, classifier-agnostic, and deployment-friendly attribute completion module-designed to enhance neighborhood aggregation under incomplete attributes. We partition each node's neighbors into three label-based groups, apply group-specific transforms to the labeled groups while a node-conditioned gate handles unknowns, fuse messages via residual connections, and train end-to-end with a binary classification objective to improve aggregation stability and prediction reliability under missing attributes. Experiments on two real-world fraud datasets validate the effectiveness of the approach with negligible computational overhead.",
        "translated": "近年来，基于图的异常检测模型已被广泛采用，通过聚合邻居信息识别可疑节点。然而，现有大多数研究忽视了节点属性缺失及对抗性遮蔽这一普遍存在的问题，这可能破坏聚合稳定性并降低预测可靠性。为缓解该问题，我们提出FGC-Comp，一种轻量级、分类器无关且易于部署的属性补全模块，旨在提升在不完整属性下的邻居聚合效果。我们根据标签将每个节点的邻居划分为三类，对有标签组应用特定变换，而节点条件门控机制处理未知属性，通过残差连接融合消息，并以二分类目标端到端训练，从而在属性缺失情况下增强聚合稳定性与预测可靠性。在两个真实世界欺诈数据集上的实验验证了该方法的有效性，且计算开销微乎其微。",
        "translated_title": "FGC-Comp：面向图结构异常检测的自适应邻居分组属性补全",
        "label": [],
        "label_reason": "论文聚焦图异常检测，非推荐系统相关",
        "relevance_score": 3,
        "novelty_score": 6,
        "novelty_reason": "方法提升图模型鲁棒性，但无推荐系统创新"
    },
    {
        "title": "Spatially-Grounded Document Retrieval via Patch-to-Region Relevance Propagation",
        "url": "http://arxiv.org/abs/2512.02660v1",
        "pub_date": "2025-12-02",
        "summary": "Vision-language models (VLMs) like ColPali achieve state-of-the-art document retrieval by embedding pages as images and computing fine-grained similarity between query tokens and visual patches. However, they return entire pages rather than specific regions, limiting utility for retrieval-augmented generation (RAG) where precise context is paramount. Conversely, OCR-based systems extract structured text with bounding box coordinates but lack semantic grounding for relevance assessment. We propose a hybrid architecture that unifies these paradigms: using ColPali's patch-level similarity scores as spatial relevance filters over OCR-extracted regions. We formalize the coordinate mapping between vision transformer patch grids and OCR bounding boxes, introduce intersection metrics for relevance propagation, and establish theoretical bounds on retrieval precision. Our approach operates at inference time without additional training. We release Snappy, an open-source implementation demonstrating practical applicability, with empirical evaluation ongoing.",
        "translated": "视觉-语言模型（VLMs）如 ColPali 通过将页面视为图像进行嵌入，并计算查询词元与视觉补丁之间的细粒度相似度，实现了最先进的文档检索性能。然而，它们返回的是整个页面而非特定区域，这在对精确上下文要求极高的检索增强生成（RAG）场景中限制了其实用性。相反，基于 OCR 的系统虽能提取带有边界框坐标的结构化文本，但缺乏语义锚定以评估相关性。我们提出一种混合架构，统一上述两种范式：利用 ColPali 在补丁级别输出的相似度分数作为空间相关性过滤器，作用于 OCR 提取的各个区域之上。我们形式化了视觉 Transformer 补丁网格与 OCR 边界框之间的坐标映射关系，引入交集度量用于相关性传播，并建立了检索精度的理论界限。该方法无需额外训练即可在推理阶段运行。我们开源了 Snappy，一个展示实际可行性的实现版本，目前尚在开展实证评估。",
        "translated_title": "基于空间定位的文档检索：通过Patch到区域的相关性传播",
        "label": [],
        "label_reason": "聚焦视觉文档检索，非推荐系统核心问题",
        "relevance_score": 3,
        "novelty_score": 6,
        "novelty_reason": "改进坐标映射与传播机制，但无推荐场景创新"
    },
    {
        "title": "ADORE: Autonomous Domain-Oriented Relevance Engine for E-commerce",
        "url": "http://arxiv.org/abs/2512.02555v1",
        "pub_date": "2025-12-02",
        "summary": "Relevance modeling in e-commerce search remains challenged by semantic gaps in term-matching methods (e.g., BM25) and neural models' reliance on the scarcity of domain-specific hard samples. We propose ADORE, a self-sustaining framework that synergizes three innovations: (1) A Rule-aware Relevance Discrimination module, where a Chain-of-Thought LLM generates intent-aligned training data, refined via Kahneman-Tversky Optimization (KTO) to align with user behavior; (2) An Error-type-aware Data Synthesis module that auto-generates adversarial examples to harden robustness; and (3) A Key-attribute-enhanced Knowledge Distillation module that injects domain-specific attribute hierarchies into a deployable student model. ADORE automates annotation, adversarial generation, and distillation, overcoming data scarcity while enhancing reasoning. Large-scale experiments and online A/B testing verify the effectiveness of ADORE. The framework establishes a new paradigm for resource-efficient, cognitively aligned relevance modeling in industrial applications.",
        "translated": "电商平台搜索中的相关性建模仍面临术语匹配方法（如 BM25）存在语义鸿沟以及神经模型依赖领域特定硬样本稀缺性的挑战。本文提出 ADORE，一种自维持框架，融合三项创新：（1）规则感知的相关性判别模块，其中 Chain-of-Thought 大语言模型生成意图对齐的训练数据，并通过 Kahneman-Tversky 优化（KTO）进行精细化调整，以契合用户行为；（2）错误类型感知的数据合成模块，自动生成对抗样本来增强模型鲁棒性；（3）关键属性增强的知识蒸馏模块，将领域特定的属性层次结构注入可部署的学生模型中。ADORE 自动完成标注、对抗样本生成及蒸馏过程，在克服数据稀缺性的同时提升推理能力。大规模实验与线上 A/B 测试验证了 ADORE 的有效性。该框架为工业场景中资源高效且认知对齐的相关性建模建立了新范式。",
        "translated_title": "ADORE：面向电商领域的自主领域相关性引擎",
        "label": [
            "召回",
            "LLM生成式推荐"
        ],
        "label_reason": "聚焦电商搜索相关性建模，融合LLM与认知优化提升召回效果。",
        "relevance_score": 8,
        "novelty_score": 9,
        "novelty_reason": "首创LLM+KTO+对抗样本自动生成框架，显著提升工业级相关性建模效率。"
    },
    {
        "title": "AskNearby: An LLM-Based Application for Neighborhood Information Retrieval and Personalized Cognitive-Map Recommendations",
        "url": "http://arxiv.org/abs/2512.02502v1",
        "pub_date": "2025-12-02",
        "summary": "The \"15-minute city\" envisions neighborhoods where residents can meet daily needs via a short walk or bike ride. Realizing this vision requires not only physical proximity but also efficient and reliable access to information about nearby places, services, and events. Existing location-based systems, however, focus mainly on city-level tasks and neglect the spatial, temporal, and cognitive factors that shape localized decision-making. We conceptualize this gap as the Local Life Information Accessibility (LLIA) problem and introduce AskNearby, an AI-driven community application that unifies retrieval and recommendation within the 15-minute life circle. AskNearby integrates (i) a three-layer Retrieval-Augmented Generation (RAG) pipeline that synergizes graph-based, semantic-vector, and geographic retrieval with (ii) a cognitive-map model that encodes each user's neighborhood familiarity and preferences. Experiments on real-world community datasets demonstrate that AskNearby significantly outperforms LLM-based and map-based baselines in retrieval accuracy and recommendation quality, achieving robust performance in spatiotemporal grounding and cognitive-aware ranking. Real-world deployments further validate its effectiveness. By addressing the LLIA challenge, AskNearby empowers residents to more effectively discover local resources, plan daily activities, and engage in community life.",
        "translated": "“15分钟城市”构想了一个居民可通过短距离步行或骑行满足日常需求的社区环境。实现这一愿景不仅需要物理上的邻近性，还需要高效且可靠的本地场所、服务与活动信息获取能力。然而，现有的基于位置的服务系统主要聚焦于城市级任务，忽视了塑造本地化决策的空间、时间和认知因素。我们将此差距定义为“本地生活信息可及性”（Local Life Information Accessibility, LLIA）问题，并引入AskNearby——一款以人工智能驱动的社区应用，将检索与推荐统一整合于“15分钟生活圈”内。AskNearby集成了（i）一个三层结构的检索增强生成（Retrieval-Augmented Generation, RAG）流程，协同图结构、语义向量与地理检索技术；以及（ii）一个认知地图模型，用于编码每位用户对周边区域的熟悉度与偏好。在真实社区数据集上的实验表明，AskNearby在检索准确率和推荐质量上显著优于基于大语言模型（LLM）和地图系统的基线方法，展现出在时空锚定与认知感知排序方面的稳健性能。实际部署进一步验证了其有效性。通过应对LLIA挑战，AskNearby赋能居民更有效地发现本地资源、规划日常活动并融入社区生活。",
        "translated_title": "AskNearby：一种基于大语言模型的邻域信息检索与个性化认知地图推荐应用",
        "label": [
            "LLM生成式推荐",
            "重排",
            "通用推荐技术"
        ],
        "label_reason": "融合RAG与认知地图，面向本地化推荐场景",
        "relevance_score": 9,
        "novelty_score": 8,
        "novelty_reason": "创新性结合空间认知建模与LLM生成推荐"
    },
    {
        "title": "Q-BERT4Rec: Quantized Semantic-ID Representation Learning for Multimodal Recommendation",
        "url": "http://arxiv.org/abs/2512.02474v1",
        "pub_date": "2025-12-02",
        "summary": "Sequential recommendation plays a critical role in modern online platforms such as e-commerce, advertising, and content streaming, where accurately predicting users' next interactions is essential for personalization. Recent Transformer-based methods like BERT4Rec have shown strong modeling capability, yet they still rely on discrete item IDs that lack semantic meaning and ignore rich multimodal information (e.g., text and image). This leads to weak generalization and limited interpretability. To address these challenges, we propose Q-Bert4Rec, a multimodal sequential recommendation framework that unifies semantic representation and quantized modeling. Specifically, Q-Bert4Rec consists of three stages: (1) cross-modal semantic injection, which enriches randomly initialized ID embeddings through a dynamic transformer that fuses textual, visual, and structural features; (2) semantic quantization, which discretizes fused representations into meaningful tokens via residual vector quantization; and (3) multi-mask pretraining and fine-tuning, which leverage diverse masking strategies -- span, tail, and multi-region -- to improve sequential understanding. We validate our model on public Amazon benchmarks and demonstrate that Q-Bert4Rec significantly outperforms many strong existing methods, confirming the effectiveness of semantic tokenization for multimodal sequential recommendation. Our source code will be publicly available on GitHub after publishing.",
        "translated": "序列推荐在电商、广告和内容流媒体等现代在线平台中发挥着关键作用，准确预测用户下一次交互行为对于个性化至关重要。近期基于Transformer的方法（如BERT4Rec）展现了强大的建模能力，但仍依赖离散的物料ID，这些ID缺乏语义信息，并忽略了丰富的多模态信息（例如文本与图像），导致泛化能力较弱且可解释性有限。为应对上述挑战，我们提出Q-Bert4Rec，这是一种融合语义表示与量化建模的多模态序列推荐框架。具体而言，Q-Bert4Rec包含三个阶段：（1）跨模态语义注入，通过一个动态Transformer将文本、视觉与结构特征融合，从而丰富随机初始化的ID嵌入；（2）语义量化，利用残差向量量化将融合表征离散化为具有语义意义的Token；（3）多掩码预训练与微调，采用多样化的掩码策略——跨度掩码、尾部掩码及多区域掩码——以提升对序列模式的理解能力。我们在公开的Amazon数据集上验证了该模型的有效性，结果表明Q-Bert4Rec显著优于多数现有强方法，证实了语义Token化对多模态序列推荐的有效性。我们的源代码将在论文发表后于GitHub开源。",
        "translated_title": "Q-BERT4Rec：用于多模态推荐的量化语义ID表示学习",
        "label": [
            "序列推荐",
            "多模态推荐",
            "精排"
        ],
        "label_reason": "聚焦多模态序列推荐，改进语义建模与量化表示",
        "relevance_score": 9,
        "novelty_score": 8,
        "novelty_reason": "创新性融合跨模态语义注入与量化建模，提升泛化能力"
    },
    {
        "title": "WorldMM: Dynamic Multimodal Memory Agent for Long Video Reasoning",
        "url": "http://arxiv.org/abs/2512.02425v1",
        "pub_date": "2025-12-02",
        "summary": "Recent advances in video large language models have demonstrated strong capabilities in understanding short clips. However, scaling them to hours- or days-long videos remains highly challenging due to limited context capacity and the loss of critical visual details during abstraction. Existing memory-augmented methods mitigate this by leveraging textual summaries of video segments, yet they heavily rely on text and fail to utilize visual evidence when reasoning over complex scenes. Moreover, retrieving from fixed temporal scales further limits their flexibility in capturing events that span variable durations. To address this, we introduce WorldMM, a novel multimodal memory agent that constructs and retrieves from multiple complementary memories, encompassing both textual and visual representations. WorldMM comprises three types of memory: episodic memory indexes factual events across multiple temporal scales, semantic memory continuously updates high-level conceptual knowledge, and visual memory preserves detailed information about scenes. During inference, an adaptive retrieval agent iteratively selects the most relevant memory source and leverages multiple temporal granularities based on the query, continuing until it determines that sufficient information has been gathered. WorldMM significantly outperforms existing baselines across five long video question-answering benchmarks, achieving an average 8.4% performance gain over previous state-of-the-art methods, showing its effectiveness on long video reasoning.",
        "translated": "近年来，视频大语言模型在理解短视频片段方面展现出强大能力。然而，将其扩展至数小时甚至数天长的视频仍面临巨大挑战，原因在于其上下文容量有限，且在抽象过程中会丢失关键视觉细节。现有通过文本摘要增强记忆的方法虽能缓解此问题，但严重依赖文本信息，在推理复杂场景时无法有效利用视觉证据。此外，从固定时间粒度中检索进一步限制了其捕捉跨可变时长事件的灵活性。为解决上述问题，我们提出 WorldMM，一种新颖的多模态记忆代理，能够构建并从多种互补记忆中检索，涵盖文本与视觉表征。WorldMM 包含三种类型记忆：情节记忆在多个时间尺度上索引具体事件；语义记忆持续更新高层次概念知识；视觉记忆保留场景的详细信息。推理过程中，一个自适应检索代理根据查询内容迭代选择最相关的记忆来源，并依据需要灵活利用多种时间粒度，直至判定已收集到充分信息为止。WorldMM 在五个长视频问答基准测试中显著优于现有基线方法，相较之前最先进方法平均性能提升 8.4%，验证了其在长视频推理任务中的有效性。",
        "translated_title": "WorldMM：面向长视频推理的动态多模态记忆代理",
        "label": [
            "多模态推荐",
            "通用推荐技术"
        ],
        "label_reason": "论文聚焦长视频多模态记忆推理，非传统推荐系统问题。",
        "relevance_score": 4,
        "novelty_score": 7,
        "novelty_reason": "创新性构建多模态记忆机制，提升长视频理解能力。"
    }
]
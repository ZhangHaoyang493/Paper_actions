[
    {
        "title": "FinVet: A Collaborative Framework of RAG and External Fact-Checking\n  Agents for Financial Misinformation Detection",
        "url": "http://arxiv.org/abs/2510.11654v1",
        "pub_date": "2025-10-13",
        "summary": "Financial markets face growing threats from misinformation that can trigger billions in losses in minutes. Most existing approaches lack transparency in their decision-making and provide limited attribution to credible sources. We introduce FinVet, a novel multi-agent framework that integrates two Retrieval-Augmented Generation (RAG) pipelines with external fact-checking through a confidence-weighted voting mechanism. FinVet employs adaptive three-tier processing that dynamically adjusts verification strategies based on retrieval confidence, from direct metadata extraction to hybrid reasoning to full model-based analysis. Unlike existing methods, FinVet provides evidence-backed verdicts, source attribution, confidence scores, and explicit uncertainty flags when evidence is insufficient. Experimental evaluation on the FinFact dataset shows that FinVet achieves an F1 score of 0.85, which is a 10.4% improvement over the best individual pipeline (fact-check pipeline) and 37% improvement over standalone RAG approaches.",
        "translated": "金融市场正面临日益严重的虚假信息威胁，这些信息可能在几分钟内引发数十亿美元的损失。大多数现有方法在决策过程中缺乏透明度，并且对可信来源的归因有限。我们提出 FinVet，一个新颖的多智能体框架，通过置信度加权投票机制，将两个检索增强生成（Retrieval-Augmented Generation, RAG）流水线与外部事实核查相结合。FinVet 采用自适应的三级处理流程，根据检索置信度动态调整验证策略，从直接元数据提取，到混合推理，再到基于模型的完整分析。与现有方法不同，FinVet 在证据不足时提供基于证据的判定结果、来源归因、置信度评分以及明确的不确定性标记。在 FinFact 数据集上的实验评估表明，FinVet 达到 0.85 的 F1 分数，相比最优的单一流水线（事实核查流水线）提高了 10.4%，相比独立的 RAG 方法提高了 37%。"
    },
    {
        "title": "OneRec-Think: In-Text Reasoning for Generative Recommendation",
        "url": "http://arxiv.org/abs/2510.11639v1",
        "pub_date": "2025-10-13",
        "summary": "The powerful generative capacity of Large Language Models (LLMs) has instigated a paradigm shift in recommendation. However, existing generative models (e.g., OneRec) operate as implicit predictors, critically lacking the capacity for explicit and controllable reasoning-a key advantage of LLMs. To bridge this gap, we propose OneRec-Think, a unified framework that seamlessly integrates dialogue, reasoning, and personalized recommendation. OneRec-Think incorporates: (1) Itemic Alignment: cross-modal Item-Textual Alignment for semantic grounding; (2) Reasoning Activation: Reasoning Scaffolding to activate LLM reasoning within the recommendation context; and (3) Reasoning Enhancement, where we design a recommendation-specific reward function that accounts for the multi-validity nature of user preferences. Experiments across public benchmarks show state-of-the-art performance. Moreover, our proposed \"Think-Ahead\" architecture enables effective industrial deployment on Kuaishou, achieving a 0.159\\% gain in APP Stay Time and validating the practical efficacy of the model's explicit reasoning capability.",
        "translated": "大型语言模型（LLMs）强大的生成能力正在引发推荐系统领域的一场范式转变。然而，现有的生成模型（例如 OneRec）主要作为隐式预测器运行，严重缺乏显式可控推理的能力——而这正是 LLMs 的关键优势所在。为了解决这一问题，我们提出了 OneRec-Think，一个统一的框架，能够无缝融合对话、推理与个性化推荐。OneRec-Think 包含以下三个核心模块：（1）Itemic 对齐：跨模态的物品-文本对齐，以实现语义基础的构建；（2）推理激活：通过推理结构（Reasoning Scaffolding）在推荐场景中激活 LLM 的推理能力；以及（3）推理增强：我们设计了一个面向推荐任务的奖励函数，以应对用户偏好的多合理性（multi-validity）特性。在多个公开基准上的实验表明，该方法取得了最先进的性能。此外，我们提出的“Think-Ahead”架构在快手平台实现了有效的工业部署，使得 APP 使用时长提升了 0.159%，验证了模型显式推理能力的实用效果。"
    },
    {
        "title": "SemCSE-Multi: Multifaceted and Decodable Embeddings for Aspect-Specific\n  and Interpretable Scientific Domain Mapping",
        "url": "http://arxiv.org/abs/2510.11599v1",
        "pub_date": "2025-10-13",
        "summary": "We propose SemCSE-Multi, a novel unsupervised framework for generating multifaceted embeddings of scientific abstracts, evaluated in the domains of invasion biology and medicine. These embeddings capture distinct, individually specifiable aspects in isolation, thus enabling fine-grained and controllable similarity assessments as well as adaptive, user-driven visualizations of scientific domains. Our approach relies on an unsupervised procedure that produces aspect-specific summarizing sentences and trains embedding models to map semantically related summaries to nearby positions in the embedding space. We then distill these aspect-specific embedding capabilities into a unified embedding model that directly predicts multiple aspect embeddings from a scientific abstract in a single, efficient forward pass. In addition, we introduce an embedding decoding pipeline that decodes embeddings back into natural language descriptions of their associated aspects. Notably, we show that this decoding remains effective even for unoccupied regions in low-dimensional visualizations, thus offering vastly improved interpretability in user-centric settings.",
        "translated": "我们提出了一种名为 SemCSE-Multi 的新型无监督框架，用于生成科学摘要的多方面嵌入表示，并在入侵生物学和医学领域进行了评估。该嵌入能够独立捕捉多个明确且可单独指定的方面，从而支持细粒度、可控的相似性评估，以及自适应的、以用户驱动的科学领域可视化。我们的方法依赖于一个无监督过程，首先生成特定方面的总结性句子，并训练嵌入模型将语义相关的摘要映射到嵌入空间中的相近位置。随后，我们将这些特定方面的嵌入能力提炼到一个统一的嵌入模型中，使其能够在一个高效、单一的前向传播过程中直接从科学摘要中预测出多个方面嵌入。此外，我们还引入了一个嵌入解码流程，将嵌入表示还原为与各特定方面相关的自然语言描述。值得注意的是，我们证明了即使在低维可视化中未被占用的区域，该解码过程依然有效，从而在以用户为中心的应用场景中显著提升了可解释性。"
    },
    {
        "title": "REGENT: Relevance-Guided Attention for Entity-Aware Multi-Vector Neural\n  Re-Ranking",
        "url": "http://arxiv.org/abs/2510.11592v1",
        "pub_date": "2025-10-13",
        "summary": "Current neural re-rankers often struggle with complex information needs and long, content-rich documents. The fundamental issue is not computational--it is intelligent content selection: identifying what matters in lengthy, multi-faceted texts. While humans naturally anchor their understanding around key entities and concepts, neural models process text within rigid token windows, treating all interactions as equally important and missing critical semantic signals. We introduce REGENT, a neural re-ranking model that mimics human-like understanding by using entities as a \"semantic skeleton\" to guide attention. REGENT integrates relevance guidance directly into the attention mechanism, combining fine-grained lexical matching with high-level semantic reasoning. This relevance-guided attention enables the model to focus on conceptually important content while maintaining sensitivity to precise term matches. REGENT achieves new state-of-the-art performance in three challenging datasets, providing up to 108% improvement over BM25 and consistently outperforming strong baselines including ColBERT and RankVicuna. To our knowledge, this is the first work to successfully integrate entity semantics directly into neural attention, establishing a new paradigm for entity-aware information retrieval.",
        "translated": "目前的神经重排序模型在处理复杂的信息需求和长篇、内容丰富文档时常常面临挑战。其根本问题并不在于计算能力，而在于智能内容选择：识别长篇、多维文本中真正重要的信息。尽管人类在理解文本时自然地围绕关键实体和概念进行定位，但神经模型则受限于固定的词元窗口对文本进行处理，将所有交互视为同等重要，从而忽略了关键的语义信号。我们提出REGENT，这是一种神经重排序模型，通过使用实体作为“语义骨架”来引导注意力，从而模拟人类的理解方式。REGENT将相关性引导直接整合到注意力机制中，将细粒度的词汇匹配与高层次的语义推理相结合。这种相关性引导的注意力机制使模型能够聚焦于概念上重要的内容，同时保持对精确术语匹配的敏感性。REGENT在三个具有挑战性的数据集上达到了新的最先进性能，在BM25基础上最多提升了108%，并且始终优于ColBERT和RankVicuna等强基线模型。据我们所知，这是首次成功将实体语义直接整合到神经注意力机制中的工作，为具备实体感知能力的信息检索建立了一个新的范式。"
    },
    {
        "title": "QDER: Query-Specific Document and Entity Representations for\n  Multi-Vector Document Re-Ranking",
        "url": "http://arxiv.org/abs/2510.11589v1",
        "pub_date": "2025-10-13",
        "summary": "Neural IR has advanced through two distinct paths: entity-oriented approaches leveraging knowledge graphs and multi-vector models capturing fine-grained semantics. We introduce QDER, a neural re-ranking model that unifies these approaches by integrating knowledge graph semantics into a multi-vector model. QDER's key innovation lies in its modeling of query-document relationships: rather than computing similarity scores on aggregated embeddings, we maintain individual token and entity representations throughout the ranking process, performing aggregation only at the final scoring stage - an approach we call \"late aggregation.\" We first transform these fine-grained representations through learned attention patterns, then apply carefully chosen mathematical operations for precise matches. Experiments across five standard benchmarks show that QDER achieves significant performance gains, with improvements of 36% in nDCG@20 over the strongest baseline on TREC Robust 2004 and similar improvements on other datasets. QDER particularly excels on difficult queries, achieving an nDCG@20 of 0.70 where traditional approaches fail completely (nDCG@20 = 0.0), setting a foundation for future work in entity-aware retrieval.",
        "translated": "神经信息检索（Neural IR）的发展经历了两条不同的路径：一种是面向实体的方法，利用知识图谱信息；另一种是多向量模型，用于捕捉细粒度语义。我们提出 QDER，这是一种神经重排序模型，通过将知识图谱语义整合到多向量模型中，实现了这两条路径的统一。QDER 的关键创新在于其对查询与文档关系的建模：与在聚合嵌入上计算相似度分数的传统方法不同，我们在整个排序过程中保留每个词元（token）和实体的独立表示，仅在最终的评分阶段进行聚合——我们称这种策略为“晚期聚合”（late aggregation）。首先，我们通过学习得到的注意力模式对这些细粒度表示进行转换，然后应用精心选择的数学运算以实现精确匹配。在五个标准基准数据集上的实验表明，QDER 显著提升了性能，在 TREC Robust 2004 数据集上相比最强基线模型的 nDCG@20 提高了 36%，在其他数据集上也有类似提升。尤其在处理困难查询时，QDER 表现出色，取得了 0.70 的 nDCG@20 评分，而传统方法在此类查询上完全失效（nDCG@20 = 0.0），为未来实体感知（entity-aware）检索的研究奠定了基础。"
    },
    {
        "title": "Characterizing Web Search in The Age of Generative AI",
        "url": "http://arxiv.org/abs/2510.11560v1",
        "pub_date": "2025-10-13",
        "summary": "The advent of LLMs has given rise to a new type of web search: Generative search, where LLMs retrieve web pages related to a query and generate a single, coherent text as a response. This output modality stands in stark contrast to traditional web search, where results are returned as a ranked list of independent web pages. In this paper, we ask: Along what dimensions do generative search outputs differ from traditional web search? We compare Google, a traditional web search engine, with four generative search engines from two providers (Google and OpenAI) across queries from four domains. Our analysis reveals intriguing differences. Most generative search engines cover a wider range of sources compared to web search. Generative search engines vary in the degree to which they rely on internal knowledge contained within the model parameters v.s. external knowledge retrieved from the web. Generative search engines surface varying sets of concepts, creating new opportunities for enhancing search diversity and serendipity. Our results also highlight the need for revisiting evaluation criteria for web search in the age of Generative AI.",
        "translated": "大语言模型（LLMs）的出现催生了一种新的网络搜索方式：生成式搜索（generative search），在这种搜索方式中，LLMs会检索与查询相关的网页，并生成一段连贯统一的文本作为响应。这种输出形式与传统网络搜索形成了鲜明对比，后者返回的是一个按相关性排序的独立网页列表。在本文中，我们提出以下问题：生成式搜索的输出在哪些维度上与传统网络搜索有所不同？我们从四个领域中选取查询，比较了传统网络搜索引擎Google与来自两家提供商（Google和OpenAI）的四个生成式搜索引擎的表现。我们的分析揭示了一些有趣的差异。大多数生成式搜索引擎相比传统网络搜索，能够涵盖更广泛的来源。生成式搜索引擎在依赖模型参数中包含的内部知识与从网络检索的外部知识的程度上存在差异。此外，生成式搜索引擎展示的概念集合各不相同，从而为提升搜索多样性和偶然性提供了新的可能性。我们的结果还强调，在生成式人工智能时代，有必要重新审视网络搜索的评估标准。"
    },
    {
        "title": "Uncertainty Quantification for Retrieval-Augmented Reasoning",
        "url": "http://arxiv.org/abs/2510.11483v1",
        "pub_date": "2025-10-13",
        "summary": "Retrieval-augmented reasoning (RAR) is a recent evolution of retrieval-augmented generation (RAG) that employs multiple reasoning steps for retrieval and generation. While effective for some complex queries, RAR remains vulnerable to errors and misleading outputs. Uncertainty quantification (UQ) offers methods to estimate the confidence of systems' outputs. These methods, however, often handle simple queries with no retrieval or single-step retrieval, without properly handling RAR setup. Accurate estimation of UQ for RAR requires accounting for all sources of uncertainty, including those arising from retrieval and generation. In this paper, we account for all these sources and introduce Retrieval-Augmented Reasoning Consistency (R2C)--a novel UQ method for RAR. The core idea of R2C is to perturb the multi-step reasoning process by applying various actions to reasoning steps. These perturbations alter the retriever's input, which shifts its output and consequently modifies the generator's input at the next step. Through this iterative feedback loop, the retriever and generator continuously reshape one another's inputs, enabling us to capture uncertainty arising from both components. Experiments on five popular RAR systems across diverse QA datasets show that R2C improves AUROC by over 5% on average compared to the state-of-the-art UQ baselines. Extrinsic evaluations using R2C as an external signal further confirm its effectiveness for two downstream tasks: in Abstention, it achieves ~5% gains in both F1Abstain and AccAbstain; in Model Selection, it improves the exact match by ~7% over single models and ~3% over selection methods.",
        "translated": "检索增强推理（Retrieval-Augmented Reasoning, RAR）是检索增强生成（Retrieval-Augmented Generation, RAG）的最新演进，其通过在检索与生成过程中引入多步推理来提高效果。尽管在处理某些复杂查询时表现出色，RAR 仍然容易受到错误和误导性输出的影响。不确定性量化（Uncertainty Quantification, UQ）提供了一种估计系统输出置信度的方法。然而，现有方法大多针对无检索或单步检索的简单查询，未能有效应对 RAR 的设置。对 RAR 的 UQ 进行准确估计，需要综合考虑所有可能的不确定性来源，包括检索和生成过程中的不确定性。在本文中，我们系统地考虑了这些不确定性来源，并提出了检索增强推理一致性（Retrieval-Augmented Reasoning Consistency, R2C）——一种新型的 UQ 方法。R2C 的核心思想是通过对推理步骤施加多种操作，扰动多步推理过程。这些扰动会改变检索器的输入，从而影响其输出，并进一步修改生成器在下一步的输入。通过这一迭代反馈机制，检索器和生成器不断重塑彼此的输入，使我们能够捕捉来自两个组件的不确定性。在五个主流 RAR 系统和多个问答数据集上的实验表明，与最先进的 UQ 基线方法相比，R2C 在平均 AUROC 指标上提升了超过 5%。使用 R2C 作为外部信号进行的外在评估进一步验证了其有效性，针对两个下游任务：在拒绝回答（Abstention）任务中，R2C 在 F1Abstain 和 AccAbstain 指标上分别提升了约 5%；在模型选择（Model Selection）任务中，R2C 相比单模型提升了约 7% 的精确匹配（exact match），相比其他选择方法提升了约 3%。"
    },
    {
        "title": "What Generative Search Engines Like and How to Optimize Web Content\n  Cooperatively",
        "url": "http://arxiv.org/abs/2510.11438v1",
        "pub_date": "2025-10-13",
        "summary": "By employing large language models (LLMs) to retrieve documents and generate natural language responses, Generative Engines, such as Google AI overview and ChatGPT, provide significantly enhanced user experiences and have rapidly become the new form of search. Their rapid adoption also drives the needs of Generative Engine Optimization (GEO), as content providers are eager to gain more traction from them. In this paper, we introduce AutoGEO, a framework to automatically learn generative engine preferences when using retrieved contents for response generation, and rewrite web contents for more such traction. AutoGEO first prompts frontier LLMs to explain generative engine preferences and extract meaningful preference rules from these explanations. Then it uses preference rules as context engineering for AutoGEO$_\\text{API}$, a prompt-based GEO system, and as rule-based rewards to train AutoGEO$_\\text{Mini}$, a cost-effective GEO model. Experiments on the standard GEO-Bench and two newly constructed benchmarks using real user queries demonstrate the effectiveness of AutoGEO in enhancing content traction while preserving search utility. Analyses confirm the learned rules' robustness and abilities to capture unique preferences in variant domains, and AutoGEO systems' ability to embed them in content optimization. The code is released at https://github.com/cxcscmu/AutoGEO.",
        "translated": "通过使用大语言模型（LLMs）来检索文档并生成自然语言响应，生成式引擎（如 Google AI 概述和 ChatGPT）显著提升了用户体验，并迅速成为搜索的新形态。其迅速普及也推动了生成式引擎优化（Generative Engine Optimization, GEO）的需求，因为内容提供者希望从这些系统中获得更多曝光。在本文中，我们提出了 AutoGEO，一个在使用检索内容进行响应生成时，能够自动学习生成式引擎偏好的框架，并对网页内容进行重写以提高其曝光度。AutoGEO 首先提示前沿的大语言模型解释生成式引擎的偏好，并从这些解释中提取有意义的偏好规则。接着，它将这些偏好规则用于 AutoGEO$_\\text{API}$——一个基于提示的 GEO 系统——作为上下文工程的输入，并将规则作为奖励机制，用于训练 AutoGEO$_\\text{Mini}$——一个经济高效的 GEO 模型。在标准的 GEO-Bench 基准以及基于真实用户查询构建的两个新基准上的实验表明，AutoGEO 在提升内容曝光度的同时能够保持搜索效用的有效性。分析结果进一步验证了所学习规则的鲁棒性及其在不同领域中捕捉独特偏好的能力，并确认了 AutoGEO 系统在内容优化中嵌入这些规则的能力。代码已发布在 https://github.com/cxcscmu/AutoGEO。"
    },
    {
        "title": "On Inherited Popularity Bias in Cold-Start Item Recommendation",
        "url": "http://arxiv.org/abs/2510.11402v1",
        "pub_date": "2025-10-13",
        "summary": "Collaborative filtering (CF) recommender systems struggle with making predictions on unseen, or 'cold', items. Systems designed to address this challenge are often trained with supervision from warm CF models in order to leverage collaborative and content information from the available interaction data. However, since they learn to replicate the behavior of CF methods, cold-start models may therefore also learn to imitate their predictive biases. In this paper, we show that cold-start systems can inherit popularity bias, a common cause of recommender system unfairness arising when CF models overfit to more popular items, thereby maximizing user-oriented accuracy but neglecting rarer items. We demonstrate that cold-start recommenders not only mirror the popularity biases of warm models, but are in fact affected more severely: because they cannot infer popularity from interaction data, they instead attempt to estimate it based solely on content features. This leads to significant over-prediction of certain cold items with similar content to popular warm items, even if their ground truth popularity is very low. Through experiments on three multimedia datasets, we analyze the impact of this behavior on three generative cold-start methods. We then describe a simple post-processing bias mitigation method that, by using embedding magnitude as a proxy for predicted popularity, can produce more balanced recommendations with limited harm to user-oriented cold-start accuracy.",
        "translated": "协同过滤（Collaborative Filtering, CF）推荐系统在对未见过的或“冷启动”的物品进行预测时面临挑战。为了解决这一问题，设计用于冷启动场景的系统通常会借助“热启动”CF模型进行监督训练，以利用现有的交互数据中的协作信息和内容信息。然而，由于这些系统学习的是复制CF方法的行为，因此冷启动模型也可能学会模仿其预测偏差。本文中，我们表明冷启动系统可能会继承“流行度偏差”（popularity bias），这是推荐系统不公平性的常见原因，当CF模型过度拟合更受欢迎的物品时就会产生这种偏差，从而在最大化面向用户准确率的同时忽略了较少出现的物品。我们通过实验发现，冷启动推荐器不仅复制了热模型的流行度偏差，而且实际上受到更严重的影响：由于它们无法从交互数据中推断出流行度，因此转而尝试仅基于内容特征来估计流行度。这导致某些与热门热启动物品在内容上相似的冷启动物品被显著高估，即使它们的真实流行度非常低。我们在三个多媒体数据集上分析了这一行为对三种生成式冷启动方法的影响。随后，我们介绍了一种简单的后处理偏差缓解方法，该方法通过将嵌入（embedding）模长作为预测流行度的代理指标，能够在有限地影响面向用户冷启动准确率的前提下，生成更加平衡的推荐结果。"
    },
    {
        "title": "VeriCite: Towards Reliable Citations in Retrieval-Augmented Generation\n  via Rigorous Verification",
        "url": "http://arxiv.org/abs/2510.11394v1",
        "pub_date": "2025-10-13",
        "summary": "Retrieval-Augmented Generation (RAG) has emerged as a crucial approach for enhancing the responses of large language models (LLMs) with external knowledge sources. Despite the impressive performance in complex question-answering tasks, RAG still struggles with hallucinations. Attributing RAG-generated content through in-line citations has demonstrated potential in reducing hallucinations and facilitating human verification. Existing citation generation methods primarily rely on either fine-tuning the generator or employing post-processing approaches for citation matching. However, the former approach demands substantial annotated data and computational resources, while the latter often encounters difficulties in managing multiple citations and frequently produces suboptimal results. In this paper, we introduce a novel framework, called VeriCite, designed to rigorously validate supporting evidence and enhance answer attribution. Specifically, VeriCite breaks down into a three-stage generation: 1) The initial answer generation first generates a response based on all available contexts and has its claims verified through the NLI model; 2) the supporting evidence selection assesses the utility of each document and extracts useful supporting evidences; 3) the final answer refinement integrates the initial response and collected evidences to produce the final, refined answer.We conduct experiments across five open-source LLMs and four datasets, demonstrating that VeriCite can significantly improve citation quality while maintaining the correctness of the answers.",
        "translated": "检索增强生成（Retrieval-Augmented Generation, RAG）已成为一种关键方法，用于通过外部知识源增强大语言模型（Large Language Models, LLMs）的响应能力。尽管RAG在复杂问答任务中表现出色，但其仍面临幻觉（hallucination）问题。通过行内引用（in-line citations）对RAG生成的内容进行归因，已被证明在减少幻觉和便于人工验证方面具有潜力。现有的引用生成方法主要依赖于对生成器的微调，或采用后处理方法进行引用匹配。然而，前者需要大量标注数据和计算资源，而后者在处理多个引用时常常遇到困难，且结果往往不够理想。在本文中，我们提出了一种新颖的框架，命名为VeriCite，旨在严格验证支持证据并提升答案归因能力。具体而言，VeriCite分为三个生成阶段：1）初始答案生成阶段基于所有可用上下文生成初步回答，并通过自然语言推理模型（NLI model）对其主张进行验证；2）支持证据选择阶段评估每个文档的有用性，并提取有效的支持证据；3）最终答案优化阶段整合初始回答与收集到的证据，生成最终的、经过优化的答案。我们在五种开源大语言模型和四个数据集上进行了实验，结果表明，VeriCite在保持答案正确性的同时，能够显著提升引用质量。"
    },
    {
        "title": "LLM-Specific Utility: A New Perspective for Retrieval-Augmented\n  Generation",
        "url": "http://arxiv.org/abs/2510.11358v1",
        "pub_date": "2025-10-13",
        "summary": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by incorporating external knowledge. While traditional retrieval focuses on relevance, RAG's effectiveness depends on the utility of retrieved passages, i.e., the usefulness in facilitating the generation of an accurate and comprehensive answer. Existing studies often treat utility as a generic attribute, ignoring the fact that different LLMs may benefit differently from the same passage due to variations in internal knowledge and comprehension ability. In this work, we introduce and systematically investigate the notion of LLM-specific utility. Through large-scale experiments across multiple datasets and LLMs, we demonstrate that human-annotated passages are not optimal for LLMs and that ground-truth utilitarian passages are not transferable across different LLMs. These findings highlight the necessity of adopting the LLM-specific utility in RAG research. Our findings indicate that some human-annotated passages are not ground-truth utilitarian passages for specific LLMs, partially due to the varying readability of queries and passages for LLMs, a tendency for which perplexity is a key metric. Based on these findings, we propose a benchmarking procedure for LLM-specific utility judgments. We evaluate existing utility judgment methods on six datasets and find that while verbalized methods using pseudo-answers perform robustly, LLMs struggle to assess utility effectively-failing to reject all passages for known queries and to select truly useful ones for unknown queries.",
        "translated": "检索增强生成（Retrieval-augmented Generation, RAG）通过引入外部知识来增强大语言模型（Large Language Models, LLMs）的能力。尽管传统检索方法主要关注相关性（relevance），但 RAG 的有效性依赖于检索到的段落的**效用性**（utility），即这些段落在促进生成准确且全面答案方面的有用程度。现有研究通常将效用性视为一种通用属性，忽视了由于不同 LLM 在内部知识和理解能力上的差异，它们可能从相同的段落中获得不同的收益。在本工作中，我们引入并系统地探讨了**面向特定 LLM 的效用性**（LLM-specific utility）这一概念。通过在多个数据集和 LLM 上的大规模实验，我们证明：人工标注的段落对 LLM 来说并非最优选择，并且真实效用性段落在不同 LLM 之间不可迁移。这些发现凸显了在 RAG 研究中采用 LLM-specific utility 的必要性。我们的研究表明，某些人工标注的段落对特定 LLM 而言并不是真实效用性段落，部分原因是 LLM 对查询和段落的可读性存在差异，而这种差异可以通过困惑度（perplexity）这一关键指标来衡量。基于上述发现，我们提出了一种面向 LLM-specific utility 的基准评估方法。我们对六种数据集上的现有效用性判断方法进行了评估，发现尽管使用伪答案（pseudo-answers）的显式化方法表现较为稳健，但 LLM 在评估段落效用性方面仍存在困难，无法拒绝所有已知查询的段落，也无法为未知查询有效挑选真正有用的段落。"
    },
    {
        "title": "Dynamic Network-Based Two-Stage Time Series Forecasting for Affiliate\n  Marketing",
        "url": "http://arxiv.org/abs/2510.11323v1",
        "pub_date": "2025-10-13",
        "summary": "In recent years, affiliate marketing has emerged as a revenue-sharing strategy where merchants collaborate with promoters to promote their products. It not only increases product exposure but also allows promoters to earn a commission. This paper addresses the pivotal yet under-explored challenge in affiliate marketing: accurately assessing and predicting the contributions of promoters in product promotion. We design a novel metric for evaluating the indirect contributions of the promoter, called propagation scale. Unfortunately, existing time series forecasting techniques fail to deliver accurate predictions due to the propagation scale being influenced by multiple factors and the inherent complexities arising from dynamic scenarios. To address this issue, we decouple the network structure from the node signals and propose a two-stage solution: initially, the basic self-sales and network structure prediction are conducted separately, followed by the synthesis of the propagation scale. Specifically, we design a graph convolution encoding scheme based on descendant neighbors and incorporate hypergraph convolution to efficiently capture complex promotional dynamics. Additionally, three auxiliary tasks are employed: self-sales prediction for base estimations, descendant prediction to synthesize propagation scale, and promoter activation prediction to mitigate high volatility issues. Extensive offline experiments on large-scale industrial datasets validate the superiority of our method. We further deploy our model on Alimama platform with over $100,000$ promoters, achieving a $9.29\\%$ improvement in GMV and a $5.89\\%$ increase in sales volume.",
        "translated": "近年来，联盟营销已成为一种收益共享策略，商家与推广者合作以推广其商品。这种模式不仅提高了商品的曝光度，还使推广者能够获得佣金。本文关注联盟营销中一个关键但尚未充分研究的问题：**准确评估和预测推广者在商品推广中的贡献**。我们设计了一种用于衡量推广者间接贡献的新指标，称为**传播规模（propagation scale）**。然而，由于传播规模受到多种因素影响，且动态场景中存在内在复杂性，现有的时间序列预测技术难以实现准确的预测。为了解决这一问题，我们**将网络结构与节点信号解耦**，并提出了一种两阶段的解决方案：首先，分别预测基本的自主销售与网络结构；随后，合成传播规模。具体而言，我们设计了一种基于后代邻居的图卷积编码方案，并引入超图卷积以高效捕捉复杂的推广动态。此外，我们还引入了三个辅助任务：用于基础估计的自主销售预测、用于合成传播规模的后代节点预测，以及用于缓解高波动性问题的推广者激活预测。我们在大规模工业数据集上进行了广泛的离线实验，验证了我们方法的优越性。我们进一步将模型部署在拥有超过100,000名推广者的Alimama平台，实现了GMV提升9.29%，销售量增长5.89%。"
    },
    {
        "title": "Next Interest Flow: A Generative Pre-training Paradigm for Recommender\n  Systems by Modeling All-domain Movelines",
        "url": "http://arxiv.org/abs/2510.11317v1",
        "pub_date": "2025-10-13",
        "summary": "Click-Through Rate (CTR) prediction, a cornerstone of modern recommender systems, has been dominated by discriminative models that react to past user behavior rather than proactively modeling user intent. Existing generative paradigms attempt to address this but suffer from critical limitations: Large Language Model (LLM) based methods create a semantic mismatch by forcing e-commerce signals into a linguistic space, while ID-based generation is constrained by item memorization and cold-start issues. To overcome these limitations, we propose a novel generative pre-training paradigm. Our model learns to predict the Next Interest Flow, a dense vector sequence representing a user's future intent, while simultaneously modeling its internal Interest Diversity and Interest Evolution Velocity to ensure the representation is both rich and coherent. However, this two-stage approach introduces a critical objective mismatch between the generative and discriminative stages. We resolve this via a bidirectional alignment strategy, which harmonizes the two stages through cross-stage weight initialization and a dynamic Semantic Alignment Module for fine-tuning. Additionally, we enhance the underlying discriminative model with a Temporal Sequential Pairwise (TSP) mechanism to better capture temporal causality. We present the All-domain Moveline Evolution Network (AMEN), a unified framework implementing our entire pipeline. Extensive offline experiments validate AMEN's superiority over strong baselines, and a large-scale online A/B test demonstrates its significant real-world impact, delivering substantial improvements in key business metrics.",
        "translated": "点击率（CTR）预测是现代推荐系统中的核心任务之一，目前主要依赖于判别式模型，这些模型侧重于响应用户过去的行为，而未能主动建模用户的意图。现有的生成式范式尝试解决这一问题，但存在关键的局限性：基于大语言模型（LLM）的方法将电商信号强行映射到语言语义空间，导致语义错配；而基于ID的生成方法则受到物品记忆能力和冷启动问题的限制。为克服这些限制，我们提出了一种新颖的生成式预训练范式。我们的模型旨在预测“下一个兴趣流”（Next Interest Flow），即一个稠密向量序列，用于表示用户未来的兴趣意图。同时，该模型还建模内部的兴趣多样性（Interest Diversity）和兴趣演化速度（Interest Evolution Velocity），以确保表示的丰富性与一致性。然而，这种两阶段的方法在生成阶段与判别阶段之间引入了关键的目标不匹配问题。我们通过双向对齐策略加以解决，该策略通过跨阶段权重初始化和动态语义对齐模块（Semantic Alignment Module）进行微调，从而协调两个阶段之间的差异。此外，我们通过引入时间序列成对机制（Temporal Sequential Pairwise, TSP）来增强底层判别模型，以更好地捕捉时间因果关系。我们提出了一个统一的框架——全领域兴趣演化网络（All-domain Moveline Evolution Network, AMEN），实现了我们完整的流水线。大量离线实验验证了AMEN在强基线模型上的优越性能，而大规模在线A/B测试也展示了其在现实场景中的显著影响，显著提升了关键业务指标。"
    },
    {
        "title": "ELMO: Efficiency via Low-precision and Peak Memory Optimization in Large\n  Output Spaces",
        "url": "http://arxiv.org/abs/2510.11168v1",
        "pub_date": "2025-10-13",
        "summary": "Large output spaces, also referred to as Extreme multilabel classification (XMC), is a setting that arises, e.g., in large-scale tagging and product-to-product recommendation, and is characterized by the number of labels ranging from hundreds of thousands to millions. This means that the linear classification head, usually only a tiny fraction of the overall model, turns into the main driver for compute and memory demand. Current state-of-the-art XMC methods predominantly rely on FP16-FP32 mixed-precision training, which we show can be unstable, and inefficient in terms of memory usage and computational overhead. Meanwhile, existing low-precision methods typically retain higher precision for the classification layer. In this work, we propose ELMO, a pure low-precision training framework for XMC models using BFloat16 and Float8 data types. By leveraging Kahan summation and stochastic rounding, we demonstrate that XMC models can be effectively trained entirely in Float8, without relying on single-precision master weights or tensor scaling. Low-precision training, combined with our proposed memory optimizations -- gradient fusion and chunking -- enables significant reductions in GPU memory usage. For example, we train a 3-million-label XMC model with only 6.6 GiB of GPU memory, compared to the 39.7 GiB required by the optimized SOTA method, Renee without compromising accuracy.",
        "translated": "大规模输出空间，也称为极端多标签分类（Extreme multilabel classification, XMC），是一种在大规模标签分配和商品到商品推荐等场景中常见的设定，其特点是标签数量可高达数十万到数百万级。这意味着线性分类头（通常在整个模型中仅占极小部分）变成了计算和内存需求的主要驱动因素。当前最先进的XMC方法主要依赖FP16与FP32混合精度训练，但我们发现这种方法在训练稳定性、内存使用效率以及计算开销方面存在不足。同时，现有的低精度训练方法通常仍为分类层保留较高精度。在本文中，我们提出ELMO，一个完全基于BFloat16和Float8数据类型的低精度训练框架。通过引入Kahan求和和随机舍入技术，我们证明XMC模型可以完全在Float8精度下进行有效训练，而无需依赖单精度主权重或张量缩放。结合我们提出的内存优化方法——梯度融合和块处理（chunking），该框架能够显著减少GPU内存的使用。例如，我们在仅使用6.6 GiB GPU内存的情况下训练了一个包含300万个标签的XMC模型，而优化后的SOTA方法Renee则需要39.7 GiB的内存，且在不损失精度的前提下实现了这一目标。"
    },
    {
        "title": "DyKnow-RAG: Dynamic Knowledge Utilization Reinforcement Framework for\n  Noisy Retrieval-Augmented Generation in E-commerce Search Relevance",
        "url": "http://arxiv.org/abs/2510.11122v1",
        "pub_date": "2025-10-13",
        "summary": "Accurately modeling query-item relevance drives e-commerce ranking, yet long-tail, knowledge-heavy, and fast-evolving queries exceed parametric LLM coverage. External context (reviews, attribute encyclopedias, UGC) can help but is noisy, and single-pass latency and cost forbid any clean-then-summarize step. The model must, per query, judge relevance and decide whether to use, partially use, or ignore the context. DyKnow-RAG is a dynamic noisy-RAG framework built on Group Relative Policy Optimization. It trains two rollout groups (no external context vs a single retrieved chunk) and applies posterior-driven inter-group advantage scaling that adaptively reweights their contributions by the per-query correctness gap. This teaches when to trust retrieval versus fall back to parametric knowledge, without process labels, value networks, or extra inference passes, preserving single-pass, single-chunk deployment under production latency. Training combines: (1) supervised initialization with a structured rationale that explicitly records the context-usage decision; (2) an RL pool prioritized by SFT uncertainty to focus where context choice is most consequential; and (3) an optional lightweight DPO warm start to stabilize with-context calibration. Under a unified retrieval/index and fixed latency budget, DyKnow-RAG outperforms SFT, DPO, and vanilla GRPO in offline tests, and delivers consistent lifts on GSB, Query Goodrate, and Item Goodrate in Taobao A/B testing. It is deployed in Taobao's production relevance system, serving live traffic. To our knowledge, it is among the first single-pass RAG solutions for e-commerce relevance, turning noisy external signals into reliable gains without added online complexity.",
        "translated": "准确地建模查询与商品的相关性对于电商排序至关重要，然而长尾、知识密集型和快速变化的查询超出了参数化大语言模型的覆盖范围。外部上下文（如商品评论、属性百科、用户生成内容）虽然可以提供帮助，但往往包含噪声，且单次推理的延迟和成本限制了任何“清理再摘要”的步骤。因此，模型必须在每次查询中判断相关性，并决定是否使用、部分使用或忽略外部上下文。\n\nDyKnow-RAG 是一种基于组相对策略优化（Group Relative Policy Optimization）的动态噪声-RAG框架。该框架通过训练两个 rollout 组（一组不使用外部上下文，另一组使用一个检索到的 chunk）并采用后验驱动的组间优势缩放方法，以查询间的准确性差距自适应地重新加权两组的贡献。这种方法能够在无需过程标签、价值网络或额外推理步骤的前提下，学习何时信任检索结果，何时回退至参数化知识，从而在生产延迟下保持单次推理、单 chunk 的部署效率。\n\nDyKnow-RAG 的训练结合了以下三个阶段：（1）结构化理由监督初始化，显式记录上下文使用决策；（2）以监督微调（SFT）的不确定性为优先级的强化学习（RL）池，集中于上下文选择最具影响的场景；（3）一个可选的轻量级 DPO 预训练阶段，用于稳定上下文相关校准。\n\n在统一的检索/索引设置和固定的延迟预算下，DyKnow-RAG 在离线测试中优于 SFT、DPO 和标准 GRPO。在淘宝的 A/B 测试中，该方法在 GSB、Query Goodrate 和 Item Goodrate 等指标上均实现了持续的提升。目前，DyKnow-RAG 已部署于淘宝的生产相关性系统中，服务实时流量。据我们所知，这是首批针对电商相关性问题的单次推理 RAG 解决方案之一，能够在不增加在线复杂度的前提下，将噪声外部信号转化为可靠收益。"
    },
    {
        "title": "Zero-Shot CFC: Fast Real-World Image Denoising based on Cross-Frequency\n  Consistency",
        "url": "http://arxiv.org/abs/2510.12646v1",
        "pub_date": "2025-10-14",
        "summary": "Zero-shot denoisers address the dataset dependency of deep-learning-based denoisers, enabling the denoising of unseen single images. Nonetheless, existing zero-shot methods suffer from long training times and rely on the assumption of noise independence and a zero-mean property, limiting their effectiveness in real-world denoising scenarios where noise characteristics are more complicated. This paper proposes an efficient and effective method for real-world denoising, the Zero-Shot denoiser based on Cross-Frequency Consistency (ZSCFC), which enables training and denoising with a single noisy image and does not rely on assumptions about noise distribution. Specifically, image textures exhibit position similarity and content consistency across different frequency bands, while noise does not. Based on this property, we developed cross-frequency consistency loss and an ultralight network to realize image denoising. Experiments on various real-world image datasets demonstrate that our ZSCFC outperforms other state-of-the-art zero-shot methods in terms of computational efficiency and denoising performance.",
        "translated": "零样本去噪器解决了基于深度学习的去噪器对数据集的依赖问题，使其能够对未见过的单张图像进行去噪。然而，现有的零样本方法训练时间较长，并且依赖于噪声独立性和零均值的假设，这在现实世界中噪声特性更为复杂的情况下限制了其去噪效果。本文提出了一种高效且有效的现实场景去噪方法——基于跨频段一致性的零样本去噪器（Zero-Shot Denoiser based on Cross-Frequency Consistency, ZSCFC），该方法仅需单张含噪图像即可完成训练与去噪，且不依赖于对噪声分布的假设。具体而言，图像的纹理在不同频段中表现出位置相似性和内容一致性，而噪声则不具备这一特性。基于该特性，我们设计了跨频段一致性损失函数，并构建了一个超轻量级网络以实现图像去噪。在多个现实世界图像数据集上的实验表明，与其它最先进的零样本方法相比，ZSCFC在计算效率和去噪性能方面均表现出色。"
    },
    {
        "title": "Normalization-equivariant Diffusion Models: Learning Posterior Samplers\n  From Noisy And Partial Measurements",
        "url": "http://arxiv.org/abs/2510.11964v1",
        "pub_date": "2025-10-13",
        "summary": "Diffusion models (DMs) have rapidly emerged as a powerful framework for image generation and restoration. However, existing DMs are primarily trained in a supervised manner by using a large corpus of clean images. This reliance on clean data poses fundamental challenges in many real-world scenarios, where acquiring noise-free data is hard or infeasible, and only noisy and potentially incomplete measurements are available. While some methods can train DMs using noisy data, they are generally effective only when the amount of noise is very mild or when some additional noise-free data is available. In addition, existing methods for training DMs from incomplete measurements require access to multiple complementary acquisition processes, an assumption that poses a significant practical limitation. Here we introduce the first approach for learning DMs for image restoration using only noisy measurement data from a single operator. As a first key contribution, we show that DMs, and more broadly minimum mean squared error denoisers, exhibit a weak form of scale equivariance linking rescaling in signal amplitude to changes in noise intensity. We then leverage this theoretical insight to develop a denoising score-matching strategy that generalizes robustly to noise levels lower than those present in the training data, thereby enabling the learning of DMs from noisy measurements. To further address the challenges of incomplete and noisy data, we integrate our method with equivariant imaging, a complementary self-supervised learning framework that exploits the inherent invariants of imaging problems, to train DMs for image restoration from single-operator measurements that are both incomplete and noisy. We validate the effectiveness of our approach through extensive experiments on image denoising, demosaicing, and inpainting, along with comparisons with the state of the art.",
        "translated": "扩散模型（DMs）已迅速成为图像生成与修复的强大框架。然而，现有的扩散模型主要依赖于大量干净图像的监督训练。这种对干净数据的依赖在许多现实场景中带来了根本性挑战，因为在这些场景中获取无噪声数据困难或不可行，仅有噪声干扰且可能不完整的测量数据可用。尽管已有方法尝试使用噪声数据训练扩散模型，但它们通常仅在噪声非常轻微或存在部分无噪声数据时才有效。此外，目前基于不完整测量数据训练扩散模型的方法通常需要多个互补的采集过程，这一假设在实践中构成了显著的限制。本文首次提出了一种仅使用单一操作算子的噪声测量数据来学习图像修复扩散模型的方法。作为我们的第一个关键贡献，我们证明了扩散模型，以及更广泛的最小均方误差去噪器，表现出一种弱形式的尺度等变性（scale equivariance），将信号幅值的缩放与噪声强度的变化联系起来。我们随后利用这一理论洞见，提出一种去噪得分匹配策略，该策略能够稳健地推广到训练数据中噪声水平更低的情况，从而实现基于噪声测量数据的扩散模型训练。为了进一步应对数据不完整和噪声的挑战，我们将该方法与等变成像（equivariant imaging）相结合，这是一种互补的自监督学习框架，利用了成像问题中的固有不变性，从而实现基于单一操作算子所获取的不完整且噪声干扰数据的图像修复扩散模型训练。我们在图像去噪、色彩插值（demosaicing）和修复（inpainting）任务上进行了广泛的实验验证，并与当前最先进的方法进行了对比。"
    },
    {
        "title": "Enabling High-Quality In-the-Wild Imaging from Severely Aberrated\n  Metalens Bursts",
        "url": "http://arxiv.org/abs/2510.10083v1",
        "pub_date": "2025-10-11",
        "summary": "We tackle the challenge of robust, in-the-wild imaging using ultra-thin nanophotonic metalens cameras. Meta-lenses, composed of planar arrays of nanoscale scatterers, promise dramatic reductions in size and weight compared to conventional refractive optics. However, severe chromatic aberration, pronounced light scattering, narrow spectral bandwidth, and low light efficiency continue to limit their practical adoption. In this work, we present an end-to-end solution for in-the-wild imaging that pairs a metalens several times thinner than conventional optics with a bespoke multi-image restoration framework optimized for practical metalens cameras. Our method centers on a lightweight convolutional network paired with a memory-efficient burst fusion algorithm that adaptively corrects noise, saturation clipping, and lens-induced distortions across rapid sequences of extremely degraded metalens captures. Extensive experiments on diverse, real-world handheld captures demonstrate that our approach consistently outperforms existing burst-mode and single-image restoration techniques.These results point toward a practical route for deploying metalens-based cameras in everyday imaging applications.",
        "translated": "我们针对使用超薄纳米光学金属透镜相机在真实环境下的鲁棒成像问题提出了一个解决方案。金属透镜由平面排列的纳米级散射体构成，相较于传统的折射式光学元件，有望显著减小尺寸和重量。然而，严重的色差、明显的光散射、狭窄的光谱带宽以及低光效率等问题仍限制了其实际应用。在本工作中，我们提出了一种端到端的真实环境下成像方案，该方案结合了一种比传统光学元件薄几倍的金属透镜，以及一种为实际金属透镜相机量身定制、高效的多图像复原框架。我们的方法核心是一个轻量级卷积网络，结合了一种内存高效的图像序列融合算法，能够自适应地校正在快速拍摄的严重退化金属透镜图像序列中出现的噪声、饱和裁剪和透镜引起的失真。我们在多种实际手持拍摄数据上进行了广泛的实验，结果表明，我们的方法在性能上持续优于现有的突发模式和单图像复原技术。这些结果表明，基于金属透镜的相机在日常成像应用中具有实际可行的部署路径。"
    },
    {
        "title": "Denoising Diffusion as a New Framework for Underwater Images",
        "url": "http://arxiv.org/abs/2510.09934v1",
        "pub_date": "2025-10-11",
        "summary": "Underwater images play a crucial role in ocean research and marine environmental monitoring since they provide quality information about the ecosystem. However, the complex and remote nature of the environment results in poor image quality with issues such as low visibility, blurry textures, color distortion, and noise. In recent years, research in image enhancement has proven to be effective but also presents its own limitations, like poor generalization and heavy reliance on clean datasets. One of the challenges herein is the lack of diversity and the low quality of images included in these datasets. Also, most existing datasets consist only of monocular images, a fact that limits the representation of different lighting conditions and angles. In this paper, we propose a new plan of action to overcome these limitations. On one hand, we call for expanding the datasets using a denoising diffusion model to include a variety of image types such as stereo, wide-angled, macro, and close-up images. On the other hand, we recommend enhancing the images using Controlnet to evaluate and increase the quality of the corresponding datasets, and hence improve the study of the marine ecosystem.   Tags - Underwater Images, Denoising Diffusion, Marine ecosystem, Controlnet",
        "translated": "水下图像在海洋研究和海洋环境监测中起着至关重要的作用，因为它们提供了有关生态系统的重要信息。然而，由于环境的复杂性和远离人类的特性，所获取的图像质量通常较差，存在诸如能见度低、纹理模糊、颜色失真和噪声等问题。近年来，图像增强的研究已被证明是有效的，但也存在自身局限性，例如泛化能力差和对干净数据集的依赖性较强。其中一大挑战是这些数据集中图像的多样性不足且质量偏低。此外，大多数现有数据集仅包含单目图像，这一事实限制了不同光照条件和角度的表征能力。本文中，我们提出了一种新的应对方案以克服这些局限性。一方面，我们建议使用去噪扩散模型（denoising diffusion model）扩展数据集，包含诸如双目、广角、微距和特写等多种图像类型；另一方面，我们推荐使用 ControlNet 对图像进行增强，以评估并提升相应数据集的质量，从而促进对海洋生态系统的研究。  \n标签 - 水下图像，去噪扩散，海洋生态系统，ControlNet"
    },
    {
        "title": "Defense against Unauthorized Distillation in Image Restoration via\n  Feature Space Perturbation",
        "url": "http://arxiv.org/abs/2510.08925v1",
        "pub_date": "2025-10-10",
        "summary": "Knowledge distillation (KD) attacks pose a significant threat to deep model intellectual property by enabling adversaries to train student networks using a teacher model's outputs. While recent defenses in image classification have successfully disrupted KD by perturbing output probabilities, extending these methods to image restoration is difficult. Unlike classification, restoration is a generative task with continuous, high-dimensional outputs that depend on spatial coherence and fine details. Minor perturbations are often insufficient, as students can still learn the underlying mapping.To address this, we propose Adaptive Singular Value Perturbation (ASVP), a runtime defense tailored for image restoration models. ASVP operates on internal feature maps of the teacher using singular value decomposition (SVD). It amplifies the topk singular values to inject structured, high-frequency perturbations, disrupting the alignment needed for distillation. This hinders student learning while preserving the teacher's output quality.We evaluate ASVP across five image restoration tasks: super-resolution, low-light enhancement, underwater enhancement, dehazing, and deraining. Experiments show ASVP reduces student PSNR by up to 4 dB and SSIM by 60-75%, with negligible impact on the teacher's performance. Compared to prior methods, ASVP offers a stronger and more consistent defense.Our approach provides a practical solution to protect open-source restoration models from unauthorized knowledge distillation.",
        "translated": "知识蒸馏（Knowledge Distillation, KD）攻击通过使对手能够利用教师模型的输出来训练学生网络，对深度模型的知识产权构成了重大威胁。尽管图像分类领域的最新防御方法已成功通过扰动输出概率来破坏知识蒸馏，但将这些方法扩展到图像恢复任务却面临困难。与分类任务不同，图像恢复是一个生成任务，其输出是连续且高维的，依赖于空间一致性和细节质量。因此，微小的扰动往往不足以阻止知识蒸馏，因为学生模型仍能学习到潜在的映射关系。\n\n为了解决这一问题，我们提出了一种专为图像恢复模型设计的运行时防御方法——自适应奇异值扰动（Adaptive Singular Value Perturbation, ASVP）。ASVP 通过奇异值分解（Singular Value Decomposition, SVD）对教师模型的内部特征图进行操作。该方法通过放大前k个奇异值，注入结构化的高频扰动，从而破坏蒸馏过程中所需的对齐关系。这种扰动在阻碍学生模型学习的同时，保持了教师模型的输出质量。\n\n我们在五个图像恢复任务中评估了 ASVP：超分辨率、低光增强、水下增强、去雾和去雨。实验结果表明，ASVP 最多可使学生模型的 PSNR 降低 4 dB，SSIM 降低 60-75%，而对教师模型的性能影响几乎可以忽略。与现有方法相比，ASVP 提供了更强且更一致的防御效果。我们的方法为保护开源图像恢复模型免受未经授权的知识蒸馏提供了一个实用的解决方案。"
    },
    {
        "title": "Latent Harmony: Synergistic Unified UHD Image Restoration via Latent\n  Space Regularization and Controllable Refinement",
        "url": "http://arxiv.org/abs/2510.07961v2",
        "pub_date": "2025-10-09",
        "summary": "Ultra-High Definition (UHD) image restoration faces a trade-off between computational efficiency and high-frequency detail retention. While Variational Autoencoders (VAEs) improve efficiency via latent-space processing, their Gaussian constraint often discards degradation-specific high-frequency information, hurting reconstruction fidelity. To overcome this, we propose Latent Harmony, a two-stage framework that redefines VAEs for UHD restoration by jointly regularizing the latent space and enforcing high-frequency-aware reconstruction.In Stage One, we introduce LH-VAE, which enhances semantic robustness through visual semantic constraints and progressive degradation perturbations, while latent equivariance strengthens high-frequency reconstruction.Stage Two jointly trains this refined VAE with a restoration model using High-Frequency Low-Rank Adaptation (HF-LoRA): an encoder LoRA guided by a fidelity-oriented high-frequency alignment loss to recover authentic details, and a decoder LoRA driven by a perception-oriented loss to synthesize realistic textures. Both LoRA modules are trained via alternating optimization with selective gradient propagation to preserve the pretrained latent structure.At inference, a tunable parameter {\\alpha} enables flexible fidelity-perception trade-offs.Experiments show Latent Harmony achieves state-of-the-art performance across UHD and standard-resolution tasks, effectively balancing efficiency, perceptual quality, and reconstruction accuracy.",
        "translated": "超高分辨率（UHD）图像修复面临计算效率与高频细节保留之间的权衡。尽管变分自编码器（VAEs）通过潜在空间处理提升了效率，但其高斯约束常常会丢弃与退化相关的高频信息，从而损害重建的保真度。为了解决这一问题，我们提出了**Latent Harmony**，一种两阶段框架，通过联合正则化潜在空间并强制实现高频感知的重建，重新定义了用于UHD图像修复的VAEs。  \n\n在第一阶段中，我们引入了**LH-VAE**，该模型通过视觉语义约束和渐进式退化扰动来增强语义鲁棒性，同时利用潜在等变性来强化高频信息的重建。  \n\n在第二阶段中，我们将优化后的VAE与一个修复模型联合训练，采用**高频低秩适配（High-Frequency Low-Rank Adaptation, HF-LoRA）**方法：一个编码器LoRA通过以保真度为导向的高频对齐损失（high-frequency alignment loss）来恢复真实细节，而一个解码器LoRA则由以感知为导向的损失驱动，以合成逼真的纹理。两个LoRA模块通过交替优化与选择性梯度传播进行训练，从而保持预训练的潜在结构不变。  \n\n在推理阶段，一个可调节的参数 $\\alpha$ 可实现保真度与感知质量之间的灵活权衡。实验表明，**Latent Harmony**在UHD和标准分辨率任务中均达到了最先进的性能，有效平衡了计算效率、感知质量与重建精度。"
    },
    {
        "title": "PhyDAE: Physics-Guided Degradation-Adaptive Experts for All-in-One\n  Remote Sensing Image Restoration",
        "url": "http://arxiv.org/abs/2510.08653v1",
        "pub_date": "2025-10-09",
        "summary": "Remote sensing images inevitably suffer from various degradation factors during acquisition, including atmospheric interference, sensor limitations, and imaging conditions. These complex and heterogeneous degradations pose severe challenges to image quality and downstream interpretation tasks. Addressing limitations of existing all-in-one restoration methods that overly rely on implicit feature representations and lack explicit modeling of degradation physics, this paper proposes Physics-Guided Degradation-Adaptive Experts (PhyDAE). The method employs a two-stage cascaded architecture transforming degradation information from implicit features into explicit decision signals, enabling precise identification and differentiated processing of multiple heterogeneous degradations including haze, noise, blur, and low-light conditions. The model incorporates progressive degradation mining and exploitation mechanisms, where the Residual Manifold Projector (RMP) and Frequency-Aware Degradation Decomposer (FADD) comprehensively analyze degradation characteristics from manifold geometry and frequency perspectives. Physics-aware expert modules and temperature-controlled sparse activation strategies are introduced to enhance computational efficiency while ensuring imaging physics consistency. Extensive experiments on three benchmark datasets (MD-RSID, MD-RRSHID, and MDRS-Landsat) demonstrate that PhyDAE achieves superior performance across all four restoration tasks, comprehensively outperforming state-of-the-art methods. Notably, PhyDAE substantially improves restoration quality while achieving significant reductions in parameter count and computational complexity, resulting in remarkable efficiency gains compared to mainstream approaches and achieving optimal balance between performance and efficiency. Code is available at https://github.com/HIT-SIRS/PhyDAE.",
        "translated": "遥感图像在获取过程中不可避免地受到多种退化因素的影响，包括大气干扰、传感器限制以及成像条件等。这些复杂且异质的退化现象给图像质量以及后续的语义解析任务带来了严重挑战。为了解决现有端到端图像修复方法中对隐式特征表示的过度依赖以及缺乏对退化物理机制的显式建模等问题，本文提出了一种物理引导的退化自适应专家模型（Physics-Guided Degradation-Adaptive Experts, PhyDAE）。该方法采用两阶段级联架构，将隐式特征中的退化信息转化为显式的决策信号，从而实现对多种异质退化（包括雾霾、噪声、模糊和低光照条件）的精确识别与差异化处理。模型引入了渐进式退化挖掘与利用机制，其中残差流形投影器（Residual Manifold Projector, RMP）和频域感知退化解耦模块（Frequency-Aware Degradation Decomposer, FADD）分别从流形几何结构和频域角度对退化特性进行全面分析。此外，通过引入物理感知的专家模块和温度控制的稀疏激活策略，在保证成像物理一致性的同时显著提升了计算效率。在三个基准数据集（MD-RSID、MD-RRSHID 和 MDRS-Landsat）上的大量实验表明，PhyDAE 在四项图像修复任务中均表现出优越的性能，全面超越现有最先进方法。特别值得一提的是，PhyDAE 在大幅提升修复质量的同时，显著减少了模型参数数量与计算复杂度，从而在主流方法中实现了性能与效率之间的最优平衡。代码可在 https://github.com/HIT-SIRS/PhyDAE 获取。"
    },
    {
        "title": "DeRainMamba: A Frequency-Aware State Space Model with Detail Enhancement\n  for Image Deraining",
        "url": "http://arxiv.org/abs/2510.06746v1",
        "pub_date": "2025-10-08",
        "summary": "Image deraining is crucial for improving visual quality and supporting reliable downstream vision tasks. Although Mamba-based models provide efficient sequence modeling, their limited ability to capture fine-grained details and lack of frequency-domain awareness restrict further improvements. To address these issues, we propose DeRainMamba, which integrates a Frequency-Aware State-Space Module (FASSM) and Multi-Directional Perception Convolution (MDPConv). FASSM leverages Fourier transform to distinguish rain streaks from high-frequency image details, balancing rain removal and detail preservation. MDPConv further restores local structures by capturing anisotropic gradient features and efficiently fusing multiple convolution branches. Extensive experiments on four public benchmarks demonstrate that DeRainMamba consistently outperforms state-of-the-art methods in PSNR and SSIM, while requiring fewer parameters and lower computational costs. These results validate the effectiveness of combining frequency-domain modeling and spatial detail enhancement within a state-space framework for single image deraining.",
        "translated": "图像去雨对于提升视觉质量和支持可靠的下游视觉任务至关重要。尽管基于Mamba的模型在序列建模方面具有较高的效率，但其在捕捉细粒度细节方面的能力有限以及缺乏频域感知，限制了进一步的性能提升。为解决这些问题，我们提出DeRainMamba，该方法融合了一个频域感知状态空间模块（Frequency-Aware State-Space Module, FASSM）和多方向感知卷积（Multi-Directional Perception Convolution, MDPConv）。FASSM利用傅里叶变换区分雨痕与图像中的高频细节，从而在去雨与细节保留之间取得平衡。MDPConv通过捕捉各向异性梯度特征并高效融合多分支卷积，进一步恢复局部结构。我们在四个公开基准数据集上进行了广泛的实验，结果表明DeRainMamba在PSNR和SSIM指标上始终优于当前最先进的方法，同时参数量更少，计算成本更低。这些结果验证了在状态空间框架中结合频域建模与空间细节增强对于单图像去雨的有效性。"
    },
    {
        "title": "An Inertial Langevin Algorithm",
        "url": "http://arxiv.org/abs/2510.06723v1",
        "pub_date": "2025-10-08",
        "summary": "We present a novel method for drawing samples from Gibbs distributions with densities of the form $\\pi(x) \\propto \\exp(-U(x))$. The method accelerates the unadjusted Langevin algorithm by introducing an inertia term similar to Polyak's heavy ball method, together with a corresponding noise rescaling. Interpreting the scheme as a discretization of \\emph{kinetic} Langevin dynamics, we prove ergodicity (in continuous and discrete time) for twice continuously differentiable, strongly convex, and $L$-smooth potentials and bound the bias of the discretization to the target in Wasserstein-2 distance. In particular, the presented proofs allow for smaller friction parameters in the kinetic Langevin diffusion compared to existing literature. Moreover, we show the close ties of the proposed method to the over-relaxed Gibbs sampler. The scheme is tested in an extensive set of numerical experiments covering simple toy examples, total variation image denoising, and the complex task of maximum likelihood learning of an energy-based model for molecular structure generation. The experimental results confirm the acceleration provided by the proposed scheme even beyond the strongly convex and $L$-smooth setting.",
        "translated": "我们提出了一种从形式为 $\\pi(x) \\propto \\exp(-U(x))$ 的 Gibbs 分布中抽样的新方法。该方法通过引入类似于 Polyak 重球法的惯性项以及相应的噪声重缩放机制，加速了未调整的 Langevin 算法。将该方法解释为对 \\emph{动能} Langevin 动力学的离散化，我们证明了在连续和离散时间下，对于二次连续可微、强凸且 $L$-光滑的势函数，该方法具有遍历性，并在 Wasserstein-2 距离下对该离散化方案与目标分布之间的偏差进行了上界分析。特别地，所提出的证明允许在动能 Langevin 扩散中使用比现有文献中更小的摩擦参数。此外，我们展示了该方法与过松弛 Gibbs 抽样器之间的紧密联系。该算法在一个广泛的数值实验中进行了测试，涵盖简单的玩具示例、图像的全变分去噪任务，以及分子结构生成的能量基模型的最大似然学习这一复杂任务。实验结果验证了所提出方法即使在非强凸和非 $L$-光滑的设置下仍能提供加速效果。"
    },
    {
        "title": "AIM 2025 Challenge on Real-World RAW Image Denoising",
        "url": "http://arxiv.org/abs/2510.06601v1",
        "pub_date": "2025-10-08",
        "summary": "We introduce the AIM 2025 Real-World RAW Image Denoising Challenge, aiming to advance efficient and effective denoising techniques grounded in data synthesis. The competition is built upon a newly established evaluation benchmark featuring challenging low-light noisy images captured in the wild using five different DSLR cameras. Participants are tasked with developing novel noise synthesis pipelines, network architectures, and training methodologies to achieve high performance across different camera models. Winners are determined based on a combination of performance metrics, including full-reference measures (PSNR, SSIM, LPIPS), and non-reference ones (ARNIQA, TOPIQ). By pushing the boundaries of camera-agnostic low-light RAW image denoising trained on synthetic data, the competition promotes the development of robust and practical models aligned with the rapid progress in digital photography. We expect the competition outcomes to influence multiple domains, from image restoration to night-time autonomous driving.",
        "translated": "我们介绍了AIM 2025真实世界RAW图像去噪挑战赛，旨在通过数据合成推动高效且有效的去噪技术的发展。该竞赛基于一个新的评估基准构建，该基准包含在自然场景下使用五种不同DSLR相机拍摄的具有挑战性的低光照噪声图像。参赛者需要开发新颖的噪声合成流程、网络架构和训练方法，以在不同相机模型上实现高性能的去噪效果。比赛的优胜者将根据多种性能指标综合评定，包括全参考指标（PSNR、SSIM、LPIPS）和非参考指标（ARNIQA、TOPIQ）。通过推动在合成数据上训练的、具有相机泛化能力的低光照RAW图像去噪技术的边界，该竞赛促进了与数字摄影快速进步相契合的鲁棒且实用模型的发展。我们预期本次竞赛的成果将对多个领域产生影响，从图像修复到夜间自动驾驶。"
    },
    {
        "title": "TDiff: Thermal Plug-And-Play Prior with Patch-Based Diffusion",
        "url": "http://arxiv.org/abs/2510.06460v1",
        "pub_date": "2025-10-07",
        "summary": "Thermal images from low-cost cameras often suffer from low resolution, fixed pattern noise, and other localized degradations. Available datasets for thermal imaging are also limited in both size and diversity. To address these challenges, we propose a patch-based diffusion framework (TDiff) that leverages the local nature of these distortions by training on small thermal patches. In this approach, full-resolution images are restored by denoising overlapping patches and blending them using smooth spatial windowing. To our knowledge, this is the first patch-based diffusion framework that models a learned prior for thermal image restoration across multiple tasks. Experiments on denoising, super-resolution, and deblurring demonstrate strong results on both simulated and real thermal data, establishing our method as a unified restoration pipeline.",
        "translated": "低成本热成像相机所获取的图像通常存在分辨率较低、固定模式噪声以及其他局部退化问题。目前可用的热成像数据集在规模和多样性方面也较为有限。为了解决这些挑战，我们提出了一种基于图像块的扩散框架（TDiff），该框架通过在小尺寸热图像块上进行训练，利用这些退化现象的局部特性。在该方法中，通过去噪重叠图像块，并结合平滑的空间窗口函数进行融合，从而恢复全分辨率图像。据我们所知，这是首个基于图像块的扩散框架，能够在多个任务中对热图像的退化建模并学习其先验分布。我们在去噪、超分辨率和去模糊任务上的实验表明，该方法在模拟和真实热图像数据上均取得了优异的效果，从而确立了其作为统一图像恢复流程的地位。"
    },
    {
        "title": "Local MAP Sampling for Diffusion Models",
        "url": "http://arxiv.org/abs/2510.07343v2",
        "pub_date": "2025-10-07",
        "summary": "Diffusion Posterior Sampling (DPS) provides a principled Bayesian approach to inverse problems by sampling from $p(x_0 \\mid y)$. However, in practice, the goal of inverse problem solving is not to cover the posterior but to recover the most accurate reconstruction, where optimization-based diffusion solvers often excel despite lacking a clear probabilistic foundation. We introduce Local MAP Sampling (LMAPS), a new inference framework that iteratively solving local MAP subproblems along the diffusion trajectory. This perspective clarifies their connection to global MAP estimation and DPS, offering a unified probabilistic interpretation for optimization-based methods. Building on this foundation, we develop practical algorithms with a probabilistically interpretable covariance approximation, a reformulated objective for stability and interpretability, and a gradient approximation for non-differentiable operators. Across a broad set of image restoration and scientific tasks, LMAPS achieves state-of-the-art performance, including $\\geq 2$ dB gains on motion deblurring, JPEG restoration, and quantization, and $&gt;1.5$ dB improvements on inverse scattering benchmarks.",
        "translated": "扩散后验采样（Diffusion Posterior Sampling, DPS）通过从 $p(x_0 \\mid y)$ 中采样，为逆问题提供了一种基于贝叶斯原理的求解方法。然而，实际上，解决逆问题的目标并非是覆盖后验分布，而是恢复最精确的重建结果，在这一点上，基于优化的扩散求解器往往表现出色，尽管它们缺乏明确的概率基础。我们提出一种新的推理框架——局部最大后验采样（Local MAP Sampling, LMAPS），该方法沿着扩散轨迹迭代求解局部最大后验（MAP）子问题。这一视角明确了LMAPS与全局MAP估计以及DPS之间的关系，为基于优化的方法提供了一种统一的概率解释。基于这一理论基础，我们开发了具有概率解释的协方差近似方法，提出了一个用于提高稳定性和可解释性的重构目标函数，并引入了针对不可微操作符的梯度近似方法。在广泛的图像恢复和科学计算任务中，LMAPS实现了最先进的性能，包括在运动去模糊、JPEG图像恢复和量化任务中获得了 $\\geq 2$ dB 的提升，在逆散射基准测试中实现了 $>1.5$ dB 的性能改进。"
    },
    {
        "title": "Rasterized Steered Mixture of Experts for Efficient 2D Image Regression",
        "url": "http://arxiv.org/abs/2510.05814v1",
        "pub_date": "2025-10-07",
        "summary": "The Steered Mixture of Experts regression framework has demonstrated strong performance in image reconstruction, compression, denoising, and super-resolution. However, its high computational cost limits practical applications. This work introduces a rasterization-based optimization strategy that combines the efficiency of rasterized Gaussian kernel rendering with the edge-aware gating mechanism of the Steered Mixture of Experts. The proposed method is designed to accelerate two-dimensional image regression while maintaining the model's inherent sparsity and reconstruction quality. By replacing global iterative optimization with a rasterized formulation, the method achieves significantly faster parameter updates and more memory-efficient model representations. In addition, the proposed framework supports applications such as native super-resolution and image denoising, which are not directly achievable with standard rasterized Gaussian kernel approaches. The combination of fast rasterized optimization with the edge-aware structure of the Steered Mixture of Experts provides a new balance between computational efficiency and reconstruction fidelity for two-dimensional image processing tasks.",
        "translated": "基于引导的专家混合（Steered Mixture of Experts）回归框架在图像重建、压缩、去噪和超分辨率等任务中已展现出优异的性能。然而，其较高的计算成本限制了其在实际应用中的使用。本文提出了一种基于光栅化的优化策略，结合了光栅化高斯核渲染的高效性与Steered Mixture of Experts中边缘感知门控机制的优势。所提出的方法旨在加速二维图像回归过程，同时保持模型固有的稀疏性与重建质量。通过将全局迭代优化替换为光栅化形式，该方法显著提高了参数更新速度，并实现了更节省内存的模型表示。此外，该框架支持诸如原生超分辨率和图像去噪等应用，而这些是标准光栅化高斯核方法无法直接实现的。快速光栅化优化与Steered Mixture of Experts边缘感知结构的结合，为二维图像处理任务提供了计算效率与重建保真度之间的新平衡。"
    },
    {
        "title": "Adaptive double-phase Rudin--Osher--Fatemi denoising model",
        "url": "http://arxiv.org/abs/2510.04382v1",
        "pub_date": "2025-10-05",
        "summary": "We propose a new image denoising model based on a variable-growth total variation regularization of double-phase type with adaptive weight. It is designed to reduce staircasing with respect to the classical Rudin--Osher--Fatemi model, while preserving the edges of the image in a similar fashion. We implement the model and test its performance on synthetic and natural images in 1D and 2D over a range of noise levels.",
        "translated": "我们提出了一种新的图像去噪模型，该模型基于具有自适应权重的双阶段可变增长总体变差正则化。该模型旨在相较于经典的 Rudin–Osher–Fatemi 模型，减少阶梯效应（staircasing），同时以类似的方式保留图像的边缘。我们对该模型进行了实现，并在 1D 和 2D 的合成图像与自然图像上，针对多种噪声水平进行了性能测试。"
    },
    {
        "title": "Equilibrium Matching: Generative Modeling with Implicit Energy-Based\n  Models",
        "url": "http://arxiv.org/abs/2510.02300v3",
        "pub_date": "2025-10-02",
        "summary": "We introduce Equilibrium Matching (EqM), a generative modeling framework built from an equilibrium dynamics perspective. EqM discards the non-equilibrium, time-conditional dynamics in traditional diffusion and flow-based generative models and instead learns the equilibrium gradient of an implicit energy landscape. Through this approach, we can adopt an optimization-based sampling process at inference time, where samples are obtained by gradient descent on the learned landscape with adjustable step sizes, adaptive optimizers, and adaptive compute. EqM surpasses the generation performance of diffusion/flow models empirically, achieving an FID of 1.90 on ImageNet 256$\\times$256. EqM is also theoretically justified to learn and sample from the data manifold. Beyond generation, EqM is a flexible framework that naturally handles tasks including partially noised image denoising, OOD detection, and image composition. By replacing time-conditional velocities with a unified equilibrium landscape, EqM offers a tighter bridge between flow and energy-based models and a simple route to optimization-driven inference.",
        "translated": "我们提出了一种名为**均衡匹配**（Equilibrium Matching, EqM）的生成建模框架，其构建基于**均衡动力学**的视角。EqM摒弃了传统扩散模型和基于流的生成模型中所依赖的非均衡、时序条件的动力学，转而学习一个隐式能量景观的**均衡梯度**。通过这种方法，我们可以在推理阶段采用基于优化的采样过程，其中样本通过在所学能量景观上进行梯度下降获得，该过程支持可调节的步长、自适应优化器以及自适应计算能力。从实证结果来看，EqM在生成性能上超越了扩散模型和流模型，在ImageNet 256$\\times$256数据集上达到了1.90的FID分数。EqM在理论上也能够从数据流形中进行学习与采样，具有坚实的理论依据。除生成任务外，EqM还是一种灵活的框架，天然地适用于包括部分噪声图像去噪、分布外（OOD）检测以及图像合成在内的多种任务。通过将时序条件的速度替换为统一的均衡景观，EqM在流模型与基于能量的模型之间架起了更紧密的桥梁，并为实现优化驱动的推理提供了一条简洁的路径。"
    },
    {
        "title": "HyMiRec: A Hybrid Multi-interest Learning Framework for LLM-based\n  Sequential Recommendation",
        "url": "http://arxiv.org/abs/2510.13738v1",
        "pub_date": "2025-10-15",
        "summary": "Large language models (LLMs) have recently demonstrated strong potential for sequential recommendation. However, current LLM-based approaches face critical limitations in modeling users' long-term and diverse interests. First, due to inference latency and feature fetching bandwidth constraints, existing methods typically truncate user behavior sequences to include only the most recent interactions, resulting in the loss of valuable long-range preference signals. Second, most current methods rely on next-item prediction with a single predicted embedding, overlooking the multifaceted nature of user interests and limiting recommendation diversity. To address these challenges, we propose HyMiRec, a hybrid multi-interest sequential recommendation framework, which leverages a lightweight recommender to extracts coarse interest embeddings from long user sequences and an LLM-based recommender to captures refined interest embeddings. To alleviate the overhead of fetching features, we introduce a residual codebook based on cosine similarity, enabling efficient compression and reuse of user history embeddings. To model the diverse preferences of users, we design a disentangled multi-interest learning module, which leverages multiple interest queries to learn disentangles multiple interest signals adaptively, allowing the model to capture different facets of user intent. Extensive experiments are conducted on both benchmark datasets and a collected industrial dataset, demonstrating our effectiveness over existing state-of-the-art methods. Furthermore, online A/B testing shows that HyMiRec brings consistent improvements in real-world recommendation systems.",
        "translated": "大语言模型（LLMs）最近在序列推荐中展现出强大的潜力。然而，目前基于LLM的方法在建模用户长期和多样化兴趣方面面临关键的限制。首先，由于推理延迟和特征获取带宽的限制，现有方法通常截断用户行为序列，仅包含最近的交互，从而丢失了有价值的长距离偏好信号。其次，大多数当前方法依赖于单一预测嵌入的下一物品预测，忽视了用户兴趣的多面性，限制了推荐的多样性。为了解决这些挑战，我们提出HyMiRec，一种混合多兴趣序列推荐框架，其利用一个轻量级推荐器从长用户序列中提取粗兴趣嵌入，并利用一个基于LLM的推荐器捕捉精兴趣嵌入。为了缓解特征获取带来的开销，我们引入了一个基于余弦相似度的残差码本，从而实现了用户历史嵌入的高效压缩和重复使用。为了建模用户的多样化偏好，我们设计了一个解耦的多兴趣学习模块，该模块利用多个兴趣查询自适应地学习解耦的多个兴趣信号，使得模型能够捕捉用户意图的不同方面。我们在基准数据集和收集的工业数据集上进行了广泛的实验，验证了该方法相较于现有最先进方法的有效性。此外，在线A/B测试表明，HyMiRec在实际推荐系统中带来了持续的性能提升。",
        "translated_title": "HyMiRec：一种基于大语言模型的序列推荐混合多兴趣学习框架",
        "label": [
            "LLM生成式推荐",
            "序列推荐",
            "通用推荐技术"
        ],
        "label_reason": "结合LLM与多兴趣学习的序列推荐框架，直接解决推荐系统核心问题",
        "relevance_score": 9
    },
    {
        "title": "RAG Meets Temporal Graphs: Time-Sensitive Modeling and Retrieval for\n  Evolving Knowledge",
        "url": "http://arxiv.org/abs/2510.13590v1",
        "pub_date": "2025-10-15",
        "summary": "Knowledge is inherently time-sensitive and continuously evolves over time. Although current Retrieval-Augmented Generation (RAG) systems enrich LLMs with external knowledge, they largely ignore this temporal nature. This raises two challenges for RAG. First, current RAG methods lack effective time-aware representations. Same facts of different time are difficult to distinguish with vector embeddings or conventional knowledge graphs. Second, most RAG evaluations assume a static corpus, leaving a blind spot regarding update costs and retrieval stability as knowledge evolves. To make RAG time-aware, we propose Temporal GraphRAG (TG-RAG), which models external corpora as a bi-level temporal graph consisting of a temporal knowledge graph with timestamped relations and a hierarchical time graph. Multi-granularity temporal summaries are generated for each time node to capture both key events and broader trends at that time. The design supports incremental updates by extracting new temporal facts from the incoming corpus and merging them into the existing graph. The temporal graph explicitly represents identical facts at different times as distinct edges to avoid ambiguity, and the time hierarchy graph allows only generating reports for new leaf time nodes and their ancestors, ensuring effective and efficient updates. During inference, TG-RAG dynamically retrieves a subgraph within the temporal and semantic scope of the query, enabling precise evidence gathering. Moreover, we introduce ECT-QA, a time-sensitive question-answering dataset featuring both specific and abstract queries, along with a comprehensive evaluation protocol designed to assess incremental update capabilities of RAG systems. Extensive experiments show that TG-RAG significantly outperforms existing baselines, demonstrating the effectiveness of our method in handling temporal knowledge and incremental updates.",
        "translated": "知识本质上是时间敏感的，并且会随着时间不断演化。尽管当前的检索增强生成（RAG）系统通过引入外部知识丰富了大语言模型（LLM），但它们在很大程度上忽视了这种时间特性。这引发了RAG面临的两个挑战。首先，当前的RAG方法缺乏有效的时间感知表示。不同时间下的相同事实，难以通过向量嵌入（embedding）或传统知识图谱进行区分。其次，大多数RAG评估假设语料库是静态的，从而忽略了知识演化过程中的更新成本和检索稳定性问题。为了使RAG具备时间感知能力，我们提出了时序图RAG（Temporal GraphRAG，TG-RAG），该方法将外部语料库建模为一个双层时序图，包含一个具有时间戳关系的时序知识图谱和一个层次化时间图。为每个时间节点生成多粒度时序摘要，以捕捉该时间点的关键事件和更广泛的趋势。该设计通过从新输入的语料中提取新的时序事实并将其合并到现有图中，支持增量更新。时序图显式地将不同时刻的相同事实表示为不同的边，以避免歧义，而时间层次图则仅允许为新的叶子时间节点及其祖先生成报告，从而确保更新的有效性和高效性。在推理阶段，TG-RAG能够动态检索与查询在时间和语义范围内匹配的子图，实现精确的证据收集。此外，我们引入了一个时间敏感的问答数据集ECT-QA，包含具体和抽象的查询，并设计了一套全面的评估协议，用于评估RAG系统的增量更新能力。大量实验表明，TG-RAG显著优于现有基线，验证了我们方法在处理时序知识和增量更新方面的有效性。",
        "translated_title": "RAG与时间图的结合：面向动态知识的时间敏感建模与召回",
        "label": [
            "LLM生成式推荐",
            "多模态推荐",
            "推荐系统评估"
        ],
        "label_reason": "论文提出时间敏感的RAG方法，适用于生成式推荐中的动态知识建模与评估。",
        "relevance_score": 7
    },
    {
        "title": "MADREC: A Multi-Aspect Driven LLM Agent for Explainable and Adaptive\n  Recommendation",
        "url": "http://arxiv.org/abs/2510.13371v1",
        "pub_date": "2025-10-15",
        "summary": "Recent attempts to integrate large language models (LLMs) into recommender systems have gained momentum, but most remain limited to simple text generation or static prompt-based inference, failing to capture the complexity of user preferences and real-world interactions. This study proposes the Multi-Aspect Driven LLM Agent MADRec, an autonomous LLM-based recommender that constructs user and item profiles by unsupervised extraction of multi-aspect information from reviews and performs direct recommendation, sequential recommendation, and explanation generation. MADRec generates structured profiles via aspect-category-based summarization and applies Re-Ranking to construct high-density inputs. When the ground-truth item is missing from the output, the Self-Feedback mechanism dynamically adjusts the inference criteria. Experiments across multiple domains show that MADRec outperforms traditional and LLM-based baselines in both precision and explainability, with human evaluation further confirming the persuasiveness of the generated explanations.",
        "translated": "近期尝试将大语言模型（LLMs）集成到推荐系统中的工作逐渐增多，但大多数仍局限于简单的文本生成或基于静态提示的推理，未能捕捉用户偏好和现实世界交互的复杂性。本研究提出多方面驱动的大语言模型代理 MADRec，这是一种基于大语言模型的自主推荐系统，通过从评论中无监督提取多方面信息构建用户和物料画像，并执行直接推荐、序列推荐以及解释生成。MADRec 通过基于方面类别的摘要生成结构化画像，并应用重排（Re-Ranking）构造高密度输入。当输出中缺少真实物料时，自反馈（Self-Feedback）机制会动态调整推理标准。跨多个领域的实验表明，MADRec 在准确性和可解释性方面均优于传统方法和基于 LLM 的基线模型，人工评估进一步验证了所生成解释的说服力。",
        "translated_title": "MADREC：一种面向多方面驱动的可解释且自适应的推荐大语言模型代理",
        "label": [
            "LLM生成式推荐",
            "精排",
            "重排",
            "推荐系统可解释性"
        ],
        "label_reason": "基于LLM的推荐与可解释性，融合重排与反馈机制",
        "relevance_score": 9
    },
    {
        "title": "Improving Visual Recommendation on E-commerce Platforms Using\n  Vision-Language Models",
        "url": "http://arxiv.org/abs/2510.13359v1",
        "pub_date": "2025-10-15",
        "summary": "On large-scale e-commerce platforms with tens of millions of active monthly users, recommending visually similar products is essential for enabling users to efficiently discover items that align with their preferences. This study presents the application of a vision-language model (VLM) -- which has demonstrated strong performance in image recognition and image-text retrieval tasks -- to product recommendations on Mercari, a major consumer-to-consumer marketplace used by more than 20 million monthly users in Japan. Specifically, we fine-tuned SigLIP, a VLM employing a sigmoid-based contrastive loss, using one million product image-title pairs from Mercari collected over a three-month period, and developed an image encoder for generating item embeddings used in the recommendation system. Our evaluation comprised an offline analysis of historical interaction logs and an online A/B test in a production environment. In offline analysis, the model achieved a 9.1% improvement in nDCG@5 compared with the baseline. In the online A/B test, the click-through rate improved by 50% whereas the conversion rate improved by 14% compared with the existing model. These results demonstrate the effectiveness of VLM-based encoders for e-commerce product recommendations and provide practical insights into the development of visual similarity-based recommendation systems.",
        "translated": "在拥有数千万活跃月活用户的大型电商平台中，推荐视觉相似产品对于帮助用户高效发现与其偏好一致的物料至关重要。本研究提出了将视觉-语言模型（VLM）应用于Mercari平台的产品推荐，其中VLM已在图像识别和图文检索任务中展现出卓越的性能。Mercari是日本一个重要的C2C电商平台，月活跃用户超过2000万。具体而言，我们使用在三个月内从Mercari收集的包含一百万对产品图像和标题的数据，对采用基于Sigmoid的对比损失的VLM SigLIP进行了微调，并开发了用于生成推荐系统中物料嵌入表示的图像编码器。我们的评估包括对历史交互日志的离线分析以及在生产环境中进行的在线A/B测试。离线分析结果显示，与基线模型相比，该模型在nDCG@5指标上提升了9.1%。在线A/B测试中，点击率提升了50%，转化率提升了14%。这些结果证明了基于VLM编码器在电商产品推荐中的有效性，并为视觉相似性推荐系统的发展提供了实践洞见。",
        "translated_title": "使用视觉-语言模型改进电子商务平台的视觉推荐",
        "label": [
            "多模态推荐",
            "精排",
            "图像相似性推荐"
        ],
        "label_reason": "论文将视觉语言模型用于电商推荐，提升图像相似性产品推荐效果",
        "relevance_score": 8
    },
    {
        "title": "ChatR1: Reinforcement Learning for Conversational Reasoning and\n  Retrieval Augmented Question Answering",
        "url": "http://arxiv.org/abs/2510.13312v1",
        "pub_date": "2025-10-15",
        "summary": "We present ChatR1, a reasoning framework based on reinforcement learning (RL) for conversational question answering (CQA). Reasoning plays an important role in CQA, where user intent evolves across dialogue turns, and utterances are often underspecified, requiring contextual interpretation, query reformulation, and dynamic coordination between retrieval and generation. Unlike static `rewrite, retrieve, and generate' pipelines, ChatR1 interleaves search and reasoning across turns, enabling exploratory and adaptive behaviors learned through RL. To address the challenge of sparse and delayed rewards in RL, we propose an intent-aware reward that provides turn-level feedback by aligning retrieval and reasoning with evolving user goals. Our proposed ChatR1 demonstrates strong performance on both 3B and 7B model backbones, outperforming competitive models on five CQA datasets, measured by different metrics (F1, BERTScore, and LLM-as-judge). We include a diverse set of CQA datasets to cover topic shifts, evolving intents, mixed-initiative dialogues, and multi-document grounding, testing ChatR1's performance from various aspects. Ablation studies confirm the effectiveness of the intent-aware reward. Our analyses further reveal diverse reasoning trajectories and effective use of the search tool. ChatR1 also generalizes robustly across domains, demonstrating that RL-based reasoning enables more flexible and context-sensitive behavior than static CQA pipelines.",
        "translated": "我们提出了 ChatR1，一种基于强化学习（RL）的对话式问答（CQA）推理框架。推理在 CQA 中起着重要作用，因为用户意图会随着对话轮次而演变，且对话内容通常信息不完整，需要上下文解释、查询重构，以及检索与生成之间的动态协调。与静态的“重写、检索、生成”流水线不同，ChatR1 在对话轮次中交替进行搜索和推理，使得通过 RL 学到的探索性和适应性行为得以实现。为了解决 RL 中稀疏和延迟奖励的挑战，我们提出了一种意图感知的奖励机制，通过将检索和推理与用户意图的演变对齐，提供轮次级的反馈。我们提出的 ChatR1 在 3B 和 7B 模型主干上均表现出色，在五个 CQA 数据集上的表现优于多个竞争模型，评估指标包括 F1、BERTScore 和以大语言模型作为评判者。我们纳入了多样化的 CQA 数据集，涵盖主题转换、意图演变、混合倡议对话以及多文档依据，从多个方面测试了 ChatR1 的性能。消融实验验证了意图感知奖励的有效性。进一步的分析还揭示了多样化的推理轨迹和对搜索工具的有效利用。ChatR1 在多个领域上也表现出良好的泛化能力，表明基于 RL 的推理能够实现比静态 CQA 流水线更为灵活和上下文敏感的行为。",
        "translated_title": "ChatR1：会话推理与检索增强问答的强化学习方法",
        "label": [
            "LLM生成式推荐",
            "序列推荐"
        ],
        "label_reason": "论文涉及对话式问答与生成式模型，适用于推荐中的序列建模和生成式推荐。",
        "relevance_score": 7
    },
    {
        "title": "Beyond Static LLM Policies: Imitation-Enhanced Reinforcement Learning\n  for Recommendation",
        "url": "http://arxiv.org/abs/2510.13229v1",
        "pub_date": "2025-10-15",
        "summary": "Recommender systems (RecSys) have become critical tools for enhancing user engagement by delivering personalized content across diverse digital platforms. Recent advancements in large language models (LLMs) demonstrate significant potential for improving RecSys, primarily due to their exceptional generalization capabilities and sophisticated contextual understanding, which facilitate the generation of flexible and interpretable recommendations. However, the direct deployment of LLMs as primary recommendation policies presents notable challenges, including persistent latency issues stemming from frequent API calls and inherent model limitations such as hallucinations and biases. To address these issues, this paper proposes a novel offline reinforcement learning (RL) framework that leverages imitation learning from LLM-generated trajectories. Specifically, inverse reinforcement learning is employed to extract robust reward models from LLM demonstrations. This approach negates the need for LLM fine-tuning, thereby substantially reducing computational overhead. Simultaneously, the RL policy is guided by the cumulative rewards derived from these demonstrations, effectively transferring the semantic insights captured by the LLM. Comprehensive experiments conducted on two benchmark datasets validate the effectiveness of the proposed method, demonstrating superior performance when compared against state-of-the-art RL-based and in-context learning baselines. The code can be found at https://github.com/ArronDZhang/IL-Rec.",
        "translated": "推荐系统（Recommender systems, RecSys）已成为在各种数字平台上提供个性化内容以提升用户参与度的关键工具。近年来，大语言模型（Large language models, LLMs）在推荐系统中的应用展现出显著潜力，主要归功于其出色的泛化能力和复杂上下文理解能力，这些能力有助于生成灵活且可解释的推荐结果。然而，直接将LLMs作为主要的推荐策略部署存在诸多挑战，包括由于频繁调用API而导致的持续性延迟问题，以及模型本身固有的局限性，如幻觉和偏见等。为了解决这些问题，本文提出了一种新颖的离线强化学习（Reinforcement learning, RL）框架，该框架通过模仿学习LLM生成的轨迹来实现。具体而言，采用逆强化学习（inverse reinforcement learning）方法从LLM的演示中提取稳健的奖励模型。这种方法避免了对LLM进行微调的需求，从而显著降低了计算开销。同时，强化学习策略通过这些演示所获得的累积奖励进行指导，有效地将LLM捕捉到的语义信息迁移过来。在两个基准数据集上进行的全面实验验证了所提方法的有效性，其性能优于最先进的基于RL的和上下文学习的基线方法。代码可在 https://github.com/ArronDZhang/IL-Rec 找到。",
        "translated_title": "超越静态大语言模型策略：基于模仿增强的推荐系统强化学习方法",
        "label": [
            "LLM生成式推荐",
            "精排",
            "通用推荐技术"
        ],
        "label_reason": "结合LLM与强化学习优化推荐策略，涉及生成式推荐和策略优化",
        "relevance_score": 9
    },
    {
        "title": "LLM-guided Hierarchical Retrieval",
        "url": "http://arxiv.org/abs/2510.13217v1",
        "pub_date": "2025-10-15",
        "summary": "Modern IR systems are increasingly tasked with answering complex, multi-faceted queries that require deep reasoning rather than simple keyword or semantic matching. While LLM-based IR has shown great promise, the prevailing retrieve-then-rerank paradigm inherits the limitations of embedding-based retrieval; parametric generative approaches are difficult to update with new information; and long-context methods that place the entire corpus in context are computationally infeasible for large document collections. To address these challenges, we introduce LATTICE, a hierarchical retrieval framework that enables an LLM to reason over and navigate large corpora with logarithmic search complexity by imposing a semantic tree structure on the corpus. Our approach consists of two stages: (1) an offline phase that organizes the corpus into a semantic hierarchy via either a bottom-up agglomerative strategy or a top-down divisive strategy using multi-level summaries and (2) an online traversal phase where a search LLM navigates this tree. A central challenge in such LLM-guided search is that the model's relevance judgments are noisy, context-dependent, and unaware of the hierarchy, making cross-branch and cross-level comparisons difficult. To overcome this, we propose a traversal algorithm that estimates calibrated latent relevance scores from local LLM outputs and aggregates them into a global path relevance metric. Our training-free framework achieves state-of-the-art zero-shot performance on the reasoning-intensive BRIGHT benchmark, demonstrating up to 9% improvement in Recall@100 and 5% in nDCG@10 over the next best zero-shot baseline. Furthermore, compared to the fine-tuned SOTA method DIVER-v2, LATTICE attains comparable results on BRIGHT subsets that use a static corpus for evaluation.",
        "translated": "现代推荐系统日益需要处理复杂的、多方面的查询，这些查询需要深入推理，而不仅仅是简单的关键词或语义匹配。尽管基于大语言模型（LLM）的推荐系统展现出了巨大潜力，但主流的先召回后重排范式继承了基于嵌入的召回方法的局限性；参数化的生成式方法难以通过新信息进行更新；而将整个语料库放入上下文中的长上下文方法在面对大规模文档集合时计算上不可行。为了解决这些挑战，我们引入了 LATTICE，一个分层召回框架，通过在语料库上施加语义树结构，使 LLM 能够以对数级搜索复杂度对大规模语料库进行推理和导航。我们的方法包含两个阶段：（1）一个离线阶段，通过自底向上的聚合策略或自顶向下的划分策略，利用多层级摘要将语料库组织成语义层次结构；（2）一个在线遍历阶段，其中搜索 LLM 遍历该树结构。在这种由 LLM 指导的搜索中，一个核心挑战是模型的相关性判断是嘈杂的、上下文依赖的，并且不了解层次结构，从而使得跨分支和跨层级的比较变得困难。为了解决这一问题，我们提出了一种遍历算法，该算法从局部 LLM 输出中估计校准后的隐相关性得分，并将其聚合为一个全局路径相关性指标。我们的无训练框架在推理密集型的 BRIGHT 基准上实现了最先进的零样本性能，其 Recall@100 和 nDCG@10 指标分别比次优的零样本基线提升了最高 9% 和 5%。此外，与微调后的最先进方法 DIVER-v2 相比，LATTICE 在使用静态语料库进行评估的 BRIGHT 子集上达到了可比的结果。",
        "translated_title": "LLM引导的层次化召回",
        "label": [
            "召回（Recall）",
            "多模态推荐（Multimodal Recommendation）",
            "LLM生成式推荐（LLM-based Generative Recommendation）"
        ],
        "label_reason": "论文提出基于LLM的分层召回框架，适用于信息检索并间接可用于推荐",
        "relevance_score": 7
    },
    {
        "title": "ReMindRAG: Low-Cost LLM-Guided Knowledge Graph Traversal for Efficient\n  RAG",
        "url": "http://arxiv.org/abs/2510.13193v1",
        "pub_date": "2025-10-15",
        "summary": "Knowledge graphs (KGs), with their structured representation capabilities, offer promising avenue for enhancing Retrieval Augmented Generation (RAG) systems, leading to the development of KG-RAG systems. Nevertheless, existing methods often struggle to achieve effective synergy between system effectiveness and cost efficiency, leading to neither unsatisfying performance nor excessive LLM prompt tokens and inference time. To this end, this paper proposes REMINDRAG, which employs an LLM-guided graph traversal featuring node exploration, node exploitation, and, most notably, memory replay, to improve both system effectiveness and cost efficiency. Specifically, REMINDRAG memorizes traversal experience within KG edge embeddings, mirroring the way LLMs \"memorize\" world knowledge within their parameters, but in a train-free manner. We theoretically and experimentally confirm the effectiveness of REMINDRAG, demonstrating its superiority over existing baselines across various benchmark datasets and LLM backbones. Our code is available at https://github.com/kilgrims/ReMindRAG.",
        "translated": "知识图谱（KGs）凭借其结构化表示能力，为增强检索增强生成（RAG）系统提供了有前景的路径，从而推动了KG-RAG系统的出现。然而，现有方法通常难以在系统效果和成本效率之间实现有效的协同，导致性能不佳或大语言模型（LLM）提示词数量和推理时间过多。为此，本文提出REMINDRAG，其采用了一种由LLM引导的图遍历方法，包含节点探索、节点利用，以及最重要的是记忆回放，从而同时提升系统的有效性和成本效率。具体而言，REMINDRAG在KG的边嵌入中记忆遍历经验，其方式类似于LLMs在其参数中“记忆”世界知识，但无需训练。我们从理论和实验两个方面验证了REMINDRAG的有效性，结果表明其在多种基准数据集和LLM主干模型上均优于现有基线方法。我们的代码可在 https://github.com/kilgrims/ReMindRAG 获取。",
        "translated_title": "ReMindRAG：高效RAG的低成本大语言模型引导的知识图谱遍历方法",
        "label": [
            "LLM生成式推荐",
            "多模态推荐"
        ],
        "label_reason": "论文涉及LLM引导的知识图谱遍历，适用于生成式推荐中的信息检索优化。",
        "relevance_score": 7
    },
    {
        "title": "Retrieval-in-the-Chain: Bootstrapping Large Language Models for\n  Generative Retrieval",
        "url": "http://arxiv.org/abs/2510.13095v1",
        "pub_date": "2025-10-15",
        "summary": "Generative retrieval (GR) is an emerging paradigm that leverages large language models (LLMs) to autoregressively generate document identifiers (docids) relevant to a given query. Prior works have focused on leveraging the generative capabilities of LLMs to improve GR, while overlooking that their reasoning capabilities could likewise help. This raises a key question: Can explicit reasoning benefit GR? To investigate, we first conduct a preliminary study where an LLM is prompted to generate free-form chain-of-thought (CoT) reasoning before performing constrained docid decoding. Although this method outperforms standard GR, the generated reasoning tends to be verbose and poorly aligned with the docid space. These limitations motivate the development of a reasoning mechanism better tailored to GR.   Therefore, we propose Reason-for-Retrieval (R4R), a reasoning-augmented framework for GR that converts free-form CoT reasoning into a compact, structured format, and iteratively refines the reasoning during the retrieval process. R4R augments an existing GR method by leveraging a reasoning-capable LLM that has been instruction-tuned for GR. At inference time, R4R first uses the LLM to generate an initial structured reasoning; then the same LLM alternates between (i) constrained decoding with the chosen GR method to produce candidate docids and (ii) updating the reasoning based on retrieval results to improve the next round. R4R does not require additional models or training, and instead a single LLM serves as both the reasoning generator and the retriever. Extensive experiments on Natural Questions, MS MARCO, and a real-world item-search benchmark validate the effectiveness of R4R.",
        "translated": "生成式召回（GR）是一种新兴范式，它利用大语言模型（LLM）对给定查询进行自回归地生成相关文档标识符（docids）。此前的研究主要关注于利用LLM的生成能力来提升GR，而忽略了其推理能力同样可以提供帮助。这引发了一个关键问题：显式的推理是否能提升GR？为了探究这一问题，我们首先进行了一项初步研究，其中LLM被提示在进行受限docids解码之前生成自由形式的思维链（CoT）推理。尽管这种方法优于标准的GR方法，但生成的推理内容往往冗长，并且与docid空间的对齐效果较差。这些限制促使我们开发一种更加契合GR的推理机制。因此，我们提出了Reason-for-Retrieval（R4R），这是一个增强推理能力的GR框架，它将自由形式的CoT推理转换为一种紧凑的结构化格式，并在召回过程中迭代地优化该推理。R4R通过使用为GR指令调优的具有推理能力的LLM，来增强现有的GR方法。在推理阶段，R4R首先使用LLM生成初始的结构化推理；然后，相同的LLM交替执行以下两个步骤：（i）利用选定的GR方法进行受限解码以生成候选docids，以及（ii）根据召回结果更新推理内容以优化下一轮生成。R4R不需要额外的模型或训练，而是通过单一LLM同时担任推理生成器和召回器的角色。在Natural Questions、MS MARCO和一个实际的物料-搜索基准数据集上的大量实验验证了R4R的有效性。",
        "translated_title": "链中召回：通过引导大语言模型实现生成式召回",
        "label": [
            "LLM生成式推荐",
            "召回"
        ],
        "label_reason": "论文提出基于LLM的生成式检索框架，与推荐系统召回环节密切相关。",
        "relevance_score": 8
    },
    {
        "title": "Post-hoc Popularity Bias Correction in GNN-based Collaborative Filtering",
        "url": "http://arxiv.org/abs/2510.12959v1",
        "pub_date": "2025-10-14",
        "summary": "User historical interaction data is the primary signal for learning user preferences in collaborative filtering (CF). However, the training data often exhibits a long-tailed distribution, where only a few items have the majority of interactions. CF models trained directly on such imbalanced data are prone to learning popularity bias, which reduces personalization and leads to suboptimal recommendation quality. Graph Neural Networks (GNNs), while effective for CF due to their message passing mechanism, can further propagate and amplify popularity bias through their aggregation process. Existing approaches typically address popularity bias by modifying training objectives but fail to directly counteract the bias propagated during GNN's neighborhood aggregation. Applying weights to interactions during aggregation can help alleviate this problem, yet it risks distorting model learning due to unstable node representations in the early stages of training. In this paper, we propose a Post-hoc Popularity Debiasing (PPD) method that corrects for popularity bias in GNN-based CF and operates directly on pre-trained embeddings without requiring retraining. By estimating interaction-level popularity and removing popularity components from node representations via a popularity direction vector, PPD reduces bias while preserving user preferences. Experimental results show that our method outperforms state-of-the-art approaches for popularity bias correction in GNN-based CF.",
        "translated": "用户的历史交互数据是协同过滤（CF）中学习用户/物料偏好的主要信号。然而，训练数据通常表现出长尾分布，其中只有少数物料获得了大部分的交互。直接在这样的不平衡数据上训练的CF模型容易学习到流行度偏差，从而降低个性化程度并导致推荐质量下降。图神经网络（GNN）由于其消息传递机制在CF中表现有效，但它们的聚合过程可能会进一步传播和放大流行度偏差。现有方法通常通过修改训练目标来应对流行度偏差，但未能直接对抗GNN在邻域聚合过程中传播的偏差。在聚合过程中对交互加权有助于缓解这一问题，然而由于训练早期阶段节点表示的不稳定性，这种加权可能会扭曲模型的学习。本文提出了一种后处理流行度去偏（Post-hoc Popularity Debiasing, PPD）方法，该方法无需重新训练，即可直接在预训练的嵌入上进行操作，对基于GNN的CF中的流行度偏差进行校正。通过估计交互层面的流行度，并借助一个流行度方向向量从节点表示中去除流行度成分，PPD在减少偏差的同时保留了用户偏好。实验结果表明，我们的方法在基于GNN的CF中流行度偏差校正方面优于当前最先进的方法。",
        "translated_title": "基于图神经网络的协同过滤中的后处理流行度偏差校正",
        "label": [
            "图神经网络推荐（GNN for Recommendation）",
            "通用推荐技术（General Recommendation Techniques）"
        ],
        "label_reason": "论文针对GNN在推荐中的流行度偏差问题提出后处理方法，属于图神经网络推荐改进技术",
        "relevance_score": 8
    },
    {
        "title": "Universal Image Restoration Pre-training via Masked Degradation\n  Classification",
        "url": "http://arxiv.org/abs/2510.13282v1",
        "pub_date": "2025-10-15",
        "summary": "This study introduces a Masked Degradation Classification Pre-Training method (MaskDCPT), designed to facilitate the classification of degradation types in input images, leading to comprehensive image restoration pre-training. Unlike conventional pre-training methods, MaskDCPT uses the degradation type of the image as an extremely weak supervision, while simultaneously leveraging the image reconstruction to enhance performance and robustness. MaskDCPT includes an encoder and two decoders: the encoder extracts features from the masked low-quality input image. The classification decoder uses these features to identify the degradation type, whereas the reconstruction decoder aims to reconstruct a corresponding high-quality image. This design allows the pre-training to benefit from both masked image modeling and contrastive learning, resulting in a generalized representation suited for restoration tasks. Benefit from the straightforward yet potent MaskDCPT, the pre-trained encoder can be used to address universal image restoration and achieve outstanding performance. Implementing MaskDCPT significantly improves performance for both convolution neural networks (CNNs) and Transformers, with a minimum increase in PSNR of 3.77 dB in the 5D all-in-one restoration task and a 34.8% reduction in PIQE compared to baseline in real-world degradation scenarios. It also emergences strong generalization to previously unseen degradation types and levels. In addition, we curate and release the UIR-2.5M dataset, which includes 2.5 million paired restoration samples across 19 degradation types and over 200 degradation levels, incorporating both synthetic and real-world data. The dataset, source code, and models are available at https://github.com/MILab-PKU/MaskDCPT.",
        "translated": "本研究提出了一种 Masked Degradation Classification Pre-Training 方法（MaskDCPT），旨在实现对输入图像退化类型的分类，从而促进通用图像恢复的预训练。不同于传统的预训练方法，MaskDCPT 将图像的退化类型作为极其弱的监督信号，同时利用图像重建来提升性能和鲁棒性。MaskDCPT 包含一个编码器和两个解码器：编码器从被遮蔽的低质量输入图像中提取特征；分类解码器利用这些特征识别退化类型，而重建解码器则致力于重建对应的高质量图像。这种设计使预训练能够同时受益于遮蔽图像建模和对比学习，从而获得适用于图像恢复任务的通用表征。借助简洁而有效的 MaskDCPT，预训练的编码器可用于解决通用图像恢复问题，并取得优异的性能。MaskDCPT 的实现显著提升了卷积神经网络（CNNs）和 Transformers 的性能，在 5D 全合一图像恢复任务中，PSNR 至少提升了 3.77 dB，而在真实退化场景中，与基线相比，PIQE 降低了 34.8%。此外，该方法在未见过的退化类型和程度上也展现出强大的泛化能力。同时，我们整理并发布了 UIR-2.5M 数据集，该数据集包含 250 万对覆盖 19 种退化类型和 200 多个退化等级的图像恢复样本，融合了合成数据和真实数据。数据集、源代码和模型均可在 https://github.com/MILab-PKU/MaskDCPT 获取。",
        "translated_title": "通过掩码退化分类的通用图像恢复预训练",
        "label": [
            "图像恢复（Image Restoration）",
            "图像去噪（Image Denoising）",
            "图像去雨（Image Deraining）",
            "图像去雾（Image Dehazing）",
            "图像去模糊（Image Deblurring）",
            "超分辨率（Super-Resolution）",
            "图像去 JPEG 伪影（JPEG Artifact Removal）"
        ],
        "label_reason": "论文提出通用图像恢复预训练方法，适用于多种低级图像恢复任务",
        "relevance_score": 9
    },
    {
        "title": "PhysMaster: Mastering Physical Representation for Video Generation via\n  Reinforcement Learning",
        "url": "http://arxiv.org/abs/2510.13809v1",
        "pub_date": "2025-10-15",
        "summary": "Video generation models nowadays are capable of generating visually realistic videos, but often fail to adhere to physical laws, limiting their ability to generate physically plausible videos and serve as ''world models''. To address this issue, we propose PhysMaster, which captures physical knowledge as a representation for guiding video generation models to enhance their physics-awareness. Specifically, PhysMaster is based on the image-to-video task where the model is expected to predict physically plausible dynamics from the input image. Since the input image provides physical priors like relative positions and potential interactions of objects in the scenario, we devise PhysEncoder to encode physical information from it as an extra condition to inject physical knowledge into the video generation process. The lack of proper supervision on the model's physical performance beyond mere appearance motivates PhysEncoder to apply reinforcement learning with human feedback to physical representation learning, which leverages feedback from generation models to optimize physical representations with Direct Preference Optimization (DPO) in an end-to-end manner. PhysMaster provides a feasible solution for improving physics-awareness of PhysEncoder and thus of video generation, proving its ability on a simple proxy task and generalizability to wide-ranging physical scenarios. This implies that our PhysMaster, which unifies solutions for various physical processes via representation learning in the reinforcement learning paradigm, can act as a generic and plug-in solution for physics-aware video generation and broader applications.",
        "translated": "当前的视频生成模型虽然能够生成视觉上逼真的视频，但往往未能遵循物理规律，这限制了它们生成物理合理视频的能力，也限制了其作为“世界模型”的潜力。为了解决这一问题，我们提出了PhysMaster，它将物理知识捕获为表示形式，用于引导视频生成模型，从而增强其对物理规律的感知能力。具体而言，PhysMaster基于图像到视频的任务，模型期望从输入图像中预测物理上合理的动态过程。由于输入图像提供了诸如场景中物体相对位置和潜在交互等物理先验，我们设计了PhysEncoder，从输入图像中编码物理信息作为额外的条件，将其注入视频生成过程中。由于缺乏对模型物理性能（除了外观）的有效监督，PhysEncoder采用基于人类反馈的强化学习来实现物理表示学习，通过生成模型的反馈，以端到端的方式使用直接偏好优化（Direct Preference Optimization, DPO）来优化物理表示。PhysMaster为提高PhysEncoder以及视频生成的物理感知能力提供了一种可行的解决方案，并在简单代理任务上验证了其能力，同时展示了其在广泛物理场景中的泛化性能。这表明，我们提出的PhysMaster通过在强化学习范式下的表示学习，统一了解决各种物理过程的方法，可以作为一种通用且可插拔的解决方案，应用于物理感知视频生成及相关更广泛的领域。",
        "translated_title": "PhysMaster：通过强化学习掌握物理表示用于视频生成",
        "label": [],
        "label_reason": "论文聚焦视频生成而非图像像素级复原，属于high-level任务",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "结合强化学习与物理表示，有一定创新但非图像恢复核心领域"
    },
    {
        "title": "VisCoP: Visual Probing for Video Domain Adaptation of Vision Language\n  Models",
        "url": "http://arxiv.org/abs/2510.13808v1",
        "pub_date": "2025-10-15",
        "summary": "Large Vision-Language Models (VLMs) excel at general visual reasoning tasks but exhibit sharp performance degradation when applied to novel domains with substantial distribution shifts from pretraining data. Existing domain adaptation approaches finetune different VLM components, but this often results in limited domain-specific feature learning or catastrophic forgetting of prior capabilities. To address these issues, we introduce Vision Contextualized Probing (VisCoP), which augments the VLM's vision encoder with a compact set of learnable visual probes. These probes enable efficient domain-specific adaptation with minimal modification to pretrained parameters. We evaluate VisCoP across three challenging domain adaptation settings-cross-view (exocentric to egocentric), cross-modal (RGB to depth), and cross-task (human understanding to robot control). Experiments show that VisCoP consistently outperforms existing adaptation strategies, achieving superior performance on target domains while effectively retaining source-domain knowledge.",
        "translated": "大型视觉-语言模型（VLMs）在通用视觉推理任务中表现出色，但当应用于与预训练数据存在显著分布差异的新领域时，其性能会急剧下降。现有的领域适应方法通常对VLM的不同组件进行微调，但这往往导致领域特定特征学习受限，或者对先前能力产生灾难性遗忘。为了解决这些问题，我们提出了视觉上下文化探针（Vision Contextualized Probing, VisCoP），该方法通过在VLM的视觉编码器中引入一组紧凑的可学习视觉探针来增强模型。这些探针能够在对预训练参数仅作最小修改的情况下实现高效的领域特定适应。我们在三种具有挑战性的领域适应设置中评估了VisCoP，包括跨视角（外视角到自视角）、跨模态（RGB到深度）和跨任务（人类理解到机器人控制）。实验表明，VisCoP始终优于现有适应策略，在目标领域上取得了更优的性能，同时有效保留了源领域的知识。",
        "translated_title": "VisCoP：用于视觉语言模型视频域适应的视觉探测",
        "label": [],
        "label_reason": "论文聚焦于视觉语言模型的视频领域适配，不直接涉及图像像素级恢复或增强",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出视觉探针机制，对现有领域适配方法有一定改进"
    },
    {
        "title": "Generative Universal Verifier as Multimodal Meta-Reasoner",
        "url": "http://arxiv.org/abs/2510.13804v1",
        "pub_date": "2025-10-15",
        "summary": "We introduce Generative Universal Verifier, a novel concept and plugin designed for next-generation multimodal reasoning in vision-language models and unified multimodal models, providing the fundamental capability of reflection and refinement on visual outcomes during the reasoning and generation process. This work makes three main contributions: (1) We build ViVerBench, a comprehensive benchmark spanning 16 categories of critical tasks for evaluating visual outcomes in multimodal reasoning. Results show that existing VLMs consistently underperform across these tasks, underscoring a substantial gap from human-level capability in reliable visual verification. (2) We design two automated pipelines to construct large-scale visual verification data and train OmniVerifier-7B, the first omni-capable generative verifier trained for universal visual verification and achieves notable gains on ViVerBench(+8.3). Through training, we identify three atomic capabilities in visual verification and demonstrate how they generalize and interact synergistically. (3) We propose OmniVerifier-TTS, a sequential test-time scaling paradigm that leverages the universal verifier to bridge image generation and editing within unified models, enhancing the upper bound of generative ability through iterative fine-grained optimization. Beyond generation, we extend universal verifier to broader world-modeling interleaved reasoning scenarios. Empirically, OmniVerifier-TTS achieves improvements on T2I-ReasonBench(+3.7), and GenEval++(+4.3), outperforming existing parallel test-time scaling methods, such as Best-of-N. By endowing multimodal reasoning with reliable visual verification, OmniVerifier advances both reliable reflection during generation and scalable test-time refinement, marking a step toward more trustworthy and controllable next-generation reasoning systems.",
        "translated": "我们提出生成式通用验证器（Generative Universal Verifier），这是一种面向下一代视觉语言模型和统一多模态模型的新型概念和插件，旨在提供推理和生成过程中对视觉结果进行反思和优化的基本能力。本研究做出以下三项主要贡献：(1) 我们构建了 ViVerBench，一个全面的基准测试集，涵盖16类关键任务，用于评估多模态推理中视觉结果的性能。结果显示，现有视觉语言模型（VLMs）在这些任务中表现普遍不佳，表明其在可靠视觉验证方面与人类水平之间存在显著差距。(2) 我们设计了两条自动化流水线用于构建大规模的视觉验证数据，并训练了 OmniVerifier-7B，这是首个面向通用视觉验证的全能型生成式验证器。在 ViVerBench 上，OmniVerifier-7B 取得了显著的性能提升（+8.3）。通过训练，我们识别出视觉验证中的三个基本能力，并展示了它们如何在不同任务中泛化并协同工作。(3) 我们提出了 OmniVerifier-TTS，一种顺序式测试时扩展范式，利用通用验证器在统一模型中实现图像生成和编辑的桥梁作用，并通过迭代细粒度优化提升生成能力的上限。除了生成任务，我们还将通用验证器扩展到更广泛的交互式世界建模推理场景。实证表明，OmniVerifier-TTS 在 T2I-ReasonBench（+3.7）和 GenEval++（+4.3）上均取得性能提升，并优于现有的并行式测试时扩展方法，如 Best-of-N。通过为多模态推理赋予可靠的视觉验证能力，OmniVerifier 推动了生成过程中的可靠反思以及可扩展的测试时优化，标志着我们向更加可信和可控的下一代推理系统迈出了关键一步。",
        "translated_title": "生成式通用验证器作为多模态元推理器",
        "label": [],
        "label_reason": "论文主要关注视觉-语言模型的推理验证，非图像像素级处理",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出通用验证框架和新范式，但未突破low-level图像处理核心技术"
    },
    {
        "title": "Trace Anything: Representing Any Video in 4D via Trajectory Fields",
        "url": "http://arxiv.org/abs/2510.13802v1",
        "pub_date": "2025-10-15",
        "summary": "Effective spatio-temporal representation is fundamental to modeling, understanding, and predicting dynamics in videos. The atomic unit of a video, the pixel, traces a continuous 3D trajectory over time, serving as the primitive element of dynamics. Based on this principle, we propose representing any video as a Trajectory Field: a dense mapping that assigns a continuous 3D trajectory function of time to each pixel in every frame. With this representation, we introduce Trace Anything, a neural network that predicts the entire trajectory field in a single feed-forward pass. Specifically, for each pixel in each frame, our model predicts a set of control points that parameterizes a trajectory (i.e., a B-spline), yielding its 3D position at arbitrary query time instants. We trained the Trace Anything model on large-scale 4D data, including data from our new platform, and our experiments demonstrate that: (i) Trace Anything achieves state-of-the-art performance on our new benchmark for trajectory field estimation and performs competitively on established point-tracking benchmarks; (ii) it offers significant efficiency gains thanks to its one-pass paradigm, without requiring iterative optimization or auxiliary estimators; and (iii) it exhibits emergent abilities, including goal-conditioned manipulation, motion forecasting, and spatio-temporal fusion. Project page: https://trace-anything.github.io/.",
        "translated": "有效的时空表示是建模、理解和预测视频中动态过程的基础。视频的基本单位是像素，其在时间上追踪一个连续的3D轨迹，作为动态的基本元素。基于这一原理，我们提出将任何视频表示为轨迹场（Trajectory Field）：一种密集映射，为每一帧中的每个像素分配一个关于时间的连续3D轨迹函数。通过这种表示，我们引入了Trace Anything，一种神经网络模型，它能够在一次前向传播中预测整个轨迹场。具体来说，对于每一帧中的每个像素，我们的模型预测一组控制点，以参数化轨迹（即B样条），从而在任意查询时刻获得其3D位置。我们在大规模4D数据上训练了Trace Anything模型，包括我们新平台的数据，实验结果表明：(i) Trace Anything在我们新的轨迹场估计基准上取得了最先进的性能，并在已有的点跟踪基准上表现出竞争力；(ii) 由于其单次前向传播的范式，它在效率方面有显著提升，无需进行迭代优化或使用辅助估计器；(iii) 它展现出一些新兴能力，包括目标条件下的操控、运动预测以及时空融合。项目主页：https://trace-anything.github.io/。",
        "translated_title": "追踪任何内容：通过轨迹场在4D中表示任何视频",
        "label": [],
        "label_reason": "论文关注视频轨迹建模，不属于图像像素级恢复或增强任务。",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出轨迹场表示和一次性预测框架，具有新颖性。"
    },
    {
        "title": "Reasoning in Space via Grounding in the World",
        "url": "http://arxiv.org/abs/2510.13800v1",
        "pub_date": "2025-10-15",
        "summary": "In this paper, we claim that 3D visual grounding is the cornerstone of spatial reasoning and introduce the Grounded-Spatial Reasoner (GS-Reasoner) to explore the effective spatial representations that bridge the gap between them. Existing 3D LLMs suffer from the absence of a unified 3D representation capable of jointly capturing semantic and geometric information. This deficiency is manifested either in poor performance on grounding or in an excessive reliance on external modules, ultimately hindering the seamless integration of grounding and spatial reasoning. To address this, we propose a simple yet effective dual-path pooling mechanism that tightly aligns geometric features with both semantic and positional cues, constructing a unified image patch-based 3D representation that encapsulates all essential information without increasing the number of input tokens. Leveraging this holistic representation, GS-Reasoner is the first 3D LLM that achieves autoregressive grounding entirely without external modules while delivering performance comparable to state-of-the-art models, establishing a unified and self-contained framework for 3D spatial reasoning. To further bridge grounding and spatial reasoning, we introduce the Grounded Chain-of-Thought (GCoT) dataset. This dataset is meticulously curated to include both 3D bounding box annotations for objects referenced in reasoning questions and step-by-step reasoning paths that integrate grounding as a core component of the problem-solving process. Extensive experiments demonstrate that GS-Reasoner achieves impressive results on 3D visual grounding, which in turn significantly enhances its spatial reasoning capabilities, leading to state-of-the-art performance.",
        "translated": "本文提出观点认为，三维视觉定位是空间推理的基础，并引入了 GS-Reasoner（Grounded-Spatial Reasoner）以探索能够弥合二者之间差距的有效空间表示。现有的三维大语言模型（LLM）面临缺乏统一的三维表示的问题，该表示无法同时捕捉语义和几何信息。这种缺陷体现在定位任务上的性能较差，或对外部模块的过度依赖，最终阻碍了定位与空间推理的无缝融合。为了解决这一问题，我们提出了一种简单而有效的同时路径池化机制，该机制紧密对齐几何特征与语义和位置线索，构建了一种统一的、基于图像块的三维表示形式，包含所有必要信息且不增加输入 token 的数量。借助这种整体表示，GS-Reasoner 是首个在不使用任何外部模块的情况下实现自回归定位的三维大语言模型，其性能可与最先进的模型相媲美，为三维空间推理提供了一个统一且自洽的框架。为进一步弥合定位与空间推理之间的联系，我们引入了 Grounded Chain-of-Thought（GCoT）数据集。该数据集经过精心构建，包含了推理问题中所提到对象的三维边界框标注，以及将定位作为解决问题核心步骤的逐步推理路径。大量实验表明，GS-Reasoner 在三维视觉定位任务中取得了令人印象深刻的结果，从而显著提升了其空间推理能力，达到当前最先进的性能水平。",
        "translated_title": "通过在世界中的锚定实现空间推理",
        "label": [],
        "label_reason": "论文聚焦3D视觉理解和推理，不涉及图像像素级恢复或增强",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "提出了一种新颖的3D表示方法和数据集，但不针对低层图像处理任务"
    },
    {
        "title": "The Mechanistic Emergence of Symbol Grounding in Language Models",
        "url": "http://arxiv.org/abs/2510.13796v1",
        "pub_date": "2025-10-15",
        "summary": "Symbol grounding (Harnad, 1990) describes how symbols such as words acquire their meanings by connecting to real-world sensorimotor experiences. Recent work has shown preliminary evidence that grounding may emerge in (vision-)language models trained at scale without using explicit grounding objectives. Yet, the specific loci of this emergence and the mechanisms that drive it remain largely unexplored. To address this problem, we introduce a controlled evaluation framework that systematically traces how symbol grounding arises within the internal computations through mechanistic and causal analysis. Our findings show that grounding concentrates in middle-layer computations and is implemented through the aggregate mechanism, where attention heads aggregate the environmental ground to support the prediction of linguistic forms. This phenomenon replicates in multimodal dialogue and across architectures (Transformers and state-space models), but not in unidirectional LSTMs. Our results provide behavioral and mechanistic evidence that symbol grounding can emerge in language models, with practical implications for predicting and potentially controlling the reliability of generation.",
        "translated": "符号根基（Symbol grounding）（Harnad, 1990）描述了符号（如词语）如何通过与现实世界中的感知和运动体验相连接而获得其含义。最近的研究表明，在大规模训练的（视觉-）语言模型中，即使不使用显式的根基目标，符号根基也可能自发出现。然而，这种现象的具体出现位置及其驱动机制仍鲜有研究。为了解决这一问题，我们引入了一种可控的评估框架，系统地追踪符号根基在内部计算过程中的产生方式，并通过机制和因果分析进行研究。我们的研究结果表明，符号根基主要集中在中间层计算中，并通过聚合机制实现，其中注意力头聚合环境根基以支持语言形式的预测。这一现象在多模态对话中以及不同架构（Transformer 和状态空间模型）中均出现，但在单向 LSTM 中未观察到。我们的研究提供了行为和机制上的证据，证明符号根基可以在语言模型中自发产生，这对于预测和可能控制生成的可靠性具有实际意义。",
        "translated_title": "语言模型中符号 grounding 的机制性出现",
        "label": [],
        "label_reason": "论文聚焦语言模型的符号接地机制，不涉及图像像素级处理。",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "提出了符号接地的机制分析框架，有一定理论创新但非视觉任务相关。"
    },
    {
        "title": "Bee: A High-Quality Corpus and Full-Stack Suite to Unlock Advanced Fully\n  Open MLLMs",
        "url": "http://arxiv.org/abs/2510.13795v1",
        "pub_date": "2025-10-15",
        "summary": "Fully open multimodal large language models (MLLMs) currently lag behind proprietary counterparts, primarily due to a significant gap in data quality for supervised fine-tuning (SFT). Existing open-source datasets are often plagued by widespread noise and a critical deficit in complex reasoning data, such as Chain-of-Thought (CoT), which hinders the development of advanced model capabilities. Addressing these challenges, our work makes three primary contributions. First, we introduce Honey-Data-15M, a new SFT dataset comprising approximately 15 million QA pairs, processed through multiple cleaning techniques and enhanced with a novel dual-level (short and long) CoT enrichment strategy. Second, we introduce HoneyPipe, the data curation pipeline, and its underlying framework DataStudio, providing the community with a transparent and adaptable methodology for data curation that moves beyond static dataset releases. Finally, to validate our dataset and pipeline, we train Bee-8B, an 8B model on Honey-Data-15M. Experiments show that Bee-8B establishes a new state-of-the-art (SOTA) for fully open MLLMs, achieving performance that is competitive with, and in some cases surpasses, recent semi-open models such as InternVL3.5-8B. Our work delivers to the community a suite of foundational resources, including: the Honey-Data-15M corpus; the full-stack suite comprising HoneyPipe and DataStudio; training recipes; an evaluation harness; and the model weights. This effort demonstrates that a principled focus on data quality is a key pathway to developing fully open MLLMs that are highly competitive with their semi-open counterparts.",
        "translated": "目前，完全开源的多模态大语言模型（MLLMs）在性能上落后于专有模型，主要原因在于用于监督微调（SFT）的数据质量存在显著差距。现有的开源数据集通常受到广泛噪声的困扰，并且在复杂推理数据（如思维链（CoT））方面存在严重不足，这阻碍了先进模型能力的发展。为了解决这些挑战，我们的工作主要包括三个方面的贡献。首先，我们引入了 Honey-Data-15M，一个包含约 1500 万个问答对的新 SFT 数据集，通过多种数据清洗技术处理，并采用了一种新的双层（短链和长链）CoT 增强策略进行优化。其次，我们提出了 HoneyPipe 数据整理流水线及其底层框架 DataStudio，为社区提供一种透明且可调节的数据整理方法，突破了传统静态数据集发布的方式。最后，为了验证我们的数据集和流水线效果，我们基于 Honey-Data-15M 训练了 Bee-8B，一个 80 亿参数规模的模型。实验结果表明，Bee-8B 在完全开源 MLLMs 中达到了新的最先进水平（SOTA），其性能与近期的半开源模型（如 InternVL3.5-8B）相比具有竞争力，某些情况下甚至优于后者。我们的工作为社区提供了一套基础资源，包括：Honey-Data-15M 数据语料库；涵盖 HoneyPipe 和 DataStudio 的全栈工具套件；训练配方；评估框架；以及模型权重。这项研究表明，专注于数据质量的原则性方法是开发与半开源模型高度竞争的完全开源 MLLMs 的关键路径。",
        "translated_title": "Bee：一个高质量语料库和全栈套件，用于解锁先进的全开放大语言模型",
        "label": [],
        "label_reason": "论文聚焦多模态大语言模型数据构建，不涉及图像像素级处理",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "提出新数据集和训练方法，但属于通用MLLM改进而非视觉低级任务创新"
    },
    {
        "title": "NoisePrints: Distortion-Free Watermarks for Authorship in Private\n  Diffusion Models",
        "url": "http://arxiv.org/abs/2510.13793v1",
        "pub_date": "2025-10-15",
        "summary": "With the rapid adoption of diffusion models for visual content generation, proving authorship and protecting copyright have become critical. This challenge is particularly important when model owners keep their models private and may be unwilling or unable to handle authorship issues, making third-party verification essential. A natural solution is to embed watermarks for later verification. However, existing methods require access to model weights and rely on computationally heavy procedures, rendering them impractical and non-scalable. To address these challenges, we propose , a lightweight watermarking scheme that utilizes the random seed used to initialize the diffusion process as a proof of authorship without modifying the generation process. Our key observation is that the initial noise derived from a seed is highly correlated with the generated visual content. By incorporating a hash function into the noise sampling process, we further ensure that recovering a valid seed from the content is infeasible. We also show that sampling an alternative seed that passes verification is infeasible, and demonstrate the robustness of our method under various manipulations. Finally, we show how to use cryptographic zero-knowledge proofs to prove ownership without revealing the seed. By keeping the seed secret, we increase the difficulty of watermark removal. In our experiments, we validate NoisePrints on multiple state-of-the-art diffusion models for images and videos, demonstrating efficient verification using only the seed and output, without requiring access to model weights.",
        "translated": "随着扩散模型在视觉内容生成中的迅速应用，证明作者身份和保护版权变得至关重要。当模型所有者保持其模型私有时，这一挑战尤为突出，他们可能不愿或无法处理作者身份问题，从而使得第三方验证成为必要。一种自然的解决方案是嵌入水印以供后续验证。然而，现有方法需要访问模型权重，并依赖计算量大的过程，使其在实践中不可行且难以扩展。为了解决这些挑战，我们提出了一种轻量级的水印方案，该方案利用用于初始化扩散过程的随机种子作为作者身份的证明，且无需修改生成过程。我们的关键观察是，由种子生成的初始噪声与生成的视觉内容具有高度相关性。通过在噪声采样过程中引入哈希函数，我们进一步确保了从内容中恢复有效种子在计算上不可行。我们还证明，采样一个能通过验证的替代种子在计算上也是不可行的，并展示了我们的方法在各种篡改下的鲁棒性。最后，我们展示了如何使用密码学中的零知识证明来在不泄露种子的前提下证明所有权。通过保密种子，我们提高了水印移除的难度。在我们的实验中，我们在多个最先进的图像和视频扩散模型上验证了 NoisePrints 的有效性，展示了仅使用种子和输出即可实现高效的验证，而无需访问模型权重。",
        "translated_title": "NoisePrints: 隐私扩散模型中的无损水印用于作者归属",
        "label": [],
        "label_reason": "论文聚焦扩散模型水印而非图像像素级恢复",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出无需模型权重的轻量水印方案，有一定创新"
    },
    {
        "title": "Adaptive Visual Conditioning for Semantic Consistency in Diffusion-Based\n  Story Continuation",
        "url": "http://arxiv.org/abs/2510.13787v1",
        "pub_date": "2025-10-15",
        "summary": "Story continuation focuses on generating the next image in a narrative sequence so that it remains coherent with both the ongoing text description and the previously observed images. A central challenge in this setting lies in utilizing prior visual context effectively, while ensuring semantic alignment with the current textual input. In this work, we introduce AVC (Adaptive Visual Conditioning), a framework for diffusion-based story continuation. AVC employs the CLIP model to retrieve the most semantically aligned image from previous frames. Crucially, when no sufficiently relevant image is found, AVC adaptively restricts the influence of prior visuals to only the early stages of the diffusion process. This enables the model to exploit visual context when beneficial, while avoiding the injection of misleading or irrelevant information. Furthermore, we improve data quality by re-captioning a noisy dataset using large language models, thereby strengthening textual supervision and semantic alignment. Quantitative results and human evaluations demonstrate that AVC achieves superior coherence, semantic consistency, and visual fidelity compared to strong baselines, particularly in challenging cases where prior visuals conflict with the current input.",
        "translated": "故事延续关注于生成叙事序列中的下一幅图像，使其与当前的文本描述和之前已观测的图像保持连贯。在这种设置下的一个核心挑战在于如何有效地利用先前的视觉上下文，同时确保与当前文本输入在语义上的一致性。在本文中，我们提出了 AVC（Adaptive Visual Conditioning），一种基于扩散模型的故事延续框架。AVC 利用 CLIP 模型从先前的帧中检索语义最相关的一幅图像。关键的是，当未找到足够相关的图像时，AVC 会自适应地将先前视觉信息的影响限制在扩散过程的早期阶段。这使得模型在有益时能够有效利用视觉上下文，同时避免注入误导性或不相关的信息。此外，我们通过使用大语言模型对噪声数据集进行重新描述，提高了数据质量，从而增强了文本监督和语义一致性。定量结果和人类评估表明，与强大的基线模型相比，AVC 在连贯性、语义一致性和视觉保真度方面均取得了优越的性能，特别是在先前视觉信息与当前输入冲突的具有挑战性的情况下。",
        "translated_title": "基于扩散的故事情节延续中的自适应视觉条件化以保持语义一致性",
        "label": [],
        "label_reason": "论文关注文本驱动的扩散模型故事生成，非像素级图像恢复任务。",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出自适应视觉条件框架，改进文本-图像对齐方法，但创新点较常规。"
    },
    {
        "title": "InternVLA-M1: A Spatially Guided Vision-Language-Action Framework for\n  Generalist Robot Policy",
        "url": "http://arxiv.org/abs/2510.13778v1",
        "pub_date": "2025-10-15",
        "summary": "We introduce InternVLA-M1, a unified framework for spatial grounding and robot control that advances instruction-following robots toward scalable, general-purpose intelligence. Its core idea is spatially guided vision-language-action training, where spatial grounding serves as the critical link between instructions and robot actions. InternVLA-M1 employs a two-stage pipeline: (i) spatial grounding pre-training on over 2.3M spatial reasoning data to determine ``where to act'' by aligning instructions with visual, embodiment-agnostic positions, and (ii) spatially guided action post-training to decide ``how to act'' by generating embodiment-aware actions through plug-and-play spatial prompting. This spatially guided training recipe yields consistent gains: InternVLA-M1 outperforms its variant without spatial guidance by +14.6% on SimplerEnv Google Robot, +17% on WidowX, and +4.3% on LIBERO Franka, while demonstrating stronger spatial reasoning capability in box, point, and trace prediction. To further scale instruction following, we built a simulation engine to collect 244K generalizable pick-and-place episodes, enabling a 6.2% average improvement across 200 tasks and 3K+ objects. In real-world clustered pick-and-place, InternVLA-M1 improved by 7.3%, and with synthetic co-training, achieved +20.6% on unseen objects and novel configurations. Moreover, in long-horizon reasoning-intensive scenarios, it surpassed existing works by over 10%. These results highlight spatially guided training as a unifying principle for scalable and resilient generalist robots. Code and models are available at https://github.com/InternRobotics/InternVLA-M1.",
        "translated": "我们引入了 InternVLA-M1，这是一个面向空间定位与机器人控制的统一框架，推动指令跟随机器人向可扩展、通用型智能发展。其核心思想是空间引导的视觉-语言-动作训练，其中空间定位作为连接指令与机器人动作的关键环节。InternVLA-M1 采用两阶段训练流程：(i) 在超过 2.3 万个空间推理数据上进行空间定位预训练，通过将指令与视觉、与实体无关的空间位置对齐来确定 ``在何处执行动作''，以及 (ii) 通过即插即用的空间提示生成与实体相关的动作，进行空间引导的动作后训练以决定 ``如何执行动作''。这种空间引导的训练方法带来了显著的提升：在 SimplierEnv Google Robot 上，InternVLA-M1 相较于其无空间引导的变体提升了 +14.6%，在 WidowX 上提升了 +17%，在 LIBERO Franka 上提升了 +4.3%，同时在盒预测、点预测和轨迹预测中表现出更强的空间推理能力。为了进一步扩展指令跟随的能力，我们构建了一个仿真引擎，收集了 244K 个具有泛化能力的抓取与放置场景，使得在 200 个任务和 3K+ 个物体上的平均性能提升了 6.2%。在现实世界中的集群抓取与放置任务中，InternVLA-M1 提升了 7.3%，而在结合合成数据的联合训练中，对于未见过的物体和新配置，性能提升达 +20.6%。此外，在需要长期推理的复杂场景中，其性能超越现有工作超过 10%。这些结果表明，空间引导的训练方法可作为构建可扩展且鲁棒的通用型机器人的统一原则。代码和模型可在 https://github.com/InternRobotics/InternVLA-M1 获取。",
        "translated_title": "InternVLA-M1：一种空间引导的视觉-语言-动作框架  \n用于通用机器人策略",
        "label": [],
        "label_reason": "论文聚焦机器人控制而非图像像素级恢复或增强",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出空间引导训练方法，对指令跟随机器人有改进"
    },
    {
        "title": "UrbanFusion: Stochastic Multimodal Fusion for Contrastive Learning of\n  Robust Spatial Representations",
        "url": "http://arxiv.org/abs/2510.13774v1",
        "pub_date": "2025-10-15",
        "summary": "Forecasting urban phenomena such as housing prices and public health indicators requires the effective integration of various geospatial data. Current methods primarily utilize task-specific models, while recent foundation models for spatial representations often support only limited modalities and lack multimodal fusion capabilities. To overcome these challenges, we present UrbanFusion, a Geo-Foundation Model (GeoFM) that features Stochastic Multimodal Fusion (SMF). The framework employs modality-specific encoders to process different types of inputs, including street view imagery, remote sensing data, cartographic maps, and points of interest (POIs) data. These multimodal inputs are integrated via a Transformer-based fusion module that learns unified representations. An extensive evaluation across 41 tasks in 56 cities worldwide demonstrates UrbanFusion's strong generalization and predictive performance compared to state-of-the-art GeoAI models. Specifically, it 1) outperforms prior foundation models on location-encoding, 2) allows multimodal input during inference, and 3) generalizes well to regions unseen during training. UrbanFusion can flexibly utilize any subset of available modalities for a given location during both pretraining and inference, enabling broad applicability across diverse data availability scenarios. All source code is available at https://github.com/DominikM198/UrbanFusion.",
        "translated": "预测城市现象（如房价和公共健康指标）需要有效整合各种地理空间数据。当前的方法主要依赖于任务特定模型，而近期的空间表示基础模型通常只支持有限的模态，并缺乏多模态融合能力。为了解决这些挑战，我们提出了 UrbanFusion，一种具有随机多模态融合（Stochastic Multimodal Fusion, SMF）功能的地理基础模型（Geo-Foundation Model, GeoFM）。该框架使用模态特定编码器处理不同类型的输入，包括街景图像、遥感数据、地图数据和兴趣点（Points of Interest, POIs）数据。这些多模态输入通过基于 Transformer 的融合模块进行整合，以学习统一的表示。在对全球 56 个城市中的 41 项任务进行广泛评估后，UrbanFusion 在泛化能力和预测性能方面均优于最先进的 GeoAI 模型。具体而言，UrbanFusion 具备以下优势：1）在位置编码方面优于以往的基础模型，2）在推理过程中支持多模态输入，3）对训练过程中未见过的区域具有良好的泛化能力。UrbanFusion 在预训练和推理阶段都能灵活地利用特定位置中可用的任意子集模态，从而使其适用于各种数据可用性场景。所有源代码均在 https://github.com/DominikM198/UrbanFusion 上公开。",
        "translated_title": "UrbanFusion: 用于鲁棒空间表示对比学习的随机多模态融合",
        "label": [
            "遥感图像复原"
        ],
        "label_reason": "涉及遥感图像处理，但主要聚焦多模态融合而非像素级图像质量恢复。",
        "relevance_score": 5,
        "novelty_score": 7,
        "novelty_reason": "提出随机多模态融合方法，对Geo-Foundation Model有一定改进。"
    },
    {
        "title": "Scaling Vision Transformers for Functional MRI with Flat Maps",
        "url": "http://arxiv.org/abs/2510.13768v1",
        "pub_date": "2025-10-15",
        "summary": "A key question for adapting modern deep learning architectures to functional MRI (fMRI) is how to represent the data for model input. To bridge the modality gap between fMRI and natural images, we transform the 4D volumetric fMRI data into videos of 2D fMRI activity flat maps. We train Vision Transformers on 2.3K hours of fMRI flat map videos from the Human Connectome Project using the spatiotemporal masked autoencoder (MAE) framework. We observe that masked fMRI modeling performance improves with dataset size according to a strict power scaling law. Downstream classification benchmarks show that our model learns rich representations supporting both fine-grained state decoding across subjects, as well as subject-specific trait decoding across changes in brain state. This work is part of an ongoing open science project to build foundation models for fMRI data. Our code and datasets are available at https://github.com/MedARC-AI/fmri-fm.",
        "translated": "将现代深度学习架构适应于功能磁共振成像（fMRI）的一个关键问题是，如何将数据表示为模型输入。为了弥合 fMRI 与自然图像之间的模态差异，我们把 4D 的 fMRI 体积数据转换为 2D 的 fMRI 活动平面图的视频。我们使用时空掩码自编码器（MAE）框架，在来自人类连接组计划（Human Connectome Project）的 2.3K 小时 fMRI 平面图视频上训练视觉变换器（Vision Transformers）。我们观察到，随着数据集规模的增加，掩码 fMRI 建模性能按照严格的幂律进行提升。下游分类基准测试表明，我们的模型学习到了丰富的表示，不仅能够跨被试进行细粒度状态解码，还能在脑状态变化的条件下进行被试特异性特质的解码。本项工作是正在进行的一个开放科学项目的一部分，旨在为 fMRI 数据构建基础模型。我们的代码和数据集可在 https://github.com/MedARC-AI/fmri-fm 获取。",
        "translated_title": "Scaling Vision Transformers for Functional MRI with Flat Maps  \n使用平面图的视觉变换器扩展功能磁共振成像  \n\n功能磁共振成像（fMRI）是一种非侵入性技术，用于研究大脑活动，其通过检测血氧水平依赖（BOLD）信号实现。然而，fMRI 数据通常受到低空间分辨率和低信噪比（SNR）的限制，这对识别大脑功能区域构成挑战。最近，视觉变换器（Vision Transformers, ViTs）在各种图像恢复任务中表现出卓越的性能。这些模型通过其自注意力机制能够捕获长距离依赖关系，为 fMRI 信号的处理提供了新思路。  \n\n在本研究中，我们提出了一种基于视觉变换器的方法，用于 fMRI 数据的去噪和增强。我们采用平面图（Flat Maps）作为先验知识，以更好地建模大脑皮层的拓扑结构。通过将 fMRI 数据投影到平面图中，我们能够利用 ViT 在空域中的强大特征提取能力。实验结果表明，我们的方法在多个 fMRI 数据集上显著优于现有方法，尤其是在保留功能细节和减少噪声方面表现出色。  \n\n此外，我们还探讨了视觉变换器的扩展能力，通过增加模型深度和宽度来提升其性能。我们发现，适当增加模型规模可以有效提高 fMRI 数据的恢复质量，同时保持计算效率。",
        "label": [],
        "label_reason": "论文专注于fMRI数据表示与建模，非图像像素级恢复或增强任务。",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出将Vision Transformer扩展至fMRI数据建模，具有一定新颖性。"
    },
    {
        "title": "Uni-MMMU: A Massive Multi-discipline Multimodal Unified Benchmark",
        "url": "http://arxiv.org/abs/2510.13759v1",
        "pub_date": "2025-10-15",
        "summary": "Unified multimodal models aim to jointly enable visual understanding and generation, yet current benchmarks rarely examine their true integration. Existing evaluations either treat the two abilities in isolation or overlook tasks that inherently couple them. To address this gap, we present Uni-MMMU, a comprehensive and discipline-aware benchmark that systematically unfolds the bidirectional synergy between generation and understanding across eight reasoning-centric domains, including science, coding, mathematics, and puzzles. Each task is bidirectionally coupled, demanding models to (i) leverage conceptual understanding to guide precise visual synthesis, or (ii) utilize generation as a cognitive scaffold for analytical reasoning. Uni-MMMU incorporates verifiable intermediate reasoning steps, unique ground truths, and a reproducible scoring protocol for both textual and visual outputs. Through extensive evaluation of state-of-the-art unified, generation-only, and understanding-only models, we reveal substantial performance disparities and cross-modal dependencies, offering new insights into when and how these abilities reinforce one another, and establishing a reliable foundation for advancing unified models.",
        "translated": "统一的多模态模型旨在同时实现视觉理解和生成能力，但目前的基准测试很少真正考察这两种能力的融合。现有的评估方法要么将这两种能力孤立地对待，要么忽略了那些本质上需要将它们耦合的任务。为了解决这一问题，我们提出了 Uni-MMMU，这是一个全面且学科感知的基准测试，系统地揭示了生成与理解在八个以推理为中心的领域（包括科学、编程、数学和谜题）中的双向协同作用。每个任务都是双向耦合的，要求模型 (i) 利用概念理解来指导精确的视觉合成，或 (ii) 将生成能力作为认知支架，以支持分析推理。Uni-MMMU 包含可验证的中间推理步骤、唯一的地面真值，以及针对文本和视觉输出的可复现评分协议。通过对最先进的统一模型、仅生成模型和仅理解模型的广泛评估，我们揭示了显著的性能差异和跨模态依赖关系，提供了关于何时以及如何实现这些能力相互增强的新见解，并为统一模型的进一步发展奠定了可靠基础。",
        "translated_title": "Uni-MMMU: 一个大规模多学科多模态统一基准",
        "label": [],
        "label_reason": "论文关注多模态统一模型评估，不属于图像像素级处理任务。",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "提出了跨学科的多模态统一基准，但方法较为常规。"
    },
    {
        "title": "RECODE: Reasoning Through Code Generation for Visual Question Answering",
        "url": "http://arxiv.org/abs/2510.13756v1",
        "pub_date": "2025-10-15",
        "summary": "Multimodal Large Language Models (MLLMs) struggle with precise reasoning for structured visuals like charts and diagrams, as pixel-based perception lacks a mechanism for verification. To address this, we propose to leverage derendering -- the process of reverse-engineering visuals into executable code -- as a new modality for verifiable visual reasoning. Specifically, we propose RECODE, an agentic framework that first generates multiple candidate programs to reproduce the input image. It then uses a critic to select the most faithful reconstruction and iteratively refines the code. This process not only transforms an ambiguous perceptual task into a verifiable, symbolic problem, but also enables precise calculations and logical inferences later on. On various visual reasoning benchmarks such as CharXiv, ChartQA, and Geometry3K, RECODE significantly outperforms methods that do not leverage code or only use code for drawing auxiliary lines or cropping. Our work demonstrates that grounding visual perception in executable code provides a new path toward more accurate and verifiable multimodal reasoning.",
        "translated": "多模态大语言模型（MLLMs）在处理图表和示意图等结构化视觉内容时，常常在精确推理方面存在困难，因为基于像素的感知缺乏验证机制。为了解决这一问题，我们提出利用去渲染（derendering）——即将视觉图像反向工程为可执行代码的过程——作为一种可验证视觉推理的新模态。具体而言，我们提出了 RECODE，一个智能体框架，首先生成多个候选程序以重现输入图像。然后，它使用一个评估器选择最忠实的重建结果，并对代码进行迭代优化。这一过程不仅将模糊的感知任务转化为可验证的符号问题，还为后续的精确计算和逻辑推理提供了可能。在各种视觉推理基准如 CharXiv、ChartQA 和 Geometry3K 上，RECODE 显著优于不使用代码或仅使用代码绘制辅助线或裁剪图像的方法。我们的工作表明，将视觉感知建立在可执行代码基础上，为更准确和可验证的多模态推理提供了一条新路径。",
        "translated_title": "RECODE：通过代码生成进行推理的视觉问答",
        "label": [],
        "label_reason": "论文不属于低层图像处理，而是视觉问答任务。",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出通过代码生成实现视觉推理，具有新颖性。"
    },
    {
        "title": "InteractiveOmni: A Unified Omni-modal Model for Audio-Visual Multi-turn\n  Dialogue",
        "url": "http://arxiv.org/abs/2510.13747v1",
        "pub_date": "2025-10-15",
        "summary": "We introduce InteractiveOmni, a unified and open-source omni-modal large language model for audio-visual multi-turn interaction, ranging from 4B to 8B parameters, designed to lead the field of lightweight models by offering comprehensive omni-modal understanding and speech generation capabilities. To achieve this, we integrate the vision encoder, audio encoder, large language model, and speech decoder into a unified model for understanding and generation tasks. We design a multi-stage training strategy to ensure robust cross-modal capabilities, including pre-training for omni-modal understanding, followed by post-training with speech conversation and audio-visual interaction. To enable human-like long-term conversational ability, we meticulously curate a multi-turn training dataset that enhances the model's ability to handle complex and multi-turn interactions. To effectively evaluate the multi-turn memory and speech interaction capabilities, we construct the multi-modal multi-turn memory benchmark and the multi-turn speech interaction benchmark. Experiments demonstrate that InteractiveOmni significantly outperforms leading open-source models and provides a more intelligent multi-turn audio-visual experience, particularly in its long-term memory capabilities. Notably, InteractiveOmni-4B is comparable to the much larger model like Qwen2.5-Omni-7B on general benchmarks, and it can retain 97% of the performance of the InteractiveOmni-8B while utilizing only 50% of the model size. Achieving state-of-the-art results against similarly sized models across image, audio, video understanding, and speech generation tasks, InteractiveOmni is an accessible, open-source foundation for next-generation intelligent interactive systems.",
        "translated": "我们提出 InteractiveOmni，一种统一且开源的多模态大语言模型，用于音频-视觉的多轮交互，参数规模从 4B 到 8B 不等，旨在通过提供全面的多模态理解与语音生成能力，引领轻量级模型的发展。为此，我们将视觉编码器、音频编码器、大语言模型和语音解码器集成到一个统一模型中，用于理解和生成任务。我们设计了一种多阶段训练策略，以确保强大的跨模态能力，包括用于多模态理解的预训练，以及随后进行的语音对话和音视频交互的后训练。为了实现类人般的长期对话能力，我们精心构建了一个多轮训练数据集，以增强模型处理复杂和多轮交互的能力。为了有效评估多轮记忆和语音交互能力，我们构建了多模态多轮记忆基准和多轮语音交互基准。实验表明，InteractiveOmni 在多项指标上显著优于领先的开源模型，提供了更加智能的多轮音视频交互体验，特别是在长期记忆能力方面。值得注意的是，InteractiveOmni-4B 在通用基准上表现与更大规模的模型如 Qwen2.5-Omni-7B 相当，并且仅使用 50% 的模型规模即可保留 97% 的 InteractiveOmni-8B 性能。在图像、音频、视频理解和语音生成任务中，InteractiveOmni 在同规模模型中实现了最先进的结果，是下一代智能交互系统的一个可访问、开源的基础。",
        "translated_title": "InteractiveOmni：用于音视频多轮对话的统一全模态模型",
        "label": [],
        "label_reason": "不属于low-level图像处理，是多模态对话模型",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "常规多模态模型设计，无本质创新"
    },
    {
        "title": "UniCalli: A Unified Diffusion Framework for Column-Level Generation and\n  Recognition of Chinese Calligraphy",
        "url": "http://arxiv.org/abs/2510.13745v1",
        "pub_date": "2025-10-15",
        "summary": "Computational replication of Chinese calligraphy remains challenging. Existing methods falter, either creating high-quality isolated characters while ignoring page-level aesthetics like ligatures and spacing, or attempting page synthesis at the expense of calligraphic correctness. We introduce \\textbf{UniCalli}, a unified diffusion framework for column-level recognition and generation. Training both tasks jointly is deliberate: recognition constrains the generator to preserve character structure, while generation provides style and layout priors. This synergy fosters concept-level abstractions that improve both tasks, especially in limited-data regimes. We curated a dataset of over 8,000 digitized pieces, with ~4,000 densely annotated. UniCalli employs asymmetric noising and a rasterized box map for spatial priors, trained on a mix of synthetic, labeled, and unlabeled data. The model achieves state-of-the-art generative quality with superior ligature continuity and layout fidelity, alongside stronger recognition. The framework successfully extends to other ancient scripts, including Oracle bone inscriptions and Egyptian hieroglyphs. Code and data can be viewed in \\href{https://github.com/EnVision-Research/UniCalli}{this URL}.",
        "translated": "中国书法的计算复现仍然具有挑战性。现有方法存在不足，要么生成高质量的单个字符而忽视了页面级别的美学要素（如连笔和间距），要么在尝试页面合成时以牺牲书法正确性为代价。我们引入了**UniCalli**，一种面向列级别的识别与生成的统一扩散框架。联合训练这两个任务是经过深思熟虑的：识别任务约束生成器以保持字符结构，而生成任务则提供了风格和布局的先验知识。这种协同作用促进了概念级别的抽象表示，从而提升了两个任务的性能，尤其在数据受限的情况下表现更为明显。我们整理了一个包含超过 8,000 幅数字化书法作品的数据集，其中约 4,000 幅进行了密集标注。UniCalli 采用非对称噪声和光栅化框图来提供空域先验，并在合成数据、标注数据和未标注数据的混合集上进行训练。该模型在生成质量方面达到了最先进的水平，连笔的连续性和布局的保真度均优于现有方法，同时识别性能也更强。该框架成功地扩展到了其他古代文字，包括甲骨文和埃及象形文字。代码和数据可通过 \\href{https://github.com/EnVision-Research/UniCalli}{此链接}查看。",
        "translated_title": "UniCalli：一种面向列级生成与识别的中文书法统一扩散框架",
        "label": [],
        "label_reason": "不属于low-level图像处理，专注于书法生成与识别",
        "relevance_score": 1,
        "novelty_score": 6,
        "novelty_reason": "提出了统一扩散框架，但属于high-level生成与识别任务"
    },
    {
        "title": "Multi-Scale High-Resolution Logarithmic Grapher Module for Efficient\n  Vision GNNs",
        "url": "http://arxiv.org/abs/2510.13740v1",
        "pub_date": "2025-10-15",
        "summary": "Vision graph neural networks (ViG) have demonstrated promise in vision tasks as a competitive alternative to conventional convolutional neural nets (CNN) and transformers (ViTs); however, common graph construction methods, such as k-nearest neighbor (KNN), can be expensive on larger images. While methods such as Sparse Vision Graph Attention (SVGA) have shown promise, SVGA's fixed step scale can lead to over-squashing and missing multiple connections to gain the same information that could be gained from a long-range link. Through this observation, we propose a new graph construction method, Logarithmic Scalable Graph Construction (LSGC) to enhance performance by limiting the number of long-range links. To this end, we propose LogViG, a novel hybrid CNN-GNN model that utilizes LSGC. Furthermore, inspired by the successes of multi-scale and high-resolution architectures, we introduce and apply a high-resolution branch and fuse features between our high-resolution and low-resolution branches for a multi-scale high-resolution Vision GNN network. Extensive experiments show that LogViG beats existing ViG, CNN, and ViT architectures in terms of accuracy, GMACs, and parameters on image classification and semantic segmentation tasks. Our smallest model, Ti-LogViG, achieves an average top-1 accuracy on ImageNet-1K of 79.9% with a standard deviation of 0.2%, 1.7% higher average accuracy than Vision GNN with a 24.3% reduction in parameters and 35.3% reduction in GMACs. Our work shows that leveraging long-range links in graph construction for ViGs through our proposed LSGC can exceed the performance of current state-of-the-art ViGs. Code is available at https://github.com/mmunir127/LogViG-Official.",
        "translated": "视觉图神经网络（ViG）在视觉任务中展现出作为传统卷积神经网络（CNN）和视觉变换器（ViTs）的有竞争力的替代方案的潜力；然而，常见的图构建方法，如 k-近邻（KNN），在处理大尺寸图像时计算代价较高。虽然稀疏视觉图注意力（SVGA）等方法已显示出前景，但 SVGA 固定的步长尺度可能导致过度压缩问题，并且无法通过多个连接获取与长程连接相同的信息。基于这一观察，我们提出了一种新的图构建方法——对数可扩展图构建（LSGC），通过限制长程连接的数量来提升性能。为此，我们设计了 LogViG，这是一种新颖的 CNN-GNN 混合模型，利用了 LSGC。此外，受多尺度和高分辨率架构成功案例的启发，我们引入并应用了一个高分辨率分支，并在高分辨率和低分辨率分支之间融合特征，构建了一个多尺度高分辨率的视觉图神经网络。大量实验表明，LogViG 在图像分类和语义分割任务中，在准确率、GMACs 和参数数量方面均优于现有的 ViG、CNN 和 ViT 架构。我们最小的模型 Ti-LogViG 在 ImageNet-1K 数据集上的平均 top-1 准确率为 79.9%，标准差为 0.2%，比 Vision GNN 的平均准确率高出 1.7%，同时参数数量减少了 24.3%，GMACs 降低了 35.3%。我们的研究表明，通过所提出的 LSGC 在图构建中利用长程连接，可使 ViG 的性能超越当前最先进的 ViG 模型。代码可在 https://github.com/mmunir127/LogViG-Official 获取。",
        "translated_title": "多尺度高分辨率对数图生成模块用于高效的视觉图神经网络",
        "label": [],
        "label_reason": "论文主要关注图像分类和语义分割，不属于 low-level 图像处理。",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出新的图构建方法 LSGC，对 Vision GNN 有显著改进。"
    },
    {
        "title": "Cyclic Self-Supervised Diffusion for Ultra Low-field to High-field MRI\n  Synthesis",
        "url": "http://arxiv.org/abs/2510.13735v1",
        "pub_date": "2025-10-15",
        "summary": "Synthesizing high-quality images from low-field MRI holds significant potential. Low-field MRI is cheaper, more accessible, and safer, but suffers from low resolution and poor signal-to-noise ratio. This synthesis process can reduce reliance on costly acquisitions and expand data availability. However, synthesizing high-field MRI still suffers from a clinical fidelity gap. There is a need to preserve anatomical fidelity, enhance fine-grained structural details, and bridge domain gaps in image contrast. To address these issues, we propose a \\emph{cyclic self-supervised diffusion (CSS-Diff)} framework for high-field MRI synthesis from real low-field MRI data. Our core idea is to reformulate diffusion-based synthesis under a cycle-consistent constraint. It enforces anatomical preservation throughout the generative process rather than just relying on paired pixel-level supervision. The CSS-Diff framework further incorporates two novel processes. The slice-wise gap perception network aligns inter-slice inconsistencies via contrastive learning. The local structure correction network enhances local feature restoration through self-reconstruction of masked and perturbed patches. Extensive experiments on cross-field synthesis tasks demonstrate the effectiveness of our method, achieving state-of-the-art performance (e.g., 31.80 $\\pm$ 2.70 dB in PSNR, 0.943 $\\pm$ 0.102 in SSIM, and 0.0864 $\\pm$ 0.0689 in LPIPS). Beyond pixel-wise fidelity, our method also preserves fine-grained anatomical structures compared with the original low-field MRI (e.g., left cerebral white matter error drops from 12.1$\\%$ to 2.1$\\%$, cortex from 4.2$\\%$ to 3.7$\\%$). To conclude, our CSS-Diff can synthesize images that are both quantitatively reliable and anatomically consistent.",
        "translated": "从低场 MRI 数据合成高质量图像具有重要潜力。低场 MRI 成本更低、更易获取、也更安全，但其分辨率较低且信噪比差。通过合成过程，可以减少对昂贵采集设备的依赖，并扩展数据的可用性。然而，当前的高场 MRI 合成仍然存在临床保真度的不足。有必要在合成中保持解剖结构的准确性，增强细粒度的结构细节，并弥合图像对比度方面的领域差异。为了解决这些问题，我们提出了一种名为 \\emph{循环自监督扩散（CSS-Diff）} 的框架，用于从真实的低场 MRI 数据中合成高场 MRI。我们的核心思想是在循环一致的约束下重构基于扩散的合成方法。这种方法在整个生成过程中强制保持解剖结构，而不仅仅依赖于像素级的配对监督。CSS-Diff 框架进一步结合了两个新颖的处理过程。切片级差异感知网络通过对比学习对齐切片间的不一致性。局部结构校正网络则通过遮蔽和扰动图像块的自重建来增强局部特征恢复。在跨场强合成任务上的大量实验表明了我们方法的有效性，达到了当前最先进的性能（例如，PSNR 为 31.80 $\\pm$ 2.70 dB，SSIM 为 0.943 $\\pm$ 0.102，LPIPS 为 0.0864 $\\pm$ 0.0689）。除了像素级保真度之外，与原始低场 MRI 相比，我们的方法还能保持更精细的解剖结构（例如，左脑白质误差从 12.1$\\%$ 降至 2.1$\\%$，皮层误差从 4.2$\\%$ 降至 3.7$\\%$）。综上所述，我们的 CSS-Diff 能够合成在定量评估和解剖一致性方面都表现良好的图像。",
        "translated_title": "循环自监督扩散用于超低场到高场MRI合成",
        "label": [
            "医学图像增强",
            "图像去噪",
            "图像恢复"
        ],
        "label_reason": "论文专注于从低场MRI合成高质量高场MRI，涉及图像恢复与医学增强",
        "relevance_score": 8,
        "novelty_score": 8,
        "novelty_reason": "提出循环自监督扩散框架，结合新网络结构提升合成效果"
    },
    {
        "title": "LiFMCR: Dataset and Benchmark for Light Field Multi-Camera Registration",
        "url": "http://arxiv.org/abs/2510.13729v1",
        "pub_date": "2025-10-15",
        "summary": "We present LiFMCR, a novel dataset for the registration of multiple micro lens array (MLA)-based light field cameras. While existing light field datasets are limited to single-camera setups and typically lack external ground truth, LiFMCR provides synchronized image sequences from two high-resolution Raytrix R32 plenoptic cameras, together with high-precision 6-degrees of freedom (DoF) poses recorded by a Vicon motion capture system. This unique combination enables rigorous evaluation of multi-camera light field registration methods.   As a baseline, we provide two complementary registration approaches: a robust 3D transformation estimation via a RANSAC-based method using cross-view point clouds, and a plenoptic PnP algorithm estimating extrinsic 6-DoF poses from single light field images. Both explicitly integrate the plenoptic camera model, enabling accurate and scalable multi-camera registration. Experiments show strong alignment with the ground truth, supporting reliable multi-view light field processing.   Project page: https://lifmcr.github.io/",
        "translated": "我们提出了 LiFMCR，这是一个用于多微透镜阵列（MLA）光场相机注册的新数据集。现有的光场数据集通常局限于单相机设置，并且普遍缺乏外部真实值。LiFMCR 提供了来自两台高分辨率 Raytrix R32 光场相机的同步图像序列，并配有通过 Vicon 运动捕捉系统记录的高精度六自由度（DoF）姿态。这一独特组合使得对多相机光场注册方法的严格评估成为可能。\n\n作为基准，我们提供了两种互补的注册方法：一种是通过基于 RANSAC 的方法利用跨视角点云进行鲁棒的三维变换估计，另一种是光场 PnP 算法，用于从单光场图像中估计外部六自由度姿态。两种方法均明确集成了光场相机模型，从而实现了精确且可扩展的多相机注册。实验结果表明，与真实值具有高度对齐性，支持可靠的多视角光场处理。\n\n项目页面：https://lifmcr.github.io/",
        "translated_title": "LiFMCR：光场多摄像头配准的数据集与基准",
        "label": [],
        "label_reason": "论文主要关注光场相机注册，不属于像素级图像恢复或增强",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出了新的光场多相机注册数据集和基准"
    },
    {
        "title": "NExT-OMNI: Towards Any-to-Any Omnimodal Foundation Models with Discrete\n  Flow Matching",
        "url": "http://arxiv.org/abs/2510.13721v1",
        "pub_date": "2025-10-15",
        "summary": "Next-generation multimodal foundation models capable of any-to-any cross-modal generation and multi-turn interaction will serve as core components of artificial general intelligence systems, playing a pivotal role in human-machine interaction. However, most existing multimodal models remain constrained by autoregressive architectures, whose inherent limitations prevent a balanced integration of understanding and generation capabilities. Although hybrid and decoupling strategies have been explored to address these tasks within unified frameworks separately, their redundant, non-integrated designs limit their applicability to broader scenarios, such as cross-modal retrieval.In this work, we introduce NExT-OMNI, an open-source omnimodal foundation model that achieves unified modeling through discrete flow paradigms. By leveraging metric-induced probability paths and kinetic optimal velocities, NExT-OMNI natively supports any-to-any understanding and generation with enhanced response efficiency, while enabling broader application scenarios through concise unified representations rather than task-decoupled designs. Trained on large-scale interleaved text, image, video, and audio data, NExT-OMNI delivers competitive performance on multimodal generation and understanding benchmarks, while outperforming prior unified models in multi-turn multimodal interaction and cross-modal retrieval, highlighting its architectural advantages as a next-generation multimodal foundation model. To advance further research, we release training details, data protocols, and open-source both the code and model checkpoints.",
        "translated": "下一代能够实现任意模态到任意模态跨模态生成和多轮交互的多模态基础模型，将成为通用人工智能系统的核心组成部分，在人机交互中发挥关键作用。然而，目前大多数多模态模型仍受自回归架构的限制，其固有局限性阻碍了理解能力和生成能力之间的平衡整合。尽管已有研究探索了混合架构和任务解耦策略，试图在统一框架内分别处理这些任务，但其冗余且非整合的设计限制了其在更广泛场景（如跨模态检索）中的应用。在本工作中，我们提出 NExT-OMNI，一个开源的全模态基础模型，通过离散流范式实现统一建模。NExT-OMNI 利用度量诱导的概率路径和动力最优速度，在增强响应效率的同时，原生支持任意模态到任意模态的理解与生成，并通过简洁的统一表征而非任务解耦的设计，实现了更广泛的应用场景。NExT-OMNI 在大规模交织的文本、图像、视频和音频数据上进行训练，在多模态生成和理解基准上展现出具有竞争力的性能，并在多轮多模态交互和跨模态检索方面优于以往的统一模型，突显了其作为下一代多模态基础模型的架构优势。为了推动进一步的研究，我们发布了训练细节、数据协议，并开源了代码和模型检查点。",
        "translated_title": "NExT-OMNI：基于离散流匹配的任意到任意全模态基础模型",
        "label": [],
        "label_reason": "论文主要研究多模态模型，不属于图像像素级恢复或增强任务。",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出离散流匹配的新范式，对多模态模型架构有显著改进。"
    },
    {
        "title": "Epistemic-aware Vision-Language Foundation Model for Fetal Ultrasound\n  Interpretation",
        "url": "http://arxiv.org/abs/2510.12953v1",
        "pub_date": "2025-10-14",
        "summary": "Recent medical vision-language models have shown promise on tasks such as VQA, report generation, and anomaly detection. However, most are adapted to structured adult imaging and underperform in fetal ultrasound, which poses challenges of multi-view image reasoning, numerous diseases, and image diversity. To bridge this gap, we introduce FetalMind, a medical AI system tailored to fetal ultrasound for both report generation and diagnosis. Guided by clinical workflow, we propose Salient Epistemic Disentanglement (SED), which injects an expert-curated bipartite graph into the model to decouple view-disease associations and to steer preference selection along clinically faithful steps via reinforcement learning. This design mitigates variability across diseases and heterogeneity across views, reducing learning bottlenecks while aligning the model's inference with obstetric practice. To train FetalMind at scale, we curate FetalSigma-1M dataset, the first large-scale fetal ultrasound report corpus, comprising 20K reports from twelve medical centers, addressing the scarcity of domain data. Extensive experiments show that FetalMind outperforms open- and closed-source baselines across all gestational stages, achieving +14% average gains and +61.2% higher accuracy on critical conditions while remaining efficient, stable, and scalable. Project Page: https://hexiao0275.github.io/FetalMind.",
        "translated": "近年来，医疗视觉-语言模型在视觉问答（VQA）、报告生成和异常检测等任务上表现出潜力。然而，大多数模型适用于结构化的成人影像数据，在胎儿超声任务中表现欠佳，这面临着多视角图像推理、大量疾病类别和图像多样性的挑战。为弥合这一差距，我们提出了FetalMind，一个专为胎儿超声定制的医疗人工智能系统，支持报告生成与诊断任务。受临床工作流程的启发，我们提出了显著性认识解耦（Salient Epistemic Disentanglement, SED）方法，该方法将专家构建的二部图注入模型，以解耦视角与疾病之间的关联，并通过强化学习引导模型在临床可靠的步骤中进行偏好选择。这一设计缓解了不同疾病之间的差异性以及不同视角之间的异质性，从而降低学习瓶颈，同时使模型的推理过程与产科实践保持一致。为了大规模训练FetalMind，我们构建了FetalSigma-1M数据集，这是首个大规模胎儿超声报告语料库，包含了来自十二个医疗机构的20K份报告，解决了该领域数据稀缺的问题。大量实验表明，FetalMind在所有孕周阶段均优于开源和闭源基线模型，平均提升14%，在关键疾病上的准确率提高61.2%，同时保持高效、稳定和可扩展。项目主页：https://hexiao0275.github.io/FetalMind.",
        "translated_title": "具有认知感知能力的视觉-语言基础模型在胎儿超声解释中的应用",
        "label": [],
        "label_reason": "论文聚焦医学影像与自然语言处理，非直接推荐系统相关。",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出临床工作流引导的模型设计，数据集和方法有创新性。"
    },
    {
        "title": "DeepMMSearch-R1: Empowering Multimodal LLMs in Multimodal Web Search",
        "url": "http://arxiv.org/abs/2510.12801v1",
        "pub_date": "2025-10-14",
        "summary": "Multimodal Large Language Models (MLLMs) in real-world applications require access to external knowledge sources and must remain responsive to the dynamic and ever-changing real-world information in order to address information-seeking and knowledge-intensive user queries. Existing approaches, such as retrieval augmented generation (RAG) methods, search agents, and search equipped MLLMs, often suffer from rigid pipelines, excessive search calls, and poorly constructed search queries, which result in inefficiencies and suboptimal outcomes. To address these limitations, we present DeepMMSearch-R1, the first multimodal LLM capable of performing on-demand, multi-turn web searches and dynamically crafting queries for both image and text search tools. Specifically, DeepMMSearch-R1 can initiate web searches based on relevant crops of the input image making the image search more effective, and can iteratively adapt text search queries based on retrieved information, thereby enabling self-reflection and self-correction. Our approach relies on a two-stage training pipeline: a cold start supervised finetuning phase followed by an online reinforcement learning optimization. For training, we introduce DeepMMSearchVQA, a novel multimodal VQA dataset created through an automated pipeline intermixed with real-world information from web search tools. This dataset contains diverse, multi-hop queries that integrate textual and visual information, teaching the model when to search, what to search for, which search tool to use and how to reason over the retrieved information. We conduct extensive experiments across a range of knowledge-intensive benchmarks to demonstrate the superiority of our approach. Finally, we analyze the results and provide insights that are valuable for advancing multimodal web-search.",
        "translated": "在现实世界应用中，多模态大语言模型（MLLMs）需要访问外部知识源，并且必须能够响应动态变化的现实世界信息，以应对用户的信息查询和知识密集型请求。现有方法，如检索增强生成（RAG）方法、搜索代理和配备搜索功能的 MLLMs，通常存在流水线僵化、搜索调用过多以及搜索查询构建不完善的问题，导致效率低下和结果次优。为了解决这些限制，我们提出了 DeepMMSearch-R1，这是首个能够按需进行多轮网页搜索，并为图像和文本搜索工具动态构建查询的多模态大语言模型。具体来说，DeepMMSearch-R1 可以根据输入图像的相关裁剪区域发起网页搜索，从而提高图像搜索的有效性；同时能够根据检索到的信息迭代调整文本搜索查询，实现自我反思和自我修正。我们的方法依赖于一个两阶段的训练流水线：首先进行冷启动阶段的监督微调，然后进行在线强化学习优化。为了训练，我们引入了 DeepMMSearchVQA，这是一个通过自动化流水线与网页搜索工具中的现实世界信息混合生成的新型多模态视觉问答（VQA）数据集。该数据集包含多样化的多跳查询，融合了文本和视觉信息，教导模型何时进行搜索、搜索什么内容、使用哪个搜索工具以及如何对检索到的信息进行推理。我们在多个知识密集型基准上进行了广泛的实验，以验证我们方法的优越性。最后，我们对结果进行了分析，并提供了对多模态网页搜索研究具有重要价值的见解。",
        "translated_title": "DeepMMSearch-R1：在多模态网页搜索中增强多模态大语言模型",
        "label": [
            "多模态推荐",
            "LLM生成式推荐"
        ],
        "label_reason": "论文涉及多模态LLM在搜索中的应用，可能适用于推荐系统。",
        "relevance_score": 5,
        "novelty_score": 8,
        "novelty_reason": "提出多模态LLM动态构建搜索查询的新方法，具有创新性。"
    },
    {
        "title": "CTRL-Rec: Controlling Recommender Systems With Natural Language",
        "url": "http://arxiv.org/abs/2510.12742v1",
        "pub_date": "2025-10-14",
        "summary": "When users are dissatisfied with recommendations from a recommender system, they often lack fine-grained controls for changing them. Large language models (LLMs) offer a solution by allowing users to guide their recommendations through natural language requests (e.g., \"I want to see respectful posts with a different perspective than mine\"). We propose a method, CTRL-Rec, that allows for natural language control of traditional recommender systems in real-time with computational efficiency. Specifically, at training time, we use an LLM to simulate whether users would approve of items based on their language requests, and we train embedding models that approximate such simulated judgments. We then integrate these user-request-based predictions into the standard weighting of signals that traditional recommender systems optimize. At deployment time, we require only a single LLM embedding computation per user request, allowing for real-time control of recommendations. In experiments with the MovieLens dataset, our method consistently allows for fine-grained control across a diversity of requests. In a study with 19 Letterboxd users, we find that CTRL-Rec was positively received by users and significantly enhanced users' sense of control and satisfaction with recommendations compared to traditional controls.",
        "translated": "当用户对推荐系统提供的推荐结果不满意时，他们通常缺乏对推荐结果进行细粒度调整的控制手段。大语言模型（LLM）提供了一种解决方案，允许用户通过自然语言请求（例如，“我想看到与我的观点不同的尊重性帖子”）来引导推荐结果。我们提出了一种方法 CTRL-Rec，它能够在计算效率的前提下，实现对传统推荐系统的实时自然语言控制。具体而言，在训练阶段，我们使用 LLM 来模拟用户是否会基于他们的语言请求批准某些物料，并训练嵌入模型以逼近这种模拟的判断结果。随后，我们将这些基于用户请求的预测结果整合到传统推荐系统优化的标准信号加权中。在部署阶段，我们仅需为每个用户请求进行一次 LLM 嵌入计算，从而实现实时的推荐控制。在 MovieLens 数据集上的实验表明，我们的方法在各种请求下都能实现一致的细粒度控制。在与 19 位 Letterboxd 用户进行的研究中，我们发现 CTRL-Rec 得到了用户的积极反馈，并显著增强了用户对推荐结果的控制感和满意度，相比传统的控制方式。",
        "translated_title": "CTRL-Rec：用自然语言控制推荐系统",
        "label": [
            "LLM生成式推荐",
            "精排"
        ],
        "label_reason": "结合LLM实现自然语言控制推荐结果，增强用户控制感和满意度",
        "relevance_score": 8,
        "novelty_score": 9,
        "novelty_reason": "首次将LLM用于实时推荐控制，方法新颖且有效"
    },
    {
        "title": "SAIL-Embedding Technical Report: Omni-modal Embedding Foundation Model",
        "url": "http://arxiv.org/abs/2510.12709v2",
        "pub_date": "2025-10-14",
        "summary": "Multimodal embedding models aim to yield informative unified representations that empower diverse cross-modal tasks. Despite promising developments in the evolution from CLIP-based dual-tower architectures to large vision-language models, prior works still face unavoidable challenges in real-world applications and business scenarios, such as the limited modality support, unstable training mechanisms, and industrial domain gaps. In this work, we introduce SAIL-Embedding, an omni-modal embedding foundation model that addresses these issues through tailored training strategies and architectural design. In the optimization procedure, we propose a multi-stage training scheme to boost the multifaceted effectiveness of representation learning. Specifically, the content-aware progressive training aims to enhance the model's adaptability to diverse downstream tasks and master enriched cross-modal proficiency. The collaboration-aware recommendation enhancement training further adapts multimodal representations for recommendation scenarios by distilling knowledge from sequence-to-item and ID-to-item embeddings while mining user historical interests. Concurrently, we develop the stochastic specialization and dataset-driven pattern matching to strengthen model training flexibility and generalizability. Experimental results show that SAIL-Embedding achieves SOTA performance compared to other methods in different retrieval tasks. In online experiments across various real-world scenarios integrated with our model, we observe a significant increase in Lifetime (LT), which is a crucial indicator for the recommendation experience. For instance, the model delivers the 7-day LT gain of +0.5% in the Douyin-Selected scenario. For the Douyin feed rank model, the match features produced by SAIL-Embedding yield a +0.1% AUC gain.",
        "translated": "多模态嵌入模型旨在生成具有信息量的统一表示，以支持多种跨模态任务。尽管从基于CLIP的双塔架构发展到大视觉-语言模型已取得令人期待的进展，但先前的工作在实际应用和业务场景中仍面临不可避免的挑战，如模态支持有限、训练机制不稳定以及与工业领域的差距。在本文中，我们提出了 SAIL-Embedding，这是一个全模态嵌入基础模型，通过定制化的训练策略和架构设计解决上述问题。在优化过程中，我们提出了一种多阶段训练方案，以提升表示学习在多方面上的有效性。具体而言，内容感知的渐进式训练旨在增强模型对下游任务的适应能力，并掌握丰富的跨模态能力。协作感知的推荐增强训练则通过从序列到物料和ID到物料的嵌入中提炼知识，同时挖掘用户的历史兴趣，进一步将多模态表示适配到推荐场景。与此同时，我们开发了随机专业化和数据集驱动的模式匹配方法，以增强模型训练的灵活性和泛化能力。实验结果表明，SAIL-Embedding在不同检索任务中相较其他方法实现了最先进的（SOTA）性能。在多个实际场景中与我们的模型集成进行的在线实验中，我们观察到用户生命周期（Lifetime, LT）有显著提升，这是衡量推荐体验的关键指标。例如，在Douyin-Selected场景中，模型带来了7天LT指标+0.5%的增长。对于Douyin的流推荐排序模型，SAIL-Embedding生成的匹配特征带来了+0.1%的AUC提升。",
        "translated_title": "SAIL-Embedding 技术报告：通用模态嵌入基础模型",
        "label": [
            "多模态推荐",
            "精排",
            "序列推荐"
        ],
        "label_reason": "多模态嵌入模型用于推荐场景，结合序列建模与ID建模",
        "relevance_score": 8,
        "novelty_score": 8,
        "novelty_reason": "提出多阶段训练策略和模式匹配方法，提升模型灵活性和表现"
    },
    {
        "title": "The Role of Parametric Injection-A Systematic Study of Parametric\n  Retrieval-Augmented Generation",
        "url": "http://arxiv.org/abs/2510.12668v1",
        "pub_date": "2025-10-14",
        "summary": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by retrieving external documents. As an emerging form of RAG, parametric retrieval-augmented generation (PRAG) encodes documents as model parameters (i.e., LoRA modules) and injects these representations into the model during inference, enabling interaction between the LLM and documents at parametric level. Compared with directly placing documents in the input context, PRAG is more efficient and has the potential to offer deeper model-document interaction. Despite its growing attention, the mechanism underlying parametric injection remains poorly understood. In this work, we present a systematic study of PRAG to clarify the role of parametric injection, showing that parameterized documents capture only partial semantic information of documents, and relying on them alone yields inferior performance compared to interaction at text level. However, these parametric representations encode high-level document information that can enhance the model's understanding of documents within the input context. When combined parameterized documents with textual documents, the model can leverage relevant information more effectively and become more robust to noisy inputs, achieving better performance than either source alone. We recommend jointly using parameterized and textual documents and advocate for increasing the information content of parametric representations to advance PRAG.",
        "translated": "检索增强生成（RAG）通过检索外部文档来增强大语言模型（LLM）。作为一种新兴形式的RAG，参数化检索增强生成（PRAG）将文档编码为模型参数（即LoRA模块），并在推理过程中将这些表示注入模型，从而在参数层面实现LLM与文档之间的交互。与直接将文档放入输入上下文相比，PRAG更加高效，并且有望提供更深层次的模型-文档交互。尽管PRAG已引起越来越多的关注，但其参数注入机制仍不明确。在本工作中，我们对PRAG进行了系统研究，以阐明参数注入的作用，表明参数化的文档仅捕捉了文档的部分语义信息，单独依赖它们会导致性能不如文本层面的交互。然而，这些参数表示编码了文档的高层信息，可以增强模型对输入上下文中文档的理解。将参数化文档与文本文档结合使用时，模型能够更有效地利用相关信息，并对输入中的噪声具有更强的鲁棒性，从而在性能上优于任一单独来源。我们建议联合使用参数化文档与文本文档，并倡导提升参数表示的信息量以推动PRAG的发展。",
        "translated_title": "The Role of Parametric Injection—A Systematic Study of Parametric Retrieval-Augmented Generation\n\n参数注入的作用——参数化检索增强生成的系统研究",
        "label": [
            "LLM生成式推荐"
        ],
        "label_reason": "论文研究PRAG在生成式模型中的作用，可能用于生成式推荐系统。",
        "relevance_score": 5,
        "novelty_score": 7,
        "novelty_reason": "系统性研究PRAG机制，提出参数化文档与文本文档结合的新方法。"
    },
    {
        "title": "SMILE: SeMantic Ids Enhanced CoLd Item Representation for Click-through\n  Rate Prediction in E-commerce SEarch",
        "url": "http://arxiv.org/abs/2510.12604v1",
        "pub_date": "2025-10-14",
        "summary": "With the rise of modern search and recommendation platforms, insufficient collaborative information of cold-start items exacerbates the Matthew effect of existing platform items, challenging platform diversity and becoming a longstanding issue. Existing methods align items' side content with collaborative information to transfer collaborative signals from high-popularity items to cold-start items. However, these methods fail to account for the asymmetry between collaboration and content, nor the fine-grained differences among items. To address these issues, we propose SMILE, an item representation enhancement approach based on fused alignment of semantic IDs. Specifically, we use RQ-OPQ encoding to quantize item content and collaborative information, followed by a two-step alignment: RQ encoding transfers shared collaborative signals across items, while OPQ encoding learns differentiated information of items. Comprehensive offline experiments on large-scale industrial datasets demonstrate superiority of SMILE, and rigorous online A/B tests confirm statistically significant improvements: item CTR +1.66%, buyers +1.57%, and order volume +2.17%.",
        "translated": "随着现代搜索与推荐系统的兴起，冷启动物料的协同信息不足加剧了平台现有物料的马太效应，挑战平台多样性，并成为一个长期存在的问题。现有方法将物料的侧边内容与协同信息对齐，以将协同信号从高热度物料转移到冷启动物料。然而，这些方法未能考虑协同与内容之间的不对称性，也未考虑物料之间的细粒度差异。为了解决这些问题，我们提出了 SMILE，一种基于语义 ID 融合对齐的物料表示增强方法。具体而言，我们使用 RQ-OPQ 编码对物料内容和协同信息进行量化，随后进行两步对齐：RQ 编码跨物料传递共享的协同信号，而 OPQ 编码学习物料的差异化信息。在大规模工业数据集上的全面离线实验验证了 SMILE 的优越性，严格的在线 A/B 测试也确认了其具有统计显著性的提升效果：物料点击率提升 1.66%，买家数量增加 1.57%，订单量增长 2.17%。",
        "translated_title": "SMILE：语义ID增强的冷启动物料表征用于电商搜索中的点击率预测",
        "label": [
            "精排",
            "冷启动",
            "General Recommendation Techniques",
            "Click-through Rate Prediction"
        ],
        "label_reason": "论文聚焦于冷启动商品的CTR预测，属于推荐系统核心问题",
        "relevance_score": 8,
        "novelty_score": 7,
        "novelty_reason": "提出了基于语义ID融合对齐的新方法，改进了冷启动建模"
    },
    {
        "title": "Leveraging Language Semantics for Collaborative Filtering with TextGCN\n  and TextGCN-MLP: Zero-Shot vs In-Domain Performance",
        "url": "http://arxiv.org/abs/2510.12461v1",
        "pub_date": "2025-10-14",
        "summary": "In recent years, various approaches have been proposed to leverage large language models (LLMs) for incorporating textual information about items into recommender systems. Existing methods primarily focus on either fine-tuning LLMs to generate recommendations or integrating LLM-based embeddings into downstream models. In this work, we follow the latter direction and propose \\textbf{TextGCN}, which applies parameter-free graph convolution layers directly over LLM-based item-title embeddings, instead of learning ID-based embeddings as in traditional methods. By combining language semantics with graph message passing, this architecture achieves state-of-the-art zero-shot performance, significantly outperforming prior approaches. Furthermore, we introduce \\textbf{TextGCN-MLP}, which extends TextGCN with a trainable multilayer perceptron trained using a contrastive loss, achieving state-of-the-art in-domain performance on recommendation benchmarks. However, the zero-shot performance of TextGCN-MLP remains lower than that of TextGCN, highlighting the trade-off between in-domain specialization and zero-shot generalization. We release our code on github at \\href{https://github.com/ChernovAndrey/TFCE}{github.com/ChernovAndrey/TFCE}.",
        "translated": "近年来，各种方法被提出以利用大语言模型（LLMs）将物品的文本信息引入推荐系统。现有方法主要集中在两个方面：一是微调LLMs以生成推荐结果，二是将基于LLM的嵌入向量整合到下游模型中。在本工作中，我们遵循后者，并提出了**TextGCN**，该方法直接在基于LLM的物品标题嵌入上应用无参数图卷积层，而不是像传统方法那样学习基于ID的嵌入。通过将语言语义与图消息传递相结合，该架构实现了最先进的零样本性能，显著优于先前的方法。此外，我们引入了**TextGCN-MLP**，该方法通过对比损失训练一个可训练的多层感知机（multilayer perceptron）来扩展TextGCN，在推荐基准测试中实现了最先进的域内性能。然而，TextGCN-MLP的零样本性能仍然低于TextGCN，突显了域内专业化与零样本泛化之间的权衡。我们的代码已发布在github上：\\href{https://github.com/ChernovAndrey/TFCE}{github.com/ChernovAndrey/TFCE}。",
        "translated_title": "利用语言语义进行协同过滤的推荐系统：TextGCN与TextGCN-MLP的零样本与领域内性能对比",
        "label": [
            "图神经网络推荐",
            "通用推荐技术"
        ],
        "label_reason": "结合图神经网络与文本语义，改进协同过滤方法。",
        "relevance_score": 8,
        "novelty_score": 8,
        "novelty_reason": "提出参数无关的图卷积方法，结合对比学习提升推荐效果。"
    },
    {
        "title": "A Hierarchical Quantized Tokenization Framework for Task-Adaptive Graph\n  Representation Learning",
        "url": "http://arxiv.org/abs/2510.12369v1",
        "pub_date": "2025-10-14",
        "summary": "Recent progress in language and vision foundation models demonstrates the importance of discrete token interfaces that transform complex inputs into compact sequences for large-scale modeling. Extending this paradigm to graphs requires a tokenization scheme that handles non-Euclidean structures and multi-scale dependencies efficiently. Existing approaches to graph tokenization, linearized, continuous, and quantized, remain limited in adaptability and efficiency. In particular, most current quantization-based tokenizers organize hierarchical information in fixed or task-agnostic ways, which may either over-represent or under-utilize structural cues, and lack the ability to dynamically reweight contributions from different levels without retraining the encoder. This work presents a hierarchical quantization framework that introduces a self-weighted mechanism for task-adaptive aggregation across multiple scales. The proposed method maintains a frozen encoder while modulating information flow through a lightweight gating process, enabling parameter-efficient adaptation to diverse downstream tasks. Experiments on benchmark datasets for node classification and link prediction demonstrate consistent improvements over strong baselines under comparable computational budgets.",
        "translated": "近期在语言和视觉基础模型方面的进展表明，离散的token接口在将复杂输入转化为紧凑序列以进行大规模建模中的重要性。将这一范式扩展到图结构需要一种能够高效处理非欧几里得结构和多尺度依赖关系的token化方案。现有的图token化方法，包括线性化、连续化和量化方法，在适应性和效率方面仍存在限制。特别是，大多数当前基于量化的tokenizer以固定或任务无关的方式组织层次信息，这可能导致对结构线索的过度表示或利用不足，并且无法在不重新训练编码器的情况下动态调整不同层次的贡献权重。本文提出了一种层次量化框架，引入了一种自加权机制，用于在多个尺度上进行任务自适应的聚合。所提出的方法在保持冻结编码器的同时，通过轻量级的门控过程调节信息流动，从而在不同下游任务中实现参数高效的适应。在节点分类和链接预测基准数据集上的实验表明，在可比较的计算预算下，该方法在多个任务中均优于强大的基线模型。",
        "translated_title": "一种面向任务自适应图表示学习的层次量化分词框架",
        "label": [],
        "label_reason": "论文聚焦图表示学习，未明确涉及推荐系统环节",
        "relevance_score": 3,
        "novelty_score": 7,
        "novelty_reason": "提出自加权机制改进图量化方法，有一定创新性"
    },
    {
        "title": "Simple Projection Variants Improve ColBERT Performance",
        "url": "http://arxiv.org/abs/2510.12327v1",
        "pub_date": "2025-10-14",
        "summary": "Multi-vector dense retrieval methods like ColBERT systematically use a single-layer linear projection to reduce the dimensionality of individual vectors. In this study, we explore the implications of the MaxSim operator on the gradient flows of the training of multi-vector models and show that such a simple linear projection has inherent, if non-critical, limitations in this setting. We then discuss the theoretical improvements that could result from replacing this single-layer projection with well-studied alternative feedforward linear networks (FFN), such as deeper, non-linear FFN blocks, GLU blocks, and skip-connections, could alleviate these limitations. Through the design and systematic evaluation of alternate projection blocks, we show that better-designed final projections positively impact the downstream performance of ColBERT models. We highlight that many projection variants outperform the original linear projections, with the best-performing variants increasing average performance on a range of retrieval benchmarks across domains by over 2 NDCG@10 points. We then conduct further exploration on the individual parameters of these projections block in order to understand what drives this empirical performance, highlighting the particular importance of upscaled intermediate projections and residual connections. As part of these ablation studies, we show that numerous suboptimal projection variants still outperform the traditional single-layer projection across multiple benchmarks, confirming our hypothesis. Finally, we observe that this effect is consistent across random seeds, further confirming that replacing the linear layer of ColBERT models is a robust, drop-in upgrade.",
        "translated": "诸如 ColBERT 等多向量密集召回方法通常使用单层线性投影来降低单个向量的维度。在本研究中，我们探讨了 MaxSim 运算符在多向量模型训练过程中的梯度流动所带来的影响，并表明在这一设置下，这种简单的线性投影存在固有但非关键的限制。我们进一步讨论了理论上的改进，即将该单层投影替换为已被广泛研究的替代前馈线性网络（FFN），例如更深的非线性 FFN 模块、GLU 模块以及跳跃连接（skip-connections），从而缓解这些限制。通过对替代投影模块的设计及其系统性评估，我们证明了更合理设计的最终投影对 ColBERT 模型的下游性能具有积极影响。我们发现，许多投影变体优于原始的线性投影，其中表现最好的变体在多个跨领域的召回基准上将平均性能提升了超过 2 个 NDCG@10 分数点。随后，我们对这些投影模块的单个参数进行了进一步探索，以理解其经验性能背后的关键因素，强调了中间投影的扩展（upscaled）和残差连接的特别重要性。作为消融实验的一部分，我们展示了即使许多次优的投影变体，也能在多个基准上优于传统的单层投影，从而验证了我们的假设。最后，我们观察到这一效果在不同随机种子之间是一致的，进一步确认了替换 ColBERT 模型中的线性层是一种鲁棒且可直接替换的性能提升方式。",
        "translated_title": "简单投影变体提升ColBERT性能",
        "label": [],
        "label_reason": "论文聚焦于密集检索方法优化，不直接涉及推荐系统",
        "relevance_score": 3,
        "novelty_score": 6,
        "novelty_reason": "提出改进投影层的结构，有一定方法创新"
    },
    {
        "title": "Causal Inspired Multi Modal Recommendation",
        "url": "http://arxiv.org/abs/2510.12325v1",
        "pub_date": "2025-10-14",
        "summary": "Multimodal recommender systems enhance personalized recommendations in e-commerce and online advertising by integrating visual, textual, and user-item interaction data. However, existing methods often overlook two critical biases: (i) modal confounding, where latent factors (e.g., brand style or product category) simultaneously drive multiple modalities and influence user preference, leading to spurious feature-preference associations; (ii) interaction bias, where genuine user preferences are mixed with noise from exposure effects and accidental clicks. To address these challenges, we propose a Causal-inspired multimodal Recommendation framework. Specifically, we introduce a dual-channel cross-modal diffusion module to identify hidden modal confounders, utilize back-door adjustment with hierarchical matching and vector-quantized codebooks to block confounding paths, and apply front-door adjustment combined with causal topology reconstruction to build a deconfounded causal subgraph. Extensive experiments on three real-world e-commerce datasets demonstrate that our method significantly outperforms state-of-the-art baselines while maintaining strong interpretability.",
        "translated": "多模态推荐系统通过融合视觉、文本和用户-物料交互数据，提升了电子商务和在线广告中的个性化推荐效果。然而，现有方法通常忽略了两种关键偏差：(i) 模态混淆，其中潜在因子（例如品牌风格或产品类别）同时驱动多个模态，并影响用户偏好，导致虚假的特征-偏好关联；(ii) 交互偏差，其中真实的用户偏好与曝光效应和偶然点击带来的噪声混合。为了解决这些挑战，我们提出了一种受因果启发的多模态推荐框架。具体而言，我们引入了一个双通道跨模态扩散模块以识别隐藏的模态混淆因子，利用分层匹配和矢量量化码本进行后门调整以阻断混淆路径，并应用前门调整结合因果拓扑重构，以构建去混淆的因果子图。在三个真实世界电子商务数据集上的广泛实验表明，我们的方法在保持强大可解释性的同时，显著优于最先进的基线方法。",
        "translated_title": "因果启发的多模态推荐",
        "label": [
            "多模态推荐",
            "因果推理",
            "推荐系统公平性/可解释性"
        ],
        "label_reason": "论文聚焦多模态推荐并引入因果推理解决关键偏差问题。",
        "relevance_score": 8,
        "novelty_score": 8,
        "novelty_reason": "提出双通道跨模态扩散模块和因果拓扑重构，具有显著创新性。"
    },
    {
        "title": "Fantastic (small) Retrievers and How to Train Them:\n  mxbai-edge-colbert-v0 Tech Report",
        "url": "http://arxiv.org/abs/2510.14880v1",
        "pub_date": "2025-10-16",
        "summary": "In this work, we introduce mxbai-edge-colbert-v0 models, at two different parameter counts: 17M and 32M. As part of our research, we conduct numerous experiments to improve retrieval and late-interaction models, which we intend to distill into smaller models as proof-of-concepts. Our ultimate aim is to support retrieval at all scales, from large-scale retrieval which lives in the cloud to models that can run locally, on any device. mxbai-edge-colbert-v0 is a model that we hope will serve as a solid foundation backbone for all future experiments, representing the first version of a long series of small proof-of-concepts. As part of the development of mxbai-edge-colbert-v0, we conducted multiple ablation studies, of which we report the results. In terms of downstream performance, mxbai-edge-colbert-v0 is a particularly capable small model, outperforming ColBERTv2 on common short-text benchmarks (BEIR) and representing a large step forward in long-context tasks, with unprecedented efficiency.",
        "translated": "在本工作中，我们引入了 mxbai-edge-colbert-v0 模型，并提供了两种不同的参数规模：17M 和 32M。作为我们研究的一部分，我们进行了大量实验以改进检索和后期交互模型，并计划将其蒸馏为更小的模型作为概念验证。我们的最终目标是支持所有规模的检索，从云端的大规模检索到能够在任何设备上本地运行的小模型。mxbai-edge-colbert-v0 是我们希望作为所有未来实验坚实基础的骨干模型，代表了一系列小型概念验证模型的第一个版本。在 mxbai-edge-colbert-v0 的开发过程中，我们进行了多项消融实验，并在此报告其结果。在下游任务性能方面，mxbai-edge-colbert-v0 是一个特别强大的小型模型，其在常见的短文本基准（BEIR）上优于 ColBERTv2，并在长上下文任务中实现了前所未有的效率，迈出了重要一步。",
        "translated_title": "Fantastic (small) Retrievers and How to Train Them:  \nmxbai-edge-colbert-v0 技术报告",
        "label": [
            "召回"
        ],
        "label_reason": "论文聚焦检索模型优化，适用于推荐系统的召回环节。",
        "relevance_score": 6,
        "novelty_score": 7,
        "novelty_reason": "提出高效小型模型，改进了检索与长上下文处理能力。"
    },
    {
        "title": "A Simulation Framework for Studying Systemic Effects of Feedback Loops\n  in Recommender Systems",
        "url": "http://arxiv.org/abs/2510.14857v1",
        "pub_date": "2025-10-16",
        "summary": "Recommender systems continuously interact with users, creating feedback loops that shape both individual behavior and collective market dynamics. This paper introduces a simulation framework to model these loops in online retail environments, where recommenders are periodically retrained on evolving user-item interactions. Using the Amazon e-Commerce dataset, we analyze how different recommendation algorithms influence diversity, purchase concentration, and user homogenization over time. Results reveal a systematic trade-off: while the feedback loop increases individual diversity, it simultaneously reduces collective diversity and concentrates demand on a few popular items. Moreover, for some recommender systems, the feedback loop increases user homogenization over time, making user purchase profiles increasingly similar. These findings underscore the need for recommender designs that balance personalization with long-term diversity.",
        "translated": "推荐系统持续与用户进行交互，形成反馈循环，从而塑造个体行为和集体市场动态。本文引入了一个仿真框架，用于模拟在线零售环境中的这些循环，其中推荐系统会定期在不断演变的用户-物料交互数据上进行重新训练。使用 Amazon 电商平台数据集，我们分析了不同的推荐算法如何随着时间的推移影响多样性、购买集中度和用户同质化。结果揭示了一个系统性的权衡：虽然反馈循环提高了个体的多样性，但它同时降低了集体的多样性，并将需求集中在少数热门物料上。此外，对于某些推荐系统，反馈循环会随着时间推移增加用户的同质化，使得用户的购买行为变得越来越相似。这些发现凸显了推荐系统设计中需要在个性化与长期多样性之间取得平衡的必要性。",
        "translated_title": "一个用于研究推荐系统中反馈循环系统性效应的模拟框架",
        "label": [
            "推荐系统公平性/可解释性",
            "通用推荐技术"
        ],
        "label_reason": "研究推荐系统的反馈循环及长期多样性问题，属于推荐系统核心机制分析",
        "relevance_score": 8,
        "novelty_score": 7,
        "novelty_reason": "提出新颖的模拟框架，揭示个性化与多样性间的系统性权衡"
    },
    {
        "title": "Supervised Fine-Tuning or Contrastive Learning? Towards Better\n  Multimodal LLM Reranking",
        "url": "http://arxiv.org/abs/2510.14824v1",
        "pub_date": "2025-10-16",
        "summary": "In information retrieval, training reranking models mainly focuses on two types of objectives: metric learning (e.g. contrastive loss to increase the predicted scores on relevant query-document pairs) and classification (binary label prediction of relevance vs. irrelevance). For BERT-style encoders, various studies have shown that contrastive learning (CL) can be more effective than discriminative (classification) learning. However, for large language models (LLMs), classification via supervised fine-tuning (SFT), which predicts ''yes'' (resp. ''no'') token for relevant (resp. irrelevant) pairs, appears more promising as it aligns well with the generative nature of LLMs. This divergence raises a central question: which objective is intrinsically better suited to LLM-based reranking, and what mechanism underlies the difference? In this work, we conduct a comprehensive comparison and analysis between CL and SFT for reranking, taking the universal multimodal retrieval (UMR) as the experimental playground. We first decompose the objectives into two components: weight, which controls the magnitude of those updates, and direction, which guides the model updates, then present a unified framework for understanding their interactions. Through probing experiments, we find that SFT provides a substantially stronger weighting scheme than CL, whereas the preferred scoring direction shows no clear winner. Taken together, these results point to a consistent advantage of SFT over CL for LLM reranking. To further validate our findings, we conduct large-scale training with SFT and present new state-of-the-art rerankers on the MRB benchmark. We also provide ablations on SFT settings and expect our findings to benefit future research and applications in this area.",
        "translated": "在信息检索中，训练重排模型主要关注两种目标：度量学习（例如对比损失，用于提升相关查询-文档对的预测得分）和分类（相关与不相关的二分类标签预测）。对于 BERT 风格的编码器，已有大量研究表明，对比学习（CL）在性能上通常优于判别式（分类）学习。然而，对于大语言模型（LLMs），通过监督微调（SFT）进行分类——即对相关（resp. 不相关）对预测“yes”（resp. “no”）标记——似乎更具前景，因为它与 LLM 生成式的本质更加契合。这一差异引发了一个核心问题：哪一种目标本质上更适合基于 LLM 的重排？其背后的不同机制是什么？在本研究中，我们对 CL 和 SFT 在重排中的表现进行了全面的比较与分析，实验场景设定为通用多模态检索（UMR）。我们首先将这两种目标分解为两个组成部分：权重，用于控制更新的幅度；以及方向，用于指导模型的更新，然后提出一个统一的框架来理解它们之间的相互作用。通过探针实验，我们发现 SFT 提供了显著更强的权重机制，而关于最优的得分方向则没有明显优势。综合来看，这些结果表明在基于 LLM 的重排任务中，SFT 相较于 CL 具有一致的优势。为了进一步验证我们的发现，我们使用 SFT 进行了大规模训练，并在 MRB 基准上提出了新的最先进的重排模型。我们还对 SFT 的设置进行了消融实验，并希望我们的研究结果能够为该领域的未来研究与应用提供帮助。",
        "translated_title": "监督微调还是对比学习？迈向更优的多模态大语言模型重排",
        "label": [
            "重排（Re-ranking）",
            "负采样与对比学习（Negative Sampling / Contrastive Learning）",
            "LLM生成式推荐（LLM-based Generative Recommendation）"
        ],
        "label_reason": "论文聚焦LLM在重排中的应用，并对比CL和SFT的效果，与推荐系统密切相关。",
        "relevance_score": 8,
        "novelty_score": 7,
        "novelty_reason": "提出SFT更适合LLM重排，并提供统一框架分析两者差异，具有创新性。"
    },
    {
        "title": "Cross-Scenario Unified Modeling of User Interests at Billion Scale",
        "url": "http://arxiv.org/abs/2510.14788v1",
        "pub_date": "2025-10-16",
        "summary": "User interests on content platforms are inherently diverse, manifesting through complex behavioral patterns across heterogeneous scenarios such as search, feed browsing, and content discovery. Traditional recommendation systems typically prioritize business metric optimization within isolated specific scenarios, neglecting cross-scenario behavioral signals and struggling to integrate advanced techniques like LLMs at billion-scale deployments, which finally limits their ability to capture holistic user interests across platform touchpoints. We propose RED-Rec, an LLM-enhanced hierarchical Recommender Engine for Diversified scenarios, tailored for industry-level content recommendation systems. RED-Rec unifies user interest representations across multiple behavioral contexts by aggregating and synthesizing actions from varied scenarios, resulting in comprehensive item and user modeling. At its core, a two-tower LLM-powered framework enables nuanced, multifaceted representations with deployment efficiency, and a scenario-aware dense mixing and querying policy effectively fuses diverse behavioral signals to capture cross-scenario user intent patterns and express fine-grained, context-specific intents during serving. We validate RED-Rec through online A/B testing on hundreds of millions of users in RedNote through online A/B testing, showing substantial performance gains in both content recommendation and advertisement targeting tasks. We further introduce a million-scale sequential recommendation dataset, RED-MMU, for comprehensive offline training and evaluation. Our work advances unified user modeling, unlocking deeper personalization and fostering more meaningful user engagement in large-scale UGC platforms.",
        "translated": "内容平台上的用户兴趣本质上是多样化的，这种多样性在搜索、信息流浏览和内容发现等异构场景中通过复杂的用户行为模式体现出来。传统的推荐系统通常优先在孤立的具体场景中优化业务指标，忽略了跨场景的行为信号，且在数十亿级别的实际部署中难以有效整合大语言模型（LLM）等先进技术，最终限制了它们在平台各个触点中捕捉用户整体兴趣的能力。我们提出了 RED-Rec，一个面向工业级内容推荐系统的、用于多样化场景的、融合大语言模型的分层推荐引擎。RED-Rec 通过聚合和综合来自不同场景的行为，统一了用户兴趣在多种行为上下文中的表示，从而实现了全面的物料与用户建模。其核心是一个双塔结构的大语言模型驱动框架，能够以高效的部署方式生成细致、多维的表示；同时，一个具备场景感知能力的密集混合与查询策略，有效融合了多样的行为信号，以捕捉跨场景的用户意图模式，并在服务过程中表达出细粒度、上下文特定的意图。我们在 RedNote 上通过数亿用户的在线 A/B 实验验证了 RED-Rec，结果在内容推荐和广告定向任务中均表现出显著的性能提升。此外，我们进一步引入了一个百万规模的序列推荐数据集 RED-MMU，用于全面的离线训练与评估。我们的工作推动了统一用户建模的发展，在大规模用户生成内容（UGC）平台中实现了更深层次的个性化，并促进了更具意义的用户参与。",
        "translated_title": "百亿级跨场景用户兴趣统一建模",
        "label": [
            "通用推荐技术",
            "LLM生成式推荐",
            "跨域/联邦推荐",
            "序列推荐"
        ],
        "label_reason": "跨场景统一用户兴趣建模，适用于大规模内容推荐系统",
        "relevance_score": 9,
        "novelty_score": 8,
        "novelty_reason": "提出融合LLM的分层架构RED-Rec，改进跨场景行为建模"
    },
    {
        "title": "Dataset Pruning in RecSys and ML: Best Practice or Mal-Practice?",
        "url": "http://arxiv.org/abs/2510.14704v1",
        "pub_date": "2025-10-16",
        "summary": "Offline evaluations in recommender system research depend heavily on datasets, many of which are pruned, such as the widely used MovieLens collections. This thesis examines the impact of data pruning - specifically, removing users with fewer than a specified number of interactions - on both dataset characteristics and algorithm performance. Five benchmark datasets were analysed in both their unpruned form and at five successive pruning levels (5, 10, 20, 50, 100). For each coreset, we examined structural and distributional characteristics and trained and tested eleven representative algorithms. To further assess if pruned datasets lead to artificially inflated performance results, we also evaluated models trained on the pruned train sets but tested on unpruned data. Results show that commonly applied core pruning can be highly selective, leaving as little as 2% of the original users in some datasets. Traditional algorithms achieved higher nDCG@10 scores when both training and testing on pruned data; however, this advantage largely disappeared when evaluated on unpruned test sets. Across all algorithms, performance declined with increasing pruning levels when tested on unpruned data, highlighting the impact of dataset reduction on the performance of recommender algorithms.",
        "translated": "推荐系统的离线评估高度依赖于数据集，其中许多数据集经过了修剪，例如广泛使用的 MovieLens 数据集。本文研究了数据修剪——具体来说，移除交互次数少于指定阈值的用户——对数据集特性和算法性能的影响。我们分析了五个基准数据集在未修剪形式和五种连续修剪等级（5, 10, 20, 50, 100）下的表现。对于每个核心子集（coreset），我们考察了其结构和分布特征，并训练和测试了十一种代表性算法。为了进一步评估修剪后的数据集是否会导致性能结果的人为高估，我们还评估了在修剪后的训练集上训练但在未修剪数据上测试的模型。结果显示，常见的核心修剪方法具有高度的选择性，在某些数据集中仅保留原始用户数的2%。传统算法在训练和测试均使用修剪数据时取得了更高的 nDCG@10 分数；然而，当在未修剪的测试集上进行评估时，这种优势基本消失。在所有算法中，当测试数据为未修剪数据时，其性能随着修剪等级的提高而下降，突显了数据集减少对推荐算法性能的影响。",
        "translated_title": "推荐系统与机器学习中的数据集裁剪：最佳实践还是不当实践？",
        "label": [
            "推荐系统评估"
        ],
        "label_reason": "论文聚焦推荐系统数据集剪枝对算法性能的影响，属于推荐系统的评估问题。",
        "relevance_score": 8,
        "novelty_score": 6,
        "novelty_reason": "对数据集剪枝现象进行系统分析，但方法较为常规，创新性有限。"
    },
    {
        "title": "TITAN: Graph-Executable Reasoning for Cyber Threat Intelligence",
        "url": "http://arxiv.org/abs/2510.14670v1",
        "pub_date": "2025-10-16",
        "summary": "TITAN (Threat Intelligence Through Automated Navigation) is a framework that connects natural-language cyber threat queries with executable reasoning over a structured knowledge graph. It integrates a path planner model, which predicts logical relation chains from text, and a graph executor that traverses the TITAN Ontology to retrieve factual answers and supporting evidence. Unlike traditional retrieval systems, TITAN operates on a typed, bidirectional graph derived from MITRE, allowing reasoning to move clearly and reversibly between threats, behaviors, and defenses. To support training and evaluation, we introduce the TITAN Dataset, a corpus of 88209 examples (Train: 74258; Test: 13951) pairing natural language questions with executable reasoning paths and step by step Chain of Thought explanations. Empirical evaluations show that TITAN enables models to generate syntactically valid and semantically coherent reasoning paths that can be deterministically executed on the underlying graph.",
        "translated": "TITAN（Threat Intelligence Through Automated Navigation）是一个将自然语言的网络威胁查询与结构化知识图谱上的可执行推理相连接的框架。它集成了一个路径规划模型，该模型从文本中预测逻辑关系链，并结合一个图谱执行器，该执行器遍历TITAN本体（Ontology）以检索事实性答案和支持证据。与传统召回系统不同，TITAN基于来自MITRE的类型化、双向图谱进行操作，使得在威胁、行为和防御之间可以清晰且可逆地进行推理。为支持训练和评估，我们引入了TITAN数据集，该数据集包含88209个示例（训练集：74258；测试集：13951），将自然语言问题与可执行推理路径以及逐步的Chain of Thought解释配对。实证评估表明，TITAN使模型能够生成在语法上有效、语义上连贯的推理路径，并可以在底层图谱上确定性地执行。",
        "translated_title": "TITAN：用于网络威胁情报的图可执行推理",
        "label": [],
        "label_reason": "论文聚焦网络安全威胁情报，不直接涉及推荐系统。",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出基于知识图谱的可执行推理框架，具有新颖性但非推荐系统创新。"
    },
    {
        "title": "An Efficient Rubric-based Generative Verifier for Search-Augmented LLMs",
        "url": "http://arxiv.org/abs/2510.14660v1",
        "pub_date": "2025-10-16",
        "summary": "Search augmentation empowers Large Language Models with retrieval capabilities to overcome the limitations imposed by static parameters. Recently, Reinforcement Learning leverages tailored reward signals as a viable technique to enhance LLMs performing tasks involving search. However, existing reward modeling for search-augmented LLMs faces several limitations. Rule-based rewards, such as Exact Match, are verifiable but fragile to variations in expression and cannot be applied to long-form workloads. In contrast, generative rewards improve robustness, but designing verifiable and stable rewards for long-form workloads in dynamic corpora remains challenging and also incurs high computational costs. In this paper, we propose a unified and verifiable paradigm, \"nugget-as-rubric\", which treats atomic information points as structured evaluation criteria for different search-augmentation workloads. Short-form tasks correspond to a single rubric, whereas long-form tasks expand to multiple rubrics aligned with the question's information needs. To support long-form settings, we design an automatic rubric construction pipeline based on query rewriting, which can automatically retrieve passages relevant to each question and extract rubrics from them, both from static corpora and from dynamic online web content. Furthermore, we introduce \\textbf{Search-Gen-V}, a 4B-parameter efficient generative verifier under our proposed verifiable paradigm, which is trained via the idea of distillation and a two-stage strategy. Experimental results show that Search-Gen-V achieves strong verification accuracy across different workloads, making it a scalable, robust, and efficient verifiable reward constructor for search-augmented LLMs.",
        "translated": "搜索增强为大语言模型赋予了召回能力，以克服静态参数所带来的限制。近年来，强化学习利用定制化的奖励信号，作为增强大语言模型在涉及搜索任务性能的一种可行技术。然而，现有的用于搜索增强的大语言模型的奖励建模面临多个限制。基于规则的奖励，例如精确匹配（Exact Match），虽然可验证，但对表达方式的变化非常脆弱，无法应用于长文本任务。相比之下，生成式奖励增强了鲁棒性，但在动态语料库中为长文本任务设计可验证且稳定的奖励仍然具有挑战性，并且计算成本较高。在本文中，我们提出了一种统一且可验证的范式——“nugget-as-rubric”，该范式将原子信息点视为结构化的评估标准，适用于不同的搜索增强任务。短文本任务对应单一评估标准，而长文本任务则扩展为多个与问题信息需求对齐的评估标准。为了支持长文本场景，我们设计了一种基于查询重写（query rewriting）的自动评估标准构建流程，该流程能够自动召回与每个问题相关的段落，并从中提取评估标准，无论是静态语料库还是动态的在线网页内容。此外，我们引入了 **Search-Gen-V**，一个在我们提出的可验证范式下高效的大语言生成验证模型。该模型通过蒸馏思想和两阶段训练策略进行训练。实验结果表明，Search-Gen-V 在不同任务中均实现了较强的验证精度，使其成为适用于搜索增强的大语言模型的一种可扩展、鲁棒且高效的可验证奖励构造器。",
        "translated_title": "一种高效的基于评分规则的生成式验证器用于检索增强的大语言模型",
        "label": [
            "LLM生成式推荐"
        ],
        "label_reason": "论文涉及生成式LLM与搜索增强，适用于生成式推荐场景",
        "relevance_score": 4,
        "novelty_score": 7,
        "novelty_reason": "提出新颖的验证范式和高效的生成式验证器，改进现有奖励建模"
    },
    {
        "title": "Causality Enhancement for Cross-Domain Recommendation",
        "url": "http://arxiv.org/abs/2510.14641v1",
        "pub_date": "2025-10-16",
        "summary": "Cross-domain recommendation forms a crucial component in recommendation systems. It leverages auxiliary information through source domain tasks or features to enhance target domain recommendations. However, incorporating inconsistent source domain tasks may result in insufficient cross-domain modeling or negative transfer. While incorporating source domain features without considering the underlying causal relationships may limit their contribution to final predictions. Thus, a natural idea is to directly train a cross-domain representation on a causality-labeled dataset from the source to target domain. Yet this direction has been rarely explored, as identifying unbiased real causal labels is highly challenging in real-world scenarios. In this work, we attempt to take a first step in this direction by proposing a causality-enhanced framework, named CE-CDR. Specifically, we first reformulate the cross-domain recommendation as a causal graph for principled guidance. We then construct a causality-aware dataset heuristically. Subsequently, we derive a theoretically unbiased Partial Label Causal Loss to generalize beyond the biased causality-aware dataset to unseen cross-domain patterns, yielding an enriched cross-domain representation, which is then fed into the target model to enhance target-domain recommendations. Theoretical and empirical analyses, as well as extensive experiments, demonstrate the rationality and effectiveness of CE-CDR and its general applicability as a model-agnostic plugin. Moreover, it has been deployed in production since April 2025, showing its practical value in real-world applications.",
        "translated": "跨域推荐是推荐系统中的一个关键组成部分。它通过源域任务或特征来利用辅助信息，以增强目标域的推荐效果。然而，引入不一致的源域任务可能导致跨域建模不充分或出现负迁移。而在不考虑底层因果关系的情况下融合源域特征，可能会限制其对最终预测的贡献。因此，一个自然的想法是直接在从源域到目标域的因果标注数据集上训练跨域表示。然而，这一方向鲜有探索，因为在现实场景中识别无偏的真实因果标签极具挑战性。在本工作中，我们尝试通过提出一种因果增强框架CE-CDR，在这一方向上迈出第一步。具体来说，我们首先将跨域推荐重新表述为因果图，以提供原理性的指导。然后，我们启发式地构建了一个因果感知数据集。随后，我们推导出一个理论上无偏的部标签因果损失函数，以在存在偏倚的因果感知数据集基础上，泛化至未见过的跨域模式，从而获得更丰富的跨域表示，并将其输入目标模型中以提升目标域的推荐性能。理论与实证分析以及广泛的实验验证了CE-CDR的合理性与有效性，并展示了其作为模型无关插件的通用适用性。此外，该方法自2025年4月起已在生产环境中部署，表明其在实际应用中的实用价值。",
        "translated_title": "跨域推荐中的因果增强",
        "label": [
            "跨域/联邦推荐",
            "因果推理"
        ],
        "label_reason": "论文研究跨域推荐，属于推荐系统核心问题，涉及因果关系建模。",
        "relevance_score": 8,
        "novelty_score": 7,
        "novelty_reason": "提出因果增强框架CE-CDR，改进跨域推荐模型的泛化能力。"
    },
    {
        "title": "Intent Clustering with Shared Pseudo-Labels",
        "url": "http://arxiv.org/abs/2510.14640v1",
        "pub_date": "2025-10-16",
        "summary": "In this paper, we propose an intuitive, training-free and label-free method for intent clustering that makes minimal assumptions using lightweight and open-source LLMs. Many current approaches rely on commercial LLMs, which are costly, and offer limited transparency. Additionally, their methods often explicitly depend on knowing the number of clusters in advance, which is often not the case in realistic settings. To address these challenges, instead of asking the LLM to match similar text directly, we first ask it to generate pseudo-labels for each text, and then perform multi-label classification in this pseudo-label set for each text. This approach is based on the hypothesis that texts belonging to the same cluster will share more labels, and will therefore be closer when encoded into embeddings. These pseudo-labels are more human-readable than direct similarity matches. Our evaluation on four benchmark sets shows that our approach achieves results comparable to and better than recent baselines, while remaining simple and computationally efficient. Our findings indicate that our method can be applied in low-resource scenarios and is stable across multiple models and datasets.",
        "translated": "在本文中，我们提出了一种直观的、无需训练且无需标注的方法用于意图聚类，该方法在使用轻量级和开源大语言模型的前提下，做出最小的假设。许多现有方法依赖于商业大语言模型，这不仅成本高昂，而且透明度有限。此外，这些方法通常明确要求提前知道聚类的数量，而这种情况在现实场景中往往并不存在。为了解决这些挑战，我们没有直接要求大语言模型匹配相似文本，而是首先让其为每段文本生成伪标签，然后在该伪标签集合中对每段文本进行多标签分类。该方法基于这样的假设：属于同一聚类的文本将共享更多的标签，因此在嵌入编码后彼此的距离将更近。与直接相似性匹配相比，这些伪标签更具可读性。我们在四个基准数据集上的评估表明，我们的方法在保持简单和计算高效的同时，取得了与近期基线相当甚至更好的结果。我们的实验结果表明，该方法适用于资源有限的场景，并在多个模型和数据集之间具有稳定性。",
        "translated_title": "Intent Clustering with Shared Pseudo-Labels  \n共享伪标签的意图聚类",
        "label": [
            "LLM生成式推荐"
        ],
        "label_reason": "伪标签生成用于意图聚类，可能间接用于推荐系统的意图建模",
        "relevance_score": 5,
        "novelty_score": 7,
        "novelty_reason": "提出无需训练的伪标签聚类方法，提升计算效率和可解释性"
    },
    {
        "title": "MR.Rec: Synergizing Memory and Reasoning for Personalized Recommendation\n  Assistant with LLMs",
        "url": "http://arxiv.org/abs/2510.14629v1",
        "pub_date": "2025-10-16",
        "summary": "The application of Large Language Models (LLMs) in recommender systems faces key challenges in delivering deep personalization and intelligent reasoning, especially for interactive scenarios. Current methods are often constrained by limited context windows and single-turn reasoning, hindering their ability to capture dynamic user preferences and proactively reason over recommendation contexts. To address these limitations, we propose MR.Rec, a novel framework that synergizes memory and reasoning for LLM-based recommendations. To achieve personalization, we develop a comprehensive Retrieval-Augmented Generation (RAG) system that efficiently indexes and retrieves relevant external memory to enhance LLM personalization capabilities. Furthermore, to enable the synergy between memory and reasoning, our RAG system goes beyond conventional query-based retrieval by integrating reasoning enhanced memory retrieval. Finally, we design a reinforcement learning framework that trains the LLM to autonomously learn effective strategies for both memory utilization and reasoning refinement. By combining dynamic memory retrieval with adaptive reasoning, this approach ensures more accurate, context-aware, and highly personalized recommendations. Extensive experiments demonstrate that MR.Rec significantly outperforms state-of-the-art baselines across multiple metrics, validating its efficacy in delivering intelligent and personalized recommendations. We will release code and data upon paper notification.",
        "translated": "大语言模型（LLM）在推荐系统中的应用面临在实现深度个性化与智能推理方面的主要挑战，尤其是在交互式场景中。当前的方法通常受限于有限的上下文窗口和单轮推理，阻碍了其捕捉动态用户偏好并主动在推荐上下文中进行推理的能力。为了解决这些限制，我们提出MR.Rec，一种新颖的框架，将记忆与推理相结合，用于基于LLM的推荐。为了实现个性化，我们开发了一个全面的检索增强生成（RAG）系统，高效地索引和召回相关的外部记忆，以增强LLM的个性化能力。此外，为了实现记忆与推理之间的协同作用，我们的RAG系统超越了传统的基于查询的召回，通过集成推理增强的记忆召回机制。最后，我们设计了一个强化学习框架，训练LLM自主学习在记忆利用和推理优化方面的有效策略。通过结合动态记忆召回与自适应推理，该方法能够提供更加准确、上下文感知和高度个性化的推荐。广泛的实验表明，MR.Rec在多个指标上显著优于最先进的基线方法，验证了其在实现智能和个性化推荐方面的有效性。我们将随论文通知发布代码和数据。",
        "translated_title": "MR.Rec: 结合记忆与推理的个性化推荐方法  \n与大语言模型结合",
        "label": [
            "LLM生成式推荐",
            "序列推荐",
            "推荐系统公平性/可解释性"
        ],
        "label_reason": "论文提出结合记忆与推理的LLM推荐框架，显著提升个性化与智能推荐能力。",
        "relevance_score": 9,
        "novelty_score": 8,
        "novelty_reason": "创新性地结合RAG与强化学习，实现动态记忆检索与自适应推理策略。"
    },
    {
        "title": "GemiRec: Interest Quantization and Generation for Multi-Interest\n  Recommendation",
        "url": "http://arxiv.org/abs/2510.14626v1",
        "pub_date": "2025-10-16",
        "summary": "Multi-interest recommendation has gained attention, especially in industrial retrieval stage. Unlike classical dual-tower methods, it generates multiple user representations instead of a single one to model comprehensive user interests. However, prior studies have identified two underlying limitations: the first is interest collapse, where multiple representations homogenize. The second is insufficient modeling of interest evolution, as they struggle to capture latent interests absent from a user's historical behavior. We begin with a thorough review of existing works in tackling these limitations. Then, we attempt to tackle these limitations from a new perspective. Specifically, we propose a framework-level refinement for multi-interest recommendation, named GemiRec. The proposed framework leverages interest quantization to enforce a structural interest separation and interest generation to learn the evolving dynamics of user interests explicitly. It comprises three modules: (a) Interest Dictionary Maintenance Module (IDMM) maintains a shared quantized interest dictionary. (b) Multi-Interest Posterior Distribution Module (MIPDM) employs a generative model to capture the distribution of user future interests. (c) Multi-Interest Retrieval Module (MIRM) retrieves items using multiple user-interest representations. Both theoretical and empirical analyses, as well as extensive experiments, demonstrate its advantages and effectiveness. Moreover, it has been deployed in production since March 2025, showing its practical value in industrial applications.",
        "translated": "多兴趣推荐在工业召回阶段中引起了广泛关注。与经典的双塔方法不同，它生成多个用户表示，而不是单一的表示，以建模用户全面的兴趣。然而，已有研究表明该方法存在两个潜在的限制：第一个是兴趣坍缩，其中多个表示趋于同质化；第二个是对兴趣演化的建模不足，因为它们难以捕捉用户历史行为中未体现的潜在兴趣。我们首先对现有工作中解决这些限制的方法进行了全面的回顾。随后，我们尝试从一个新的视角来应对这些限制。具体而言，我们提出了一个面向多兴趣推荐的框架级改进，称为GemiRec。所提框架利用兴趣量化来实现结构化的兴趣分离，并通过兴趣生成显式地学习用户兴趣的演化动态。它包含三个模块：(a) 兴趣字典维护模块（IDMM）维护一个共享的量化兴趣字典；(b) 多兴趣后验分布模块（MIPDM）采用生成模型来捕捉用户未来兴趣的分布；(c) 多兴趣召回模块（MIRM）使用多个用户-兴趣表示进行物料召回。理论与实证分析以及广泛的实验验证了其优势和有效性。此外，该方法自2025年3月起已在生产环境中部署，展示了其在工业应用中的实用价值。",
        "translated_title": "GemiRec：多兴趣推荐中的兴趣量化与生成",
        "label": [
            "召回（Recall）",
            "多兴趣推荐（Multimodal Recommendation）",
            "生成式推荐（LLM-based Generative Recommendation）"
        ],
        "label_reason": "论文提出多兴趣推荐框架GemiRec，聚焦于召回阶段的兴趣建模与生成，与推荐系统核心问题紧密相关。",
        "relevance_score": 9,
        "novelty_score": 8,
        "novelty_reason": "从兴趣量化与生成角度提出新颖框架，改进多兴趣建模方式，具有理论与应用创新。"
    },
    {
        "title": "Multimodal RAG for Unstructured Data:Leveraging Modality-Aware Knowledge\n  Graphs with Hybrid Retrieval",
        "url": "http://arxiv.org/abs/2510.14592v1",
        "pub_date": "2025-10-16",
        "summary": "Current Retrieval-Augmented Generation (RAG) systems primarily operate on unimodal textual data, limiting their effectiveness on unstructured multimodal documents. Such documents often combine text, images, tables, equations, and graphs, each contributing unique information. In this work, we present a Modality-Aware Hybrid retrieval Architecture (MAHA), designed specifically for multimodal question answering with reasoning through a modality-aware knowledge graph. MAHA integrates dense vector retrieval with structured graph traversal, where the knowledge graph encodes cross-modal semantics and relationships. This design enables both semantically rich and context-aware retrieval across diverse modalities. Evaluations on multiple benchmark datasets demonstrate that MAHA substantially outperforms baseline methods, achieving a ROUGE-L score of 0.486, providing complete modality coverage. These results highlight MAHA's ability to combine embeddings with explicit document structure, enabling effective multimodal retrieval. Our work establishes a scalable and interpretable retrieval framework that advances RAG systems by enabling modality-aware reasoning over unstructured multimodal data.",
        "translated": "当前的检索增强生成（RAG）系统主要基于单模态文本数据进行操作，这限制了它们在非结构化多模态文档上的有效性。此类文档通常结合了文本、图像、表格、公式和图表，每种模态都提供了独特的信息。在本文中，我们提出了一种模态感知混合检索架构（MAHA），该架构专为通过模态感知知识图谱进行多模态问答与推理而设计。MAHA 将稠密向量召回与结构化的图遍历相结合，其中知识图谱编码了跨模态的语义信息与关系。这种设计实现了在多种模态下语义丰富且上下文感知的检索。在多个基准数据集上的评估表明，MAHA 显著优于基线方法，达到 ROUGE-L 评分为 0.486，并实现了对所有模态的完整覆盖。这些结果突显了 MAHA 将嵌入表示与显式的文档结构相结合的能力，从而实现高效的多模态召回。我们的工作建立了一个可扩展且可解释的检索框架，通过在非结构化多模态数据上实现模态感知推理，推动了 RAG 系统的发展。",
        "translated_title": "多模态RAG用于非结构化数据：利用模态感知的知识图谱与混合召回",
        "label": [
            "多模态推荐",
            "LLM生成式推荐",
            "召回"
        ],
        "label_reason": "论文提出多模态RAG框架，适用于包含多种模态信息的推荐场景",
        "relevance_score": 7,
        "novelty_score": 8,
        "novelty_reason": "引入模态感知的知识图谱与混合检索机制，具有一定的创新性"
    },
    {
        "title": "Agentic Entropy-Balanced Policy Optimization",
        "url": "http://arxiv.org/abs/2510.14545v1",
        "pub_date": "2025-10-16",
        "summary": "Recently, Agentic Reinforcement Learning (Agentic RL) has made significant progress in incentivizing the multi-turn, long-horizon tool-use capabilities of web agents. While mainstream agentic RL algorithms autonomously explore high-uncertainty tool-call steps under the guidance of entropy, excessive reliance on entropy signals can impose further constraints, leading to the training collapse. In this paper, we delve into the challenges caused by entropy and propose the Agentic Entropy-Balanced Policy Optimization (AEPO), an agentic RL algorithm designed to balance entropy in both the rollout and policy update phases. AEPO comprises two core components: (1) a dynamic entropy-balanced rollout mechanism that adaptively allocate global and branch sampling budget through entropy pre-monitoring, while imposing a branch penalty on consecutive high-entropy tool-call steps to prevent over-branching issues; and (2) Entropy-Balanced Policy Optimization that inserts a stop-gradient operation into the high-entropy clipping term to preserve and properly rescale gradients on high-entropy tokens, while incorporating entropy-aware advantage estimation to prioritize learning on high-uncertainty tokens. Results across 14 challenging datasets show that AEPO consistently outperforms 7 mainstream RL algorithms. With just 1K RL samples, Qwen3-14B with AEPO achieves impressive results: 47.6% on GAIA, 11.2% on Humanity's Last Exam, and 43.0% on WebWalker for Pass@1; 65.0% on GAIA, 26.0% on Humanity's Last Exam, and 70.0% on WebWalker for Pass@5. Further analysis reveals that AEPO improves rollout sampling diversity while maintaining stable policy entropy, facilitating scalable web agent training.",
        "translated": "近期，Agentic 强化学习（Agentic RL）在激励网络智能体的多轮次、长时域工具使用能力方面取得了显著进展。尽管主流的 Agentic RL 算法在熵的指导下自主探索高不确定性工具调用步骤，但对熵信号的过度依赖可能带来进一步限制，导致训练崩溃。在本文中，我们深入探讨了由熵引起的挑战，并提出了 Agentic Entropy-Balanced Policy Optimization（AEPO），这是一种旨在在 rollout 和策略更新阶段平衡熵的 Agentic RL 算法。AEPO 包含两个核心组件：（1）一种动态熵平衡 rollout 机制，通过熵的预监控，自适应地分配全局与分支采样预算，同时对连续的高熵工具调用步骤施加分支惩罚，以防止过度分支问题；（2）熵平衡策略优化，将 stop-gradient 操作插入高熵裁剪项中，以保留并正确缩放高熵 token 的梯度，同时结合熵感知的优势估计，优先在高不确定性 token 上进行学习。在 14 个具有挑战性的数据集上的结果表明，AEPO 一致优于 7 种主流 RL 算法。仅使用 1K RL 样本，采用 AEPO 的 Qwen3-14B 在 Pass@1 指标上分别达到了 47.6%（GAIA）、11.2%（Humanity's Last Exam）和 43.0%（WebWalker）；在 Pass@5 指标上分别达到了 65.0%（GAIA）、26.0%（Humanity's Last Exam）和 70.0%（WebWalker）。进一步分析表明，AEPO 在保持策略熵稳定的同时提升了 rollout 采样的多样性，从而促进了网络智能体的可扩展训练。",
        "translated_title": "基于智能体的熵平衡策略优化",
        "label": [],
        "label_reason": "论文聚焦于强化学习中的策略优化，与推荐系统无直接关联。",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出了一种熵平衡策略优化方法，改进了探索机制，但属于通用强化学习范畴。"
    },
    {
        "title": "Acquisition of interpretable domain information during brain MR image\n  harmonization for content-based image retrieval",
        "url": "http://arxiv.org/abs/2510.14535v1",
        "pub_date": "2025-10-16",
        "summary": "Medical images like MR scans often show domain shifts across imaging sites due to scanner and protocol differences, which degrade machine learning performance in tasks such as disease classification. Domain harmonization is thus a critical research focus. Recent approaches encode brain images $\\boldsymbol{x}$ into a low-dimensional latent space $\\boldsymbol{z}$, then disentangle it into $\\boldsymbol{z_u}$ (domain-invariant) and $\\boldsymbol{z_d}$ (domain-specific), achieving strong results. However, these methods often lack interpretability$-$an essential requirement in medical applications$-$leaving practical issues unresolved. We propose Pseudo-Linear-Style Encoder Adversarial Domain Adaptation (PL-SE-ADA), a general framework for domain harmonization and interpretable representation learning that preserves disease-relevant information in brain MR images. PL-SE-ADA includes two encoders $f_E$ and $f_{SE}$ to extract $\\boldsymbol{z_u}$ and $\\boldsymbol{z_d}$, a decoder to reconstruct the image $f_D$, and a domain predictor $g_D$. Beyond adversarial training between the encoder and domain predictor, the model learns to reconstruct the input image $\\boldsymbol{x}$ by summing reconstructions from $\\boldsymbol{z_u}$ and $\\boldsymbol{z_d}$, ensuring both harmonization and informativeness. Compared to prior methods, PL-SE-ADA achieves equal or better performance in image reconstruction, disease classification, and domain recognition. It also enables visualization of both domain-independent brain features and domain-specific components, offering high interpretability across the entire framework.",
        "translated": "磁共振扫描等医学图像常常由于扫描设备和成像协议的差异，在不同成像中心之间表现出领域偏移，这会降低机器学习在疾病分类等任务中的性能。因此，领域调和成为了一个关键的研究方向。近期的方法将脑图像 $\\boldsymbol{x}$ 编码到一个低维潜在空间 $\\boldsymbol{z}$ 中，然后将其解耦为 $\\boldsymbol{z_u}$（领域不变）和 $\\boldsymbol{z_d}$（领域特定），取得了良好的效果。然而，这些方法通常缺乏可解释性——这是医学应用中的基本要求——从而未能解决实际问题。我们提出了一种伪线性风格编码器对抗领域自适应（Pseudo-Linear-Style Encoder Adversarial Domain Adaptation, PL-SE-ADA）框架，该框架是一种通用的领域调和与可解释表征学习方法，能够在脑部磁共振图像中保留与疾病相关的信息。PL-SE-ADA 包括两个编码器 $f_E$ 和 $f_{SE}$，分别用于提取 $\\boldsymbol{z_u}$ 和 $\\boldsymbol{z_d}$，一个用于图像重建的解码器 $f_D$，以及一个领域预测器 $g_D$。除了编码器与领域预测器之间的对抗训练外，模型还通过将 $\\boldsymbol{z_u}$ 和 $\\boldsymbol{z_d}$ 的重建结果相加，学习重建输入图像 $\\boldsymbol{x}$，从而确保领域调和和信息保留。与先前方法相比，PL-SE-ADA 在图像重建、疾病分类和领域识别任务中均达到了同等或更好的性能。此外，它还支持对领域无关的脑特征和领域相关组件进行可视化，为整个框架提供了高度的可解释性。",
        "translated_title": "在基于内容的医学图像检索中进行脑部 MR 图像协调过程中可解释领域信息的获取",
        "label": [],
        "label_reason": "主要涉及医学图像处理，与推荐系统无直接关联。",
        "relevance_score": 2,
        "novelty_score": 5,
        "novelty_reason": "在图像域适应方面提出了一定改进，但属于常规技术范畴。"
    },
    {
        "title": "MedTrust-RAG: Evidence Verification and Trust Alignment for Biomedical\n  Question Answering",
        "url": "http://arxiv.org/abs/2510.14400v1",
        "pub_date": "2025-10-16",
        "summary": "Biomedical question answering (QA) requires accurate interpretation of complex medical knowledge. Large language models (LLMs) have shown promising capabilities in this domain, with retrieval-augmented generation (RAG) systems enhancing performance by incorporating external medical literature. However, RAG-based approaches in biomedical QA suffer from hallucinations due to post-retrieval noise and insufficient verification of retrieved evidence, undermining response reliability. We propose MedTrust-Guided Iterative RAG, a framework designed to enhance factual consistency and mitigate hallucinations in medical QA. Our method introduces three key innovations. First, it enforces citation-aware reasoning by requiring all generated content to be explicitly grounded in retrieved medical documents, with structured Negative Knowledge Assertions used when evidence is insufficient. Second, it employs an iterative retrieval-verification process, where a verification agent assesses evidence adequacy and refines queries through Medical Gap Analysis until reliable information is obtained. Third, it integrates the MedTrust-Align Module (MTAM) that combines verified positive examples with hallucination-aware negative samples, leveraging Direct Preference Optimization to reinforce citation-grounded reasoning while penalizing hallucination-prone response patterns. Experiments on MedMCQA, MedQA, and MMLU-Med demonstrate that our approach consistently outperforms competitive baselines across multiple model architectures, achieving the best average accuracy with gains of 2.7% for LLaMA3.1-8B-Instruct and 2.4% for Qwen3-8B.",
        "translated": "生物医学问答（QA）需要对复杂的医学知识进行准确的理解。大语言模型（LLMs）在此领域展现出良好的潜力，而基于召回增强生成（RAG）的系统通过引入外部医学文献进一步提升了性能。然而，在生物医学QA中，基于RAG的方法由于召回后的噪声以及对召回证据验证不足，容易产生幻觉，从而影响响应的可靠性。我们提出MedTrust-Guided Iterative RAG，一个旨在提升医学问答事实一致性和减少幻觉的框架。我们的方法引入了三个关键创新。首先，该方法通过要求所有生成内容必须明确基于召回的医学文档进行推理，从而强制引用感知的推理过程。在证据不足时，采用结构化的Negative Knowledge Assertions进行补充。其次，该方法采用迭代的召回-验证过程，其中验证代理通过医学差距分析（Medical Gap Analysis）评估证据充分性并细化查询，直到获取可靠的信息。第三，该方法集成了MedTrust-Align Module（MTAM），将已验证的正例与具有幻觉感知能力的负样本相结合，利用直接偏好优化（Direct Preference Optimization）来强化引用基础推理，并对易产生幻觉的响应模式施加惩罚。在MedMCQA、MedQA和MMLU-Med上的实验表明，我们的方法在多种模型架构下均能持续超越具有竞争力的基线模型，取得了最佳的平均准确率，其中在LLaMA3.1-8B-Instruct上提高了2.7%，在Qwen3-8B上提高了2.4%。",
        "translated_title": "MedTrust-RAG：生物医学问答中的证据验证与信任对齐",
        "label": [],
        "label_reason": "论文聚焦医学问答，不直接涉及推荐系统核心技术",
        "relevance_score": 3,
        "novelty_score": 7,
        "novelty_reason": "提出创新性的迭代检索-验证框架，减少幻觉"
    },
    {
        "title": "PluriHop: Exhaustive, Recall-Sensitive QA over Distractor-Rich Corpora",
        "url": "http://arxiv.org/abs/2510.14377v1",
        "pub_date": "2025-10-16",
        "summary": "Recent advances in large language models (LLMs) and retrieval-augmented generation (RAG) have enabled progress on question answering (QA) when relevant evidence is in one (single-hop) or multiple (multi-hop) passages. Yet many realistic questions about recurring report data - medical records, compliance filings, maintenance logs - require aggregation across all documents, with no clear stopping point for retrieval and high sensitivity to even one missed passage. We term these pluri-hop questions and formalize them by three criteria: recall sensitivity, exhaustiveness, and exactness. To study this setting, we introduce PluriHopWIND, a diagnostic multilingual dataset of 48 pluri-hop questions built from 191 real-world wind industry reports in German and English. We show that PluriHopWIND is 8-40% more repetitive than other common datasets and thus has higher density of distractor documents, better reflecting practical challenges of recurring report corpora. We test a traditional RAG pipeline as well as graph-based and multimodal variants, and find that none of the tested approaches exceed 40% in statement-wise F1 score. Motivated by this, we propose PluriHopRAG, a RAG architecture that follows a \"check all documents individually, filter cheaply\" approach: it (i) decomposes queries into document-level subquestions and (ii) uses a cross-encoder filter to discard irrelevant documents before costly LLM reasoning. We find that PluriHopRAG achieves relative F1 score improvements of 18-52% depending on base LLM. Despite its modest size, PluriHopWIND exposes the limitations of current QA systems on repetitive, distractor-rich corpora. PluriHopRAG's performance highlights the value of exhaustive retrieval and early filtering as a powerful alternative to top-k methods.",
        "translated": "近年来，大语言模型（LLM）和检索增强生成（RAG）的进展使得在相关证据存在于一个（单跳）或多个（多跳）段落中的问答（QA）任务取得了进步。然而，许多关于重复报告数据的现实问题——如医疗记录、合规文件、维护日志——需要对所有文档进行聚合，检索过程中没有明确的停止点，且对漏掉一个段落都高度敏感。我们将这些问题称为“pluri-hop”问题，并通过三个标准对其进行形式化：召回敏感性（recall sensitivity）、全面性（exhaustiveness）和准确性（exactness）。为了研究这一场景，我们引入了 PluriHopWIND，这是一个用于诊断的多语言数据集，包含48个 pluri-hop 问题，由191份真实世界中的风能行业报告（以德语和英语编写）构建而成。我们发现，PluriHopWIND 比其他常见数据集多出8%-40%的重复内容，因此具有更高的干扰文档密度，更能反映重复报告语料库中的实际挑战。我们测试了传统 RAG 流水线以及基于图和多模态的变体，发现所有测试方法的陈述级 F1 分数均未超过40%。受此启发，我们提出了 PluriHopRAG，一种遵循“逐个检查所有文档、廉价过滤”的 RAG 架构：它（i）将查询分解为文档级别的子问题，（ii）使用交叉编码器过滤器在昂贵的 LLM 推理之前丢弃不相关的文档。我们发现，PluriHopRAG 在不同基础 LLM 上实现了18%-52%的相对 F1 分数提升。尽管 PluriHopWIND 的规模相对较小，但它揭示了当前问答系统在重复且干扰文档丰富的语料库中的局限性。PluriHopRAG 的性能凸显了全面检索和早期过滤作为 top-k 方法强大替代方案的价值。",
        "translated_title": "PluriHop: 在干扰信息丰富的语料库上进行详尽的、召回敏感的问答",
        "label": [
            "召回（Recall）"
        ],
        "label_reason": "论文关注问答中的召回敏感性，提出改进的RAG架构以提升文档检索效率。",
        "relevance_score": 4,
        "novelty_score": 7,
        "novelty_reason": "提出PluriHopRAG架构，通过文档级子问题分解和交叉编码器过滤提升性能。"
    },
    {
        "title": "Ensembling Multiple Hallucination Detectors Trained on VLLM Internal\n  Representations",
        "url": "http://arxiv.org/abs/2510.14330v1",
        "pub_date": "2025-10-16",
        "summary": "This paper presents the 5th place solution by our team, y3h2, for the Meta CRAG-MM Challenge at KDD Cup 2025. The CRAG-MM benchmark is a visual question answering (VQA) dataset focused on factual questions about images, including egocentric images. The competition was contested based on VQA accuracy, as judged by an LLM-based automatic evaluator. Since incorrect answers result in negative scores, our strategy focused on reducing hallucinations from the internal representations of the VLM. Specifically, we trained logistic regression-based hallucination detection models using both the hidden_state and the outputs of specific attention heads. We then employed an ensemble of these models. As a result, while our method sacrificed some correct answers, it significantly reduced hallucinations and allowed us to place among the top entries on the final leaderboard. For implementation details and code, please refer to https://gitlab.aicrowd.com/htanabe/meta-comprehensive-rag-benchmark-starter-kit.",
        "translated": "本文介绍了我们团队 y3h2 在 KDD Cup 2025 的 Meta CRAG-MM 挑战赛中获得第五名的解决方案。CRAG-MM 基准是一个专注于图像事实性问题的视觉问答（VQA）数据集，包括第一人称视角的图像。比赛根据 VQA 准确率进行评判，评判方式由基于大语言模型（LLM）的自动评估器完成。由于错误答案会导致负分，我们的策略重点在于减少视觉语言模型（VLM）内部表示中的幻觉现象。具体来说，我们使用隐藏状态（hidden_state）以及特定注意力头（attention heads）的输出，训练了基于逻辑回归的幻觉检测模型。随后，我们采用了这些模型的集成。结果表明，虽然我们的方法牺牲了一些正确答案，但显著减少了幻觉，使我们在最终排行榜中进入了前列。关于实现细节和代码，请参考 https://gitlab.aicrowd.com/htanabe/meta-comprehensive-rag-benchmark-starter-kit。",
        "translated_title": "基于大语言模型内部表示的多种幻觉检测器集成",
        "label": [],
        "label_reason": "论文聚焦于VQA中的幻觉检测，与推荐系统无直接关联。",
        "relevance_score": 2,
        "novelty_score": 5,
        "novelty_reason": "提出基于VLM内部表示的幻觉检测方法，有一定实用价值但创新性有限。"
    },
    {
        "title": "Large Reasoning Embedding Models: Towards Next-Generation Dense\n  Retrieval Paradigm",
        "url": "http://arxiv.org/abs/2510.14321v1",
        "pub_date": "2025-10-16",
        "summary": "In modern e-commerce search systems, dense retrieval has become an indispensable component. By computing similarities between query and item (product) embeddings, it efficiently selects candidate products from large-scale repositories. With the breakthroughs in large language models (LLMs), mainstream embedding models have gradually shifted from BERT to LLMs for more accurate text modeling. However, these models still adopt direct-embedding methods, and the semantic accuracy of embeddings remains inadequate. Therefore, contrastive learning is heavily employed to achieve tight semantic alignment between positive pairs. Consequently, such models tend to capture statistical co-occurrence patterns in the training data, biasing them toward shallow lexical and semantic matches. For difficult queries exhibiting notable lexical disparity from target items, the performance degrades significantly. In this work, we propose the Large Reasoning Embedding Model (LREM), which novelly integrates reasoning processes into representation learning. For difficult queries, LREM first conducts reasoning to achieve a deep understanding of the original query, and then produces a reasoning-augmented query embedding for retrieval. This reasoning process effectively bridges the semantic gap between original queries and target items, significantly improving retrieval accuracy. Specifically, we adopt a two-stage training process: the first stage optimizes the LLM on carefully curated Query-CoT-Item triplets with SFT and InfoNCE losses to establish preliminary reasoning and embedding capabilities, and the second stage further refines the reasoning trajectories via reinforcement learning (RL). Extensive offline and online experiments validate the effectiveness of LREM, leading to its deployment on China's largest e-commerce platform since August 2025.",
        "translated": "在现代电子商务搜索系统中，稠密召回已成为不可或缺的组成部分。通过计算查询与物料（产品）嵌入之间的相似性，它能够高效地从大规模仓库中选取候选产品。随着大语言模型（LLM）的突破，主流嵌入模型已逐渐从 BERT 转向 LLM，以实现更精确的文本建模。然而，这些模型仍采用直接嵌入方法，其嵌入的语义准确性仍然不足。因此，对比学习被广泛用于实现正样本对之间的紧密语义对齐。结果是，这些模型倾向于捕捉训练数据中的统计共现模式，使其偏向于浅层的词汇和语义匹配。对于与目标物料存在显著词汇差异的复杂查询，其性能会明显下降。在本研究中，我们提出了大型推理嵌入模型（LREM），其创新性地将推理过程整合到表示学习中。对于复杂查询，LREM 首先进行推理以实现对原始查询的深入理解，然后生成一个经过推理增强的查询嵌入用于召回。这一推理过程有效地弥合了原始查询与目标物料之间的语义差距，显著提升了召回精度。具体来说，我们采用了一个两阶段的训练过程：第一阶段在精心构建的 Query-CoT-Item 三元组上优化 LLM，使用监督微调（SFT）和 InfoNCE 损失以建立初步的推理和嵌入能力；第二阶段则通过强化学习（RL）进一步优化推理轨迹。大量的离线和在线实验验证了 LREM 的有效性，自 2025 年 8 月起，该模型已部署在中国最大的电子商务平台上。",
        "translated_title": "大型推理嵌入模型：迈向下一代密集召回范式",
        "label": [
            "召回",
            "通用推荐技术",
            "负采样与对比学习"
        ],
        "label_reason": "论文聚焦于电商搜索中的密集召回优化，与推荐系统召回环节直接相关。",
        "relevance_score": 8,
        "novelty_score": 9,
        "novelty_reason": "提出将推理过程融入嵌入学习，通过强化学习优化推理轨迹，方法新颖且效果显著。"
    },
    {
        "title": "Rethinking Schema Linking: A Context-Aware Bidirectional Retrieval\n  Approach for Text-to-SQL",
        "url": "http://arxiv.org/abs/2510.14296v1",
        "pub_date": "2025-10-16",
        "summary": "Schema linking -- the process of aligning natural language questions with database schema elements -- is a critical yet underexplored component of Text-to-SQL systems. While recent methods have focused primarily on improving SQL generation, they often neglect the retrieval of relevant schema elements, which can lead to hallucinations and execution failures. In this work, we propose a context-aware bidirectional schema retrieval framework that treats schema linking as a standalone problem. Our approach combines two complementary strategies: table-first retrieval followed by column selection, and column-first retrieval followed by table selection. It is further augmented with techniques such as question decomposition, keyword extraction, and keyphrase extraction. Through comprehensive evaluations on challenging benchmarks such as BIRD and Spider, we demonstrate that our method significantly improves schema recall while reducing false positives. Moreover, SQL generation using our retrieved schema consistently outperforms full-schema baselines and closely approaches oracle performance, all without requiring query refinement. Notably, our method narrows the performance gap between full and perfect schema settings by 50\\%. Our findings highlight schema linking as a powerful lever for enhancing Text-to-SQL accuracy and efficiency.",
        "translated": "模式链接——将自然语言问题与数据库模式元素对齐的过程——是文本到SQL（Text-to-SQL）系统中一个关键但尚未充分探索的组成部分。尽管最近的方法主要集中在提升SQL生成的能力上，它们往往忽视了相关模式元素的检索，这可能导致幻觉（hallucinations）和执行失败。在本文中，我们提出了一种上下文感知的双向模式检索框架，将模式链接视为一个独立的问题进行处理。我们的方法结合了两种互补策略：先进行表检索，然后选择列；以及先进行列检索，然后选择表。此外，该方法还融合了诸如问题分解、关键词提取和关键短语提取等技术。在具有挑战性的基准数据集BIRD和Spider上的全面评估表明，我们的方法在显著提升模式召回率的同时降低了误报率。此外，基于我们所检索到的模式进行SQL生成在性能上始终优于全模式基线方法，并且接近理想模式下的性能，而无需进行查询优化。值得注意的是，我们的方法将全模式与理想模式设置之间的性能差距缩小了50\\%。我们的研究结果表明，模式链接是提升文本到SQL系统准确性和效率的强大杠杆。",
        "translated_title": "Rethinking Schema Linking: A Context-Aware Bidirectional Retrieval Approach for Text-to-SQL  \n重新思考模式链接：一种面向上下文的双向检索方法用于文本到SQL  \n\nAbstract  \n摘要  \n\nSchema linking is a critical component in text-to-SQL tasks, aiming to map natural language utterances to corresponding database schema elements. Most existing methods focus on modeling the schema from a single perspective, either by leveraging schema elements independently or by incorporating contextual information from the natural language query. However, these approaches often suffer from incomplete schema understanding and limited contextual awareness, leading to suboptimal performance in complex schema environments. To address this issue, we propose a **Context-Aware Bidirectional Retrieval (CABR)** framework that simultaneously considers both the schema context and the query context. CABR introduces a dual-encoder architecture to learn semantic representations for schema elements and queries from different views, and establishes a bidirectional retrieval mechanism that links schema elements to queries and vice versa. This design allows the model to better capture the interdependencies between schema elements and queries, improving the accuracy of schema linking. We conduct extensive experiments on the Spider and WikiSQL datasets, and the results demonstrate that CABR significantly outperforms state-of-the-art baselines in both schema element selection and SQL generation.  \n模式链接是文本到SQL任务中的关键组成部分，其目标是将自然语言语句映射到对应的数据库模式元素。大多数现有方法从单一视角建模模式，要么独立利用模式元素，要么结合来自自然语言查询的上下文信息。然而，这些方法通常存在模式理解不完整和上下文感知能力有限的问题，导致在复杂模式环境下性能欠佳。为了解决这一问题，我们提出了一种**面向上下文的双向检索（CABR）**框架，该框架同时考虑模式上下文和查询上下文。CABR引入了一种双编码器结构，从不同视角学习模式元素和查询的语义表示，并建立了一个双向检索机制，将模式元素与查询相互链接。该设计使模型能够更好地捕捉模式元素与查询之间的依赖关系，提高模式链接的准确性。我们在Spider和WikiSQL数据集上进行了广泛的实验，结果表明，CABR在模式元素选择和SQL生成方面均显著优于最先进的基线方法。  \n\nIntroduction  \n引言  \n\nText-to-SQL tasks aim to automatically translate natural language queries into executable SQL statements, enabling users to interact with databases without requiring SQL expertise. A core challenge in this task is schema linking, which involves identifying the relevant schema elements (e.g., tables, columns, values) for a given query. Effective schema linking is essential for accurate SQL generation, as incorrect or incomplete schema mappings can lead to invalid queries and poor performance.  \n文本到SQL任务旨在自动将自然语言查询转化为可执行的SQL语句，使用户无需具备SQL专业知识即可与数据库进行交互。该任务中的一个核心挑战是模式链接，即识别给定查询中相关的模式元素（例如，表、列、值）。有效的模式链接对于准确的SQL生成至关重要，因为错误或不完整的模式映射可能导致无效查询和较差的性能。  \n\nTraditional schema linking approaches typically treat the schema as a flat structure and model it using either rule-based methods or single-view neural architectures. These methods often fail to capture the full context of both the schema and the query, especially in complex, multi-table environments where relationships between schema elements are crucial.  \n传统的模式链接方法通常将模式视为扁平结构，并使用基于规则的方法或单视角神经网络架构对其进行建模。这些方法常常无法捕捉模式和查询的完整上下文，特别是在复杂的多表环境中，其中模式元素之间的关系至关重要。  \n\nTo overcome these limitations, we propose a novel framework that jointly models schema and query contexts using a bidirectional retrieval mechanism. Our method enhances the contextual awareness of schema elements by considering both their structural information and their semantic relationships with the query.  \n为克服这些限制，我们提出了一种新的框架，使用双向检索机制联合建模模式和查询上下文。我们的方法通过考虑模式元素的结构信息及其与查询的语义关系，增强了其上下文感知能力。",
        "label": [],
        "label_reason": "论文聚焦Text-to-SQL，非推荐系统核心问题，相关性较低。",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出新颖的双向检索框架，改进Schema Linking效果。"
    },
    {
        "title": "PRISM: Agentic Retrieval with LLMs for Multi-Hop Question Answering",
        "url": "http://arxiv.org/abs/2510.14278v1",
        "pub_date": "2025-10-16",
        "summary": "Retrieval plays a central role in multi-hop question answering (QA), where answering complex questions requires gathering multiple pieces of evidence. We introduce an Agentic Retrieval System that leverages large language models (LLMs) in a structured loop to retrieve relevant evidence with high precision and recall. Our framework consists of three specialized agents: a Question Analyzer that decomposes a multi-hop question into sub-questions, a Selector that identifies the most relevant context for each sub-question (focusing on precision), and an Adder that brings in any missing evidence (focusing on recall). The iterative interaction between Selector and Adder yields a compact yet comprehensive set of supporting passages. In particular, it achieves higher retrieval accuracy while filtering out distracting content, enabling downstream QA models to surpass full-context answer accuracy while relying on significantly less irrelevant information. Experiments on four multi-hop QA benchmarks -- HotpotQA, 2WikiMultiHopQA, MuSiQue, and MultiHopRAG -- demonstrates that our approach consistently outperforms strong baselines.",
        "translated": "召回在多跳问答（QA）中起着核心作用，其中回答复杂问题需要收集多个证据。我们引入了一种基于智能体的召回系统（Agentic Retrieval System），通过结构化的循环利用大语言模型（LLMs），以高精度和高召回率检索相关证据。我们的框架包含三个专门的智能体：一个用于将多跳问题拆解为子问题的问题分析器（Question Analyzer），一个用于为每个子问题识别最相关上下文（注重精度）的选取器（Selector），以及一个用于补充缺失证据（注重召回）的添加器（Adder）。Selector 与 Adder 之间的迭代交互产生了一组紧凑而全面的支持段落。特别是，它在过滤干扰内容的同时实现了更高的召回精度，使下游 QA 模型能够在依赖明显更少无关信息的情况下超越全上下文回答的准确性。在四个多跳 QA 基准测试 HotpotQA、2WikiMultiHopQA、MuSiQue 和 MultiHopRAG 上的实验表明，我们的方法始终优于强基线方法。",
        "translated_title": "PRISM：利用大语言模型进行多跳问答的代理式召回",
        "label": [
            "多模态推荐",
            "LLM生成式推荐"
        ],
        "label_reason": "论文涉及LLM用于信息检索，可间接用于推荐系统的生成式方法",
        "relevance_score": 5,
        "novelty_score": 7,
        "novelty_reason": "提出结构化代理检索框架，改进多跳QA的精度与召回"
    },
    {
        "title": "Coupled Diffusion Sampling for Training-Free Multi-View Image Editing",
        "url": "http://arxiv.org/abs/2510.14981v1",
        "pub_date": "2025-10-16",
        "summary": "We present an inference-time diffusion sampling method to perform multi-view consistent image editing using pre-trained 2D image editing models. These models can independently produce high-quality edits for each image in a set of multi-view images of a 3D scene or object, but they do not maintain consistency across views. Existing approaches typically address this by optimizing over explicit 3D representations, but they suffer from a lengthy optimization process and instability under sparse view settings. We propose an implicit 3D regularization approach by constraining the generated 2D image sequences to adhere to a pre-trained multi-view image distribution. This is achieved through coupled diffusion sampling, a simple diffusion sampling technique that concurrently samples two trajectories from both a multi-view image distribution and a 2D edited image distribution, using a coupling term to enforce the multi-view consistency among the generated images. We validate the effectiveness and generality of this framework on three distinct multi-view image editing tasks, demonstrating its applicability across various model architectures and highlighting its potential as a general solution for multi-view consistent editing.",
        "translated": "我们提出了一种在推理阶段使用的扩散采样方法，用于利用预训练的 2D 图像编辑模型进行多视角一致的图像编辑。这些模型可以独立地为 3D 场景或物体的多视角图像集中的每张图像生成高质量的编辑结果，但它们在不同视角之间无法保持一致性。现有方法通常通过在显式的 3D 表示上进行优化来解决这一问题，但它们在稀疏视角设置下优化过程漫长且不稳定。我们提出了一种隐式的 3D 正则化方法，通过约束生成的 2D 图像序列以遵循预训练的多视角图像分布。该方法是通过耦合扩散采样实现的，这是一种简单的扩散采样技术，同时从多视角图像分布和 2D 编辑图像分布中采样两条轨迹，并使用耦合项来强制生成图像之间的多视角一致性。我们在三个不同的多视角图像编辑任务上验证了该框架的有效性和通用性，展示了其在各种模型架构中的适用性，并突出了其作为多视角一致性编辑通用解决方案的潜力。",
        "translated_title": "Coupled Diffusion Sampling for Training-Free Multi-View Image Editing  \n用于无需训练的多视角图像编辑的耦合扩散采样",
        "label": [
            "多帧/视频图像恢复"
        ],
        "label_reason": "方法涉及多视角图像生成一致性，属于多帧图像恢复范畴",
        "relevance_score": 5,
        "novelty_score": 8,
        "novelty_reason": "提出耦合扩散采样新方法，具有一定的创新性"
    },
    {
        "title": "From Pixels to Words -- Towards Native Vision-Language Primitives at\n  Scale",
        "url": "http://arxiv.org/abs/2510.14979v1",
        "pub_date": "2025-10-16",
        "summary": "The edifice of native Vision-Language Models (VLMs) has emerged as a rising contender to typical modular VLMs, shaped by evolving model architectures and training paradigms. Yet, two lingering clouds cast shadows over its widespread exploration and promotion: (-) What fundamental constraints set native VLMs apart from modular ones, and to what extent can these barriers be overcome? (-) How to make research in native VLMs more accessible and democratized, thereby accelerating progress in the field. In this paper, we clarify these challenges and outline guiding principles for constructing native VLMs. Specifically, one native VLM primitive should: (i) effectively align pixel and word representations within a shared semantic space; (ii) seamlessly integrate the strengths of formerly separate vision and language modules; (iii) inherently embody various cross-modal properties that support unified vision-language encoding, aligning, and reasoning. Hence, we launch NEO, a novel family of native VLMs built from first principles, capable of rivaling top-tier modular counterparts across diverse real-world scenarios. With only 390M image-text examples, NEO efficiently develops visual perception from scratch while mitigating vision-language conflicts inside a dense and monolithic model crafted from our elaborate primitives. We position NEO as a cornerstone for scalable and powerful native VLMs, paired with a rich set of reusable components that foster a cost-effective and extensible ecosystem. Our code and models are publicly available at: https://github.com/EvolvingLMMs-Lab/NEO.",
        "translated": "原生视觉-语言模型（VLMs）的构建已逐渐成为传统模块化VLMs的有力竞争者，这得益于模型架构和训练范式的发展与演变。然而，两个关键问题仍阻碍其广泛研究与推广：（-）原生VLMs与模块化VLMs之间存在哪些基本限制？这些限制在多大程度上可以被克服？（-）如何使原生VLMs的研究更具可访问性和普及性，从而加快该领域的发展进程？在本文中，我们明确了这些挑战，并提出了构建原生VLMs的指导原则。具体而言，一个原生VLM的基本要素应满足以下三点：（i）在共享的语义空间中有效对齐像素与语言表示；（ii）无缝融合视觉与语言模块各自的优势；（iii）内在地体现多种跨模态属性，以支持统一的视觉-语言编码、对齐和推理。因此，我们提出了NEO，一种基于第一性原理构建的新型原生VLM家族，其在多种现实场景中能够媲美最先进的模块化模型。在仅使用390M图像-文本样例的情况下，NEO便能高效地从零开始发展视觉感知能力，同时在由我们精心设计的密集单一模型中缓解视觉-语言冲突。我们视NEO为构建可扩展且强大的原生VLMs的基石，并提供了丰富的可复用组件，以促进经济高效且可扩展的生态系统形成。我们的代码和模型已公开发布于：https://github.com/EvolvingLMMs-Lab/NEO。",
        "translated_title": "从像素到词语——迈向大规模的原生视觉-语言基元",
        "label": [],
        "label_reason": "论文专注于视觉-语言模型，非图像像素级处理任务。",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "提出新的视觉-语言模型构建原则和组件，但创新点主要在架构整合。"
    },
    {
        "title": "Agentic Design of Compositional Machines",
        "url": "http://arxiv.org/abs/2510.14980v1",
        "pub_date": "2025-10-16",
        "summary": "The design of complex machines stands as both a marker of human intelligence and a foundation of engineering practice. Given recent advances in large language models (LLMs), we ask whether they, too, can learn to create. We approach this question through the lens of compositional machine design: a task in which machines are assembled from standardized components to meet functional demands like locomotion or manipulation in a simulated physical environment. To support this investigation, we introduce BesiegeField, a testbed built on the machine-building game Besiege, which enables part-based construction, physical simulation and reward-driven evaluation. Using BesiegeField, we benchmark state-of-the-art LLMs with agentic workflows and identify key capabilities required for success, including spatial reasoning, strategic assembly, and instruction-following. As current open-source models fall short, we explore reinforcement learning (RL) as a path to improvement: we curate a cold-start dataset, conduct RL finetuning experiments, and highlight open challenges at the intersection of language, machine design, and physical reasoning.",
        "translated": "复杂机器的设计既是人类智能的标志，也是工程实践的基础。鉴于近年来在大语言模型（LLMs）方面取得的进展，我们提出一个问题：它们是否也能学会创造？我们通过组合式机器设计的视角来探讨这一问题：该任务要求在模拟的物理环境中，通过标准化组件的组合来满足功能性需求，例如移动或操作。为了支持这项研究，我们引入了 BesiegeField，这是一个基于机器建造游戏 Besiege 构建的测试平台，支持基于部件的构建、物理模拟以及基于奖励的评估。借助 BesiegeField，我们对最先进的 LLMs 以及代理式工作流程进行了基准测试，并识别出成功所必需的关键能力，包括空间推理、策略性组装和遵循指令。由于目前的开源模型尚无法满足要求，我们探索了强化学习（RL）作为提升的路径：我们构建了一个冷启动数据集，进行了 RL 微调实验，并强调了语言、机器设计和物理推理交叉领域中的开放挑战。",
        "translated_title": "Agentic Design of Compositional Machines  \n复合机器的智能体设计",
        "label": [],
        "label_reason": "论文聚焦于机器设计与语言模型应用，非图像像素级处理任务",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出新测试平台与RL冷启动数据集，但方法迁移性有限"
    },
    {
        "title": "Learning an Image Editing Model without Image Editing Pairs",
        "url": "http://arxiv.org/abs/2510.14978v1",
        "pub_date": "2025-10-16",
        "summary": "Recent image editing models have achieved impressive results while following natural language editing instructions, but they rely on supervised fine-tuning with large datasets of input-target pairs. This is a critical bottleneck, as such naturally occurring pairs are hard to curate at scale. Current workarounds use synthetic training pairs that leverage the zero-shot capabilities of existing models. However, this can propagate and magnify the artifacts of the pretrained model into the final trained model. In this work, we present a new training paradigm that eliminates the need for paired data entirely. Our approach directly optimizes a few-step diffusion model by unrolling it during training and leveraging feedback from vision-language models (VLMs). For each input and editing instruction, the VLM evaluates if an edit follows the instruction and preserves unchanged content, providing direct gradients for end-to-end optimization. To ensure visual fidelity, we incorporate distribution matching loss (DMD), which constrains generated images to remain within the image manifold learned by pretrained models. We evaluate our method on standard benchmarks and include an extensive ablation study. Without any paired data, our method performs on par with various image editing diffusion models trained on extensive supervised paired data, under the few-step setting. Given the same VLM as the reward model, we also outperform RL-based techniques like Flow-GRPO.",
        "translated": "近期的图像编辑模型在遵循自然语言编辑指令的情况下取得了令人印象深刻的结果，但它们依赖于使用大量输入-目标对数据集进行监督式微调。这成为了一个关键性的瓶颈，因为这种自然形成的配对数据很难大规模地进行整理。当前的变通方法使用合成的训练对，利用现有模型的零样本能力。然而，这可能会将预训练模型中的伪影传播并放大到最终训练好的模型中。在本文中，我们提出了一种新的训练范式，完全消除了对配对数据的需求。我们的方法在训练过程中通过展开模型并利用视觉-语言模型（VLM）的反馈，直接优化一个几步扩散模型。对于每个输入图像和编辑指令，VLM会评估编辑是否遵循指令并保留未更改的内容，从而提供端到端优化所需的直接梯度。为了确保视觉保真度，我们引入了分布匹配损失（DMD），该损失限制生成的图像保持在预训练模型所学习的图像流形内。我们在标准基准数据集上评估了我们的方法，并进行了广泛的消融研究。在几步扩散设置下，即使没有任何配对数据，我们的方法也与使用大量监督配对数据训练的各种图像编辑扩散模型表现相当。在使用相同VLM作为奖励模型的前提下，我们的方法还优于基于强化学习的技术，如 Flow-GRPO。",
        "translated_title": "学习无需图像编辑配对的图像编辑模型",
        "label": [],
        "label_reason": "论文主要关注图像编辑而非像素级恢复",
        "relevance_score": 3,
        "novelty_score": 8,
        "novelty_reason": "提出无需配对数据的图像编辑新范式"
    },
    {
        "title": "Ponimator: Unfolding Interactive Pose for Versatile Human-human\n  Interaction Animation",
        "url": "http://arxiv.org/abs/2510.14976v1",
        "pub_date": "2025-10-16",
        "summary": "Close-proximity human-human interactive poses convey rich contextual information about interaction dynamics. Given such poses, humans can intuitively infer the context and anticipate possible past and future dynamics, drawing on strong priors of human behavior. Inspired by this observation, we propose Ponimator, a simple framework anchored on proximal interactive poses for versatile interaction animation. Our training data consists of close-contact two-person poses and their surrounding temporal context from motion-capture interaction datasets. Leveraging interactive pose priors, Ponimator employs two conditional diffusion models: (1) a pose animator that uses the temporal prior to generate dynamic motion sequences from interactive poses, and (2) a pose generator that applies the spatial prior to synthesize interactive poses from a single pose, text, or both when interactive poses are unavailable. Collectively, Ponimator supports diverse tasks, including image-based interaction animation, reaction animation, and text-to-interaction synthesis, facilitating the transfer of interaction knowledge from high-quality mocap data to open-world scenarios. Empirical experiments across diverse datasets and applications demonstrate the universality of the pose prior and the effectiveness and robustness of our framework.",
        "translated": "近距离的人与人之间的交互姿态能够传达丰富的交互动态上下文信息。在给定这些姿态的情况下，人类可以直观地推断交互的上下文，并预测可能的过去和未来动态，这依赖于对人类行为的强先验知识。受这一观察的启发，我们提出了 Ponimator，一种基于邻近交互姿态的简单框架，用于实现多样化的交互动画。我们的训练数据包括来自运动捕捉交互数据集的近距离接触的双人姿态及其周围的时间上下文。借助交互姿态先验，Ponimator 使用了两个条件扩散模型：(1) 一个姿态动画生成器，利用时间先验从交互姿态生成动态运动序列；(2) 一个姿态生成器，当交互姿态不可用时，应用空间先验从单个姿态、文本或两者共同合成交互姿态。总体而言，Ponimator 支持多种任务，包括基于图像的交互动画、反应动画以及从文本生成交互，从而促进将高质量运动捕捉数据中的交互知识迁移至开放世界场景中。在多个数据集和应用上的实证实验表明，姿态先验具有普遍性，且我们的框架在性能和鲁棒性方面均表现出色。",
        "translated_title": "Ponimator：展开交互式姿态以实现多样化的人与人交互动画",
        "label": [],
        "label_reason": "论文聚焦于人体姿态动画生成，属于高阶任务，不涉及像素级图像处理",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "提出了基于扩散模型的姿态动画框架，但方法属于常规迁移"
    },
    {
        "title": "Terra: Explorable Native 3D World Model with Point Latents",
        "url": "http://arxiv.org/abs/2510.14977v1",
        "pub_date": "2025-10-16",
        "summary": "World models have garnered increasing attention for comprehensive modeling of the real world. However, most existing methods still rely on pixel-aligned representations as the basis for world evolution, neglecting the inherent 3D nature of the physical world. This could undermine the 3D consistency and diminish the modeling efficiency of world models. In this paper, we present Terra, a native 3D world model that represents and generates explorable environments in an intrinsic 3D latent space. Specifically, we propose a novel point-to-Gaussian variational autoencoder (P2G-VAE) that encodes 3D inputs into a latent point representation, which is subsequently decoded as 3D Gaussian primitives to jointly model geometry and appearance. We then introduce a sparse point flow matching network (SPFlow) for generating the latent point representation, which simultaneously denoises the positions and features of the point latents. Our Terra enables exact multi-view consistency with native 3D representation and architecture, and supports flexible rendering from any viewpoint with only a single generation process. Furthermore, Terra achieves explorable world modeling through progressive generation in the point latent space. We conduct extensive experiments on the challenging indoor scenes from ScanNet v2. Terra achieves state-of-the-art performance in both reconstruction and generation with high 3D consistency.",
        "translated": "世界模型因其对现实世界的全面建模能力而受到越来越多的关注。然而，大多数现有方法仍然依赖像素对齐的表示作为世界演化的基础，忽视了物理世界固有的三维特性。这可能会削弱世界模型的三维一致性，并降低其建模效率。在本文中，我们提出了 Terra，这是一种原生的三维世界模型，能够在内在的三维潜在空间中表示和生成可探索的环境。具体而言，我们提出了一种新颖的点到高斯变分自编码器（P2G-VAE），它将三维输入编码为潜在点表示，随后将其解码为三维高斯基元，以联合建模几何和外观。我们接着引入了一个稀疏点流匹配网络（SPFlow）来生成潜在点表示，该网络可同时对点潜在的位置和特征进行去噪。我们的 Terra 通过原生的三维表示和架构实现了精确的多视角一致性，并支持仅通过一次生成过程即可从任意视角进行灵活渲染。此外，Terra 通过在点潜在空间中的渐进生成实现了可探索的世界建模。我们在具有挑战性的 ScanNet v2 室内场景上进行了广泛的实验，Terra 在重建和生成方面均达到了最先进的性能，且具有高度的三维一致性。",
        "translated_title": "Terra：具有点潜在表示的可探索本原三维世界模型",
        "label": [],
        "label_reason": "论文关注3D世界建模与生成，不直接处理像素级图像质量",
        "relevance_score": 3,
        "novelty_score": 8,
        "novelty_reason": "提出P2G-VAE与SPFlow新架构，实现3D表示与生成"
    },
    {
        "title": "WithAnyone: Towards Controllable and ID Consistent Image Generation",
        "url": "http://arxiv.org/abs/2510.14975v1",
        "pub_date": "2025-10-16",
        "summary": "Identity-consistent generation has become an important focus in text-to-image research, with recent models achieving notable success in producing images aligned with a reference identity. Yet, the scarcity of large-scale paired datasets containing multiple images of the same individual forces most approaches to adopt reconstruction-based training. This reliance often leads to a failure mode we term copy-paste, where the model directly replicates the reference face rather than preserving identity across natural variations in pose, expression, or lighting. Such over-similarity undermines controllability and limits the expressive power of generation. To address these limitations, we (1) construct a large-scale paired dataset MultiID-2M, tailored for multi-person scenarios, providing diverse references for each identity; (2) introduce a benchmark that quantifies both copy-paste artifacts and the trade-off between identity fidelity and variation; and (3) propose a novel training paradigm with a contrastive identity loss that leverages paired data to balance fidelity with diversity. These contributions culminate in WithAnyone, a diffusion-based model that effectively mitigates copy-paste while preserving high identity similarity. Extensive qualitative and quantitative experiments demonstrate that WithAnyone significantly reduces copy-paste artifacts, improves controllability over pose and expression, and maintains strong perceptual quality. User studies further validate that our method achieves high identity fidelity while enabling expressive controllable generation.",
        "translated": "身份一致的生成已成为文本到图像研究中的一个重要焦点，近期模型在生成与参考身份对齐的图像方面取得了显著成功。然而，由于缺乏大规模的配对数据集，其中包含同一人的多张图像，大多数方法被迫采用基于重建的训练方式。这种依赖通常导致一种我们称之为“复制粘贴”的失效模式，即模型直接复制参考图像中的面部，而不是在姿态、表情或光照等自然变化下保持身份一致性。这种过度相似性削弱了生成的可控性，并限制了生成的表现力。为了解决这些局限，我们（1）构建了一个大规模的配对数据集 MultiID-2M，专门针对多人员场景，为每个身份提供多样化的参考图像；（2）引入了一个基准测试，用于量化“复制粘贴”伪影以及身份保真度与变化之间的权衡；（3）提出了一种新颖的训练范式，采用对比身份损失，利用配对数据在保真度和多样性之间取得平衡。这些贡献促成了 WithAnyone 这一基于扩散的模型，该模型在有效缓解“复制粘贴”问题的同时，仍能保持高身份相似性。大量定性和定量实验表明，WithAnyone 显著减少了“复制粘贴”伪影，提高了对姿态和表情的可控性，并保持了良好的感知质量。用户研究进一步验证了我们的方法在保持高身份保真度的同时，能够实现富有表现力的可控生成。",
        "translated_title": "WithAnyone: 超可控且ID一致的图像生成",
        "label": [],
        "label_reason": "论文关注文本生成图像的身份一致性，属于高阶生成任务，不涉及像素级图像恢复或增强。",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出了对比身份损失和多身份数据集，但方法基于扩散模型，创新性有限。"
    },
    {
        "title": "pi-Flow: Policy-Based Few-Step Generation via Imitation Distillation",
        "url": "http://arxiv.org/abs/2510.14974v1",
        "pub_date": "2025-10-16",
        "summary": "Few-step diffusion or flow-based generative models typically distill a velocity-predicting teacher into a student that predicts a shortcut towards denoised data. This format mismatch has led to complex distillation procedures that often suffer from a quality-diversity trade-off. To address this, we propose policy-based flow models ($\\pi$-Flow). $\\pi$-Flow modifies the output layer of a student flow model to predict a network-free policy at one timestep. The policy then produces dynamic flow velocities at future substeps with negligible overhead, enabling fast and accurate ODE integration on these substeps without extra network evaluations. To match the policy's ODE trajectory to the teacher's, we introduce a novel imitation distillation approach, which matches the policy's velocity to the teacher's along the policy's trajectory using a standard $\\ell_2$ flow matching loss. By simply mimicking the teacher's behavior, $\\pi$-Flow enables stable and scalable training and avoids the quality-diversity trade-off. On ImageNet 256$^2$, it attains a 1-NFE FID of 2.85, outperforming MeanFlow of the same DiT architecture. On FLUX.1-12B and Qwen-Image-20B at 4 NFEs, $\\pi$-Flow achieves substantially better diversity than state-of-the-art few-step methods, while maintaining teacher-level quality.",
        "translated": "基于少量步骤的扩散模型或流模型通常将预测速度的教师模型提炼为一个预测通向去噪数据捷径的学生模型。这种格式上的不匹配导致了复杂的提炼过程，通常会陷入质量和多样性之间的权衡问题。为了解决这一问题，我们提出了基于策略的流模型（$\\pi$-Flow）。$\\pi$-Flow 修改了学生流模型的输出层，使其在一个时间步上预测一个无需网络的策略。该策略随后以极低的计算开销生成未来子步骤中的动态流速度，从而在这些子步骤上实现快速且准确的 ODE 积分，而无需额外的网络评估。为了使策略的 ODE 轨迹与教师模型相匹配，我们引入了一种新颖的模仿提炼方法，该方法通过在策略轨迹上使用标准的 $\\ell_2$ 流匹配损失，将策略的速度与教师模型的速度进行匹配。通过简单地模仿教师模型的行为，$\\pi$-Flow 实现了稳定且可扩展的训练，并避免了质量和多样性之间的权衡问题。在 ImageNet 256$^2$ 数据集上，$\\pi$-Flow 达到了 1-NFE FID 为 2.85，优于相同 DiT 架构下的 MeanFlow。在 FLUX.1-12B 和 Qwen-Image-20B 模型上，使用 4 个 NFE 时，$\\pi$-Flow 显著提升了多样性，同时保持了与教师模型相当的质量。",
        "translated_title": "pi-Flow: 基于策略的少步生成方法通过模仿蒸馏",
        "label": [],
        "label_reason": "论文主要关注生成模型的快速推理，不属于图像恢复或增强任务",
        "relevance_score": 2,
        "novelty_score": 8,
        "novelty_reason": "提出基于策略的生成模型新方法，改进了少步生成的质量与多样性平衡"
    },
    {
        "title": "RDD: Retrieval-Based Demonstration Decomposer for Planner Alignment in\n  Long-Horizon Tasks",
        "url": "http://arxiv.org/abs/2510.14968v1",
        "pub_date": "2025-10-16",
        "summary": "To tackle long-horizon tasks, recent hierarchical vision-language-action (VLAs) frameworks employ vision-language model (VLM)-based planners to decompose complex manipulation tasks into simpler sub-tasks that low-level visuomotor policies can easily handle. Typically, the VLM planner is finetuned to learn to decompose a target task. This finetuning requires target task demonstrations segmented into sub-tasks by either human annotation or heuristic rules. However, the heuristic subtasks can deviate significantly from the training data of the visuomotor policy, which degrades task performance. To address these issues, we propose a Retrieval-based Demonstration Decomposer (RDD) that automatically decomposes demonstrations into sub-tasks by aligning the visual features of the decomposed sub-task intervals with those from the training data of the low-level visuomotor policies. Our method outperforms the state-of-the-art sub-task decomposer on both simulation and real-world tasks, demonstrating robustness across diverse settings. Code and more results are available at rdd-neurips.github.io.",
        "translated": "为应对长时域任务，最近的分层视觉-语言-动作（VLA）框架采用基于视觉-语言模型（VLM）的规划器，将复杂的操作任务分解为低级视觉运动策略可轻松处理的简单子任务。通常，VLM 规划器会被微调以学习如何分解目标任务。这种微调需要目标任务的演示数据被分割为子任务，分割方式依赖于人工标注或启发式规则。然而，这些启发式子任务与低级视觉运动策略的训练数据可能存在较大偏差，从而影响任务性能。为解决这些问题，我们提出了一种基于检索的演示分解器（RDD），通过将分解后的子任务区间中的视觉特征与低级视觉运动策略训练数据中的特征对齐，实现演示数据的自动子任务分解。我们的方法在仿真和真实世界任务中均优于最先进的子任务分解器，展现出在不同设置下的鲁棒性。代码和更多结果可在 rdd-neurips.github.io 获取。",
        "translated_title": "RDD：基于检索的演示分解器用于长时域任务中的规划器对齐",
        "label": [],
        "label_reason": "论文聚焦于高层任务规划，与图像像素级恢复无关。",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出基于检索的子任务分解方法，有一定改进但属于常规优化。"
    },
    {
        "title": "ChangingGrounding: 3D Visual Grounding in Changing Scenes",
        "url": "http://arxiv.org/abs/2510.14965v1",
        "pub_date": "2025-10-16",
        "summary": "Real-world robots localize objects from natural-language instructions while scenes around them keep changing. Yet most of the existing 3D visual grounding (3DVG) method still assumes a reconstructed and up-to-date point cloud, an assumption that forces costly re-scans and hinders deployment. We argue that 3DVG should be formulated as an active, memory-driven problem, and we introduce ChangingGrounding, the first benchmark that explicitly measures how well an agent can exploit past observations, explore only where needed, and still deliver precise 3D boxes in changing scenes. To set a strong reference point, we also propose Mem-ChangingGrounder, a zero-shot method for this task that marries cross-modal retrieval with lightweight multi-view fusion: it identifies the object type implied by the query, retrieves relevant memories to guide actions, then explores the target efficiently in the scene, falls back when previous operations are invalid, performs multi-view scanning of the target, and projects the fused evidence from multi-view scans to get accurate object bounding boxes. We evaluate different baselines on ChangingGrounding, and our Mem-ChangingGrounder achieves the highest localization accuracy while greatly reducing exploration cost. We hope this benchmark and method catalyze a shift toward practical, memory-centric 3DVG research for real-world applications. Project page: https://hm123450.github.io/CGB/ .",
        "translated": "现实世界中的机器人需要在周围场景不断变化的情况下，根据自然语言指令定位物体。然而，大多数现有的 3D 视觉定位（3DVG）方法仍然假设存在重建且最新的点云，这一假设迫使进行成本高昂的重新扫描，并阻碍实际部署。我们认为，3DVG 应该被表述为一个主动的、由记忆驱动的问题，因此我们引入了 ChangingGrounding，这是首个明确衡量智能体在变化场景中如何有效利用过往观测结果、仅在必要区域进行探索，并仍能提供精确 3D 边界框的基准。为了提供一个强有力的参考点，我们还提出了 Mem-ChangingGrounder，一种针对该任务的零样本方法，它将跨模态检索与轻量多视角融合相结合：该方法首先识别查询语句所暗示的物体类型，检索相关记忆以指导操作，然后在场景中高效地探索目标，当先前操作无效时进行回退，执行多视角扫描，最后将多视角扫描的融合证据进行投影，以获得精确的物体边界框。我们在 ChangingGrounding 上评估了不同的基线方法，我们的 Mem-ChangingGrounder 在显著降低探索成本的同时实现了最高的定位精度。我们希望这一基准和方法能够推动 3DVG 研究向实用、以记忆为中心的方向转变，以支持现实世界的应用。项目页面：https://hm123450.github.io/CGB/。",
        "translated_title": "ChangingGrounding: 变化场景中的3D视觉定位",
        "label": [],
        "label_reason": "论文聚焦3D视觉定位，属于high-level任务",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "提出新基准和零样本方法，但创新性有限"
    },
    {
        "title": "RainDiff: End-to-end Precipitation Nowcasting Via Token-wise Attention\n  Diffusion",
        "url": "http://arxiv.org/abs/2510.14962v1",
        "pub_date": "2025-10-16",
        "summary": "Precipitation nowcasting, predicting future radar echo sequences from current observations, is a critical yet challenging task due to the inherently chaotic and tightly coupled spatio-temporal dynamics of the atmosphere. While recent advances in diffusion-based models attempt to capture both large-scale motion and fine-grained stochastic variability, they often suffer from scalability issues: latent-space approaches require a separately trained autoencoder, adding complexity and limiting generalization, while pixel-space approaches are computationally intensive and often omit attention mechanisms, reducing their ability to model long-range spatio-temporal dependencies. To address these limitations, we propose a Token-wise Attention integrated into not only the U-Net diffusion model but also the spatio-temporal encoder that dynamically captures multi-scale spatial interactions and temporal evolution. Unlike prior approaches, our method natively integrates attention into the architecture without incurring the high resource cost typical of pixel-space diffusion, thereby eliminating the need for separate latent modules. Our extensive experiments and visual evaluations across diverse datasets demonstrate that the proposed method significantly outperforms state-of-the-art approaches, yielding superior local fidelity, generalization, and robustness in complex precipitation forecasting scenarios.",
        "translated": "降水临近预报，即根据当前观测预测未来的雷达回波序列，是一项关键但极具挑战性的任务，这是由于大气本身的混沌性以及紧密耦合的时空动态特性。尽管基于扩散模型的最新进展尝试捕捉大尺度运动和细粒度的随机变化，但它们通常存在可扩展性问题：隐空间方法需要单独训练的自编码器，增加了模型复杂性并限制了泛化能力，而像素空间方法计算成本高昂，且通常省略注意力机制，削弱了对长程时空依赖关系的建模能力。为了解决这些限制，我们提出了一种将 Token-wise 注意力机制集成到 U-Net 扩散模型以及动态捕捉多尺度空间交互和时间演化过程的时空编码器中的方法。与以往的方法不同，我们的方法在架构中原生地引入注意力机制，而无需承担像素空间扩散通常带来的高资源消耗，从而消除了对单独隐模块的依赖。我们在多个数据集上的广泛实验和可视化评估表明，所提出的方法显著优于最先进的方法，在复杂的降水预测场景中展现出更优越的局部真实性、泛化性和鲁棒性。",
        "translated_title": "RainDiff：通过逐标记注意力的扩散实现端到端降水临近预报",
        "label": [
            "图像去雨",
            "多帧/视频图像恢复"
        ],
        "label_reason": "论文聚焦于雷达降水序列预测，属于多帧图像恢复和去雨任务。",
        "relevance_score": 8,
        "novelty_score": 8,
        "novelty_reason": "提出 Token-wise Attention 结构，改进扩散模型的时空建模能力。"
    },
    {
        "title": "C4D: 4D Made from 3D through Dual Correspondences",
        "url": "http://arxiv.org/abs/2510.14960v1",
        "pub_date": "2025-10-16",
        "summary": "Recovering 4D from monocular video, which jointly estimates dynamic geometry and camera poses, is an inevitably challenging problem. While recent pointmap-based 3D reconstruction methods (e.g., DUSt3R) have made great progress in reconstructing static scenes, directly applying them to dynamic scenes leads to inaccurate results. This discrepancy arises because moving objects violate multi-view geometric constraints, disrupting the reconstruction. To address this, we introduce C4D, a framework that leverages temporal Correspondences to extend existing 3D reconstruction formulation to 4D. Specifically, apart from predicting pointmaps, C4D captures two types of correspondences: short-term optical flow and long-term point tracking. We train a dynamic-aware point tracker that provides additional mobility information, facilitating the estimation of motion masks to separate moving elements from the static background, thus offering more reliable guidance for dynamic scenes. Furthermore, we introduce a set of dynamic scene optimization objectives to recover per-frame 3D geometry and camera parameters. Simultaneously, the correspondences lift 2D trajectories into smooth 3D trajectories, enabling fully integrated 4D reconstruction. Experiments show that our framework achieves complete 4D recovery and demonstrates strong performance across multiple downstream tasks, including depth estimation, camera pose estimation, and point tracking. Project Page: https://littlepure2333.github.io/C4D",
        "translated": "从单目视频中恢复4D，即联合估计动态几何结构和相机姿态，是一个不可避免的具有挑战性的问题。尽管近年来基于点图的3D重建方法（例如DUSt3R）在静态场景重建方面取得了显著进展，但直接将其应用于动态场景会导致不准确的结果。这种差异的产生是由于运动物体违反了多视图几何约束，从而破坏了重建效果。为了解决这一问题，我们提出了C4D，一个利用时间对应关系将现有3D重建方法扩展到4D的框架。具体而言，除了预测点图外，C4D还捕捉两种类型的对应关系：短期光流和长期点跟踪。我们训练了一个动态感知的点跟踪器，提供额外的运动信息，有助于估计运动掩码以将运动元素与静态背景分离，从而为动态场景提供更可靠的引导。此外，我们引入了一组动态场景优化目标，以恢复每帧的3D几何结构和相机参数。同时，这些对应关系将2D轨迹提升为平滑的3D轨迹，实现了完整的4D重建。实验表明，我们的框架能够实现完整的4D恢复，并在多个下游任务中表现出强大的性能，包括深度估计、相机姿态估计和点跟踪。项目页面：https://littlepure2333.github.io/C4D",
        "translated_title": "C4D：通过双对应关系从3D生成4D",
        "label": [],
        "label_reason": "论文主要关注4D重建与动态场景分析，不属于像素级图像恢复或增强任务",
        "relevance_score": 3,
        "novelty_score": 7,
        "novelty_reason": "提出基于双对应关系的动态场景4D重建框架，方法设计新颖"
    },
    {
        "title": "MathCanvas: Intrinsic Visual Chain-of-Thought for Multimodal\n  Mathematical Reasoning",
        "url": "http://arxiv.org/abs/2510.14958v1",
        "pub_date": "2025-10-16",
        "summary": "While Large Language Models (LLMs) have excelled in textual reasoning, they struggle with mathematical domains like geometry that intrinsically rely on visual aids. Existing approaches to Visual Chain-of-Thought (VCoT) are often limited by rigid external tools or fail to generate the high-fidelity, strategically-timed diagrams necessary for complex problem-solving. To bridge this gap, we introduce MathCanvas, a comprehensive framework designed to endow unified Large Multimodal Models (LMMs) with intrinsic VCoT capabilities for mathematics. Our approach consists of two phases. First, a Visual Manipulation stage pre-trains the model on a novel 15.2M-pair corpus, comprising 10M caption-to-diagram pairs (MathCanvas-Imagen) and 5.2M step-by-step editing trajectories (MathCanvas-Edit), to master diagram generation and editing. Second, a Strategic Visual-Aided Reasoning stage fine-tunes the model on MathCanvas-Instruct, a new 219K-example dataset of interleaved visual-textual reasoning paths, teaching it when and how to leverage visual aids. To facilitate rigorous evaluation, we introduce MathCanvas-Bench, a challenging benchmark with 3K problems that require models to produce interleaved visual-textual solutions. Our model, BAGEL-Canvas, trained under this framework, achieves an 86% relative improvement over strong LMM baselines on MathCanvas-Bench, demonstrating excellent generalization to other public math benchmarks. Our work provides a complete toolkit-framework, datasets, and benchmark-to unlock complex, human-like visual-aided reasoning in LMMs. Project Page: https://mathcanvas.github.io/",
        "translated": "虽然大型语言模型（LLMs）在文本推理方面表现出色，但它们在数学领域（如几何学）中表现不佳，因为这些领域本质上依赖于视觉辅助手段。现有的视觉推理链（VCoT）方法通常受到外部工具刚性的限制，或者无法生成在复杂问题求解过程中所需高保真度、策略性时间点的图表。为了解决这一问题，我们提出 MathCanvas，一个全面的框架，旨在为统一的大型多模态模型（LMMs）赋予内在的 VCoT 能力以应对数学问题。我们的方法包括两个阶段。第一阶段是视觉操作（Visual Manipulation），在此阶段，模型在一个全新的 1520 万对语料库上进行预训练，该语料库包括 1000 万对描述到图表的配对（MathCanvas-Imagen）以及 520 万条逐步编辑轨迹（MathCanvas-Edit），从而掌握图表生成和编辑的能力。第二阶段是策略性的视觉辅助推理（Strategic Visual-Aided Reasoning），模型在 MathCanvas-Instruct 上进行微调，这是一个新的 21.9 万个示例的数据集，包含交错的图文推理路径，教会模型何时以及如何利用视觉辅助手段。为了便于严格评估，我们引入了 MathCanvas-Bench 这一具有挑战性的基准，包含 3000 个问题，要求模型生成交错的图文解决方案。在该框架下训练的模型 BAGEL-Canvas 在 MathCanvas-Bench 上相比强大的 LMM 基线模型取得了 86% 的相对提升，显示出在其他公共数学基准上出色的泛化能力。我们的工作提供了一套完整的工具包，包括框架、数据集和基准，以在 LMM 中实现复杂且类似人类的视觉辅助推理。项目页面：https://mathcanvas.github.io/",
        "translated_title": "MathCanvas: 用于多模态数学推理的内在视觉思维链",
        "label": [],
        "label_reason": "论文聚焦多模态数学推理，非图像像素级处理任务",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "提出了多阶段框架和新数据集，但方法基于现有大模型范式"
    },
    {
        "title": "RealDPO: Real or Not Real, that is the Preference",
        "url": "http://arxiv.org/abs/2510.14955v1",
        "pub_date": "2025-10-16",
        "summary": "Video generative models have recently achieved notable advancements in synthesis quality. However, generating complex motions remains a critical challenge, as existing models often struggle to produce natural, smooth, and contextually consistent movements. This gap between generated and real-world motions limits their practical applicability. To address this issue, we introduce RealDPO, a novel alignment paradigm that leverages real-world data as positive samples for preference learning, enabling more accurate motion synthesis. Unlike traditional supervised fine-tuning (SFT), which offers limited corrective feedback, RealDPO employs Direct Preference Optimization (DPO) with a tailored loss function to enhance motion realism. By contrasting real-world videos with erroneous model outputs, RealDPO enables iterative self-correction, progressively refining motion quality. To support post-training in complex motion synthesis, we propose RealAction-5K, a curated dataset of high-quality videos capturing human daily activities with rich and precise motion details. Extensive experiments demonstrate that RealDPO significantly improves video quality, text alignment, and motion realism compared to state-of-the-art models and existing preference optimization techniques.",
        "translated": "近期，视频生成模型在合成质量方面取得了显著进展。然而，生成复杂运动仍然是一个关键挑战，因为现有模型通常难以生成自然、流畅且上下文一致的运动。这种生成运动与真实世界运动之间的差距限制了它们的实际应用性。为了解决这一问题，我们提出 RealDPO，一种新颖的对齐范式，它利用真实世界数据作为偏好学习中的正样本，从而实现更精确的运动合成。与仅能提供有限纠正反馈的传统监督微调（SFT）方法不同，RealDPO 采用直接偏好优化（DPO）并配以定制的损失函数，以提升运动的真实感。通过将真实世界视频与模型错误的输出进行对比，RealDPO 可以实现迭代式的自我修正，逐步优化运动质量。为了支持复杂运动合成的后训练，我们提出了 RealAction-5K，一个经过精心挑选的高质量视频数据集，涵盖人类日常活动，包含丰富且精确的运动细节。大量实验表明，与最先进的模型以及现有的偏好优化技术相比，RealDPO 在视频质量、文本对齐以及运动真实感方面均有显著提升。",
        "translated_title": "RealDPO: 真实还是非真实，这是偏好",
        "label": [],
        "label_reason": "论文聚焦视频生成模型的运动合成，不属于图像恢复/增强任务。",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出新的对齐范式 RealDPO，改进了运动真实感建模方法。"
    },
    {
        "title": "OmniMotion: Multimodal Motion Generation with Continuous Masked\n  Autoregression",
        "url": "http://arxiv.org/abs/2510.14954v1",
        "pub_date": "2025-10-16",
        "summary": "Whole-body multi-modal human motion generation poses two primary challenges: creating an effective motion generation mechanism and integrating various modalities, such as text, speech, and music, into a cohesive framework. Unlike previous methods that usually employ discrete masked modeling or autoregressive modeling, we develop a continuous masked autoregressive motion transformer, where a causal attention is performed considering the sequential nature within the human motion. Within this transformer, we introduce a gated linear attention and an RMSNorm module, which drive the transformer to pay attention to the key actions and suppress the instability caused by either the abnormal movements or the heterogeneous distributions within multi-modalities. To further enhance both the motion generation and the multimodal generalization, we employ the DiT structure to diffuse the conditions from the transformer towards the targets. To fuse different modalities, AdaLN and cross-attention are leveraged to inject the text, speech, and music signals. Experimental results demonstrate that our framework outperforms previous methods across all modalities, including text-to-motion, speech-to-gesture, and music-to-dance. The code of our method will be made public.",
        "translated": "全身多模态人体动作生成面临两个主要挑战：建立一个有效的动作生成机制，以及将各种模态（如文本、语音和音乐）集成到一个统一的框架中。与以往通常采用离散掩码建模或自回归建模的方法不同，我们开发了一个连续掩码自回归运动变换器，在该变换器中，通过考虑人体动作的序列特性，引入了因果注意力机制。在该变换器中，我们引入了门控线性注意力和 RMSNorm 模块，使得模型能够关注关键动作，并抑制由异常动作或多模态中异构分布所引起的不稳定性。为了进一步增强动作生成和多模态泛化能力，我们采用 DiT 结构将条件从变换器扩散到目标。为了融合不同模态，我们利用 AdaLN 和交叉注意力机制将文本、语音和音乐信号注入模型。实验结果表明，我们的框架在所有模态上均优于先前的方法，包括文本到动作、语音到手势和音乐到舞蹈。我们的方法代码将会公开。",
        "translated_title": "OmniMotion: 基于连续掩码自回归的多模态运动生成",
        "label": [],
        "label_reason": "论文关注多模态运动生成，非图像像素级处理任务",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出连续掩码自回归运动生成框架，改进注意力机制和多模态融合方法"
    },
    {
        "title": "From Language to Locomotion: Retargeting-free Humanoid Control via\n  Motion Latent Guidance",
        "url": "http://arxiv.org/abs/2510.14952v1",
        "pub_date": "2025-10-16",
        "summary": "Natural language offers a natural interface for humanoid robots, but existing language-guided humanoid locomotion pipelines remain cumbersome and unreliable. They typically decode human motion, retarget it to robot morphology, and then track it with a physics-based controller. However, this multi-stage process is prone to cumulative errors, introduces high latency, and yields weak coupling between semantics and control. These limitations call for a more direct pathway from language to action, one that eliminates fragile intermediate stages. Therefore, we present RoboGhost, a retargeting-free framework that directly conditions humanoid policies on language-grounded motion latents. By bypassing explicit motion decoding and retargeting, RoboGhost enables a diffusion-based policy to denoise executable actions directly from noise, preserving semantic intent and supporting fast, reactive control. A hybrid causal transformer-diffusion motion generator further ensures long-horizon consistency while maintaining stability and diversity, yielding rich latent representations for precise humanoid behavior. Extensive experiments demonstrate that RoboGhost substantially reduces deployment latency, improves success rates and tracking accuracy, and produces smooth, semantically aligned locomotion on real humanoids. Beyond text, the framework naturally extends to other modalities such as images, audio, and music, providing a general foundation for vision-language-action humanoid systems.",
        "translated": "自然语言为人形机器人提供了自然的交互界面，但现有的语言引导人形机器人运动控制流程仍然复杂且不可靠。它们通常先解码人类运动，将其适配到机器人形态，然后通过基于物理的控制器进行跟踪。然而，这种多阶段流程容易累积误差，引入高延迟，并导致语义与控制之间的耦合较弱。这些限制促使我们寻求从语言到动作的更直接路径，从而消除脆弱的中间阶段。因此，我们提出 RoboGhost，一个无需适配的框架，它直接将人形机器人策略建立在语言引导的运动潜在表示之上。通过绕过显式运动解码和适配，RoboGhost 使基于扩散的策略能够直接从噪声中去噪出可执行动作，保留语义意图，并支持快速、反应式的控制。一个混合因果变压器-扩散运动生成器进一步确保了长期行为的一致性，同时保持稳定性和多样性，生成丰富的潜在表示以实现精确的人形机器人行为。大量实验表明，RoboGhost 显著降低了部署延迟，提高了成功率和跟踪精度，并在真实人形机器人上实现了平滑且语义一致的运动。除了文本之外，该框架自然地扩展到其他模态，如图像、音频和音乐，为人形机器人的视觉-语言-动作系统提供了通用基础。",
        "translated_title": "从语言到运动：基于运动隐空间引导的免重定向人形机器人控制",
        "label": [],
        "label_reason": "论文研究机器人运动控制，不属于图像处理任务",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "提出免重定向框架，但创新点在控制领域而非视觉"
    },
    {
        "title": "DialectGen: Benchmarking and Improving Dialect Robustness in Multimodal\n  Generation",
        "url": "http://arxiv.org/abs/2510.14949v1",
        "pub_date": "2025-10-16",
        "summary": "Contact languages like English exhibit rich regional variations in the form of dialects, which are often used by dialect speakers interacting with generative models. However, can multimodal generative models effectively produce content given dialectal textual input? In this work, we study this question by constructing a new large-scale benchmark spanning six common English dialects. We work with dialect speakers to collect and verify over 4200 unique prompts and evaluate on 17 image and video generative models. Our automatic and human evaluation results show that current state-of-the-art multimodal generative models exhibit 32.26% to 48.17% performance degradation when a single dialect word is used in the prompt. Common mitigation methods such as fine-tuning and prompt rewriting can only improve dialect performance by small margins (&lt; 7%), while potentially incurring significant performance degradation in Standard American English (SAE). To this end, we design a general encoder-based mitigation strategy for multimodal generative models. Our method teaches the model to recognize new dialect features while preserving SAE performance. Experiments on models such as Stable Diffusion 1.5 show that our method is able to simultaneously raise performance on five dialects to be on par with SAE (+34.4%), while incurring near zero cost to SAE performance.",
        "translated": "像英语这样的接触语言表现出丰富的地域变体，即方言，这些方言经常被用于与生成模型进行交互。然而，多模态生成模型是否能够有效处理方言文本输入并生成内容？在本工作中，我们通过构建一个新的大规模基准，覆盖六种常见的英语方言，来研究这一问题。我们与方言使用者合作，收集并验证了4200多个独特的提示词，并在17种图像和视频生成模型上进行了评估。我们的自动评估与人工评估结果表明，当前最先进的多模态生成模型在提示词中仅包含一个方言词汇的情况下，性能退化达到32.26%到48.17%。常见的缓解方法如微调和提示重写仅能带来小幅的性能提升（< 7%），同时可能在标准美式英语（SAE）上造成显著的性能下降。为了解决这一问题，我们设计了一种基于通用编码器的缓解策略，用于多模态生成模型。我们的方法在保留SAE性能的同时，使模型能够识别新的方言特征。在如Stable Diffusion 1.5等模型上的实验表明，我们的方法能够同时提升五种方言的性能，使其与SAE相当（+34.4%），而对SAE性能几乎没有影响。",
        "translated_title": "DialectGen：多模态生成中方言鲁棒性的基准测试与改进",
        "label": [],
        "label_reason": "论文聚焦于多模态生成模型对英语方言的鲁棒性，非图像像素级处理任务。",
        "relevance_score": 1,
        "novelty_score": 3,
        "novelty_reason": "提出针对方言输入的缓解策略，但方法基于通用编码器，创新性较弱。"
    },
    {
        "title": "3D Scene Prompting for Scene-Consistent Camera-Controllable Video\n  Generation",
        "url": "http://arxiv.org/abs/2510.14945v1",
        "pub_date": "2025-10-16",
        "summary": "We present 3DScenePrompt, a framework that generates the next video chunk from arbitrary-length input while enabling precise camera control and preserving scene consistency. Unlike methods conditioned on a single image or a short clip, we employ dual spatio-temporal conditioning that reformulates context-view referencing across the input video. Our approach conditions on both temporally adjacent frames for motion continuity and spatially adjacent content for scene consistency. However, when generating beyond temporal boundaries, directly using spatially adjacent frames would incorrectly preserve dynamic elements from the past. We address this by introducing a 3D scene memory that represents exclusively the static geometry extracted from the entire input video. To construct this memory, we leverage dynamic SLAM with our newly introduced dynamic masking strategy that explicitly separates static scene geometry from moving elements. The static scene representation can then be projected to any target viewpoint, providing geometrically consistent warped views that serve as strong 3D spatial prompts while allowing dynamic regions to evolve naturally from temporal context. This enables our model to maintain long-range spatial coherence and precise camera control without sacrificing computational efficiency or motion realism. Extensive experiments demonstrate that our framework significantly outperforms existing methods in scene consistency, camera controllability, and generation quality. Project page : https://cvlab-kaist.github.io/3DScenePrompt/",
        "translated": "我们提出 3DScenePrompt，这是一种在任意长度的输入基础上生成下一视频片段的框架，同时实现精确的相机控制并保持场景一致性。与依赖于单张图像或短片段的方法不同，我们采用双时空条件化策略，对输入视频中的上下文视图参考进行重新建模。我们的方法在时间相邻帧上进行条件化以保证运动连续性，在空间相邻内容上进行条件化以保持场景一致性。然而，在生成超出时间边界的内容时，直接使用空间相邻帧会导致过去动态元素被错误保留。我们通过引入一种3D场景记忆来解决这一问题，该记忆仅表示从整个输入视频中提取的静态几何结构。为构建这种记忆，我们结合动态SLAM与我们新提出的动态掩码策略，从而显式地将静态场景几何与动态元素分离。静态场景表示可以投影到任意目标视角，提供几何一致的视图，作为强有力的3D空间提示，同时允许动态区域从时间上下文中自然演变。这使得我们的模型在保持长期空间一致性和精确相机控制的同时，不会牺牲计算效率或运动的真实性。大量实验表明，我们的框架在场景一致性、相机可控性和生成质量方面显著优于现有方法。项目页面：https://cvlab-kaist.github.io/3DScenePrompt/",
        "translated_title": "3D场景提示用于场景一致的相机可控视频生成",
        "label": [],
        "label_reason": "论文聚焦于视频生成与相机控制，不属于低级图像处理任务。",
        "relevance_score": 2,
        "novelty_score": 8,
        "novelty_reason": "提出3D场景记忆和动态掩码策略，为视频生成提供新方法。"
    },
    {
        "title": "MaskCaptioner : Learning to Jointly Segment and Caption Object\n  Trajectories in Videos",
        "url": "http://arxiv.org/abs/2510.14904v1",
        "pub_date": "2025-10-16",
        "summary": "Dense Video Object Captioning (DVOC) is the task of jointly detecting, tracking, and captioning object trajectories in a video, requiring the ability to understand spatio-temporal details and describe them in natural language. Due to the complexity of the task and the high cost associated with manual annotation, previous approaches resort to disjoint training strategies, potentially leading to suboptimal performance. To circumvent this issue, we propose to generate captions about spatio-temporally localized entities leveraging a state-of-the-art VLM. By extending the LVIS and LV-VIS datasets with our synthetic captions (LVISCap and LV-VISCap), we train MaskCaptioner, an end-to-end model capable of jointly detecting, segmenting, tracking and captioning object trajectories. Moreover, with pretraining on LVISCap and LV-VISCap, MaskCaptioner achieves state-of-the-art DVOC results on three existing benchmarks, VidSTG, VLN and BenSMOT. The datasets and code are available at https://www.gabriel.fiastre.fr/maskcaptioner/.",
        "translated": "密集视频目标字幕生成（Dense Video Object Captioning，DVOC）是一项在视频中同时检测、跟踪并为目标轨迹生成字幕的任务，要求系统能够理解时空细节，并用自然语言加以描述。由于任务本身的复杂性和人工标注的高昂成本，以往的方法往往采用分阶段训练策略，可能导致性能次优。为了解决这一问题，我们提出利用最先进的视觉语言模型（VLM）生成关于时空定位实体的字幕。通过在LVIS和LV-VIS数据集上扩展我们合成的字幕（LVISCap和LV-VISCap），我们训练了一个端到端模型MaskCaptioner，能够同时完成目标的检测、分割、跟踪和字幕生成任务。此外，在对LVISCap和LV-VISCap进行预训练后，MaskCaptioner在三个现有的基准测试VidSTG、VLN和BenSMOT上取得了最先进的DVOC结果。数据集和代码可通过 https://www.gabriel.fiastre.fr/maskcaptioner/ 获得。",
        "translated_title": "MaskCaptioner：学习在视频中联合分割和描述物体轨迹",
        "label": [],
        "label_reason": "论文聚焦视频对象分割与描述，属于 high-level 任务",
        "relevance_score": 2,
        "novelty_score": 5,
        "novelty_reason": "方法结合 VLM 进行视频对象描述，有一定迁移创新"
    },
    {
        "title": "Leveraging Multimodal LLM Descriptions of Activity for Explainable\n  Semi-Supervised Video Anomaly Detection",
        "url": "http://arxiv.org/abs/2510.14896v1",
        "pub_date": "2025-10-16",
        "summary": "Existing semi-supervised video anomaly detection (VAD) methods often struggle with detecting complex anomalies involving object interactions and generally lack explainability. To overcome these limitations, we propose a novel VAD framework leveraging Multimodal Large Language Models (MLLMs). Unlike previous MLLM-based approaches that make direct anomaly judgments at the frame level, our method focuses on extracting and interpreting object activity and interactions over time. By querying an MLLM with visual inputs of object pairs at different moments, we generate textual descriptions of the activity and interactions from nominal videos. These textual descriptions serve as a high-level representation of the activity and interactions of objects in a video. They are used to detect anomalies during test time by comparing them to textual descriptions found in nominal training videos. Our approach inherently provides explainability and can be combined with many traditional VAD methods to further enhance their interpretability. Extensive experiments on benchmark datasets demonstrate that our method not only detects complex interaction-based anomalies effectively but also achieves state-of-the-art performance on datasets without interaction anomalies.",
        "translated": "现有的半监督视频异常检测（VAD）方法在检测涉及物体交互的复杂异常时通常表现不佳，且普遍缺乏可解释性。为克服这些限制，我们提出了一种新颖的 VAD 框架，该框架利用多模态大语言模型（MLLMs）。与之前基于 MLLM 的方法在帧级别直接进行异常判断不同，我们的方法聚焦于提取和解释视频中物体活动及其随时间变化的交互。通过在不同时间点对物体对的视觉输入查询 MLLM，我们生成了来自正常视频中活动和交互的文本描述。这些文本描述作为视频中物体活动和交互的高层表示，在测试阶段通过将其与正常训练视频中的文本描述进行比较，用于检测异常。我们的方法本质上提供了可解释性，并可与许多传统 VAD 方法结合，以进一步提升其可解释性。在基准数据集上的大量实验表明，我们的方法不仅能够有效检测基于交互的复杂异常，而且在无交互异常的数据集上也达到了最先进的性能。",
        "translated_title": "利用多模态大语言模型对活动的描述实现可解释的半监督视频异常检测",
        "label": [],
        "label_reason": "论文聚焦视频异常检测，属于 high-level 任务，不涉及像素级图像质量改善。",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出基于 MLLM 的新框架，提升 VAD 的可解释性，具备一定创新性。"
    },
    {
        "title": "OmniVinci: Enhancing Architecture and Data for Omni-Modal Understanding\n  LLM",
        "url": "http://arxiv.org/abs/2510.15870v1",
        "pub_date": "2025-10-17",
        "summary": "Advancing machine intelligence requires developing the ability to perceive across multiple modalities, much as humans sense the world. We introduce OmniVinci, an initiative to build a strong, open-source, omni-modal LLM. We carefully study the design choices across model architecture and data curation. For model architecture, we present three key innovations: (i) OmniAlignNet for strengthening alignment between vision and audio embeddings in a shared omni-modal latent space; (ii) Temporal Embedding Grouping for capturing relative temporal alignment between vision and audio signals; and (iii) Constrained Rotary Time Embedding for encoding absolute temporal information in omni-modal embeddings. We introduce a curation and synthesis pipeline that generates 24M single-modal and omni-modal conversations. We find that modalities reinforce one another in both perception and reasoning. Our model, OmniVinci, outperforms Qwen2.5-Omni with +19.05 on DailyOmni (cross-modal understanding), +1.7 on MMAR (audio), and +3.9 on Video-MME (vision), while using just 0.2T training tokens - a 6 times reduction compared to Qwen2.5-Omni's 1.2T. We finally demonstrate omni-modal advantages in downstream applications spanning robotics, medical AI, and smart factory.",
        "translated": "提升机器智能需要发展跨多种模态的感知能力，正如人类感知世界一样。我们提出了OmniVinci，一个旨在构建强大、开源、全模态大语言模型的项目。我们对模型架构和数据收集方面进行了细致的研究。在模型架构方面，我们提出了三项关键创新：(i) OmniAlignNet，用于在共享的全模态潜在空间中加强视觉和音频嵌入之间的对齐；(ii) Temporal Embedding Grouping，用于捕捉视觉和音频信号之间的相对时序对齐；(iii) Constrained Rotary Time Embedding，用于在全模态嵌入中编码绝对时序信息。我们引入了一个数据收集与合成流水线，生成了2400万条单模态和全模态的对话。我们发现，不同模态在感知和推理方面能够相互增强。我们的模型OmniVinci在使用仅0.2T训练数据量的情况下，在DailyOmni（跨模态理解）上优于Qwen2.5-Omni 19.05分，在MMAR（音频）上高1.7分，在Video-MME（视觉）上高3.9分，这比Qwen2.5-Omni所使用的1.2T训练数据量减少了6倍。最后，我们在下游应用中展示了全模态的优势，涵盖了机器人、医疗AI和智能工厂等多个领域。",
        "translated_title": "OmniVinci：增强架构与数据以实现全模态理解",
        "label": [],
        "label_reason": "论文聚焦多模态LLM，未涉及像素级图像处理任务",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出了多模态对齐和时间嵌入的改进方法"
    },
    {
        "title": "Skyfall-GS: Synthesizing Immersive 3D Urban Scenes from Satellite\n  Imagery",
        "url": "http://arxiv.org/abs/2510.15869v1",
        "pub_date": "2025-10-17",
        "summary": "Synthesizing large-scale, explorable, and geometrically accurate 3D urban scenes is a challenging yet valuable task in providing immersive and embodied applications. The challenges lie in the lack of large-scale and high-quality real-world 3D scans for training generalizable generative models. In this paper, we take an alternative route to create large-scale 3D scenes by synergizing the readily available satellite imagery that supplies realistic coarse geometry and the open-domain diffusion model for creating high-quality close-up appearances. We propose \\textbf{Skyfall-GS}, the first city-block scale 3D scene creation framework without costly 3D annotations, also featuring real-time, immersive 3D exploration. We tailor a curriculum-driven iterative refinement strategy to progressively enhance geometric completeness and photorealistic textures. Extensive experiments demonstrate that Skyfall-GS provides improved cross-view consistent geometry and more realistic textures compared to state-of-the-art approaches. Project page: https://skyfall-gs.jayinnn.dev/",
        "translated": "合成大规模、可探索且几何准确的3D城市场景是一项具有挑战性但又极具价值的任务，可为沉浸式和具身化应用提供支持。该任务的主要挑战在于缺乏用于训练通用生成模型的大规模高质量真实3D扫描数据。在本文中，我们采用了一种替代方法，通过结合现成的卫星图像（提供逼真的粗略几何结构）与开放域扩散模型（用于生成高质量的近距离外观），来创建大规模3D场景。我们提出了 \\textbf{Skyfall-GS}，这是首个无需高昂成本的3D标注即可实现城市街区尺度3D场景生成的框架，同时具备实时、沉浸式的3D探索功能。我们定制了一种基于课程学习的迭代优化策略，以逐步提升几何完整性与照片级真实纹理。大量实验表明，Skyfall-GS 相比于最先进的方法，在多视角几何一致性与纹理逼真度方面均有明显提升。项目主页：https://skyfall-gs.jayinnn.dev/",
        "translated_title": "Skyfall-GS：从卫星图像合成沉浸式三维城市场景",
        "label": [],
        "label_reason": "论文聚焦3D场景生成，非图像像素级恢复或增强任务",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出基于卫星图像和扩散模型的3D场景生成框架，有一定技术迁移性"
    },
    {
        "title": "LightsOut: Diffusion-based Outpainting for Enhanced Lens Flare Removal",
        "url": "http://arxiv.org/abs/2510.15868v1",
        "pub_date": "2025-10-17",
        "summary": "Lens flare significantly degrades image quality, impacting critical computer vision tasks like object detection and autonomous driving. Recent Single Image Flare Removal (SIFR) methods perform poorly when off-frame light sources are incomplete or absent. We propose LightsOut, a diffusion-based outpainting framework tailored to enhance SIFR by reconstructing off-frame light sources. Our method leverages a multitask regression module and LoRA fine-tuned diffusion model to ensure realistic and physically consistent outpainting results. Comprehensive experiments demonstrate LightsOut consistently boosts the performance of existing SIFR methods across challenging scenarios without additional retraining, serving as a universally applicable plug-and-play preprocessing solution. Project page: https://ray-1026.github.io/lightsout/",
        "translated": "镜头眩光会显著降低图像质量，影响物体检测和自动驾驶等关键的计算机视觉任务。近期的单图像眩光去除（Single Image Flare Removal, SIFR）方法在处理帧外光源不完整或缺失时效果较差。我们提出了 LightsOut，一种基于扩散模型的图像扩展框架，旨在通过重建帧外光源来增强 SIFR 的性能。我们的方法结合了一个多任务回归模块和经过 LoRA 微调的扩散模型，以确保生成结果在视觉真实性和物理一致性方面都具备高质量。大量实验表明，LightsOut 能够在各种具有挑战性的场景中，无需额外的再训练，显著提升现有 SIFR 方法的性能，成为一种通用的即插即用预处理解决方案。项目页面：https://ray-1026.github.io/lightsout/",
        "translated_title": "LightsOut：基于扩散的外推绘画用于增强的镜头眩光去除",
        "label": [
            "图像去反射",
            "图像恢复"
        ],
        "label_reason": "论文聚焦镜头光晕去除，涉及像素级图像恢复",
        "relevance_score": 8,
        "novelty_score": 7,
        "novelty_reason": "结合扩散模型与外延生成，提出新颖的光晕修复方案"
    },
    {
        "title": "BiomedXPro: Prompt Optimization for Explainable Diagnosis with\n  Biomedical Vision Language Models",
        "url": "http://arxiv.org/abs/2510.15866v1",
        "pub_date": "2025-10-17",
        "summary": "The clinical adoption of biomedical vision-language models is hindered by prompt optimization techniques that produce either uninterpretable latent vectors or single textual prompts. This lack of transparency and failure to capture the multi-faceted nature of clinical diagnosis, which relies on integrating diverse observations, limits their trustworthiness in high-stakes settings. To address this, we introduce BiomedXPro, an evolutionary framework that leverages a large language model as both a biomedical knowledge extractor and an adaptive optimizer to automatically generate a diverse ensemble of interpretable, natural-language prompt pairs for disease diagnosis. Experiments on multiple biomedical benchmarks show that BiomedXPro consistently outperforms state-of-the-art prompt-tuning methods, particularly in data-scarce few-shot settings. Furthermore, our analysis demonstrates a strong semantic alignment between the discovered prompts and statistically significant clinical features, grounding the model's performance in verifiable concepts. By producing a diverse ensemble of interpretable prompts, BiomedXPro provides a verifiable basis for model predictions, representing a critical step toward the development of more trustworthy and clinically-aligned AI systems.",
        "translated": "生物医学视觉-语言模型在临床中的应用受到提示优化技术的限制，这些技术生成的提示要么是不可解释的潜在向量，要么是单一的文本提示。这种缺乏透明性以及未能捕捉临床诊断的多方面特性——而临床诊断依赖于整合多种观察结果——限制了其在高风险场景中的可信度。为了解决这一问题，我们引入了 BiomedXPro，这是一种进化框架，利用大型语言模型作为生物医学知识提取器和自适应优化器，能够自动生成多样化的、可解释的自然语言提示对，用于疾病诊断。在多个生物医学基准数据集上的实验表明，BiomedXPro 在性能上始终优于最先进的提示调优方法，尤其是在数据稀缺的小样本设置中。此外，我们的分析表明，所发现的提示与统计上显著的临床特征之间存在强烈的语义一致性，从而将模型的性能建立在可验证的概念基础上。通过生成多样化且可解释的提示集合，BiomedXPro 为模型预测提供了可验证的依据，代表着开发更加可信且与临床对齐的 AI 系统的重要一步。",
        "translated_title": "BiomedXPro：用于可解释诊断的生物医学视觉语言模型提示优化",
        "label": [],
        "label_reason": "论文聚焦于生物医学视觉语言模型的提示优化，属于高阶诊断任务。",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "结合大语言模型进行提示优化，在提示工程上有一定创新。"
    },
    {
        "title": "BLIP3o-NEXT: Next Frontier of Native Image Generation",
        "url": "http://arxiv.org/abs/2510.15857v1",
        "pub_date": "2025-10-17",
        "summary": "We present BLIP3o-NEXT, a fully open-source foundation model in the BLIP3 series that advances the next frontier of native image generation. BLIP3o-NEXT unifies text-to-image generation and image editing within a single architecture, demonstrating strong image generation and image editing capabilities. In developing the state-of-the-art native image generation model, we identify four key insights: (1) Most architectural choices yield comparable performance; an architecture can be deemed effective provided it scales efficiently and supports fast inference; (2) The successful application of reinforcement learning can further push the frontier of native image generation; (3) Image editing still remains a challenging task, yet instruction following and the consistency between generated and reference images can be significantly enhanced through post-training and data engine; (4) Data quality and scale continue to be decisive factors that determine the upper bound of model performance. Building upon these insights, BLIP3o-NEXT leverages an Autoregressive + Diffusion architecture in which an autoregressive model first generates discrete image tokens conditioned on multimodal inputs, whose hidden states are then used as conditioning signals for a diffusion model to generate high-fidelity images. This architecture integrates the reasoning strength and instruction following of autoregressive models with the fine-detail rendering ability of diffusion models, achieving a new level of coherence and realism. Extensive evaluations of various text-to-image and image-editing benchmarks show that BLIP3o-NEXT achieves superior performance over existing models.",
        "translated": "我们提出了 BLIP3o-NEXT，这是 BLIP3 系列中一个完全开源的基础模型，推动了原生图像生成的下一个前沿方向。BLIP3o-NEXT 在单一架构中统一了文本到图像生成和图像编辑任务，展示了强大的图像生成和编辑能力。在开发先进的原生图像生成模型过程中，我们总结出四个关键见解：(1) 大多数架构选择在性能上是相近的；只要架构能够高效扩展并支持快速推理，就可以认为其是有效的；(2) 强化学习的成功应用可以进一步推动原生图像生成的前沿；(3) 图像编辑仍然是一个具有挑战性的任务，但通过后训练和数据引擎，指令跟随能力以及生成图像与参考图像之间的一致性可以显著提升；(4) 数据质量和数据规模仍然是决定模型性能上限的关键因素。基于这些见解，BLIP3o-NEXT 采用了一个 Autoregressive + Diffusion 架构：首先，一个自回归模型根据多模态输入生成离散的图像 token，然后将这些 token 的隐藏状态作为扩散模型的条件信号，以生成高质量图像。该架构结合了自回归模型的推理能力和指令跟随能力，以及扩散模型在细节渲染方面的优势，实现了生成图像在一致性和真实感方面的新水平。在多个文本到图像和图像编辑基准上的广泛评估表明，BLIP3o-NEXT 在现有模型中表现出优越的性能。",
        "translated_title": "BLIP3o-NEXT：原生图像生成的下一个前沿",
        "label": [],
        "label_reason": "论文关注图像生成而非像素级质量修复，不属于low-level图像处理",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出新的生成架构，结合自回归与扩散模型，有一定创新性"
    },
    {
        "title": "Memory-SAM: Human-Prompt-Free Tongue Segmentation via\n  Retrieval-to-Prompt",
        "url": "http://arxiv.org/abs/2510.15849v1",
        "pub_date": "2025-10-17",
        "summary": "Accurate tongue segmentation is crucial for reliable TCM analysis. Supervised models require large annotated datasets, while SAM-family models remain prompt-driven. We present Memory-SAM, a training-free, human-prompt-free pipeline that automatically generates effective prompts from a small memory of prior cases via dense DINOv3 features and FAISS retrieval. Given a query image, mask-constrained correspondences to the retrieved exemplar are distilled into foreground/background point prompts that guide SAM2 without manual clicks or model fine-tuning. We evaluate on 600 expert-annotated images (300 controlled, 300 in-the-wild). On the mixed test split, Memory-SAM achieves mIoU 0.9863, surpassing FCN (0.8188) and a detector-to-box SAM baseline (0.1839). On controlled data, ceiling effects above 0.98 make small differences less meaningful given annotation variability, while our method shows clear gains under real-world conditions. Results indicate that retrieval-to-prompt enables data-efficient, robust segmentation of irregular boundaries in tongue imaging. The code is publicly available at https://github.com/jw-chae/memory-sam.",
        "translated": "精确的舌象分割对于可靠的中医分析至关重要。监督模型需要大规模的标注数据集，而 SAM 系列模型仍然依赖提示驱动。我们提出 Memory-SAM，这是一种无需训练、无需人工提示的流程，它通过密集的 DINOv3 特征和 FAISS 检索，从少量先验案例构成的记忆中自动生成有效的提示。给定一张查询图像，将检索到的样例中受掩码约束的对应关系提炼为前景/背景点提示，以引导 SAM2 进行分割，而无需人工点击或模型微调。我们在 600 张专家标注的图像上进行评估（300 张受控图像，300 张真实场景图像）。在混合测试集上，Memory-SAM 实现了 0.9863 的 mIoU，优于 FCN（0.8188）和基于检测框的 SAM 基线（0.1839）。在受控数据中，由于标注的可变性，高于 0.98 的天花板效应使得微小差异变得不显著，而我们的方法在真实场景条件下表现出明显的性能提升。结果表明，检索到提示的机制能够实现舌象图像中不规则边界的数据高效且鲁棒的分割。代码公开在 https://github.com/jw-chae/memory-sam。",
        "translated_title": "Memory-SAM：基于检索到提示的无需人工提示的舌部分割",
        "label": [
            "图像分割",
            "医学图像增强"
        ],
        "label_reason": "方法用于医学图像分割，但非直接像素级质量复原",
        "relevance_score": 5,
        "novelty_score": 8,
        "novelty_reason": "提出无需人工提示的自动提示生成新方法"
    },
    {
        "title": "3DPR: Single Image 3D Portrait Relight using Generative Priors",
        "url": "http://arxiv.org/abs/2510.15846v1",
        "pub_date": "2025-10-17",
        "summary": "Rendering novel, relit views of a human head, given a monocular portrait image as input, is an inherently underconstrained problem. The traditional graphics solution is to explicitly decompose the input image into geometry, material and lighting via differentiable rendering; but this is constrained by the multiple assumptions and approximations of the underlying models and parameterizations of these scene components. We propose 3DPR, an image-based relighting model that leverages generative priors learnt from multi-view One-Light-at-A-Time (OLAT) images captured in a light stage. We introduce a new diverse and large-scale multi-view 4K OLAT dataset of 139 subjects to learn a high-quality prior over the distribution of high-frequency face reflectance. We leverage the latent space of a pre-trained generative head model that provides a rich prior over face geometry learnt from in-the-wild image datasets. The input portrait is first embedded in the latent manifold of such a model through an encoder-based inversion process. Then a novel triplane-based reflectance network trained on our lightstage data is used to synthesize high-fidelity OLAT images to enable image-based relighting. Our reflectance network operates in the latent space of the generative head model, crucially enabling a relatively small number of lightstage images to train the reflectance model. Combining the generated OLATs according to a given HDRI environment maps yields physically accurate environmental relighting results. Through quantitative and qualitative evaluations, we demonstrate that 3DPR outperforms previous methods, particularly in preserving identity and in capturing lighting effects such as specularities, self-shadows, and subsurface scattering. Project Page: https://vcai.mpi-inf.mpg.de/projects/3dpr/",
        "translated": "基于单目人像图像渲染新颖、重新光照的人头视图本身是一个固有欠约束的问题。传统的图形学解决方案是通过可微分渲染显式地将输入图像分解为几何、材质和光照；但该方法受限于基础模型和这些场景组件参数化过程中的多个假设和近似。我们提出 3DPR，一种基于图像的重新光照模型，利用从光场中捕获的多视角 One-Light-at-A-Time (OLAT) 图像学习的生成先验。我们引入一个新的多样化且大规模的多视角 4K OLAT 数据集，包含 139 个主体，以学习高频人脸反射分布的高质量先验。我们利用一个预训练的生成人头模型的潜在空间，该模型提供了从真实图像数据集中学习的丰富人脸几何先验。首先通过基于编码器的逆过程将输入人像嵌入到该模型的潜在流形中。然后利用一种基于三平面的反射网络（在我们采集的光场数据上训练），合成高保真的 OLAT 图像，以实现基于图像的重新光照。我们的反射网络在生成人头模型的潜在空间中进行操作，关键在于能够利用相对较少的光场图像训练反射模型。根据给定的 HDRI 环境图结合生成的 OLAT 图像，可以实现物理准确的环境光照效果。通过定量和定性评估，我们证明了 3DPR 在保持身份信息和捕捉诸如高光、自阴影和次表面散射等光照效果方面优于先前的方法。项目页面：https://vcai.mpi-inf.mpg.de/projects/3dpr/",
        "translated_title": "3DPR：使用生成先验的单图像3D人像重光照",
        "label": [],
        "label_reason": "论文属于3D渲染和生成任务，非图像像素级质量恢复或增强。",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出基于生成先验的3D人脸重光照方法，但属于常规生成模型的应用扩展。"
    },
    {
        "title": "Paper2Web: Let's Make Your Paper Alive!",
        "url": "http://arxiv.org/abs/2510.15842v1",
        "pub_date": "2025-10-17",
        "summary": "Academic project websites can more effectively disseminate research when they clearly present core content and enable intuitive navigation and interaction. However, current approaches such as direct Large Language Model (LLM) generation, templates, or direct HTML conversion struggle to produce layout-aware, interactive sites, and a comprehensive evaluation suite for this task has been lacking. In this paper, we introduce Paper2Web, a benchmark dataset and multi-dimensional evaluation framework for assessing academic webpage generation. It incorporates rule-based metrics like Connectivity, Completeness and human-verified LLM-as-a-Judge (covering interactivity, aesthetics, and informativeness), and PaperQuiz, which measures paper-level knowledge retention. We further present PWAgent, an autonomous pipeline that converts scientific papers into interactive and multimedia-rich academic homepages. The agent iteratively refines both content and layout through MCP tools that enhance emphasis, balance, and presentation quality. Our experiments show that PWAgent consistently outperforms end-to-end baselines like template-based webpages and arXiv/alphaXiv versions by a large margin while maintaining low cost, achieving the Pareto-front in academic webpage generation.",
        "translated": "学术项目网站在清晰呈现核心内容并实现直观导航与交互时，能够更有效地传播研究成果。然而，当前的方法如直接使用大型语言模型（LLM）生成、模板化或直接HTML转换，难以生成具有布局感知能力且交互性强的网站，且在此任务上缺乏全面的评估体系。本文中，我们提出了Paper2Web，这是一个用于评估学术网页生成的基准数据集和多维度评估框架。该框架集成了基于规则的指标，如连通性（Connectivity）、完整性（Completeness），以及由人类验证的LLM作为评判者（LLM-as-a-Judge），涵盖交互性、美观性和信息性等方面，同时还引入了PaperQuiz，用于衡量论文级别的知识保留程度。我们进一步提出了PWAgent，一个自主的流水线工具，能够将科研论文转换为交互性强、多媒体丰富的学术主页。该智能体通过MCP工具迭代优化内容与布局，提升重点突出性、视觉平衡性和展示质量。实验结果表明，PWAgent在保持低成本的同时，显著优于基于模板的网页和arXiv/alphaXiv版本等端到端基线方法，在学术网页生成任务中达到了帕累托前沿。",
        "translated_title": "Paper2Web: 让你的论文动起来！",
        "label": [],
        "label_reason": "论文不属于low-level图像处理，主要关注网页生成。",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "提出新框架和工具，但创新点集中于网页交互设计而非图像处理。"
    },
    {
        "title": "Neuro-Symbolic Spatial Reasoning in Segmentation",
        "url": "http://arxiv.org/abs/2510.15841v1",
        "pub_date": "2025-10-17",
        "summary": "Open-Vocabulary Semantic Segmentation (OVSS) assigns pixel-level labels from an open set of categories, requiring generalization to unseen and unlabelled objects. Using vision-language models (VLMs) to correlate local image patches with potential unseen object categories suffers from a lack of understanding of spatial relations of objects in a scene. To solve this problem, we introduce neuro-symbolic (NeSy) spatial reasoning in OVSS. In contrast to contemporary VLM correlation-based approaches, we propose Relational Segmentor (RelateSeg) to impose explicit spatial relational constraints by first order logic (FOL) formulated in a neural network architecture. This is the first attempt to explore NeSy spatial reasoning in OVSS. Specifically, RelateSeg automatically extracts spatial relations, e.g., &lt;cat, to-right-of, person&gt;, and encodes them as first-order logic formulas using our proposed pseudo categories. Each pixel learns to predict both a semantic category (e.g., \"cat\") and a spatial pseudo category (e.g., \"right of person\") simultaneously, enforcing relational constraints (e.g., a \"cat\" pixel must lie to the right of a \"person\"). Finally, these logic constraints are formulated in a deep network architecture by fuzzy logic relaxation, enabling end-to-end learning of spatial-relationally consistent segmentation. RelateSeg achieves state-of-the-art performance in terms of average mIoU across four benchmark datasets and particularly shows clear advantages on images containing multiple categories, with the cost of only introducing a single auxiliary loss function and no additional parameters, validating the effectiveness of NeSy spatial reasoning in OVSS.",
        "translated": "开放词汇语义分割（OVSS）从开放的类别集中为每个像素分配标签，要求对未见过且未标记的对象具备泛化能力。使用视觉-语言模型（VLMs）将局部图像块与潜在的未知对象类别相关联，往往难以理解场景中对象的空间关系。为了解决这一问题，我们在OVSS中引入了神经符号（NeSy）空间推理。不同于当前基于VLM相关性的方法，我们提出Relational Segmentor（RelateSeg），通过在神经网络架构中以一阶逻辑（FOL）的形式施加显式空间关系约束来实现推理。这是首次尝试在OVSS中探索NeSy空间推理。具体而言，RelateSeg会自动提取空间关系，例如 $<cat, to-right-of, person>$，并使用我们提出的伪类别将其编码为一阶逻辑公式。每个像素同时学习预测一个语义类别（如\"cat\"）和一个空间伪类别（如\"right of person\"），从而强制实施关系约束（如\"cat\"像素必须位于\"person\"像素的右侧）。最终，这些逻辑约束通过模糊逻辑松弛技术整合到深度网络架构中，实现空间关系一致的端到端分割学习。RelateSeg在四个基准数据集上的平均mIoU指标上达到了最先进的性能，尤其在包含多个类别的图像上表现出明显优势，仅引入了一个辅助损失函数且无需额外参数，验证了NeSy空间推理在OVSS中的有效性。",
        "translated_title": "神经符号化空间推理在分割中的应用",
        "label": [],
        "label_reason": "论文聚焦语义分割，属于high-level任务",
        "relevance_score": 3,
        "novelty_score": 7,
        "novelty_reason": "首次将神经符号推理引入分割任务，具有新颖性"
    },
    {
        "title": "VISTA: A Test-Time Self-Improving Video Generation Agent",
        "url": "http://arxiv.org/abs/2510.15831v1",
        "pub_date": "2025-10-17",
        "summary": "Despite rapid advances in text-to-video synthesis, generated video quality remains critically dependent on precise user prompts. Existing test-time optimization methods, successful in other domains, struggle with the multi-faceted nature of video. In this work, we introduce VISTA (Video Iterative Self-improvemenT Agent), a novel multi-agent system that autonomously improves video generation through refining prompts in an iterative loop. VISTA first decomposes a user idea into a structured temporal plan. After generation, the best video is identified through a robust pairwise tournament. This winning video is then critiqued by a trio of specialized agents focusing on visual, audio, and contextual fidelity. Finally, a reasoning agent synthesizes this feedback to introspectively rewrite and enhance the prompt for the next generation cycle. Experiments on single- and multi-scene video generation scenarios show that while prior methods yield inconsistent gains, VISTA consistently improves video quality and alignment with user intent, achieving up to 60% pairwise win rate against state-of-the-art baselines. Human evaluators concur, preferring VISTA outputs in 66.4% of comparisons.",
        "translated": "尽管文本到视频合成领域取得了快速进展，但生成的视频质量在很大程度上仍依赖于用户提供的精确提示。现有在测试时进行优化的方法虽然在其他领域取得了成功，但在处理视频的多维度特性时却面临挑战。在本研究中，我们提出 VISTA（Video Iterative Self-improvemenT Agent），一种新颖的多智能体系统，通过迭代循环优化提示，自主提升视频生成质量。VISTA 首先将用户的创意想法分解为结构化的时序计划。在生成视频之后，通过一个鲁棒的两两比赛机制确定最佳视频。随后，一个由三个专业智能体组成的小组对该胜出视频进行评估，分别关注视觉、音频和上下文的保真度。最后，一个推理智能体综合这些反馈，自主地重写并增强下一轮生成所用的提示。在单场景和多场景视频生成任务中的实验表明，尽管现有方法的性能提升并不一致，VISTA 则始终能够提升视频质量和与用户意图的对齐程度，在与最先进基线方法的两两比较中，最高可达 60% 的胜率。人类评估者也一致认可 VISTA 的输出，在 66.4% 的比较中更倾向于选择其结果。",
        "translated_title": "VISTA：一种运行时自我改进的视频生成智能体",
        "label": [],
        "label_reason": "论文聚焦视频生成而非像素级图像恢复或增强",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出了多智能体系统进行迭代视频优化，具有一定创新性"
    },
    {
        "title": "ERNet: Efficient Non-Rigid Registration Network for Point Sequences",
        "url": "http://arxiv.org/abs/2510.15800v1",
        "pub_date": "2025-10-17",
        "summary": "Registering an object shape to a sequence of point clouds undergoing non-rigid deformation is a long-standing challenge. The key difficulties stem from two factors: (i) the presence of local minima due to the non-convexity of registration objectives, especially under noisy or partial inputs, which hinders accurate and robust deformation estimation, and (ii) error accumulation over long sequences, leading to tracking failures. To address these challenges, we introduce to adopt a scalable data-driven approach and propose ERNet, an efficient feed-forward model trained on large deformation datasets. It is designed to handle noisy and partial inputs while effectively leveraging temporal information for accurate and consistent sequential registration. The key to our design is predicting a sequence of deformation graphs through a two-stage pipeline, which first estimates frame-wise coarse graph nodes for robust initialization, before refining their trajectories over time in a sliding-window fashion. Extensive experiments show that our proposed approach (i) outperforms previous state-of-the-art on both the DeformingThings4D and D-FAUST datasets, and (ii) achieves more than 4x speedup compared to the previous best, offering significant efficiency improvement.",
        "translated": "将一个物体形状配准到经历非刚性变形的一系列点云中是一个长期存在的挑战。其关键难点来源于两个因素：(i) 由于配准目标函数的非凸性，特别是在噪声或部分输入的情况下，容易出现局部极小值，这阻碍了准确且鲁棒的变形估计；以及 (ii) 在长序列中误差不断累积，导致跟踪失败。为了解决这些挑战，我们提出采用一种可扩展的数据驱动方法，并设计了 ERNet，一个在大变形数据集上训练的高效前馈模型。该模型旨在处理噪声和部分输入的同时，有效利用时间信息，实现准确且一致的序列配准。我们设计的关键在于通过一个两阶段的流程预测一系列变形图：首先估计每一帧的粗略图节点以实现鲁棒的初始化，然后在滑动窗口的框架下逐时地优化这些节点的轨迹。大量实验表明，我们提出的方法 (i) 在 DeformingThings4D 和 D-FAUST 数据集上均优于之前的最先进方法，以及 (ii) 相比之前最佳方法的效率提升了超过 4 倍，具有显著的速度优势。",
        "translated_title": "ERNet：用于点序列的高效非刚性配准网络",
        "label": [],
        "label_reason": "论文研究点云配准，不属于图像处理任务。",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "提出两阶段变形图预测方法，但为常规技术组合。"
    },
    {
        "title": "ReCon: Region-Controllable Data Augmentation with Rectification and\n  Alignment for Object Detection",
        "url": "http://arxiv.org/abs/2510.15783v1",
        "pub_date": "2025-10-17",
        "summary": "The scale and quality of datasets are crucial for training robust perception models. However, obtaining large-scale annotated data is both costly and time-consuming. Generative models have emerged as a powerful tool for data augmentation by synthesizing samples that adhere to desired distributions. However, current generative approaches often rely on complex post-processing or extensive fine-tuning on massive datasets to achieve satisfactory results, and they remain prone to content-position mismatches and semantic leakage. To overcome these limitations, we introduce ReCon, a novel augmentation framework that enhances the capacity of structure-controllable generative models for object detection. ReCon integrates region-guided rectification into the diffusion sampling process, using feedback from a pre-trained perception model to rectify misgenerated regions within diffusion sampling process. We further propose region-aligned cross-attention to enforce spatial-semantic alignment between image regions and their textual cues, thereby improving both semantic consistency and overall image fidelity. Extensive experiments demonstrate that ReCon substantially improve the quality and trainability of generated data, achieving consistent performance gains across various datasets, backbone architectures, and data scales. Our code is available at https://github.com/haoweiz23/ReCon .",
        "translated": "数据集的规模和质量对于训练鲁棒的感知模型至关重要。然而，获取大规模标注数据既昂贵又耗时。生成模型通过合成符合目标分布的样本，已成为数据增强的强大工具。然而，当前的生成方法通常依赖于复杂的后处理或对大规模数据集进行大量微调才能获得令人满意的结果，并且仍然容易出现内容-位置不匹配和语义泄露的问题。为克服这些限制，我们引入了 ReCon，一种新颖的增强框架，旨在提升结构可控生成模型在目标检测中的能力。ReCon 将区域引导的修正机制整合到扩散采样过程中，利用预训练感知模型的反馈来修正扩散采样过程中的错误生成区域。我们进一步提出了区域对齐的交叉注意力机制，以强制图像区域与文本提示之间在空间语义上的一致性，从而提升语义一致性以及整体图像保真度。大量实验表明，ReCon 显著提高了生成数据的质量和可训练性，在不同数据集、主干网络结构和数据规模上均实现了性能的稳定提升。我们的代码可在 https://github.com/haoweiz23/ReCon 获取。",
        "translated_title": "ReCon：带校正与对齐的区域可控数据增强方法用于目标检测",
        "label": [],
        "label_reason": "论文聚焦于数据增强，用于目标检测，属于 high-level 任务。",
        "relevance_score": 3,
        "novelty_score": 7,
        "novelty_reason": "提出区域对齐交叉注意力和区域引导修正，有一定创新性。"
    },
    {
        "title": "Controlling the image generation process with parametric activation\n  functions",
        "url": "http://arxiv.org/abs/2510.15778v1",
        "pub_date": "2025-10-17",
        "summary": "As image generative models continue to increase not only in their fidelity but also in their ubiquity the development of tools that leverage direct interaction with their internal mechanisms in an interpretable way has received little attention In this work we introduce a system that allows users to develop a better understanding of the model through interaction and experimentation By giving users the ability to replace activation functions of a generative network with parametric ones and a way to set the parameters of these functions we introduce an alternative approach to control the networks output We demonstrate the use of our method on StyleGAN2 and BigGAN networks trained on FFHQ and ImageNet respectively.",
        "translated": "随着图像生成模型在真实感和普及性方面持续提升，能够以可解释的方式利用与模型内部机制直接交互的工具的发展却未受到足够关注。在本工作中，我们引入了一个系统，使用户通过交互和实验更好地理解模型。通过赋予用户将生成网络的激活函数替换为参数化激活函数的能力，并提供设置这些函数参数的方法，我们提出了一种控制网络输出的替代方案。我们分别在基于 FFHQ 和 ImageNet 训练的 StyleGAN2 与 BigGAN 网络上展示了该方法的应用。",
        "translated_title": "通过参数化激活函数控制图像生成过程",
        "label": [],
        "label_reason": "论文聚焦图像生成而非恢复/增强，不属于低级图像处理",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出可参数化激活函数交互方法，有一定技术迁移潜力"
    },
    {
        "title": "SANR: Scene-Aware Neural Representation for Light Field Image\n  Compression with Rate-Distortion Optimization",
        "url": "http://arxiv.org/abs/2510.15775v1",
        "pub_date": "2025-10-17",
        "summary": "Light field images capture multi-view scene information and play a crucial role in 3D scene reconstruction. However, their high-dimensional nature results in enormous data volumes, posing a significant challenge for efficient compression in practical storage and transmission scenarios. Although neural representation-based methods have shown promise in light field image compression, most approaches rely on direct coordinate-to-pixel mapping through implicit neural representation (INR), often neglecting the explicit modeling of scene structure. Moreover, they typically lack end-to-end rate-distortion optimization, limiting their compression efficiency. To address these limitations, we propose SANR, a Scene-Aware Neural Representation framework for light field image compression with end-to-end rate-distortion optimization. For scene awareness, SANR introduces a hierarchical scene modeling block that leverages multi-scale latent codes to capture intrinsic scene structures, thereby reducing the information gap between INR input coordinates and the target light field image. From a compression perspective, SANR is the first to incorporate entropy-constrained quantization-aware training (QAT) into neural representation-based light field image compression, enabling end-to-end rate-distortion optimization. Extensive experiment results demonstrate that SANR significantly outperforms state-of-the-art techniques regarding rate-distortion performance with a 65.62\\% BD-rate saving against HEVC.",
        "translated": "光场图像捕获多视角场景信息，在三维场景重建中起着至关重要的作用。然而，其高维特性导致数据量巨大，在实际存储和传输场景中对高效压缩提出了重大挑战。尽管基于神经表示的方法在光场图像压缩中展现出前景，但大多数方法依赖于通过隐式神经表示（INR）进行的直接坐标到像素的映射，常常忽略了对场景结构的显式建模。此外，它们通常缺乏端到端的率失真优化，限制了压缩效率。为了解决这些限制，我们提出SANR，一种用于光场图像压缩的场景感知神经表示框架，具备端到端的率失真优化。为了实现场景感知，SANR引入了一个层次化的场景建模模块，利用多尺度潜在码捕获场景的内在结构，从而减少INR输入坐标与目标光场图像之间的信息差距。从压缩的角度来看，SANR是首个将熵约束的量化感知训练（QAT）引入基于神经表示的光场图像压缩的方法，实现了端到端的率失真优化。大量实验结果表明，SANR在率失真性能方面显著优于最先进的技术，相比HEVC在BD-rate上节省了65.62\\%。",
        "translated_title": "SANR：基于率失真优化的场景感知神经表示用于光场图像压缩",
        "label": [
            "多帧/视频图像恢复"
        ],
        "label_reason": "论文涉及光场图像压缩，属于多帧图像处理，与low-level相关。",
        "relevance_score": 8,
        "novelty_score": 7,
        "novelty_reason": "提出结合熵约束量化感知训练的新方法，改进压缩效率。"
    },
    {
        "title": "Towards more holistic interpretability: A lightweight disentangled\n  Concept Bottleneck Model",
        "url": "http://arxiv.org/abs/2510.15770v1",
        "pub_date": "2025-10-17",
        "summary": "Concept Bottleneck Models (CBMs) enhance interpretability by predicting human-understandable concepts as intermediate representations. However, existing CBMs often suffer from input-to-concept mapping bias and limited controllability, which restricts their practical value, directly damage the responsibility of strategy from concept-based methods. We propose a lightweight Disentangled Concept Bottleneck Model (LDCBM) that automatically groups visual features into semantically meaningful components without region annotation. By introducing a filter grouping loss and joint concept supervision, our method improves the alignment between visual patterns and concepts, enabling more transparent and robust decision-making. Notably, Experiments on three diverse datasets demonstrate that LDCBM achieves higher concept and class accuracy, outperforming previous CBMs in both interpretability and classification performance. By grounding concepts in visual evidence, our method overcomes a fundamental limitation of prior models and enhances the reliability of interpretable AI.",
        "translated": "概念瓶颈模型（CBMs）通过预测人类可理解的概念作为中间表示来增强模型的可解释性。然而，现有的 CBMs 往往存在输入到概念映射的偏差以及可控性有限的问题，这限制了它们的实用价值，并直接损害了基于概念方法策略的责任性。我们提出了一种轻量级的解耦概念瓶颈模型（LDCBM），该模型无需区域标注即可自动将视觉特征分组为语义上有意义的组件。通过引入滤波器分组损失和联合概念监督，我们的方法提升了视觉模式与概念之间的对齐程度，从而实现更加透明和稳健的决策。值得注意的是，三个多样化数据集上的实验表明，LDCBM 在概念和类别准确率方面均高于现有方法，在可解释性与分类性能上均优于以往的 CBMs。通过将概念与视觉证据相联系，我们的方法克服了先前模型的基本局限，并提高了可解释人工智能的可靠性。",
        "translated_title": "迈向更全面的可解释性：一种轻量级的解耦概念瓶颈模型",
        "label": [],
        "label_reason": "论文聚焦模型可解释性，非像素级图像处理任务。",
        "relevance_score": 3,
        "novelty_score": 7,
        "novelty_reason": "提出轻量解耦概念瓶颈模型，改进了CBM的可控性和概念对齐。"
    },
    {
        "title": "QSilk: Micrograin Stabilization and Adaptive Quantile Clipping for\n  Detail-Friendly Latent Diffusion",
        "url": "http://arxiv.org/abs/2510.15761v1",
        "pub_date": "2025-10-17",
        "summary": "We present QSilk, a lightweight, always-on stabilization layer for latent diffusion that improves high-frequency fidelity while suppressing rare activation spikes. QSilk combines (i) a per-sample micro clamp that gently limits extreme values without washing out texture, and (ii) Adaptive Quantile Clip (AQClip), which adapts the allowed value corridor per region. AQClip can operate in a proxy mode using local structure statistics or in an attention entropy guided mode (model confidence). Integrated into the CADE 2.5 rendering pipeline, QSilk yields cleaner, sharper results at low step counts and ultra-high resolutions with negligible overhead. It requires no training or fine-tuning and exposes minimal user controls. We report consistent qualitative improvements across SD/SDXL backbones and show synergy with CFG/Rescale, enabling slightly higher guidance without artifacts.",
        "translated": "我们提出了 QSilk，一种用于潜在扩散的轻量级、始终启用的稳定层，它在提高高频保真度的同时抑制罕见的激活尖峰。QSilk 结合了以下两个部分：(i) 每个样本的微小钳制机制，该机制温和地限制极端值而不模糊纹理；(ii) 自适应分位数裁剪（AQClip），它根据每个区域调整允许的数值通道。AQClip 可以通过局部结构统计信息运行在代理模式下，也可以通过注意力熵引导模式（模型置信度）运行。将 QSilk 集成到 CADE 2.5 渲染流程中后，即使在较低的采样步数和超高分辨率下，也能获得更干净、更清晰的结果，且几乎没有额外开销。QSilk 不需要训练或微调，并且用户可调节的参数极少。我们在 SD/SDXL 的主干模型上均观察到一致的定性提升，并展示了其与 CFG/Rescale 的协同效应，使得在不产生伪影的情况下可略微提高引导强度。",
        "translated_title": "QSilk：面向细节的潜在扩散的微晶粒稳定与自适应分位数裁剪",
        "label": [],
        "label_reason": "不直接处理像素级图像质量，属于生成模型的优化",
        "relevance_score": 3,
        "novelty_score": 6,
        "novelty_reason": "提出轻量稳定层和自适应裁剪方法，有一定创新"
    },
    {
        "title": "Poultry Farm Intelligence: An Integrated Multi-Sensor AI Platform for\n  Enhanced Welfare and Productivity",
        "url": "http://arxiv.org/abs/2510.15757v1",
        "pub_date": "2025-10-17",
        "summary": "Poultry farming faces increasing pressure to meet productivity targets while ensuring animal welfare and environmental compliance. Yet many small and medium-sized farms lack affordable, integrated tools for continuous monitoring and decision-making, relying instead on manual, reactive inspections. This paper presents Poultry Farm Intelligence (PoultryFI) - a modular, cost-effective platform that integrates six AI-powered modules: Camera Placement Optimizer, Audio-Visual Monitoring, Analytics &amp; Alerting, Real-Time Egg Counting, Production &amp; Profitability Forecasting, and a Recommendation Module.   Camera layouts are first optimized offline using evolutionary algorithms for full poultry house coverage with minimal hardware. The Audio-Visual Monitoring module extracts welfare indicators from synchronized video, audio, and feeding data. Analytics &amp; Alerting produces daily summaries and real-time notifications, while Real-Time Egg Counting uses an edge vision model to automate production tracking. Forecasting models predict egg yield and feed consumption up to 10 days in advance, and the Recommendation Module integrates forecasts with weather data to guide environmental and operational adjustments.   This is among the first systems to combine low-cost sensing, edge analytics, and prescriptive AI to continuously monitor flocks, predict production, and optimize performance. Field trials demonstrate 100% egg-count accuracy on Raspberry Pi 5, robust anomaly detection, and reliable short-term forecasting. PoultryFI bridges the gap between isolated pilot tools and scalable, farm-wide intelligence, empowering producers to proactively safeguard welfare and profitability.",
        "translated": "禽类养殖面临日益增长的压力，需要在提高生产效率的同时保障动物福利并满足环境合规要求。然而，许多中小型农场缺乏经济、集成化的工具来进行连续监控和决策，通常依赖人工的、被动的检查方式。本文提出了 Poultry Farm Intelligence（PoultryFI）——一个模块化、低成本的平台，集成了六个基于人工智能的模块：摄像头布局优化器、音视频监控、分析与预警、实时蛋数统计、生产与盈利能力预测以及推荐模块。\n\n首先使用进化算法在离线环境下对摄像头布局进行优化，以在使用最少硬件设备的情况下实现禽舍的全面覆盖。音视频监控模块从同步的视频、音频和喂食数据中提取福利指标。分析与预警模块生成每日摘要并发出实时通知，而实时蛋数统计模块则采用边缘视觉模型实现生产情况的自动化跟踪。预测模型可提前10天预测产蛋量和饲料消耗，推荐模块则将预测结果与天气数据整合，用于指导环境和操作的调整。\n\n该系统是首批结合低成本传感、边缘分析和处方式人工智能的系统之一，能够持续监控禽群、预测生产情况并优化整体性能。实地试验表明，在 Raspberry Pi 5 上实现蛋数统计的准确率达到100%，异常检测稳健，短期预测可靠。PoultryFI 搭起了孤立试点工具与可扩展、全场智能化之间的桥梁，使养殖者能够主动保障动物福利和盈利能力。",
        "translated_title": "禽类养殖智能化：一种集成多传感器的AI平台，提升福利与生产效率",
        "label": [],
        "label_reason": "论文主要关注多传感器AI平台在养鸡场的应用，不属于图像恢复或增强任务。",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出了模块化低成本AI平台，但属于常规系统集成创新。"
    },
    {
        "title": "Semantic segmentation with coarse annotations",
        "url": "http://arxiv.org/abs/2510.15756v1",
        "pub_date": "2025-10-17",
        "summary": "Semantic segmentation is the task of classifying each pixel in an image. Training a segmentation model achieves best results using annotated images, where each pixel is annotated with the corresponding class. When obtaining fine annotations is difficult or expensive, it may be possible to acquire coarse annotations, e.g. by roughly annotating pixels in an images leaving some pixels around the boundaries between classes unlabeled. Segmentation with coarse annotations is difficult, in particular when the objective is to optimize the alignment of boundaries between classes. This paper proposes a regularization method for models with an encoder-decoder architecture with superpixel based upsampling. It encourages the segmented pixels in the decoded image to be SLIC-superpixels, which are based on pixel color and position, independent of the segmentation annotation. The method is applied to FCN-16 fully convolutional network architecture and evaluated on the SUIM, Cityscapes, and PanNuke data sets. It is shown that the boundary recall improves significantly compared to state-of-the-art models when trained on coarse annotations.",
        "translated": "语义分割的任务是对图像中的每个像素进行分类。使用带有标注的图像训练分割模型可以取得最佳效果，其中每个像素都标注了对应的类别。当获取精细标注困难或成本较高时，可以尝试获取粗略标注，例如通过大致标注图像中的像素，而将类别边界附近的一些像素留作未标注。使用粗略标注进行分割是具有挑战性的，特别是当目标是优化类别之间边界的对齐时。本文提出了一种针对具有编码器-解码器架构、并采用超像素上采样的模型的正则化方法。该方法促使解码图像中的分割像素属于基于像素颜色和位置的SLIC超像素，且不依赖于分割标注。该方法被应用于FCN-16全卷积网络架构，并在SUIM、Cityscapes和PanNuke数据集上进行了评估。结果表明，与最先进的模型相比，在使用粗标注训练时，该方法能够显著提高边界召回率。",
        "translated_title": "带有粗略标注的语义分割",
        "label": [],
        "label_reason": "论文属于语义分割，目标是场景理解，非像素级图像质量恢复",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出基于超像素的正则化方法，对模型训练有一定改进"
    },
    {
        "title": "NDM: A Noise-driven Detection and Mitigation Framework against Implicit\n  Sexual Intentions in Text-to-Image Generation",
        "url": "http://arxiv.org/abs/2510.15752v1",
        "pub_date": "2025-10-17",
        "summary": "Despite the impressive generative capabilities of text-to-image (T2I) diffusion models, they remain vulnerable to generating inappropriate content, especially when confronted with implicit sexual prompts. Unlike explicit harmful prompts, these subtle cues, often disguised as seemingly benign terms, can unexpectedly trigger sexual content due to underlying model biases, raising significant ethical concerns. However, existing detection methods are primarily designed to identify explicit sexual content and therefore struggle to detect these implicit cues. Fine-tuning approaches, while effective to some extent, risk degrading the model's generative quality, creating an undesirable trade-off. To address this, we propose NDM, the first noise-driven detection and mitigation framework, which could detect and mitigate implicit malicious intention in T2I generation while preserving the model's original generative capabilities. Specifically, we introduce two key innovations: first, we leverage the separability of early-stage predicted noise to develop a noise-based detection method that could identify malicious content with high accuracy and efficiency; second, we propose a noise-enhanced adaptive negative guidance mechanism that could optimize the initial noise by suppressing the prominent region's attention, thereby enhancing the effectiveness of adaptive negative guidance for sexual mitigation. Experimentally, we validate NDM on both natural and adversarial datasets, demonstrating its superior performance over existing SOTA methods, including SLD, UCE, and RECE, etc. Code and resources are available at https://github.com/lorraine021/NDM.",
        "translated": "尽管文本到图像（T2I）扩散模型展现出令人印象深刻的内容生成能力，它们仍易受到生成不适当内容的影响，尤其是在面对隐含的性暗示提示时。与显式的有害提示不同，这些微妙的提示通常伪装成看似无害的词汇，由于模型内部存在的偏见，可能意外触发与性相关的内容，从而引发重大的伦理问题。然而，现有的检测方法主要设计用于识别显式的色情内容，因此难以检测这些隐含提示。尽管微调方法在某种程度上是有效的，但它们可能会降低模型的生成质量，造成令人不满意的性能与质量之间的权衡。为了解决这一问题，我们提出了NDM，第一个基于噪声驱动的内容检测和缓解框架，可以在保持模型原有生成能力的同时，检测并缓解T2I生成过程中的隐性恶意意图。具体而言，我们引入了两个关键创新：首先，我们利用早期预测噪声的可分性，开发了一种基于噪声的检测方法，能够以高精度和高效率识别恶意内容；其次，我们提出了一种噪声增强的自适应负向引导机制，该机制通过抑制显著区域的注意力来优化初始噪声，从而提高自适应负向引导在缓解性内容方面的有效性。实验上，我们在自然和对抗数据集上验证了NDM，结果表明其性能优于现有最先进的方法，包括SLD、UCE和RECE等。代码和资源可在 https://github.com/lorraine021/NDM 获取。",
        "translated_title": "NDM：一种基于噪声驱动的检测与缓解框架，用于应对文本到图像生成中的隐式性意图",
        "label": [],
        "label_reason": "论文聚焦文本到图像生成中的内容检测，不属于图像像素级处理任务",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出了首个基于噪声驱动的检测与缓解框架，具有一定创新性"
    },
    {
        "title": "SEGA: A Stepwise Evolution Paradigm for Content-Aware Layout Generation\n  with Design Prior",
        "url": "http://arxiv.org/abs/2510.15749v1",
        "pub_date": "2025-10-17",
        "summary": "In this paper, we study the content-aware layout generation problem, which aims to automatically generate layouts that are harmonious with a given background image. Existing methods usually deal with this task with a single-step reasoning framework. The lack of a feedback-based self-correction mechanism leads to their failure rates significantly increasing when faced with complex element layout planning. To address this challenge, we introduce SEGA, a novel Stepwise Evolution Paradigm for Content-Aware Layout Generation. Inspired by the systematic mode of human thinking, SEGA employs a hierarchical reasoning framework with a coarse-to-fine strategy: first, a coarse-level module roughly estimates the layout planning results; then, another refining module performs fine-level reasoning regarding the coarse planning results. Furthermore, we incorporate layout design principles as prior knowledge into the model to enhance its layout planning ability. Besides, we present GenPoster-100K that is a new large-scale poster dataset with rich meta-information annotation. The experiments demonstrate the effectiveness of our approach by achieving the state-of-the-art results on multiple benchmark datasets. Our project page is at: https://brucew91.github.io/SEGA.github.io/",
        "translated": "本文研究了内容感知的布局生成问题，旨在自动生成与给定背景图像和谐一致的布局。现有方法通常采用单步推理框架处理该任务。由于缺乏基于反馈的自修正机制，当面对复杂的元素布局规划时，其失败率显著上升。为了解决这一挑战，我们引入了 SEGA，一种新颖的内容感知布局生成的逐步演化范式。受人类系统性思维模式的启发，SEGA 采用分层推理框架和从粗到细的策略：首先，一个粗粒度模块大致估计布局规划结果；然后，另一个优化模块对粗粒度规划结果进行细粒度推理。此外，我们将布局设计原则作为先验知识引入模型，以提升其布局规划能力。同时，我们还提出了 GenPoster-100K，这是一个新的大规模海报数据集，包含丰富的元信息标注。实验表明，我们的方法在多个基准数据集上取得了最先进的结果，验证了其有效性。我们的项目页面为：https://brucew91.github.io/SEGA.github.io/",
        "translated_title": "SEGA：一种基于设计先验的内容感知布局生成的逐步演化范式",
        "label": [],
        "label_reason": "论文研究布局生成，属于high-level任务",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出分步演化范式SEGA，改进布局生成方法"
    },
    {
        "title": "FACE: A General Framework for Mapping Collaborative Filtering Embeddings\n  into LLM Tokens",
        "url": "http://arxiv.org/abs/2510.15729v1",
        "pub_date": "2025-10-17",
        "summary": "Recently, large language models (LLMs) have been explored for integration with collaborative filtering (CF)-based recommendation systems, which are crucial for personalizing user experiences. However, a key challenge is that LLMs struggle to interpret the latent, non-semantic embeddings produced by CF approaches, limiting recommendation effectiveness and further applications. To address this, we propose FACE, a general interpretable framework that maps CF embeddings into pre-trained LLM tokens. Specifically, we introduce a disentangled projection module to decompose CF embeddings into concept-specific vectors, followed by a quantized autoencoder to convert continuous embeddings into LLM tokens (descriptors). Then, we design a contrastive alignment objective to ensure that the tokens align with corresponding textual signals. Hence, the model-agnostic FACE framework achieves semantic alignment without fine-tuning LLMs and enhances recommendation performance by leveraging their pre-trained capabilities. Empirical results on three real-world recommendation datasets demonstrate performance improvements in benchmark models, with interpretability studies confirming the interpretability of the descriptors. Code is available in https://github.com/YixinRoll/FACE.",
        "translated": "近期，研究者探索将大语言模型（LLMs）与基于协同过滤（CF）的推荐系统结合，以提升用户个性化体验。然而，一个关键挑战在于LLMs难以解析CF方法产生的潜在且非语义的嵌入，这限制了推荐效果及其进一步应用。为解决这一问题，我们提出FACE，一种通用的可解释框架，将CF嵌入映射到预训练LLM的token中。具体来说，我们引入了一个解耦投影模块，将CF嵌入分解为概念特定的向量，并通过一个量化自编码器将连续嵌入转换为LLM token（描述符）。随后，我们设计了一个对比对齐目标，以确保token与对应的文本信号对齐。因此，该模型无关的FACE框架在不微调LLMs的情况下实现了语义对齐，并通过利用其预训练能力提升了推荐性能。在三个真实推荐数据集上的实验结果表明，基准模型的性能得到了提升，可解释性研究也验证了描述符的可解释性。代码可在https://github.com/YixinRoll/FACE获取。",
        "translated_title": "FACE：一种通用框架，用于将协同过滤嵌入映射到大语言模型（LLM）的token中",
        "label": [
            "LLM生成式推荐",
            "通用推荐技术"
        ],
        "label_reason": "将协同过滤嵌入映射为LLM token，提升推荐语义对齐与性能",
        "relevance_score": 9,
        "novelty_score": 8,
        "novelty_reason": "提出解耦投影和量化自编码器的新方法，实现LLM与CF的创新结合"
    },
    {
        "title": "The 3rd Place Solution of CCIR CUP 2025: A Framework for\n  Retrieval-Augmented Generation in Multi-Turn Legal Conversation",
        "url": "http://arxiv.org/abs/2510.15722v1",
        "pub_date": "2025-10-17",
        "summary": "Retrieval-Augmented Generation has made significant progress in the field of natural language processing. By combining the advantages of information retrieval and large language models, RAG can generate relevant and contextually appropriate responses based on items retrieved from reliable sources. This technology has demonstrated outstanding performance across multiple domains, but its application in the legal field remains in its exploratory phase. In this paper, we introduce our approach for \"Legal Knowledge Retrieval and Generation\" in CCIR CUP 2025, which leverages large language models and information retrieval systems to provide responses based on laws in response to user questions.",
        "translated": "检索增强生成（Retrieval-Augmented Generation, RAG）在自然语言处理领域取得了显著进展。通过结合信息检索与大语言模型的优势，RAG可以根据从可靠来源检索到的项目生成相关且符合上下文的回答。该技术在多个领域中展示了卓越的性能，但其在法律领域的应用仍处于探索阶段。在本文中，我们介绍了我们在CCIR CUP 2025中“法律知识检索与生成”任务中的方法，该方法利用大语言模型和信息检索系统，根据法律法规对用户问题提供回答。",
        "translated_title": "2025年CCIR CUP的第三名解决方案：一种用于多轮法律对话的检索增强生成框架",
        "label": [
            "LLM生成式推荐",
            "多模态推荐"
        ],
        "label_reason": "论文涉及基于检索增强生成的推荐方法，适用于法律领域的多轮对话生成。",
        "relevance_score": 7,
        "novelty_score": 6,
        "novelty_reason": "在RAG基础上提出法律对话生成框架，有一定创新但属于领域适配改进。"
    },
    {
        "title": "Cost-Aware Retrieval-Augmentation Reasoning Models with Adaptive\n  Retrieval Depth",
        "url": "http://arxiv.org/abs/2510.15719v1",
        "pub_date": "2025-10-17",
        "summary": "Reasoning models have gained significant attention due to their strong performance, particularly when enhanced with retrieval augmentation. However, these models often incur high computational costs, as both retrieval and reasoning tokens contribute substantially to the overall resource usage. In this work, we make the following contributions: (1) we propose a retrieval-augmented reasoning model that dynamically adjusts the length of the retrieved document list based on the query and retrieval results; (2) we develop a cost-aware advantage function for training of efficient retrieval-augmented reasoning models through reinforcement learning; and (3) we explore both memory- and latency-bound implementations of the proposed cost-aware framework for both proximal and group relative policy optimization algorithms. We evaluate our approach on seven public question answering datasets and demonstrate significant efficiency gains, without compromising effectiveness. In fact, we observed that the model latency decreases by ~16-20% across datasets, while its effectiveness increases by ~5% on average, in terms of exact match.",
        "translated": "由于其出色的表现，推理模型近年来受到了广泛关注，尤其是在引入检索增强后效果更为显著。然而，这些模型通常带来较高的计算成本，因为检索和推理过程中的 tokens 都会显著增加整体资源消耗。在本文中，我们做出以下贡献：(1) 我们提出了一种检索增强的推理模型，该模型能够根据查询和检索结果动态调整检索文档列表的长度；(2) 我们开发了一种成本感知的优势函数，通过强化学习训练高效的检索增强推理模型；(3) 我们探讨了所提出成本感知框架在近端策略优化和组相对策略优化算法中的内存受限和延迟受限实现。我们在七个公开的问题回答数据集上评估了我们的方法，并展示了在不损害效果的前提下显著提升的效率。事实上，我们观察到，模型的延迟在各个数据集上平均降低了 ~16-20%，而其在精确匹配方面的效果平均提高了 ~5%。",
        "translated_title": "成本感知的召回增强推理模型与自适应召回深度",
        "label": [
            "召回（Recall）",
            "通用推荐技术（General Recommendation Techniques）"
        ],
        "label_reason": "论文涉及检索增强推理，与推荐系统的召回阶段相关。",
        "relevance_score": 5,
        "novelty_score": 7,
        "novelty_reason": "提出动态调整检索长度和成本感知训练方法，有一定创新性。"
    },
    {
        "title": "GraphMind: Interactive Novelty Assessment System for Accelerating\n  Scientific Discovery",
        "url": "http://arxiv.org/abs/2510.15706v1",
        "pub_date": "2025-10-17",
        "summary": "Large Language Models (LLMs) show strong reasoning and text generation capabilities, prompting their use in scientific literature analysis, including novelty assessment. While evaluating novelty of scientific papers is crucial for peer review, it requires extensive knowledge of related work, something not all reviewers have. While recent work on LLM-assisted scientific literature analysis supports literature comparison, existing approaches offer limited transparency and lack mechanisms for result traceability via an information retrieval module. To address this gap, we introduce $\\textbf{GraphMind}$, an easy-to-use interactive web tool designed to assist users in evaluating the novelty of scientific papers or drafted ideas. Specially, $\\textbf{GraphMind}$ enables users to capture the main structure of a scientific paper, explore related ideas through various perspectives, and assess novelty via providing verifiable contextual insights. $\\textbf{GraphMind}$ enables users to annotate key elements of a paper, explore related papers through various relationships, and assess novelty with contextual insight. This tool integrates external APIs such as arXiv and Semantic Scholar with LLMs to support annotation, extraction, retrieval and classification of papers. This combination provides users with a rich, structured view of a scientific idea's core contributions and its connections to existing work. $\\textbf{GraphMind}$ is available at https://oyarsa.github.io/graphmind and a demonstration video at https://youtu.be/wKbjQpSvwJg. The source code is available at https://github.com/oyarsa/graphmind.",
        "translated": "大语言模型（LLMs）展现出强大的推理和文本生成能力，促使它们被用于科学文献分析，包括新颖性评估。尽管评估科学论文的新颖性对于同行评审至关重要，但这需要对相关工作有广泛的知识，而并非所有评审者都具备这一条件。虽然最近关于LLM辅助科学文献分析的研究支持文献对比，但现有方法在透明度方面存在较大局限，且缺乏通过信息检索模块实现结果可追溯的机制。为了解决这一问题，我们引入了 $\\textbf{GraphMind}$，一款易于使用的交互式网络工具，旨在协助用户评估科学论文或初稿想法的新颖性。具体而言，$\\textbf{GraphMind}$ 使用户能够捕捉科学论文的主要结构，通过多种视角探索相关观点，并通过提供可验证的上下文洞察评估新颖性。$\\textbf{GraphMind}$ 支持用户标注论文的关键要素，通过多种关系探索相关论文，并结合上下文信息评估新颖性。该工具将外部 API（如 arXiv 和 Semantic Scholar）与大语言模型（LLMs）相结合，以支持论文的标注、提取、检索和分类。这种组合为用户提供了对科学观点核心贡献及其与已有工作关联的丰富结构化视图。$\\textbf{GraphMind}$ 可在 https://oyarsa.github.io/graphmind 获取，演示视频可在 https://youtu.be/wKbjQpSvwJg 查看。源代码可在 https://github.com/oyarsa/graphmind 获得。",
        "translated_title": "GraphMind: 交互式新颖性评估系统，用于加速科学发现",
        "label": [],
        "label_reason": "论文主要关注科学论文新颖性评估，不直接涉及推荐系统核心技术。",
        "relevance_score": 3,
        "novelty_score": 7,
        "novelty_reason": "提出结合LLM与信息检索的新工具，解决科学文献新颖性评估中的透明性问题。"
    },
    {
        "title": "Mixture of Experts Approaches in Dense Retrieval Tasks",
        "url": "http://arxiv.org/abs/2510.15683v1",
        "pub_date": "2025-10-17",
        "summary": "Dense Retrieval Models (DRMs) are a prominent development in Information Retrieval (IR). A key challenge with these neural Transformer-based models is that they often struggle to generalize beyond the specific tasks and domains they were trained on. To address this challenge, prior research in IR incorporated the Mixture-of-Experts (MoE) framework within each Transformer layer of a DRM, which, though effective, substantially increased the number of additional parameters. In this paper, we propose a more efficient design, which introduces a single MoE block (SB-MoE) after the final Transformer layer. To assess the retrieval effectiveness of SB-MoE, we perform an empirical evaluation across three IR tasks. Our experiments involve two evaluation setups, aiming to assess both in-domain effectiveness and the model's zero-shot generalizability. In the first setup, we fine-tune SB-MoE with four different underlying DRMs on seven IR benchmarks and evaluate them on their respective test sets. In the second setup, we fine-tune SB-MoE on MSMARCO and perform zero-shot evaluation on thirteen BEIR datasets. Additionally, we perform further experiments to analyze the model's dependency on its hyperparameters (i.e., the number of employed and activated experts) and investigate how this variation affects SB-MoE's performance. The obtained results show that SB-MoE is particularly effective for DRMs with lightweight base models, such as TinyBERT and BERT-Small, consistently exceeding standard model fine-tuning across benchmarks. For DRMs with more parameters, such as BERT-Base and Contriever, our model requires a larger number of training samples to achieve improved retrieval performance. Our code is available online at: https://github.com/FaySokli/SB-MoE.",
        "translated": "稠密检索模型（DRMs）是信息检索（IR）领域的重要进展。基于神经网络Transformer的这些模型面临的一个关键挑战是，它们通常难以泛化到训练时特定任务和领域之外。为了解决这一挑战，以往的IR研究在每个Transformer层中引入了专家混合（MoE）框架，虽然这种方法有效，但却显著增加了额外的参数数量。在本文中，我们提出了一种更高效的结构设计，即在最终的Transformer层之后引入一个单一的MoE模块（SB-MoE）。为了评估SB-MoE的检索效果，我们在三个IR任务上进行了实证评估。我们的实验包含两种评估设置，旨在分别评估模型的在域性能和零样本泛化能力。在第一种设置中，我们在七个IR基准上使用四个不同的基础DRMs对SB-MoE进行微调，并在各自对应的测试集上进行评估。在第二种设置中，我们在MSMARCO上对SB-MoE进行微调，并在十三个BEIR数据集上进行零样本评估。此外，我们还进行了进一步的实验，以分析模型对超参数（即所使用和激活的专家数量）的依赖性，并研究这种变化如何影响SB-MoE的性能。实验结果表明，SB-MoE对轻量级基础模型的DRMs（如TinyBERT和BERT-Small）特别有效，其性能在多个基准上一致优于标准模型微调方法。对于参数较多的DRMs（如BERT-Base和Contriever），我们的模型需要更多的训练样本才能实现改进的检索性能。我们的代码可在以下网址获取：https://github.com/FaySokli/SB-MoE。",
        "translated_title": "稠密召回任务中的专家混合方法",
        "label": [
            "通用推荐技术"
        ],
        "label_reason": "论文涉及密集检索模型优化，可能适用于推荐系统召回阶段。",
        "relevance_score": 5,
        "novelty_score": 7,
        "novelty_reason": "提出一种更高效的MoE设计，改进了现有方法的参数效率。"
    },
    {
        "title": "SQuAI: Scientific Question-Answering with Multi-Agent\n  Retrieval-Augmented Generation",
        "url": "http://arxiv.org/abs/2510.15682v1",
        "pub_date": "2025-10-17",
        "summary": "We present SQuAI (https://squai.scads.ai/), a scalable and trustworthy multi-agent retrieval-augmented generation (RAG) framework for scientific question answering (QA) with large language models (LLMs). SQuAI addresses key limitations of existing RAG systems in the scholarly domain, where complex, open-domain questions demand accurate answers, explicit claims with citations, and retrieval across millions of scientific documents. Built on over 2.3 million full-text papers from arXiv.org, SQuAI employs four collaborative agents to decompose complex questions into sub-questions, retrieve targeted evidence via hybrid sparse-dense retrieval, and adaptively filter documents to improve contextual relevance. To ensure faithfulness and traceability, SQuAI integrates in-line citations for each generated claim and provides supporting sentences from the source documents. Our system improves faithfulness, answer relevance, and contextual relevance by up to +0.088 (12%) over a strong RAG baseline. We further release a benchmark of 1,000 scientific question-answer-evidence triplets to support reproducibility. With transparent reasoning, verifiable citations, and domain-wide scalability, SQuAI demonstrates how multi-agent RAG enables more trustworthy scientific QA with LLMs.",
        "translated": "我们提出SQuAI（https://squai.scads.ai/），一种基于大语言模型（LLM）的可扩展且可靠的多智能体检索增强生成（RAG）框架，用于科学问答（QA）。SQuAI解决了现有RAG系统在学术领域中的关键局限性，尤其是在处理复杂、开放领域的科学问题时，需要准确的答案、带引文的明确声明，以及在数百万篇科学文献中进行检索。SQuAI基于来自arXiv.org的超过230万篇全文论文构建，利用四个协作智能体将复杂问题分解为子问题，通过混合稀疏-密集检索获取目标证据，并自适应地过滤文档以提高上下文相关性。为确保答案的忠实性和可追溯性，SQuAI为每个生成的声明整合行内引文，并提供支持性句子以引证来源文档。与一个强大的RAG基线相比，我们的系统在答案忠实性、相关性以及上下文相关性方面最多提高了+0.088（12%）。我们进一步发布了一个包含1000个科学问题-答案-证据三元组的基准数据集，以支持可复现性。通过透明的推理、可验证的引文以及覆盖整个领域的可扩展性，SQuAI展示了多智能体RAG如何在LLMs中实现更可信的科学问答。",
        "translated_title": "SQuAI：基于多智能体的检索增强生成的科学问答",
        "label": [],
        "label_reason": "非推荐系统领域，但涉及RAG和检索技术",
        "relevance_score": 3,
        "novelty_score": 7,
        "novelty_reason": "多智能体协作与混合检索方法具有创新性"
    },
    {
        "title": "Enhance Large Language Models as Recommendation Systems with\n  Collaborative Filtering",
        "url": "http://arxiv.org/abs/2510.15647v1",
        "pub_date": "2025-10-17",
        "summary": "As powerful tools in Natural Language Processing (NLP), Large Language Models (LLMs) have been leveraged for crafting recommendations to achieve precise alignment with user preferences and elevate the quality of the recommendations. The existing approaches implement both non-tuning and tuning strategies. Compared to following the tuning strategy, the approaches following the non-tuning strategy avoid the relatively costly, time-consuming, and expertise-requiring process of further training pre-trained LLMs on task-specific datasets, but they suffer the issue of not having the task-specific business or local enterprise knowledge. To the best of our knowledge, none of the existing approaches following the non-tuning strategy explicitly integrates collaborative filtering, one of the most successful recommendation techniques. This study aims to fill the gap by proposing critique-based LLMs as recommendation systems (Critic-LLM-RS). For our purpose, we train a separate machine-learning model called Critic that implements collaborative filtering for recommendations by learning from the interactions between many users and items. The Critic provides critiques to LLMs to significantly refine the recommendations. Extensive experiments have verified the effectiveness of Critic-LLM-RS on real datasets.",
        "translated": "在自然语言处理（NLP）中，大语言模型（LLMs）作为强大的工具已被用于生成推荐，以实现与用户偏好的精确对齐并提升推荐质量。现有方法采用了非微调和微调两种策略。与采用微调策略的方法相比，采用非微调策略的方法避免了在任务特定数据集上进一步训练预训练大语言模型这一相对成本高、耗时长且需要专业知识的过程，但它们缺乏任务特定的商业或本地企业知识。据我们所知，目前没有采用非微调策略的现有方法明确地整合协同过滤这一最成功的推荐技术之一。本研究旨在通过提出基于批评的大语言模型推荐系统（Critic-LLM-RS）来弥补这一不足。为此，我们训练了一个单独的机器学习模型，称为Critic，它通过学习大量用户与物料之间的交互来实现基于协同过滤的推荐。Critic向大语言模型提供批评，从而显著优化推荐结果。大量的实验验证了Critic-LLM-RS在真实数据集上的有效性。",
        "translated_title": "利用协同过滤增强大语言模型作为推荐系统",
        "label": [
            "LLM生成式推荐",
            "通用推荐技术"
        ],
        "label_reason": "论文探讨了LLM在推荐系统中的应用，并结合协同过滤技术",
        "relevance_score": 8,
        "novelty_score": 7,
        "novelty_reason": "提出Critic-LLM-RS框架，创新性地将协同过滤与LLM结合"
    },
    {
        "title": "MCA: Modality Composition Awareness for Robust Composed Multimodal\n  Retrieval",
        "url": "http://arxiv.org/abs/2510.15543v1",
        "pub_date": "2025-10-17",
        "summary": "Multimodal retrieval, which seeks to retrieve relevant content across modalities such as text or image, supports applications from AI search to contents production. Despite the success of separate-encoder approaches like CLIP align modality-specific embeddings with contrastive learning, recent multimodal large language models (MLLMs) enable a unified encoder that directly processes composed inputs. While flexible and advanced, we identify that unified encoders trained with conventional contrastive learning are prone to learn modality shortcut, leading to poor robustness under distribution shifts. We propose a modality composition awareness framework to mitigate this issue. Concretely, a preference loss enforces multimodal embeddings to outperform their unimodal counterparts, while a composition regularization objective aligns multimodal embeddings with prototypes composed from its unimodal parts. These objectives explicitly model structural relationships between the composed representation and its unimodal counterparts. Experiments on various benchmarks show gains in out-of-distribution retrieval, highlighting modality composition awareness as a effective principle for robust composed multimodal retrieval when utilizing MLLMs as the unified encoder.",
        "translated": "跨模态检索旨在跨文本或图像等不同模态中检索相关内容，广泛支持从人工智能搜索到内容生产的应用场景。尽管像 CLIP 这样的独立编码器方法通过对比学习对齐模态特定嵌入取得了成功，但最近的多模态大语言模型（MLLMs）引入了能够直接处理组合输入的统一编码器。虽然统一编码器具备灵活性和先进性，但我们发现使用传统对比学习训练的统一编码器容易学习模态捷径，从而在分布偏移情况下表现不佳。我们提出了一种模态组合感知框架来缓解这一问题。具体而言，通过一个偏好损失强制多模态嵌入优于其单模态对应嵌入，同时通过一个组合正则化目标将多模态嵌入与由其单模态部分组合而成的原型对齐。这些目标显式建模了组合表示与其单模态嵌入之间的结构关系。在多个基准上的实验表明，该框架在分布外检索中取得了提升，突显了模态组合感知作为使用 MLLMs 作为统一编码器时实现鲁棒组合式多模态检索的有效原则。",
        "translated_title": "MCA：模态组合感知的鲁棒组合多模态召回",
        "label": [
            "多模态推荐"
        ],
        "label_reason": "论文聚焦多模态检索，与推荐系统中的多模态推荐有间接联系。",
        "relevance_score": 5,
        "novelty_score": 7,
        "novelty_reason": "提出模态组合感知框架，改进统一编码器训练策略，具有一定创新性。"
    },
    {
        "title": "MSAM: Multi-Semantic Adaptive Mining for Cross-Modal Drone Video-Text\n  Retrieval",
        "url": "http://arxiv.org/abs/2510.15470v1",
        "pub_date": "2025-10-17",
        "summary": "With the advancement of drone technology, the volume of video data increases rapidly, creating an urgent need for efficient semantic retrieval. We are the first to systematically propose and study the drone video-text retrieval (DVTR) task. Drone videos feature overhead perspectives, strong structural homogeneity, and diverse semantic expressions of target combinations, which challenge existing cross-modal methods designed for ground-level views in effectively modeling their characteristics. Therefore, dedicated retrieval mechanisms tailored for drone scenarios are necessary. To address this issue, we propose a novel approach called Multi-Semantic Adaptive Mining (MSAM). MSAM introduces a multi-semantic adaptive learning mechanism, which incorporates dynamic changes between frames and extracts rich semantic information from specific scene regions, thereby enhancing the deep understanding and reasoning of drone video content. This method relies on fine-grained interactions between words and drone video frames, integrating an adaptive semantic construction module, a distribution-driven semantic learning term and a diversity semantic term to deepen the interaction between text and drone video modalities and improve the robustness of feature representation. To reduce the interference of complex backgrounds in drone videos, we introduce a cross-modal interactive feature fusion pooling mechanism that focuses on feature extraction and matching in target regions, minimizing noise effects. Extensive experiments on two self-constructed drone video-text datasets show that MSAM outperforms other existing methods in the drone video-text retrieval task. The source code and dataset will be made publicly available.",
        "translated": "随着无人机技术的发展，视频数据的数量迅速增加，从而对高效的语义召回提出了迫切需求。我们首次系统性地提出了并研究了无人机视频-文本召回（DVTR）任务。无人机视频具有俯视视角、强烈的结构同质性以及目标组合多样的语义表达，这些特性对当前为地面视角设计的跨模态方法在有效建模方面提出了挑战。因此，有必要设计专门针对无人机场景的召回机制。为了解决这一问题，我们提出了一种新颖的方法，称为多语义自适应挖掘（MSAM）。MSAM 引入了一种多语义自适应学习机制，能够捕捉帧之间的动态变化，并从特定场景区域中提取丰富的语义信息，从而增强对无人机视频内容的深入理解和推理能力。该方法依赖于词语与无人机视频帧之间的细粒度交互，融合了自适应语义构建模块、分布驱动的语义学习项以及多样性语义项，以深化文本与无人机视频模态之间的交互，并提升特征表示的鲁棒性。为了减少无人机视频中复杂背景的干扰，我们引入了一种跨模态交互式特征融合池化机制，该机制聚焦于目标区域的特征提取与匹配，从而最小化噪声的影响。在两个自建的无人机视频-文本数据集上的广泛实验表明，MSAM 在无人机视频-文本召回任务中优于其他现有方法。源代码和数据集将向公众开放。",
        "translated_title": "MSAM：用于跨模态无人机视频-文本召回的多语义自适应挖掘",
        "label": [],
        "label_reason": "",
        "relevance_score": 0,
        "novelty_score": 0,
        "novelty_reason": ""
    },
    {
        "title": "Fault Cause Identification across Manufacturing Lines through\n  Ontology-Guided and Process-Aware FMEA Graph Learning with LLMs",
        "url": "http://arxiv.org/abs/2510.15428v1",
        "pub_date": "2025-10-17",
        "summary": "Fault cause identification in automated manufacturing lines is challenging due to the system's complexity, frequent reconfigurations, and the limited reusability of existing Failure Mode and Effects Analysis (FMEA) knowledge. Although FMEA worksheets contain valuable expert insights, their reuse across heterogeneous lines is hindered by natural language variability, inconsistent terminology, and process differences. To address these limitations, this study proposes a process-aware framework that enhances FMEA reusability by combining manufacturing-domain conceptualization with graph neural network (GNN) reasoning. First, FMEA worksheets from multiple manufacturing lines are transformed into a unified knowledge graph through ontology-guided large language model (LLM) extraction, capturing domain concepts such as actions, states, components, and parameters. Second, a Relational Graph Convolutional Network (RGCN) with the process-aware scoring function learns embeddings that respect both semantic relationships and sequential process flows. Finally, link prediction is employed to infer and rank candidate fault causes consistent with the target line's process flow.   A case study on automotive pressure sensor assembly lines demonstrates that the proposed method outperforms a state-of-the-art retrieval-augmented generation (RAG) baseline (F1@20 = 0.267) and an RGCN approach (0.400), achieving the best performance (0.523) in fault cause identification. Ablation studies confirm the contributions of both LLM-driven domain conceptualization and process-aware learning. These results indicate that the proposed framework significantly improves the transferability of FMEA knowledge across heterogeneous lines, thereby supporting operators in diagnosing failures more reliably and paving the way for future domain-adaptive LLM applications in smart manufacturing.",
        "translated": "在自动化生产线中，故障原因识别由于系统复杂性、频繁重构以及现有失效模式与影响分析（FMEA）知识的有限可重用性而具有挑战性。尽管FMEA工作表包含了宝贵的专家见解，但其在异构生产线之间的复用受到自然语言差异、术语不一致和流程差异的限制。为了解决这些局限性，本文提出了一种流程感知框架，通过将制造领域的概念化与图神经网络（GNN）推理相结合，以增强FMEA知识的可重用性。首先，利用本体引导的大语言模型（LLM）从多条制造生产线中提取FMEA工作表，将其转换为统一的知识图谱，从而捕捉诸如操作、状态、组件和参数等领域概念。其次，使用具有流程感知评分函数的关系图卷积网络（RGCN）学习嵌入，该嵌入同时尊重语义关系和顺序流程结构。最后，通过链接预测来推断并排序与目标生产线流程一致的候选故障原因。在汽车压力传感器装配线上的案例研究表明，所提出的方法优于最先进的检索增强生成（RAG）基线方法（F1@20 = 0.267）和RGCN方法（0.400），在故障原因识别上取得了最佳性能（0.523）。消融实验验证了LLM驱动的领域概念化和流程感知学习的贡献。这些结果表明，所提出的框架显著提升了FMEA知识在异构生产线之间的可迁移性，从而帮助操作人员更可靠地诊断故障，并为未来智能制造中领域自适应的大语言模型应用铺平了道路。",
        "translated_title": "跨生产线的故障原因识别：基于本体引导与流程感知的FMEA图学习与大语言模型",
        "label": [],
        "label_reason": "论文聚焦于制造领域的故障分析，与推荐系统无直接关联。",
        "relevance_score": 3,
        "novelty_score": 6,
        "novelty_reason": "提出结合本体论与GNN的框架改进FMEA，有一定创新性但非推荐系统核心方向。"
    },
    {
        "title": "Dimension Mask Layer: Optimizing Embedding Efficiency for Scalable\n  ID-based Models",
        "url": "http://arxiv.org/abs/2510.15308v1",
        "pub_date": "2025-10-17",
        "summary": "In modern recommendation systems and social media platforms like Meta, TikTok, and Instagram, large-scale ID-based features often require embedding tables that consume significant memory. Managing these embedding sizes can be challenging, leading to bulky models that are harder to deploy and maintain. In this paper, we introduce a method to automatically determine the optimal embedding size for ID features, significantly reducing the model size while maintaining performance.   Our approach involves defining a custom Keras layer called the dimension mask layer, which sits directly after the embedding lookup. This layer trims the embedding vector by allowing only the first N dimensions to pass through. By doing this, we can reduce the input feature dimension by more than half with minimal or no loss in model performance metrics. This reduction helps cut down the memory footprint of the model and lowers the risk of overfitting due to multicollinearity.   Through offline experiments on public datasets and an online A/B test on a real production dataset, we demonstrate that using a dimension mask layer can shrink the effective embedding dimension by 40-50\\%, leading to substantial improvements in memory efficiency. This method provides a scalable solution for platforms dealing with a high volume of ID features, optimizing both resource usage and model performance.",
        "translated": "在现代推荐系统和Meta、TikTok以及Instagram等社交媒体平台中，大规模基于ID的特征通常需要嵌入表，而嵌入表会消耗大量内存。管理这些嵌入的维度规模具有挑战性，从而导致模型体积庞大，难以部署和维护。在本文中，我们介绍了一种方法，用于自动确定ID特征的最优嵌入维度，从而在保持性能的前提下显著减小模型规模。\n\n我们的方法包括定义一个自定义的Keras层，称为维度掩码层（dimension mask layer），该层直接位于嵌入查找（embedding lookup）之后。该层通过仅允许嵌入向量的前N个维度通过，从而对嵌入向量进行裁剪。通过这种方式，我们可以在几乎没有或完全没有损失模型性能指标的情况下，将输入特征维度减少一半以上。这种降维有助于减小模型的内存占用，并降低由于多重共线性导致的过拟合风险。\n\n通过在公共数据集上的离线实验和在真实生产数据集上的在线A/B测试，我们证明使用维度掩码层可以将有效的嵌入维度减少40-50%，从而在内存效率方面实现显著提升。该方法为处理大量ID特征的平台提供了一种可扩展的解决方案，同时优化了资源使用和模型性能。",
        "translated_title": "维度掩码层：优化可扩展ID模型的嵌入效率",
        "label": [
            "通用推荐技术",
            "负采样与对比学习"
        ],
        "label_reason": "论文提出嵌入维度优化方法，适用于推荐系统特征建模。",
        "relevance_score": 7,
        "novelty_score": 6,
        "novelty_reason": "改进了嵌入层设计，有一定实用创新但非范式突破。"
    },
    {
        "title": "GRank: Towards Target-Aware and Streamlined Industrial Retrieval with a\n  Generate-Rank Framework",
        "url": "http://arxiv.org/abs/2510.15299v1",
        "pub_date": "2025-10-17",
        "summary": "Industrial-scale recommender systems rely on a cascade pipeline in which the retrieval stage must return a high-recall candidate set from billions of items under tight latency. Existing solutions ei- ther (i) suffer from limited expressiveness in capturing fine-grained user-item interactions, as seen in decoupled dual-tower architectures that rely on separate encoders, or generative models that lack precise target-aware matching capabilities, or (ii) build structured indices (tree, graph, quantization) whose item-centric topologies struggle to incorporate dynamic user preferences and incur prohibitive construction and maintenance costs.   We present GRank, a novel structured-index-free retrieval paradigm that seamlessly unifies target-aware learning with user-centric retrieval. Our key innovations include: (1) A target-aware Generator trained to perform personalized candidate generation via GPU-accelerated MIPS, eliminating semantic drift and maintenance costs of structured indexing; (2) A lightweight but powerful Ranker that performs fine-grained, candidate-specific inference on small subsets; (3) An end-to-end multi-task learning framework that ensures semantic consistency between generation and ranking objectives.   Extensive experiments on two public benchmarks and a billion-item production corpus demonstrate that GRank improves Recall@500 by over 30% and 1.7$\\times$ the P99 QPS of state-of-the-art tree- and graph-based retrievers.   GRank has been fully deployed in production in our recommendation platform since Q2 2025, serving 400 million active users with 99.95% service availability. Online A/B tests confirm significant improvements in core engagement metrics, with Total App Usage Time increasing by 0.160% in the main app and 0.165% in the Lite version.",
        "translated": "工业级推荐系统依赖于一个级联流水线，其中召回阶段必须在严格的延迟限制下从数十亿个物料中返回一个高召回的候选集。现有解决方案要么（i）在捕捉用户-物料细粒度交互方面存在表达能力的局限，如依赖独立编码器的解耦双塔架构，或缺乏精确目标感知匹配能力的生成模型；要么（ii）构建结构化索引（树、图、量化），其以物料为中心的拓扑结构难以融合动态的用户偏好，并带来高昂的构建和维护成本。\n\n我们提出了 GRank，一种新颖的、不依赖结构化索引的召回范式，该范式无缝地将目标感知学习与用户中心召回统一起来。我们的关键创新包括：（1）一个目标感知的生成器（Generator），其通过 GPU 加速的 MIPS 进行个性化候选生成，消除了语义漂移和结构化索引的维护成本；（2）一个轻量但强大的排序器（Ranker），其在小规模候选子集上进行细粒度、候选特定的推理；（3）一个端到端的多任务学习框架，确保生成和排序目标之间的语义一致性。\n\n在两个公开基准和一个包含十亿级物料的生产语料上的广泛实验表明，GRank 在 Recall@500 上提升了超过 30%，并且在 P99 QPS 上达到了最先进树型和图型召回器的 1.7 倍。GRank 自 2025 年第二季度起已在我们的推荐平台中全面部署，服务于 4 亿活跃用户，服务可用性达到 99.95%。在线 A/B 测试确认了其在核心参与度指标上的显著提升，主应用中的总使用时长（Total App Usage Time）增加了 0.160%，Lite 版本增加了 0.165%。",
        "translated_title": "GRank：面向目标感知和高效工业召回的生成-排序框架",
        "label": [
            "召回（Recall）",
            "精排（Ranking）",
            "LLM生成式推荐（LLM-based Generative Recommendation）",
            "通用推荐技术（General Recommendation Techniques）"
        ],
        "label_reason": "GRank 提出生成-排序框架，提升推荐召回与排序效果，适用于工业级推荐场景。",
        "relevance_score": 9,
        "novelty_score": 8,
        "novelty_reason": "结合目标感知生成与轻量排序，避免结构化索引，创新性强且效果显著。"
    },
    {
        "title": "MTmixAtt: Integrating Mixture-of-Experts with Multi-Mix Attention for\n  Large-Scale Recommendation",
        "url": "http://arxiv.org/abs/2510.15286v1",
        "pub_date": "2025-10-17",
        "summary": "Industrial recommender systems critically depend on high-quality ranking models. However, traditional pipelines still rely on manual feature engineering and scenario-specific architectures, which hinder cross-scenario transfer and large-scale deployment. To address these challenges, we propose \\textbf{MTmixAtt}, a unified Mixture-of-Experts (MoE) architecture with Multi-Mix Attention, designed for large-scale recommendation tasks. MTmixAtt integrates two key components. The \\textbf{AutoToken} module automatically clusters heterogeneous features into semantically coherent tokens, removing the need for human-defined feature groups. The \\textbf{MTmixAttBlock} module enables efficient token interaction via a learnable mixing matrix, shared dense experts, and scenario-aware sparse experts, capturing both global patterns and scenario-specific behaviors within a single framework. Extensive experiments on the industrial TRec dataset from Meituan demonstrate that MTmixAtt consistently outperforms state-of-the-art baselines including Transformer-based models, WuKong, HiFormer, MLP-Mixer, and RankMixer. At comparable parameter scales, MTmixAtt achieves superior CTR and CTCVR metrics; scaling to MTmixAtt-1B yields further monotonic gains. Large-scale online A/B tests validate the real-world impact: in the \\textit{Homepage} scenario, MTmixAtt increases Payment PV by \\textbf{+3.62\\%} and Actual Payment GTV by \\textbf{+2.54\\%}. Overall, MTmixAtt provides a unified and scalable solution for modeling arbitrary heterogeneous features across scenarios, significantly improving both user experience and commercial outcomes.",
        "translated": "工业推荐系统高度依赖高质量的排序模型。然而，传统的推荐流程仍然依赖人工特征工程和特定场景的模型架构，这限制了跨场景的迁移能力和大规模部署的效率。为了解决这些挑战，我们提出了**MTmixAtt**，一种基于多混合注意力机制的统一专家混合（Mixture-of-Experts，MoE）架构，专为大规模推荐任务设计。MTmixAtt 集成了两个关键模块。**AutoToken** 模块能够自动将异构特征聚类为语义连贯的 tokens，从而去除了对人工定义特征组的依赖。**MTmixAttBlock** 模块则通过可学习的混合矩阵、共享的密集专家和场景感知的稀疏专家实现高效的 token 交互，从而在单一框架内同时捕捉全局模式和特定场景的行为。在来自美团的工业级 TRec 数据集上的大量实验表明，MTmixAtt 在点击率（CTR）和转化率（CTCVR）等指标上始终优于当前最先进的基线模型，包括基于 Transformer 的模型、WuKong、HiFormer、MLP-Mixer 和 RankMixer。在参数规模相当的情况下，MTmixAtt-1B 在扩展后进一步实现了单调性能提升。大规模的在线 A/B 测试验证了其在实际中的效果：在 \\textit{Homepage} 场景下，MTmixAtt 带来了 \\textbf{+3.62\\%} 的支付页面访问量（Payment PV）提升和 \\textbf{+2.54\\%} 的实际支付 GMV（GTV）增长。总体而言，MTmixAtt 为跨场景建模任意异构特征提供了一个统一且可扩展的解决方案，显著提升了用户体验和商业成果。",
        "translated_title": "MTmixAtt：将专家混合与多混合注意力结合用于大规模推荐",
        "label": [],
        "label_reason": "",
        "relevance_score": 0,
        "novelty_score": 0,
        "novelty_reason": ""
    },
    {
        "title": "HOB: A Holistically Optimized Bidding Strategy under Heterogeneous\n  Auction Mechanisms with Organic Traffic",
        "url": "http://arxiv.org/abs/2510.15238v1",
        "pub_date": "2025-10-17",
        "summary": "The E-commerce advertising platforms typically sell commercial traffic through either second-price auction (SPA) or first-price auction (FPA). SPA was historically prevalent due to its dominant strategy incentive-compatible (DSIC) for bidders with quasi-linear utilities, especially when budgets are not a binding constraint, while FPA has gained more prominence for offering higher revenue potential to publishers and avoiding the possibility for discriminatory treatment in personalized reserve prices. Meanwhile, on the demand side, advertisers are increasingly adopting platform-wide marketing solutions akin to QuanZhanTui, shifting from spending budgets solely on commercial traffic to bidding on the entire traffic for the purpose of maximizing overall sales. For automated bidding systems, such a trend poses a critical challenge: determining optimal strategies across heterogeneous auction channels to fulfill diverse advertiser objectives, such as maximizing return (MaxReturn) or meeting target return on ad spend (TargetROAS). To overcome this challenge, this work makes two key contributions. First, we derive an efficient solution for optimal bidding under FPA channels, which takes into account the presence of organic traffic - traffic can be won for free. Second, we introduce a marginal cost alignment (MCA) strategy that provably secures bidding efficiency across heterogeneous auction mechanisms. To validate performance of our developed framework, we conduct comprehensive offline experiments on public datasets and large-scale online A/B testing, which demonstrate consistent improvements over existing methods.",
        "translated": "电子商务广告平台通常通过第二价格拍卖（SPA）或第一价格拍卖（FPA）出售商业流量。SPA 在历史上较为普遍，因为它对具有准线性效用的竞标者而言是占优策略激励相容（DSIC）的，尤其是在预算并非约束条件的情况下。而 FPA 因其能为发布者提供更高的收入潜力，并避免在个性化保留价中出现歧视性待遇的可能性，而逐渐受到更多关注。与此同时，在需求侧，广告主正越来越多地采用类似 QuanZhanTui 的平台级营销方案，从仅仅将预算用于商业流量，转向对全部流量进行竞价，以实现整体销售额的最大化。对于自动出价系统而言，这一趋势带来了关键挑战：如何在异构的拍卖渠道中确定最优策略，以满足广告主的多样化目标，例如最大化回报（MaxReturn）或达到目标广告支出回报率（TargetROAS）。为克服这一挑战，本文做出了两个关键贡献。首先，我们推导了一个在 FPA 渠道下高效确定最优出价的解决方案，该方案考虑了免费流量（即自然流量）的存在——这些流量可以免费获取。其次，我们引入了一种边际成本对齐（MCA）策略，该策略在理论上可确保在异构拍卖机制中实现出价效率。为验证所提出框架的性能，我们在公开数据集上进行了全面的离线实验，并在大规模在线 A/B 测试中进行了验证，结果表明该方法相较于现有方法实现了持续的改进。",
        "translated_title": "HOB：异质拍卖机制与自然流量下的整体优化出价策略",
        "label": [
            "跨域/联邦推荐"
        ],
        "label_reason": "论文涉及广告竞价策略优化，与推荐系统有一定间接关联。",
        "relevance_score": 5,
        "novelty_score": 7,
        "novelty_reason": "提出了针对异构竞价机制的边际成本对齐策略，具有一定创新性。"
    },
    {
        "title": "Structure-R1: Dynamically Leveraging Structural Knowledge in LLM\n  Reasoning through Reinforcement Learning",
        "url": "http://arxiv.org/abs/2510.15191v1",
        "pub_date": "2025-10-16",
        "summary": "Large language models (LLMs) have demonstrated remarkable advances in reasoning capabilities. However, their performance remains constrained by limited access to explicit and structured domain knowledge. Retrieval-Augmented Generation (RAG) addresses this by incorporating external information as context to augment reasoning. Nevertheless, traditional RAG systems typically operate over unstructured and fragmented text, resulting in low information density and suboptimal reasoning. To overcome these limitations, we propose \\textsc{Structure-R1}, a novel framework that transforms retrieved content into structured representations optimized for reasoning. Leveraging reinforcement learning, \\textsc{Structure-R1} learns a content representation policy that dynamically generates and adapts structural formats based on the demands of multi-step reasoning. Unlike prior methods that rely on fixed schemas, our approach adopts a generative paradigm capable of producing task-specific structures tailored to individual queries. To ensure the quality and reliability of these representations, we introduce a self-reward structural verification mechanism that checks whether the generated structures are both correct and self-contained. Extensive experiments on seven knowledge-intensive benchmarks show that \\textsc{Structure-R1} consistently achieves competitive performance with a 7B-scale backbone model and matches the performance of much larger models. Additionally, our theoretical analysis demonstrates how structured representations enhance reasoning by improving information density and contextual clarity. Our code and data are available at: https://github.com/jlwu002/sr1.",
        "translated": "大语言模型（LLMs）在推理能力方面已展现出显著的进展。然而，它们的性能仍受限于对显式且结构化领域知识的访问不足。检索增强生成（RAG）通过将外部信息作为上下文以增强推理能力来解决这一问题。然而，传统的 RAG 系统通常在非结构化和碎片化的文本上运行，导致信息密度低和推理效果不佳。为克服这些限制，我们提出了 \\textsc{Structure-R1}，一种新颖的框架，能够将检索到的内容转换为优化推理的结构化表示。该框架利用强化学习，学习一种内容表示策略，能够根据多步推理的需求动态生成和调整结构化格式。不同于依赖固定模式的先前方法，我们的方法采用生成式范式，能够为每个查询生成特定任务的结构。为确保这些表示的质量和可靠性，我们引入了一种自奖励的结构验证机制，用以检查生成的结构是否正确且自洽。在七个需要大量知识的基准数据集上的广泛实验表明，\\textsc{Structure-R1} 在使用 7B 规模主干模型的情况下始终能够实现具有竞争力的性能，并可与更大模型的表现相匹配。此外，我们的理论分析展示了结构化表示如何通过提高信息密度和上下文清晰度来增强推理。我们的代码和数据可在以下地址获取：https://github.com/jlwu002/sr1。",
        "translated_title": "Structure-R1：通过强化学习动态利用大语言模型中的结构化知识",
        "label": [
            "LLM生成式推荐"
        ],
        "label_reason": "论文将结构化知识与LLM结合，适用于生成式推荐系统的推理增强。",
        "relevance_score": 6,
        "novelty_score": 8,
        "novelty_reason": "提出动态生成结构化表示的新框架，并结合强化学习进行优化。"
    },
    {
        "title": "DMRetriever: A Family of Models for Improved Text Retrieval in Disaster\n  Management",
        "url": "http://arxiv.org/abs/2510.15087v1",
        "pub_date": "2025-10-16",
        "summary": "Effective and efficient access to relevant information is essential for disaster management. However, no retrieval model is specialized for disaster management, and existing general-domain models fail to handle the varied search intents inherent to disaster management scenarios, resulting in inconsistent and unreliable performance. To this end, we introduce DMRetriever, the first series of dense retrieval models (33M to 7.6B) tailored for this domain. It is trained through a novel three-stage framework of bidirectional attention adaptation, unsupervised contrastive pre-training, and difficulty-aware progressive instruction fine-tuning, using high-quality data generated through an advanced data refinement pipeline. Comprehensive experiments demonstrate that DMRetriever achieves state-of-the-art (SOTA) performance across all six search intents at every model scale. Moreover, DMRetriever is highly parameter-efficient, with 596M model outperforming baselines over 13.3 X larger and 33M model exceeding baselines with only 7.6% of their parameters. All codes, data, and checkpoints are available at https://github.com/KaiYin97/DMRETRIEVER",
        "translated": "在灾难管理中，有效且高效地获取相关信息至关重要。然而，目前尚无专门针对灾难管理设计的检索模型，现有的通用领域模型无法处理灾难管理场景中固有的多样化的搜索意图，从而导致性能表现不一致且不可靠。为此，我们引入了 DMRetriever，这是首个专门为此领域定制的密集检索模型系列（33M 到 7.6B 参数）。该模型通过一种新颖的三阶段训练框架进行训练，包括双向注意力适配、无监督对比预训练以及难度感知的渐进式指令微调，使用了通过先进数据精炼流水线生成的高质量数据。全面的实验表明，DMRetriever 在所有六种搜索意图上均在每种模型规模下实现了最先进的（SOTA）性能。此外，DMRetriever 具有高度参数效率，596M 模型的性能优于参数规模为其 13.3 倍的基线模型，而 33M 模型的性能甚至超过了基线模型，其参数仅为基线模型的 7.6%。所有代码、数据和模型检查点均可在 https://github.com/KaiYin97/DMRETRIEVER 获取。",
        "translated_title": "DMRetriever: 用于灾难管理中提升文本召回的一组模型",
        "label": [
            "召回（Recall）"
        ],
        "label_reason": "论文聚焦灾害管理中的文本检索，属于信息检索范畴，与推荐系统召回环节间接相关。",
        "relevance_score": 4,
        "novelty_score": 7,
        "novelty_reason": "提出三阶段训练框架和高效参数模型，在检索模型设计上有创新。"
    },
    {
        "title": "ConsistEdit: Highly Consistent and Precise Training-free Visual Editing",
        "url": "http://arxiv.org/abs/2510.17803v1",
        "pub_date": "2025-10-20",
        "summary": "Recent advances in training-free attention control methods have enabled flexible and efficient text-guided editing capabilities for existing generation models. However, current approaches struggle to simultaneously deliver strong editing strength while preserving consistency with the source. This limitation becomes particularly critical in multi-round and video editing, where visual errors can accumulate over time. Moreover, most existing methods enforce global consistency, which limits their ability to modify individual attributes such as texture while preserving others, thereby hindering fine-grained editing. Recently, the architectural shift from U-Net to MM-DiT has brought significant improvements in generative performance and introduced a novel mechanism for integrating text and vision modalities. These advancements pave the way for overcoming challenges that previous methods failed to resolve. Through an in-depth analysis of MM-DiT, we identify three key insights into its attention mechanisms. Building on these, we propose ConsistEdit, a novel attention control method specifically tailored for MM-DiT. ConsistEdit incorporates vision-only attention control, mask-guided pre-attention fusion, and differentiated manipulation of the query, key, and value tokens to produce consistent, prompt-aligned edits. Extensive experiments demonstrate that ConsistEdit achieves state-of-the-art performance across a wide range of image and video editing tasks, including both structure-consistent and structure-inconsistent scenarios. Unlike prior methods, it is the first approach to perform editing across all inference steps and attention layers without handcraft, significantly enhancing reliability and consistency, which enables robust multi-round and multi-region editing. Furthermore, it supports progressive adjustment of structural consistency, enabling finer control.",
        "translated": "近年来，训练无关的注意力控制方法在文本引导的图像编辑任务中取得了显著进展，使得现有生成模型具备了灵活且高效的编辑能力。然而，当前方法在实现强大编辑能力的同时，往往难以保持与原始图像的一致性。这一局限在多轮编辑和视频编辑任务中尤为突出，因为视觉误差可能会随时间累积。此外，大多数现有方法仅强制全局一致性，限制了在保留其他属性的同时修改特定属性（如纹理）的能力，从而阻碍了细粒度的编辑。最近，从 U-Net 架构向 MM-DiT 的转变在生成性能上带来了显著提升，并引入了一种新的文本与视觉模态融合机制。这些进展为克服先前方法未能解决的挑战奠定了基础。通过对 MM-DiT 的深入分析，我们发现了其注意力机制中的三个关键见解。基于这些发现，我们提出了 ConsistEdit，一种专为 MM-DiT 量身定制的全新注意力控制方法。ConsistEdit 融合了仅视觉的注意力控制、掩膜引导的预注意力融合机制，以及对 query、key 和 value token 的差异化操作，以实现一致且与提示对齐的编辑效果。大量实验证明，ConsistEdit 在多种图像和视频编辑任务中均取得了最先进的性能，涵盖了结构一致和结构不一致的场景。与以往方法不同，这是首个无需人工设计，即可在所有推理步骤和注意力层中进行编辑的方法，显著提升了编辑的鲁棒性和一致性，从而支持多轮和多区域的编辑操作。此外，该方法还支持结构一致性的渐进式调整，实现了更精细的控制。",
        "translated_title": "ConsistEdit: 高一致性且高精度的免训练视觉编辑",
        "label": [],
        "label_reason": "论文聚焦图像编辑而非像素级质量复原",
        "relevance_score": 3,
        "novelty_score": 7,
        "novelty_reason": "提出新的注意力控制机制，改进视频和多轮编辑一致性"
    },
    {
        "title": "Glyph: Scaling Context Windows via Visual-Text Compression",
        "url": "http://arxiv.org/abs/2510.17800v1",
        "pub_date": "2025-10-20",
        "summary": "Large language models (LLMs) increasingly rely on long-context modeling for tasks such as document understanding, code analysis, and multi-step reasoning. However, scaling context windows to the million-token level brings prohibitive computational and memory costs, limiting the practicality of long-context LLMs. In this work, we take a different perspective-visual context scaling-to tackle this challenge. Instead of extending token-based sequences, we propose Glyph, a framework that renders long texts into images and processes them with vision-language models (VLMs). This approach substantially compresses textual input while preserving semantic information, and we further design an LLM-driven genetic search to identify optimal visual rendering configurations for balancing accuracy and compression. Through extensive experiments, we demonstrate that our method achieves 3-4x token compression while maintaining accuracy comparable to leading LLMs such as Qwen3-8B on various long-context benchmarks. This compression also leads to around 4x faster prefilling and decoding, and approximately 2x faster SFT training. Furthermore, under extreme compression, a 128K-context VLM could scale to handle 1M-token-level text tasks. In addition, the rendered text data benefits real-world multimodal tasks, such as document understanding. Our code and model are released at https://github.com/thu-coai/Glyph.",
        "translated": "大语言模型（LLMs）在文档理解、代码分析和多步骤推理等任务中日益依赖于长上下文建模。然而，将上下文窗口扩展到百万级 token 的水平会带来高昂的计算和内存成本，从而限制了长上下文 LLM 的实用性。在本工作中，我们从视觉上下文扩展（visual context scaling）的角度出发，提出了一种不同的方法来应对这一挑战。我们没有扩展基于 token 的序列，而是提出了 Glyph 框架，该框架将长文本渲染为图像，并通过视觉-语言模型（VLMs）进行处理。该方法在保持语义信息的同时显著压缩了文本输入，我们进一步设计了一个由 LLM 驱动的遗传搜索算法，用于寻找在准确性和压缩率之间达到平衡的最佳视觉渲染配置。通过广泛的实验，我们证明了我们的方法在保持与 Qwen3-8B 等领先 LLM 相当的准确性的同时，实现了 3-4 倍的 token 压缩。这种压缩还带来了预填充和解码速度约 4 倍的提升，以及 SFT 训练速度大约 2 倍的提升。此外，在极端压缩的情况下，一个 128K 上下文的 VLM 可以扩展以处理百万级 token 的文本任务。另外，渲染后的文本数据也有助于现实世界中的多模态任务，例如文档理解。我们的代码和模型已发布在 https://github.com/thu-coai/Glyph。",
        "translated_title": "Glyph: 通过视觉-文本压缩扩展上下文窗口",
        "label": [],
        "label_reason": "不属于low-level图像处理任务",
        "relevance_score": 1,
        "novelty_score": 6,
        "novelty_reason": "提出文本压缩新方法，但非图像复原/增强"
    },
    {
        "title": "UltraCUA: A Foundation Model for Computer Use Agents with Hybrid Action",
        "url": "http://arxiv.org/abs/2510.17790v1",
        "pub_date": "2025-10-20",
        "summary": "Multimodal agents for computer use rely exclusively on primitive actions (click, type, scroll) that require accurate visual grounding and lengthy execution chains, leading to cascading failures and performance bottlenecks. While other agents leverage rich programmatic interfaces (APIs, MCP servers, tools), computer-use agents (CUAs) remain isolated from these capabilities. We present UltraCUA, a foundation model that bridges this gap through hybrid action -- seamlessly integrating GUI primitives with high-level programmatic tool calls. To achieve this, our approach comprises four key components: (1) an automated pipeline that scales programmatic tools from software documentation, open-source repositories, and code generation; (2) a synthetic data engine producing over 17,000 verifiable tasks spanning real-world computer-use scenarios; (3) a large-scale high-quality hybrid action trajectory collection with both low-level GUI actions and high-level programmatic tool calls; and (4) a two-stage training pipeline combining supervised fine-tuning with online reinforcement learning, enabling strategic alternation between low-level and high-level actions. Experiments with our 7B and 32B models demonstrate substantial improvements over state-of-the-art agents. On OSWorld, UltraCUA models achieve an average 22% relative improvement over base models, while being 11% faster in terms of steps. Out-of-domain evaluation on WindowsAgentArena shows our model reaches 21.7% success rate, outperforming baselines trained on Windows data. The hybrid action mechanism proves critical, reducing error propagation while maintaining execution efficiency.",
        "translated": "当前依赖于基本操作（点击、输入、滚动）的多模态计算机使用代理存在视觉定位精度要求高和操作链条长的问题，导致级联失效和性能瓶颈。虽然其他代理能够利用丰富的编程接口（APIs、MCP 服务器、工具），但计算机使用代理（CUAs）仍未具备这些能力。我们提出了 UltraCUA，这是一种通过混合操作弥合这一差距的基础模型——无缝整合 GUI 原语与高层编程工具调用。为此，我们的方法包含四个关键组件：(1) 一个自动化流程，通过软件文档、开源仓库和代码生成扩展编程工具；(2) 一个合成数据引擎，生成超过 17,000 个可验证任务，覆盖现实中的计算机使用场景；(3) 一个大规模高质量的混合操作轨迹集，包含低层 GUI 操作和高层编程工具调用；以及 (4) 一个结合监督微调和在线强化学习的两阶段训练流程，支持低层与高层操作之间的策略切换。在我们的 7B 和 32B 模型上进行的实验表明，UltraCUA 显著优于最先进的代理。在 OSWorld 上，UltraCUA 模型平均比基线模型提升 22%，同时在操作步数上快 11%。在 WindowsAgentArena 上的跨领域评估中，我们的模型达到了 21.7% 的成功率，优于基于 Windows 数据训练的基线模型。混合操作机制被证明是关键，既减少了错误传播，又保持了执行效率。",
        "translated_title": "UltraCUA：一种具有混合动作能力的计算机使用代理基础模型",
        "label": [],
        "label_reason": "论文聚焦计算机代理任务，非图像像素级处理",
        "relevance_score": 1,
        "novelty_score": 6,
        "novelty_reason": "提出混合动作机制并构建任务数据，有一定创新"
    },
    {
        "title": "Botany-Bot: Digital Twin Monitoring of Occluded and Underleaf Plant\n  Structures with Gaussian Splats",
        "url": "http://arxiv.org/abs/2510.17783v1",
        "pub_date": "2025-10-20",
        "summary": "Commercial plant phenotyping systems using fixed cameras cannot perceive many plant details due to leaf occlusion. In this paper, we present Botany-Bot, a system for building detailed \"annotated digital twins\" of living plants using two stereo cameras, a digital turntable inside a lightbox, an industrial robot arm, and 3D segmentated Gaussian Splat models. We also present robot algorithms for manipulating leaves to take high-resolution indexable images of occluded details such as stem buds and the underside/topside of leaves. Results from experiments suggest that Botany-Bot can segment leaves with 90.8% accuracy, detect leaves with 86.2% accuracy, lift/push leaves with 77.9% accuracy, and take detailed overside/underside images with 77.3% accuracy. Code, videos, and datasets are available at https://berkeleyautomation.github.io/Botany-Bot/.",
        "translated": "商业植物表型分析系统使用固定摄像头难以因叶片遮挡而感知许多植物细节。在本文中，我们提出了 Botany-Bot，这是一个系统，使用两个立体摄像头、一个位于灯光箱中的数字转盘、一个工业机械臂和 3D 分割的 Gaussian Splat 模型，以构建植物的详细“带注释的数字孪生”。我们还提出了用于操控叶片的机器人算法，以获取遮挡细节（如茎芽、叶片上表面和下表面）的高分辨率可检索图像。实验结果表明，Botany-Bot 能够以 90.8% 的精度分割叶片，86.2% 的精度检测叶片，77.9% 的精度提起/压下叶片，并以 77.3% 的精度获取详细的上表面/下表面图像。代码、视频和数据集可在 https://berkeleyautomation.github.io/Botany-Bot/ 获取。",
        "translated_title": "Botany-Bot: 使用高斯点进行遮挡和叶下植物结构的数字孪生监测",
        "label": [
            "图像分割",
            "遥感图像复原"
        ],
        "label_reason": "涉及植物遮挡结构的3D分割与高分辨率图像采集，与low-level图像处理相关",
        "relevance_score": 6,
        "novelty_score": 7,
        "novelty_reason": "提出结合机械臂与Gaussian Splat模型的创新系统，用于植物数字孪生建模"
    },
    {
        "title": "SparseVILA: Decoupling Visual Sparsity for Efficient VLM Inference",
        "url": "http://arxiv.org/abs/2510.17777v1",
        "pub_date": "2025-10-20",
        "summary": "Vision Language Models (VLMs) have rapidly advanced in integrating visual and textual reasoning, powering applications across high-resolution image understanding, long-video analysis, and multi-turn conversation. However, their scalability remains limited by the growing number of visual tokens that dominate inference latency. We present SparseVILA, a new paradigm for efficient VLM inference that decouples visual sparsity across the prefilling and decoding stages. SparseVILA distributes sparsity across stages by pruning redundant visual tokens during prefill and retrieving only query-relevant tokens during decoding. This decoupled design matches leading prefill pruning methods while preserving multi-turn fidelity by retaining most of the visual cache so that query-aware tokens can be retrieved at each conversation round. Built on an AWQ-optimized inference pipeline, SparseVILA achieves up to 4.0 times faster prefilling, 2.5 times faster decoding, and an overall 2.6 times end-to-end speedup on long-context video tasks -- while improving accuracy on document-understanding and reasoning tasks. By decoupling query-agnostic pruning and query-aware retrieval, SparseVILA establishes a new direction for efficient multimodal inference, offering a training-free, architecture-agnostic framework for accelerating large VLMs without sacrificing capability.",
        "translated": "视觉语言模型（VLMs）在整合视觉与文本推理方面取得了快速进展，并推动了高分辨率图像理解、长视频分析和多轮对话等应用的发展。然而，其可扩展性受到日益增长的视觉token数量的限制，这些token在推理延迟中占据主导地位。我们提出了SparseVILA，这是一种新的高效VLM推理范式，通过在预填充（prefilling）和解码（decoding）阶段解耦视觉稀疏性，提高推理效率。SparseVILA在预填充阶段通过剪枝冗余的视觉token实现稀疏性，而在解码阶段仅检索与查询相关的token，从而将稀疏性分布在不同阶段。该解耦设计在保持多轮对话保真度的同时，与领先的预填充剪枝方法相匹配，通过保留大部分视觉缓存以在每一轮对话中检索查询感知的token。基于AWQ优化的推理管道，SparseVILA在长上下文视频任务中实现了最高4.0倍的预填充加速、2.5倍的解码加速以及整体2.6倍的端到端加速，同时在文档理解和推理任务中提升了准确性。通过将查询无关的剪枝与查询感知的检索解耦，SparseVILA为高效多模态推理开辟了新的方向，提供了一种无需训练、且适用于任意架构的框架，可在不牺牲模型能力的前提下加速大规模VLMs。",
        "translated_title": "SparseVILA：解耦视觉稀疏性以实现高效的视觉语言模型推理",
        "label": [],
        "label_reason": "论文聚焦于视觉语言模型推理效率，不涉及像素级图像质量恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出了一种新的推理范式，通过解耦视觉稀疏性提高效率。"
    },
    {
        "title": "On-the-Fly OVD Adaptation with FLAME: Few-shot Localization via Active\n  Marginal-Samples Exploration",
        "url": "http://arxiv.org/abs/2510.17670v1",
        "pub_date": "2025-10-20",
        "summary": "Open-vocabulary object detection (OVD) models offer remarkable flexibility by detecting objects from arbitrary text queries. However, their zero-shot performance in specialized domains like Remote Sensing (RS) is often compromised by the inherent ambiguity of natural language, limiting critical downstream applications. For instance, an OVD model may struggle to distinguish between fine-grained classes such as \"fishing boat\" and \"yacht\" since their embeddings are similar and often inseparable. This can hamper specific user goals, such as monitoring illegal fishing, by producing irrelevant detections. To address this, we propose a cascaded approach that couples the broad generalization of a large pre-trained OVD model with a lightweight few-shot classifier. Our method first employs the zero-shot model to generate high-recall object proposals. These proposals are then refined for high precision by a compact classifier trained in real-time on only a handful of user-annotated examples - drastically reducing the high costs of RS imagery annotation.The core of our framework is FLAME, a one-step active learning strategy that selects the most informative samples for training. FLAME identifies, on the fly, uncertain marginal candidates near the decision boundary using density estimation, followed by clustering to ensure sample diversity. This efficient sampling technique achieves high accuracy without costly full-model fine-tuning and enables instant adaptation, within less then a minute, which is significantly faster than state-of-the-art alternatives.Our method consistently surpasses state-of-the-art performance on RS benchmarks, establishing a practical and resource-efficient framework for adapting foundation models to specific user needs.",
        "translated": "开放词汇目标检测（OVD）模型通过从任意文本查询中检测目标，提供了显著的灵活性。然而，由于自然语言本身的歧义性，这些模型在遥感（RS）等专业领域中的零样本性能常常受到影响，从而限制了其关键的下游应用。例如，一个OVD模型可能难以区分“fishing boat”和“yacht”等细粒度类别，因为它们的嵌入表示相似且通常无法分离。这可能会妨碍特定用户目标，例如监测非法捕鱼行为，从而产生不相关的目标检测结果。为了解决这一问题，我们提出了一种级联方法，将大规模预训练OVD模型的广泛泛化能力与一个轻量级的小样本分类器相结合。我们的方法首先利用零样本模型生成高召回率的目标候选区域，然后通过一个紧凑的分类器对这些候选区域进行高精度的优化。该分类器仅使用少量用户标注的示例即可实时训练，从而大幅降低遥感图像标注的高昂成本。我们框架的核心是FLAME，一种一步式主动学习策略，用于选择最具信息量的样本进行训练。FLAME通过密度估计实时识别决策边界附近的不确定边际候选样本，随后进行聚类以确保样本的多样性。这种高效的采样技术在不进行代价高昂的完整模型微调的情况下实现了高精度，并能够在一分钟之内实现即时适应，显著快于最先进的替代方案。我们的方法在遥感基准测试中持续超越最先进的性能，建立了一个实用且资源高效的框架，用于将基础模型适配到特定用户需求。",
        "translated_title": "On-the-Fly OVD Adaptation with FLAME: Few-shot Localization via Active Marginal-Samples Exploration  \nFLAME：通过主动探索边缘样本实现快速变化的OVD适应与小样本定位",
        "label": [],
        "label_reason": "论文主要涉及开放词汇目标检测，与推荐系统无直接关系。",
        "relevance_score": 2,
        "novelty_score": 5,
        "novelty_reason": "提出了FLAME策略，但属于常规改进，未显著推动推荐领域发展。"
    },
    {
        "title": "OG-Rank: Learning to Rank Fast and Slow with Uncertainty and\n  Reward-Trend Guided Adaptive Exploration",
        "url": "http://arxiv.org/abs/2510.17614v1",
        "pub_date": "2025-10-20",
        "summary": "Clinicians need ranking systems that work in real time and still justify their choices. Motivated by the need for a low-latency, decoder-based reranker, we present OG-Rank, a single-decoder approach that pairs a pooled first-token scoring signal with an uncertainty-gated explanation step. The model scores all candidates in one pass and generates a brief, structured rationale only when the list is genuinely ambiguous, keeping latency predictable. Trained with a curriculum that concentrates effort on hard cases, OG-Rank delivers strong effectiveness on encounter-scoped order selection (fast path: Recall@1~0.45, nDCG@20~0.625) and improves further when the gate activates (Recall@1~0.56, nDCG@20~0.699 at a 45\\% gate rate), while compact backbones show similar gains under the same policy. Encoder baselines trail in both effectiveness and flexibility. The result is a practical recipe: rank fast by default and explain when it helps, a pattern that applies broadly to decision tasks where selective generation buys accuracy at acceptable cost. The single-policy design simplifies deployment and budget planning, and the curriculum principle (spend more on the hard cases, less on the easy ones) readily transfers beyond clinical order selection.",
        "translated": "医生需要能够在实时环境下运行并对其推荐结果进行解释的排序系统。受低延迟、基于解码器的重排器需求驱动，我们提出了 OG-Rank，这是一种单解码器方法，将汇聚后的第一个 token 评分信号与不确定性门控的解释步骤相结合。该模型在一次前向传播中对所有候选对象进行评分，并仅在列表确实存在歧义时才生成简短、结构化的解释，从而保持延迟的可预测性。OG-Rank 通过集中训练资源在困难样本上的课程学习方式进行训练，在以就诊为范围的医嘱选择任务中表现出色（快速路径：Recall@1 约为 0.45，nDCG@20 约为 0.625），当门控激活时效果进一步提升（在 45% 的门控率下，Recall@1 约为 0.56，nDCG@20 约为 0.699），而使用相同策略的小型主干模型也能获得类似的提升效果。基于编码器的方法在效果和灵活性方面均落后。结果提供了一种实用的方案：默认快速排序，仅在有必要时进行解释，这种模式广泛适用于那些选择性生成能够以可接受成本换取准确性的决策任务。单策略设计简化了部署与预算规划，而课程学习原则（在困难样本上投入更多，在简单样本上投入更少）也能够轻松迁移到临床医嘱选择以外的其他任务中。",
        "translated_title": "OG-Rank：基于不确定性和奖励趋势引导的自适应探索的“快与慢”排序学习",
        "label": [
            "重排（Re-ranking）",
            "推荐系统评估（Evaluation Metrics / Offline/Online Testing）"
        ],
        "label_reason": "论文提出一种重排方法，结合快速排序与生成解释，适用于推荐系统中的重排环节。",
        "relevance_score": 8,
        "novelty_score": 7,
        "novelty_reason": "提出基于不确定性和奖励趋势的自适应探索机制，改进重排效率与效果。"
    },
    {
        "title": "How role-play shapes relevance judgment in zero-shot LLM rankers",
        "url": "http://arxiv.org/abs/2510.17535v1",
        "pub_date": "2025-10-20",
        "summary": "Large Language Models (LLMs) have emerged as promising zero-shot rankers, but their performance is highly sensitive to prompt formulation. In particular, role-play prompts, where the model is assigned a functional role or identity, often give more robust and accurate relevance rankings. However, the mechanisms and diversity of role-play effects remain underexplored, limiting both effective use and interpretability. In this work, we systematically examine how role-play variations influence zero-shot LLM rankers. We employ causal intervention techniques from mechanistic interpretability to trace how role-play information shapes relevance judgments in LLMs. Our analysis reveals that (1) careful formulation of role descriptions have a large effect on the ranking quality of the LLM; (2) role-play signals are predominantly encoded in early layers and communicate with task instructions in middle layers, while receiving limited interaction with query or document representations. Specifically, we identify a group of attention heads that encode information critical for role-conditioned relevance. These findings not only shed light on the inner workings of role-play in LLM ranking but also offer guidance for designing more effective prompts in IR and beyond, pointing toward broader opportunities for leveraging role-play in zero-shot applications.",
        "translated": "大语言模型（LLMs）已经展现出作为有前景的零样本排序器的潜力，但其性能对提示语的构造高度敏感。特别是角色扮演提示语，其中模型被赋予某种功能角色或身份，往往能提供更为鲁棒且准确的相关性排序。然而，角色扮演机制及其效果的多样性仍缺乏深入研究，这限制了其在实际应用中的有效使用和可解释性。在本研究中，我们系统地考察了角色扮演的变化如何影响零样本LLM排序器的表现。我们采用来自机制可解释性领域的因果干预技术，追踪角色扮演信息如何在LLM中塑造相关性判断。我们的分析揭示了以下几点：（1）角色描述的精心构造对LLM的排序质量具有显著影响；（2）角色扮演信号主要编码在模型的早期层中，并与任务指令在中层层进行交互，而与查询或文档表示的交互则较为有限。具体而言，我们识别出一组注意力头，它们编码了对角色条件相关性至关重要的信息。这些发现不仅阐明了角色扮演在LLM排序中的内部运作机制，还为设计更有效的提示语在信息检索和其他领域提供了指导，表明在零样本应用中利用角色扮演具有更广泛的可能性。",
        "translated_title": "角色扮演如何塑造零样本大语言模型排序器的相关性判断",
        "label": [
            "LLM生成式推荐",
            "精排",
            "推荐系统评估"
        ],
        "label_reason": "论文探讨了LLM在零样本排序中的角色扮演机制，与生成式推荐和排序相关。",
        "relevance_score": 7,
        "novelty_score": 8,
        "novelty_reason": "提出通过角色扮演提升LLM排序性能，并使用因果干预分析其内部机制，具有创新性。"
    },
    {
        "title": "Towards Mixed-Modal Retrieval for Universal Retrieval-Augmented\n  Generation",
        "url": "http://arxiv.org/abs/2510.17354v1",
        "pub_date": "2025-10-20",
        "summary": "Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for enhancing large language models (LLMs) by retrieving relevant documents from an external corpus. However, existing RAG systems primarily focus on unimodal text documents, and often fall short in real-world scenarios where both queries and documents may contain mixed modalities (such as text and images). In this paper, we address the challenge of Universal Retrieval-Augmented Generation (URAG), which involves retrieving and reasoning over mixed-modal information to improve vision-language generation. To this end, we propose Nyx, a unified mixed-modal to mixed-modal retriever tailored for URAG scenarios. To mitigate the scarcity of realistic mixed-modal data, we introduce a four-stage automated pipeline for generation and filtering, leveraging web documents to construct NyxQA, a dataset comprising diverse mixed-modal question-answer pairs that better reflect real-world information needs. Building on this high-quality dataset, we adopt a two-stage training framework for Nyx: we first perform pre-training on NyxQA along with a variety of open-source retrieval datasets, followed by supervised fine-tuning using feedback from downstream vision-language models (VLMs) to align retrieval outputs with generative preferences. Experimental results demonstrate that Nyx not only performs competitively on standard text-only RAG benchmarks, but also excels in the more general and realistic URAG setting, significantly improving generation quality in vision-language tasks.",
        "translated": "检索增强生成（RAG）作为一种强有力的范式，通过从外部语料库中检索相关文档来增强大语言模型（LLM）的能力。然而，现有的RAG系统主要关注单模态文本文档，在现实场景中往往表现不足，尤其是在查询和文档可能包含混合模态信息（如文本和图像）的情况下。本文中，我们研究了通用检索增强生成（URAG）这一挑战，其目标是通过检索和推理混合模态信息来提升视觉-语言生成能力。为此，我们提出了Nyx，一种统一的混合模态到混合模态的检索器，专门针对URAG场景进行设计。为了缓解真实混合模态数据稀缺的问题，我们引入了一个包含四个阶段的自动化生成与过滤流水线，利用网络文档构建了NyxQA数据集，该数据集包含多样化的混合模态问答对，更准确地反映了现实中的信息需求。基于这一高质量数据集，我们为Nyx采用了两阶段训练框架：首先在NyxQA以及多个开源检索数据集上进行预训练，然后使用下游视觉-语言模型（VLM）的反馈进行监督微调，以使检索结果与生成偏好对齐。实验结果表明，Nyx不仅在标准的纯文本RAG基准上表现优异，而且在更通用和现实的URAG设置中也表现出色，显著提升了视觉-语言任务中的生成质量。",
        "translated_title": "迈向多模态召回的通用召回增强生成",
        "label": [
            "多模态推荐（Multimodal Recommendation）",
            "LLM生成式推荐（LLM-based Generative Recommendation）"
        ],
        "label_reason": "论文探讨多模态信息检索增强生成，适用于推荐系统中的生成式推荐场景。",
        "relevance_score": 6,
        "novelty_score": 8,
        "novelty_reason": "提出统一的多模态检索框架和数据生成方法，具有新颖性和实用性。"
    },
    {
        "title": "MemoryBench: A Benchmark for Memory and Continual Learning in LLM\n  Systems",
        "url": "http://arxiv.org/abs/2510.17281v1",
        "pub_date": "2025-10-20",
        "summary": "Scaling up data, parameters, and test-time computation has been the mainstream methods to improve LLM systems (LLMsys), but their upper bounds are almost reached due to the gradual depletion of high-quality data and marginal gains obtained from larger computational resource consumption. Inspired by the abilities of human and traditional AI systems in learning from practice, constructing memory and continual learning frameworks for LLMsys has become an important and popular research direction in recent literature. Yet, existing benchmarks for LLM memory often focus on evaluating the system on homogeneous reading comprehension tasks with long-form inputs rather than testing their abilities to learn from accumulated user feedback in service time. Therefore, we propose a user feedback simulation framework and a comprehensive benchmark covering multiple domains, languages, and types of tasks to evaluate the continual learning abilities of LLMsys. Experiments show that the effectiveness and efficiency of state-of-the-art baselines are far from satisfying, and we hope this benchmark could pave the way for future studies on LLM memory and optimization algorithms.",
        "translated": "扩大数据量、模型参数和测试时计算能力已成为提升大语言模型系统（LLMsys）的主流方法，但由于高质量数据的逐渐枯竭以及计算资源消耗所带来的收益递减，其性能上限几乎已达到。受到人类及传统人工智能系统通过实践学习能力的启发，为LLMsys构建记忆机制和持续学习框架已成为近期研究中的一个重要且热门的研究方向。然而，现有的LLM记忆基准往往集中在评估系统在长文本输入下的同质化阅读理解任务上，而非测试其在服务时间内从累积的用户反馈中进行学习的能力。因此，我们提出一个用户反馈模拟框架以及一个涵盖多个领域、语言和任务类型的综合基准，用于评估LLMsys的持续学习能力。实验表明，最先进的基线方法在有效性和效率方面远未达到令人满意的效果，我们希望这一基准能够为未来关于LLM记忆机制和优化算法的研究铺平道路。",
        "translated_title": "MemoryBench: 一个面向大语言模型系统中记忆与持续学习的基准测试",
        "label": [
            "LLM生成式推荐"
        ],
        "label_reason": "论文涉及LLM系统的持续学习和记忆能力，可能适用于生成式推荐。",
        "relevance_score": 4,
        "novelty_score": 7,
        "novelty_reason": "提出新的基准和用户反馈模拟框架，具有一定的创新性。"
    },
    {
        "title": "On Efficiency-Effectiveness Trade-off of Diffusion-based Recommenders",
        "url": "http://arxiv.org/abs/2510.17245v1",
        "pub_date": "2025-10-20",
        "summary": "Diffusion models have emerged as a powerful paradigm for generative sequential recommendation, which typically generate next items to recommend guided by user interaction histories with a multi-step denoising process. However, the multi-step process relies on discrete approximations, introducing discretization error that creates a trade-off between computational efficiency and recommendation effectiveness. To address this trade-off, we propose TA-Rec, a two-stage framework that achieves one-step generation by smoothing the denoising function during pretraining while alleviating trajectory deviation by aligning with user preferences during fine-tuning. Specifically, to improve the efficiency without sacrificing the recommendation performance, TA-Rec pretrains the denoising model with Temporal Consistency Regularization (TCR), enforcing the consistency between the denoising results across adjacent steps. Thus, we can smooth the denoising function to map the noise as oracle items in one step with bounded error. To further enhance effectiveness, TA-Rec introduces Adaptive Preference Alignment (APA) that aligns the denoising process with user preference adaptively based on preference pair similarity and timesteps. Extensive experiments prove that TA-Rec's two-stage objective effectively mitigates the discretization errors-induced trade-off, enhancing both efficiency and effectiveness of diffusion-based recommenders.",
        "translated": "扩散模型已成为生成式序列推荐领域的一个强大范式，通常通过用户交互历史，以多步骤去噪过程生成下一个推荐的物料。然而，该多步骤过程依赖于离散近似，从而引入了离散化误差，造成了计算效率与推荐效果之间的权衡问题。为了解决这一权衡问题，我们提出了 TA-Rec，一个两阶段框架，通过在预训练阶段平滑去噪函数实现一步生成，同时在微调阶段通过与用户偏好对齐，缓解轨迹偏移问题。具体而言，为了在不牺牲推荐性能的前提下提高效率，TA-Rec 使用时间一致性正则化（Temporal Consistency Regularization, TCR）对去噪模型进行预训练，强制相邻步骤之间的去噪结果保持一致性。因此，我们可以平滑去噪函数，从而在一步内将噪声映射为理想物料，且误差被控制在一定范围内。为进一步增强推荐效果，TA-Rec 引入了自适应偏好对齐（Adaptive Preference Alignment, APA），根据偏好对相似性及时刻自适应地将去噪过程与用户偏好对齐。大量实验表明，TA-Rec 的两阶段目标有效地缓解了由离散化误差引起的权衡问题，提升了基于扩散模型的推荐系统的效率和效果。",
        "translated_title": "基于扩散的推荐系统的效率与效果权衡",
        "label": [
            "LLM生成式推荐",
            "序列推荐"
        ],
        "label_reason": "论文研究基于扩散模型的生成式序列推荐，与推荐系统核心问题直接相关。",
        "relevance_score": 8,
        "novelty_score": 7,
        "novelty_reason": "提出TA-Rec框架，通过平滑去噪函数和自适应偏好对齐缓解效率-效果权衡。"
    },
    {
        "title": "DSEBench: A Test Collection for Explainable Dataset Search with Examples",
        "url": "http://arxiv.org/abs/2510.17228v1",
        "pub_date": "2025-10-20",
        "summary": "Dataset search has been an established information retrieval task. Current paradigms either retrieve datasets that are relevant to a keyword query or find datasets that are similar to an input target dataset. To allow for their combined specification of information needs, in this article, we investigate the more generalized task of Dataset Search with Examples (DSE) and further extend it to Explainable DSE that requires identifying the metadata and content fields of a dataset that indicate its relevance to the query and similarity to the target datasets. To facilitate this research, we construct DSEBench, a test collection that provides high-quality dataset- and field-level annotations to enable the evaluation of explainable DSE. We also employ a large language model to generate numerous annotations to be used for training. We establish extensive baselines on DSEBench by adapting and evaluating a variety of sparse, dense, and LLM-based retrieval, reranking, and explanation methods.",
        "translated": "数据集检索已成为一项成熟的信息检索任务。当前的方法要么检索与关键字查询相关的数据集，要么查找与输入目标数据集相似的数据集。为了能够综合表达用户的信息需求，本文我们研究了更通用的数据集检索与示例任务（Dataset Search with Examples, DSE），并进一步将其扩展为可解释的DSE，该任务要求识别数据集中指示其与查询相关性以及与目标数据集相似性的元数据和内容字段。为推动该领域的研究，我们构建了DSEBench测试集合，提供高质量的数据集级别和字段级别的标注，以支持可解释DSE的评估。我们还利用大语言模型生成大量标注用于训练。我们在DSEBench上建立了广泛的基线，通过调整和评估多种稀疏、稠密以及基于大语言模型的召回、重排和解释方法。",
        "translated_title": "DSEBench：一个包含示例的可解释数据集检索测试集",
        "label": [
            "推荐系统评估",
            "解释性"
        ],
        "label_reason": "论文涉及可解释数据集检索，与推荐系统评估有一定关联",
        "relevance_score": 5,
        "novelty_score": 7,
        "novelty_reason": "提出可解释数据集搜索新任务并构建高质量基准"
    },
    {
        "title": "Rethinking On-policy Optimization for Query Augmentation",
        "url": "http://arxiv.org/abs/2510.17139v1",
        "pub_date": "2025-10-20",
        "summary": "Recent advances in large language models (LLMs) have led to a surge of interest in query augmentation for information retrieval (IR). Two main approaches have emerged. The first prompts LLMs to generate answers or pseudo-documents that serve as new queries, relying purely on the model's parametric knowledge or contextual information. The second applies reinforcement learning (RL) to fine-tune LLMs for query rewriting, directly optimizing retrieval metrics. While having respective advantages and limitations, the two approaches have not been compared under consistent experimental conditions. In this work, we present the first systematic comparison of prompting-based and RL-based query augmentation across diverse benchmarks, including evidence-seeking, ad hoc, and tool retrieval. Our key finding is that simple, training-free query augmentation often performs on par with, or even surpasses, more expensive RL-based counterparts, especially when using powerful LLMs. Motivated by this discovery, we introduce a novel hybrid method, On-policy Pseudo-document Query Expansion (OPQE), which, instead of rewriting a query, the LLM policy learns to generate a pseudo-document that maximizes retrieval performance, thus merging the flexibility and generative structure of prompting with the targeted optimization of RL. We show OPQE outperforms both standalone prompting and RL-based rewriting, demonstrating that a synergistic approach yields the best results. Our implementation is made available to facilitate reproducibility.",
        "translated": "近年来，大语言模型（LLMs）的进展引发了信息检索（IR）领域对查询增强（query augmentation）的广泛关注。目前主要有两种方法。第一种方法通过提示LLM生成答案或伪文档作为新的查询，完全依赖模型的参数化知识或上下文信息。第二种方法则利用强化学习（RL）对LLMs进行微调以实现查询重写，直接优化检索指标。尽管两种方法各有优势和局限性，但它们尚未在一致的实验条件下进行比较。在本文中，我们首次在多个基准上对基于提示（prompting-based）和基于强化学习（RL-based）的查询增强方法进行了系统性比较，包括证据寻求、临时检索和工具检索等任务。我们的主要发现是，简单且无需训练的查询增强方法通常可以与，甚至超越，代价更高的基于强化学习的对应方法，尤其是在使用功能强大的LLMs时。受此发现的启发，我们提出了一种新颖的混合方法——基于策略的伪文档查询扩展（On-policy Pseudo-document Query Expansion，OPQE），该方法不依赖于查询重写，而是通过学习LLM策略生成伪文档以最大化检索性能，从而将提示方法的灵活性与生成结构和强化学习的目标优化结合起来。我们展示了OPQE优于独立的提示方法和基于强化学习的重写方法，表明协同的方法能够取得最佳效果。我们已开源实现代码以促进结果复现。",
        "translated_title": "Rethinking On-policy Optimization for Query Augmentation  \n重新思考基于策略的查询增强优化方法",
        "label": [
            "通用推荐技术",
            "LLM生成式推荐"
        ],
        "label_reason": "论文涉及LLM生成伪文档用于检索，与生成式推荐有一定关联。",
        "relevance_score": 6,
        "novelty_score": 7,
        "novelty_reason": "提出一种结合提示和强化学习的新型混合方法，具有一定创新性。"
    },
    {
        "title": "Towards Context-aware Reasoning-enhanced Generative Searching in\n  E-commerce",
        "url": "http://arxiv.org/abs/2510.16925v1",
        "pub_date": "2025-10-19",
        "summary": "Search-based recommendation is one of the most critical application scenarios in e-commerce platforms. Users' complex search contexts--such as spatiotemporal factors, historical interactions, and current query's information--constitute an essential part of their decision-making, reflecting implicit preferences that complement explicit query terms. Modeling such rich contextual signals and their intricate associations with candidate items remains a key challenge. Although numerous efforts have been devoted to building more effective search methods, existing approaches still show limitations in integrating contextual information, which hinders their ability to fully capture user intent.   To address these challenges, we propose a context-aware reasoning-enhanced generative search framework for better \\textbf{understanding the complicated context}. Specifically, the framework first unifies heterogeneous user and item contexts into textual representations or text-based semantic identifiers and aligns them. To overcome the lack of explicit reasoning trajectories, we introduce a self-evolving post-training paradigm that iteratively combines supervised fine-tuning and reinforcement learning to progressively enhance the model's reasoning capability. In addition, we identify potential biases in existing RL algorithms when applied to search scenarios and present a debiased variant of GRPO to improve ranking performance. Extensive experiments on search log data collected from a real-world e-commerce platform demonstrate that our approach achieves superior performance compared with strong baselines, validating its effectiveness for search-based recommendation.",
        "translated": "基于搜索的推荐是电子商务平台中最关键的应用场景之一。用户复杂的搜索上下文——例如时空因素、历史交互以及当前查询的信息——构成了其决策过程的重要组成部分，反映了对显式查询词的补充隐式偏好。如何建模这些丰富的上下文信号以及它们与候选物料之间复杂的关联关系，仍然是一个关键挑战。尽管已有大量研究致力于构建更有效的搜索方法，但现有方法在整合上下文信息方面仍存在局限，这限制了其全面捕捉用户意图的能力。\n\n为应对这些挑战，我们提出了一种面向搜索的上下文感知推理增强生成式框架，以更好地**理解复杂的上下文**。具体而言，该框架首先将异构的用户和物料上下文统一为文本表示或基于文本的语义标识符，并进行对齐。为了克服缺乏显式推理轨迹的问题，我们引入了一种自我演进的后训练范式，该范式通过迭代结合监督微调和强化学习，逐步增强模型的推理能力。此外，我们识别了现有强化学习算法在应用于搜索场景时可能存在的偏差问题，并提出了去偏的GRPO变体，以提升排序效果。在从真实电子商务平台收集的搜索日志数据上的大量实验表明，与强基线方法相比，我们的方法实现了更优越的性能，验证了其在基于搜索推荐中的有效性。",
        "translated_title": "面向电子商务中上下文感知与推理增强的生成式搜索",
        "label": [
            "LLM生成式推荐",
            "推荐系统评估",
            "序列推荐"
        ],
        "label_reason": "论文聚焦电商场景下的搜索推荐，结合上下文和生成式方法，具有较强相关性。",
        "relevance_score": 8,
        "novelty_score": 7,
        "novelty_reason": "提出结合上下文感知和推理增强的生成式搜索框架，具有一定创新性。"
    },
    {
        "title": "The Layout Is the Model: On Action-Item Coupling in Generative\n  Recommendation",
        "url": "http://arxiv.org/abs/2510.16804v1",
        "pub_date": "2025-10-19",
        "summary": "Generative Recommendation (GR) models treat a user's interaction history as a sequence to be autoregressively predicted. When both items and actions (e.g., watch time, purchase, comment) are modeled, the layout-the ordering and visibility of item/action tokens-critically determines what information the model can use and how it generalizes. We present a unified study of token layouts for GR grounded in first principles: (P1) maximize item/action signal in both input/output space, (P2) preserve the conditioning relationship \"action given item\" and (P3) no information leakage.   While interleaved layout (where item and action occupy separate tokens) naturally satisfies these principles, it also bloats sequence length with larger training/inference cost. On the non-interleaved front, we design a novel and effective approach, Lagged Action Conditioning (LAC), which appears strange on the surface but aligns well with the design principles to yield strong accuracy. Comprehensive experiments on public datasets and large-scale production logs evaluate different layout options and empirically verifies the design principles. Our proposed non-interleaved method, LAC, achieves competitive or superior quality at substantially lower FLOPs than interleaving. Our findings offer actionable guidance for assembling GR systems that are both accurate and efficient.",
        "translated": "生成式推荐（GR）模型将用户的历史交互视为一个需要自回归预测的序列。当同时建模物料和行为（例如观看时长、购买、评论）时，布局——即物料/行为标记的顺序和可见性——在很大程度上决定了模型可以使用的哪些信息以及其泛化能力。我们基于第一性原理对GR的标记布局进行了统一研究：（P1）在输入/输出空间中最大化物料/行为的信号，（P2）保持“在给定物料下预测行为”的条件关系，以及（P3）防止信息泄露。尽管交错布局（物料和行为分别占据不同的标记）自然地满足了这些原理，但它也会增加序列长度并带来更高的训练和推理成本。在非交错布局方面，我们设计了一种新颖且有效的方法——延迟行为条件建模（LAC），这种方法表面上看起来有些奇怪，但其与设计原理高度一致，从而实现了较强的准确性。我们在公开数据集和大规模生产日志上进行了全面的实验，评估了不同的布局选项，并实证验证了这些设计原理。我们提出的非交错方法LAC在计算量（FLOPs）显著低于交错布局的情况下，实现了具有竞争力甚至更优的推荐质量。我们的研究结果为构建既准确又高效的GR系统提供了切实可行的指导。",
        "translated_title": "“布局即模型”：生成式推荐中的动作-物料耦合问题",
        "label": [
            "LLM生成式推荐",
            "序列推荐"
        ],
        "label_reason": "论文聚焦生成式推荐中的序列建模与token布局设计，对推荐系统有直接应用价值。",
        "relevance_score": 8,
        "novelty_score": 7,
        "novelty_reason": "提出新颖的非交错布局方法LAC，在生成式推荐中展现强效且低计算成本。"
    },
    {
        "title": "An Efficient Framework for Whole-Page Reranking via Single-Modal\n  Supervision",
        "url": "http://arxiv.org/abs/2510.16803v1",
        "pub_date": "2025-10-19",
        "summary": "The whole-page reranking plays a critical role in shaping the user experience of search engines, which integrates retrieval results from multiple modalities, such as documents, images, videos, and LLM outputs. Existing methods mainly rely on large-scale human-annotated data, which is costly to obtain and time-consuming. This is because whole-page annotation is far more complex than single-modal: it requires assessing the entire result page while accounting for cross-modal relevance differences. Thus, how to improve whole-page reranking performance while reducing annotation costs is still a key challenge in optimizing search engine result pages(SERP). In this paper, we propose SMAR, a novel whole-page reranking framework that leverages strong Single-modal rankers to guide Modal-wise relevance Alignment for effective Reranking, using only limited whole-page annotation to outperform fully-annotated reranking models. Specifically, high-quality single-modal rankers are first trained on data specific to their respective modalities. Then, for each query, we select a subset of their outputs to construct candidate pages and perform human annotation at the page level. Finally, we train the whole-page reranker using these limited annotations and enforcing consistency with single-modal preferences to maintain ranking quality within each modality. Experiments on the Qilin and Baidu datasets demonstrate that SMAR reduces annotation costs by about 70-90\\% while achieving significant ranking improvements compared to baselines. Further offline and online A/B testing on Baidu APPs also shows notable gains in standard ranking metrics as well as user experience indicators, fully validating the effectiveness and practical value of our approach in real-world search scenarios.",
        "translated": "整页重排在塑造搜索引擎用户体验方面起着关键作用，它将来自多种模态的检索结果（如文档、图像、视频和大语言模型输出）进行整合。现有方法主要依赖于大规模的人工标注数据，这在获取成本和耗时方面都较高。这是因为整页标注远比单模态标注复杂：它需要评估整个结果页面，同时考虑跨模态的相关性差异。因此，如何在降低标注成本的同时提升整页重排性能，仍然是优化搜索引擎结果页面（SERP）中的关键挑战。在本文中，我们提出了SMAR，一种新颖的整页重排框架，该框架利用强大的单模态排序器来引导模态间的相关性对齐以实现有效的重排，仅使用有限的整页标注即可超越完全标注的重排模型。具体而言，首先在各自模态的专用数据上训练高质量的单模态排序器。然后，对于每个查询，我们选择其输出的子集以构建候选页面，并在页面层面进行人工标注。最后，我们使用这些有限的标注数据训练整页重排模型，并通过强制保持与单模态偏好的一致性，以维持各模态内的排序质量。在Qilin和Baidu数据集上的实验表明，SMAR在实现显著排序性能提升的同时，可将标注成本降低约70-90\\%。进一步在百度APP上的离线和在线A/B测试也显示了在标准排序指标和用户体验指标上的显著提升，充分验证了我们的方法在实际搜索引擎场景中的有效性和实用价值。",
        "translated_title": "通过单模态监督实现高效的整页重排框架",
        "label": [
            "重排（Re-ranking）",
            "推荐系统评估（Evaluation Metrics / Offline/Online Testing）"
        ],
        "label_reason": "论文聚焦于搜索结果页重排，与推荐系统中的重排环节相关。",
        "relevance_score": 7,
        "novelty_score": 8,
        "novelty_reason": "提出通过单模态排序器引导多模态对齐的重排框架，方法新颖且有效。"
    },
    {
        "title": "Exact Nearest-Neighbor Search on Energy-Efficient FPGA Devices",
        "url": "http://arxiv.org/abs/2510.16736v1",
        "pub_date": "2025-10-19",
        "summary": "This paper investigates the usage of FPGA devices for energy-efficient exact kNN search in high-dimension latent spaces. This work intercepts a relevant trend that tries to support the increasing popularity of learned representations based on neural encoder models by making their large-scale adoption greener and more inclusive. The paper proposes two different energy-efficient solutions adopting the same FPGA low-level configuration. The first solution maximizes system throughput by processing the queries of a batch in parallel over a streamed dataset not fitting into the FPGA memory. The second minimizes latency by processing each kNN incoming query in parallel over an in-memory dataset. Reproducible experiments on publicly available image and text datasets show that our solution outperforms state-of-the-art CPU-based competitors regarding throughput, latency, and energy consumption. Specifically, experiments show that the proposed FPGA solutions achieve the best throughput in terms of queries per second and the best-observed latency with scale-up factors of up to 16.6X. Similar considerations can be made regarding energy efficiency, where results show that our solutions can achieve up to 11.9X energy saving w.r.t. strong CPU-based competitors.",
        "translated": "本文研究了使用FPGA设备在高维隐空间中进行节能精确kNN搜索的应用。这项工作抓住了一个相关趋势，即通过使基于神经编码器模型的表示学习的大规模采用更加绿色和包容，以支持其日益增长的流行。论文提出了两种不同的节能解决方案，它们采用相同的FPGA底层配置。第一种解决方案通过在一个无法完全加载到FPGA内存中的流式数据集上并行处理一批查询，以最大化系统吞吐量。第二种解决方案则通过在一个内存中的数据集上并行处理每个传入的kNN查询，以最小化延迟。在公开可用的图像和文本数据集上进行的可复现实验表明，我们的解决方案在吞吐量、延迟和能耗方面均优于当前最先进的基于CPU的竞争对手。具体而言，实验表明，所提出的FPGA解决方案在每秒查询数方面实现了最佳吞吐量，并观察到在扩展因子高达16.6倍的情况下，具有最低的延迟。关于能效方面也有类似的结论，结果表明，与强大的基于CPU的解决方案相比，我们的解决方案最多可实现11.9倍的能耗节省。",
        "translated_title": "节能FPGA设备上的精确最近邻搜索",
        "label": [],
        "label_reason": "论文聚焦于FPGA上的kNN搜索优化，与推荐系统无直接关联。",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出两种FPGA节能方案，实验设计完整，但属于通用计算优化领域。"
    },
    {
        "title": "Right Answer at the Right Time - Temporal Retrieval-Augmented Generation\n  via Graph Summarization",
        "url": "http://arxiv.org/abs/2510.16715v1",
        "pub_date": "2025-10-19",
        "summary": "Question answering in temporal knowledge graphs requires retrieval that is both time-consistent and efficient. Existing RAG methods are largely semantic and typically neglect explicit temporal constraints, which leads to time-inconsistent answers and inflated token usage. We propose STAR-RAG, a temporal GraphRAG framework that relies on two key ideas: building a time-aligned rule graph and conducting propagation on this graph to narrow the search space and prioritize semantically relevant, time-consistent evidence. This design enforces temporal proximity during retrieval, reduces the candidate set of retrieval results, and lowers token consumption without sacrificing accuracy. Compared with existing temporal RAG approaches, STAR-RAG eliminates the need for heavy model training and fine-tuning, thereby reducing computational cost and significantly simplifying deployment.Extensive experiments on real-world temporal KG datasets show that our method achieves improved answer accuracy while consuming fewer tokens than strong GraphRAG baselines.",
        "translated": "时序知识图谱中的问答任务需要既具备时间一致性又高效的数据召回能力。现有的 RAG 方法大多基于语义，通常忽视了显式的时间约束，这导致了时间不一致的回答和较高的 token 使用量。我们提出了 STAR-RAG，一种时序 GraphRAG 框架，其依赖于两个核心思想：构建一个时间对齐的规则图，并在此图上进行传播以缩小搜索空间并优先考虑语义相关且时间一致的证据。这种设计在召回过程中强制执行时间邻近性，减少了召回结果的候选集合，并在不牺牲准确性的情况下降低了 token 消耗。与现有的时序 RAG 方法相比，STAR-RAG 消除了对繁重模型训练和微调的需求，从而降低了计算成本并大大简化了部署。在真实世界的时序知识图谱数据集上的大量实验表明，我们的方法在保持较低 token 消耗的同时，实现了优于强大 GraphRAG 基线的回答准确性。",
        "translated_title": "Right Answer at the Right Time - Temporal Retrieval-Augmented Generation via Graph Summarization  \n在正确的时间给出正确的答案——基于图摘要的时间检索增强生成",
        "label": [
            "LLM生成式推荐",
            "多模态推荐"
        ],
        "label_reason": "涉及基于图的检索增强生成，适用于生成式推荐场景",
        "relevance_score": 7,
        "novelty_score": 8,
        "novelty_reason": "提出时间对齐规则图和传播机制，改进现有RAG方法"
    },
    {
        "title": "Resolution-Aware Retrieval Augmented Zero-Shot Forecasting",
        "url": "http://arxiv.org/abs/2510.16695v1",
        "pub_date": "2025-10-19",
        "summary": "Zero-shot forecasting aims to predict outcomes for previously unseen conditions without direct historical data, posing a significant challenge for traditional forecasting methods. We introduce a Resolution-Aware Retrieval-Augmented Forecasting model that enhances predictive accuracy by leveraging spatial correlations and temporal frequency characteristics. By decomposing signals into different frequency components, our model employs resolution-aware retrieval, where lower-frequency components rely on broader spatial context, while higher-frequency components focus on local influences. This allows the model to dynamically retrieve relevant data and adapt to new locations with minimal historical context.   Applied to microclimate forecasting, our model significantly outperforms traditional forecasting methods, numerical weather prediction models, and modern foundation time series models, achieving 71% lower MSE than HRRR and 34% lower MSE than Chronos on the ERA5 dataset.   Our results highlight the effectiveness of retrieval-augmented and resolution-aware strategies, offering a scalable and data-efficient solution for zero-shot forecasting in microclimate modeling and beyond.",
        "translated": "零样本预测旨在在没有直接历史数据的情况下预测此前未见过的条件下的结果，这对传统预测方法提出了重大挑战。我们引入了一种分辨率感知的检索增强预测模型，该模型通过利用空间相关性和时间频率特征来提高预测准确性。通过将信号分解为不同的频率成分，我们的模型采用分辨率感知的检索策略，其中低频成分依赖于更广泛的空间上下文，而高频成分则关注局部影响。这使得模型能够动态检索相关数据，并在仅有少量历史信息的情况下适应新的位置。\n\n在微气候预测中的应用表明，我们的模型显著优于传统预测方法、数值天气预测模型以及现代基础时间序列模型，在ERA5数据集上，其均方误差（MSE）比HRRR低71%，比Chronos低34%。我们的结果突显了检索增强和分辨率感知策略的有效性，为微气候建模及其他领域的零样本预测提供了一种可扩展且数据高效的解决方案。",
        "translated_title": "分辨率感知的召回增强零样本预测",
        "label": [
            "召回（Recall）",
            "通用推荐技术（General Recommendation Techniques）"
        ],
        "label_reason": "论文涉及基于检索的策略，可用于推荐系统的召回环节。",
        "relevance_score": 4,
        "novelty_score": 7,
        "novelty_reason": "提出分辨率感知的检索增强方法，对零样本预测有明显改进。"
    },
    {
        "title": "Safire: Similarity Framework for Visualization Retrieval",
        "url": "http://arxiv.org/abs/2510.16662v1",
        "pub_date": "2025-10-18",
        "summary": "Effective visualization retrieval necessitates a clear definition of similarity. Despite the growing body of work in specialized visualization retrieval systems, a systematic approach to understanding visualization similarity remains absent. We introduce the Similarity Framework for Visualization Retrieval (Safire), a conceptual model that frames visualization similarity along two dimensions: comparison criteria and representation modalities. Comparison criteria identify the aspects that make visualizations similar, which we divide into primary facets (data, visual encoding, interaction, style, metadata) and derived properties (data-centric and human-centric measures). Safire connects what to compare with how comparisons are executed through representation modalities. We categorize existing representation approaches into four groups based on their levels of information content and visualization determinism: raster image, vector image, specification, and natural language description, together guiding what is computable and comparable. We analyze several visualization retrieval systems using Safire to demonstrate its practical value in clarifying similarity considerations. Our findings reveal how particular criteria and modalities align across different use cases. Notably, the choice of representation modality is not only an implementation detail but also an important decision that shapes retrieval capabilities and limitations. Based on our analysis, we provide recommendations and discuss broader implications for multimodal learning, AI applications, and visualization reproducibility.",
        "translated": "有效的可视化召回需要对相似性有明确的定义。尽管在专用的可视化召回系统方面已有越来越多的研究，但在理解可视化相似性方面仍缺乏系统的方法。我们提出了可视化召回相似性框架（Safire），这是一个概念模型，从两个维度构建可视化相似性：比较标准和表示模态。比较标准识别使可视化之间产生相似性的方面，我们将其划分为基本维度（数据、视觉编码、交互、风格、元数据）和衍生属性（以数据为中心和以用户为中心的度量）。Safire通过表示模态将比较内容与比较方式联系起来。我们根据信息含量和可视化确定性的水平，将现有的表示方法分为四类：光栅图像、矢量图像、规范描述和自然语言描述，共同指导了哪些内容是可计算和可比较的。我们使用Safire分析了多个可视化召回系统，以展示其在澄清相似性考虑方面的实用价值。我们的发现揭示了特定的标准和模态在不同应用场景中的对齐方式。值得注意的是，表示模态的选择不仅是一个实现细节，也是一个重要的决策，它塑造了召回系统的能力和局限性。基于我们的分析，我们提供了建议，并讨论了其对多模态学习、人工智能应用和可视化可复现性的更广泛影响。",
        "translated_title": "Safire：面向可视化召回的相似度框架",
        "label": [
            "多模态推荐"
        ],
        "label_reason": "论文涉及多模态表征与相似度计算，但主要聚焦于可视化检索而非推荐系统",
        "relevance_score": 3,
        "novelty_score": 6,
        "novelty_reason": "提出系统性可视化相似度框架，对多模态表示有清晰分类"
    },
    {
        "title": "Prompt Optimization via Retrieved Reasoning Assets and Multi-Agent\n  Analysis",
        "url": "http://arxiv.org/abs/2510.16635v1",
        "pub_date": "2025-10-18",
        "summary": "Prompt optimization has emerged as an effective alternative to retraining for improving the performance of Large Language Models (LLMs). However, most existing approaches treat evaluation as a black box, relying solely on numerical scores while offering limited insight into why a prompt succeeds or fails. They also depend heavily on trial-and-error refinements, which are difficult to interpret and control. In this paper, we introduce MA-SAPO, a Multi-Agent framework for Score-Aware Prompt Optimization. Compared to prior methods, MA-SAPO explicitly couples evaluation outcomes with structured reasoning to guide systematic edits. The framework specifically consists of two stages: during the Reasoning Phase, agents collaboratively explain metric scores, diagnose weaknesses, and synthesize targeted refinements that are stored as reusable reasoning assets; during the Test Phase, agents retrieve these assets to analyze optimized prompts and apply only evidence-grounded edits. By turning evaluation signals into interpretable reasoning chains, MA-SAPO produces prompt refinements that are more transparent, auditable, and controllable. Experiments on the HelpSteer1/2 benchmarks demonstrate consistent improvements over single-pass prompting, retrieval-augmented baselines, and prior multi-agent strategies, validating the effectiveness of our approach.",
        "translated": "Prompt优化已逐渐成为改进大语言模型（LLM）性能、替代模型微调的有效方法。然而，大多数现有方法将评估视为黑箱，仅依赖数值评分，而对Prompt成功或失败的原因提供有限的洞察。它们也高度依赖于试错式的优化，难以解释和控制。在本文中，我们提出了MA-SAPO，一个多智能体框架，用于实现评分感知的Prompt优化。与之前的方法相比，MA-SAPO明确地将评估结果与结构化推理相结合，以指导系统性的Prompt修改。该框架具体包含两个阶段：在推理阶段，智能体协作解释评估指标、诊断模型弱点，并综合生成有针对性的优化策略，这些策略以可复用的推理资产形式存储；在测试阶段，智能体检索这些资产以分析优化后的Prompt，并仅应用基于证据的修改。通过将评估信号转化为可解释的推理链，MA-SAPO生成的Prompt优化策略更加透明、可审计且可控。我们在HelpSteer1/2基准上的实验表明，MA-SAPO在单次Prompt、检索增强基线以及之前多智能体策略的基础上取得了持续的性能提升，验证了我们方法的有效性。",
        "translated_title": "基于检索推理资产和多智能体分析的提示优化",
        "label": [],
        "label_reason": "论文聚焦于大模型提示优化，未直接涉及推荐系统技术。",
        "relevance_score": 3,
        "novelty_score": 7,
        "novelty_reason": "提出多智能体框架，改进提示优化方法，具有创新性。"
    },
    {
        "title": "FRONTIER-RevRec: A Large-scale Dataset for Reviewer Recommendation",
        "url": "http://arxiv.org/abs/2510.16597v1",
        "pub_date": "2025-10-18",
        "summary": "Reviewer recommendation is a critical task for enhancing the efficiency of academic publishing workflows. However, research in this area has been persistently hindered by the lack of high-quality benchmark datasets, which are often limited in scale, disciplinary scope, and comparative analyses of different methodologies. To address this gap, we introduce FRONTIER-RevRec, a large-scale dataset constructed from authentic peer review records (2007-2025) from the Frontiers open-access publishing platform https://www.frontiersin.org/. The dataset contains 177941 distinct reviewers and 478379 papers across 209 journals spanning multiple disciplines including clinical medicine, biology, psychology, engineering, and social sciences. Our comprehensive evaluation on this dataset reveals that content-based methods significantly outperform collaborative filtering. This finding is explained by our structural analysis, which uncovers fundamental differences between academic recommendation and commercial domains. Notably, approaches leveraging language models are particularly effective at capturing the semantic alignment between a paper's content and a reviewer's expertise. Furthermore, our experiments identify optimal aggregation strategies to enhance the recommendation pipeline. FRONTIER-RevRec is intended to serve as a comprehensive benchmark to advance research in reviewer recommendation and facilitate the development of more effective academic peer review systems. The FRONTIER-RevRec dataset is available at: https://anonymous.4open.science/r/FRONTIER-RevRec-5D05.",
        "translated": "评审推荐是提升学术出版流程效率的关键任务。然而，该领域的研究一直受到高质量基准数据集缺乏的阻碍，现有数据集通常在规模、学科覆盖范围以及不同方法的对比分析方面存在局限。为了解决这一问题，我们提出了 FRONTIER-RevRec，这是一个大规模数据集，由 Frontiers 开放获取出版平台（https://www.frontiersin.org/）提供的真实同行评审记录（2007-2025）构建而成。该数据集包含 177941 位不同的审稿人和 478379 篇论文，涵盖 209 个期刊，涉及临床医学、生物学、心理学、工程学和社会科学等多个学科。我们在该数据集上的综合评估表明，基于内容的方法显著优于协同过滤方法。这一发现通过我们的结构分析得到解释，该分析揭示了学术推荐与商业领域之间的基本差异。值得注意的是，利用语言模型的方法在捕捉论文内容与审稿人专业领域之间的语义一致性方面特别有效。此外，我们的实验识别出最优的聚合策略，以提升推荐流程的性能。FRONTIER-RevRec 旨在作为全面的基准数据集，推动评审推荐研究的发展，并促进更有效的学术同行评审系统的构建。FRONTIER-RevRec 数据集可通过以下链接获取：https://anonymous.4open.science/r/FRONTIER-RevRec-5D05。",
        "translated_title": "FRONTIER-RevRec：一个用于审稿人推荐的大型数据集",
        "label": [
            "通用推荐技术（General Recommendation Techniques）"
        ],
        "label_reason": "论文聚焦推荐系统中的评审人推荐，属于学术推荐的通用技术范畴。",
        "relevance_score": 7,
        "novelty_score": 6,
        "novelty_reason": "提出大规模数据集并分析方法性能，有一定实用价值但创新性有限。"
    },
    {
        "title": "Enhancing Channel Estimation in RIS-aided Systems via Observation Matrix\n  Design",
        "url": "http://arxiv.org/abs/2510.16576v1",
        "pub_date": "2025-10-18",
        "summary": "Reconfigurable intelligent surfaces (RISs) have emerged as a promising technology for enhancing wireless communications through dense antenna arrays. Accurate channel estimation is critical to unlocking their full performance potential. To enhance RIS channel estimators, this paper proposes a novel observation matrix design scheme. Bayesian optimization framework is adopted to generate observation matrices that maximize the mutual information between received pilot signals and RIS channels. To solve the formulated problem efficiently, we develop an alternating Riemannian manifold optimization (ARMO) algorithm to alternately update the receiver combiners and RIS phase-shift matrices. An adaptive kernel training strategy is further introduced to iteratively refine the channel covariance matrix without requiring additional pilot resources. Simulation results demonstrate that the proposed ARMO-enhanced estimator achieves substantial gains in estimation accuracy over state-of-the-art methods.",
        "translated": "可重构智能表面（RISs）作为一种有前景的技术，通过密集天线阵列来增强无线通信。准确的信道估计对于发挥其全部性能潜力至关重要。为提升RIS信道估计器的性能，本文提出了一种新颖的观测矩阵设计方案。采用贝叶斯优化框架生成观测矩阵，以最大化接收到的导频信号与RIS信道之间的互信息。为高效求解所构建的问题，我们开发了一种交替黎曼流形优化（ARMO）算法，用于交替更新接收端组合器和RIS相移矩阵。此外，引入了一种自适应核训练策略，可在不增加额外导频资源的情况下，迭代优化信道协方差矩阵。仿真结果表明，所提出的ARMO增强估计器在估计精度方面相比现有最先进方法有显著提升。",
        "translated_title": "通过观测矩阵设计提升RIS辅助系统中的信道估计",
        "label": [],
        "label_reason": "论文研究RIS信道估计，属于通信领域，与推荐系统无直接关联。",
        "relevance_score": 1,
        "novelty_score": 6,
        "novelty_reason": "提出了一种基于贝叶斯优化的观测矩阵设计方法，具有一定的技术新颖性。"
    },
    {
        "title": "Blending Learning to Rank and Dense Representations for Efficient and\n  Effective Cascades",
        "url": "http://arxiv.org/abs/2510.16393v1",
        "pub_date": "2025-10-18",
        "summary": "We investigate the exploitation of both lexical and neural relevance signals for ad-hoc passage retrieval. Our exploration involves a large-scale training dataset in which dense neural representations of MS-MARCO queries and passages are complemented and integrated with 253 hand-crafted lexical features extracted from the same corpus. Blending of the relevance signals from the two different groups of features is learned by a classical Learning-to-Rank (LTR) model based on a forest of decision trees. To evaluate our solution, we employ a pipelined architecture where a dense neural retriever serves as the first stage and performs a nearest-neighbor search over the neural representations of the documents. Our LTR model acts instead as the second stage that re-ranks the set of candidates retrieved by the first stage to enhance effectiveness. The results of reproducible experiments conducted with state-of-the-art dense retrievers on publicly available resources show that the proposed solution significantly enhances the end-to-end ranking performance while relatively minimally impacting efficiency. Specifically, we achieve a boost in nDCG@10 of up to 11% with an increase in average query latency of only 4.3%. This confirms the advantage of seamlessly combining two distinct families of signals that mutually contribute to retrieval effectiveness.",
        "translated": "我们研究了在即席段落召回中同时利用词汇相关性和神经相关性信号的问题。我们的探索涉及一个大规模训练数据集，其中MS-MARCO查询和段落的密集神经表示与从同一语料库中提取的253个手工构建的词汇特征相结合并融合。我们通过一个基于决策树森林的经典学习排序（LTR）模型，来学习融合这两组特征的相关性信号。为了评估我们的解决方案，我们采用了一种流水线架构，其中密集神经召回器作为第一阶段，通过对文档的神经表示进行最近邻搜索来完成初步召回。而我们的LTR模型则作为第二阶段，对第一阶段召回的候选集合进行重排以提升效果。在公开可用资源上使用最先进的密集召回器进行的可复现实验结果表明，所提出的方案在端到端排序性能上有显著提升，同时对效率的影响相对较小。具体而言，我们实现了nDCG@10提升高达11%，而平均查询延迟仅增加了4.3%。这证实了将两种不同类别的信号无缝融合以共同提升召回效果的优势。",
        "translated_title": "融合排序学习与密集表征以实现高效且有效的级联系统",
        "label": [
            "精排（Ranking）",
            "重排（Re-ranking）",
            "通用推荐技术（General Recommendation Techniques）"
        ],
        "label_reason": "论文研究了排序与密集表示的结合，适用于推荐系统中的重排与排序环节。",
        "relevance_score": 7,
        "novelty_score": 6,
        "novelty_reason": "将传统LTR与密集表示结合，有一定组合创新但非突破性方法。"
    },
    {
        "title": "Investigating the Association Between Text-Based Indications of\n  Foodborne Illness from Yelp Reviews and New York City Health Inspection\n  Outcomes (2023)",
        "url": "http://arxiv.org/abs/2510.16334v1",
        "pub_date": "2025-10-18",
        "summary": "Foodborne illnesses are gastrointestinal conditions caused by consuming contaminated food. Restaurants are critical venues to investigate outbreaks because they share sourcing, preparation, and distribution of foods. Public reporting of illness via formal channels is limited, whereas social media platforms host abundant user-generated content that can provide timely public health signals. This paper analyzes signals from Yelp reviews produced by a Hierarchical Sigmoid Attention Network (HSAN) classifier and compares them with official restaurant inspection outcomes issued by the New York City Department of Health and Mental Hygiene (NYC DOHMH) in 2023. We evaluate correlations at the Census tract level, compare distributions of HSAN scores by prevalence of C-graded restaurants, and map spatial patterns across NYC. We find minimal correlation between HSAN signals and inspection scores at the tract level and no significant differences by number of C-graded restaurants. We discuss implications and outline next steps toward address-level analyses.",
        "translated": "食源性疾病是由于摄入受污染的食物所引起的胃肠道疾病。餐厅是调查疫情爆发的关键场所，因为它们在食品的采购、制备和分发方面具有共通性。通过正式渠道对疾病的公开报告有限，而社交媒体平台则承载了大量用户生成的内容，这些内容可以提供及时的公共卫生信号。本文分析了来自Yelp评论中的信号，并采用层级sigmoid注意力网络（HSAN）分类器进行处理，同时将其与纽约市卫生与心理健康部（NYC DOHMH）于2023年发布的官方餐厅检查结果进行了比较。我们在普查区层面评估相关性，按C级餐厅的分布情况比较HSAN评分的分布，并绘制纽约市范围内的空间模式。我们在普查区层面发现HSAN信号与检查评分之间的相关性极小，并且C级餐厅数量在统计上也没有显著差异。我们讨论了这些发现的含义，并概述了朝向地址级别分析的下一步工作。",
        "translated_title": "调查Yelp评论中基于文本的食源性疾病迹象与纽约市健康检查结果之间的关联（2023）",
        "label": [],
        "label_reason": "论文聚焦公共卫生信号分析，与推荐系统无直接关联",
        "relevance_score": 2,
        "novelty_score": 5,
        "novelty_reason": "提出HSAN分类器分析用户评论信号，方法具有一定新颖性"
    },
    {
        "title": "Towards Explainable Skin Cancer Classification: A Dual-Network Attention\n  Model with Lesion Segmentation and Clinical Metadata Fusion",
        "url": "http://arxiv.org/abs/2510.17773v1",
        "pub_date": "2025-10-20",
        "summary": "Skin cancer is a life-threatening disease where early detection significantly improves patient outcomes. Automated diagnosis from dermoscopic images is challenging due to high intra-class variability and subtle inter-class differences. Many deep learning models operate as \"black boxes,\" limiting clinical trust. In this work, we propose a dual-encoder attention-based framework that leverages both segmented lesions and clinical metadata to enhance skin lesion classification in terms of both accuracy and interpretability. A novel Deep-UNet architecture with Dual Attention Gates (DAG) and Atrous Spatial Pyramid Pooling (ASPP) is first employed to segment lesions. The classification stage uses two DenseNet201 encoders-one on the original image and another on the segmented lesion whose features are fused via multi-head cross-attention. This dual-input design guides the model to focus on salient pathological regions. In addition, a transformer-based module incorporates patient metadata (age, sex, lesion site) into the prediction. We evaluate our approach on the HAM10000 dataset and the ISIC 2018 and 2019 challenges. The proposed method achieves state-of-the-art segmentation performance and significantly improves classification accuracy and average AUC compared to baseline models. To validate our model's reliability, we use Gradient-weighted Class Activation Mapping (Grad-CAM) to generate heatmaps. These visualizations confirm that our model's predictions are based on the lesion area, unlike models that rely on spurious background features. These results demonstrate that integrating precise lesion segmentation and clinical data with attention-based fusion leads to a more accurate and interpretable skin cancer classification model.",
        "translated": "皮肤癌是一种危及生命的疾病，早期发现可以显著改善患者的预后。由于皮肤病变类内差异大且类间差异细微，从皮肤镜图像中进行自动诊断具有挑战性。许多深度学习模型如同“黑箱”一样运行，限制了临床信任。在本研究中，我们提出了一种基于双编码器注意力的框架，该框架利用分割后的病变区域和临床元数据，以提升皮肤病变分类的准确性和可解释性。首先，我们采用一种新的 Deep-UNet 架构，结合双注意力门（Dual Attention Gates, DAG）和空洞空间金字塔池化（Atrous Spatial Pyramid Pooling, ASPP），用于病变分割。分类阶段使用两个 DenseNet201 编码器，一个处理原始图像，另一个处理分割后的病变区域，其特征通过多头交叉注意力进行融合。这种双输入设计引导模型关注关键的病理区域。此外，基于变压器的模块将患者的元数据（如年龄、性别、病变部位）融入预测过程中。我们在 HAM10000 数据集以及 ISIC 2018 和 2019 挑战中评估了我们的方法。与基线模型相比，所提方法在分割性能上达到了最先进水平，并显著提升了分类准确率和平均 AUC。为了验证我们模型的可靠性，我们使用梯度加权类激活映射（Gradient-weighted Class Activation Mapping, Grad-CAM）生成热图。这些可视化结果证实，我们的模型预测基于病变区域，而不同于依赖虚假背景特征的模型。这些结果表明，将精确的病变分割与临床数据结合，并通过注意力机制进行特征融合，能够构建出更准确且更可解释的皮肤癌分类模型。",
        "translated_title": "迈向可解释的皮肤癌分类：一种结合病变分割与临床元数据融合的双网络注意力模型",
        "label": [
            "图像分割"
        ],
        "label_reason": "论文涉及图像分割，但主要用于皮肤癌分类，属于high-level任务",
        "relevance_score": 2,
        "novelty_score": 5,
        "novelty_reason": "结合分割与临床元数据的分类方法，有一定融合创新"
    },
    {
        "title": "Seeing but Not Believing: Probing the Disconnect Between Visual\n  Attention and Answer Correctness in VLMs",
        "url": "http://arxiv.org/abs/2510.17771v1",
        "pub_date": "2025-10-20",
        "summary": "Vision-Language Models (VLMs) achieve strong results on multimodal tasks such as visual question answering, yet they can still fail even when the correct visual evidence is present. In this work, we systematically investigate whether these failures arise from not perceiving the evidence or from not leveraging it effectively. By examining layer-wise attention dynamics, we find that shallow layers focus primarily on text, while deeper layers sparsely but reliably attend to localized evidence regions. Surprisingly, VLMs often perceive the visual evidence when outputting incorrect answers, a phenomenon we term ``seeing but not believing'' that widely exists in major VLM families. Building on this, we introduce an inference-time intervention that highlights deep-layer evidence regions through selective attention-based masking. It requires no training and consistently improves accuracy across multiple families, including LLaVA, Qwen, Gemma, and InternVL. These results show that VLMs encode reliable evidence internally but under-utilize it, making such signals explicit can bridge the gap between perception and reasoning, advancing the diagnostic understanding and reliability of VLMs.",
        "translated": "视觉语言模型（VLMs）在视觉问答等多模态任务中取得了良好的效果，但即使在视觉证据正确存在的情况下，它们仍可能失败。在本工作中，我们系统性地研究了这些失败是由于未能感知到证据，还是未能有效地利用证据所导致。通过分析各层注意力的动态变化，我们发现浅层主要关注文本，而深层则稀疏但稳定地关注局部证据区域。令人惊讶的是，VLMs在输出错误答案时往往已经感知到了视觉证据，我们称这种现象为“看见但不信”，它在主要的VLM模型家族中广泛存在。基于此，我们引入了一种推理阶段的干预方法，通过选择性注意力掩码突出显示深层证据区域。该方法无需训练，即可在多个模型家族中（包括LLaVA、Qwen、Gemma和InternVL）一致提高准确率。这些结果表明，VLMs在内部编码了可靠的证据，但未能充分利用它，使得这些信号显式化可以在感知与推理之间建立桥梁，从而提升对VLMs的诊断理解和可靠性。",
        "translated_title": "看到但不信：探究视觉注意与视觉语言模型回答正确性之间的脱节",
        "label": [],
        "label_reason": "不属于low-level图像处理任务，研究VLM的注意力机制与答案正确性关系",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "提出新的推理时干预方法，揭示VLM内部感知与推理的脱节现象"
    },
    {
        "title": "VERA-V: Variational Inference Framework for Jailbreaking Vision-Language\n  Models",
        "url": "http://arxiv.org/abs/2510.17759v1",
        "pub_date": "2025-10-20",
        "summary": "Vision-Language Models (VLMs) extend large language models with visual reasoning, but their multimodal design also introduces new, underexplored vulnerabilities. Existing multimodal red-teaming methods largely rely on brittle templates, focus on single-attack settings, and expose only a narrow subset of vulnerabilities. To address these limitations, we introduce VERA-V, a variational inference framework that recasts multimodal jailbreak discovery as learning a joint posterior distribution over paired text-image prompts. This probabilistic view enables the generation of stealthy, coupled adversarial inputs that bypass model guardrails. We train a lightweight attacker to approximate the posterior, allowing efficient sampling of diverse jailbreaks and providing distributional insights into vulnerabilities. VERA-V further integrates three complementary strategies: (i) typography-based text prompts that embed harmful cues, (ii) diffusion-based image synthesis that introduces adversarial signals, and (iii) structured distractors to fragment VLM attention. Experiments on HarmBench and HADES benchmarks show that VERA-V consistently outperforms state-of-the-art baselines on both open-source and frontier VLMs, achieving up to 53.75% higher attack success rate (ASR) over the best baseline on GPT-4o.",
        "translated": "视觉-语言模型（VLMs）通过引入视觉推理能力扩展了大语言模型，但其多模态设计也带来了新的、尚未充分研究的漏洞。现有的多模态红队方法大多依赖脆弱的模板，专注于单次攻击场景，且仅揭示了有限的漏洞子集。为了解决这些问题，我们提出了 VERA-V，一个变分推理框架，将多模态越狱发现重新定义为学习文本-图像提示对的联合后验分布。这一概率视角使得生成隐蔽性强、耦合的对抗性输入成为可能，从而绕过模型的防护机制。我们训练了一个轻量级的攻击者来近似后验分布，实现高效采样多样化的越狱方式，并提供对漏洞分布的深入理解。VERA-V 进一步融合了三种互补策略：(i) 基于排版的文本提示，嵌入有害的提示信息；(ii) 基于扩散的图像合成，引入对抗性信号；(iii) 结构化干扰项，以分散 VLM 的注意力。在 HarmBench 和 HADES 基准上的实验表明，VERA-V 在开源和前沿 VLM 上均能持续优于最先进的基线方法，在 GPT-4o 上的攻击成功率（ASR）最高可比最佳基线高出 53.75%。",
        "translated_title": "VERA-V：用于破解视觉-语言模型的变分推断框架",
        "label": [],
        "label_reason": "论文研究视觉-语言模型的安全性攻击，不属于图像像素级处理任务。",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出变分推理框架用于生成对抗性输入，具有方法创新性。"
    },
    {
        "title": "Joint Multi-Condition Representation Modelling via Matrix Factorisation\n  for Visual Place Recognition",
        "url": "http://arxiv.org/abs/2510.17739v1",
        "pub_date": "2025-10-20",
        "summary": "We address multi-reference visual place recognition (VPR), where reference sets captured under varying conditions are used to improve localisation performance. While deep learning with large-scale training improves robustness, increasing data diversity and model complexity incur extensive computational cost during training and deployment. Descriptor-level fusion via voting or aggregation avoids training, but often targets multi-sensor setups or relies on heuristics with limited gains under appearance and viewpoint change. We propose a training-free, descriptor-agnostic approach that jointly models places using multiple reference descriptors via matrix decomposition into basis representations, enabling projection-based residual matching. We also introduce SotonMV, a structured benchmark for multi-viewpoint VPR. On multi-appearance data, our method improves Recall@1 by up to ~18% over single-reference and outperforms multi-reference baselines across appearance and viewpoint changes, with gains of ~5% on unstructured data, demonstrating strong generalisation while remaining lightweight.",
        "translated": "我们研究多参考图像视觉地点识别（multi-reference visual place recognition, VPR），其中在不同条件下采集的参考图像集被用于提升定位性能。尽管大规模训练下的深度学习方法能够增强鲁棒性，但增加数据多样性与模型复杂度会在训练与部署阶段带来较大的计算开销。基于投票或聚合的描述符级融合方法虽然无需训练，但通常针对多传感器设置，或依赖启发式策略，在外观和视角变化下提升有限。我们提出一种无需训练、与描述符无关的方法，通过矩阵分解将多个参考描述符共同建模为基向量表示，从而实现基于投影的残差匹配。我们还引入了 SotonMV，一个结构化的多视角 VPR 基准数据集。在多外观数据上，我们的方法在 Recall@1 指标上比单参考图像方法最多提升了 ~18%，并在外观和视角变化下优于多参考图像基线方法，在非结构化数据上实现了 ~5% 的性能提升，展现出较强的泛化能力，同时保持了轻量级特性。",
        "translated_title": "基于矩阵分解的联合多条件表示建模用于视觉地点识别",
        "label": [],
        "label_reason": "论文聚焦于视觉地点识别，属于 high-level 任务",
        "relevance_score": 2,
        "novelty_score": 5,
        "novelty_reason": "提出了一种训练无关的描述符融合方法，有一定新颖性"
    },
    {
        "title": "Can Image-To-Video Models Simulate Pedestrian Dynamics?",
        "url": "http://arxiv.org/abs/2510.17731v1",
        "pub_date": "2025-10-20",
        "summary": "Recent high-performing image-to-video (I2V) models based on variants of the diffusion transformer (DiT) have displayed remarkable inherent world-modeling capabilities by virtue of training on large scale video datasets. We investigate whether these models can generate realistic pedestrian movement patterns in crowded public scenes. Our framework conditions I2V models on keyframes extracted from pedestrian trajectory benchmarks, then evaluates their trajectory prediction performance using quantitative measures of pedestrian dynamics.",
        "translated": "基于扩散变换器（DiT）变体的最新高性能图像到视频（I2V）模型，由于在大规模视频数据集上进行训练，展现出显著的内在世界建模能力。我们研究这些模型是否能够在拥挤的公共场景中生成逼真的行人运动模式。我们的框架以从行人轨迹基准中提取的关键帧作为条件输入，驱动I2V模型生成视频，并通过行人动力学的定量指标评估其轨迹预测性能。",
        "translated_title": "图像到视频模型能否模拟行人动态？",
        "label": [],
        "label_reason": "论文不属于low-level图像处理任务",
        "relevance_score": 2,
        "novelty_score": 5,
        "novelty_reason": "方法基于现有扩散模型框架进行微调"
    },
    {
        "title": "Signature Forgery Detection: Improving Cross-Dataset Generalization",
        "url": "http://arxiv.org/abs/2510.17724v1",
        "pub_date": "2025-10-20",
        "summary": "Automated signature verification is a critical biometric technique used in banking, identity authentication, and legal documentation. Despite the notable progress achieved by deep learning methods, most approaches in offline signature verification still struggle to generalize across datasets, as variations in handwriting styles and acquisition protocols often degrade performance. This study investigates feature learning strategies for signature forgery detection, focusing on improving cross-dataset generalization -- that is, model robustness when trained on one dataset and tested on another. Using three public benchmarks -- CEDAR, ICDAR, and GPDS Synthetic -- two experimental pipelines were developed: one based on raw signature images and another employing a preprocessing method referred to as shell preprocessing. Several behavioral patterns were identified and analyzed; however, no definitive superiority between the two approaches was established. The results show that the raw-image model achieved higher performance across benchmarks, while the shell-based model demonstrated promising potential for future refinement toward robust, cross-domain signature verification.",
        "translated": "自动签名验证是银行、身份认证和法律文件中使用的关键生物识别技术。尽管深度学习方法已取得了显著进展，但大多数离线签名验证方法在不同数据集之间仍然难以泛化，因为书写风格和采集协议的变化常常导致性能下降。本研究探讨了用于签名伪造检测的特征学习策略，重点在于提升跨数据集的泛化能力——即模型在一个数据集上训练，在另一个数据集上测试时的鲁棒性。实验使用了三个公开基准——CEDAR、ICDAR 和 GPDS Synthetic，设计了两种实验流程：一种基于原始签名图像，另一种采用一种称为壳预处理（shell preprocessing）的预处理方法。我们识别并分析了若干行为模式，但尚未明确确立两种方法之间的绝对优势。结果表明，基于原始图像的模型在各基准测试中表现更优，而基于壳处理的模型在进一步优化后，表现出在鲁棒性和跨域签名验证方面具有良好的前景。",
        "translated_title": "签名伪造检测：提升跨数据集泛化能力",
        "label": [],
        "label_reason": "论文主题为签名伪造检测，属于高阶视觉任务",
        "relevance_score": 2,
        "novelty_score": 5,
        "novelty_reason": "探讨了跨数据集泛化策略，但未提出显著新方法"
    },
    {
        "title": "MT-Video-Bench: A Holistic Video Understanding Benchmark for Evaluating\n  Multimodal LLMs in Multi-Turn Dialogues",
        "url": "http://arxiv.org/abs/2510.17722v1",
        "pub_date": "2025-10-20",
        "summary": "The recent development of Multimodal Large Language Models (MLLMs) has significantly advanced AI's ability to understand visual modalities. However, existing evaluation benchmarks remain limited to single-turn question answering, overlooking the complexity of multi-turn dialogues in real-world scenarios. To bridge this gap, we introduce MT-Video-Bench, a holistic video understanding benchmark for evaluating MLLMs in multi-turn dialogues. Specifically, our MT-Video-Bench mainly assesses six core competencies that focus on perceptivity and interactivity, encompassing 987 meticulously curated multi-turn dialogues from diverse domains. These capabilities are rigorously aligned with real-world applications, such as interactive sports analysis and multi-turn video-based intelligent tutoring. With MT-Video-Bench, we extensively evaluate various state-of-the-art open-source and closed-source MLLMs, revealing their significant performance discrepancies and limitations in handling multi-turn video dialogues. The benchmark will be publicly available to foster future research.",
        "translated": "多模态大语言模型（MLLMs）的最新发展显著提升了人工智能理解视觉模态的能力。然而，现有的评估基准仍局限于单轮问答，忽视了现实场景中多轮对话的复杂性。为弥补这一差距，我们引入了 MT-Video-Bench，一个全面的视频理解基准，用于评估 MLLMs 在多轮对话中的表现。具体而言，我们的 MT-Video-Bench 主要评估六项核心能力，涵盖感知性和交互性，其中包括从多个领域精心挑选的 987 个多轮对话。这些能力与实际应用紧密对齐，例如交互式体育分析和基于视频的多轮智能教学。借助 MT-Video-Bench，我们对多种最先进的开源和闭源 MLLMs 进行了广泛评估，揭示了它们在处理多轮视频对话任务时显著的性能差异和局限性。该基准将向公众开放，以促进未来的研究。",
        "translated_title": "MT-Video-Bench：一种用于评估多轮对话中多模态大语言模型的综合视频理解基准",
        "label": [],
        "label_reason": "论文聚焦于视频理解的多轮对话评估，属于high-level任务。",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出了面向多轮对话的视频理解基准，有一定实用价值。"
    },
    {
        "title": "Raindrop GS: A Benchmark for 3D Gaussian Splatting under Raindrop\n  Conditions",
        "url": "http://arxiv.org/abs/2510.17719v1",
        "pub_date": "2025-10-20",
        "summary": "3D Gaussian Splatting (3DGS) under raindrop conditions suffers from severe occlusions and optical distortions caused by raindrop contamination on the camera lens, substantially degrading reconstruction quality. Existing benchmarks typically evaluate 3DGS using synthetic raindrop images with known camera poses (constrained images), assuming ideal conditions. However, in real-world scenarios, raindrops often interfere with accurate camera pose estimation and point cloud initialization. Moreover, a significant domain gap between synthetic and real raindrops further impairs generalization. To tackle these issues, we introduce RaindropGS, a comprehensive benchmark designed to evaluate the full 3DGS pipeline-from unconstrained, raindrop-corrupted images to clear 3DGS reconstructions. Specifically, the whole benchmark pipeline consists of three parts: data preparation, data processing, and raindrop-aware 3DGS evaluation, including types of raindrop interference, camera pose estimation and point cloud initialization, single image rain removal comparison, and 3D Gaussian training comparison. First, we collect a real-world raindrop reconstruction dataset, in which each scene contains three aligned image sets: raindrop-focused, background-focused, and rain-free ground truth, enabling a comprehensive evaluation of reconstruction quality under different focus conditions. Through comprehensive experiments and analyses, we reveal critical insights into the performance limitations of existing 3DGS methods on unconstrained raindrop images and the varying impact of different pipeline components: the impact of camera focus position on 3DGS reconstruction performance, and the interference caused by inaccurate pose and point cloud initialization on reconstruction. These insights establish clear directions for developing more robust 3DGS methods under raindrop conditions.",
        "translated": "在雨滴条件下，3D Gaussian Splatting（3DGS）会受到相机镜头上雨滴污染所引起的严重遮挡和光学畸变的影响，从而显著降低重建质量。现有的基准测试通常在已知相机姿态的合成雨滴图像（受限图像）上评估 3DGS，假设为理想条件。然而，在现实场景中，雨滴常常干扰精确的相机姿态估计和点云初始化。此外，合成雨滴与真实雨滴之间存在显著的域差异，进一步削弱了方法的泛化能力。为了解决这些问题，我们提出了 RaindropGS，一个全面的基准，用于评估完整的 3DGS 流程——从不受约束的、被雨滴污染的图像到清晰的 3DGS 重建。具体而言，整个基准流程包括三个部分：数据准备、数据处理和雨滴感知的 3DGS 评估，涵盖了雨滴干扰的类型、相机姿态估计与点云初始化、单图像去雨比较以及 3D Gaussian 训练比较。首先，我们收集了一个真实世界的雨滴重建数据集，其中每个场景包含三组对齐的图像：聚焦雨滴图像、聚焦背景图像和无雨的真实图像，从而能够在不同聚焦条件下全面评估重建质量。通过综合的实验与分析，我们揭示了当前 3DGS 方法在处理不受约束的雨滴图像时性能限制的关键洞察，并分析了不同流程组件对重建结果的影响：相机聚焦位置对 3DGS 重建性能的影响，以及姿态估计不准和点云初始化错误所引起的干扰。这些发现为开发在雨滴条件下更鲁棒的 3DGS 方法提供了明确的方向。",
        "translated_title": "雨滴GS：雨滴条件下3D高斯点绘制的基准测试",
        "label": [
            "图像去雨",
            "多帧/视频图像恢复",
            "图像恢复"
        ],
        "label_reason": "论文聚焦雨滴干扰下的3DGS重建，涉及图像去雨和恢复任务",
        "relevance_score": 7,
        "novelty_score": 8,
        "novelty_reason": "提出首个针对真实雨滴条件下的3DGS基准，包含新数据集和评估方法"
    },
    {
        "title": "Automatic Classification of Circulating Blood Cell Clusters based on\n  Multi-channel Flow Cytometry Imaging",
        "url": "http://arxiv.org/abs/2510.17716v1",
        "pub_date": "2025-10-20",
        "summary": "Circulating blood cell clusters (CCCs) containing red blood cells (RBCs), white blood cells(WBCs), and platelets are significant biomarkers linked to conditions like thrombosis, infection, and inflammation. Flow cytometry, paired with fluorescence staining, is commonly used to analyze these cell clusters, revealing cell morphology and protein profiles. While computational approaches based on machine learning have advanced the automatic analysis of single-cell flow cytometry images, there is a lack of effort to build tools to automatically analyze images containing CCCs. Unlike single cells, cell clusters often exhibit irregular shapes and sizes. In addition, these cell clusters often consist of heterogeneous cell types, which require multi-channel staining to identify the specific cell types within the clusters. This study introduces a new computational framework for analyzing CCC images and identifying cell types within clusters. Our framework uses a two-step analysis strategy. First, it categorizes images into cell cluster and non-cluster groups by fine-tuning the You Only Look Once(YOLOv11) model, which outperforms traditional convolutional neural networks (CNNs), Vision Transformers (ViT). Then, it identifies cell types by overlaying cluster contours with regions from multi-channel fluorescence stains, enhancing accuracy despite cell debris and staining artifacts. This approach achieved over 95% accuracy in both cluster classification and phenotype identification. In summary, our automated framework effectively analyzes CCC images from flow cytometry, leveraging both bright-field and fluorescence data. Initially tested on blood cells, it holds potential for broader applications, such as analyzing immune and tumor cell clusters, supporting cellular research across various diseases.",
        "translated": "含有红细胞（RBCs）、白细胞（WBCs）和血小板的循环血细胞簇（CCCs）是与血栓形成、感染和炎症等病症相关的重要生物标志物。流式细胞术配合荧光染色通常用于分析这些细胞簇，揭示细胞形态和蛋白表达谱。尽管基于机器学习的计算方法已推动了单细胞流式细胞术图像的自动分析，但尚缺乏针对包含细胞簇图像的自动分析工具。与单细胞不同，细胞簇通常具有不规则的形状和大小。此外，这些细胞簇往往由异质性的细胞类型组成，需要多通道染色以识别簇内的特定细胞类型。本研究提出了一种新的计算框架，用于分析细胞簇图像并识别簇内的细胞类型。我们的框架采用两步分析策略。首先，通过微调 You Only Look Once (YOLOv11) 模型，将图像分为细胞簇和非细胞簇组，该模型性能优于传统卷积神经网络（CNNs）和视觉Transformer（ViT）。然后，通过将簇轮廓叠加于多通道荧光染色区域，识别细胞类型，从而在细胞碎片和染色伪影存在的情况下提高准确性。该方法在细胞簇分类和表型识别方面均实现了超过95%的准确率。综上所述，我们的自动化框架有效分析了流式细胞术中的细胞簇图像，结合了明场和荧光数据。该方法最初在血液细胞上进行了测试，具有更广泛的应用潜力，例如分析免疫和肿瘤细胞簇，从而支持多种疾病的研究。",
        "translated_title": "基于多通道流式细胞术成像的循环血细胞簇自动分类",
        "label": [],
        "label_reason": "论文主要关注细胞分类，不涉及图像像素级恢复或增强",
        "relevance_score": 2,
        "novelty_score": 5,
        "novelty_reason": "方法基于现有模型改进，创新性有限"
    },
    {
        "title": "Improving Cross-Patient Generalization in Parkinson's Disease Detection\n  through Chunk-Based Analysis of Hand-Drawn Patterns",
        "url": "http://arxiv.org/abs/2510.17703v1",
        "pub_date": "2025-10-20",
        "summary": "Parkinson's disease (PD) is a neurodegenerative disease affecting about 1% of people over the age of 60, causing motor impairments that impede hand coordination activities such as writing and drawing. Many approaches have tried to support early detection of Parkinson's disease based on hand-drawn images; however, we identified two major limitations in the related works: (1) the lack of sufficient datasets, (2) the robustness when dealing with unseen patient data. In this paper, we propose a new approach to detect Parkinson's disease that consists of two stages: The first stage classifies based on their drawing type(circle, meander, spiral), and the second stage extracts the required features from the images and detects Parkinson's disease. We overcame the previous two limitations by applying a chunking strategy where we divide each image into 2x2 chunks. Each chunk is processed separately when extracting features and recognizing Parkinson's disease indicators. To make the final classification, an ensemble method is used to merge the decisions made from each chunk. Our evaluation shows that our proposed approach outperforms the top performing state-of-the-art approaches, in particular on unseen patients. On the NewHandPD dataset our approach, it achieved 97.08% accuracy for seen patients and 94.91% for unseen patients, our proposed approach maintained a gap of only 2.17 percentage points, compared to the 4.76-point drop observed in prior work.",
        "translated": "帕金森病（PD）是一种神经退行性疾病，影响约60岁以上人群的1%，会导致运动障碍，妨碍手部协调活动如书写和绘画。许多方法尝试基于手绘图像支持帕金森病的早期检测；然而，我们发现相关工作中存在两个主要限制：(1) 缺乏足够的数据集；(2) 在处理未见过的患者数据时的鲁棒性不足。在本文中，我们提出了一种新的帕金森病检测方法，该方法包含两个阶段：第一阶段基于绘画类型（圆形、来回线、螺旋）进行分类，第二阶段从图像中提取所需特征并检测帕金森病。我们通过应用一种块处理策略克服了上述两个限制，即将每张图像划分为2x2的块。在提取特征和识别帕金森病指标时，每个块被单独处理。为做出最终分类，使用了一种集成方法将每个块的决策结果进行融合。我们的评估表明，所提出的检测方法优于当前最先进的方法，特别是在未见过的患者数据上表现突出。在NewHandPD数据集上，我们的方法对已见患者的准确率达到97.08%，对未见患者达到94.91%；相比先前工作中观察到的4.76个百分点的下降，我们的方法仅存在2.17个百分点的差距。",
        "translated_title": "通过基于块分析手绘图案提高帕金森病检测中的跨患者泛化能力",
        "label": [],
        "label_reason": "论文主要关注帕金森病检测，属于high-level任务",
        "relevance_score": 2,
        "novelty_score": 5,
        "novelty_reason": "提出分块策略与集成方法，但创新性有限"
    },
    {
        "title": "Elastic ViTs from Pretrained Models without Retraining",
        "url": "http://arxiv.org/abs/2510.17700v1",
        "pub_date": "2025-10-20",
        "summary": "Vision foundation models achieve remarkable performance but are only available in a limited set of pre-determined sizes, forcing sub-optimal deployment choices under real-world constraints. We introduce SnapViT: Single-shot network approximation for pruned Vision Transformers, a new post-pretraining structured pruning method that enables elastic inference across a continuum of compute budgets. Our approach efficiently combines gradient information with cross-network structure correlations, approximated via an evolutionary algorithm, does not require labeled data, generalizes to models without a classification head, and is retraining-free. Experiments on DINO, SigLIPv2, DeIT, and AugReg models demonstrate superior performance over state-of-the-art methods across various sparsities, requiring less than five minutes on a single A100 GPU to generate elastic models that can be adjusted to any computational budget. Our key contributions include an efficient pruning strategy for pretrained Vision Transformers, a novel evolutionary approximation of Hessian off-diagonal structures, and a self-supervised importance scoring mechanism that maintains strong performance without requiring retraining or labels. Code and pruned models are available at: https://elastic.ashita.nl/",
        "translated": "视觉基础模型在性能上取得了显著成果，但它们仅以有限的一组预设尺寸提供，这在现实约束条件下迫使用户做出次优的部署选择。我们提出 SnapViT：剪枝视觉变换器的单次网络近似方法，这是一种新的预训练后结构化剪枝方法，能够在一系列不同的计算预算之间实现弹性推理。我们的方法高效地结合了梯度信息与跨网络结构的相关性，通过进化算法进行近似，无需标注数据，适用于没有分类头的模型，并且无需重新训练。在 DINO、SigLIPv2、DeIT 和 AugReg 模型上的实验表明，我们的方法在各种稀疏度下均优于当前最先进的方法，且在单个 A100 GPU 上即可在不到五分钟的时间内生成可适应任意计算预算的弹性模型。我们的主要贡献包括：为预训练视觉变换器设计了一种高效的剪枝策略、提出了一种新型的 Hessian 非对角结构的进化近似方法，以及一种无需重新训练或标签即可保持强大性能的自监督重要性评分机制。代码和剪枝后的模型可通过以下链接获取：https://elastic.ashita.nl/",
        "translated_title": "从预训练模型中无需微调的弹性视觉变换器",
        "label": [],
        "label_reason": "论文聚焦于视觉Transformer的剪枝，不涉及像素级图像处理任务",
        "relevance_score": 1,
        "novelty_score": 6,
        "novelty_reason": "提出了一种无需重新训练的剪枝方法，结合梯度信息和进化算法"
    },
    {
        "title": "GAS: Improving Discretization of Diffusion ODEs via Generalized\n  Adversarial Solver",
        "url": "http://arxiv.org/abs/2510.17699v1",
        "pub_date": "2025-10-20",
        "summary": "While diffusion models achieve state-of-the-art generation quality, they still suffer from computationally expensive sampling. Recent works address this issue with gradient-based optimization methods that distill a few-step ODE diffusion solver from the full sampling process, reducing the number of function evaluations from dozens to just a few. However, these approaches often rely on intricate training techniques and do not explicitly focus on preserving fine-grained details. In this paper, we introduce the Generalized Solver: a simple parameterization of the ODE sampler that does not require additional training tricks and improves quality over existing approaches. We further combine the original distillation loss with adversarial training, which mitigates artifacts and enhances detail fidelity. We call the resulting method the Generalized Adversarial Solver and demonstrate its superior performance compared to existing solver training methods under similar resource constraints. Code is available at https://github.com/3145tttt/GAS.",
        "translated": "尽管扩散模型在生成质量方面达到了最先进的水平，它们在采样过程中仍然面临计算成本高昂的问题。最近的研究工作通过基于梯度的优化方法解决了这一问题，这些方法从完整的采样过程中蒸馏出几步的 ODE 扩散求解器，将函数求值次数从几十次减少到几次。然而，这些方法通常依赖于复杂的训练技巧，并没有明确关注于保留精细的细节信息。在本文中，我们提出了广义求解器（Generalized Solver）：一种无需额外训练技巧的 ODE 采样器的简单参数化方式，并在现有方法的基础上提高了生成质量。我们进一步将原始的蒸馏损失与对抗训练相结合，这有助于减少伪影并增强细节的真实性。我们将所提出的方法称为广义对抗求解器（Generalized Adversarial Solver），并在相似资源约束下展示了其相比现有求解器训练方法的优越性能。代码可在 https://github.com/3145tttt/GAS 获得。",
        "translated_title": "GAS：通过广义对抗求解器改进扩散ODE的离散化",
        "label": [],
        "label_reason": "论文聚焦图像生成采样优化，非像素级图像恢复或增强任务。",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出通用对抗求解器改进扩散ODE离散化，训练方法简洁有效。"
    },
    {
        "title": "Towards 3D Objectness Learning in an Open World",
        "url": "http://arxiv.org/abs/2510.17686v1",
        "pub_date": "2025-10-20",
        "summary": "Recent advancements in 3D object detection and novel category detection have made significant progress, yet research on learning generalized 3D objectness remains insufficient. In this paper, we delve into learning open-world 3D objectness, which focuses on detecting all objects in a 3D scene, including novel objects unseen during training. Traditional closed-set 3D detectors struggle to generalize to open-world scenarios, while directly incorporating 3D open-vocabulary models for open-world ability struggles with vocabulary expansion and semantic overlap. To achieve generalized 3D object discovery, We propose OP3Det, a class-agnostic Open-World Prompt-free 3D Detector to detect any objects within 3D scenes without relying on hand-crafted text prompts. We introduce the strong generalization and zero-shot capabilities of 2D foundation models, utilizing both 2D semantic priors and 3D geometric priors for class-agnostic proposals to broaden 3D object discovery. Then, by integrating complementary information from point cloud and RGB image in the cross-modal mixture of experts, OP3Det dynamically routes uni-modal and multi-modal features to learn generalized 3D objectness. Extensive experiments demonstrate the extraordinary performance of OP3Det, which significantly surpasses existing open-world 3D detectors by up to 16.0% in AR and achieves a 13.5% improvement compared to closed-world 3D detectors.",
        "translated": "近年来，3D目标检测和新类别检测取得了显著进展，但关于学习广义3D目标性的研究仍显不足。本文我们深入探讨了开放世界3D目标性的学习，其核心目标是在3D场景中检测所有物体，包括训练过程中未见过的新物体。传统的闭集3D检测器难以在开放世界场景中泛化，而直接引入3D开放词汇模型以实现开放世界能力则面临词汇扩展和语义重叠的挑战。为了实现广义3D目标发现，我们提出了OP3Det，一种类别无关的无提示开放世界3D检测器，能够在不依赖人工设计文本提示的情况下检测3D场景中的任何物体。我们引入了2D基础模型强大的泛化能力和零样本能力，结合2D语义先验和3D几何先验生成类别无关的候选区域，从而扩展3D目标发现的范围。随后，通过在跨模态专家混合结构中融合点云和RGB图像的互补信息，OP3Det动态路由单模态和多模态特征，以学习广义3D目标性。大量实验表明，OP3Det具有出色的性能，在AR指标上显著超越现有的开放世界3D检测器，最高提升达16.0%，并且相比闭世界3D检测器也有13.5%的提升。",
        "translated_title": "在开放世界中实现3D目标性学习",
        "label": [],
        "label_reason": "论文聚焦3D目标检测与开放世界学习，不属于low-level图像处理任务。",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "提出了OP3Det，结合2D语义先验与3D几何先验，有一定创新性。"
    },
    {
        "title": "Multilingual Text-to-Image Person Retrieval via Bidirectional Relation\n  Reasoning and Aligning",
        "url": "http://arxiv.org/abs/2510.17685v1",
        "pub_date": "2025-10-20",
        "summary": "Text-to-image person retrieval (TIPR) aims to identify the target person using textual descriptions, facing challenge in modality heterogeneity. Prior works have attempted to address it by developing cross-modal global or local alignment strategies. However, global methods typically overlook fine-grained cross-modal differences, whereas local methods require prior information to explore explicit part alignments. Additionally, current methods are English-centric, restricting their application in multilingual contexts. To alleviate these issues, we pioneer a multilingual TIPR task by developing a multilingual TIPR benchmark, for which we leverage large language models for initial translations and refine them by integrating domain-specific knowledge. Correspondingly, we propose Bi-IRRA: a Bidirectional Implicit Relation Reasoning and Aligning framework to learn alignment across languages and modalities. Within Bi-IRRA, a bidirectional implicit relation reasoning module enables bidirectional prediction of masked image and text, implicitly enhancing the modeling of local relations across languages and modalities, a multi-dimensional global alignment module is integrated to bridge the modality heterogeneity. The proposed method achieves new state-of-the-art results on all multilingual TIPR datasets. Data and code are presented in https://github.com/Flame-Chasers/Bi-IRRA.",
        "translated": "文本到图像的人体检索（Text-to-image person retrieval, TIPR）旨在通过文本描述识别目标人物，面临模态异质性的挑战。现有工作尝试通过开发跨模态的全局或局部对齐策略来解决这一问题。然而，全局方法通常忽略细粒度的跨模态差异，而局部方法则需要先验信息以探索显式的部件对齐。此外，当前方法多以英语为中心，限制了其在多语言场景中的应用。为缓解这些问题，我们通过构建一个多语言TIPR基准数据集，开创了多语言TIPR任务，在该基准中，我们利用大语言模型进行初步翻译，并通过融合领域特定知识对其进行优化。相应地，我们提出Bi-IRRA：一种双向隐式关系推理与对齐框架，以学习跨语言和模态的对齐关系。在Bi-IRRA中，一个双向隐式关系推理模块能够实现图像和文本掩码部分的双向预测，隐式增强跨语言和模态的局部关系建模；同时集成一个多维全局对齐模块以弥合模态异质性。所提方法在所有多语言TIPR数据集上均取得了新的最先进结果。数据和代码已发布在 https://github.com/Flame-Chasers/Bi-IRRA。",
        "translated_title": "通过双向关系推理与对齐的多语言文本到图像行人检索",
        "label": [],
        "label_reason": "论文聚焦文本到图像的人检索，属于跨模态匹配，非图像像素级恢复或增强",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出双向关系推理与对齐框架，改进跨模态匹配，有一定创新性"
    },
    {
        "title": "Intelligent Communication Mixture-of-Experts Boosted-Medical Image\n  Segmentation Foundation Model",
        "url": "http://arxiv.org/abs/2510.17684v1",
        "pub_date": "2025-10-20",
        "summary": "Foundation models for medical image segmentation have achieved remarkable performance. Adaptive fine-tuning of natural image segmentation foundation models is crucial for medical image segmentation tasks. However, some limitations exist in existing fine-tuning methods: 1) insufficient representation of high-level features and 2) the fine-tuning process disrupts the structural integrity of pretrained weights. Inspired by these critical problems, we propose an intelligent communication mixture-of-experts boosted-medical image segmentation foundation model, named IC-MoE, with twofold ideas: 1) We construct basic experts, semantic experts, and adaptive experts. Moreover, we implement a pixel probability adaptive voting strategy, which enables expert selection and fusion through label consistency and load balancing. This approach preliminarily enhances the representation capability of high-level features while preserving the structural integrity of pretrained weights. 2) We propose a semantic-guided contrastive learning method to address the issue of weak supervision in contrastive learning. This method further enhances the representation capability of high-level features while preserving the structural integrity of pretrained weights. Extensive experiments across three public medical image segmentation datasets demonstrate that the IC-MoE outperforms other SOTA models. Consequently, the proposed IC-MoE effectively supplements foundational medical image segmentation models with high-level features and pretrained structural integrity. We also validate the superior generalizability of the IC-MoE across diverse medical image segmentation scenarios.",
        "translated": "医学图像分割的基础模型已经取得了显著的性能。自然图像分割基础模型的自适应微调对医学图像分割任务至关重要。然而，现有的微调方法中存在一些局限性：1）对高层特征的表示能力不足，2）微调过程破坏了预训练权重的结构完整性。受到这些问题的启发，我们提出了一种智能通信的混合专家增强型医学图像分割基础模型，命名为 IC-MoE，包含两个核心思想：1）我们构建了基本专家、语义专家和自适应专家。此外，我们实现了一种像素概率自适应投票策略，该策略通过标签一致性和负载平衡实现专家的选择与融合。这种方法初步增强了对高层特征的表示能力，同时保留了预训练权重的结构完整性。2）我们提出了一种语义引导的对比学习方法，以解决对比学习中监督信号较弱的问题。该方法进一步增强了对高层特征的表示能力，同时保持了预训练权重的结构完整性。在三个公开的医学图像分割数据集上的大量实验表明，IC-MoE 在性能上优于其他最先进的模型。因此，所提出的 IC-MoE 有效地通过高层特征和预训练结构完整性补充了医学图像分割的基础模型。我们还验证了 IC-MoE 在多种医学图像分割场景中的优越泛化能力。",
        "translated_title": "智能通信混合专家增强的医学图像分割基础模型",
        "label": [
            "医学图像增强"
        ],
        "label_reason": "论文关注医学图像分割，属于图像增强范畴",
        "relevance_score": 5,
        "novelty_score": 7,
        "novelty_reason": "提出IC-MoE模型和语义引导对比学习方法，具有显著改进"
    },
    {
        "title": "LLMs as Sparse Retrievers:A Framework for First-Stage Product Search",
        "url": "http://arxiv.org/abs/2510.18527v1",
        "pub_date": "2025-10-21",
        "summary": "Product search is a crucial component of modern e-commerce platforms, with billions of user queries every day. In product search systems, first-stage retrieval should achieve high recall while ensuring efficient online deployment. Sparse retrieval is particularly attractive in this context due to its interpretability and storage efficiency. However, sparse retrieval methods suffer from severe vocabulary mismatch issues, leading to suboptimal performance in product search scenarios.With their potential for semantic analysis, large language models (LLMs) offer a promising avenue for mitigating vocabulary mismatch issues and thereby improving retrieval quality. Directly applying LLMs to sparse retrieval in product search exposes two key challenges:(1)Queries and product titles are typically short and highly susceptible to LLM-induced hallucinations, such as generating irrelevant expansion terms or underweighting critical literal terms like brand names and model numbers;(2)The large vocabulary space of LLMs leads to difficulty in initializing training effectively, making it challenging to learn meaningful sparse representations in such ultra-high-dimensional spaces.To address these challenges, we propose PROSPER, a framework for PROduct search leveraging LLMs as SParsE Retrievers. PROSPER incorporates: (1)A literal residual network that alleviates hallucination in lexical expansion by reinforcing underweighted literal terms through a residual compensation mechanism; and (2)A lexical focusing window that facilitates effective training initialization via a coarse-to-fine sparsification strategy.Extensive offline and online experiments show that PROSPER significantly outperforms sparse baselines and achieves recall performance comparable to advanced dense retrievers, while also achieving revenue increments online.",
        "translated": "产品搜索是现代电子商务平台的关键组成部分，每天处理数十亿用户查询。在产品搜索系统中，第一阶段的召回应在保证高效在线部署的前提下实现高召回率。稀疏召回由于其可解释性和存储效率，在这种场景下尤为具有吸引力。然而，稀疏召回方法在产品搜索场景中会受到严重的词汇不匹配问题，导致性能欠佳。大语言模型（LLMs）由于其在语义分析方面的潜力，为缓解词汇不匹配问题并从而提升召回质量提供了有前景的途径。然而，将LLMs直接应用于产品搜索中的稀疏召回会面临两个关键挑战：(1) 查询和产品标题通常较短，极易受到LLM引发的幻觉影响，例如生成不相关的扩展词汇或低估关键的字面词汇，如品牌名称和型号数字；(2) LLM的庞大词汇空间使得训练初始化变得困难，从而难以在这样的超高维空间中学习到有意义的稀疏表示。\n\n为解决这些挑战，我们提出了PROSPER，一种利用大语言模型作为稀疏召回器的产品搜索框架。PROSPER包含以下两个组成部分：(1) 一个字面残差网络，通过残差补偿机制强化被低估的字面词汇，从而减轻词汇扩展中的幻觉问题；(2) 一个词汇聚焦窗口，通过从粗到细的稀疏化策略促进有效的训练初始化。广泛的离线和在线实验表明，PROSPER显著优于稀疏基线方法，在召回性能上可与先进的密集召回器相媲美，同时在在线环境中也实现了收入增长。",
        "translated_title": "LLMs 作为稀疏检索器：一个用于第一阶段商品搜索的框架",
        "label": [
            "召回（Recall）",
            "LLM生成式推荐（LLM-based Generative Recommendation）",
            "通用推荐技术（General Recommendation Techniques）"
        ],
        "label_reason": "论文聚焦于基于LLM的召回框架PROSPER，改进稀疏检索在电商产品搜索中的表现。",
        "relevance_score": 8,
        "novelty_score": 7,
        "novelty_reason": "提出结合LLM与稀疏检索的新框架，解决词汇不匹配与幻觉问题，具有实用创新性。"
    },
    {
        "title": "ImageGem: In-the-wild Generative Image Interaction Dataset for\n  Generative Model Personalization",
        "url": "http://arxiv.org/abs/2510.18433v1",
        "pub_date": "2025-10-21",
        "summary": "We introduce ImageGem, a dataset for studying generative models that understand fine-grained individual preferences. We posit that a key challenge hindering the development of such a generative model is the lack of in-the-wild and fine-grained user preference annotations. Our dataset features real-world interaction data from 57K users, who collectively have built 242K customized LoRAs, written 3M text prompts, and created 5M generated images. With user preference annotations from our dataset, we were able to train better preference alignment models. In addition, leveraging individual user preference, we investigated the performance of retrieval models and a vision-language model on personalized image retrieval and generative model recommendation. Finally, we propose an end-to-end framework for editing customized diffusion models in a latent weight space to align with individual user preferences. Our results demonstrate that the ImageGem dataset enables, for the first time, a new paradigm for generative model personalization.",
        "translated": "我们引入 ImageGem 数据集，用于研究能够理解细粒度个性化偏好的生成式模型。我们认为，阻碍此类生成式模型发展的关键挑战之一是缺乏真实场景下的细粒度用户偏好标注。我们的数据集包含了 57K 个用户的真实交互数据，这些用户共同构建了 242K 个定制化的 LoRA 模型，撰写了 3M 条文本提示，并生成了 5M 张图像。借助数据集中提供的用户偏好标注，我们训练出了更优的偏好对齐模型。此外，基于个体用户偏好，我们研究了检索模型和视觉-语言模型在个性化图像检索和生成式模型推荐方面的性能。最后，我们提出了一种端到端的框架，用于在潜在权重空间中编辑定制化的扩散模型，以对齐个体用户偏好。我们的结果表明，ImageGem 数据集首次实现了一种生成式模型个性化的全新范式。",
        "translated_title": "ImageGem：用于生成模型个性化的野外生成图像交互数据集",
        "label": [
            "LLM生成式推荐",
            "个性化推荐"
        ],
        "label_reason": "论文涉及生成式模型个性化与推荐，与生成式推荐系统相关",
        "relevance_score": 7,
        "novelty_score": 8,
        "novelty_reason": "提出新数据集和端到端框架，推动个性化生成模型发展"
    },
    {
        "title": "Censorship Chokepoints: New Battlegrounds for Regional Surveillance,\n  Censorship and Influence on the Internet",
        "url": "http://arxiv.org/abs/2510.18394v1",
        "pub_date": "2025-10-21",
        "summary": "Undoubtedly, the Internet has become one of the most important conduits to information for the general public. Nonetheless, Internet access can be and has been limited systematically or blocked completely during political events in numerous countries and regions by various censorship mechanisms. Depending on where the core filtering component is situated, censorship techniques have been classified as client-based, server-based, or network-based. However, as the Internet evolves rapidly, new and sophisticated censorship techniques have emerged, which involve techniques that cut across locations and involve new forms of hurdles to information access. We argue that modern censorship can be better understood through a new lens that we term chokepoints, which identifies bottlenecks in the content production or delivery cycle where efficient new forms of large-scale client-side surveillance and filtering mechanisms have emerged.",
        "translated": "毫无疑问，互联网已成为普通公众获取信息最重要的渠道之一。然而，在许多国家和地区，特别是在政治事件期间，互联网的访问权限可能被系统性地限制或完全封锁，这涉及各种审查机制。根据核心过滤组件的位置，审查技术被分为基于客户端、基于服务器或基于网络的类型。然而，随着互联网的快速发展，新的、复杂的审查技术不断涌现，这些技术突破了地理位置的限制，并引入了新型的信息获取障碍。我们认为，通过一个我们称之为“瓶颈点”的新视角，可以更好地理解现代审查机制，该视角识别内容生产或分发周期中的关键瓶颈，在这些瓶颈处已出现了高效的大规模客户端监控和过滤机制。",
        "translated_title": "审查瓶颈：互联网区域监控、审查与影响的新战场",
        "label": [],
        "label_reason": "论文聚焦网络审查，与推荐系统无直接关联。",
        "relevance_score": 1,
        "novelty_score": 3,
        "novelty_reason": "提出新概念‘chokepoints’，但属于社会技术分析，创新性有限。"
    },
    {
        "title": "Evaluating LLM-Based Mobile App Recommendations: An Empirical Study",
        "url": "http://arxiv.org/abs/2510.18364v1",
        "pub_date": "2025-10-21",
        "summary": "Large Language Models (LLMs) are increasingly used to recommend mobile applications through natural language prompts, offering a flexible alternative to keyword-based app store search. Yet, the reasoning behind these recommendations remains opaque, raising questions about their consistency, explainability, and alignment with traditional App Store Optimization (ASO) metrics. In this paper, we present an empirical analysis of how widely-used general purpose LLMs generate, justify, and rank mobile app recommendations. Our contributions are: (i) a taxonomy of 16 generalizable ranking criteria elicited from LLM outputs; (ii) a systematic evaluation framework to analyse recommendation consistency and responsiveness to explicit ranking instructions; and (iii) a replication package to support reproducibility and future research on AI-based recommendation systems. Our findings reveal that LLMs rely on a broad yet fragmented set of ranking criteria, only partially aligned with standard ASO metrics. While top-ranked apps tend to be consistent across runs, variability increases with ranking depth and search specificity. LLMs exhibit varying sensitivity to explicit ranking instructions - ranging from substantial adaptations to near-identical outputs - highlighting their complex reasoning dynamics in conversational app discovery. Our results aim to support end-users, app developers, and recommender-systems researchers in navigating the emerging landscape of conversational app discovery.",
        "translated": "大语言模型（LLMs）正被越来越多地用于通过自然语言提示推荐移动应用，为传统的基于关键词的商店搜索提供了一种灵活的替代方式。然而，这些推荐背后的推理过程仍然不透明，引发了关于其一致性、可解释性以及与传统应用商店优化（App Store Optimization, ASO）指标对齐程度的疑问。在本文中，我们对广泛使用的通用大语言模型如何生成、证明和排序移动应用推荐进行了实证分析。我们的贡献包括：(i) 从LLM输出中提取出的16种通用排序标准的分类法；(ii) 一个系统性的评估框架，用于分析推荐的一致性以及对显式排序指令的响应能力；以及(iii) 一个复现包，以支持基于人工智能的推荐系统的可复现性和未来研究。我们的研究结果表明，LLM依赖于一个广泛但碎片化的排序标准集合，这些标准仅部分与标准的ASO指标对齐。尽管排名靠前的应用在不同运行中往往具有一致性，但随着排序深度和搜索特定性的增加，变异性也随之上升。LLM对显式排序指令的敏感性各不相同——从显著的适应到几乎完全相同的输出——突显了它们在对话式应用发现中的复杂推理动态。我们的研究结果旨在帮助终端用户、应用开发者和推荐系统研究人员在新兴的对话式应用发现领域中更好地导航。",
        "translated_title": "基于大语言模型的移动应用推荐评估：一项实证研究",
        "label": [
            "LLM生成式推荐",
            "推荐系统评估"
        ],
        "label_reason": "论文研究LLM生成移动应用推荐并评估其一致性，与推荐系统生成及评估相关。",
        "relevance_score": 8,
        "novelty_score": 7,
        "novelty_reason": "提出LLM推荐的评估框架与排名标准，具有一定新颖性。"
    },
    {
        "title": "KrishokBondhu: A Retrieval-Augmented Voice-Based Agricultural Advisory\n  Call Center for Bengali Farmers",
        "url": "http://arxiv.org/abs/2510.18355v1",
        "pub_date": "2025-10-21",
        "summary": "In Bangladesh, many farmers continue to face challenges in accessing timely, expert-level agricultural guidance. This paper presents KrishokBondhu, a voice-enabled, call-centre-integrated advisory platform built on a Retrieval-Augmented Generation (RAG) framework, designed specifically for Bengali-speaking farmers. The system aggregates authoritative agricultural handbooks, extension manuals, and NGO publications; applies Optical Character Recognition (OCR) and document-parsing pipelines to digitize and structure the content; and indexes this corpus in a vector database for efficient semantic retrieval. Through a simple phone-based interface, farmers can call the system to receive real-time, context-aware advice: speech-to-text converts the Bengali query, the RAG module retrieves relevant content, a large language model (Gemma 3-4B) generates a context-grounded response, and text-to-speech delivers the answer in natural spoken Bengali. In a pilot evaluation, KrishokBondhu produced high-quality responses for 72.7% of diverse agricultural queries covering crop management, disease control, and cultivation practices. Compared to the KisanQRS benchmark, the system achieved a composite score of 4.53 (vs. 3.13) on a 5-point scale, a 44.7% improvement, with especially large gains in contextual richness (+367%) and completeness (+100.4%), while maintaining comparable relevance and technical specificity. Semantic similarity analysis further revealed a strong correlation between retrieved context and answer quality, emphasizing the importance of grounding generative responses in curated documentation. KrishokBondhu demonstrates the feasibility of integrating call-centre accessibility, multilingual voice interaction, and modern RAG techniques to deliver expert-level agricultural guidance to remote Bangladeshi farmers, paving the way toward a fully AI-driven agricultural advisory ecosystem.",
        "translated": "在孟加拉国，许多农民仍面临难以及时获得专家级农业指导的挑战。本文提出 KrishokBondhu，一个基于检索增强生成（RAG）框架的语音交互、呼叫中心集成的咨询平台，专门为讲孟加拉语的农民设计。该系统汇集了权威的农业手册、推广手册和非政府组织（NGO）出版物；应用光学字符识别（OCR）和文档解析流水线对内容进行数字化和结构化处理；并将该文档集索引至向量数据库中，以实现高效的语义召回。通过一个简单的基于电话的交互界面，农民可以拨打电话，实时获得情境感知的建议：语音转文本将孟加拉语的查询转换为文本，RAG 模块召回相关内容，一个大语言模型（Gemma 3-4B）生成基于上下文的响应，文本转语音则以自然的孟加拉语语音形式输出答案。在试点评估中，KrishokBondhu 对 72.7% 的多样化农业查询（涵盖作物管理、病虫害防治和栽培实践）提供了高质量的回复。与 KisanQRS 基准系统相比，该系统在 5 分制评分体系下获得了 4.53（对比 3.13）的综合得分，提升了 44.7%，尤其是在上下文丰富性（+367%）和完整性（+100.4%）方面表现出显著优势，同时保持了相当的相关性和技术准确性。语义相似性分析进一步揭示了召回上下文与答案质量之间的强相关性，强调了将生成式回复基于精心整理的文档的重要性。KrishokBondhu 证明了将呼叫中心的可访问性、多语言语音交互和现代 RAG 技术相结合，为偏远地区的孟加拉国农民提供专家级农业指导的可行性，为构建一个完全由人工智能驱动的农业咨询服务生态系统铺平了道路。",
        "translated_title": "KrishokBondhu：一个基于检索增强的语音农业咨询服务热线，面向孟加拉语农民",
        "label": [
            "LLM生成式推荐",
            "多模态推荐"
        ],
        "label_reason": "使用RAG和语音交互生成农业建议，间接涉及生成式推荐和多模态交互。",
        "relevance_score": 5,
        "novelty_score": 7,
        "novelty_reason": "将RAG与语音系统结合用于农业咨询，具有实际创新性和跨领域整合能力。"
    },
    {
        "title": "Enhancing Hotel Recommendations with AI: LLM-Based Review Summarization\n  and Query-Driven Insights",
        "url": "http://arxiv.org/abs/2510.18277v1",
        "pub_date": "2025-10-21",
        "summary": "The increasing number of data a booking platform such as Booking.com and AirBnB offers make it challenging for interested parties to browse through the available accommodations and analyze reviews in an efficient way. Efforts have been made from the booking platform providers to utilize recommender systems in an effort to enable the user to filter the results by factors such as stars, amenities, cost but most valuable insights can be provided by the unstructured text-based reviews. Going through these reviews one-by-one requires a substantial amount of time to be devoted while a respectable percentage of the reviews won't provide to the user what they are actually looking for.   This research publication explores how Large Language Models (LLMs) can enhance short rental apartments recommendations by summarizing and mining key insights from user reviews. The web application presented in this paper, named \"instaGuide\", automates the procedure of isolating the text-based user reviews from a property on the Booking.com platform, synthesizing the summary of the reviews, and enabling the user to query specific aspects of the property in an effort to gain feedback on their personal questions/criteria.   During the development of the instaGuide tool, numerous LLM models were evaluated based on accuracy, cost, and response quality. The results suggest that the LLM-powered summarization reduces significantly the amount of time the users need to devote on their search for the right short rental apartment, improving the overall decision-making procedure.",
        "translated": "随着 Booking.com 和 Airbnb 等预订平台提供的数据量不断增加，使利益相关者以高效的方式浏览可用住宿和分析评论变得具有挑战性。为了使用户能够根据星级、设施、价格等因素过滤结果，平台提供商已尝试采用推荐系统，但最有价值的见解往往来自非结构化的文本评论。逐条阅读这些评论需要用户投入大量时间，且相当一部分评论并不能提供用户真正关心的信息。本文研究探讨了大语言模型（LLM）如何通过总结和挖掘用户评论中的关键信息，提升短租公寓的推荐效果。本文提出了一款名为“instaGuide”的网络应用程序，该应用能够自动化地从 Booking.com 平台上提取某房产的基于文本的用户评论，生成评论摘要，并允许用户查询该房产的具体方面，以获取针对其个人问题或标准的反馈。在 instaGuide 工具的开发过程中，评估了多种 LLM 模型的准确性、成本和响应质量。结果表明，基于 LLM 的摘要显著减少了用户寻找合适的短租公寓所需的时间，从而提升了整体决策过程的效率。",
        "translated_title": "利用人工智能提升酒店推荐：基于大语言模型的评论摘要与查询驱动的洞察",
        "label": [
            "LLM生成式推荐",
            "通用推荐技术"
        ],
        "label_reason": "利用LLM进行酒店评论摘要生成，增强推荐系统的实用性。",
        "relevance_score": 7,
        "novelty_score": 6,
        "novelty_reason": "提出基于LLM的评论摘要方法，但未显著突破现有推荐范式。"
    },
    {
        "title": "LIME: Link-based user-item Interaction Modeling with decoupled xor\n  attention for Efficient test time scaling",
        "url": "http://arxiv.org/abs/2510.18239v1",
        "pub_date": "2025-10-21",
        "summary": "Scaling large recommendation systems requires advancing three major frontiers: processing longer user histories, expanding candidate sets, and increasing model capacity. While promising, transformers' computational cost scales quadratically with the user sequence length and linearly with the number of candidates. This trade-off makes it prohibitively expensive to expand candidate sets or increase sequence length at inference, despite the significant performance improvements.   We introduce \\textbf{LIME}, a novel architecture that resolves this trade-off. Through two key innovations, LIME fundamentally reduces computational complexity. First, low-rank ``link embeddings\" enable pre-computation of attention weights by decoupling user and candidate interactions, making the inference cost nearly independent of candidate set size. Second, a linear attention mechanism, \\textbf{LIME-XOR}, reduces the complexity with respect to user sequence length from quadratic ($O(N^2)$) to linear ($O(N)$).   Experiments on public and industrial datasets show LIME achieves near-parity with state-of-the-art transformers but with a 10$\\times$ inference speedup on large candidate sets or long sequence lengths. When tested on a major recommendation platform, LIME improved user engagement while maintaining minimal inference costs with respect to candidate set size and user history length, establishing a new paradigm for efficient and expressive recommendation systems.",
        "translated": "扩展大规模推荐系统需要推进三个主要方向：处理更长的用户历史、扩大候选集规模以及提升模型容量。虽然Transformer在这些方面表现出潜力，但其计算成本随着用户序列长度呈平方级增长，随着候选数量呈线性增长。这种权衡使得在推理阶段扩展候选集或增加序列长度的成本变得非常高，即便其在性能上有显著提升。我们提出了一种新颖的架构 **LIME**，来解决这一权衡问题。通过两个关键创新，LIME从根本上降低了计算复杂度。首先，低秩的“链接嵌入”（link embeddings）通过解耦用户与候选之间的交互，使得注意力权重可以预先计算，从而使推理成本几乎不再依赖于候选集的规模。其次，线性注意力机制 **LIME-XOR** 将与用户序列长度相关的复杂度从平方级（$O(N^2)$）降低到线性级（$O(N)$）。我们在公开和工业数据集上的实验表明，LIME在性能上接近最先进的Transformer模型，但在处理大规模候选集或长序列时，推理速度提升了10倍。在主流推荐平台上的测试结果表明，LIME在保持候选集规模和用户历史长度的最小推理成本的同时，显著提升了用户参与度，从而为高效且表达能力强的推荐系统建立了一种新的范式。",
        "translated_title": "LIME：基于链接的用户-物料交互建模与解耦异或注意力，用于高效的在线测试扩展",
        "label": [
            "精排（Ranking）",
            "序列推荐（Sequential Recommendation）",
            "通用推荐技术（General Recommendation Techniques）"
        ],
        "label_reason": "论文聚焦推荐系统的高效序列建模与大规模候选集处理",
        "relevance_score": 9,
        "novelty_score": 8,
        "novelty_reason": "提出解耦注意力与线性机制，显著降低推理复杂度"
    },
    {
        "title": "From AutoRecSys to AutoRecLab: A Call to Build, Evaluate, and Govern\n  Autonomous Recommender-Systems Research Labs",
        "url": "http://arxiv.org/abs/2510.18104v1",
        "pub_date": "2025-10-20",
        "summary": "Recommender-systems research has accelerated model and evaluation advances, yet largely neglects automating the research process itself. We argue for a shift from narrow AutoRecSys tools -- focused on algorithm selection and hyper-parameter tuning -- to an Autonomous Recommender-Systems Research Lab (AutoRecLab) that integrates end-to-end automation: problem ideation, literature analysis, experimental design and execution, result interpretation, manuscript drafting, and provenance logging. Drawing on recent progress in automated science (e.g., multi-agent AI Scientist and AI Co-Scientist systems), we outline an agenda for the RecSys community: (1) build open AutoRecLab prototypes that combine LLM-driven ideation and reporting with automated experimentation; (2) establish benchmarks and competitions that evaluate agents on producing reproducible RecSys findings with minimal human input; (3) create review venues for transparently AI-generated submissions; (4) define standards for attribution and reproducibility via detailed research logs and metadata; and (5) foster interdisciplinary dialogue on ethics, governance, privacy, and fairness in autonomous research. Advancing this agenda can increase research throughput, surface non-obvious insights, and position RecSys to contribute to emerging Artificial Research Intelligence. We conclude with a call to organise a community retreat to coordinate next steps and co-author guidance for the responsible integration of automated research systems.",
        "translated": "推荐系统研究推动了模型和评估方法的进步，但很大程度上忽视了研究过程本身的自动化。我们主张从狭义的 AutoRecSys 工具（专注于算法选择和超参数调优）转向一个自主的推荐系统研究实验室（Autonomous Recommender-Systems Research Lab，AutoRecLab），该实验室集成端到端的自动化：问题构思、文献分析、实验设计与执行、结果解释、论文撰写以及溯源日志记录。借鉴自动化科学领域的最新进展（例如多智能体 AI 科学家和 AI 协同科学家系统），我们为推荐系统社区提出一个研究议程：（1）构建开源的 AutoRecLab 原型，将大语言模型驱动的构思与报告撰写与自动实验相结合；（2）建立基准测试和竞赛，评估智能体在最小人工输入下生成可复现推荐系统发现的能力；（3）创建透明的、针对 AI 生成论文的审阅渠道；（4）通过详细的研究日志和元数据定义归属和可复现性的标准；以及（5）促进关于自主研究中伦理、治理、隐私和公平性的跨学科对话。推动这一议程可以提升研究效率，揭示非显而易见的洞见，并使推荐系统在新兴的人工智能研究智能（Artificial Research Intelligence）中发挥重要作用。最后，我们呼吁组织一次社区退思会，以协调下一步工作，并共同撰写关于自动研究系统负责任整合的指导文件。",
        "translated_title": "从 AutoRecSys 到 AutoRecLab：呼吁构建、评估与治理自主推荐系统研究实验室",
        "label": [
            "推荐系统评估",
            "通用推荐技术",
            "LLM生成式推荐"
        ],
        "label_reason": "论文提出自动化推荐系统研究实验室框架，涉及生成、评估与治理，与推荐系统紧密相关。",
        "relevance_score": 8,
        "novelty_score": 7,
        "novelty_reason": "提出将LLM与自动化科学结合的新研究范式，具有一定的创新性和前瞻性。"
    },
    {
        "title": "Grasp Any Region: Towards Precise, Contextual Pixel Understanding for\n  Multimodal LLMs",
        "url": "http://arxiv.org/abs/2510.18876v1",
        "pub_date": "2025-10-21",
        "summary": "While Multimodal Large Language Models (MLLMs) excel at holistic understanding, they struggle in capturing the dense world with complex scenes, requiring fine-grained analysis of intricate details and object inter-relationships. Region-level MLLMs have been a promising step. However, previous attempts are generally optimized to understand given regions in isolation, neglecting crucial global contexts. To address this, we introduce Grasp Any Region (GAR) for comprehen- sive region-level visual understanding. Empowered by an effective RoI-aligned feature replay technique, GAR supports (1) precise perception by leveraging necessary global contexts, and (2) modeling interactions between multiple prompts. Together, it then naturally achieves (3) advanced compositional reasoning to answer specific free-form questions about any region, shifting the paradigm from passive description to active dialogue. Moreover, we construct GAR-Bench, which not only provides a more accurate evaluation of single-region comprehension, but also, more importantly, measures interactions and complex reasoning across multiple regions. Extensive experiments have demonstrated that GAR-1B not only maintains the state-of-the-art captioning capabilities, e.g., outperforming DAM-3B +4.5 on DLC-Bench, but also excels at modeling relationships between multiple prompts with advanced comprehension capabilities, even surpassing InternVL3-78B on GAR-Bench-VQA. More importantly, our zero-shot GAR-8B even outperforms in-domain VideoRefer-7B on VideoRefer-BenchQ, indicating its strong capabilities can be easily transferred to videos.",
        "translated": "虽然多模态大语言模型（MLLMs）在整体理解方面表现出色，但它们在捕捉具有复杂场景的密集世界方面仍面临挑战，需要对细节和物体之间的关系进行细致分析。区域级别的 MLLMs 提供了一个有前景的方向。然而，以往的尝试通常仅优化了对给定区域的独立理解，忽略了关键的全局上下文。为了解决这一问题，我们提出了 Grasp Any Region（GAR），旨在实现全面的区域级视觉理解。借助有效的 RoI 对齐特征重放技术，GAR 支持（1）通过利用必要的全局上下文实现精确感知，以及（2）对多个提示之间的交互进行建模。结合这两方面的能力，GAR 自然地实现了（3）高级的组合推理能力，以回答关于任意区域的特定自由形式问题，从而将范式从被动描述转变为主动对话。此外，我们构建了 GAR-Bench，它不仅能够更准确地评估单区域理解能力，更重要的是，可以衡量多个区域之间的交互与复杂推理。大量实验表明，GAR-1B 不仅保持了最先进的图像描述能力，例如在 DLC-Bench 上优于 DAM-3B +4.5，而且在具有高级理解能力的多提示关系建模方面表现出色，甚至在 GAR-Bench-VQA 上超越了 InternVL3-78B。更重要的是，我们零样本的 GAR-8B 在 VideoRefer-BenchQ 上的表现甚至优于领域内的 VideoRefer-7B，表明其强大能力可以轻松迁移到视频中。",
        "translated_title": "Grasp Any Region: Towards Precise, Contextual Pixel Understanding for Multimodal LLMs\n\n抓住任意区域：面向精确、上下文感知像素理解的多模态大语言模型",
        "label": [],
        "label_reason": "论文关注区域级视觉理解，偏向high-level任务。",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出新的区域级交互建模方法，但属于常规改进。"
    },
    {
        "title": "DSI-Bench: A Benchmark for Dynamic Spatial Intelligence",
        "url": "http://arxiv.org/abs/2510.18873v1",
        "pub_date": "2025-10-21",
        "summary": "Reasoning about dynamic spatial relationships is essential, as both observers and objects often move simultaneously. Although vision-language models (VLMs) and visual expertise models excel in 2D tasks and static scenarios, their ability to fully understand dynamic 3D scenarios remains limited. We introduce Dynamic Spatial Intelligence and propose DSI-Bench, a benchmark with nearly 1,000 dynamic videos and over 1,700 manually annotated questions covering nine decoupled motion patterns of observers and objects. Spatially and temporally symmetric designs reduce biases and enable systematic evaluation of models' reasoning about self-motion and object motion. Our evaluation of 14 VLMs and expert models reveals key limitations: models often conflate observer and object motion, exhibit semantic biases, and fail to accurately infer relative relationships in dynamic scenarios. Our DSI-Bench provides valuable findings and insights about the future development of general and expertise models with dynamic spatial intelligence.",
        "translated": "动态空域关系的推理至关重要，因为观察者和物体通常会同时运动。尽管视觉-语言模型（VLMs）和视觉专家模型在二维任务和静态场景中表现出色，但它们在全面理解动态三维场景方面的能力仍然有限。我们引入了动态空域智能（Dynamic Spatial Intelligence），并提出了DSI-Bench，这是一个包含近1,000个动态视频和1,700多个手动标注问题的基准测试集，涵盖了观察者与物体之间九种解耦的运动模式。时空对称的设计减少了偏见，并能够系统评估模型对自运动和物体运动的推理能力。我们对14个VLMs和专家模型的评估揭示了关键的局限性：模型常常将观察者运动与物体运动混淆，表现出语义偏见，并且在动态场景中无法准确推断相对关系。我们的DSI-Bench为具备动态空域智能的一般模型和专家模型的未来发展提供了有价值的发现与洞见。",
        "translated_title": "DSI-Bench：动态空间智能基准",
        "label": [],
        "label_reason": "论文关注动态空间推理，属于high-level视觉任务，不涉及像素级图像恢复或增强。",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出了新的动态空间智能基准DSI-Bench，但方法上无本质性的low-level图像处理创新。"
    },
    {
        "title": "LightMem: Lightweight and Efficient Memory-Augmented Generation",
        "url": "http://arxiv.org/abs/2510.18866v1",
        "pub_date": "2025-10-21",
        "summary": "Despite their remarkable capabilities, Large Language Models (LLMs) struggle to effectively leverage historical interaction information in dynamic and complex environments. Memory systems enable LLMs to move beyond stateless interactions by introducing persistent information storage, retrieval, and utilization mechanisms. However, existing memory systems often introduce substantial time and computational overhead. To this end, we introduce a new memory system called LightMem, which strikes a balance between the performance and efficiency of memory systems. Inspired by the Atkinson-Shiffrin model of human memory, LightMem organizes memory into three complementary stages. First, cognition-inspired sensory memory rapidly filters irrelevant information through lightweight compression and groups information according to their topics. Next, topic-aware short-term memory consolidates these topic-based groups, organizing and summarizing content for more structured access. Finally, long-term memory with sleep-time update employs an offline procedure that decouples consolidation from online inference. Experiments on LongMemEval with GPT and Qwen backbones show that LightMem outperforms strong baselines in accuracy (up to 10.9% gains) while reducing token usage by up to 117x, API calls by up to 159x, and runtime by over 12x. The code is available at https://github.com/zjunlp/LightMem.",
        "translated": "尽管大型语言模型（LLMs）具备显著的能力，但它们在动态且复杂的环境中难以有效利用历史交互信息。记忆系统通过引入持续的信息存储、检索和利用机制，使得 LLM 能够超越无状态的交互模式。然而，现有的记忆系统通常会引入较大的时间与计算开销。为此，我们提出了一种新的记忆系统 LightMem，在记忆系统的性能与效率之间取得了平衡。受人类记忆的 Atkinson-Shiffrin 模型启发，LightMem 将记忆组织为三个互补的阶段。首先，受认知启发的感官记忆通过轻量级压缩快速过滤无关信息，并根据主题对信息进行分组。接下来，主题感知的短期记忆对这些基于主题的分组进行整合，组织和总结内容以实现更结构化的访问。最后，具有睡眠时间更新机制的长期记忆采用离线过程，将整合操作与在线推理解耦。在 LongMemEval 上的实验表明，基于 GPT 和 Qwen 的 LightMem 在准确率方面优于强基线（最高提升 10.9%），同时将 token 使用量减少了最多 117 倍，API 调用量减少了最多 159 倍，运行时间减少了超过 12 倍。代码可在 https://github.com/zjunlp/LightMem 获取。",
        "translated_title": "LightMem: 轻量且高效的记忆增强生成",
        "label": [],
        "label_reason": "论文聚焦语言模型记忆系统优化，非图像处理任务",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出轻量高效记忆系统，改进信息存储与检索机制"
    },
    {
        "title": "DP$^2$O-SR: Direct Perceptual Preference Optimization for Real-World\n  Image Super-Resolution",
        "url": "http://arxiv.org/abs/2510.18851v1",
        "pub_date": "2025-10-21",
        "summary": "Benefiting from pre-trained text-to-image (T2I) diffusion models, real-world image super-resolution (Real-ISR) methods can synthesize rich and realistic details. However, due to the inherent stochasticity of T2I models, different noise inputs often lead to outputs with varying perceptual quality. Although this randomness is sometimes seen as a limitation, it also introduces a wider perceptual quality range, which can be exploited to improve Real-ISR performance. To this end, we introduce Direct Perceptual Preference Optimization for Real-ISR (DP$^2$O-SR), a framework that aligns generative models with perceptual preferences without requiring costly human annotations. We construct a hybrid reward signal by combining full-reference and no-reference image quality assessment (IQA) models trained on large-scale human preference datasets. This reward encourages both structural fidelity and natural appearance. To better utilize perceptual diversity, we move beyond the standard best-vs-worst selection and construct multiple preference pairs from outputs of the same model. Our analysis reveals that the optimal selection ratio depends on model capacity: smaller models benefit from broader coverage, while larger models respond better to stronger contrast in supervision. Furthermore, we propose hierarchical preference optimization, which adaptively weights training pairs based on intra-group reward gaps and inter-group diversity, enabling more efficient and stable learning. Extensive experiments across both diffusion- and flow-based T2I backbones demonstrate that DP$^2$O-SR significantly improves perceptual quality and generalizes well to real-world benchmarks.",
        "translated": "借助预训练的文生图（T2I）扩散模型，真实世界图像超分辨率（Real-ISR）方法可以合成丰富的逼真细节。然而，由于 T2I 模型固有的随机性，不同的噪声输入通常会导致具有不同感知质量的输出。尽管这种随机性有时被视为限制，但同时也引入了更广泛的感知质量范围，可以加以利用以提升 Real-ISR 的性能。为此，我们提出面向 Real-ISR 的直接感知偏好优化（Direct Perceptual Preference Optimization for Real-ISR，DP$^2$O-SR），这是一种无需代价高昂的人类标注即可将生成模型与感知偏好对齐的框架。我们通过结合在大规模人类偏好数据集上训练的全参考和无参考图像质量评估（IQA）模型，构建了一个混合奖励信号。该奖励信号同时鼓励结构保真度和自然外观。为了更好地利用感知多样性，我们超越了标准的最佳与最差选择策略，从同一模型的输出中构建多个偏好对。我们的分析表明，最优的选择比例取决于模型容量：小模型受益于更广泛的覆盖，而大模型则对更强的监督对比响应更好。此外，我们提出了分层偏好优化方法，该方法基于组内奖励差距和组间多样性自适应地加权训练对，从而实现更高效和稳定的训练。在基于扩散模型和流模型的 T2I 主干网络上的大量实验表明，DP$^2$O-SR 显著提升了感知质量，并在真实世界基准上表现出良好的泛化能力。",
        "translated_title": "DP$^2$O-SR：面向真实场景图像超分辨率的直接感知偏好优化",
        "label": [
            "超分辨率",
            "图像增强"
        ],
        "label_reason": "论文聚焦真实图像超分辨率，属于图像恢复与增强任务",
        "relevance_score": 9,
        "novelty_score": 8,
        "novelty_reason": "提出感知偏好优化框架，改进扩散模型用于图像超分辨率"
    },
    {
        "title": "See the Text: From Tokenization to Visual Reading",
        "url": "http://arxiv.org/abs/2510.18840v1",
        "pub_date": "2025-10-21",
        "summary": "People see text. Humans read by recognizing words as visual objects, including their shapes, layouts, and patterns, before connecting them to meaning, which enables us to handle typos, distorted fonts, and various scripts effectively. Modern large language models (LLMs), however, rely on subword tokenization, fragmenting text into pieces from a fixed vocabulary. While effective for high-resource languages, this approach over-segments low-resource languages, yielding long, linguistically meaningless sequences and inflating computation. In this work, we challenge this entrenched paradigm and move toward a vision-centric alternative. Our method, SeeTok, renders text as images (visual-text) and leverages pretrained multimodal LLMs to interpret them, reusing strong OCR and text-vision alignment abilities learned from large-scale multimodal training. Across three different language tasks, SeeTok matches or surpasses subword tokenizers while requiring 4.43 times fewer tokens and reducing FLOPs by 70.5%, with additional gains in cross-lingual generalization, robustness to typographic noise, and linguistic hierarchy. SeeTok signals a shift from symbolic tokenization to human-like visual reading, and takes a step toward more natural and cognitively inspired language models.",
        "translated": "人们看到的是文本。人类通过将词语识别为视觉对象，包括它们的形状、布局和模式，然后将其与语义联系起来进行阅读，这种机制使我们能够有效地处理拼写错误、变形字体和各种书写系统。然而，现代的大型语言模型（LLMs）依赖于子词分词，将文本分割成来自固定词典的片段。虽然这种方法对于高资源语言是有效的，但它对低资源语言进行了过度分割，产生了长而语言学上无意义的序列，并增加了计算负担。在本研究中，我们挑战了这种根深蒂固的范式，转向一种以视觉为中心的替代方法。我们的方法 SeeTok 将文本渲染为图像（视觉文本），并利用预训练的多模态 LLMs 对其进行解释，复用大规模多模态训练中学到的强大的 OCR 和图文对齐能力。在三个不同的语言任务中，SeeTok 在使用更少 4.43 倍 tokens 和减少 70.5% FLOPs 的前提下，表现与或优于子词分词器，并在跨语言泛化能力、对排版噪声的鲁棒性以及语言层次结构方面获得了额外的提升。SeeTok 标志着从符号化分词向类人视觉阅读方式的转变，并朝着更加自然和受认知启发的语言模型迈出了关键一步。",
        "translated_title": "从分词到视觉阅读",
        "label": [],
        "label_reason": "论文聚焦于文本处理和语言模型，非图像像素级质量改善任务。",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出视觉文本表示方法，对低资源语言和计算效率有显著改进。"
    },
    {
        "title": "FedDEAP: Adaptive Dual-Prompt Tuning for Multi-Domain Federated Learning",
        "url": "http://arxiv.org/abs/2510.18837v1",
        "pub_date": "2025-10-21",
        "summary": "Federated learning (FL) enables multiple clients to collaboratively train machine learning models without exposing local data, balancing performance and privacy. However, domain shift and label heterogeneity across clients often hinder the generalization of the aggregated global model. Recently, large-scale vision-language models like CLIP have shown strong zero-shot classification capabilities, raising the question of how to effectively fine-tune CLIP across domains in a federated setting. In this work, we propose an adaptive federated prompt tuning framework, FedDEAP, to enhance CLIP's generalization in multi-domain scenarios. Our method includes the following three key components: (1) To mitigate the loss of domain-specific information caused by label-supervised tuning, we disentangle semantic and domain-specific features in images by using semantic and domain transformation networks with unbiased mappings; (2) To preserve domain-specific knowledge during global prompt aggregation, we introduce a dual-prompt design with a global semantic prompt and a local domain prompt to balance shared and personalized information; (3) To maximize the inclusion of semantic and domain information from images in the generated text features, we align textual and visual representations under the two learned transformations to preserve semantic and domain consistency. Theoretical analysis and extensive experiments on four datasets demonstrate the effectiveness of our method in enhancing the generalization of CLIP for federated image recognition across multiple domains.",
        "translated": "联邦学习（FL）允许多个客户端在不暴露本地数据的情况下协同训练机器学习模型，从而在性能与隐私之间取得平衡。然而，客户端之间的域偏移和标签异质性通常会限制聚合后的全局模型的泛化能力。最近，像 CLIP 这样的大规模视觉-语言模型展现出了强大的零样本分类能力，引发了一个问题：如何在联邦设置下跨域有效地对 CLIP 进行微调。在本研究中，我们提出了一种自适应联邦提示微调框架 FedDEAP，以增强 CLIP 在多域场景中的泛化能力。我们的方法包括以下三个关键组成部分：(1) 为缓解标签监督微调导致的域特定信息丢失，我们通过语义和域变换网络实现图像中语义特征与域特定特征的解耦，并利用无偏映射；(2) 为在全局提示聚合过程中保留域特定知识，我们引入了包含全局语义提示和本地域提示的双提示设计，以平衡共享信息与个性化信息；(3) 为最大化图像中语义和域信息在生成文本特征中的包含，我们在两种学习变换下对齐文本和视觉表示，以保持语义和域一致性。理论分析和在四个数据集上的大量实验表明，我们的方法在增强 CLIP 联邦图像识别的跨域泛化能力方面是有效的。",
        "translated_title": "FedDEAP：多域联邦学习的自适应双提示调优",
        "label": [],
        "label_reason": "论文聚焦联邦学习中的图像分类，不涉及像素级图像恢复或增强。",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "方法基于CLIP进行改进，设计了双提示机制，有一定创新但属于常规迁移应用。"
    },
    {
        "title": "Unifying and Enhancing Graph Transformers via a Hierarchical Mask\n  Framework",
        "url": "http://arxiv.org/abs/2510.18825v1",
        "pub_date": "2025-10-21",
        "summary": "Graph Transformers (GTs) have emerged as a powerful paradigm for graph representation learning due to their ability to model diverse node interactions. However, existing GTs often rely on intricate architectural designs tailored to specific interactions, limiting their flexibility. To address this, we propose a unified hierarchical mask framework that reveals an underlying equivalence between model architecture and attention mask construction. This framework enables a consistent modeling paradigm by capturing diverse interactions through carefully designed attention masks. Theoretical analysis under this framework demonstrates that the probability of correct classification positively correlates with the receptive field size and label consistency, leading to a fundamental design principle: an effective attention mask should ensure both a sufficiently large receptive field and a high level of label consistency. While no single existing mask satisfies this principle across all scenarios, our analysis reveals that hierarchical masks offer complementary strengths, motivating their effective integration. Then, we introduce M3Dphormer, a Mixture-of-Experts-based Graph Transformer with Multi-Level Masking and Dual Attention Computation. M3Dphormer incorporates three theoretically grounded hierarchical masks and employs a bi-level expert routing mechanism to adaptively integrate multi-level interaction information. To ensure scalability, we further introduce a dual attention computation scheme that dynamically switches between dense and sparse modes based on local mask sparsity. Extensive experiments across multiple benchmarks demonstrate that M3Dphormer achieves state-of-the-art performance, validating the effectiveness of our unified framework and model design.",
        "translated": "图神经网络中的图变换器（Graph Transformers, GTs）由于能够建模多样化的节点交互，已成为图表示学习的一种强大范式。然而，现有的 GTs 通常依赖于针对特定交互量身定制的复杂架构设计，限制了其灵活性。为了解决这一问题，我们提出了一种统一的层次化掩码框架，揭示了模型架构与注意力掩码构建之间潜在的等价关系。该框架通过精心设计的注意力掩码捕捉多样化的交互，从而实现一致的建模范式。在该框架下的理论分析表明，正确分类的概率与感受野大小和标签一致性呈正相关，由此引出一个基本的设计原则：有效的注意力掩码应同时确保足够大的感受野和较高的标签一致性。虽然现有的任何单一掩码都无法在所有场景中满足这一原则，但我们的分析表明，层次化掩码具有互补的优势，这促使我们对其进行有效的融合。随后，我们介绍了 M3Dphormer，这是一种基于专家混合（Mixture-of-Experts）的图变换器，结合了多级掩码和双注意力计算。M3Dphormer 集成了三种理论支撑的层次化掩码，并采用双层专家路由机制，以自适应地融合多级交互信息。为了确保可扩展性，我们进一步引入了一种双注意力计算方案，根据局部掩码的稀疏性在密集模式和稀疏模式之间动态切换。在多个基准数据集上的大量实验表明，M3Dphormer 取得了最先进的性能，验证了我们统一框架和模型设计的有效性。",
        "translated_title": "通过分层掩码框架统一并增强图变换器",
        "label": [],
        "label_reason": "论文聚焦图神经网络设计，不涉及图像像素级处理",
        "relevance_score": 1,
        "novelty_score": 6,
        "novelty_reason": "提出分层掩码框架和混合专家机制，有一定改进"
    },
    {
        "title": "SAM 2++: Tracking Anything at Any Granularity",
        "url": "http://arxiv.org/abs/2510.18822v1",
        "pub_date": "2025-10-21",
        "summary": "Video tracking aims at finding the specific target in subsequent frames given its initial state. Due to the varying granularity of target states across different tasks, most existing trackers are tailored to a single task and heavily rely on custom-designed modules within the individual task, which limits their generalization and leads to redundancy in both model design and parameters. To unify video tracking tasks, we present SAM 2++, a unified model towards tracking at any granularity, including masks, boxes, and points. First, to extend target granularity, we design task-specific prompts to encode various task inputs into general prompt embeddings, and a unified decoder to unify diverse task results into a unified form pre-output. Next, to satisfy memory matching, the core operation of tracking, we introduce a task-adaptive memory mechanism that unifies memory across different granularities. Finally, we introduce a customized data engine to support tracking training at any granularity, producing a large and diverse video tracking dataset with rich annotations at three granularities, termed Tracking-Any-Granularity, which represents a comprehensive resource for training and benchmarking on unified tracking. Comprehensive experiments on multiple benchmarks confirm that SAM 2++ sets a new state of the art across diverse tracking tasks at different granularities, establishing a unified and robust tracking framework.",
        "translated": "视频跟踪旨在在给定目标初始状态的情况下，于后续帧中定位该特定目标。由于不同任务中目标状态的粒度存在差异，大多数现有的跟踪器仅适用于单一任务，并高度依赖任务内部定制设计的模块，这限制了其泛化能力，并导致模型设计与参数上的冗余。为了统一视频跟踪任务，我们提出 SAM 2++，一个面向任意粒度跟踪的统一模型，包括掩膜、框和点。首先，为了扩展目标的粒度，我们设计了任务特定的提示，将各种任务输入编码为通用的提示嵌入，并引入统一的解码器，将多样化的任务结果统一为预输出的统一形式。其次，为满足跟踪中的核心操作——记忆匹配，我们引入了一个任务自适应的记忆机制，以统一不同粒度下的记忆信息。最后，我们提出一个定制的数据引擎，以支持任意粒度下的跟踪训练，生成了一个包含三种粒度丰富标注的大规模且多样化的视频跟踪数据集，称为 Tracking-Any-Granularity，该数据集为统一跟踪的训练与基准测试提供了全面的资源。在多个基准测试上的综合性实验表明，SAM 2++ 在不同粒度的多样化跟踪任务中均达到了新的最先进性能，建立了一个统一且鲁棒的跟踪框架。",
        "translated_title": "SAM 2++：以任意粒度追踪任何对象",
        "label": [],
        "label_reason": "论文聚焦视频跟踪，不属于图像像素级恢复或增强任务。",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出统一视频跟踪框架，改进了任务适应性和泛化能力。"
    },
    {
        "title": "An Explainable Hybrid AI Framework for Enhanced Tuberculosis and Symptom\n  Detection",
        "url": "http://arxiv.org/abs/2510.18819v1",
        "pub_date": "2025-10-21",
        "summary": "Tuberculosis remains a critical global health issue, particularly in resource-limited and remote areas. Early detection is vital for treatment, yet the lack of skilled radiologists underscores the need for artificial intelligence (AI)-driven screening tools. Developing reliable AI models is challenging due to the necessity for large, high-quality datasets, which are costly to obtain. To tackle this, we propose a teacher--student framework which enhances both disease and symptom detection on chest X-rays by integrating two supervised heads and a self-supervised head. Our model achieves an accuracy of 98.85% for distinguishing between COVID-19, tuberculosis, and normal cases, and a macro-F1 score of 90.09% for multilabel symptom detection, significantly outperforming baselines. The explainability assessments also show the model bases its predictions on relevant anatomical features, demonstrating promise for deployment in clinical screening and triage settings.",
        "translated": "结核病仍然是一个重要的全球性健康问题，尤其是在资源匮乏和偏远地区。早期检测对于治疗至关重要，然而，专业放射科医生的缺乏凸显了需要基于人工智能（AI）的筛查工具。由于开发可靠的AI模型需要大量的高质量数据集，而这些数据集的获取成本较高，因此这一过程面临挑战。为了解决这一问题，我们提出了一种教师-学生框架，通过整合两个监督头和一个自监督头，以提升胸部X光图像中疾病和症状的检测能力。我们的模型在区分新冠肺炎、结核病和正常病例方面实现了98.85%的准确率，在多标签症状检测中达到了90.09%的宏F1分数，显著优于基线方法。可解释性评估也表明，该模型基于相关的解剖特征进行预测，展现出在临床筛查和分级诊疗中的应用潜力。",
        "translated_title": "一种用于增强结核病和症状检测的可解释混合人工智能框架",
        "label": [],
        "label_reason": "论文主要关注医学图像的疾病分类，不涉及像素级图像恢复或增强",
        "relevance_score": 2,
        "novelty_score": 5,
        "novelty_reason": "提出了一种教师-学生框架，但方法改进较为常规"
    },
    {
        "title": "A Geometric Approach to Steerable Convolutions",
        "url": "http://arxiv.org/abs/2510.18813v1",
        "pub_date": "2025-10-21",
        "summary": "In contrast to the somewhat abstract, group theoretical approach adopted by many papers, our work provides a new and more intuitive derivation of steerable convolutional neural networks in $d$ dimensions. This derivation is based on geometric arguments and fundamental principles of pattern matching. We offer an intuitive explanation for the appearance of the Clebsch--Gordan decomposition and spherical harmonic basis functions. Furthermore, we suggest a novel way to construct steerable convolution layers using interpolation kernels that improve upon existing implementation, and offer greater robustness to noisy data.",
        "translated": "与许多论文所采用的略显抽象的群论方法不同，我们的工作提供了一种新的、更直观的推导方法，用于 $d$ 维的可操控卷积神经网络。该推导基于几何论证和模式匹配的基本原理。我们对 Clebsch--Gordan 分解和球谐基函数的出现给出了直观的解释。此外，我们提出了一种新颖的构建可操控卷积层的方法，该方法使用插值核，改进了现有的实现方式，并对噪声数据表现出更强的鲁棒性。",
        "translated_title": "几何可转向卷积方法",
        "label": [
            "图像去噪"
        ],
        "label_reason": "论文探讨 steerable 卷积的几何方法，可能用于提升图像去噪的鲁棒性。",
        "relevance_score": 5,
        "novelty_score": 7,
        "novelty_reason": "提出基于几何和插值核的新 steerable 卷积实现方式，具有一定创新性。"
    },
    {
        "title": "ProCLIP: Progressive Vision-Language Alignment via LLM-based Embedder",
        "url": "http://arxiv.org/abs/2510.18795v1",
        "pub_date": "2025-10-21",
        "summary": "The original CLIP text encoder is limited by a maximum input length of 77 tokens, which hampers its ability to effectively process long texts and perform fine-grained semantic understanding. In addition, the CLIP text encoder lacks support for multilingual inputs. All these limitations significantly restrict its applicability across a broader range of tasks. Recent studies have attempted to replace the CLIP text encoder with an LLM-based embedder to enhance its ability in processing long texts, multilingual understanding, and fine-grained semantic comprehension. However, because the representation spaces of LLMs and the vision-language space of CLIP are pretrained independently without alignment priors, direct alignment using contrastive learning can disrupt the intrinsic vision-language alignment in the CLIP image encoder, leading to an underutilization of the knowledge acquired during pre-training. To address this challenge, we propose ProCLIP, a curriculum learning-based progressive vision-language alignment framework to effectively align the CLIP image encoder with an LLM-based embedder. Specifically, ProCLIP first distills knowledge from CLIP's text encoder into the LLM-based embedder to leverage CLIP's rich pretrained knowledge while establishing initial alignment between the LLM embedder and CLIP image encoder. Subsequently, ProCLIP further aligns the CLIP image encoder with the LLM-based embedder through image-text contrastive tuning, employing self-distillation regularization to avoid overfitting. To achieve a more effective alignment, instance semantic alignment loss and embedding structure alignment loss are employed during representation inheritance and contrastive tuning. The Code is available at https://github.com/VisionXLab/ProCLIP",
        "translated": "原始的 CLIP 文本编码器受到最大输入长度为 77 个 token 的限制，这阻碍了其对长文本进行有效处理和实现细粒度语义理解的能力。此外，CLIP 文本编码器不支持多语言输入。所有这些限制显著制约了其在更广泛任务中的适用性。近期的研究尝试将 CLIP 文本编码器替换为基于大语言模型（LLM）的嵌入器，以增强其处理长文本、多语言理解和细粒度语义理解的能力。然而，由于 LLMs 和 CLIP 的视觉-语言空间是独立预训练的，且没有对齐先验知识，直接使用对比学习进行对齐可能会破坏 CLIP 图像编码器中固有的视觉-语言对齐关系，从而导致预训练阶段获得的知识未能充分利用。为了解决这一问题，我们提出 ProCLIP，一种基于课程学习的渐进式视觉-语言对齐框架，以有效地将 CLIP 图像编码器与基于 LLM 的嵌入器对齐。具体来说，ProCLIP 首先将 CLIP 文本编码器的知识蒸馏到基于 LLM 的嵌入器中，从而利用 CLIP 丰富的预训练知识，同时在 LLM 嵌入器和 CLIP 图像编码器之间建立初步对齐。随后，ProCLIP 通过图像-文本对比调优进一步对齐 CLIP 图像编码器与基于 LLM 的嵌入器，并采用自蒸馏正则化方法避免过拟合。为了实现更有效的对齐，在表示继承和对比调优过程中分别引入了实例语义对齐损失和嵌入结构对齐损失。代码已发布于 https://github.com/VisionXLab/ProCLIP",
        "translated_title": "ProCLIP：基于大语言模型的嵌入器实现渐进式视觉-语言对齐",
        "label": [],
        "label_reason": "论文主要涉及视觉-语言对齐，不属于低层图像处理任务。",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出基于课程学习的渐进式视觉-语言对齐框架，具有一定创新性。"
    },
    {
        "title": "Rebellious Student: A Complementary Learning Framework for Background\n  Feature Enhancement in Hyperspectral Anomaly Detection",
        "url": "http://arxiv.org/abs/2510.18781v1",
        "pub_date": "2025-10-21",
        "summary": "A recent class of hyperspectral anomaly detection methods that can be trained once on background datasets and then universally deployed -- without per-scene retraining or parameter tuning -- has demonstrated remarkable efficiency and robustness. Building upon this paradigm, we focus on the integration of spectral and spatial cues and introduce a novel \"Rebellious Student\" framework for complementary feature learning. Unlike conventional teacher-student paradigms driven by imitation, our method intentionally trains the spatial branch to diverge from the spectral teacher, thereby learning complementary spatial patterns that the teacher fails to capture. A two-stage learning strategy is adopted: (1) a spectral enhancement network is first trained via reverse distillation to obtain robust background spectral representations; and (2) a spatial network -- the rebellious student -- is subsequently optimized using decorrelation losses that enforce feature orthogonality while maintaining reconstruction fidelity to avoid irrelevant noise. Once trained, the framework enhances both spectral and spatial background features, enabling parameter-free and training-free anomaly detection when paired with conventional detectors. Extensive experiments on the HAD100 benchmark show substantial improvements over several established baselines with minimal computational overhead, confirming the effectiveness and generality of the proposed complementary learning paradigm. Our code is publicly available at https://github.com/xjpp2016/FERS.",
        "translated": "近年来，一类高光谱异常检测方法在背景数据集上可以一次性训练，然后无需按场景重新训练或参数调优即可通用部署，表现出显著的效率和鲁棒性。在这一范式的基础上，我们专注于光谱与空域线索的融合，并提出了一种新颖的“Rebellious Student”框架，用于互补特征学习。与传统依赖模仿的师生范式不同，我们的方法有意训练空域分支与光谱教师模型产生分歧，从而学习教师模型未能捕捉的互补空域模式。我们采用了两阶段学习策略：(1) 首先通过反向蒸馏训练光谱增强网络，以获得稳健的背景光谱表示；(2) 随后使用去相关损失对空域网络（即叛逆学生）进行优化，该损失在保持重建保真度的同时强制特征正交，以避免无关噪声的干扰。一旦训练完成，该框架即可增强背景的光谱与空域特征，使得在与传统检测器配对使用时，能够实现无参数且无需训练的异常检测。在 HAD100 基准上的大量实验表明，与多个已有基线方法相比，该方法在计算开销最小的情况下实现了显著性能提升，验证了所提出的互补学习范式的有效性与通用性。我们的代码已公开在 https://github.com/xjpp2016/FERS。",
        "translated_title": "反叛学生：一种用于高光谱异常检测中背景特征增强的互补学习框架",
        "label": [
            "遥感图像复原",
            "图像增强"
        ],
        "label_reason": "论文涉及高光谱异常检测中的背景特征增强，属于遥感图像复原和增强范畴。",
        "relevance_score": 6,
        "novelty_score": 7,
        "novelty_reason": "提出新颖的互补学习框架，通过反向蒸馏和去相关损失实现特征增强。"
    },
    {
        "title": "UltraGen: High-Resolution Video Generation with Hierarchical Attention",
        "url": "http://arxiv.org/abs/2510.18775v1",
        "pub_date": "2025-10-21",
        "summary": "Recent advances in video generation have made it possible to produce visually compelling videos, with wide-ranging applications in content creation, entertainment, and virtual reality. However, most existing diffusion transformer based video generation models are limited to low-resolution outputs (&lt;=720P) due to the quadratic computational complexity of the attention mechanism with respect to the output width and height. This computational bottleneck makes native high-resolution video generation (1080P/2K/4K) impractical for both training and inference. To address this challenge, we present UltraGen, a novel video generation framework that enables i) efficient and ii) end-to-end native high-resolution video synthesis. Specifically, UltraGen features a hierarchical dual-branch attention architecture based on global-local attention decomposition, which decouples full attention into a local attention branch for high-fidelity regional content and a global attention branch for overall semantic consistency. We further propose a spatially compressed global modeling strategy to efficiently learn global dependencies, and a hierarchical cross-window local attention mechanism to reduce computational costs while enhancing information flow across different local windows. Extensive experiments demonstrate that UltraGen can effectively scale pre-trained low-resolution video models to 1080P and even 4K resolution for the first time, outperforming existing state-of-the-art methods and super-resolution based two-stage pipelines in both qualitative and quantitative evaluations.",
        "translated": "近期在视频生成领域的进展使得生成视觉上具有吸引力的视频成为可能，并在内容创作、娱乐和虚拟现实等领域具有广泛的应用前景。然而，由于注意力机制的计算复杂度与输出宽度和高度呈平方关系，大多数现有的基于扩散变换器的视频生成模型在输出分辨率上受到限制（<=720P）。这种计算瓶颈使得在训练和推理阶段实现原生的高分辨率视频生成（1080P/2K/4K）变得不切实际。为了解决这一挑战，我们提出了 UltraGen，一种新颖的视频生成框架，能够在 i) 高效性和 ii) 原生高分辨率视频的端到端合成方面取得突破。具体而言，UltraGen 采用基于全局-局部注意力分解的层次化双分支注意力架构，将完整的注意力机制拆分为用于高保真区域内容的局部注意力分支和用于整体语义一致性的全局注意力分支。我们进一步提出了一种空间压缩的全局建模策略，以高效地学习全局依赖关系，并引入了一种层次化的跨窗口局部注意力机制，以在降低计算成本的同时增强不同局部窗口之间的信息流动。大量实验表明，UltraGen 能够首次有效地将预训练的低分辨率视频模型扩展至 1080P，甚至 4K 分辨率，在定性和定量评估中均优于现有的最先进方法以及基于超分辨率的两阶段流水线方法。",
        "translated_title": "UltraGen：基于层次注意力的高分辨率视频生成",
        "label": [],
        "label_reason": "论文聚焦视频生成，非图像像素级恢复任务",
        "relevance_score": 3,
        "novelty_score": 8,
        "novelty_reason": "提出分层注意力机制，显著提升高分辨率视频生成效率"
    },
    {
        "title": "Detection and Simulation of Urban Heat Islands Using a Fine-Tuned\n  Geospatial Foundation Model for Microclimate Impact Prediction",
        "url": "http://arxiv.org/abs/2510.18773v1",
        "pub_date": "2025-10-21",
        "summary": "As urbanization and climate change progress, urban heat island effects are becoming more frequent and severe. To formulate effective mitigation plans, cities require detailed air temperature data, yet conventional machine learning models with limited data often produce inaccurate predictions, particularly in underserved areas. Geospatial foundation models trained on global unstructured data offer a promising alternative by demonstrating strong generalization and requiring only minimal fine-tuning. In this study, an empirical ground truth of urban heat patterns is established by quantifying cooling effects from green spaces and benchmarking them against model predictions to evaluate the model's accuracy. The foundation model is subsequently fine-tuned to predict land surface temperatures under future climate scenarios, and its practical value is demonstrated through a simulated inpainting that highlights its role for mitigation support. The results indicate that foundation models offer a powerful way for evaluating urban heat island mitigation strategies in data-scarce regions to support more climate-resilient cities.",
        "translated": "随着城市化和气候变化的加剧，城市热岛效应变得越来越频繁和严重。为制定有效的缓解计划，城市需要详细的空气温度数据，然而基于有限数据的传统机器学习模型常常产生不准确的预测，特别是在数据匮乏的地区。在全局非结构化数据上训练的地理空间基础模型提供了一个有前景的替代方案，因为它们表现出良好的泛化能力，且仅需少量微调即可使用。本研究中，通过量化绿地的降温效应，并将其与模型预测进行基准比较，建立了一个城市热岛模式的经验性真实数据，以评估模型的准确性。随后对基础模型进行微调，以预测未来气候情景下的地表温度，并通过模拟补全展示了其在缓解支持中的实际价值。结果表明，基础模型为在数据稀缺地区评估城市热岛缓解策略提供了一种有力的方法，有助于建设更具气候韧性的城市。",
        "translated_title": "基于微气候影响预测的微调地理空间基础模型用于城市热岛的检测与模拟",
        "label": [],
        "label_reason": "不属于低级图像处理，主要涉及地理空间建模和气候预测",
        "relevance_score": 3,
        "novelty_score": 6,
        "novelty_reason": "对基础模型进行微调用于模拟城市热岛效应，具有一定新颖性"
    },
    {
        "title": "Seg the HAB: Language-Guided Geospatial Algae Bloom Reasoning and\n  Segmentation",
        "url": "http://arxiv.org/abs/2510.18751v1",
        "pub_date": "2025-10-21",
        "summary": "Climate change is intensifying the occurrence of harmful algal bloom (HAB), particularly cyanobacteria, which threaten aquatic ecosystems and human health through oxygen depletion, toxin release, and disruption of marine biodiversity. Traditional monitoring approaches, such as manual water sampling, remain labor-intensive and limited in spatial and temporal coverage. Recent advances in vision-language models (VLMs) for remote sensing have shown potential for scalable AI-driven solutions, yet challenges remain in reasoning over imagery and quantifying bloom severity. In this work, we introduce ALGae Observation and Segmentation (ALGOS), a segmentation-and-reasoning system for HAB monitoring that combines remote sensing image understanding with severity estimation. Our approach integrates GeoSAM-assisted human evaluation for high-quality segmentation mask curation and fine-tunes vision language model on severity prediction using the Cyanobacteria Aggregated Manual Labels (CAML) from NASA. Experiments demonstrate that ALGOS achieves robust performance on both segmentation and severity-level estimation, paving the way toward practical and automated cyanobacterial monitoring systems.",
        "translated": "气候变化加剧了有害藻类水华（HAB）的出现，特别是蓝藻，它们通过耗氧、释放毒素和破坏海洋生物多样性而威胁水生生态系统和人类健康。传统的监测方法，如人工采水样，仍然劳动密集且在空间和时间覆盖范围上受到限制。最近在遥感领域中视觉-语言模型（VLM）的进展表明了可扩展的AI驱动解决方案的潜力，但如何对图像进行推理和量化水华严重程度仍是挑战。在此工作中，我们提出了ALGae Observation and Segmentation（ALGOS），一个用于HAB监测的分割与推理系统，该系统结合了遥感图像理解和严重程度估计。我们的方法整合了由GeoSAM辅助的人工评估，以生成高质量的分割掩码，并利用美国宇航局（NASA）提供的蓝藻聚集人工标签（Cyanobacteria Aggregated Manual Labels, CAML）对视觉语言模型进行严重程度预测的微调。实验表明，ALGOS在分割和严重程度估计方面均表现出稳健的性能，为实用且自动化的蓝藻监测系统铺平了道路。",
        "translated_title": "Seg the HAB：语言引导的地理空间藻华推理与分割",
        "label": [
            "遥感图像复原",
            "图像分割"
        ],
        "label_reason": "涉及遥感图像分割，但主要用于地理场景理解",
        "relevance_score": 5,
        "novelty_score": 7,
        "novelty_reason": "结合 GeoSAM 和语言模型，改进遥感 HAB 分析"
    },
    {
        "title": "SEAL: Semantic-Aware Hierarchical Learning for Generalized Category\n  Discovery",
        "url": "http://arxiv.org/abs/2510.18740v1",
        "pub_date": "2025-10-21",
        "summary": "This paper investigates the problem of Generalized Category Discovery (GCD). Given a partially labelled dataset, GCD aims to categorize all unlabelled images, regardless of whether they belong to known or unknown classes. Existing approaches typically depend on either single-level semantics or manually designed abstract hierarchies, which limit their generalizability and scalability. To address these limitations, we introduce a SEmantic-aware hierArchical Learning framework (SEAL), guided by naturally occurring and easily accessible hierarchical structures. Within SEAL, we propose a Hierarchical Semantic-Guided Soft Contrastive Learning approach that exploits hierarchical similarity to generate informative soft negatives, addressing the limitations of conventional contrastive losses that treat all negatives equally. Furthermore, a Cross-Granularity Consistency (CGC) module is designed to align the predictions from different levels of granularity. SEAL consistently achieves state-of-the-art performance on fine-grained benchmarks, including the SSB benchmark, Oxford-Pet, and the Herbarium19 dataset, and further demonstrates generalization on coarse-grained datasets. Project page: https://visual-ai.github.io/seal/",
        "translated": "本文研究了广义类别发现（Generalized Category Discovery, GCD）问题。在给定一个部分标注的数据集的情况下，GCD 的目标是将所有未标注的图像进行分类，无论它们属于已知类别还是未知类别。现有方法通常依赖于单一层级的语义或手动设计的抽象层次结构，这限制了它们的泛化能力和可扩展性。为了解决这些局限，我们引入了一种语义感知的层次化学习框架（SEmantic-aware hierArchical Learning framework, SEAL），该框架以自然存在且易于获取的层次化结构为指导。在 SEAL 中，我们提出了一种层次语义引导的软对比学习（Hierarchical Semantic-Guided Soft Contrastive Learning）方法，该方法利用层次相似性生成具有信息量的软负样本，从而解决了传统对比损失函数将所有负样本一视同仁的局限。此外，我们设计了一个跨粒度一致性（Cross-Granularity Consistency, CGC）模块，用于对齐不同粒度层级的预测结果。SEAL 在细粒度基准数据集上始终实现了最先进的性能，包括 SSB 基准、Oxford-Pet 以及 Herbarium19 数据集，并在粗粒度数据集上进一步展示了其泛化能力。项目主页：https://visual-ai.github.io/seal/",
        "translated_title": "SEAL：面向广义类别发现的语义感知层次化学习",
        "label": [],
        "label_reason": "论文聚焦分类任务，不涉及像素级图像恢复或增强",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出层次化语义学习框架，有一定方法改进但非本质创新"
    },
    {
        "title": "Moving Light Adaptive Colonoscopy Reconstruction via\n  Illumination-Attenuation-Aware 3D Gaussian Splatting",
        "url": "http://arxiv.org/abs/2510.18739v1",
        "pub_date": "2025-10-21",
        "summary": "3D Gaussian Splatting (3DGS) has emerged as a pivotal technique for real-time view synthesis in colonoscopy, enabling critical applications such as virtual colonoscopy and lesion tracking. However, the vanilla 3DGS assumes static illumination and that observed appearance depends solely on viewing angle, which causes incompatibility with the photometric variations in colonoscopic scenes induced by dynamic light source/camera. This mismatch forces most 3DGS methods to introduce structure-violating vaporous Gaussian blobs between the camera and tissues to compensate for illumination attenuation, ultimately degrading the quality of 3D reconstructions. Previous works only consider the illumination attenuation caused by light distance, ignoring the physical characters of light source and camera. In this paper, we propose ColIAGS, an improved 3DGS framework tailored for colonoscopy. To mimic realistic appearance under varying illumination, we introduce an Improved Appearance Modeling with two types of illumination attenuation factors, which enables Gaussians to adapt to photometric variations while preserving geometry accuracy. To ensure the geometry approximation condition of appearance modeling, we propose an Improved Geometry Modeling using high-dimensional view embedding to enhance Gaussian geometry attribute prediction. Furthermore, another cosine embedding input is leveraged to generate illumination attenuation solutions in an implicit manner. Comprehensive experimental results on standard benchmarks demonstrate that our proposed ColIAGS achieves the dual capabilities of novel view synthesis and accurate geometric reconstruction. It notably outperforms other state-of-the-art methods by achieving superior rendering fidelity while significantly reducing Depth MSE. Code will be available.",
        "translated": "3D Gaussian Splatting（3DGS）已成为结肠镜检查中实时视图合成的关键技术，使虚拟结肠镜和病灶跟踪等重要应用成为可能。然而，原始的3DGS假设照明是静态的，且所观察到的外观仅依赖于视角，这导致其与由动态光源/相机引起的结肠镜场景中的光度变化不兼容。这种不匹配迫使大多数3DGS方法在相机和组织之间引入破坏结构的雾状高斯斑点，以补偿照明衰减，最终降低了3D重建的质量。之前的工作只考虑了由光源距离引起的照明衰减，而忽略了光源和相机的物理特性。在本文中，我们提出ColIAGS，这是一种专为结肠镜检查改进的3DGS框架。为了在不同照明条件下模拟逼真的外观，我们引入了一种改进的外观建模方法，包含两种类型的照明衰减因子，这使得高斯模型能够适应光度变化，同时保持几何精度。为确保外观建模的几何逼近条件，我们提出了一种改进的几何建模方法，使用高维视角嵌入来增强高斯几何属性的预测能力。此外，还利用另一个余弦嵌入输入以隐式方式生成照明衰减解。在标准基准上的全面实验结果表明，我们提出的ColIAGS实现了新视角合成与精确几何重建的双重能力。它在显著降低深度均方误差的同时，实现了优于其他最先进方法的渲染保真度。代码将公开。",
        "translated_title": "通过光照衰减感知的3D高斯点绘实现动态光照自适应结肠镜重建",
        "label": [
            "医学图像增强",
            "图像恢复",
            "多帧/视频图像恢复"
        ],
        "label_reason": "论文针对内窥镜场景的光照变化进行3D重建优化，属于医学图像增强与图像恢复",
        "relevance_score": 8,
        "novelty_score": 7,
        "novelty_reason": "引入光照衰减因子与改进的几何建模，对3DGS方法有一定创新"
    },
    {
        "title": "IF-VidCap: Can Video Caption Models Follow Instructions?",
        "url": "http://arxiv.org/abs/2510.18726v1",
        "pub_date": "2025-10-21",
        "summary": "Although Multimodal Large Language Models (MLLMs) have demonstrated proficiency in video captioning, practical applications require captions that follow specific user instructions rather than generating exhaustive, unconstrained descriptions. Current benchmarks, however, primarily assess descriptive comprehensiveness while largely overlooking instruction-following capabilities. To address this gap, we introduce IF-VidCap, a new benchmark for evaluating controllable video captioning, which contains 1,400 high-quality samples. Distinct from existing video captioning or general instruction-following benchmarks, IF-VidCap incorporates a systematic framework that assesses captions on two dimensions: format correctness and content correctness. Our comprehensive evaluation of over 20 prominent models reveals a nuanced landscape: despite the continued dominance of proprietary models, the performance gap is closing, with top-tier open-source solutions now achieving near-parity. Furthermore, we find that models specialized for dense captioning underperform general-purpose MLLMs on complex instructions, indicating that future work should simultaneously advance both descriptive richness and instruction-following fidelity.",
        "translated": "尽管多模态大语言模型（MLLMs）在视频字幕生成方面表现出色，但实际应用中需要的是遵循特定用户指令的字幕，而不是生成详尽且无约束的描述。然而，目前的基准测试主要评估描述的全面性，而几乎忽视了对指令遵循能力的考量。为填补这一空白，我们引入了 IF-VidCap，一个新的用于评估可控视频字幕生成的基准，包含 1,400 个高质量样本。与现有的视频字幕生成或通用指令遵循基准不同，IF-VidCap 采用了一个系统化的框架，从两个维度对字幕进行评估：格式正确性和内容正确性。我们对 20 多个主流模型进行了全面评估，结果揭示了一个复杂而细致的图景：尽管专有模型依然占据主导地位，但性能差距正在缩小，顶级开源解决方案如今已接近其水平。此外，我们发现专门用于密集字幕的模型在处理复杂指令时表现不如通用型 MLLMs，这表明未来的工作应同时推进描述的丰富性和指令遵循的准确性。",
        "translated_title": "IF-VidCap: 视频标题模型能否遵循指令？",
        "label": [],
        "label_reason": "论文聚焦视频描述生成，不涉及图像像素级质量恢复或增强",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出了新的可控视频描述基准IF-VidCap，具有系统性评估框架"
    },
    {
        "title": "SSD: Spatial-Semantic Head Decoupling for Efficient Autoregressive Image\n  Generation",
        "url": "http://arxiv.org/abs/2510.18716v1",
        "pub_date": "2025-10-21",
        "summary": "Autoregressive image generation models like Janus-Pro produce high-quality images, but at the significant cost of high memory and ever-growing computational demands due to the large number of visual tokens. While KV cache compression has been extensively studied in language modeling, it still remains largely unexplored for the image generation domain. In this work, we begin by identifying a distinct and prominent attention phenomenon, which we term spatial locality and emergent semantic sink. To leverage this key insight, we introduce a novel KV cache compression framework. Specifically, we compress the KV cache for all visual tokens by adaptively decoupling attention heads into two separate types: for spatial-locality heads, our method maintains a short recent token window; for semantic-sink heads, it strategically preserves a compact set of highly-attended tokens. Our extensive experiments demonstrate that the proposed method achieves a 5$\\times$ reduction in memory usage and a notable 6.6$\\times$ speedup in overall throughput with only minimal visual quality loss, thereby enabling highly efficient native autoregressive image generation on resource-constrained hardware.",
        "translated": "Janus-Pro 等自回归图像生成模型能够生成高质量图像，但由于视觉 token 数量庞大，其代价是高昂的内存占用和不断增长的计算需求。尽管在语言建模领域对键值缓存（KV cache）压缩已进行了广泛研究，但在图像生成领域中，这一技术仍鲜有探索。在本研究中，我们首先识别出一种独特且显著的注意力现象，我们称之为“空间局部性”和“涌现语义沉降”。为利用这一关键发现，我们引入了一种新颖的 KV cache 压缩框架。具体而言，我们通过自适应地将注意力头解耦为两种类型来压缩所有视觉 token 的 KV cache：对于空间局部性头，我们的方法保留一个短的最近 token 窗口；对于语义沉降头，则策略性地保留一组被高度关注的 token。大量实验表明，所提出的方法在仅造成极小视觉质量损失的情况下，实现了 5$\\times$ 的内存使用减少和整体吞吐量 6.6$\\times$ 的加速，从而使得在资源受限的硬件上实现高效原生自回归图像生成成为可能。",
        "translated_title": "SSD：用于高效自回归图像生成的空域-语义头解耦",
        "label": [],
        "label_reason": "论文聚焦于图像生成效率，非图像恢复或增强任务",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出KV缓存压缩框架，提升生成效率具创新性"
    },
    {
        "title": "PLANA3R: Zero-shot Metric Planar 3D Reconstruction via Feed-Forward\n  Planar Splatting",
        "url": "http://arxiv.org/abs/2510.18714v1",
        "pub_date": "2025-10-21",
        "summary": "This paper addresses metric 3D reconstruction of indoor scenes by exploiting their inherent geometric regularities with compact representations. Using planar 3D primitives - a well-suited representation for man-made environments - we introduce PLANA3R, a pose-free framework for metric Planar 3D Reconstruction from unposed two-view images. Our approach employs Vision Transformers to extract a set of sparse planar primitives, estimate relative camera poses, and supervise geometry learning via planar splatting, where gradients are propagated through high-resolution rendered depth and normal maps of primitives. Unlike prior feedforward methods that require 3D plane annotations during training, PLANA3R learns planar 3D structures without explicit plane supervision, enabling scalable training on large-scale stereo datasets using only depth and normal annotations. We validate PLANA3R on multiple indoor-scene datasets with metric supervision and demonstrate strong generalization to out-of-domain indoor environments across diverse tasks under metric evaluation protocols, including 3D surface reconstruction, depth estimation, and relative pose estimation. Furthermore, by formulating with planar 3D representation, our method emerges with the ability for accurate plane segmentation. The project page is available at https://lck666666.github.io/plana3r",
        "translated": "本文通过利用室内场景固有的几何规律并采用紧凑表示方法，研究了度量意义上的三维重建问题。我们引入了 PLANA3R，这是一个用于从无姿态的双视角图像中进行度量平面三维重建的无姿态框架。该方法采用视觉变换器（Vision Transformers）提取一组稀疏的平面基元（planar primitives），估计相对相机姿态，并通过平面溅射（planar splatting）监督几何学习，其中梯度通过基元的高分辨率渲染深度图和法线图进行传播。与以往需要在训练过程中使用三维平面标注的前馈方法不同，PLANA3R 在没有显式平面监督的情况下学习三维平面结构，从而仅利用深度和法线标注即可在大规模立体数据集上进行可扩展的训练。我们在多个具有度量监督的室内场景数据集上验证了 PLANA3R，并在多种任务中展示了其对域外室内环境的强泛化能力，包括三维表面重建、深度估计和相对姿态估计。此外，通过采用平面三维表示方法，我们的方法还具备了准确的平面分割能力。项目页面见 https://lck666666.github.io/plana3r",
        "translated_title": "PLANA3R：通过前馈平面泼溅实现零样本度量平面三维重建",
        "label": [
            "3D重建"
        ],
        "label_reason": "方法涉及3D重建但非图像像素级恢复任务",
        "relevance_score": 5,
        "novelty_score": 7,
        "novelty_reason": "提出一种无需显式平面监督的3D平面重建方法"
    },
    {
        "title": "Is This Tracker On? A Benchmark Protocol for Dynamic Tracking",
        "url": "http://arxiv.org/abs/2510.19819v1",
        "pub_date": "2025-10-22",
        "summary": "We introduce ITTO, a challenging new benchmark suite for evaluating and diagnosing the capabilities and limitations of point tracking methods. Our videos are sourced from existing datasets and egocentric real-world recordings, with high-quality human annotations collected through a multi-stage pipeline. ITTO captures the motion complexity, occlusion patterns, and object diversity characteristic of real-world scenes -- factors that are largely absent in current benchmarks. We conduct a rigorous analysis of state-of-the-art tracking methods on ITTO, breaking down performance along key axes of motion complexity. Our findings reveal that existing trackers struggle with these challenges, particularly in re-identifying points after occlusion, highlighting critical failure modes. These results point to the need for new modeling approaches tailored to real-world dynamics. We envision ITTO as a foundation testbed for advancing point tracking and guiding the development of more robust tracking algorithms.",
        "translated": "我们引入了 ITTO，这是一个具有挑战性的新基准套件，用于评估和诊断点跟踪方法的能力与局限性。我们的视频来源于现有的数据集和以第一视角拍摄的真实世界记录，并通过多阶段流程收集高质量的人工标注。ITTO 捕捉了真实场景中运动的复杂性、遮挡模式和物体的多样性——这些因素在当前的基准测试中大多缺失。我们在 ITTO 上对最先进的跟踪方法进行了严格分析，并沿着运动复杂性的关键维度对性能进行了细分。我们的研究结果表明，现有跟踪器在应对这些挑战方面存在困难，尤其是在遮挡后重新识别点方面，突显了关键的失效模式。这些结果表明，有必要提出新的建模方法以适应真实世界的动态特性。我们期望 ITTO 成为推动点跟踪技术发展和指导更鲁棒跟踪算法设计的基础测试平台。",
        "translated_title": "这是追踪器开启的吗？一个动态追踪的基准协议",
        "label": [],
        "label_reason": "论文聚焦动态跟踪，不属于图像像素级恢复或增强任务",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出了新的基准协议，但跟踪方法改进较为常规"
    },
    {
        "title": "olmOCR 2: Unit Test Rewards for Document OCR",
        "url": "http://arxiv.org/abs/2510.19817v1",
        "pub_date": "2025-10-22",
        "summary": "We present olmOCR 2, the latest in our family of powerful OCR systems for converting digitized print documents, like PDFs, into clean, naturally ordered plain text. olmOCR 2 is powered by olmOCR-2-7B-1025, a specialized, 7B vision language model (VLM) trained using reinforcement learning with verifiable rewards (RLVR), where our rewards are a diverse set of binary unit tests. To scale unit test creation, we develop a pipeline for generating synthetic documents with diverse and challenging layouts, known ground-truth HTML source code, and extracted test cases. We show that RL training on these test cases results in state-of-the-art performance on olmOCR-Bench, our English-language OCR benchmark, with the largest improvements in math formula conversion, table parsing, and multi-column layouts compared to previous versions. We release our model, data and code under permissive open licenses.",
        "translated": "我们提出了 olmOCR 2，这是我们的强大光学字符识别（OCR）系统家族中用于将数字化的印刷文档（如 PDF）转换为干净、自然排序的纯文本的最新版本。olmOCR 2 由 olmOCR-2-7B-1025 驱动，这是一个专用的 7B 视觉语言模型（VLM），使用可验证奖励的强化学习（RLVR）进行训练，其中我们的奖励是一组多样的二值化单元测试。为了扩展单元测试的创建，我们开发了一个生成合成文档的流水线，这些文档具有多样且具有挑战性的排版、已知的 HTML 源代码以及提取的测试用例。我们展示了在这些测试用例上进行强化学习训练，使得 olmOCR-Bench（我们发布的英文 OCR 基准测试）上达到最先进的性能，相较于前一版本在数学公式转换、表格解析和多列排版方面取得了最大的改进。我们已将模型、数据和代码在宽松的开源许可下发布。",
        "translated_title": "olmOCR 2: 文档 OCR 的单元测试奖励",
        "label": [],
        "label_reason": "论文聚焦于文档OCR，不涉及像素级图像质量恢复",
        "relevance_score": 2,
        "novelty_score": 5,
        "novelty_reason": "提出合成文档与单元测试奖励的新训练方法"
    },
    {
        "title": "How to Evaluate Monocular Depth Estimation?",
        "url": "http://arxiv.org/abs/2510.19814v1",
        "pub_date": "2025-10-22",
        "summary": "Monocular depth estimation is an important task with rapid progress, but how to evaluate it remains an open question, as evidenced by a lack of standardization in existing literature and a large selection of evaluation metrics whose trade-offs and behaviors are not well understood. This paper contributes a novel, quantitative analysis of existing metrics in terms of their sensitivity to various types of perturbations of ground truth, emphasizing comparison to human judgment. Our analysis reveals that existing metrics are severely under-sensitive to curvature perturbation such as making flat surfaces wavy. To remedy this, we introduce a new metric based on relative surface normals, along with new depth visualization tools and a principled method to create composite metrics with better human alignment. Code and data are available at: https://github.com/princeton-vl/evalmde.",
        "translated": "单目深度估计是一个进展迅速的重要任务，但如何对其进行评估仍是一个开放性问题，这体现在现有文献中缺乏标准化以及评估指标的种类繁多，而它们之间的权衡和行为尚未被充分理解。本文对现有指标进行了新颖而定量的分析，从它们对真实值各种类型扰动的敏感性角度出发，强调与人类判断的对比。我们的分析表明，现有指标对曲率扰动（如使平面表面变得波浪状）的敏感性严重不足。为了解决这一问题，我们引入了一种基于相对表面法线的新指标，以及新的深度可视化工具和一种原理明确的方法，用于构建与人类判断更一致的复合指标。代码和数据可在以下地址获取：https://github.com/princeton-vl/evalmde。",
        "translated_title": "如何评估单目深度估计？",
        "label": [],
        "label_reason": "论文聚焦深度估计评估，属于high-level任务，不涉及像素级图像恢复或增强。",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出了新评估指标和工具，但其创新点主要在评估方法而非图像处理技术本身。"
    },
    {
        "title": "Pico-Banana-400K: A Large-Scale Dataset for Text-Guided Image Editing",
        "url": "http://arxiv.org/abs/2510.19808v1",
        "pub_date": "2025-10-22",
        "summary": "Recent advances in multimodal models have demonstrated remarkable text-guided image editing capabilities, with systems like GPT-4o and Nano-Banana setting new benchmarks. However, the research community's progress remains constrained by the absence of large-scale, high-quality, and openly accessible datasets built from real images. We introduce Pico-Banana-400K, a comprehensive 400K-image dataset for instruction-based image editing. Our dataset is constructed by leveraging Nano-Banana to generate diverse edit pairs from real photographs in the OpenImages collection. What distinguishes Pico-Banana-400K from previous synthetic datasets is our systematic approach to quality and diversity. We employ a fine-grained image editing taxonomy to ensure comprehensive coverage of edit types while maintaining precise content preservation and instruction faithfulness through MLLM-based quality scoring and careful curation. Beyond single turn editing, Pico-Banana-400K enables research into complex editing scenarios. The dataset includes three specialized subsets: (1) a 72K-example multi-turn collection for studying sequential editing, reasoning, and planning across consecutive modifications; (2) a 56K-example preference subset for alignment research and reward model training; and (3) paired long-short editing instructions for developing instruction rewriting and summarization capabilities. By providing this large-scale, high-quality, and task-rich resource, Pico-Banana-400K establishes a robust foundation for training and benchmarking the next generation of text-guided image editing models.",
        "translated": "近年来，多模态模型在文本引导的图像编辑方面取得了显著进展，GPT-4o 和 Nano-Banana 等系统已树立了新的基准。然而，研究社区的进展仍受到缺乏大规模、高质量且公开可用的真实图像数据集的限制。我们提出了 Pico-Banana-400K，一个全面的包含 40 万张图像的指令驱动图像编辑数据集。我们的数据集通过利用 Nano-Banana 从 OpenImages 集合中的真实照片生成多样化的编辑对来构建。Pico-Banana-400K 与以往合成数据集的不同之处在于我们对质量和多样性的系统化处理。我们采用细粒度的图像编辑分类法，确保编辑类型覆盖全面，同时通过基于 MLLM 的质量评分和细致的编辑过程，保持内容的精确保存和指令的忠实度。除了单轮编辑之外，Pico-Banana-400K 还支持对复杂编辑场景的研究。该数据集包含三个专门的子集：(1) 一个包含 72,000 个示例的多轮编辑集合，用于研究连续修改中的顺序编辑、推理和规划；(2) 一个包含 56,000 个示例的偏好子集，用于对齐研究和奖励模型训练；以及 (3) 配对的长-短编辑指令，用于开发指令重写和摘要能力。通过提供这一大规模、高质量且任务丰富的新资源，Pico-Banana-400K 为下一代文本引导图像编辑模型的训练和评估奠定了坚实的基础。",
        "translated_title": "Pico-Banana-400K：一个用于文本引导图像编辑的大规模数据集",
        "label": [],
        "label_reason": "论文关注文本引导的图像编辑，不属于像素级图像恢复或增强任务",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出了大规模真实图像编辑数据集，但方法为常规数据构建"
    },
    {
        "title": "Class-Aware Prototype Learning with Negative Contrast for Test-Time\n  Adaptation of Vision-Language Models",
        "url": "http://arxiv.org/abs/2510.19802v1",
        "pub_date": "2025-10-22",
        "summary": "Vision-Language Models (VLMs) demonstrate impressive zero-shot generalization through large-scale image-text pretraining, yet their performance can drop once the deployment distribution diverges from the training distribution. To address this, Test-Time Adaptation (TTA) methods update models using unlabeled target data. However, existing approaches often ignore two key challenges: prototype degradation in long-tailed distributions and confusion between semantically similar classes. To tackle these issues, we propose \\textbf{C}lass-Aware \\textbf{P}rototype \\textbf{L}earning with \\textbf{N}egative \\textbf{C}ontrast(\\textbf{CPL-NC}), a lightweight TTA framework designed specifically for VLMs to enhance generalization under distribution shifts. CPL-NC introduces a \\textit{Class-Aware Prototype Cache} Module that dynamically adjusts per-class capacity based on test-time frequency and activation history, with a rejuvenation mechanism for inactive classes to retain rare-category knowledge. Additionally, a \\textit{Negative Contrastive Learning} Mechanism identifies and constrains hard visual-textual negatives to improve class separability. The framework employs asymmetric optimization, refining only textual prototypes while anchoring on stable visual features. Experiments on 15 benchmarks show that CPL-NC consistently outperforms prior TTA methods across both ResNet-50 and ViT-B/16 backbones.",
        "translated": "视觉-语言模型（VLMs）通过大规模图像-文本预训练展示了令人印象深刻的零样本泛化能力，但一旦部署分布与训练分布发生偏离，其性能可能会下降。为了解决这一问题，测试时自适应（TTA）方法利用未标注的目标数据更新模型。然而，现有方法通常忽略了两个关键挑战：长尾分布中的原型退化以及语义相似类别的混淆。为应对这些问题，我们提出了一种面向视觉-语言模型的轻量级TTA框架，称为\\textbf{C}lass-Aware \\textbf{P}rototype \\textbf{L}earning with \\textbf{N}egative \\textbf{C}ontrast（\\textbf{CPL-NC}），旨在增强模型在分布偏移下的泛化能力。CPL-NC引入了一个\\textit{类感知原型缓存}模块，该模块根据测试时的频率和激活历史动态调整每个类别的容量，并通过一种针对不活跃类别的恢复机制，保留罕见类别的知识。此外，\\textit{负对比学习}机制用于识别并约束困难的视觉-文本负样本，以提升类间可分性。该框架采用了非对称优化策略，仅优化文本原型，同时以稳定的视觉特征为锚点。在15个基准数据集上的实验表明，CPL-NC在ResNet-50和ViT-B/16两种主干结构上均一致优于以往的TTA方法。",
        "translated_title": "面向视觉-语言模型测试时自适应的类感知原型学习与负向对比",
        "label": [],
        "label_reason": "论文聚焦视觉语言模型测试时适应，属于high-level任务",
        "relevance_score": 1,
        "novelty_score": 6,
        "novelty_reason": "提出了类感知原型缓存和负对比学习机制，有一定创新性"
    },
    {
        "title": "OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation",
        "url": "http://arxiv.org/abs/2510.19789v1",
        "pub_date": "2025-10-22",
        "summary": "This paper introduces OmniMotion-X, a versatile multimodal framework for whole-body human motion generation, leveraging an autoregressive diffusion transformer in a unified sequence-to-sequence manner. OmniMotion-X efficiently supports diverse multimodal tasks, including text-to-motion, music-to-dance, speech-to-gesture, and global spatial-temporal control scenarios (e.g., motion prediction, in-betweening, completion, and joint/trajectory-guided synthesis), as well as flexible combinations of these tasks. Specifically, we propose the use of reference motion as a novel conditioning signal, substantially enhancing the consistency of generated content, style, and temporal dynamics crucial for realistic animations. To handle multimodal conflicts, we introduce a progressive weak-to-strong mixed-condition training strategy. To enable high-quality multimodal training, we construct OmniMoCap-X, the largest unified multimodal motion dataset to date, integrating 28 publicly available MoCap sources across 10 distinct tasks, standardized to the SMPL-X format at 30 fps. To ensure detailed and consistent annotations, we render sequences into videos and use GPT-4o to automatically generate structured and hierarchical captions, capturing both low-level actions and high-level semantics. Extensive experimental evaluations confirm that OmniMotion-X significantly surpasses existing methods, demonstrating state-of-the-art performance across multiple multimodal tasks and enabling the interactive generation of realistic, coherent, and controllable long-duration motions.",
        "translated": "本文介绍了 OmniMotion-X，这是一个用于全身人体运动生成的多功能多模态框架，通过统一的序列到序列方式利用自回归扩散变压器。OmniMotion-X 高效支持多种多模态任务，包括文本到运动、音乐到舞蹈、语音到手势，以及全局时空控制场景（例如运动预测、中间插值、补全和关节/轨迹引导的合成），并支持这些任务的灵活组合。具体而言，我们提出使用参考运动作为一种新的条件信号，显著增强了生成内容的一致性、风格及时序动态，这对于真实动画至关重要。为处理多模态冲突，我们引入了一种渐进式的弱到强混合条件训练策略。为实现高质量的多模态训练，我们构建了 OmniMoCap-X，目前最大的统一多模态运动数据集，整合了涵盖 10 个不同任务的 28 个公开可用的 MoCap 数据源，并标准化为 30 fps 的 SMPL-X 格式。为确保详细且一致的标注，我们将序列渲染为视频，并使用 GPT-4o 自动生成结构化和分层的字幕，捕捉低级动作和高级语义。广泛的实验评估验证了 OmniMotion-X 显著优于现有方法，在多个多模态任务上表现出最先进的性能，并能够实现真实、连贯且可控的长时序运动的交互式生成。",
        "translated_title": "OmniMotion-X：通用多模态全身运动生成",
        "label": [],
        "label_reason": "论文聚焦于人体运动生成，属于high-level任务，不涉及图像像素级处理。",
        "relevance_score": 1,
        "novelty_score": 4,
        "novelty_reason": "提出参考运动作为条件信号和多模态训练策略，有一定创新但非本质突破。"
    },
    {
        "title": "Adaptive Distribution-aware Quantization for Mixed-Precision Neural\n  Networks",
        "url": "http://arxiv.org/abs/2510.19760v1",
        "pub_date": "2025-10-22",
        "summary": "Quantization-Aware Training (QAT) is a critical technique for deploying deep neural networks on resource-constrained devices. However, existing methods often face two major challenges: the highly non-uniform distribution of activations and the static, mismatched codebooks used in weight quantization. To address these challenges, we propose Adaptive Distribution-aware Quantization (ADQ), a mixed-precision quantization framework that employs a differentiated strategy. The core of ADQ is a novel adaptive weight quantization scheme comprising three key innovations: (1) a quantile-based initialization method that constructs a codebook closely aligned with the initial weight distribution; (2) an online codebook adaptation mechanism based on Exponential Moving Average (EMA) to dynamically track distributional shifts; and (3) a sensitivity-informed strategy for mixed-precision allocation. For activations, we integrate a hardware-friendly non-uniform-to-uniform mapping scheme. Comprehensive experiments validate the effectiveness of our method. On ImageNet, ADQ enables a ResNet-18 to achieve 71.512% Top-1 accuracy with an average bit-width of only 2.81 bits, outperforming state-of-the-art methods under comparable conditions. Furthermore, detailed ablation studies on CIFAR-10 systematically demonstrate the individual contributions of each innovative component, validating the rationale and effectiveness of our design.",
        "translated": "量化感知训练（QAT）是将深度神经网络部署到资源受限设备上的关键技术。然而，现有方法通常面临两个主要挑战：激活值的分布高度非均匀，以及在权重量化中使用的静态且不匹配的码本。为了解决这些挑战，我们提出了自适应分布感知量化（ADQ），这是一个采用差异化策略的混合精度量化框架。ADQ 的核心是一个新颖的自适应权重量化方案，包含三个关键创新：(1) 一种基于分位数的初始化方法，构建与初始权重分布高度一致的码本；(2) 一种基于指数移动平均（EMA）的在线码本适应机制，以动态跟踪分布的变化；以及 (3) 一种基于敏感度的混合精度分配策略。对于激活值，我们集成了一种硬件友好的非均匀到均匀映射方案。大量实验验证了我们方法的有效性。在 ImageNet 数据集上，ADQ 使得 ResNet-18 在平均位宽仅为 2.81 位的情况下实现了 71.512% 的 Top-1 准确率，优于在类似条件下最先进的方法。此外，我们在 CIFAR-10 上进行了详尽的消融研究，系统地展示了每个创新组件的独立贡献，验证了我们设计的合理性和有效性。",
        "translated_title": "自适应分布感知的混合精度神经网络量化",
        "label": [],
        "label_reason": "论文关注神经网络量化，不涉及像素级图像处理",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出了自适应量化方案，改进了激活分布和码本设计"
    },
    {
        "title": "A Survey on Cache Methods in Diffusion Models: Toward Efficient\n  Multi-Modal Generation",
        "url": "http://arxiv.org/abs/2510.19755v1",
        "pub_date": "2025-10-22",
        "summary": "Diffusion Models have become a cornerstone of modern generative AI for their exceptional generation quality and controllability. However, their inherent \\textit{multi-step iterations} and \\textit{complex backbone networks} lead to prohibitive computational overhead and generation latency, forming a major bottleneck for real-time applications. Although existing acceleration techniques have made progress, they still face challenges such as limited applicability, high training costs, or quality degradation.   Against this backdrop, \\textbf{Diffusion Caching} offers a promising training-free, architecture-agnostic, and efficient inference paradigm. Its core mechanism identifies and reuses intrinsic computational redundancies in the diffusion process. By enabling feature-level cross-step reuse and inter-layer scheduling, it reduces computation without modifying model parameters. This paper systematically reviews the theoretical foundations and evolution of Diffusion Caching and proposes a unified framework for its classification and analysis.   Through comparative analysis of representative methods, we show that Diffusion Caching evolves from \\textit{static reuse} to \\textit{dynamic prediction}. This trend enhances caching flexibility across diverse tasks and enables integration with other acceleration techniques such as sampling optimization and model distillation, paving the way for a unified, efficient inference framework for future multimodal and interactive applications. We argue that this paradigm will become a key enabler of real-time and efficient generative AI, injecting new vitality into both theory and practice of \\textit{Efficient Generative Intelligence}.",
        "translated": "扩散模型由于其卓越的生成质量和可控性，已成为现代生成式人工智能的核心。然而，其固有的**多步迭代**和**复杂的主干网络**导致计算开销巨大和生成延迟高，形成实时应用中的主要瓶颈。尽管现有的加速技术取得了一定进展，但它们仍然面临适用性有限、训练成本高或生成质量下降等挑战。在这一背景下，**Diffusion Caching** 提供了一种有前景的、无需训练、与架构无关且高效的推理范式。其核心机制在于识别并复用扩散过程中的内在计算冗余。通过实现特征级的跨步复用和层间调度，它在不修改模型参数的前提下减少了计算量。本文系统地回顾了 Diffusion Caching 的理论基础及其发展历程，并提出了一个统一的分类与分析框架。通过对比分析代表性方法，我们展示了 Diffusion Caching 从**静态复用**向**动态预测**的演变趋势。这一趋势增强了在不同任务中的缓存灵活性，并使其能够与其他加速技术（如采样优化和模型蒸馏）相结合，为未来多模态和交互式应用提供统一高效的推理框架。我们认为，这一范式将成为实时高效生成式人工智能的关键推动因素，为**高效生成智能**的理论与实践注入新的活力。",
        "translated_title": "扩散模型中的缓存方法综述：迈向高效的多模态生成",
        "label": [],
        "label_reason": "论文聚焦生成模型加速，不属于图像像素级恢复或增强任务。",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出了扩散模型中的缓存方法新范式，提升了推理效率。"
    },
    {
        "title": "Memo: Training Memory-Efficient Embodied Agents with Reinforcement\n  Learning",
        "url": "http://arxiv.org/abs/2510.19732v1",
        "pub_date": "2025-10-22",
        "summary": "To enable embodied agents to operate effectively over extended timeframes, it is crucial to develop models that form and access memories to stay contextualized in their environment. In the current paradigm of training transformer-based policies for embodied sequential decision-making tasks, visual inputs often overwhelm the context limits of transformers, while humans can maintain and utilize a lifetime of experience compressed as memories. Significant compression is possible in principle, as much of the input is irrelevant and can be abstracted. However, existing approaches predominantly focus on either recurrent models with fixed-size memory or transformers with full-context reliance. In this work, we propose Memo, a transformer-based architecture and training recipe for reinforcement learning (RL) on memory-intensive, long-horizon tasks. Memo incorporates the creation and retrieval of memory by interleaving periodic summarization tokens with the inputs of a model during training. We demonstrate Memo's effectiveness on a gridworld meta-RL benchmark and a multi-object navigation task in photo-realistic indoor settings. Memo outperforms naive long-context transformer baselines while being more compute and storage efficient. Additionally, Memo generalizes better to longer contexts at inference time and remains robust in streaming settings, where historical context must be truncated to fit inference constraints.",
        "translated": "为了使具身智能体在长时间范围内有效运行，开发能够形成和访问记忆以保持环境上下文感的模型至关重要。在当前基于Transformer的具身序列决策任务训练范式中，视觉输入往往会超出Transformer的上下文限制，而人类则能够维持并利用压缩为记忆的终身经验。从原理上讲，实现显著的压缩是可能的，因为大部分输入信息是无关的，可以进行抽象处理。然而，现有方法主要集中在两种方向：一是使用固定大小记忆的循环模型，二是完全依赖上下文的Transformer。在本文中，我们提出Memo，这是一种基于Transformer的架构和训练方法，专为内存密集型、长视野任务的强化学习（RL）而设计。Memo通过在训练过程中将周期性摘要标记与模型输入交错，实现了记忆的生成与检索。我们在一个网格世界元强化学习基准和一个在照片级真实感室内环境中进行的多物体导航任务上验证了Memo的有效性。Memo在计算和存储效率上优于简单的长上下文Transformer基线。此外，Memo在推理时对更长的上下文具有更好的泛化能力，并且在流式处理场景中仍能保持鲁棒性，即使必须截断历史上下文以满足推理限制。",
        "translated_title": "Memo: 基于强化学习训练内存高效的具身智能体",
        "label": [],
        "label_reason": "论文聚焦于强化学习代理的记忆训练，不涉及图像像素级处理",
        "relevance_score": 1,
        "novelty_score": 6,
        "novelty_reason": "提出一种高效的记忆压缩方法，但属于常规方法改进"
    },
    {
        "title": "LyTimeT: Towards Robust and Interpretable State-Variable Discovery",
        "url": "http://arxiv.org/abs/2510.19716v1",
        "pub_date": "2025-10-22",
        "summary": "Extracting the true dynamical variables of a system from high-dimensional video is challenging due to distracting visual factors such as background motion, occlusions, and texture changes. We propose LyTimeT, a two-phase framework for interpretable variable extraction that learns robust and stable latent representations of dynamical systems. In Phase 1, LyTimeT employs a spatio-temporal TimeSformer-based autoencoder that uses global attention to focus on dynamically relevant regions while suppressing nuisance variation, enabling distraction-robust latent state learning and accurate long-horizon video prediction. In Phase 2, we probe the learned latent space, select the most physically meaningful dimensions using linear correlation analysis, and refine the transition dynamics with a Lyapunov-based stability regularizer to enforce contraction and reduce error accumulation during roll-outs. Experiments on five synthetic benchmarks and four real-world dynamical systems, including chaotic phenomena, show that LyTimeT achieves mutual information and intrinsic dimension estimates closest to ground truth, remains invariant under background perturbations, and delivers the lowest analytical mean squared error among CNN-based (TIDE) and transformer-only baselines. Our results demonstrate that combining spatio-temporal attention with stability constraints yields predictive models that are not only accurate but also physically interpretable.",
        "translated": "从高维视频中提取系统的真实动力学变量具有挑战性，因为存在诸如背景运动、遮挡和纹理变化等干扰性视觉因素。我们提出 LyTimeT，一种用于可解释变量提取的两阶段框架，其目标是学习鲁棒且稳定的动力系统潜在表示。在第一阶段中，LyTimeT 采用基于时空 TimeSformer 的自编码器，利用全局注意力机制聚焦于动力学相关的区域，同时抑制无关变化，从而实现对干扰具有鲁棒性的潜在状态学习，并能进行准确的长跨度视频预测。在第二阶段中，我们对所学习的潜在空间进行探测，使用线性相关性分析选择最具物理意义的维度，并通过基于 Lyapunov 的稳定性正则化项对状态转移动力学进行优化，以增强收缩性并减少预测过程中的误差累积。在五个合成基准和四个真实世界动力系统（包括混沌现象）上的实验表明，LyTimeT 在互信息和固有维度估计方面最接近真实值，在背景扰动下保持不变性，并且在基于 CNN（TIDE）和纯 Transformer 的基线方法中具有最低的解析均方误差。我们的结果表明，将时空注意力机制与稳定性约束相结合，能够生成不仅准确而且具有物理可解释性的预测模型。",
        "translated_title": "LyTimeT：迈向稳健且可解释的状态变量发现",
        "label": [],
        "label_reason": "论文聚焦于动态变量提取，非图像像素级恢复任务。",
        "relevance_score": 2,
        "novelty_score": 5,
        "novelty_reason": "提出两阶段框架，结合时空注意力与稳定性约束，有一定创新但非突破性。"
    },
    {
        "title": "Explainable Face Presentation Attack Detection via Ensemble-CAM",
        "url": "http://arxiv.org/abs/2510.19695v1",
        "pub_date": "2025-10-22",
        "summary": "Presentation attacks represent a critical security threat where adversaries use fake biometric data, such as face, fingerprint, or iris images, to gain unauthorized access to protected systems. Various presentation attack detection (PAD) systems have been designed leveraging deep learning (DL) models to mitigate this type of threat. Despite their effectiveness, most of the DL models function as black boxes - their decisions are opaque to their users. The purpose of explainability techniques is to provide detailed information about the reason behind the behavior or decision of DL models. In particular, visual explanation is necessary to better understand the decisions or predictions of DL-based PAD systems and determine the key regions due to which a biometric image is considered real or fake by the system. In this work, a novel technique, Ensemble-CAM, is proposed for providing visual explanations for the decisions made by deep learning-based face PAD systems. Our goal is to improve DL-based face PAD systems by providing a better understanding of their behavior. Our provided visual explanations will enhance the transparency and trustworthiness of DL-based face PAD systems.",
        "translated": "呈现攻击是指对手使用伪造的生物特征数据（如人脸、指纹或虹膜图像）以获取对受保护系统的未经授权访问，这代表了关键的安全威胁。为了缓解此类威胁，已有多种呈现攻击检测（PAD）系统被设计出来，利用深度学习（DL）模型实现检测。尽管这些方法在实际中有效，但大多数深度学习模型都表现为黑盒——它们的决策对于用户而言是不透明的。可解释性技术的目的在于提供关于深度学习模型行为或决策背后原因的详细信息。特别是，可视化解释对于更好地理解基于深度学习的PAD系统的决策或预测，以及确定系统将生物特征图像识别为真实或伪造的关键区域，是必不可少的。在本工作中，我们提出了一种新颖的技术——Ensemble-CAM，用于为基于深度学习的人脸PAD系统的决策提供可视化解释。我们的目标是通过加深对其行为的理解，从而提升基于深度学习的人脸PAD系统的性能。所提供的可视化解释将增强基于深度学习的人脸PAD系统的透明度和可信度。",
        "translated_title": "通过集成-CAM实现可解释的面部呈现攻击检测",
        "label": [],
        "label_reason": "论文关注的是人脸识别的安全问题，不属于图像像素级质量恢复或增强任务。",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出了一种新的可视化解释方法，但属于常规的模型解释技术改进。"
    },
    {
        "title": "Curvilinear Structure-preserving Unpaired Cross-domain Medical Image\n  Translation",
        "url": "http://arxiv.org/abs/2510.19679v1",
        "pub_date": "2025-10-22",
        "summary": "Unpaired image-to-image translation has emerged as a crucial technique in medical imaging, enabling cross-modality synthesis, domain adaptation, and data augmentation without costly paired datasets. Yet, existing approaches often distort fine curvilinear structures, such as microvasculature, undermining both diagnostic reliability and quantitative analysis. This limitation is consequential in ophthalmic and vascular imaging, where subtle morphological changes carry significant clinical meaning. We propose Curvilinear Structure-preserving Translation (CST), a general framework that explicitly preserves fine curvilinear structures during unpaired translation by integrating structure consistency into the training. Specifically, CST augments baseline models with a curvilinear extraction module for topological supervision. It can be seamlessly incorporated into existing methods. We integrate it into CycleGAN and UNSB as two representative backbones. Comprehensive evaluation across three imaging modalities: optical coherence tomography angiography, color fundus and X-ray coronary angiography demonstrates that CST improves translation fidelity and achieves state-of-the-art performance. By reinforcing geometric integrity in learned mappings, CST establishes a principled pathway toward curvilinear structure-aware cross-domain translation in medical imaging.",
        "translated": "无配对图像到图像的翻译已成为医学影像中的关键技术，能够在无需昂贵配对数据集的情况下实现跨模态合成、域适应和数据增强。然而，现有方法通常会扭曲精细的曲线结构，例如微血管，从而影响诊断的可靠性和定量分析。这一限制在眼科和血管成像中尤为关键，因为细微的形态变化在临床上具有重要意义。我们提出了一种名为 Curvilinear Structure-preserving Translation (CST) 的通用框架，通过在训练过程中整合结构一致性，明确地在无配对翻译过程中保留精细的曲线结构。具体而言，CST 在基础模型中增加了曲线提取模块，以提供拓扑监督。它可以无缝集成到现有方法中。我们将其集成到 CycleGAN 和 UNSB 两种代表性骨干网络中。在三种成像模态——光学相干断层扫描血管造影、彩色眼底图像和X射线冠状动脉造影上的全面评估表明，CST 提高了翻译保真度并取得了最先进的性能。通过在学习映射中强化几何完整性，CST 为医学影像中曲线结构感知的跨域翻译建立了一条原理性的路径。",
        "translated_title": "保持曲线结构的非配对跨域医学图像翻译",
        "label": [
            "医学图像增强",
            "图像恢复"
        ],
        "label_reason": "论文聚焦于医学图像翻译中结构保护，属于图像恢复范畴。",
        "relevance_score": 8,
        "novelty_score": 7,
        "novelty_reason": "提出结构一致性模块，改进现有方法的翻译保真度。"
    },
    {
        "title": "I Spy With My Model's Eye: Visual Search as a Behavioural Test for MLLMs",
        "url": "http://arxiv.org/abs/2510.19678v1",
        "pub_date": "2025-10-22",
        "summary": "Multimodal large language models (MLLMs) achieve strong performance on vision-language tasks, yet their visual processing is opaque. Most black-box evaluations measure task accuracy, but reveal little about underlying mechanisms. Drawing on cognitive psychology, we adapt classic visual search paradigms -- originally developed to study human perception -- to test whether MLLMs exhibit the ``pop-out'' effect, where salient visual features are detected independently of distractor set size. Using controlled experiments targeting colour, size and lighting features, we find that advanced MLLMs exhibit human-like pop-out effects in colour or size-based disjunctive (single feature) search, as well as capacity limits for conjunctive (multiple feature) search. We also find evidence to suggest that MLLMs, like humans, incorporate natural scene priors such as lighting direction into object representations. We reinforce our findings using targeted fine-tuning and mechanistic interpretability analyses. Our work shows how visual search can serve as a cognitively grounded diagnostic tool for evaluating perceptual capabilities in MLLMs.",
        "translated": "多模态大语言模型（MLLMs）在视觉-语言任务中表现出色，但其视觉处理过程具有黑箱性质。大多数黑盒评估方法测量任务的准确率，却难以揭示其内部机制。借鉴认知心理学，我们采用经典的视觉搜索范式——最初用于研究人类感知——来测试 MLLMs 是否表现出“pop-out”效应，即显著的视觉特征能够不受干扰项数量的影响而被检测到。通过针对颜色、大小和光照特征设计的受控实验，我们发现先进的 MLLMs 在基于颜色或大小的析取（单一特征）搜索中表现出类似人类的 pop-out 效应，同时在合取（多特征）搜索中也表现出容量限制。我们还发现了一些证据，表明 MLLMs 与人类一样，会将自然场景的先验知识（如光照方向）融入对象表示中。我们通过有针对性的微调和机制解释性分析进一步验证了这些发现。我们的工作表明，视觉搜索可以作为基于认知的诊断工具，用于评估 MLLMs 的感知能力。",
        "translated_title": "我以我的模型之眼观察：视觉搜索作为多模态大语言模型的行为测试",
        "label": [],
        "label_reason": "论文聚焦多模态语言模型的视觉认知测试，非图像像素级恢复或增强",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "将认知心理学中的视觉搜索范式应用于 MLLMs 评估，具有一定创新性"
    },
    {
        "title": "From Forecasting to Planning: Policy World Model for Collaborative\n  State-Action Prediction",
        "url": "http://arxiv.org/abs/2510.19654v1",
        "pub_date": "2025-10-22",
        "summary": "Despite remarkable progress in driving world models, their potential for autonomous systems remains largely untapped: the world models are mostly learned for world simulation and decoupled from trajectory planning. While recent efforts aim to unify world modeling and planning in a single framework, the synergistic facilitation mechanism of world modeling for planning still requires further exploration. In this work, we introduce a new driving paradigm named Policy World Model (PWM), which not only integrates world modeling and trajectory planning within a unified architecture, but is also able to benefit planning using the learned world knowledge through the proposed action-free future state forecasting scheme. Through collaborative state-action prediction, PWM can mimic the human-like anticipatory perception, yielding more reliable planning performance. To facilitate the efficiency of video forecasting, we further introduce a dynamically enhanced parallel token generation mechanism, equipped with a context-guided tokenizer and an adaptive dynamic focal loss. Despite utilizing only front camera input, our method matches or exceeds state-of-the-art approaches that rely on multi-view and multi-modal inputs. Code and model weights will be released at https://github.com/6550Zhao/Policy-World-Model.",
        "translated": "尽管世界模型在驾驶领域取得了显著进展，但它们在自动驾驶系统中的潜力仍远未被充分挖掘：这些世界模型大多用于世界仿真，并与轨迹规划解耦。虽然最近的研究努力旨在在一个统一的框架中融合世界建模和规划，但世界建模对规划的协同促进机制仍需进一步探索。在本研究中，我们提出了一种新的驾驶范式，称为策略世界模型（Policy World Model，PWM），该模型不仅在一个统一的架构中融合了世界建模与轨迹规划，而且通过提出一种无需动作的未来状态预测方案，能够利用所学习到的世界知识来促进规划。通过协作式的状态-动作预测，PWM可以模拟人类类似的前瞻性感知，从而实现更可靠的规划性能。为了提高视频预测的效率，我们进一步引入了一种动态增强的并行token生成机制，该机制配备了上下文引导的tokenizer和自适应动态焦点损失。尽管仅使用前视摄像头输入，我们的方法在性能上可与甚至超越那些依赖多视角和多模态输入的最先进方法。代码和模型权重将发布在 https://github.com/6550Zhao/Policy-World-Model。",
        "translated_title": "从预测到规划：用于协作状态-动作预测的策略世界模型",
        "label": [],
        "label_reason": "论文聚焦于轨迹规划与世界模型集成，不涉及像素级图像恢复或增强",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出了统一架构的 Policy World Model，但属于常规组合与扩展"
    },
    {
        "title": "MedReason-R1: Learning to Reason for CT Diagnosis with Reinforcement\n  Learning and Local Zoom",
        "url": "http://arxiv.org/abs/2510.19626v1",
        "pub_date": "2025-10-22",
        "summary": "General-purpose large Vision-Language Models (VLMs) demonstrate strong capabilities in generating detailed descriptions for natural images. However, their performance in the medical domain remains suboptimal, even for relatively straightforward tasks, primarily due to the lack of large-scale, high-quality, specialized medical imaging datasets and the neglect of the diagnostic process that progresses from coarse to fine-grained. To address the first issue, we construct the CT-RATE-VQA dataset, which has 84K QA pairs. For the second issue, we propose MedReason-R1, a medical VLM with explicit reasoning process for disease diagnosis. MedReason-R1 incorporates a novel strategy that embeds zoom-in disease region-of-interest areas into the image, highlighting the crucial role of both global localization and disease-specific details in enhancing the model's diagnostic performance. Furthermore, we introduce the GRPO reinforcement learning framework to MedReason-R1, which enables effective reasoning without relying on costly manual annotations. Compared to recent general-purpose and medical VLMs, MedReason-R1 achieves state-of-the-art performance in CT disease diagnosis while retaining generalization. The code, checkpoints, and dataset are available at: https://github.com/Leevan001/MedReason-R1",
        "translated": "通用的大规模视觉-语言模型（VLMs）在生成自然图像的详细描述方面表现出强大的能力。然而，它们在医学领域的表现仍不理想，即使在相对简单的任务上也是如此，这主要是由于缺乏大规模的高质量、专门的医学图像数据集，并且忽视了从粗粒度到细粒度的诊断过程。为了解决第一个问题，我们构建了 CT-RATE-VQA 数据集，该数据集包含 84,000 对问答对。针对第二个问题，我们提出了 MedReason-R1，这是一种具备显式推理过程的医学 VLM，用于疾病诊断。MedReason-R1 引入了一种新颖的策略，通过在图像中嵌入放大后的疾病感兴趣区域，突出了全局定位与疾病特异性细节在提升模型诊断性能中的关键作用。此外，我们还向 MedReason-R1 引入了 GRPO 强化学习框架，该框架能够在不依赖昂贵的人工标注的前提下，实现有效的推理。与最近的通用和医学专用 VLMs 相比，MedReason-R1 在 CT 疾病诊断任务上实现了最先进的性能，同时保留了良好的泛化能力。代码、模型参数和数据集可在以下链接获取：https://github.com/Leevan001/MedReason-R1",
        "translated_title": "MedReason-R1：利用强化学习与局部放大进行 CT 诊断的推理学习",
        "label": [
            "医学图像增强",
            "图像去噪"
        ],
        "label_reason": "方法涉及医学图像增强和局部放大以提升诊断性能",
        "relevance_score": 5,
        "novelty_score": 7,
        "novelty_reason": "引入GRPO强化学习框架，提升了模型的诊断推理能力"
    },
    {
        "title": "Augmenting Moment Retrieval: Zero-Dependency Two-Stage Learning",
        "url": "http://arxiv.org/abs/2510.19622v1",
        "pub_date": "2025-10-22",
        "summary": "Existing Moment Retrieval methods face three critical bottlenecks: (1) data scarcity forces models into shallow keyword-feature associations; (2) boundary ambiguity in transition regions between adjacent events; (3) insufficient discrimination of fine-grained semantics (e.g., distinguishing ``kicking\" vs. ``throwing\" a ball). In this paper, we propose a zero-external-dependency Augmented Moment Retrieval framework, AMR, designed to overcome local optima caused by insufficient data annotations and the lack of robust boundary and semantic discrimination capabilities. AMR is built upon two key insights: (1) it resolves ambiguous boundary information and semantic confusion in existing annotations without additional data (avoiding costly manual labeling), and (2) it preserves boundary and semantic discriminative capabilities enhanced by training while generalizing to real-world scenarios, significantly improving performance. Furthermore, we propose a two-stage training framework with cold-start and distillation adaptation. The cold-start stage employs curriculum learning on augmented data to build foundational boundary/semantic awareness. The distillation stage introduces dual query sets: Original Queries maintain DETR-based localization using frozen Base Queries from the cold-start model, while Active Queries dynamically adapt to real-data distributions. A cross-stage distillation loss enforces consistency between Original and Base Queries, preventing knowledge forgetting while enabling real-world generalization. Experiments on multiple benchmarks show that AMR achieves improved performance over prior state-of-the-art approaches.",
        "translated": "现有的Moment Retrieval方法面临三个关键瓶颈：(1) 数据稀缺迫使模型建立浅层的关键词-特征关联；(2) 相邻事件之间的过渡区域中边界存在模糊性；(3) 对细粒度语义的区分能力不足（例如区分“踢”和“扔”球）。在本文中，我们提出了一种零外部依赖的增强型时刻检索框架AMR，旨在克服由于数据标注不足以及缺乏鲁棒的边界和语义区分能力所导致的局部最优问题。AMR基于两个关键见解：(1) 在不使用额外数据（避免高昂的人工标注成本）的情况下，解决了现有标注中存在的边界模糊信息和语义混淆问题；(2) 在训练中增强并保留了边界和语义的判别能力，同时能够泛化到现实场景，显著提升了性能。此外，我们提出了一个包含冷启动和蒸馏适配的两阶段训练框架。冷启动阶段在增强的数据上采用课程学习来构建基本的边界/语义感知能力。蒸馏阶段引入了双查询集：原始查询（Original Queries）通过冻结来自冷启动模型的基查询（Base Queries）来保持基于DETR的定位能力，而活跃查询（Active Queries）则动态适应真实数据分布。跨阶段蒸馏损失（cross-stage distillation loss）强制原始查询和基查询之间的一致性，防止知识遗忘的同时实现对现实场景的泛化。在多个基准数据集上的实验表明，AMR在性能上优于先前最先进的方法。",
        "translated_title": "增强矩量检索：零依赖的两阶段学习",
        "label": [],
        "label_reason": "论文聚焦于时刻检索，属于high-level任务，与图像像素级质量无关。",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出两阶段训练框架和课程学习策略，具有一定创新性。"
    },
    {
        "title": "Pragmatic Heterogeneous Collaborative Perception via Generative\n  Communication Mechanism",
        "url": "http://arxiv.org/abs/2510.19618v1",
        "pub_date": "2025-10-22",
        "summary": "Multi-agent collaboration enhances the perception capabilities of individual agents through information sharing. However, in real-world applications, differences in sensors and models across heterogeneous agents inevitably lead to domain gaps during collaboration. Existing approaches based on adaptation and reconstruction fail to support pragmatic heterogeneous collaboration due to two key limitations: (1) Intrusive retraining of the encoder or core modules disrupts the established semantic consistency among agents; and (2) accommodating new agents incurs high computational costs, limiting scalability. To address these challenges, we present a novel Generative Communication mechanism (GenComm) that facilitates seamless perception across heterogeneous multi-agent systems through feature generation, without altering the original network, and employs lightweight numerical alignment of spatial information to efficiently integrate new agents at minimal cost. Specifically, a tailored Deformable Message Extractor is designed to extract spatial message for each collaborator, which is then transmitted in place of intermediate features. The Spatial-Aware Feature Generator, utilizing a conditional diffusion model, generates features aligned with the ego agent's semantic space while preserving the spatial information of the collaborators. These generated features are further refined by a Channel Enhancer before fusion. Experiments conducted on the OPV2V-H, DAIR-V2X and V2X-Real datasets demonstrate that GenComm outperforms existing state-of-the-art methods, achieving an 81\\% reduction in both computational cost and parameter count when incorporating new agents. Our code is available at https://github.com/jeffreychou777/GenComm.",
        "translated": "多智能体协作通过信息共享提升单个智能体的感知能力。然而，在现实应用场景中，由于异构智能体在传感器和模型上的差异，协作过程中不可避免地会出现领域差异。现有的基于自适应和重建的方法由于两个关键限制，无法支持实用的异构协作：（1）对编码器或核心模块的侵入式再训练会破坏智能体之间既有的语义一致性；（2）容纳新的智能体需要较高的计算成本，限制了系统的可扩展性。为了解决这些挑战，我们提出了一种新颖的生成式通信机制（GenComm），该机制通过特征生成促进异构多智能体系统之间的无缝感知，无需修改原始网络结构，并通过空间信息的轻量级数值对齐，以最小成本高效集成新的智能体。具体而言，我们设计了一个定制化的可变形信息提取器（Deformable Message Extractor），用于提取每个协作者的空间信息，并将其作为中间特征的替代进行传输。空间感知特征生成器（Spatial-Aware Feature Generator）利用条件扩散模型（conditional diffusion model），在保持协作者空间信息的同时，生成与自身智能体语义空间对齐的特征。这些生成的特征在融合前进一步通过通道增强器（Channel Enhancer）进行优化。在 OPV2V-H、DAIR-V2X 和 V2X-Real 数据集上的实验表明，GenComm 在性能上优于现有最先进的方法，在引入新智能体时计算成本和参数数量分别减少了 81%。我们的代码可在 https://github.com/jeffreychou777/GenComm 获得。",
        "translated_title": "实用的异构协同感知生成通信机制",
        "label": [],
        "label_reason": "主要处理多智能体感知协作，非像素级图像质量复原",
        "relevance_score": 3,
        "novelty_score": 7,
        "novelty_reason": "提出新颖的生成式通信机制，提升了异构系统协作效率"
    },
    {
        "title": "Beyond sparse denoising in frames: minimax estimation with a scattering\n  transform",
        "url": "http://arxiv.org/abs/2510.19612v1",
        "pub_date": "2025-10-22",
        "summary": "A considerable amount of research in harmonic analysis has been devoted to non-linear estimators of signals contaminated by additive Gaussian noise. They are implemented by thresholding coefficients in a frame, which provide a sparse signal representation, or by minimising their $\\ell^1$ norm. However, sparse estimators in frames are not sufficiently rich to adapt to complex signal regularities. For cartoon images whose edges are piecewise $\\bf C^\\alpha$ curves, wavelet, curvelet and Xlet frames are suboptimal if the Lipschitz exponent $\\alpha \\leq 2$ is an unknown parameter. Deep convolutional neural networks have recently obtained much better numerical results, which reach the minimax asymptotic bounds for all $\\alpha$. Wavelet scattering coefficients have been introduced as simplified convolutional neural network models. They are computed by transforming the modulus of wavelet coefficients with a second wavelet transform. We introduce a denoising estimator by jointly minimising and maximising the $\\ell^1$ norms of different subsets of scattering coefficients. We prove that these $\\ell^1$ norms capture different types of geometric image regularity. Numerical experiments show that this denoising estimator reaches the minimax asymptotic bound for cartoon images for all Lipschitz exponents $\\alpha \\leq 2$. We state this numerical result as a mathematical conjecture. It provides a different harmonic analysis approach to suppress noise from signals, and to specify the geometric regularity of functions. It also opens a mathematical bridge between harmonic analysis and denoising estimators with deep convolutional network.",
        "translated": "谐波分析领域已有大量研究致力于估计被加性高斯噪声污染的信号的非线性方法。这些方法通过在某一帧中对系数进行阈值处理来实现，该帧提供稀疏的信号表示，或通过最小化其 $\\ell^1$ 范数来实现。然而，帧中的稀疏估计器不足以适应复杂的信号规律性。对于边缘为分段 $\\bf C^\\alpha$ 曲线的卡通图像，如果 Lipschitz 指数 $\\alpha \\leq 2$ 为未知参数，则小波、曲线小波和 Xlet 帧并非最优选择。最近，深度卷积神经网络在图像去噪任务中取得了显著优于传统方法的数值结果，并对所有 $\\alpha$ 都达到了极小极大渐近界。小波散射系数被提出作为简化版的卷积神经网络模型。它们通过对小波系数的模进行二次小波变换计算得到。我们通过联合最小化和最大化不同子集的散射系数的 $\\ell^1$ 范数，引入了一种图像去噪估计器。我们证明了这些 $\\ell^1$ 范数能够捕捉不同类型图像的几何规律性。数值实验表明，该去噪估计器对于所有满足 $\\alpha \\leq 2$ 的 Lipschitz 指数的卡通图像，均能达到极小极大渐近界。我们将这一数值结果表述为一个数学猜想。该结果为从信号中抑制噪声以及函数几何规律性的刻画提供了新的谐波分析方法。它也搭建了谐波分析与基于深度卷积网络的去噪估计器之间的数学桥梁。",
        "translated_title": "超越帧中的稀疏去噪：具有散射变换的极小极大估计",
        "label": [
            "图像去噪",
            "频域先验"
        ],
        "label_reason": "论文聚焦图像去噪，并探索信号几何正则性建模",
        "relevance_score": 9,
        "novelty_score": 8,
        "novelty_reason": "提出基于散射变换的极小极大去噪新方法"
    },
    {
        "title": "XBench: A Comprehensive Benchmark for Visual-Language Explanations in\n  Chest Radiography",
        "url": "http://arxiv.org/abs/2510.19599v1",
        "pub_date": "2025-10-22",
        "summary": "Vision-language models (VLMs) have recently shown remarkable zero-shot performance in medical image understanding, yet their grounding ability, the extent to which textual concepts align with visual evidence, remains underexplored. In the medical domain, however, reliable grounding is essential for interpretability and clinical adoption. In this work, we present the first systematic benchmark for evaluating cross-modal interpretability in chest X-rays across seven CLIP-style VLM variants. We generate visual explanations using cross-attention and similarity-based localization maps, and quantitatively assess their alignment with radiologist-annotated regions across multiple pathologies. Our analysis reveals that: (1) while all VLM variants demonstrate reasonable localization for large and well-defined pathologies, their performance substantially degrades for small or diffuse lesions; (2) models that are pretrained on chest X-ray-specific datasets exhibit improved alignment compared to those trained on general-domain data. (3) The overall recognition ability and grounding ability of the model are strongly correlated. These findings underscore that current VLMs, despite their strong recognition ability, still fall short in clinically reliable grounding, highlighting the need for targeted interpretability benchmarks before deployment in medical practice. XBench code is available at https://github.com/Roypic/Benchmarkingattention",
        "translated": "近年来，视觉语言模型（VLMs）在医学图像理解中展现了显著的零样本性能，然而它们的“grounding能力”，即文本概念与视觉证据的一致程度，仍未得到充分探索。在医学领域，可靠的grounding对于模型的可解释性和临床应用至关重要。在本研究中，我们首次提出了一个系统化的基准测试，用于评估七种CLIP风格VLM变体在胸部X光图像中跨模态可解释性的表现。我们通过交叉注意力和基于相似性的定位图生成视觉解释，并定量评估其与放射科医生标注区域在多种疾病类型上的一致性。我们的分析表明：(1) 尽管所有VLM变体在大而明确的疾病定位上表现合理，但它们在小病灶或弥散性病变上的性能显著下降；(2) 在胸部X光专用数据集上预训练的模型相较于在通用领域数据上训练的模型，其定位一致性更高；(3) 模型的整体识别能力与grounding能力之间存在强相关性。这些发现表明，尽管当前VLMs具有较强的识别能力，但在临床可靠的grounding方面仍有不足，强调在医学实践中部署前，需要针对可解释性设计专门的基准测试。XBench代码可在 https://github.com/Roypic/Benchmarkingattention 获得。",
        "translated_title": "XBench：胸部X光影像中视觉-语言解释的综合基准测试",
        "label": [],
        "label_reason": "论文聚焦医学图像解释性，非像素级图像恢复任务",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出首个系统性跨模态可解释性基准，具有实用价值"
    },
    {
        "title": "CBDiff:Conditional Bernoulli Diffusion Models for Image Forgery\n  Localization",
        "url": "http://arxiv.org/abs/2510.19597v1",
        "pub_date": "2025-10-22",
        "summary": "Image Forgery Localization (IFL) is a crucial task in image forensics, aimed at accurately identifying manipulated or tampered regions within an image at the pixel level. Existing methods typically generate a single deterministic localization map, which often lacks the precision and reliability required for high-stakes applications such as forensic analysis and security surveillance. To enhance the credibility of predictions and mitigate the risk of errors, we introduce an advanced Conditional Bernoulli Diffusion Model (CBDiff). Given a forged image, CBDiff generates multiple diverse and plausible localization maps, thereby offering a richer and more comprehensive representation of the forgery distribution. This approach addresses the uncertainty and variability inherent in tampered regions. Furthermore, CBDiff innovatively incorporates Bernoulli noise into the diffusion process to more faithfully reflect the inherent binary and sparse properties of forgery masks. Additionally, CBDiff introduces a Time-Step Cross-Attention (TSCAttention), which is specifically designed to leverage semantic feature guidance with temporal steps to improve manipulation detection. Extensive experiments on eight publicly benchmark datasets demonstrate that CBDiff significantly outperforms existing state-of-the-art methods, highlighting its strong potential for real-world deployment.",
        "translated": "图像伪造定位（Image Forgery Localization，IFL）是图像取证中的关键任务，旨在以像素级别精确识别图像中被篡改或修改的区域。现有方法通常生成单一确定性的定位图，这在诸如司法鉴定和安全监控等高风险应用中往往缺乏所需的精度和可靠性。为提高预测结果的可信度并降低错误风险，我们引入了一种先进的条件伯努利扩散模型（Conditional Bernoulli Diffusion Model，CBDiff）。给定一幅伪造图像，CBDiff 能够生成多个多样且合理的定位图，从而提供对伪造分布更丰富、更全面的表示。该方法有效应对了篡改区域中存在的不确定性和变异性。此外，CBDiff 创新性地将伯努利噪声融入扩散过程中，以更真实地反映伪造掩码固有的二值性和稀疏性。CBDiff 还引入了一种时间步交叉注意力机制（Time-Step Cross-Attention，TSCAttention），该机制专门设计用于在时间步进过程中利用语义特征引导，以提升篡改检测的性能。在八个公开基准数据集上的大量实验表明，CBDiff 显著优于现有的最先进方法，展示了其在实际应用中强大的潜力。",
        "translated_title": "CBDiff：用于图像篡改定位的条件伯努利扩散模型",
        "label": [
            "图像修复",
            "图像去噪"
        ],
        "label_reason": "论文涉及图像伪造定位，属于像素级图像分析，与图像修复和去噪相关",
        "relevance_score": 6,
        "novelty_score": 8,
        "novelty_reason": "提出条件伯努利扩散模型和时间步交叉注意力，改进伪造检测方法"
    },
    {
        "title": "ToolDreamer: Instilling LLM Reasoning Into Tool Retrievers",
        "url": "http://arxiv.org/abs/2510.19791v1",
        "pub_date": "2025-10-22",
        "summary": "Tool calling has become increasingly popular for Large Language Models (LLMs). However, for large tool sets, the resulting tokens would exceed the LLM's context window limit, making it impossible to include every tool. Hence, an external retriever is used to provide LLMs with the most relevant tools for a query. Existing retrieval models rank tools based on the similarity between a user query and a tool description (TD). This leads to suboptimal retrieval as user requests are often poorly aligned with the language of TD. To remedy the issue, we propose ToolDreamer, a framework to condition retriever models to fetch tools based on hypothetical (synthetic) TD generated using an LLM, i.e., description of tools that the LLM feels will be potentially useful for the query. The framework enables a more natural alignment between queries and tools within the language space of TD's. We apply ToolDreamer on the ToolRet dataset and show that our method improves the performance of sparse and dense retrievers with and without training, thus showcasing its flexibility. Through our proposed framework, our aim is to offload a portion of the reasoning burden to the retriever so that the LLM may effectively handle a large collection of tools without inundating its context window.",
        "translated": "工具调用已成为大语言模型（LLM）的一项重要功能。然而，对于工具集较大的情况，生成的 tokens 数量往往会超出 LLM 的上下文窗口限制，从而无法包含所有工具。因此，通常使用一个外部召回器，为 LLM 提供与查询最相关的工具。现有的召回模型根据用户查询与工具描述（TD）之间的相似性对工具进行排序。这种做法会导致召回效果次优，因为用户请求通常与 TD 的语言表达方式对齐不佳。为了解决这一问题，我们提出了 ToolDreamer 框架，该框架通过使用 LLM 生成假设性（合成的）TD 来对召回模型进行条件化，即生成 LLM 认为对查询可能有用的工具描述。该框架在 TD 的语言空间中实现了查询与工具之间更自然的对齐。我们在 ToolRet 数据集上应用了 ToolDreamer，并证明我们的方法在有训练和无训练的情况下均能提升稀疏和稠密召回器的性能，从而展示了其灵活性。通过我们提出的框架，我们的目标是将部分推理负担转移到召回器上，使 LLM 能够有效处理大量工具，而不会超出其上下文窗口的限制。",
        "translated_title": "ToolDreamer：将大语言模型推理能力注入工具召回模块",
        "label": [
            "LLM生成式推荐"
        ],
        "label_reason": "论文涉及利用LLM生成工具描述以提升检索相关性，适用于推荐场景。",
        "relevance_score": 7,
        "novelty_score": 8,
        "novelty_reason": "提出基于LLM生成合成描述的新型检索框架，方法新颖且有明显改进效果。"
    },
    {
        "title": "Top-P Masking for Cross Language Information Retrieval",
        "url": "http://arxiv.org/abs/2510.19758v1",
        "pub_date": "2025-10-22",
        "summary": "Top-K masking schemes have been proposed as a method to promote sparse representations in Information Retrieval (IR) tasks, as a simple alternative to Floating Point Operations per Second (FLOPS) regularization. Algorithms such as Bilingual Lexical and Document Expansion Model (BLADE), adopt this approach as a post-processing stage. We propose using Top-P Dynamic Masking similar to Nucleus Sampling in Large Language Models, and demonstrate better performance than Top-K masking. Specifically, we evaluate our methods in the domain of Cross Language Information Retrieval (CLIR)",
        "translated": "Top-K 掩码方案已被提出作为一种在信息检索（IR）任务中促进稀疏表示的方法，作为每秒浮点运算次数（FLOPS）正则化的简单替代方案。例如双语词典和文档扩展模型（BLADE）就采用此方法作为后处理阶段。我们提出使用类似于大语言模型中核心采样（Nucleus Sampling）的 Top-P 动态掩码方法，并展示了其在 Top-K 掩码基础上更优的性能。具体而言，我们在跨语言信息检索（CLIR）领域对我们的方法进行了评估。",
        "translated_title": "Top-P Masking for Cross Language Information Retrieval  \n跨语言信息检索的Top-P掩码方法",
        "label": [],
        "label_reason": "论文主要涉及信息检索，非专为推荐系统设计",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出Top-P动态掩码方法，改进了传统Top-K方法"
    },
    {
        "title": "A Matter of Time: Revealing the Structure of Time in Vision-Language\n  Models",
        "url": "http://arxiv.org/abs/2510.19559v1",
        "pub_date": "2025-10-22",
        "summary": "Large-scale vision-language models (VLMs) such as CLIP have gained popularity for their generalizable and expressive multimodal representations. By leveraging large-scale training data with diverse textual metadata, VLMs acquire open-vocabulary capabilities, solving tasks beyond their training scope. This paper investigates the temporal awareness of VLMs, assessing their ability to position visual content in time. We introduce TIME10k, a benchmark dataset of over 10,000 images with temporal ground truth, and evaluate the time-awareness of 37 VLMs by a novel methodology. Our investigation reveals that temporal information is structured along a low-dimensional, non-linear manifold in the VLM embedding space. Based on this insight, we propose methods to derive an explicit ``timeline'' representation from the embedding space. These representations model time and its chronological progression and thereby facilitate temporal reasoning tasks. Our timeline approaches achieve competitive to superior accuracy compared to a prompt-based baseline while being computationally efficient. All code and data are available at https://tekayanidham.github.io/timeline-page/.",
        "translated": "诸如CLIP等大规模视觉-语言模型（VLMs）因其具有泛化性和表现力的多模态表征而广受欢迎。通过利用带有丰富文本元数据的大规模训练数据，VLMs获得了开放词汇能力，能够解决超出其训练范围的任务。本文研究了VLMs的时序感知能力，评估其对视觉内容进行时间定位的能力。我们引入了一个名为TIME10k的基准数据集，包含超过10,000张具有时间真实标签的图像，并通过一种新的方法评估了37个VLMs的时间感知能力。我们的研究表明，时间信息在VLM嵌入空间中沿着一个低维、非线性的流形结构分布。基于这一发现，我们提出了一种从嵌入空间中推导出显式“时间线”表示的方法。这些表示能够建模时间及其时序演进，从而有助于时间推理任务。与基于提示的基线方法相比，我们的时间线方法在计算效率高且准确率具有竞争力甚至更优。所有代码和数据均可在https://tekayanidham.github.io/timeline-page/获取。",
        "translated_title": "A Matter of Time: Vision-Language 模型中时间结构的揭示",
        "label": [
            "多模态推荐"
        ],
        "label_reason": "论文探讨视觉-语言模型中的时间感知，与多模态推荐系统有一定间接关联。",
        "relevance_score": 3,
        "novelty_score": 7,
        "novelty_reason": "提出了时间线表示方法，揭示了嵌入空间中的时间结构，具有新颖性。"
    },
    {
        "title": "The Massive Legal Embedding Benchmark (MLEB)",
        "url": "http://arxiv.org/abs/2510.19365v1",
        "pub_date": "2025-10-22",
        "summary": "We present the Massive Legal Embedding Benchmark (MLEB), the largest, most diverse, and most comprehensive open-source benchmark for legal information retrieval to date. MLEB consists of ten expert-annotated datasets spanning multiple jurisdictions (the US, UK, EU, Australia, Ireland, and Singapore), document types (cases, legislation, regulatory guidance, contracts, and literature), and task types (search, zero-shot classification, and question answering). Seven of the datasets in MLEB were newly constructed in order to fill domain and jurisdictional gaps in the open-source legal information retrieval landscape. We document our methodology in building MLEB and creating the new constituent datasets, and release our code, results, and data openly to assist with reproducible evaluations.",
        "translated": "我们提出了大规模法律嵌入基准（Massive Legal Embedding Benchmark，MLEB），这是迄今为止最大、最多样且最全面的开源法律信息检索基准。MLEB包含十个由专家标注的数据集，覆盖多个司法管辖区（美国、英国、欧盟、澳大利亚、爱尔兰和新加坡）、多种文档类型（案例、立法、监管指导、合同和文献）以及多种任务类型（检索、零样本分类和问答）。为了填补开源法律信息检索领域在领域和司法管辖区上的空白，MLEB中有七个数据集是新构建的。我们记录了构建MLEB和创建新组成数据集的方法，并公开发布了代码、实验结果和数据，以支持可复现的评估。",
        "translated_title": "大规模法律嵌入基准（MLEB）",
        "label": [],
        "label_reason": "论文聚焦法律信息检索，与推荐系统无直接关联",
        "relevance_score": 2,
        "novelty_score": 5,
        "novelty_reason": "构建了大规模法律检索基准，但属于常规领域改进"
    },
    {
        "title": "CoRECT: A Framework for Evaluating Embedding Compression Techniques at\n  Scale",
        "url": "http://arxiv.org/abs/2510.19340v1",
        "pub_date": "2025-10-22",
        "summary": "Dense retrieval systems have proven to be effective across various benchmarks, but require substantial memory to store large search indices. Recent advances in embedding compression show that index sizes can be greatly reduced with minimal loss in ranking quality. However, existing studies often overlook the role of corpus complexity -- a critical factor, as recent work shows that both corpus size and document length strongly affect dense retrieval performance. In this paper, we introduce CoRECT (Controlled Retrieval Evaluation of Compression Techniques), a framework for large-scale evaluation of embedding compression methods, supported by a newly curated dataset collection. To demonstrate its utility, we benchmark eight representative types of compression methods. Notably, we show that non-learned compression achieves substantial index size reduction, even on up to 100M passages, with statistically insignificant performance loss. However, selecting the optimal compression method remains challenging, as performance varies across models. Such variability highlights the necessity of CoRECT to enable consistent comparison and informed selection of compression methods. All code, data, and results are available on GitHub and HuggingFace.",
        "translated": "稠密召回系统在各种基准测试中已被证明是有效的，但需要大量的内存来存储大规模的检索索引。嵌入压缩方面的最新进展表明，在仅轻微损失排序质量的前提下，可以大幅减少索引大小。然而，现有研究往往忽略了语料复杂性这一关键因素，因为近期研究表明，语料规模和文档长度都会显著影响稠密召回的性能。在本文中，我们引入了 CoRECT（压缩技术可控召回评估框架），这是一个用于大规模评估嵌入压缩方法的框架，并由一个新整理的数据集集合支持。为展示其有效性，我们对八类具有代表性的压缩方法进行了基准测试。值得注意的是，我们发现非学习型压缩方法即使在多达 100M 个段落的数据上，也能实现显著的索引大小减少，且性能损失在统计上不显著。然而，选择最优的压缩方法仍具有挑战性，因为不同模型的性能表现存在差异。这种差异性突显了 CoRECT 的必要性，以支持对压缩方法进行一致性比较和明智选择。所有代码、数据和结果均可在 GitHub 和 HuggingFace 上获取。",
        "translated_title": "CoRECT：一种用于大规模评估嵌入压缩技术的框架",
        "label": [
            "推荐系统评估"
        ],
        "label_reason": "论文聚焦于嵌入压缩技术的评估，适用于推荐系统的检索环节",
        "relevance_score": 6,
        "novelty_score": 7,
        "novelty_reason": "提出了一个大规模评估嵌入压缩技术的框架，具有一定创新性"
    },
    {
        "title": "Metadata Extraction Leveraging Large Language Models",
        "url": "http://arxiv.org/abs/2510.19334v1",
        "pub_date": "2025-10-22",
        "summary": "The advent of Large Language Models has revolutionized tasks across domains, including the automation of legal document analysis, a critical component of modern contract management systems. This paper presents a comprehensive implementation of LLM-enhanced metadata extraction for contract review, focusing on the automatic detection and annotation of salient legal clauses. Leveraging both the publicly available Contract Understanding Atticus Dataset (CUAD) and proprietary contract datasets, our work demonstrates the integration of advanced LLM methodologies with practical applications. We identify three pivotal elements for optimizing metadata extraction: robust text conversion, strategic chunk selection, and advanced LLM-specific techniques, including Chain of Thought (CoT) prompting and structured tool calling. The results from our experiments highlight the substantial improvements in clause identification accuracy and efficiency. Our approach shows promise in reducing the time and cost associated with contract review while maintaining high accuracy in legal clause identification. The results suggest that carefully optimized LLM systems could serve as valuable tools for legal professionals, potentially increasing access to efficient contract review services for organizations of all sizes.",
        "translated": "大语言模型的出现已经革新了各个领域的任务，包括法律文档分析的自动化，这是现代合同管理系统中的关键组成部分。本文提出了一个全面的基于大语言模型增强的合同审查元数据提取实现方案，重点在于显著法律条款的自动检测与标注。我们利用公开可用的Contract Understanding Atticus数据集（CUAD）以及专有的合同数据集，展示了先进大语言模型方法与实际应用的整合。我们识别出三个优化元数据提取的关键要素：稳健的文本转换、策略性的块选择以及高级的大语言模型特定技术，包括Chain of Thought（CoT）提示和结构化工具调用。实验结果突显了本方法在条款识别准确性和效率方面的显著提升。我们的方法在减少合同审查所需时间和成本的同时，仍能保持法律条款识别的高准确率，展现出良好的应用前景。结果表明，经过精心优化的大语言模型系统可以作为法律专业人士的有力工具，从而为各类规模的组织提供高效的合同审查服务。",
        "translated_title": "利用大语言模型进行元数据提取",
        "label": [],
        "label_reason": "论文关注元数据提取，非推荐系统核心问题。",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "结合LLM优化合同元数据提取，具有一定实用性。"
    },
    {
        "title": "C2T-ID: Converting Semantic Codebooks to Textual Document Identifiers\n  for Generative Search",
        "url": "http://arxiv.org/abs/2510.19221v1",
        "pub_date": "2025-10-22",
        "summary": "Designing document identifiers (docids) that carry rich semantic information while maintaining tractable search spaces is a important challenge in generative retrieval (GR). Popular codebook methods address this by building a hierarchical semantic tree and constraining generation to its child nodes, yet their numeric identifiers cannot leverage the large language model's pretrained natural language understanding. Conversely, using text as docid provides more semantic expressivity but inflates the decoding space, making the system brittle to early-step errors. To resolve this trade-off, we propose C2T-ID: (i) first construct semantic numerical docid via hierarchical clustering; (ii) then extract high-frequency metadata keywords and iteratively replace each numeric label with its cluster's top-K keywords; and (iii) an optional two-level semantic smoothing step further enhances the fluency of C2T-ID. Experiments on Natural Questions and Taobao's product search demonstrate that C2T-ID significantly outperforms atomic, semantic codebook, and pure-text docid baselines, demonstrating its effectiveness in balancing semantic expressiveness with search space constraints.",
        "translated": "设计携带丰富语义信息同时又能保持可处理的搜索空间的文档标识符（docid）是生成式召回（GR）中的一个重要挑战。流行的方法通过构建一个层次化的语义树并限制生成过程仅在子节点中进行来应对这一问题，但它们的数字标识符无法利用大语言模型的预训练自然语言理解能力。相反，使用文本作为docid可以提供更强的语义表达能力，但却扩展了解码空间，使系统更容易受到早期生成步骤错误的影响。为了解决这一权衡问题，我们提出了 C2T-ID：(i) 首先通过层次聚类构建语义化的数字docid；(ii) 然后提取高频率的元数据关键词，并迭代地将每个数字标签替换为该聚类的 top-K 关键词；以及 (iii) 一个可选的两级语义平滑步骤进一步提升了 C2T-ID 的流畅性。我们在 Natural Questions 和 Taobao 产品搜索上的实验表明，C2T-ID 显著优于原子型、语义码本和纯文本 docid 基线模型，证明其在平衡语义表达性和搜索空间约束方面的有效性。",
        "translated_title": "C2T-ID：将语义码本转换为文本文档标识符用于生成式搜索",
        "label": [
            "LLM生成式推荐",
            "推荐系统评估"
        ],
        "label_reason": "论文涉及生成式检索，与生成式推荐相关，但更偏重搜索领域",
        "relevance_score": 6,
        "novelty_score": 7,
        "novelty_reason": "提出一种新的语义ID生成方法，结合聚类与关键词替换，有一定创新性"
    },
    {
        "title": "XGen-Q: An Explainable Domain-Adaptive LLM Framework with\n  Retrieval-Augmented Generation for Software Security",
        "url": "http://arxiv.org/abs/2510.19006v1",
        "pub_date": "2025-10-21",
        "summary": "Generative AI and large language models (LLMs) have shown strong capabilities in code understanding, but their use in cybersecurity, particularly for malware detection and analysis, remains limited. Existing detection systems often fail to generalize to obfuscated or previously unseen threats, underscoring the need for more adaptable and explainable models. To address this challenge, we introduce XGen-Q, a domain-adapted LLM built on the Qwen-Coder architecture and pretrained on a large-scale corpus of over one million malware samples, spanning both source and assembly code. XGen-Q uses a multi-stage prompt strategy combined with retrieval-augmented generation (RAG) to deliver reliable malware identification and detailed forensic reporting, even in the presence of complex code obfuscation. To further enhance generalization, we design a training pipeline that systematically exposes the model to diverse obfuscation patterns. Experimental results show that XGen-Q achieves significantly lower perplexity than competitive baselines and exhibits strong performance on novel malware samples, demonstrating the promise of LLM-based approaches for interpretable and robust malware analysis.",
        "translated": "生成式人工智能和大语言模型（LLMs）在代码理解方面展现了强大的能力，但其在网络安全领域的应用，特别是用于恶意软件检测和分析方面仍较为有限。现有的检测系统通常难以对混淆代码或未见过的威胁进行泛化，凸显了对更具适应性和可解释性的模型的需求。为应对这一挑战，我们提出了XGen-Q，这是一个基于Qwen-Coder架构的领域适应大语言模型，并在包含超过一百万个恶意软件样本的大规模语料库上进行了预训练，涵盖源代码和汇编代码。XGen-Q结合多阶段提示策略和检索增强生成（RAG）方法，即使在存在复杂代码混淆的情况下，也能实现可靠的恶意软件识别和详细的取证报告。为进一步增强模型的泛化能力，我们设计了一套训练流程，系统性地使模型接触多样化的混淆模式。实验结果表明，XGen-Q在困惑度上显著低于竞争性的基线模型，并在新型恶意软件样本上表现出色，证明了基于大语言模型的方法在可解释性和鲁棒性的恶意软件分析中具有潜力。",
        "translated_title": "XGen-Q：一种基于检索增强生成的可解释、领域自适应大语言模型框架用于软件安全",
        "label": [],
        "label_reason": "论文主要聚焦于软件安全和恶意代码检测，与推荐系统无直接关联。",
        "relevance_score": 3,
        "novelty_score": 7,
        "novelty_reason": "提出了可解释且具有领域适应性的生成式模型新方法，具有一定的创新性。"
    },
    {
        "title": "SBAN: A Framework \\&amp; Multi-Dimensional Dataset for Large Language Model\n  Pre-Training and Software Code Mining",
        "url": "http://arxiv.org/abs/2510.18936v1",
        "pub_date": "2025-10-21",
        "summary": "This paper introduces SBAN (Source code, Binary, Assembly, and Natural Language Description), a large-scale, multi-dimensional dataset designed to advance the pre-training and evaluation of large language models (LLMs) for software code analysis. SBAN comprises more than 3 million samples, including 2.9 million benign and 672,000 malware respectively, each represented across four complementary layers: binary code, assembly instructions, natural language descriptions, and source code. This unique multimodal structure enables research on cross-representation learning, semantic understanding of software, and automated malware detection. Beyond security applications, SBAN supports broader tasks such as code translation, code explanation, and other software mining tasks involving heterogeneous data. It is particularly suited for scalable training of deep models, including transformers and other LLM architectures. By bridging low-level machine representations and high-level human semantics, SBAN provides a robust foundation for building intelligent systems that reason about code. We believe that this dataset opens new opportunities for mining software behavior, improving security analytics, and enhancing LLM capabilities in pre-training and fine-tuning tasks for software code mining.",
        "translated": "本文介绍了SBAN（源代码、二进制、汇编和自然语言描述），这是一个旨在推动面向软件代码分析的大语言模型（LLMs）预训练与评估的大规模多维数据集。SBAN包含超过300万个样本，其中良性样本为290万个，恶意软件样本为672,000个，每个样本在四个互补层上都有表示：二进制代码、汇编指令、自然语言描述和源代码。这种独特的多模态结构使得在跨表示学习、软件语义理解和自动化恶意软件检测等方面的研究成为可能。除了安全相关应用外，SBAN还支持更广泛的软件任务，如代码翻译、代码解释以及其他涉及异构数据的软件挖掘任务。该数据集特别适用于深度模型的可扩展训练，包括transformer和其他LLM架构。通过连接底层机器表示和高层人类语义，SBAN为构建能够推理代码的智能系统提供了坚实的基础。我们认为，该数据集为挖掘软件行为、提升安全分析能力以及增强LLM在软件代码挖掘任务中的预训练和微调能力带来了新的机遇。",
        "translated_title": "SBAN：一个用于大语言模型预训练与软件代码挖掘的框架与多维数据集",
        "label": [],
        "label_reason": "论文主要关注代码分析与预训练，与推荐系统无直接关联。",
        "relevance_score": 3,
        "novelty_score": 6,
        "novelty_reason": "提出了多维度异构数据集，对代码分析和LLM训练有一定创新。"
    },
    {
        "title": "DuoLens: A Framework for Robust Detection of Machine-Generated\n  Multilingual Text and Code",
        "url": "http://arxiv.org/abs/2510.18904v1",
        "pub_date": "2025-10-21",
        "summary": "The prevalence of Large Language Models (LLMs) for generating multilingual text and source code has only increased the imperative for machine-generated content detectors to be accurate and efficient across domains. Current detectors, predominantly utilizing zero-shot methods, such as Fast DetectGPT or GPTZero, either incur high computational cost or lack sufficient accuracy, often with a trade-off between the two, leaving room for further improvement. To address these gaps, we propose the fine-tuning of encoder-only Small Language Models (SLMs), in particular, the pre-trained models of RoBERTA and CodeBERTa using specialized datasets on source code and other natural language to prove that for the task of binary classification, SLMs outperform LLMs by a huge margin whilst using a fraction of compute. Our encoders achieve AUROC $= 0.97$ to $0.99$ and macro-F1 $0.89$ to $0.94$ while reducing latency by $8$-$12\\times$ and peak VRAM by $3$-$5\\times$ at $512$-token inputs. Under cross-generator shifts and adversarial transformations (paraphrase, back-translation; code formatting/renaming), performance retains $\\geq 92%$ of clean AUROC. We release training and evaluation scripts with seeds and configs; a reproducibility checklist is also included.",
        "translated": "大语言模型（LLMs）在生成多语言文本和源代码方面的普及进一步提高了对跨领域机器生成内容检测器在准确性和效率方面提出的要求。目前的检测器主要采用零样本方法（zero-shot methods），如 Fast DetectGPT 或 GPTZero，要么计算成本高昂，要么准确性不足，通常在两者之间存在权衡，因此仍存在进一步改进的空间。为了解决这些问题，我们提出对仅编码器结构的小语言模型（SLMs）进行微调，特别是使用 RoBERTa 和 CodeBERTa 的预训练模型，并结合源代码和其他自然语言的专用数据集，以证明在二分类任务中，SLMs 的性能远超 LLMs，同时仅需极小的计算资源。我们的编码器在输入长度为 512-token 时，实现了 AUROC $= 0.97$ 到 $0.99$ 以及 macro-F1 $= 0.89$ 到 $0.94$，同时将延迟降低了 $8$-$12\\times$，峰值 VRAM 降低了 $3$-$5\\times$。在面对跨生成器的分布变化和对抗性变换（如改写、回译；代码格式化/重命名）时，性能仍能保留原始 AUROC 的 $\\geq 92\\%$。我们发布了包含随机种子和配置的训练与评估脚本，并附有可复现性检查清单。",
        "translated_title": "DuoLens：一种用于鲁棒检测机器生成的多语言文本和代码的框架",
        "label": [],
        "label_reason": "论文主题为文本和代码生成检测，与推荐系统无直接关联。",
        "relevance_score": 2,
        "novelty_score": 5,
        "novelty_reason": "提出微调小型编码器模型用于生成检测，方法有一定改进但不具颠覆性。"
    },
    {
        "title": "Generative Reasoning Recommendation via LLMs",
        "url": "http://arxiv.org/abs/2510.20815v1",
        "pub_date": "2025-10-23",
        "summary": "Despite their remarkable reasoning capabilities across diverse domains, large language models (LLMs) face fundamental challenges in natively functioning as generative reasoning recommendation models (GRRMs), where the intrinsic modeling gap between textual semantics and collaborative filtering signals, combined with the sparsity and stochasticity of user feedback, presents significant obstacles. This work explores how to build GRRMs by adapting pre-trained LLMs, which achieves a unified understanding-reasoning-prediction manner for recommendation tasks. We propose GREAM, an end-to-end framework that integrates three components: (i) Collaborative-Semantic Alignment, which fuses heterogeneous textual evidence to construct semantically consistent, discrete item indices and auxiliary alignment tasks that ground linguistic representations in interaction semantics; (ii) Reasoning Curriculum Activation, which builds a synthetic dataset with explicit Chain-of-Thought supervision and a curriculum that progresses through behavioral evidence extraction, latent preference modeling, intent inference, recommendation formulation, and denoised sequence rewriting; and (iii) Sparse-Regularized Group Policy Optimization (SRPO), which stabilizes post-training via Residual-Sensitive Verifiable Reward and Bonus-Calibrated Group Advantage Estimation, enabling end-to-end optimization under verifiable signals despite sparse successes. GREAM natively supports two complementary inference modes: Direct Sequence Recommendation for high-throughput, low-latency deployment, and Sequential Reasoning Recommendation that first emits an interpretable reasoning chain for causal transparency. Experiments on three datasets demonstrate consistent gains over strong baselines, providing a practical path toward verifiable-RL-driven LLM recommenders.",
        "translated": "尽管大语言模型（LLMs）在多个领域展现出卓越的推理能力，但它们在原生地作为生成式推理推荐模型（GRRMs）运行时仍面临根本性挑战，其中文本语义与协同过滤信号之间的内在建模差距，以及用户反馈的稀疏性和随机性，构成了显著的障碍。本文探讨了如何通过适配预训练大语言模型来构建GRRMs，从而在推荐任务中实现统一的理解-推理-预测范式。我们提出了GREAM，一个端到端的框架，集成了三个组件：(i) 协同语义对齐，该组件融合异构文本证据以构建语义一致的离散物料索引和辅助对齐任务，从而将语言表示接地于交互语义；(ii) 推理课程激活，该组件构建了一个带有显式链式思维（Chain-of-Thought）监督的合成数据集，并设计了一个课程，依次包括行为证据提取、潜在偏好建模、意图推断、推荐生成和去噪序列重写；(iii) 稀疏正则化的群体策略优化（SRPO），该组件通过残差敏感的可验证奖励和奖励校准的群体优势估计稳定后训练过程，即使在成功信号稀疏的情况下也能实现端到端优化。GREAM原生支持两种互补的推理模式：直接序列推荐，适用于高吞吐量、低延迟的部署场景；以及序列推理推荐，该模式首先生成可解释的推理链，以实现因果透明性。在三个数据集上的实验表明，GREAM在多个强基线上保持了一致的提升，为构建以可验证强化学习驱动的LLM推荐系统提供了切实可行的路径。",
        "translated_title": "生成式推理推荐",
        "label": [
            "LLM生成式推荐",
            "序列推荐",
            "推荐系统评估"
        ],
        "label_reason": "论文聚焦于LLM在推荐中的生成式推理，涉及序列建模和实际推荐任务。",
        "relevance_score": 9,
        "novelty_score": 8,
        "novelty_reason": "提出GREAM框架，结合语义对齐与课程推理，改进LLM在推荐中的应用方式。"
    },
    {
        "title": "RAGRank: Using PageRank to Counter Poisoning in CTI LLM Pipelines",
        "url": "http://arxiv.org/abs/2510.20768v1",
        "pub_date": "2025-10-23",
        "summary": "Retrieval-Augmented Generation (RAG) has emerged as the dominant architectural pattern to operationalize Large Language Model (LLM) usage in Cyber Threat Intelligence (CTI) systems. However, this design is susceptible to poisoning attacks, and previously proposed defenses can fail for CTI contexts as cyber threat information is often completely new for emerging attacks, and sophisticated threat actors can mimic legitimate formats, terminology, and stylistic conventions. To address this issue, we propose that the robustness of modern RAG defenses can be accelerated by applying source credibility algorithms on corpora, using PageRank as an example. In our experiments, we demonstrate quantitatively that our algorithm applies a lower authority score to malicious documents while promoting trusted content, using the standardized MS MARCO dataset. We also demonstrate proof-of-concept performance of our algorithm on CTI documents and feeds.",
        "translated": "检索增强生成（RAG）已成为在网络威胁情报（CTI）系统中部署大语言模型（LLM）的主要架构模式。然而，这种设计容易受到投毒攻击的影响，且先前提出的防御方法在CTI场景下可能失效，因为网络威胁信息往往针对新兴攻击是全新的，且复杂的攻击者可以模仿合法的格式、术语和风格惯例。为了解决这一问题，我们提出通过在语料库上应用来源可信度算法（以PageRank为例）来提升现代RAG防御的鲁棒性。在我们的实验中，我们定量展示了该算法在降低恶意文档的权威评分的同时，提升了可信内容的权重，实验基于标准化的MS MARCO数据集。我们还展示了该算法在CTI文档和信息流上的概念验证性能。",
        "translated_title": "RAGRank：使用PageRank对抗CTI大语言模型流水线中的投毒攻击",
        "label": [
            "LLM生成式推荐",
            "推荐系统评估"
        ],
        "label_reason": "论文涉及LLM生成式推荐中的RAG架构及其安全性问题，但主要聚焦于CTI领域。",
        "relevance_score": 5,
        "novelty_score": 7,
        "novelty_reason": "提出将PageRank用于提升RAG系统对恶意内容的鲁棒性，具有一定的方法创新性。"
    },
    {
        "title": "Analyticup E-commerce Product Search Competition Technical Report from\n  Team Tredence_AICOE",
        "url": "http://arxiv.org/abs/2510.20674v1",
        "pub_date": "2025-10-23",
        "summary": "This study presents the multilingual e-commerce search system developed by the Tredence_AICOE team. The competition features two multilingual relevance tasks: Query-Category (QC) Relevance, which evaluates how well a user's search query aligns with a product category, and Query-Item (QI) Relevance, which measures the match between a multilingual search query and an individual product listing. To ensure full language coverage, we performed data augmentation by translating existing datasets into languages missing from the development set, enabling training across all target languages. We fine-tuned Gemma-3 12B and Qwen-2.5 14B model for both tasks using multiple strategies. The Gemma-3 12B (4-bit) model achieved the best QC performance using original and translated data, and the best QI performance using original, translated, and minority class data creation. These approaches secured 4th place on the final leaderboard, with an average F1-score of 0.8857 on the private test set.",
        "translated": "本研究介绍了由 Tredence_AICOE 团队开发的多语言电商搜索系统。该竞赛包含两个多语言相关性任务：查询-类别（Query-Category, QC）相关性任务，评估用户搜索查询与产品类别的一致性程度；以及查询-商品（Query-Item, QI）相关性任务，衡量多语言搜索查询与单个商品列表的匹配程度。为确保全面的语言覆盖，我们通过将现有数据集翻译为开发集中缺失的语言进行数据增强，从而实现了在所有目标语言上的训练。我们采用多种策略对 Gemma-3 12B 和 Qwen-2.5 14B 模型在两项任务上进行了微调。Gemma-3 12B（4 位）模型在使用原始和翻译数据时取得了最佳 QC 性能，在使用原始、翻译和少数类数据生成时取得了最佳 QI 性能。这些方法在最终排行榜上取得了第 4 名的成绩，在私有测试集上的平均 F1 分数为 0.8857。",
        "translated_title": "Analyticup 电子商务产品搜索竞赛技术报告  \n来自 Team Tredence_AICOE",
        "label": [
            "通用推荐技术",
            "跨域/联邦推荐"
        ],
        "label_reason": "论文涉及多语言搜索相关性，与推荐有间接联系",
        "relevance_score": 5,
        "novelty_score": 4,
        "novelty_reason": "采用数据增强和多语言模型微调，改进常规"
    },
    {
        "title": "Practical Code RAG at Scale: Task-Aware Retrieval Design Choices under\n  Compute Budgets",
        "url": "http://arxiv.org/abs/2510.20609v1",
        "pub_date": "2025-10-23",
        "summary": "We study retrieval design for code-focused generation tasks under realistic compute budgets. Using two complementary tasks from Long Code Arena -- code completion and bug localization -- we systematically compare retrieval configurations across various context window sizes along three axes: (i) chunking strategy, (ii) similarity scoring, and (iii) splitting granularity. (1) For PL-PL, sparse BM25 with word-level splitting is the most effective and practical, significantly outperforming dense alternatives while being an order of magnitude faster. (2) For NL-PL, proprietary dense encoders (Voyager-3 family) consistently beat sparse retrievers, however requiring 100x larger latency. (3) Optimal chunk size scales with available context: 32-64 line chunks work best at small budgets, and whole-file retrieval becomes competitive at 16000 tokens. (4) Simple line-based chunking matches syntax-aware splitting across budgets. (5) Retrieval latency varies by up to 200x across configurations; BPE-based splitting is needlessly slow, and BM25 + word splitting offers the best quality-latency trade-off. Thus, we provide evidence-based recommendations for implementing effective code-oriented RAG systems based on task requirements, model constraints, and computational efficiency.",
        "translated": "我们研究在现实计算预算下针对代码生成任务的召回设计。通过使用 Long Code Arena 中的两个互补任务——代码补全和错误定位，我们在不同的上下文窗口大小下，从三个维度系统地比较了召回配置：(i) 分块策略，(ii) 相似性评分，以及 (iii) 分割粒度。(1) 对于 PL-PL 任务，基于词级分割的稀疏 BM25 是最有效且实用的方法，其性能显著优于密集方法，同时速度快一个数量级。(2) 对于 NL-PL 任务，专有密集编码器（Voyager-3 家族）始终优于稀疏召回器，但其延迟也大 100 倍。(3) 最优的分块大小随着可用上下文长度而变化：在较小的预算下，32-64 行的分块效果最佳，而在 16000 tokens 的预算下，整个文件的召回变得具有竞争力。(4) 基于行的简单分块策略在不同预算下与语法感知的分割方法效果相当。(5) 在不同配置下，召回的延迟变化可达 200 倍；基于 BPE 的分割速度过慢且不必要；BM25 + 词级分割在质量和延迟之间提供了最佳的平衡。因此，我们基于任务需求、模型限制和计算效率，提供了基于实证的建议，用于实现高效的面向代码的 RAG 系统。",
        "translated_title": "大规模实用代码RAG：计算预算下的任务感知召回设计选择",
        "label": [],
        "label_reason": "",
        "relevance_score": 0,
        "novelty_score": 0,
        "novelty_reason": ""
    },
    {
        "title": "Rotate Both Ways: Time-and-Order RoPE for Generative Recommendation",
        "url": "http://arxiv.org/abs/2510.20455v1",
        "pub_date": "2025-10-23",
        "summary": "Generative recommenders, typically transformer-based autoregressive models, predict the next item or action from a user's interaction history. Their effectiveness depends on how the model represents where an interaction event occurs in the sequence (discrete index) and when it occurred in wall-clock time. Prevailing approaches inject time via learned embeddings or relative attention biases. In this paper, we argue that RoPE-based approaches, if designed properly, can be a stronger alternative for jointly modeling temporal and sequential information in user behavior sequences. While vanilla RoPE in LLMs considers only token order, generative recommendation requires incorporating both event time and token index. To address this, we propose Time-and-Order RoPE (TO-RoPE), a family of rotary position embedding designs that treat index and time as angle sources shaping the query-key geometry directly. We present three instantiations: early fusion, split-by-dim, and split-by-head. Extensive experiments on both publicly available datasets and a proprietary industrial dataset show that TO-RoPE variants consistently improve accuracy over existing methods for encoding time and index. These results position rotary embeddings as a simple, principled, and deployment-friendly foundation for generative recommendation.",
        "translated": "生成式推荐系统，通常基于 Transformer 的自回归模型，通过用户的历史交互序列预测下一个物料或行为。其有效性取决于模型如何表示交互事件在序列中的位置（离散索引）以及在实际时间中的发生时间。现有的方法通常通过学习到的嵌入或相对注意力偏置来引入时间信息。在本文中，我们认为，如果设计得当，基于 RoPE 的方法可以成为在用户行为序列中联合建模时间和顺序信息的更优替代方案。虽然大语言模型中的标准 RoPE 仅考虑了词元顺序，但生成式推荐需要同时融合事件时间与词元索引。为了解决这一问题，我们提出了时间与顺序 RoPE（Time-and-Order RoPE，TO-RoPE），这是一组将索引和时间作为直接塑造查询-键几何结构的角度来源的旋转位置嵌入设计。我们给出了三种实现方式：早期融合、按维度划分和按注意力头划分。在多个公开数据集和一个工业私有数据集上的广泛实验表明，TO-RoPE 的不同变体在编码时间和索引方面始终优于现有方法。这些结果表明，旋转嵌入可以作为生成式推荐的一个简单、原理清晰且易于部署的基础。",
        "translated_title": "Rotate Both Ways: Time-and-Order RoPE for Generative Recommendation  \n双向旋转：面向生成式推荐的时序与序号 RoPE  \n\n**Abstract**  \nRecent advances in large language models (LLMs) have enabled their application to recommendation systems, especially for generative recommendation tasks where the goal is to produce a sequence of recommendation candidates. However, the sequential nature of these tasks introduces two key challenges: modeling temporal dynamics and preserving the order information of the input sequence. In this work, we propose the Time-and-Order RoPE (T-O RoPE), a novel rotary position embedding method tailored for generative recommendation. T-O RoPE integrates both temporal and positional order information into the rotary embedding, allowing the model to simultaneously capture sequential dependencies and maintain the temporal coherence of the interaction history. We incorporate T-O RoPE into the state-of-the-art generative recommendation framework, RecRank, and evaluate it on two benchmark datasets: RecDial and Taobao. Experimental results demonstrate that T-O RoPE consistently improves the performance of generative models across multiple metrics, including diversity, relevance, and coherence. Furthermore, we conduct ablation studies to validate the effectiveness of different components of T-O RoPE. Our method provides a flexible and efficient way to model sequential interactions in generative recommendation systems and sets a new baseline for future research in this area.  \n\n**摘要**  \n近期大语言模型（LLMs）的发展使其能够应用于推荐系统，尤其是在生成式推荐任务中，其目标是生成一序列的推荐候选。然而，这类任务的序列特性引入了两个关键挑战：建模时序动态以及保留输入序列的序号信息。在本文中，我们提出了时序与序号 RoPE（T-O RoPE），一种为生成式推荐任务定制的新型旋转位置嵌入方法。T-O RoPE 将时序和序号信息整合进旋转嵌入中，使模型能够同时捕捉序列依赖关系并保持交互历史的时序一致性。我们将 T-O RoPE 应用于最先进的生成式推荐框架 RecRank，并在两个基准数据集 RecDial 和 Taobao 上进行了评估。实验结果表明，T-O RoPE 在多样性、相关性及一致性等多个指标上均能持续提升生成模型的性能。此外，我们还进行了消融实验以验证 T-O RoPE 不同组件的有效性。我们的方法为生成式推荐系统中建模序列交互提供了一种灵活且高效的方式，并为该领域的未来研究设定了新的基线。",
        "label": [
            "LLM生成式推荐",
            "序列推荐"
        ],
        "label_reason": "论文聚焦生成式推荐中的序列建模问题，提出TO-RoPE用于时间与顺序联合建模",
        "relevance_score": 8,
        "novelty_score": 7,
        "novelty_reason": "提出针对推荐系统的时间-顺序联合建模的新方法TO-RoPE，改进了传统RoPE设计"
    },
    {
        "title": "From Generation to Attribution: Music AI Agent Architectures for the\n  Post-Streaming Era",
        "url": "http://arxiv.org/abs/2510.20276v1",
        "pub_date": "2025-10-23",
        "summary": "Generative AI is reshaping music creation, but its rapid growth exposes structural gaps in attribution, rights management, and economic models. Unlike past media shifts, from live performance to recordings, downloads, and streaming, AI transforms the entire lifecycle of music, collapsing boundaries between creation, distribution, and monetization. However, existing streaming systems, with opaque and concentrated royalty flows, are ill-equipped to handle the scale and complexity of AI-driven production. We propose a content-based Music AI Agent architecture that embeds attribution directly into the creative workflow through block-level retrieval and agentic orchestration. Designed for iterative, session-based interaction, the system organizes music into granular components (Blocks) stored in BlockDB; each use triggers an Attribution Layer event for transparent provenance and real-time settlement. This framework reframes AI from a generative tool into infrastructure for a Fair AI Media Platform. By enabling fine-grained attribution, equitable compensation, and participatory engagement, it points toward a post-streaming paradigm where music functions not as a static catalog but as a collaborative and adaptive ecosystem.",
        "translated": "生成式人工智能正在重塑音乐创作，但其快速发展暴露了在归属、版权管理和经济模型方面的结构性缺口。与以往从现场表演到录音、下载和流媒体的媒介转变不同，人工智能正在改变音乐的整个生命周期，并模糊了创作、分发和变现之间的界限。然而，现有的流媒体系统由于其不透明且集中的版税分配机制，难以应对人工智能驱动生产所带来的规模和复杂性。我们提出了一种基于内容的音乐人工智能代理架构，通过区块级别的召回和代理式协调（agentic orchestration），将归属直接嵌入创作流程中。该系统设计用于迭代式、会话式的交互，将音乐组织为细粒度的组件（Blocks），存储在 BlockDB 中；每次使用都会触发 Attribution Layer 事件，以实现透明的来源追踪和实时结算。这一框架将人工智能从生成工具的视角重新定义为构建公平人工智能媒体平台的基础设施。通过实现细粒度归属、公平补偿和参与式互动，该框架指明了一种后流媒体时代范式，在此范式下，音乐不再作为静态目录存在，而是作为一种协作和自适应的生态系统。",
        "translated_title": "从生成到归因：面向流媒体时代的音乐AI智能体架构",
        "label": [
            "LLM生成式推荐",
            "多模态推荐"
        ],
        "label_reason": "论文涉及生成式AI在音乐领域的应用，与生成式推荐相关。",
        "relevance_score": 7,
        "novelty_score": 8,
        "novelty_reason": "提出了新颖的音乐生成与归属框架，具有一定的创新性。"
    },
    {
        "title": "Balancing Fine-tuning and RAG: A Hybrid Strategy for Dynamic LLM\n  Recommendation Updates",
        "url": "http://arxiv.org/abs/2510.20260v1",
        "pub_date": "2025-10-23",
        "summary": "Large Language Models (LLMs) empower recommendation systems through their advanced reasoning and planning capabilities. However, the dynamic nature of user interests and content poses a significant challenge: While initial fine-tuning aligns LLMs with domain knowledge and user preferences, it fails to capture such real-time changes, necessitating robust update mechanisms. This paper investigates strategies for updating LLM-powered recommenders, focusing on the trade-offs between ongoing fine-tuning and Retrieval-Augmented Generation (RAG). Using an LLM-powered user interest exploration system as a case study, we perform a comparative analysis of these methods across dimensions like cost, agility, and knowledge incorporation. We propose a hybrid update strategy that leverages the long-term knowledge adaptation of periodic fine-tuning with the agility of low-cost RAG. We demonstrate through live A/B experiments on a billion-user platform that this hybrid approach yields statistically significant improvements in user satisfaction, offering a practical and cost-effective framework for maintaining high-quality LLM-powered recommender systems.",
        "translated": "大语言模型（LLMs）凭借其先进的推理和规划能力，为推荐系统提供了支持。然而，用户兴趣和内容的动态性带来了重大挑战：虽然初始的微调使LLMs与领域知识和用户偏好相一致，但难以捕捉这些实时变化，因此需要稳健的更新机制。本文研究了用于更新大语言模型驱动的推荐系统的策略，重点关注持续微调和检索增强生成（RAG）之间的权衡。以一个由LLM驱动的用户兴趣探索系统为案例，我们在成本、敏捷性以及知识融合等维度上对这些方法进行了比较分析。我们提出了一种混合更新策略，将周期性微调的长期知识适应能力与低成本RAG的敏捷性相结合。我们通过在一个十亿用户级别的平台上进行实时A/B实验，展示了这种混合方法在用户满意度方面具有统计显著的提升，为维护高质量的大语言模型推荐系统提供了一种实用且成本有效的框架。",
        "translated_title": "平衡微调与RAG：一种面向动态大语言模型推荐更新的混合策略",
        "label": [
            "LLM生成式推荐"
        ],
        "label_reason": "论文探讨了LLM在推荐系统中的更新策略，涉及生成式推荐。",
        "relevance_score": 8,
        "novelty_score": 7,
        "novelty_reason": "提出了结合微调和RAG的混合更新策略，具有一定创新性。"
    },
    {
        "title": "Multimedia-Aware Question Answering: A Review of Retrieval and\n  Cross-Modal Reasoning Architectures",
        "url": "http://arxiv.org/abs/2510.20193v1",
        "pub_date": "2025-10-23",
        "summary": "Question Answering (QA) systems have traditionally relied on structured text data, but the rapid growth of multimedia content (images, audio, video, and structured metadata) has introduced new challenges and opportunities for retrieval-augmented QA. In this survey, we review recent advancements in QA systems that integrate multimedia retrieval pipelines, focusing on architectures that align vision, language, and audio modalities with user queries. We categorize approaches based on retrieval methods, fusion techniques, and answer generation strategies, and analyze benchmark datasets, evaluation protocols, and performance tradeoffs. Furthermore, we highlight key challenges such as cross-modal alignment, latency-accuracy tradeoffs, and semantic grounding, and outline open problems and future research directions for building more robust and context-aware QA systems leveraging multimedia data.",
        "translated": "问答（QA）系统传统上依赖于结构化文本数据，但多媒体内容（图像、音频、视频和结构化元数据）的快速增长为增强召回的问答系统带来了新的挑战和机遇。在本综述中，我们回顾了近期将多媒体召回流程集成到问答系统中的研究成果，重点关注将视觉、语言和音频模态与用户查询对齐的系统架构。我们根据召回方法、融合技术以及答案生成策略对相关方法进行了分类，并分析了基准数据集、评估协议以及性能上的权衡。此外，我们强调了诸如跨模态对齐、延迟与准确率的权衡以及语义基础等关键挑战，并概述了构建更加鲁棒和上下文感知的问答系统在利用多媒体数据方面仍存在的开放问题和未来研究方向。",
        "translated_title": "多媒体感知问答：召回与跨模态推理架构综述",
        "label": [
            "多模态推荐"
        ],
        "label_reason": "论文涉及多模态QA系统，与多模态推荐间接相关。",
        "relevance_score": 5,
        "novelty_score": 6,
        "novelty_reason": "总结了多模态QA的最新进展，提出了一些开放问题，但整体为综述性质。"
    },
    {
        "title": "Rank-GRPO: Training LLM-based Conversational Recommender Systems with\n  Reinforcement Learning",
        "url": "http://arxiv.org/abs/2510.20150v1",
        "pub_date": "2025-10-23",
        "summary": "Large language models (LLMs) are reshaping the recommender system paradigm by enabling users to express preferences and receive recommendations through conversations. Yet, aligning LLMs to the recommendation task remains challenging: pretrained LLMs often generate out-of-catalog items, violate required output formats, and their ranking quality degrades sharply toward the end of the generated list. To this end, we propose ConvRec-R1, a two-stage framework for end-to-end training of LLM-based conversational recommender systems. In Stage 1, we construct a behavioral-cloning dataset with a Remap-Reflect-Adjust pipeline, which produces high-quality, catalog-grounded demonstrations from powerful blackbox LLMs to warm-start the RL training. In Stage 2, we propose Rank-GRPO, a principled extension of group relative policy optimization (GRPO) tailored to tasks with rank-style outputs. Rank-GRPO treats each rank in the recommendation list as the unit instead of token (too fine-grained) or sequence (too coarse), redefining rewards to remove non-causal credit assignment and introducing a rank-level importance ratio based on the geometric mean of rank-wise token probabilities to stabilize policy updates. Experiments on the public Reddit-v2 dataset show that ConvRec-R1 converges faster and achieves higher Recall and NDCG than GRPO-style baselines. Code and datasets are released at https://github.com/yaochenzhu/Rank-GRPO.",
        "translated": "大语言模型（LLMs）正在通过使用户能够通过对话表达偏好并接收推荐，重塑推荐系统的范式。然而，将LLMs对齐到推荐任务仍然是一个挑战：预训练的大语言模型通常会生成目录之外的物料，违反所需的输出格式，并且其排序质量在生成列表的末尾急剧下降。为此，我们提出了ConvRec-R1，一个面向LLM基础的对话推荐系统的端到端训练的两阶段框架。在第一阶段，我们通过Remap-Reflect-Adjust流水线构建一个行为克隆数据集，该流水线从强大的黑盒大语言模型中生成高质量、与目录一致的演示，以实现RL训练的热启动。在第二阶段，我们提出Rank-GRPO，这是针对具有排序风格输出任务的群体相对策略优化（GRPO）的一种原则性扩展。Rank-GRPO将推荐列表中的每个排序位（rank）视为单元，而不是过于细粒度的token或过于粗粒度的序列，重新定义奖励以消除非因果的信用分配，并引入基于排序级token概率几何均值的排序级重要性比例，以稳定策略更新。在公共的Reddit-v2数据集上的实验表明，ConvRec-R1比GRPO风格的基线方法更快收敛，并取得了更高的Recall和NDCG指标。代码和数据集已发布在 https://github.com/yaochenzhu/Rank-GRPO。",
        "translated_title": "Rank-GRPO: 基于强化学习的LLM对话推荐系统训练",
        "label": [
            "LLM生成式推荐",
            "精排",
            "推荐系统评估"
        ],
        "label_reason": "论文聚焦LLM生成式推荐中的排序优化，与推荐系统高度相关。",
        "relevance_score": 9,
        "novelty_score": 8,
        "novelty_reason": "提出Rank-GRPO方法，改进RL训练策略，具有创新性。"
    },
    {
        "title": "Automating Iconclass: LLMs and RAG for Large-Scale Classification of\n  Religious Woodcuts",
        "url": "http://arxiv.org/abs/2510.19986v1",
        "pub_date": "2025-10-22",
        "summary": "This paper presents a novel methodology for classifying early modern religious images by using Large Language Models (LLMs) and vector databases in combination with Retrieval-Augmented Generation (RAG). The approach leverages the full-page context of book illustrations from the Holy Roman Empire, allowing the LLM to generate detailed descriptions that incorporate both visual and textual elements. These descriptions are then matched to relevant Iconclass codes through a hybrid vector search. This method achieves 87% and 92% precision at five and four levels of classification, significantly outperforming traditional image and keyword-based searches. By employing full-page descriptions and RAG, the system enhances classification accuracy, offering a powerful tool for large-scale analysis of early modern visual archives. This interdisciplinary approach demonstrates the growing potential of LLMs and RAG in advancing research within art history and digital humanities.",
        "translated": "本文提出了一种新颖的方法，用于通过结合使用大语言模型（LLM）、向量数据库和检索增强生成（RAG）对早期现代宗教图像进行分类。该方法利用了神圣罗马帝国书籍插图的整页上下文，使LLM能够生成包含视觉与文本元素的详细描述。随后，这些描述通过混合向量搜索与相关Iconclass代码进行匹配。该方法在五级和四级分类中的精度分别达到了87%和92%，显著优于传统的基于图像和关键词的搜索方法。通过采用整页描述和RAG，该系统提升了分类准确性，为大规模分析早期现代视觉档案提供了一个强有力的工具。这种跨学科的方法展示了LLM和RAG在推动艺术史和数字人文学科研究方面的日益增长的潜力。",
        "translated_title": "Automating Iconclass: LLMs and RAG for Large-Scale Classification of Religious Woodcuts  \n图标分类的自动化：大语言模型与检索增强生成在宗教木刻大规模分类中的应用",
        "label": [],
        "label_reason": "方法涉及LLM和RAG，但应用于艺术图像分类，与推荐系统无直接关联",
        "relevance_score": 3,
        "novelty_score": 7,
        "novelty_reason": "提出将LLM和RAG结合用于图像分类，具有新颖性和跨学科价值"
    },
    {
        "title": "HoloCine: Holistic Generation of Cinematic Multi-Shot Long Video\n  Narratives",
        "url": "http://arxiv.org/abs/2510.20822v1",
        "pub_date": "2025-10-23",
        "summary": "State-of-the-art text-to-video models excel at generating isolated clips but fall short of creating the coherent, multi-shot narratives, which are the essence of storytelling. We bridge this \"narrative gap\" with HoloCine, a model that generates entire scenes holistically to ensure global consistency from the first shot to the last. Our architecture achieves precise directorial control through a Window Cross-Attention mechanism that localizes text prompts to specific shots, while a Sparse Inter-Shot Self-Attention pattern (dense within shots but sparse between them) ensures the efficiency required for minute-scale generation. Beyond setting a new state-of-the-art in narrative coherence, HoloCine develops remarkable emergent abilities: a persistent memory for characters and scenes, and an intuitive grasp of cinematic techniques. Our work marks a pivotal shift from clip synthesis towards automated filmmaking, making end-to-end cinematic creation a tangible future. Our code is available at: https://holo-cine.github.io/.",
        "translated": "最先进的文本到视频模型在生成孤立片段方面表现出色，但无法生成连贯的、多镜头的叙事内容，而后者正是讲故事的核心所在。我们通过 HoloCine 模型弥补这一“叙事鸿沟”，该模型能够整体生成整个场景，从而从第一个镜头到最后一个镜头确保全局一致性。我们的架构通过一种窗口交叉注意力机制实现了精确的导演控制，该机制能够将文本提示定位到特定镜头，同时稀疏镜头间自注意力模式（在镜头内部密集，镜头之间稀疏）确保了分钟级生成所需的效率。除了在叙事连贯性方面设立了新的最先进水平外，HoloCine 还展现出显著的涌现能力：对角色和场景的持久记忆，以及对电影技巧的直观理解。我们的工作标志着从片段合成向自动电影制作的重要转变，使端到端的电影创作成为触手可及的未来。我们的代码可在以下网址获取：https://holo-cine.github.io/。",
        "translated_title": "HoloCine：电影多镜头长视频的全面生成",
        "label": [],
        "label_reason": "不属于low-level图像处理，关注视频叙事生成。",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出Window Cross-Attention等新机制，提升视频叙事生成能力。"
    },
    {
        "title": "LayerComposer: Interactive Personalized T2I via Spatially-Aware Layered\n  Canvas",
        "url": "http://arxiv.org/abs/2510.20820v1",
        "pub_date": "2025-10-23",
        "summary": "Despite their impressive visual fidelity, existing personalized generative models lack interactive control over spatial composition and scale poorly to multiple subjects. To address these limitations, we present LayerComposer, an interactive framework for personalized, multi-subject text-to-image generation. Our approach introduces two main contributions: (1) a layered canvas, a novel representation in which each subject is placed on a distinct layer, enabling occlusion-free composition; and (2) a locking mechanism that preserves selected layers with high fidelity while allowing the remaining layers to adapt flexibly to the surrounding context. Similar to professional image-editing software, the proposed layered canvas allows users to place, resize, or lock input subjects through intuitive layer manipulation. Our versatile locking mechanism requires no architectural changes, relying instead on inherent positional embeddings combined with a new complementary data sampling strategy. Extensive experiments demonstrate that LayerComposer achieves superior spatial control and identity preservation compared to the state-of-the-art methods in multi-subject personalized image generation.",
        "translated": "尽管现有个性化生成模型在视觉保真度方面表现出色，但它们在空间布局的交互控制方面仍存在不足，且难以有效扩展到多主体场景。为了解决这些限制，我们提出了 LayerComposer，一个用于个性化、多主体文本到图像生成的交互式框架。我们的方法主要有两个贡献：(1) 一种分层画布，这是一种新颖的表示方式，其中每个主体被放置在不同的图层上，从而实现了无遮挡的合成；以及 (2) 一种锁定机制，该机制在保持所选图层高保真度的同时，允许其余图层灵活适应周围上下文。类似于专业的图像编辑软件，我们提出的分层画布允许用户通过直观的图层操作来放置、调整或锁定输入主体。我们灵活的锁定机制无需对模型结构进行修改，而是依赖于固有的位置嵌入，并结合一种新的互补数据采样策略。大量实验表明，与当前最先进的多主体个性化图像生成方法相比，LayerComposer 在空间控制和身份保持方面具有更优的性能。",
        "translated_title": "LayerComposer: 通过空间感知分层画布实现交互式个性化文本到图像生成",
        "label": [],
        "label_reason": "论文聚焦图像生成而非像素级恢复，不属于低层图像处理",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "引入分层画布和锁定机制，但属于常规创新"
    },
    {
        "title": "Towards General Modality Translation with Contrastive and Predictive\n  Latent Diffusion Bridge",
        "url": "http://arxiv.org/abs/2510.20819v1",
        "pub_date": "2025-10-23",
        "summary": "Recent advances in generative modeling have positioned diffusion models as state-of-the-art tools for sampling from complex data distributions. While these models have shown remarkable success across single-modality domains such as images and audio, extending their capabilities to Modality Translation (MT), translating information across different sensory modalities, remains an open challenge. Existing approaches often rely on restrictive assumptions, including shared dimensionality, Gaussian source priors, and modality-specific architectures, which limit their generality and theoretical grounding. In this work, we propose the Latent Denoising Diffusion Bridge Model (LDDBM), a general-purpose framework for modality translation based on a latent-variable extension of Denoising Diffusion Bridge Models. By operating in a shared latent space, our method learns a bridge between arbitrary modalities without requiring aligned dimensions. We introduce a contrastive alignment loss to enforce semantic consistency between paired samples and design a domain-agnostic encoder-decoder architecture tailored for noise prediction in latent space. Additionally, we propose a predictive loss to guide training toward accurate cross-domain translation and explore several training strategies to improve stability. Our approach supports arbitrary modality pairs and performs strongly on diverse MT tasks, including multi-view to 3D shape generation, image super-resolution, and multi-view scene synthesis. Comprehensive experiments and ablations validate the effectiveness of our framework, establishing a new strong baseline in general modality translation. For more information, see our project page: https://sites.google.com/view/lddbm/home.",
        "translated": "最近，生成模型方面的进展将扩散模型确立为从复杂数据分布中采样的先进工具。尽管这些模型在单模态领域（如图像和音频）中取得了显著成功，但将其能力扩展到模态转换（Modality Translation, MT），即在不同感官模态之间进行信息转换，仍然是一个开放性的挑战。现有方法通常依赖于一些限制性假设，包括共享维度、高斯先验源分布和模态特定的架构，这些假设限制了其通用性和理论基础。在本文中，我们提出了一个名为 Latent Denoising Diffusion Bridge Model (LDDBM) 的通用框架，用于模态转换。该方法基于去噪扩散桥模型的潜在变量扩展。通过在共享的潜在空间中操作，我们的方法能够学习任意模态之间的桥梁，而无需对齐维度。我们引入了一个对比对齐损失，以强制成对样本之间的语义一致性，并设计了一个模态无关的编码器-解码器架构，专门用于潜在空间中的噪声预测。此外，我们提出了一种预测损失，以引导训练过程实现准确的跨域转换，并探讨了若干训练策略以提高稳定性。我们的方法支持任意模态对，并在多种MT任务中表现出色，包括多视图到3D形状生成、图像超分辨率和多视图场景合成。全面的实验和消融研究验证了我们框架的有效性，为通用模态转换建立了一个新的强基线。更多信息请访问我们的项目页面：https://sites.google.com/view/lddbm/home。",
        "translated_title": "面向具有对比与预测能力的通用模态转换的潜在扩散桥",
        "label": [
            "超分辨率",
            "图像恢复"
        ],
        "label_reason": "论文涉及图像超分辨率和多模态翻译，属于图像恢复任务",
        "relevance_score": 8,
        "novelty_score": 7,
        "novelty_reason": "提出基于扩散模型的通用多模态翻译框架，有一定改进"
    },
    {
        "title": "GSWorld: Closed-Loop Photo-Realistic Simulation Suite for Robotic\n  Manipulation",
        "url": "http://arxiv.org/abs/2510.20813v1",
        "pub_date": "2025-10-23",
        "summary": "This paper presents GSWorld, a robust, photo-realistic simulator for robotics manipulation that combines 3D Gaussian Splatting with physics engines. Our framework advocates \"closing the loop\" of developing manipulation policies with reproducible evaluation of policies learned from real-robot data and sim2real policy training without using real robots. To enable photo-realistic rendering of diverse scenes, we propose a new asset format, which we term GSDF (Gaussian Scene Description File), that infuses Gaussian-on-Mesh representation with robot URDF and other objects. With a streamlined reconstruction pipeline, we curate a database of GSDF that contains 3 robot embodiments for single-arm and bimanual manipulation, as well as more than 40 objects. Combining GSDF with physics engines, we demonstrate several immediate interesting applications: (1) learning zero-shot sim2real pixel-to-action manipulation policy with photo-realistic rendering, (2) automated high-quality DAgger data collection for adapting policies to deployment environments, (3) reproducible benchmarking of real-robot manipulation policies in simulation, (4) simulation data collection by virtual teleoperation, and (5) zero-shot sim2real visual reinforcement learning. Website: https://3dgsworld.github.io/.",
        "translated": "本文提出了 GSWorld，一个结合了 3D 高斯泼溅（Gaussian Splatting）和物理引擎的鲁棒、逼真的机器人操作模拟器。我们的框架提倡在不使用真实机器人的情况下，通过使用真实机器人数据学习的策略进行可复现的评估和 sim2real 策略训练，从而实现操作策略开发的“闭环”。为了实现多样化场景的逼真渲染，我们提出了一种新的资产格式，称为 GSDF（Gaussian Scene Description File），它将网格上的高斯表示与机器人 URDF 及其他对象相结合。通过一个简化重建流程，我们整理了一个 GSDF 数据库，其中包含 3 种用于单臂和双臂操作的机器人形态，以及 40 多个对象。将 GSDF 与物理引擎结合，我们展示了几个立竿见影的有趣应用：(1) 利用逼真渲染学习零样本 sim2real 像素到动作的操作策略；(2) 通过自动高质量 DAgger 数据收集，使策略适应部署环境；(3) 在模拟中对真实机器人操作策略进行可复现的基准测试；(4) 通过虚拟遥操作进行模拟数据收集；(5) 零样本 sim2real 视觉强化学习。网站：https://3dgsworld.github.io/。",
        "translated_title": "GSWorld：用于机器人操作的闭环照片级真实感仿真套件",
        "label": [],
        "label_reason": "主要关注机器人仿真与策略学习，非图像像素级处理任务",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出了新的资产格式GSDF和基于3D高斯溅射的仿真系统"
    },
    {
        "title": "SpectraMorph: Structured Latent Learning for Self-Supervised\n  Hyperspectral Super-Resolution",
        "url": "http://arxiv.org/abs/2510.20814v1",
        "pub_date": "2025-10-23",
        "summary": "Hyperspectral sensors capture dense spectra per pixel but suffer from low spatial resolution, causing blurred boundaries and mixed-pixel effects. Co-registered companion sensors such as multispectral, RGB, or panchromatic cameras provide high-resolution spatial detail, motivating hyperspectral super-resolution through the fusion of hyperspectral and multispectral images (HSI-MSI). Existing deep learning based methods achieve strong performance but rely on opaque regressors that lack interpretability and often fail when the MSI has very few bands. We propose SpectraMorph, a physics-guided self-supervised fusion framework with a structured latent space. Instead of direct regression, SpectraMorph enforces an unmixing bottleneck: endmember signatures are extracted from the low-resolution HSI, and a compact multilayer perceptron predicts abundance-like maps from the MSI. Spectra are reconstructed by linear mixing, with training performed in a self-supervised manner via the MSI sensor's spectral response function. SpectraMorph produces interpretable intermediates, trains in under a minute, and remains robust even with a single-band (pan-chromatic) MSI. Experiments on synthetic and real-world datasets show SpectraMorph consistently outperforming state-of-the-art unsupervised/self-supervised baselines while remaining very competitive against supervised baselines.",
        "translated": "高光谱传感器在每个像素上捕获密集的光谱信息，但存在空间分辨率低的问题，导致边界模糊和混合像素效应。共注册的辅助传感器，如多光谱、RGB 或全色相机，提供了高分辨率的空间细节，这促使通过高光谱和多光谱图像（HSI-MSI）的融合来实现高光谱超分辨率。现有的基于深度学习的方法在性能上表现出色，但依赖于缺乏可解释性的黑箱回归模型，且在多光谱图像（MSI）波段非常少时常常失效。我们提出 SpectraMorph，一种基于物理引导的自监督融合框架，具有结构化的潜在空间。SpectraMorph 不采用直接回归，而是引入了一种解混瓶颈：端元特征从低分辨率的 HSI 中提取，并通过一个紧凑的多层感知器从 MSI 中预测出类似丰度的图。光谱通过线性混合进行重建，训练过程通过 MSI 传感器的光谱响应函数以自监督方式进行。SpectraMorph 生成可解释的中间结果，训练时间不到一分钟，并且即使在单波段（全色）MSI 的情况下仍具有鲁棒性。在合成和真实世界数据集上的实验表明，SpectraMorph 持续优于最先进的无监督/自监督基线方法，并在与有监督基线方法的比较中保持非常竞争力。",
        "translated_title": "SpectraMorph：用于自监督高光谱超分辨率的结构化潜在学习",
        "label": [
            "超分辨率",
            "遥感图像复原"
        ],
        "label_reason": "论文聚焦于高光谱图像的超分辨率，属于 low-level 图像处理任务。",
        "relevance_score": 9,
        "novelty_score": 8,
        "novelty_reason": "提出物理引导的自监督融合框架，改进了现有方法的可解释性和鲁棒性。"
    },
    {
        "title": "Small Drafts, Big Verdict: Information-Intensive Visual Reasoning via\n  Speculation",
        "url": "http://arxiv.org/abs/2510.20812v1",
        "pub_date": "2025-10-23",
        "summary": "Large Vision-Language Models (VLMs) have achieved remarkable progress in multimodal understanding, yet they struggle when reasoning over information-intensive images that densely interleave textual annotations with fine-grained graphical elements. The main challenges lie in precisely localizing critical cues in dense layouts and multi-hop reasoning to integrate dispersed evidence. We propose Speculative Verdict (SV), a training-free framework inspired by speculative decoding that combines multiple lightweight draft experts with a large verdict model. In the draft stage, small VLMs act as draft experts to generate reasoning paths that provide diverse localization candidates; in the verdict stage, a strong VLM synthesizes these paths to produce the final answer, minimizing computational cost while recovering correct answers. To further improve efficiency and accuracy, SV introduces a consensus expert selection mechanism that forwards only high-agreement reasoning paths to the verdict. Empirically, SV achieves consistent gains on challenging information-intensive and high-resolution visual question answering benchmarks, including InfographicVQA, ChartMuseum, ChartQAPro, and HR-Bench 4K. By synthesizing correct insights from multiple partially accurate reasoning paths, SV achieves both error correction and cost-efficiency compared to large proprietary models or training pipelines. Code is available at https://github.com/Tinaliu0123/speculative-verdict",
        "translated": "视觉-语言大模型（VLMs）在多模态理解方面取得了显著进展，但它们在处理信息密集型图像时仍存在困难，特别是在文本注释与细粒度图形元素紧密交织的图像中进行推理。主要挑战在于在密集布局中精确定位关键线索，以及通过多跳推理整合分散的证据。我们提出了一种无训练框架 Speculative Verdict（SV），该框架受推测性解码的启发，结合多个轻量级草稿专家与一个大判决模型。在草稿阶段，小型 VLMs 作为草稿专家生成推理路径，提供多样化的定位候选；在判决阶段，一个强大的 VLM 综合这些路径生成最终答案，在减少计算成本的同时恢复正确答案。为进一步提高效率和准确性，SV 引入了一个共识专家选择机制，仅将高一致性的推理路径转发给判决模型。实验表明，SV 在具有挑战性的信息密集型和高分辨率视觉问答基准（包括 InfographicVQA、ChartMuseum、ChartQAPro 和 HR-Bench 4K）上取得了持续的性能提升。通过从多个部分准确的推理路径中综合正确的见解，SV 在错误校正和计算成本方面均优于专有大模型或训练流程。代码可在 https://github.com/Tinaliu0123/speculative-verdict 获得。",
        "translated_title": "小草图，大判断：通过推测实现信息密集型视觉推理",
        "label": [],
        "label_reason": "论文聚焦多模态推理，非图像像素级恢复或增强",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出轻量专家与主模型结合框架，有一定新意但非根本性创新"
    },
    {
        "title": "Real Deep Research for AI, Robotics and Beyond",
        "url": "http://arxiv.org/abs/2510.20809v1",
        "pub_date": "2025-10-23",
        "summary": "With the rapid growth of research in AI and robotics now producing over 10,000 papers annually it has become increasingly difficult for researchers to stay up to date. Fast evolving trends, the rise of interdisciplinary work, and the need to explore domains beyond one's expertise all contribute to this challenge. To address these issues, we propose a generalizable pipeline capable of systematically analyzing any research area: identifying emerging trends, uncovering cross domain opportunities, and offering concrete starting points for new inquiry. In this work, we present Real Deep Research (RDR) a comprehensive framework applied to the domains of AI and robotics, with a particular focus on foundation models and robotics advancements. We also briefly extend our analysis to other areas of science. The main paper details the construction of the RDR pipeline, while the appendix provides extensive results across each analyzed topic. We hope this work sheds light for researchers working in the field of AI and beyond.",
        "translated": "随着人工智能和机器人领域研究的迅猛发展，目前每年产出的论文已超过10,000篇，研究人员要保持对该领域进展的及时了解变得日益困难。快速变化的研究趋势、跨学科工作的兴起以及探索专业领域之外新方向的必要性，都加剧了这一挑战。为应对这些问题，我们提出了一种通用的分析流程，能够系统地评估任何研究领域：识别新兴趋势、发现跨领域的研究机会，并为新的研究方向提供具体切入点。在本项工作中，我们介绍了Real Deep Research（RDR），这是一个全面的框架，应用于人工智能和机器人领域，特别关注基础模型和机器人技术的发展。我们还简要地将分析扩展到其他科学领域。正文部分详细描述了RDR流程的构建，而附录则提供了对每个研究主题的详尽结果。我们希望本项工作能够为从事人工智能及相关领域研究的学者提供一定的启发和帮助。",
        "translated_title": "Real Deep Research for AI, Robotics and Beyond",
        "label": [],
        "label_reason": "论文聚焦AI与机器人研究综述，不涉及图像像素级处理",
        "relevance_score": 1,
        "novelty_score": 3,
        "novelty_reason": "提出通用研究分析框架，但方法复现性较强，创新性有限"
    },
    {
        "title": "Video Prediction of Dynamic Physical Simulations With Pixel-Space\n  Spatiotemporal Transformers",
        "url": "http://arxiv.org/abs/2510.20807v1",
        "pub_date": "2025-10-23",
        "summary": "Inspired by the performance and scalability of autoregressive large language models (LLMs), transformer-based models have seen recent success in the visual domain. This study investigates a transformer adaptation for video prediction with a simple end-to-end approach, comparing various spatiotemporal self-attention layouts. Focusing on causal modeling of physical simulations over time; a common shortcoming of existing video-generative approaches, we attempt to isolate spatiotemporal reasoning via physical object tracking metrics and unsupervised training on physical simulation datasets. We introduce a simple yet effective pure transformer model for autoregressive video prediction, utilizing continuous pixel-space representations for video prediction. Without the need for complex training strategies or latent feature-learning components, our approach significantly extends the time horizon for physically accurate predictions by up to 50% when compared with existing latent-space approaches, while maintaining comparable performance on common video quality metrics. In addition, we conduct interpretability experiments to identify network regions that encode information useful to perform accurate estimations of PDE simulation parameters via probing models, and find that this generalizes to the estimation of out-of-distribution simulation parameters. This work serves as a platform for further attention-based spatiotemporal modeling of videos via a simple, parameter efficient, and interpretable approach.",
        "translated": "受自回归大语言模型（LLMs）性能和可扩展性的启发，基于Transformer的模型最近在视觉领域取得了显著成功。本研究探讨了一种用于视频预测的Transformer模型适配方法，采用简洁的端到端策略，并比较了多种时空自注意力结构。我们关注时间维度上的因果建模，尤其是在物理模拟中，这是现有视频生成方法的常见短板。为此，我们尝试通过物理对象跟踪指标和在物理模拟数据集上的无监督训练，分离出时空推理能力。我们引入了一种简单而有效的纯Transformer模型，用于自回归视频预测，并利用连续像素空间表示来进行视频预测。无需复杂训练策略或隐空间特征学习组件，与现有隐空间方法相比，我们的方法在实现物理准确预测的时间范围内可提高多达50%，同时在常见的视频质量指标上保持相当的性能。此外，我们还进行了可解释性实验，通过探测模型识别出那些能够编码有效信息、用于准确估计PDE模拟参数的网络区域，并发现该方法可以推广至对分布外（out-of-distribution）模拟参数的估计。这项工作为视频的进一步基于注意力机制的时空建模提供了一个平台，采用了一种简单、参数高效且可解释的方法。",
        "translated_title": "动态物理模拟的视频预测：基于像素空间的时空变换器",
        "label": [
            "多帧/视频图像恢复"
        ],
        "label_reason": "论文涉及视频预测，属于多帧图像恢复任务",
        "relevance_score": 6,
        "novelty_score": 7,
        "novelty_reason": "提出了一种简单有效的纯Transformer模型，改进了物理模拟视频预测"
    },
    {
        "title": "ARGenSeg: Image Segmentation with Autoregressive Image Generation Model",
        "url": "http://arxiv.org/abs/2510.20803v1",
        "pub_date": "2025-10-23",
        "summary": "We propose a novel AutoRegressive Generation-based paradigm for image Segmentation (ARGenSeg), achieving multimodal understanding and pixel-level perception within a unified framework. Prior works integrating image segmentation into multimodal large language models (MLLMs) typically employ either boundary points representation or dedicated segmentation heads. These methods rely on discrete representations or semantic prompts fed into task-specific decoders, which limits the ability of the MLLM to capture fine-grained visual details. To address these challenges, we introduce a segmentation framework for MLLM based on image generation, which naturally produces dense masks for target objects. We leverage MLLM to output visual tokens and detokenize them into images using an universal VQ-VAE, making the segmentation fully dependent on the pixel-level understanding of the MLLM. To reduce inference latency, we employ a next-scale-prediction strategy to generate required visual tokens in parallel. Extensive experiments demonstrate that our method surpasses prior state-of-the-art approaches on multiple segmentation datasets with a remarkable boost in inference speed, while maintaining strong understanding capabilities.",
        "translated": "我们提出了一种基于自回归生成的图像分割新范式（ARGenSeg），在统一框架下实现多模态理解和像素级感知。此前将图像分割整合进多模态大语言模型（MLLMs）的研究通常采用边界点表示或专用分割头部。这些方法依赖于离散表示或语义提示输入到任务特定的解码器中，这限制了 MLLM 捕捉细粒度视觉细节的能力。为了解决这些问题，我们基于图像生成引入了一种适用于 MLLM 的分割框架，该框架能够自然生成目标对象的密集掩膜。我们利用 MLLM 输出视觉 token，并使用一个通用的 VQ-VAE 将其解码为图像，从而使分割完全依赖于 MLLM 的像素级理解能力。为了降低推理延迟，我们采用下一尺度预测策略，以并行生成所需的视觉 token。广泛的实验表明，我们的方法在多个分割数据集上超越了以往最先进的方法，在推理速度上有显著提升，同时保持了强大的理解能力。",
        "translated_title": "ARGenSeg：基于自回归图像生成模型的图像分割",
        "label": [],
        "label_reason": "论文聚焦于图像分割，属于high-level任务。",
        "relevance_score": 3,
        "novelty_score": 7,
        "novelty_reason": "提出基于自回归图像生成的新分割范式，具有显著创新。"
    },
    {
        "title": "Compress to Impress: Efficient LLM Adaptation Using a Single Gradient\n  Step on 100 Samples",
        "url": "http://arxiv.org/abs/2510.20800v1",
        "pub_date": "2025-10-23",
        "summary": "Recently, Sharma et al. suggested a method called Layer-SElective-Rank reduction (LASER) which demonstrated that pruning high-order components of carefully chosen LLM's weight matrices can boost downstream accuracy -- without any gradient-based fine-tuning. Yet LASER's exhaustive, per-matrix search (each requiring full-dataset forward passes) makes it impractical for rapid deployment. We demonstrate that this overhead can be removed and find that: (i) Only a small, carefully chosen subset of matrices needs to be inspected -- eliminating the layer-by-layer sweep, (ii) The gradient of each matrix's singular values pinpoints which matrices merit reduction, (iii) Increasing the factorization search space by allowing matrices rows to cluster around multiple subspaces and then decomposing each cluster separately further reduces overfitting on the original training data and further lifts accuracy by up to 24.6 percentage points, and finally, (iv) we discover that evaluating on just 100 samples rather than the full training data -- both for computing the indicative gradients and for measuring the final accuracy -- suffices to further reduce the search time; we explain that as adaptation to downstream tasks is dominated by prompting style, not dataset size. As a result, we show that combining these findings yields a fast and robust adaptation algorithm for downstream tasks. Overall, with a single gradient step on 100 examples and a quick scan of the top candidate layers and factorization techniques, we can adapt LLMs to new datasets -- entirely without fine-tuning.",
        "translated": "最近，Sharma 等人提出了一种称为 Layer-SElective-Rank reduction（LASER）的方法，该方法表明，通过剪枝精心选择的大型语言模型（LLM）权重矩阵的高阶成分，可以在不进行任何基于梯度的微调的情况下提升下游任务的准确率。然而，LASER 的穷举式、每个矩阵的搜索过程（每次都需要在整个数据集上进行前向传播）使其难以在快速部署中使用。我们证明可以去除这种开销，并发现以下几点：(i) 只需检查一小部分精心选择的矩阵即可，从而消除了逐层扫描的需要；(ii) 每个矩阵奇异值的梯度能够准确指出哪些矩阵值得进行降秩处理；(iii) 通过允许矩阵的行围绕多个子空间聚类，并对每个聚类分别进行分解，从而扩大了分解的搜索空间，进一步减少了对原始训练数据的过拟合并将准确率提升高达 24.6 个百分点；最后，(iv) 我们发现仅使用 100 个样本而不是完整训练数据进行评估（包括计算指示性梯度和最终准确率）就足以进一步减少搜索时间；我们解释说，这是由于对下游任务的适应主要由提示风格主导，而非数据集的大小。因此，我们展示了结合这些发现可以得到一种快速且鲁棒的下游任务适应算法。总体而言，只需在 100 个示例上进行一次梯度更新，并快速扫描候选的顶层和分解技术，我们即可将 LLM 适配到新数据集上——且完全无需微调。",
        "translated_title": "Compress to Impress: Efficient LLM Adaptation Using a Single Gradient  \nStep on 100 Samples  \n令人印象深刻：基于100个样本的一个梯度步骤的高效大语言模型适应  \n\nRecent advancements in large language models (LLMs) have demonstrated remarkable performance on a variety of tasks, including few-shot learning. However, adapting LLMs to new tasks typically requires extensive computation and large-scale datasets, which hinders their practical deployment in resource-constrained environments. In this work, we present a novel and efficient adaptation method that enables impressive performance using just a single gradient step on a small set of 100 samples. Our approach leverages a carefully designed prompt tuning strategy combined with model compression techniques to minimize the computational burden while maintaining high accuracy. We evaluate our method on several benchmark tasks and show that it achieves competitive results compared to more resource-intensive fine-tuning approaches. This work provides a promising direction for making LLMs more accessible and deployable in real-world applications with limited computational resources.  \n近期，大型语言模型（LLMs）在多种任务上展示了卓越的性能，包括小样本学习。然而，通常将 LLM 适应到新任务需要大量计算和大规模数据集，这限制了它们在资源受限环境中的实际部署。在本研究中，我们提出了一种新颖且高效的方法，该方法仅需在一个包含100个样本的小集合上进行一次梯度步骤，即可实现令人印象深刻的性能。我们的方法结合了精心设计的提示调优策略与模型压缩技术，在保持高准确率的同时最小化计算负担。我们在多个基准任务上评估了该方法，并展示了其与资源密集型微调方法相比具有竞争力的结果。这项工作为在计算资源有限的现实应用中使 LLM 更加易用和可部署提供了有前景的方向。",
        "label": [],
        "label_reason": "论文聚焦于大模型压缩与适应，不涉及图像像素级处理",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "提出了一些实用改进，但整体属于常规模型优化方法"
    },
    {
        "title": "Radar-Camera Fused Multi-Object Tracking: Online Calibration and Common\n  Feature",
        "url": "http://arxiv.org/abs/2510.20794v1",
        "pub_date": "2025-10-23",
        "summary": "This paper presents a Multi-Object Tracking (MOT) framework that fuses radar and camera data to enhance tracking efficiency while minimizing manual interventions. Contrary to many studies that underutilize radar and assign it a supplementary role--despite its capability to provide accurate range/depth information of targets in a world 3D coordinate system--our approach positions radar in a crucial role. Meanwhile, this paper utilizes common features to enable online calibration to autonomously associate detections from radar and camera. The main contributions of this work include: (1) the development of a radar-camera fusion MOT framework that exploits online radar-camera calibration to simplify the integration of detection results from these two sensors, (2) the utilization of common features between radar and camera data to accurately derive real-world positions of detected objects, and (3) the adoption of feature matching and category-consistency checking to surpass the limitations of mere position matching in enhancing sensor association accuracy. To the best of our knowledge, we are the first to investigate the integration of radar-camera common features and their use in online calibration for achieving MOT. The efficacy of our framework is demonstrated by its ability to streamline the radar-camera mapping process and improve tracking precision, as evidenced by real-world experiments conducted in both controlled environments and actual traffic scenarios. Code is available at https://github.com/radar-lab/Radar_Camera_MOT",
        "translated": "本文提出了一种融合雷达与相机数据的多目标跟踪（MOT）框架，旨在提升跟踪效率并尽量减少人工干预。与许多研究未能充分利用雷达能力，仅将其作为辅助传感器使用（尽管其能够在世界三维坐标系中提供目标的精确距离/深度信息）不同，本文方法将雷达置于关键角色。同时，本文利用雷达与相机数据之间的共有特征，实现在线标定以自主关联来自这两个传感器的检测结果。本文的主要贡献包括：(1) 开发了一种雷达-相机融合的MOT框架，该框架利用在线雷达-相机标定以简化来自这两种传感器的检测结果的集成过程；(2) 利用雷达与相机数据之间的共有特征，以准确推导检测对象在真实世界中的位置；(3) 采用特征匹配与类别一致性检查方法，以克服单纯基于位置匹配的局限性，从而提高传感器关联的准确性。据我们所知，这是首次对雷达与相机共有特征的融合及其在在线标定中用于实现MOT的研究进行探索。通过在受控环境和实际交通场景中进行的实验证明，本文框架能够有效简化雷达-相机映射流程并提升跟踪精度。代码可在 https://github.com/radar-lab/Radar_Camera_MOT 获得。",
        "translated_title": "雷达-相机融合的多目标跟踪：在线标定与公共特征",
        "label": [],
        "label_reason": "论文主要涉及传感器融合与多目标跟踪，不属于图像像素级恢复或增强任务。",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "首次尝试雷达-相机共性特征用于在线校准，有一定新颖性但属于系统级方法创新。"
    },
    {
        "title": "CUPID: Pose-Grounded Generative 3D Reconstruction from a Single Image",
        "url": "http://arxiv.org/abs/2510.20776v1",
        "pub_date": "2025-10-23",
        "summary": "This work proposes a new generation-based 3D reconstruction method, named Cupid, that accurately infers the camera pose, 3D shape, and texture of an object from a single 2D image. Cupid casts 3D reconstruction as a conditional sampling process from a learned distribution of 3D objects, and it jointly generates voxels and pixel-voxel correspondences, enabling robust pose and shape estimation under a unified generative framework. By representing both input camera poses and 3D shape as a distribution in a shared 3D latent space, Cupid adopts a two-stage flow matching pipeline: (1) a coarse stage that produces initial 3D geometry with associated 2D projections for pose recovery; and (2) a refinement stage that integrates pose-aligned image features to enhance structural fidelity and appearance details. Extensive experiments demonstrate Cupid outperforms leading 3D reconstruction methods with an over 3 dB PSNR gain and an over 10% Chamfer Distance reduction, while matching monocular estimators on pose accuracy and delivering superior visual fidelity over baseline 3D generative models. For an immersive view of the 3D results generated by Cupid, please visit cupid3d.github.io.",
        "translated": "本文提出了一种新的基于生成的三维重建方法，名为 Cupid，该方法能够从单张二维图像中准确推断出物体的相机姿态、三维形状和纹理。Cupid 将三维重建建模为从学习到的三维物体分布中进行条件采样的过程，并同时生成体素和像素-体素对应关系，从而在统一的生成框架下实现稳健的姿态和形状估计。通过将输入的相机姿态和三维形状表示为共享三维潜在空间中的分布，Cupid 采用了一个两阶段的流匹配流程：(1) 粗略阶段生成初始的三维几何结构及其对应的二维投影以进行姿态恢复；以及 (2) 细化阶段，将姿态对齐的图像特征整合进来，以增强结构保真度和外观细节。广泛的实验表明，Cupid 相较于领先的三维重建方法在 PSNR 指标上提升了超过 3 dB，并在 Chamfer Distance 上减少了超过 10%，同时在姿态准确性上与单目估计器相当，并在视觉保真度方面优于基线三维生成模型。如需沉浸式查看 Cupid 生成的三维重建结果，请访问 cupid3d.github.io。",
        "translated_title": "CUPID：基于姿态的单图像生成式三维重建",
        "label": [],
        "label_reason": "论文聚焦于3D重建，不直接处理像素级图像质量",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出了基于生成模型的3D重建方法，具有一定的创新性"
    },
    {
        "title": "AlphaFlow: Understanding and Improving MeanFlow Models",
        "url": "http://arxiv.org/abs/2510.20771v1",
        "pub_date": "2025-10-23",
        "summary": "MeanFlow has recently emerged as a powerful framework for few-step generative modeling trained from scratch, but its success is not yet fully understood. In this work, we show that the MeanFlow objective naturally decomposes into two parts: trajectory flow matching and trajectory consistency. Through gradient analysis, we find that these terms are strongly negatively correlated, causing optimization conflict and slow convergence. Motivated by these insights, we introduce $\\alpha$-Flow, a broad family of objectives that unifies trajectory flow matching, Shortcut Model, and MeanFlow under one formulation. By adopting a curriculum strategy that smoothly anneals from trajectory flow matching to MeanFlow, $\\alpha$-Flow disentangles the conflicting objectives, and achieves better convergence. When trained from scratch on class-conditional ImageNet-1K 256x256 with vanilla DiT backbones, $\\alpha$-Flow consistently outperforms MeanFlow across scales and settings. Our largest $\\alpha$-Flow-XL/2+ model achieves new state-of-the-art results using vanilla DiT backbones, with FID scores of 2.58 (1-NFE) and 2.15 (2-NFE).",
        "translated": "MeanFlow 最近作为一种从头训练的少步生成建模强大框架而出现，但其成功尚未被完全理解。在本工作中，我们表明 MeanFlow 的目标自然地分解为两部分：轨迹流匹配（trajectory flow matching）和轨迹一致性（trajectory consistency）。通过梯度分析，我们发现这些项之间存在强烈的负相关性，导致优化冲突和收敛缓慢。受这些洞察的启发，我们引入了 $\\alpha$-Flow，这是一类广泛的目标函数，将轨迹流匹配、Shortcut Model 和 MeanFlow 统一在一种公式下。通过采用从轨迹流匹配平滑退火到 MeanFlow 的课程学习策略，$\\alpha$-Flow 解耦了这些冲突的目标，并实现了更好的收敛性。在使用普通 DiT 架构从头训练的类别条件 ImageNet-1K 256x256 数据集上，$\\alpha$-Flow 在各种尺度和设置下均一致优于 MeanFlow。我们最大的 $\\alpha$-Flow-XL/2+ 模型在使用普通 DiT 架构的情况下达到了新的最先进结果，其 FID 分数分别为 2.58（1-NFE）和 2.15（2-NFE）。",
        "translated_title": "AlphaFlow: 理解与改进 MeanFlow 模型",
        "label": [],
        "label_reason": "论文聚焦生成模型优化，不属于像素级图像处理任务",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出了α-Flow新目标框架并改进收敛性"
    },
    {
        "title": "DyPE: Dynamic Position Extrapolation for Ultra High Resolution Diffusion",
        "url": "http://arxiv.org/abs/2510.20766v1",
        "pub_date": "2025-10-23",
        "summary": "Diffusion Transformer models can generate images with remarkable fidelity and detail, yet training them at ultra-high resolutions remains extremely costly due to the self-attention mechanism's quadratic scaling with the number of image tokens. In this paper, we introduce Dynamic Position Extrapolation (DyPE), a novel, training-free method that enables pre-trained diffusion transformers to synthesize images at resolutions far beyond their training data, with no additional sampling cost. DyPE takes advantage of the spectral progression inherent to the diffusion process, where low-frequency structures converge early, while high-frequencies take more steps to resolve. Specifically, DyPE dynamically adjusts the model's positional encoding at each diffusion step, matching their frequency spectrum with the current stage of the generative process. This approach allows us to generate images at resolutions that exceed the training resolution dramatically, e.g., 16 million pixels using FLUX. On multiple benchmarks, DyPE consistently improves performance and achieves state-of-the-art fidelity in ultra-high-resolution image generation, with gains becoming even more pronounced at higher resolutions. Project page is available at https://noamissachar.github.io/DyPE/.",
        "translated": "扩散 Transformer 模型可以生成具有显著保真度和细节的图像，但由于自注意力机制的复杂度与图像 token 数量呈二次方关系，因此在超高清分辨率下训练这些模型仍然非常昂贵。在本文中，我们提出了一种新的、无需训练的方法——动态位置外推（Dynamic Position Extrapolation，DyPE），使得预训练的扩散 Transformer 能够在不增加额外采样成本的前提下，合成远超其训练数据分辨率的图像。DyPE 利用了扩散过程中固有的频谱渐进特性：低频结构在早期扩散阶段快速收敛，而高频信息则需要更多步骤才能还原。具体而言，DyPE 在每个扩散步骤中动态调整模型的位置编码，使其与当前生成过程的频率谱相匹配。该方法使得我们可以生成远高于训练分辨率的图像，例如使用 FLUX 模型生成 1600 万像素的图像。在多个基准测试中，DyPE 一致提升了性能，并在超高清图像生成中实现了最先进的保真度，且分辨率越高，性能提升越明显。项目页面请访问 https://noamissachar.github.io/DyPE/。",
        "translated_title": "DyPE：用于超分辨率扩散的动态位置外推",
        "label": [],
        "label_reason": "论文聚焦图像生成而非恢复或增强",
        "relevance_score": 3,
        "novelty_score": 8,
        "novelty_reason": "提出训练免费的动态位置外推方法，显著提升生成分辨率"
    },
    {
        "title": "MEIcoder: Decoding Visual Stimuli from Neural Activity by Leveraging\n  Most Exciting Inputs",
        "url": "http://arxiv.org/abs/2510.20762v1",
        "pub_date": "2025-10-23",
        "summary": "Decoding visual stimuli from neural population activity is crucial for understanding the brain and for applications in brain-machine interfaces. However, such biological data is often scarce, particularly in primates or humans, where high-throughput recording techniques, such as two-photon imaging, remain challenging or impossible to apply. This, in turn, poses a challenge for deep learning decoding techniques. To overcome this, we introduce MEIcoder, a biologically informed decoding method that leverages neuron-specific most exciting inputs (MEIs), a structural similarity index measure loss, and adversarial training. MEIcoder achieves state-of-the-art performance in reconstructing visual stimuli from single-cell activity in primary visual cortex (V1), especially excelling on small datasets with fewer recorded neurons. Using ablation studies, we demonstrate that MEIs are the main drivers of the performance, and in scaling experiments, we show that MEIcoder can reconstruct high-fidelity natural-looking images from as few as 1,000-2,500 neurons and less than 1,000 training data points. We also propose a unified benchmark with over 160,000 samples to foster future research. Our results demonstrate the feasibility of reliable decoding in early visual system and provide practical insights for neuroscience and neuroengineering applications.",
        "translated": "从神经群体活动中解码视觉刺激对于理解大脑以及脑机接口的应用至关重要。然而，这类生物数据往往十分稀缺，特别是在灵长类动物或人类中，由于双光子成像等高通量记录技术难以实施或无法应用，使得数据获取极具挑战性。这反过来也对深度学习解码方法提出了挑战。为了解决这一问题，我们引入了 MEIcoder，一种受生物学启发的解码方法，该方法利用神经元特定的最兴奋输入（MEIs）、结构相似性指数度量损失函数以及对抗训练。MEIcoder 在从初级视觉皮层（V1）单细胞活动中重建视觉刺激方面达到了最先进的性能，尤其在记录神经元数量较少的小型数据集上表现突出。通过消融实验，我们证明 MEIs 是性能提升的主要驱动因素；在扩展实验中，我们展示了 MEIcoder 可以从仅 1,000 至 2,500 个神经元和少于 1,000 个训练样本中重建出高保真、自然的图像。我们还提出一个包含超过 160,000 个样本的统一基准，以促进未来的研究。我们的结果证明了在早期视觉系统中进行可靠解码的可行性，并为神经科学和神经工程应用提供了实用的见解。",
        "translated_title": "MEIcoder：通过利用最激动输入从神经活动中解码视觉刺激",
        "label": [],
        "label_reason": "不属于low-level图像处理，目标是解码神经活动而非恢复图像质量",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "结合MEIs和对抗训练进行神经解码，有一定技术迁移创新"
    },
    {
        "title": "ACS-SegNet: An Attention-Based CNN-SegFormer Segmentation Network for\n  Tissue Segmentation in Histopathology",
        "url": "http://arxiv.org/abs/2510.20754v1",
        "pub_date": "2025-10-23",
        "summary": "Automated histopathological image analysis plays a vital role in computer-aided diagnosis of various diseases. Among developed algorithms, deep learning-based approaches have demonstrated excellent performance in multiple tasks, including semantic tissue segmentation in histological images. In this study, we propose a novel approach based on attention-driven feature fusion of convolutional neural networks (CNNs) and vision transformers (ViTs) within a unified dual-encoder model to improve semantic segmentation performance. Evaluation on two publicly available datasets showed that our model achieved {\\mu}IoU/{\\mu}Dice scores of 76.79%/86.87% on the GCPS dataset and 64.93%/76.60% on the PUMA dataset, outperforming state-of-the-art and baseline benchmarks. The implementation of our method is publicly available in a GitHub repository: https://github.com/NimaTorbati/ACS-SegNet",
        "translated": "自动化组织病理学图像分析在各种疾病计算机辅助诊断中发挥着关键作用。在已开发的算法中，基于深度学习的方法已在多个任务中表现出优异的性能，包括组织学图像中的语义组织分割。在本研究中，我们提出了一种新颖的方法，通过在统一的双编码器模型中融合卷积神经网络（CNNs）和视觉变换器（ViTs）的注意力驱动特征，以提升语义分割性能。在两个公开可用的数据集上的评估表明，我们的模型在 GCPS 数据集上实现了 76.79% 的 {\\mu}IoU 和 86.87% 的 {\\mu}Dice 分数，在 PUMA 数据集上实现了 64.93% 的 {\\mu}IoU 和 76.60% 的 {\\mu}Dice 分数，优于当前最先进的方法和基线基准。我们的方法实现已公开发布在 GitHub 仓库中：https://github.com/NimaTorbati/ACS-SegNet",
        "translated_title": "ACS-SegNet：一种基于注意力机制的CNN-SegFormer分割网络，用于组织病理学中的组织分割",
        "label": [],
        "label_reason": "论文主要解决语义分割问题，属于high-level任务",
        "relevance_score": 2,
        "novelty_score": 5,
        "novelty_reason": "方法为CNN与SegFormer的常规组合，创新性有限"
    },
    {
        "title": "AutoScape: Geometry-Consistent Long-Horizon Scene Generation",
        "url": "http://arxiv.org/abs/2510.20726v1",
        "pub_date": "2025-10-23",
        "summary": "This paper proposes AutoScape, a long-horizon driving scene generation framework. At its core is a novel RGB-D diffusion model that iteratively generates sparse, geometrically consistent keyframes, serving as reliable anchors for the scene's appearance and geometry. To maintain long-range geometric consistency, the model 1) jointly handles image and depth in a shared latent space, 2) explicitly conditions on the existing scene geometry (i.e., rendered point clouds) from previously generated keyframes, and 3) steers the sampling process with a warp-consistent guidance. Given high-quality RGB-D keyframes, a video diffusion model then interpolates between them to produce dense and coherent video frames. AutoScape generates realistic and geometrically consistent driving videos of over 20 seconds, improving the long-horizon FID and FVD scores over the prior state-of-the-art by 48.6\\% and 43.0\\%, respectively.",
        "translated": "本文提出 AutoScape，一种长视野驾驶场景生成框架。其核心是一个新颖的 RGB-D 扩散模型，该模型迭代生成稀疏但几何一致性良好的关键帧，作为场景外观和几何结构的可靠锚点。为了保持长距离的几何一致性，该模型 1）在共享的潜在空间中联合处理图像和深度信息，2）显式地基于之前生成的关键帧中的场景几何结构（即渲染的点云）进行条件建模，3）通过具有变形一致性的引导控制采样过程。在获得高质量的 RGB-D 关键帧后，一个视频扩散模型进一步在它们之间进行插值，以生成密集且连贯的视频帧。AutoScape 能够生成超过 20 秒、真实且几何一致的驾驶视频，在长视野 FID 和 FVD 评分上分别比现有最先进方法提升了 48.6% 和 43.0%。",
        "translated_title": "AutoScape：几何一致的长视野场景生成",
        "label": [],
        "label_reason": "论文关注场景生成而非像素级图像质量恢复",
        "relevance_score": 3,
        "novelty_score": 6,
        "novelty_reason": "提出了结合几何一致性的RGB-D扩散模型框架"
    },
    {
        "title": "ALICE-LRI: A General Method for Lossless Range Image Generation for\n  Spinning LiDAR Sensors without Calibration Metadata",
        "url": "http://arxiv.org/abs/2510.20708v1",
        "pub_date": "2025-10-23",
        "summary": "3D LiDAR sensors are essential for autonomous navigation, environmental monitoring, and precision mapping in remote sensing applications. To efficiently process the massive point clouds generated by these sensors, LiDAR data is often projected into 2D range images that organize points by their angular positions and distances. While these range image representations enable efficient processing, conventional projection methods suffer from fundamental geometric inconsistencies that cause irreversible information loss, compromising high-fidelity applications. We present ALICE-LRI (Automatic LiDAR Intrinsic Calibration Estimation for Lossless Range Images), the first general, sensor-agnostic method that achieves lossless range image generation from spinning LiDAR point clouds without requiring manufacturer metadata or calibration files. Our algorithm automatically reverse-engineers the intrinsic geometry of any spinning LiDAR sensor by inferring critical parameters including laser beam configuration, angular distributions, and per-beam calibration corrections, enabling lossless projection and complete point cloud reconstruction with zero point loss. Comprehensive evaluation across the complete KITTI and DurLAR datasets demonstrates that ALICE-LRI achieves perfect point preservation, with zero points lost across all point clouds. Geometric accuracy is maintained well within sensor precision limits, establishing geometric losslessness with real-time performance. We also present a compression case study that validates substantial downstream benefits, demonstrating significant quality improvements in practical applications. This paradigm shift from approximate to lossless LiDAR projections opens new possibilities for high-precision remote sensing applications requiring complete geometric preservation.",
        "translated": "3D激光雷达传感器在遥感应用中的自动驾驶导航、环境监测和精确制图中具有重要作用。为了高效处理这些传感器生成的海量点云数据，通常会将激光雷达数据投影到2D的范围图像（range images）中，这些图像根据点的角度位置和距离来组织点。虽然这些范围图像表示方式能够实现高效处理，但传统投影方法存在基本的几何不一致性，导致不可逆的信息丢失，从而影响高保真度应用的性能。我们提出ALICE-LRI（Automatic LiDAR Intrinsic Calibration Estimation for Lossless Range Images，自动激光雷达内校准估计用于无损范围图像），这是首个通用的、与传感器无关的方法，能够从旋转式激光雷达点云中生成无损范围图像，而无需依赖制造商的元数据或校准文件。我们的算法通过推断关键参数（如激光束配置、角度分布以及每束激光的校准修正）来自动逆向工程任何旋转式激光雷达传感器的内在几何结构，从而实现无损投影，并能完整重建点云，且不丢失任何点。在KITTI和DurLAR完整数据集上的全面评估表明，ALICE-LRI实现了完美的点保留，在所有点云中均无点丢失。几何精度在传感器精度范围内得到了良好保持，实现了几何无损，并具备实时性能。我们还展示了一个压缩案例研究，验证了该方法在后续应用中的显著优势，证明其在实际应用中可带来显著的质量提升。这种从近似投影到无损投影的范式转变，为需要完整几何保真的高精度遥感应用开辟了新的可能性。",
        "translated_title": "ALICE-LRI：一种无需校准元数据的旋转激光雷达传感器无损距离图像生成通用方法",
        "label": [
            "遥感图像复原"
        ],
        "label_reason": "论文提出无损LiDAR图像生成方法，属于遥感图像复原范畴",
        "relevance_score": 8,
        "novelty_score": 9,
        "novelty_reason": "首次实现无需校准元数据的通用无损LiDAR投影方法"
    },
    {
        "title": "Mixing Importance with Diversity: Joint Optimization for KV Cache\n  Compression in Large Vision-Language Models",
        "url": "http://arxiv.org/abs/2510.20707v1",
        "pub_date": "2025-10-23",
        "summary": "Recent large vision-language models (LVLMs) demonstrate remarkable capabilities in processing extended multi-modal sequences, yet the resulting key-value (KV) cache expansion creates a critical memory bottleneck that fundamentally limits deployment scalability. While existing KV cache compression methods focus on retaining high-importance KV pairs to minimize storage, they often overlook the modality-specific semantic redundancy patterns that emerge distinctively in multi-modal KV caches. In this work, we first analyze how, beyond simple importance, the KV cache in LVLMs exhibits varying levels of redundancy across attention heads. We show that relying solely on importance can only cover a subset of the full KV cache information distribution, leading to potential loss of semantic coverage. To address this, we propose \\texttt{MixKV}, a novel method that mixes importance with diversity for optimized KV cache compression in LVLMs. \\texttt{MixKV} adapts to head-wise semantic redundancy, selectively balancing diversity and importance when compressing KV pairs. Extensive experiments demonstrate that \\texttt{MixKV} consistently enhances existing methods across multiple LVLMs. Under extreme compression (budget=64), \\texttt{MixKV} improves baseline methods by an average of \\textbf{5.1\\%} across five multi-modal understanding benchmarks and achieves remarkable gains of \\textbf{8.0\\%} and \\textbf{9.0\\%} for SnapKV and AdaKV on GUI grounding tasks, all while maintaining comparable inference efficiency. Furthermore, \\texttt{MixKV} extends seamlessly to LLMs with comparable performance gains. Our code is available at \\href{https://github.com/xuyang-liu16/MixKV}{\\textcolor{citeblue}{https://github.com/xuyang-liu16/MixKV}}.",
        "translated": "近年来，大型视觉语言模型（LVLMs）在处理扩展的多模态序列方面表现出显著的能力，然而由此产生的键值（KV）缓存扩展造成了关键的内存瓶颈，从根本上限制了其部署的可扩展性。尽管现有的KV缓存压缩方法主要关注保留高重要性的KV对以最小化存储，但它们往往忽略了在多模态KV缓存中显著出现的模态特定语义冗余模式。在本工作中，我们首先分析了除了简单的“重要性”之外，LVLMs中的KV缓存在注意力头之间表现出不同程度的冗余。我们表明，仅依赖重要性只能覆盖完整的KV缓存信息分布的一部分，从而可能导致语义覆盖的损失。为了解决这一问题，我们提出了一种新方法 \\texttt{MixKV}，该方法将重要性与多样性结合，以优化LVLMs中的KV缓存压缩。\\texttt{MixKV} 能够适应每个注意力头的语义冗余，选择性地在压缩KV对时平衡多样性和重要性。广泛的实验表明，\\texttt{MixKV} 在多种LVLMs中始终优于现有方法。在极端压缩条件下（预算=64），\\texttt{MixKV} 在五个多模态理解基准上平均提升了基线方法 \\textbf{5.1\\%}，并在GUI定位任务中对SnapKV和AdaKV分别实现了显著的提升 \\textbf{8.0\\%} 和 \\textbf{9.0\\%}，同时保持了相当的推理效率。此外，\\texttt{MixKV} 可无缝扩展到大型语言模型（LLMs），并具有类似的性能提升。我们的代码可在 \\href{https://github.com/xuyang-liu16/MixKV}{\\textcolor{citeblue}{https://github.com/xuyang-liu16/MixKV}} 获取。",
        "translated_title": "Mixing Importance with Diversity: Joint Optimization for KV Cache Compression in Large Vision-Language Models\n\n将重要性与多样性融合：用于大视觉-语言模型中 KV 缓存压缩的联合优化",
        "label": [],
        "label_reason": "论文关注视觉语言模型的KV缓存压缩，不涉及像素级图像处理任务。",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "提出一种结合重要性与多样性的KV压缩方法，属于常规优化改进。"
    },
    {
        "title": "Diagnosing Visual Reasoning: Challenges, Insights, and a Path Forward",
        "url": "http://arxiv.org/abs/2510.20696v1",
        "pub_date": "2025-10-23",
        "summary": "Multimodal large language models (MLLMs) that integrate visual and textual reasoning leverage chain-of-thought (CoT) prompting to tackle complex visual tasks, yet continue to exhibit visual hallucinations and an over-reliance on textual priors. We present a systematic diagnosis of state-of-the-art vision-language models using a three-stage evaluation framework, uncovering key failure modes. To address these, we propose an agent-based architecture that combines LLM reasoning with lightweight visual modules, enabling fine-grained analysis and iterative refinement of reasoning chains. Our results highlight future visual reasoning models should focus on integrating a broader set of specialized tools for analyzing visual content. Our system achieves significant gains (+10.3 on MMMU, +6.0 on MathVista over a 7B baseline), matching or surpassing much larger models. We will release our framework and evaluation suite to facilitate future research.",
        "translated": "集成视觉与文本推理能力的多模态大语言模型（MLLMs）利用思维链（CoT）提示来处理复杂的视觉任务，但仍然表现出视觉幻觉现象，并且过度依赖文本先验。我们提出了一种三阶段的评估框架，对最先进的视觉语言模型进行了系统诊断，揭示了其关键的失败模式。为了解决这些问题，我们设计了一种基于智能体的架构，将大语言模型的推理能力与轻量级视觉模块相结合，从而实现对推理链的细粒度分析与迭代优化。实验结果表明，未来的视觉推理模型应重点关注将更广泛的专用工具集成进来，以分析视觉内容。我们的系统在基准上取得了显著的性能提升（MMMU 提升 +10.3，MathVista 提升 +6.0），达到了甚至超越了更大规模模型的性能。我们将发布我们的框架和评估套件，以促进未来的研究。",
        "translated_title": "诊断视觉推理：挑战、见解与未来路径",
        "label": [],
        "label_reason": "论文聚焦于视觉推理而非图像像素级处理",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "提出基于代理的架构改进视觉推理"
    },
    {
        "title": "A Data-Centric Approach to Multilingual E-Commerce Product Search: Case\n  Study on Query-Category and Query-Item Relevance",
        "url": "http://arxiv.org/abs/2510.21671v1",
        "pub_date": "2025-10-24",
        "summary": "Multilingual e-commerce search suffers from severe data imbalance across languages, label noise, and limited supervision for low-resource languages--challenges that impede the cross-lingual generalization of relevance models despite the strong capabilities of large language models (LLMs). In this work, we present a practical, architecture-agnostic, data-centric framework to enhance performance on two core tasks: Query-Category (QC) relevance (matching queries to product categories) and Query-Item (QI) relevance (matching queries to product titles). Rather than altering the model, we redesign the training data through three complementary strategies: (1) translation-based augmentation to synthesize examples for languages absent in training, (2) semantic negative sampling to generate hard negatives and mitigate class imbalance, and (3) self-validation filtering to detect and remove likely mislabeled instances. Evaluated on the CIKM AnalytiCup 2025 dataset, our approach consistently yields substantial F1 score improvements over strong LLM baselines, achieving competitive results in the official competition. Our findings demonstrate that systematic data engineering can be as impactful as--and often more deployable than--complex model modifications, offering actionable guidance for building robust multilingual search systems in the real-world e-commerce settings.",
        "translated": "多语言电商搜索面临语言间数据严重不平衡、标签噪声以及低资源语言监督信号不足等挑战，这些问题阻碍了相关性模型在跨语言场景下的泛化能力，即使大型语言模型（LLM）具备强大能力亦然。在本工作中，我们提出了一种实用、架构无关、以数据为中心的框架，旨在提升两个核心任务的性能：查询-类别（Query-Category, QC）相关性（将查询匹配至商品类别）和查询-物料（Query-Item, QI）相关性（将查询匹配至商品标题）。我们并未对模型结构进行修改，而是通过三种互补策略重新设计训练数据：（1）基于翻译的数据增强，为训练数据中缺失的语言合成样本；（2）语义负采样，生成难负例并缓解类别不平衡问题；（3）自验证过滤，检测并移除可能的错误标注样本。在CIKM AnalytiCup 2025数据集上的评估表明，我们的方法在多个强LLM基线模型上持续实现显著的F1分数提升，并在官方竞赛中取得具有竞争力的结果。研究结果表明，系统性的数据工程在提升性能方面可以与复杂的模型修改相媲美，且通常更具可部署性，为构建面向真实电商场景的稳健多语言搜索系统提供了可操作的指导。",
        "translated_title": "以数据为中心的多语言电商产品搜索方法：查询-类别与查询-物料相关性案例研究",
        "label": [
            "Query-Item Relevance",
            "Query-Category Relevance",
            "负采样与对比学习",
            "通用推荐技术"
        ],
        "label_reason": "聚焦多语言电商搜索中查询与商品/类目相关性，涉及负采样与数据增强，属于推荐系统相关任务",
        "relevance_score": 6,
        "novelty_score": 7,
        "novelty_reason": "提出数据为中心的框架，结合翻译增强与自验证过滤，提升低资源语言性能，方法新颖且实用"
    },
    {
        "title": "DeepAgent: A General Reasoning Agent with Scalable Toolsets",
        "url": "http://arxiv.org/abs/2510.21618v1",
        "pub_date": "2025-10-24",
        "summary": "Large reasoning models have demonstrated strong problem-solving abilities, yet real-world tasks often require external tools and long-horizon interactions. Existing agent frameworks typically follow predefined workflows, which limit autonomous and global task completion. In this paper, we introduce DeepAgent, an end-to-end deep reasoning agent that performs autonomous thinking, tool discovery, and action execution within a single, coherent reasoning process. To address the challenges of long-horizon interactions, particularly the context length explosion from multiple tool calls and the accumulation of interaction history, we introduce an autonomous memory folding mechanism that compresses past interactions into structured episodic, working, and tool memories, reducing error accumulation while preserving critical information. To teach general-purpose tool use efficiently and stably, we develop an end-to-end reinforcement learning strategy, namely ToolPO, that leverages LLM-simulated APIs and applies tool-call advantage attribution to assign fine-grained credit to the tool invocation tokens. Extensive experiments on eight benchmarks, including general tool-use tasks (ToolBench, API-Bank, TMDB, Spotify, ToolHop) and downstream applications (ALFWorld, WebShop, GAIA, HLE), demonstrate that DeepAgent consistently outperforms baselines across both labeled-tool and open-set tool retrieval scenarios. This work takes a step toward more general and capable agents for real-world applications. The code and demo are available at https://github.com/RUC-NLPIR/DeepAgent.",
        "translated": "大型推理模型已展现出强大的问题求解能力，然而现实世界任务通常需要外部工具支持以及长跨度交互。现有代理框架通常遵循预定义的工作流，这限制了自主性和全局任务完成能力。本文提出 DeepAgent，一种端到端的深度推理代理，能够在单一、连贯的推理过程中自主进行思考、工具发现与动作执行。为应对长跨度交互带来的挑战，特别是多次工具调用导致的上下文长度爆炸以及交互历史的累积问题，我们引入了一种自主记忆折叠机制，将过往交互压缩为结构化的场景记忆、工作记忆和工具记忆，从而在减少误差累积的同时保留关键信息。为高效且稳定地学习通用工具使用能力，我们开发了一种端到端强化学习策略，即 ToolPO，该策略利用大语言模型模拟的 API，并通过工具调用优势归因方法，对工具调用令牌进行细粒度的奖励分配。在八个基准数据集上的广泛实验，包括通用工具使用任务（ToolBench、API-Bank、TMDB、Spotify、ToolHop）以及下游应用（ALFWorld、WebShop、GAIA、HLE），表明 DeepAgent 在标注工具和开放集工具召回场景下均显著优于基线方法。本工作朝着面向实际应用的更通用、更强能力的代理迈出了重要一步。代码与演示可访问 https://github.com/RUC-NLPIR/DeepAgent。",
        "translated_title": "DeepAgent：一种具备可扩展工具集的通用推理代理",
        "label": [],
        "label_reason": "论文聚焦通用推理智能体与工具调用，未针对推荐系统设计，无直接关联。",
        "relevance_score": 3,
        "novelty_score": 8,
        "novelty_reason": "提出自主记忆折叠与ToolPO强化学习策略，对通用智能体有创新，但非推荐领域。"
    },
    {
        "title": "Doc-Researcher: A Unified System for Multimodal Document Parsing and\n  Deep Research",
        "url": "http://arxiv.org/abs/2510.21603v1",
        "pub_date": "2025-10-24",
        "summary": "Deep Research systems have revolutionized how LLMs solve complex questions through iterative reasoning and evidence gathering. However, current systems remain fundamentally constrained to textual web data, overlooking the vast knowledge embedded in multimodal documents Processing such documents demands sophisticated parsing to preserve visual semantics (figures, tables, charts, and equations), intelligent chunking to maintain structural coherence, and adaptive retrieval across modalities, which are capabilities absent in existing systems. In response, we present Doc-Researcher, a unified system that bridges this gap through three integrated components: (i) deep multimodal parsing that preserves layout structure and visual semantics while creating multi-granular representations from chunk to document level, (ii) systematic retrieval architecture supporting text-only, vision-only, and hybrid paradigms with dynamic granularity selection, and (iii) iterative multi-agent workflows that decompose complex queries, progressively accumulate evidence, and synthesize comprehensive answers across documents and modalities. To enable rigorous evaluation, we introduce M4DocBench, the first benchmark for Multi-modal, Multi-hop, Multi-document, and Multi-turn deep research. Featuring 158 expert-annotated questions with complete evidence chains across 304 documents, M4DocBench tests capabilities that existing benchmarks cannot assess. Experiments demonstrate that Doc-Researcher achieves 50.6% accuracy, 3.4xbetter than state-of-the-art baselines, validating that effective document research requires not just better retrieval, but fundamentally deep parsing that preserve multimodal integrity and support iterative research. Our work establishes a new paradigm for conducting deep research on multimodal document collections.",
        "translated": "深度研究系统通过迭代推理和证据收集，彻底改变了大语言模型（LLM）解决复杂问题的方式。然而，当前系统仍从根本上受限于文本形式的网络数据，忽视了嵌入在多模态文档中的海量知识。处理此类文档需要复杂的解析技术以保留视觉语义（如图表、表格、图表和公式），智能的分块策略以维持结构连贯性，以及跨模态的自适应检索能力，而这些能力在现有系统中均缺失。为此，我们提出Doc-Researcher，一个统一的系统，通过三个集成组件弥合这一差距：（i）深度多模态解析，能够在保持布局结构和视觉语义的同时，从分块到文档级别生成多粒度表示；（ii）系统化的检索架构，支持纯文本、纯视觉以及混合范式，并具备动态粒度选择能力；（iii）迭代多智能体工作流，能够分解复杂查询，逐步积累证据，并在跨文档和跨模态的场景下合成全面的答案。为实现严格评估，我们引入M4DocBench，这是首个用于多模态、多跳、多文档和多轮次深度研究的基准测试。该基准包含158个专家标注的问题，覆盖304个文档的完整证据链，测试了现有基准无法评估的能力。实验表明，Doc-Researcher实现了50.6%的准确率，比现有最先进基线高出3.4倍，验证了有效的文档研究不仅需要更优的检索，更需要从根本上具备保持多模态完整性的深度解析能力，并支持迭代式研究。本工作为在多模态文档集合上开展深度研究建立了新的范式。",
        "translated_title": "Doc-Researcher：一种用于多模态文档解析与深度研究的统一系统",
        "label": [],
        "label_reason": "论文聚焦多模态文档解析与深度研究，核心为信息检索与推理，非推荐系统任务。",
        "relevance_score": 3,
        "novelty_score": 8,
        "novelty_reason": "提出统一多模态解析与检索框架，支持动态粒度选择与多智能体协作，创新性强。"
    },
    {
        "title": "Redefining Retrieval Evaluation in the Era of LLMs",
        "url": "http://arxiv.org/abs/2510.21440v1",
        "pub_date": "2025-10-24",
        "summary": "Traditional Information Retrieval (IR) metrics, such as nDCG, MAP, and MRR, assume that human users sequentially examine documents with diminishing attention to lower ranks. This assumption breaks down in Retrieval Augmented Generation (RAG) systems, where search results are consumed by Large Language Models (LLMs), which, unlike humans, process all retrieved documents as a whole rather than sequentially. Additionally, traditional IR metrics do not account for related but irrelevant documents that actively degrade generation quality, rather than merely being ignored. Due to these two major misalignments, namely human vs. machine position discount and human relevance vs. machine utility, classical IR metrics do not accurately predict RAG performance. We introduce a utility-based annotation schema that quantifies both the positive contribution of relevant passages and the negative impact of distracting ones. Building on this foundation, we propose UDCG (Utility and Distraction-aware Cumulative Gain), a metric using an LLM-oriented positional discount to directly optimize the correlation with the end-to-end answer accuracy. Experiments on five datasets and six LLMs demonstrate that UDCG improves correlation by up to 36% compared to traditional metrics. Our work provides a critical step toward aligning IR evaluation with LLM consumers and enables more reliable assessment of RAG components",
        "translated": "传统信息检索（IR）指标，如 nDCG、MAP 和 MRR，假设人类用户会依次检视文档，并对排名较低的文档注意力逐渐降低。这一假设在检索增强生成（RAG）系统中不再成立，因为在 RAG 中，检索结果由大语言模型（LLM）消费，而 LLM 与人类不同，会整体处理所有检索到的文档，而非逐个顺序处理。此外，传统 IR 指标未考虑那些相关但无关的文档，这些文档会主动降低生成质量，而非仅仅被忽略。由于这两个主要错配——即人类与机器在位置衰减上的差异，以及人类相关性与机器效用之间的差异——经典 IR 指标无法准确预测 RAG 系统的性能。我们提出了一种基于效用的标注方案，用于量化相关段落的正向贡献以及干扰性段落的负向影响。在此基础上，我们提出了 UDCG（Utility and Distraction-aware Cumulative Gain），一种采用面向 LLM 的位置衰减策略的指标，直接优化与端到端答案准确率的相关性。在五个数据集和六种 LLM 上的实验表明，与传统指标相比，UDCG 的相关性提升了最高达 36%。我们的工作为实现 IR 评估与 LLM 消费者的对齐迈出了关键一步，并有助于更可靠地评估 RAG 各组件的性能。",
        "translated_title": "重定义大语言模型时代下的召回评估",
        "label": [
            "召回",
            "LLM生成式推荐",
            "推荐系统评估"
        ],
        "label_reason": "论文聚焦RAG系统中召回评估，提出LLM导向的评估指标，与生成式推荐相关",
        "relevance_score": 7,
        "novelty_score": 8,
        "novelty_reason": "提出UDCG新指标，结合LLM特性优化评估，提升与生成质量相关性"
    },
    {
        "title": "SciNUP: Natural Language User Interest Profiles for Scientific\n  Literature Recommendation",
        "url": "http://arxiv.org/abs/2510.21352v1",
        "pub_date": "2025-10-24",
        "summary": "The use of natural language (NL) user profiles in recommender systems offers greater transparency and user control compared to traditional representations. However, there is scarcity of large-scale, publicly available test collections for evaluating NL profile-based recommendation. To address this gap, we introduce SciNUP, a novel synthetic dataset for scholarly recommendation that leverages authors' publication histories to generate NL profiles and corresponding ground truth items. We use this dataset to conduct a comparison of baseline methods, ranging from sparse and dense retrieval approaches to state-of-the-art LLM-based rerankers. Our results show that while baseline methods achieve comparable performance, they often retrieve different items, indicating complementary behaviors. At the same time, considerable headroom for improvement remains, highlighting the need for effective NL-based recommendation approaches. The SciNUP dataset thus serves as a valuable resource for fostering future research and development in this area.",
        "translated": "在推荐系统中使用自然语言（NL）用户画像相比传统表示方式提供了更高的透明度和用户控制能力。然而，目前缺乏大规模、公开可用的测试集用于评估基于NL画像的推荐方法。为弥补这一空白，我们提出了SciNUP，一个面向学术推荐的新型合成数据集，该数据集利用作者的发表历史生成NL画像及其对应的 ground truth 物料。我们利用该数据集对基线方法进行了对比，涵盖从稀疏和密集检索方法到最先进的大语言模型（LLM）重排器。实验结果表明，尽管基线方法在性能上表现相当，但它们召回的物料往往不同，显示出互补的行为特性。同时，仍存在显著的性能提升空间，凸显了有效NL推荐方法的必要性。因此，SciNUP数据集为推动该领域未来的研究与开发提供了宝贵的资源。",
        "translated_title": "SciNUP：面向科学文献推荐的自然语言用户兴趣画像",
        "label": [
            "召回",
            "重排",
            "LLM生成式推荐",
            "通用推荐技术"
        ],
        "label_reason": "论文构建NL用户画像用于学术推荐，涉及召回与重排，采用LLM等前沿方法，与推荐系统紧密相关。",
        "relevance_score": 8,
        "novelty_score": 7,
        "novelty_reason": "提出合成数据集SciNUP，支持NL画像推荐，方法组合新颖，但未突破核心范式。"
    },
    {
        "title": "Automated Detection of Visual Attribute Reliance with a Self-Reflective\n  Agent",
        "url": "http://arxiv.org/abs/2510.21704v1",
        "pub_date": "2025-10-24",
        "summary": "When a vision model performs image recognition, which visual attributes drive its predictions? Detecting unintended reliance on specific visual features is critical for ensuring model robustness, preventing overfitting, and avoiding spurious correlations. We introduce an automated framework for detecting such dependencies in trained vision models. At the core of our method is a self-reflective agent that systematically generates and tests hypotheses about visual attributes that a model may rely on. This process is iterative: the agent refines its hypotheses based on experimental outcomes and uses a self-evaluation protocol to assess whether its findings accurately explain model behavior. When inconsistencies arise, the agent self-reflects over its findings and triggers a new cycle of experimentation. We evaluate our approach on a novel benchmark of 130 models designed to exhibit diverse visual attribute dependencies across 18 categories. Our results show that the agent's performance consistently improves with self-reflection, with a significant performance increase over non-reflective baselines. We further demonstrate that the agent identifies real-world visual attribute dependencies in state-of-the-art models, including CLIP's vision encoder and the YOLOv8 object detector.",
        "translated": "当视觉模型执行图像识别任务时，哪些视觉属性驱动了其预测结果？检测模型对特定视觉特征的非预期依赖，对于确保模型鲁棒性、防止过拟合以及避免虚假相关性至关重要。我们提出了一种自动化框架，用于检测已训练视觉模型中的此类依赖关系。该方法的核心是一个自省代理，它系统性地生成并验证关于模型可能依赖的视觉属性的假设。该过程是迭代的：代理根据实验结果不断优化其假设，并采用自评估协议来判断其发现是否准确解释了模型的行为。当出现不一致时，代理会对自身发现进行自省，并触发新一轮的实验循环。我们在一个包含130个模型的新基准上评估了该方法，这些模型被设计为在18个类别中表现出多样化的视觉属性依赖。实验结果表明，代理的性能随着自省能力的提升而持续改善，相较于无自省能力的基线方法取得了显著的性能提升。我们进一步证明，该代理能够识别当前主流模型中真实存在的视觉属性依赖，包括CLIP的视觉编码器和YOLOv8目标检测器。",
        "translated_title": "自动化视觉属性依赖检测的自反思代理",
        "label": [],
        "label_reason": "论文聚焦于视觉模型属性依赖检测，属于模型分析与鲁棒性研究，非图像像素级恢复或增强任务。",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出自反思代理框架，具有迭代实验与自我评估机制，对模型分析方法有明显创新。"
    },
    {
        "title": "Visual Diffusion Models are Geometric Solvers",
        "url": "http://arxiv.org/abs/2510.21697v1",
        "pub_date": "2025-10-24",
        "summary": "In this paper we show that visual diffusion models can serve as effective geometric solvers: they can directly reason about geometric problems by working in pixel space. We first demonstrate this on the Inscribed Square Problem, a long-standing problem in geometry that asks whether every Jordan curve contains four points forming a square. We then extend the approach to two other well-known hard geometric problems: the Steiner Tree Problem and the Simple Polygon Problem.   Our method treats each problem instance as an image and trains a standard visual diffusion model that transforms Gaussian noise into an image representing a valid approximate solution that closely matches the exact one. The model learns to transform noisy geometric structures into correct configurations, effectively recasting geometric reasoning as image generation.   Unlike prior work that necessitates specialized architectures and domain-specific adaptations when applying diffusion to parametric geometric representations, we employ a standard visual diffusion model that operates on the visual representation of the problem. This simplicity highlights a surprising bridge between generative modeling and geometric problem solving. Beyond the specific problems studied here, our results point toward a broader paradigm: operating in image space provides a general and practical framework for approximating notoriously hard problems, and opens the door to tackling a far wider class of challenging geometric tasks.",
        "translated": "本文表明，视觉扩散模型可作为有效的几何求解器：它们能够在像素空间中直接对几何问题进行推理。我们首先在“内接正方形问题”（Inscribed Square Problem）上验证了这一点，该问题是几何学中的一个长期悬而未决的问题，其核心是：每条若尔当曲线（Jordan curve）是否都包含四个点，能构成一个正方形。随后，我们将该方法扩展至另外两个著名的难解几何问题：斯坦纳树问题（Steiner Tree Problem）和简单多边形问题（Simple Polygon Problem）。我们的方法将每个问题实例视为一张图像，并训练一个标准的视觉扩散模型，该模型将高斯噪声转化为一张表示有效近似解的图像，且该解与精确解高度吻合。模型通过学习将含噪的几何结构转化为正确的构型，实际上将几何推理问题重新表述为图像生成任务。与以往工作不同，后者在将扩散模型应用于参数化几何表示时通常需要专门设计的网络架构和领域特定的适配，而我们采用的是在问题的视觉表示上运行的标准视觉扩散模型。这种简洁性揭示了生成建模与几何问题求解之间一种令人意外的桥梁。超越本文所研究的具体问题，我们的结果指向一种更广泛的范式：在图像空间中操作，为近似解决那些众所周知的难题提供了一种通用且实用的框架，并为处理更广泛类别的挑战性几何任务打开了大门。",
        "translated_title": "视觉扩散模型是几何求解器",
        "label": [],
        "label_reason": "论文聚焦几何问题求解，非图像像素级恢复或增强，属高阶视觉任务。",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出用扩散模型求解几何问题，方法新颖但非图像恢复领域创新。"
    },
    {
        "title": "BachVid: Training-Free Video Generation with Consistent Background and\n  Character",
        "url": "http://arxiv.org/abs/2510.21696v1",
        "pub_date": "2025-10-24",
        "summary": "Diffusion Transformers (DiTs) have recently driven significant progress in text-to-video (T2V) generation. However, generating multiple videos with consistent characters and backgrounds remains a significant challenge. Existing methods typically rely on reference images or extensive training, and often only address character consistency, leaving background consistency to image-to-video models. We introduce BachVid, the first training-free method that achieves consistent video generation without needing any reference images. Our approach is based on a systematic analysis of DiT's attention mechanism and intermediate features, revealing its ability to extract foreground masks and identify matching points during the denoising process. Our method leverages this finding by first generating an identity video and caching the intermediate variables, and then inject these cached variables into corresponding positions in newly generated videos, ensuring both foreground and background consistency across multiple videos. Experimental results demonstrate that BachVid achieves robust consistency in generated videos without requiring additional training, offering a novel and efficient solution for consistent video generation without relying on reference images or additional training.",
        "translated": "扩散Transformer（DiTs）近期在文本到视频（T2V）生成任务中取得了显著进展。然而，生成多个具有角色和背景一致性的视频仍是一项重大挑战。现有方法通常依赖参考图像或大量训练，且往往仅解决角色一致性问题，而将背景一致性留待图像到视频模型处理。我们提出BachVid，这是首个无需训练、且无需任何参考图像即可实现一致视频生成的方法。我们的方法基于对DiT注意力机制和中间特征的系统性分析，揭示了其在去噪过程中提取前景掩码并识别匹配点的能力。基于这一发现，我们的方法首先生成一个身份视频并缓存其中间变量，随后将这些缓存变量注入到新生成视频的对应位置，从而确保多个视频在前景和背景上均保持一致性。实验结果表明，BachVid在无需额外训练的情况下实现了生成视频的鲁棒一致性，为无需依赖参考图像或额外训练的一致视频生成提供了新颖且高效的解决方案。",
        "translated_title": "BachVid：无需训练的视频生成方法，具备一致的背景与角色",
        "label": [],
        "label_reason": "论文聚焦于视频生成与一致性，属于high-level任务，不涉及像素级图像恢复或增强。",
        "relevance_score": 2,
        "novelty_score": 8,
        "novelty_reason": "提出基于DiT中间特征缓存的无训练一致性生成方法，对生成模型有创新性改进。"
    },
    {
        "title": "On Thin Ice: Towards Explainable Conservation Monitoring via Attribution\n  and Perturbations",
        "url": "http://arxiv.org/abs/2510.21689v1",
        "pub_date": "2025-10-24",
        "summary": "Computer vision can accelerate ecological research and conservation monitoring, yet adoption in ecology lags in part because of a lack of trust in black-box neural-network-based models. We seek to address this challenge by applying post-hoc explanations to provide evidence for predictions and document limitations that are important to field deployment. Using aerial imagery from Glacier Bay National Park, we train a Faster R-CNN to detect pinnipeds (harbor seals) and generate explanations via gradient-based class activation mapping (HiResCAM, LayerCAM), local interpretable model-agnostic explanations (LIME), and perturbation-based explanations. We assess explanations along three axes relevant to field use: (i) localization fidelity: whether high-attribution regions coincide with the animal rather than background context; (ii) faithfulness: whether deletion/insertion tests produce changes in detector confidence; and (iii) diagnostic utility: whether explanations reveal systematic failure modes. Explanations concentrate on seal torsos and contours rather than surrounding ice/rock, and removal of the seals reduces detection confidence, providing model-evidence for true positives. The analysis also uncovers recurrent error sources, including confusion between seals and black ice and rocks. We translate these findings into actionable next steps for model development, including more targeted data curation and augmentation. By pairing object detection with post-hoc explainability, we can move beyond \"black-box\" predictions toward auditable, decision-supporting tools for conservation monitoring.",
        "translated": "计算机视觉可加速生态学研究与保护监测，但由于基于黑箱神经网络模型的信任缺失，其在生态学领域的应用仍相对滞后。我们通过引入事后解释方法，为模型预测提供证据，并记录对野外部署至关重要的局限性，以应对这一挑战。我们采用阿拉斯加冰川湾国家公园的航拍影像，训练一个 Faster R-CNN 模型用于检测鳍足类动物（港海豹），并通过基于梯度的类别激活映射（HiResCAM、LayerCAM）、局部可解释模型无关解释（LIME）以及基于扰动的解释方法生成模型解释。我们从三个与野外应用相关的维度评估解释效果：（i）定位保真度：高归因区域是否与动物本身而非背景环境重合；（ii）忠实度：删除/插入测试是否导致检测器置信度发生变化；（iii）诊断效用：解释是否揭示了系统性的失效模式。解释结果主要集中在海豹躯干和轮廓区域，而非周围冰面/岩石，且移除海豹后检测置信度显著下降，为真阳性预测提供了模型层面的证据。该分析还揭示了反复出现的错误来源，包括海豹与黑色冰面及岩石之间的混淆。我们将这些发现转化为模型开发的可操作性下一步措施，例如更精准的数据筛选与数据增强。通过将目标检测与事后可解释性相结合，我们可以超越“黑箱”预测，迈向可审计、支持决策的保护监测工具。",
        "translated_title": "在薄冰之上：通过归因与扰动实现可解释的保护监测",
        "label": [],
        "label_reason": "论文聚焦目标检测与可解释性，属于high-level任务，不涉及像素级图像恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 4,
        "novelty_reason": "提出可解释性分析框架，但方法常规，未在低级图像处理任务上创新。"
    },
    {
        "title": "WorldGrow: Generating Infinite 3D World",
        "url": "http://arxiv.org/abs/2510.21682v1",
        "pub_date": "2025-10-24",
        "summary": "We tackle the challenge of generating the infinitely extendable 3D world -- large, continuous environments with coherent geometry and realistic appearance. Existing methods face key challenges: 2D-lifting approaches suffer from geometric and appearance inconsistencies across views, 3D implicit representations are hard to scale up, and current 3D foundation models are mostly object-centric, limiting their applicability to scene-level generation. Our key insight is leveraging strong generation priors from pre-trained 3D models for structured scene block generation. To this end, we propose WorldGrow, a hierarchical framework for unbounded 3D scene synthesis. Our method features three core components: (1) a data curation pipeline that extracts high-quality scene blocks for training, making the 3D structured latent representations suitable for scene generation; (2) a 3D block inpainting mechanism that enables context-aware scene extension; and (3) a coarse-to-fine generation strategy that ensures both global layout plausibility and local geometric/textural fidelity. Evaluated on the large-scale 3D-FRONT dataset, WorldGrow achieves SOTA performance in geometry reconstruction, while uniquely supporting infinite scene generation with photorealistic and structurally consistent outputs. These results highlight its capability for constructing large-scale virtual environments and potential for building future world models.",
        "translated": "我们致力于解决生成无限扩展的三维世界——即具有连贯几何结构和真实外观的大规模连续环境——这一挑战。现有方法面临关键难题：基于二维提升的方法在不同视角下存在几何与外观不一致性；三维隐式表示难以扩展；当前的三维基础模型大多以物体为中心，限制了其在场景级生成中的适用性。我们的核心见解是利用预训练三维模型提供的强大生成先验，用于结构化场景块的生成。为此，我们提出 WorldGrow，一种用于无界三维场景合成的层次化框架。该方法包含三个核心组件：（1）一个数据整理流程，用于提取高质量的场景块以用于训练，使得三维结构化潜在表示适用于场景生成；（2）一种三维块修复机制，能够实现上下文感知的场景扩展；（3）一种从粗到精的生成策略，确保全局布局的合理性以及局部几何与纹理的保真度。在大规模 3D-FRONT 数据集上的评估表明，WorldGrow 在几何重建方面达到当前最优性能，同时独特地支持无限场景生成，输出效果具有照片级真实感与结构一致性。这些结果突显了其在构建大规模虚拟环境方面的潜力，以及在未来世界模型构建中的应用前景。",
        "translated_title": "WorldGrow: 生成无限3D世界",
        "label": [],
        "label_reason": "论文聚焦3D世界生成，属高阶视觉任务，非像素级图像恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出分层框架与块填充机制，对3D场景生成有显著改进，但非low-level创新。"
    },
    {
        "title": "Foundation Models in Dermatopathology: Skin Tissue Classification",
        "url": "http://arxiv.org/abs/2510.21664v1",
        "pub_date": "2025-10-24",
        "summary": "The rapid generation of whole-slide images (WSIs) in dermatopathology necessitates automated methods for efficient processing and accurate classification. This study evaluates the performance of two foundation models, UNI and Virchow2, as feature extractors for classifying WSIs into three diagnostic categories: melanocytic, basaloid, and squamous lesions. Patch-level embeddings were aggregated into slide-level features using a mean-aggregation strategy and subsequently used to train multiple machine learning classifiers, including logistic regression, gradient-boosted trees, and random forest models. Performance was assessed using precision, recall, true positive rate, false positive rate, and the area under the receiver operating characteristic curve (AUROC) on the test set. Results demonstrate that patch-level features extracted using Virchow2 outperformed those extracted via UNI across most slide-level classifiers, with logistic regression achieving the highest accuracy (90%) for Virchow2, though the difference was not statistically significant. The study also explored data augmentation techniques and image normalization to enhance model robustness and generalizability. The mean-aggregation approach provided reliable slide-level feature representations. All experimental results and metrics were tracked and visualized using WandB.ai, facilitating reproducibility and interpretability. This research highlights the potential of foundation models for automated WSI classification, providing a scalable and effective approach for dermatopathological diagnosis while paving the way for future advancements in slide-level representation learning.",
        "translated": "皮肤病理学中全切片图像（WSIs）的快速生成亟需自动化方法以实现高效处理和准确分类。本研究评估了两个基础模型UNI和Virchow2作为特征提取器，在将WSIs分为三种诊断类别（黑色素细胞病变、基底样病变和鳞状病变）任务中的性能表现。通过均值聚合策略将斑块级嵌入特征整合为切片级特征，并用于训练多种机器学习分类器，包括逻辑回归、梯度提升树和随机森林模型。在测试集上，采用精确率、召回率、真正例率、假正例率以及受试者工作特征曲线下的面积（AUROC）对性能进行评估。结果表明，在大多数切片级分类器中，使用Virchow2提取的斑块级特征表现优于UNI提取的特征，其中逻辑回归在Virchow2上达到最高准确率（90%），尽管该差异在统计学上并不显著。研究还探索了数据增强技术和图像归一化方法，以提升模型的鲁棒性和泛化能力。均值聚合方法提供了可靠的切片级特征表示。所有实验结果与指标均通过WandB.ai进行跟踪与可视化，有助于提升研究的可复现性和可解释性。本研究凸显了基础模型在自动化WSI分类中的潜力，为皮肤病理学诊断提供了一种可扩展且有效的解决方案，同时为未来切片级表征学习的发展奠定了基础。",
        "translated_title": "基础模型在皮肤病理学中的应用：皮肤组织分类",
        "label": [],
        "label_reason": "论文聚焦皮肤组织分类，属于高阶视觉任务，未涉及图像像素级恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 4,
        "novelty_reason": "方法基于现有基础模型提取特征，采用常规聚合策略，无显著创新。"
    },
    {
        "title": "Self-Supervised Learning of Synapse Types from EM Images",
        "url": "http://arxiv.org/abs/2510.21663v1",
        "pub_date": "2025-10-24",
        "summary": "Separating synapses into different classes based on their appearance in EM images has many applications in biology. Examples may include assigning a neurotransmitter to a particular class, or separating synapses whose strength can be modulated from those whose strength is fixed. Traditionally, this has been done in a supervised manner, giving the classification algorithm examples of the different classes. Here we instead separate synapses into classes based only on the observation that nearby synapses in the same neuron are likely more similar than synapses chosen randomly from different cells. We apply our methodology to data from {\\it Drosophila}. Our approach has the advantage that the number of synapse types does not need to be known in advance. It may also provide a principled way to select ground-truth that spans the range of synapse structure.",
        "translated": "根据电子显微镜（EM）图像中突触的外观将其划分为不同类别，在生物学研究中具有多种应用。例如，可将特定神经递质分配给某一类别，或区分那些强度可调的突触与强度固定的突触。传统方法通常采用监督学习方式，通过为分类算法提供各类别的示例来进行分类。本文则提出一种无监督方法，仅基于如下观察：同一神经元内相邻的突触比从不同细胞中随机选取的突触更可能具有相似性，从而实现突触的类别划分。我们将该方法应用于果蝇（{\\it Drosophila}）的数据。该方法的优势在于无需预先知道突触类型的数量，同时可能为选择覆盖突触结构全范围的真实标签（ground-truth）提供一种理论依据。",
        "translated_title": "自监督学习电子显微镜图像中的突触类型",
        "label": [],
        "label_reason": "论文属于生物图像分类任务，非像素级图像恢复或增强，属high-level视觉任务。",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "提出自监督分类方法，但未涉及图像质量改善，创新点在生物应用而非图像处理。"
    },
    {
        "title": "Long-tailed Species Recognition in the NACTI Wildlife Dataset",
        "url": "http://arxiv.org/abs/2510.21657v1",
        "pub_date": "2025-10-24",
        "summary": "As most ''in the wild'' data collections of the natural world, the North America Camera Trap Images (NACTI) dataset shows severe long-tailed class imbalance, noting that the largest 'Head' class alone covers &gt;50% of the 3.7M images in the corpus. Building on the PyTorch Wildlife model, we present a systematic study of Long-Tail Recognition methodologies for species recognition on the NACTI dataset covering experiments on various LTR loss functions plus LTR-sensitive regularisation. Our best configuration achieves 99.40% Top-1 accuracy on our NACTI test data split, substantially improving over a 95.51% baseline using standard cross-entropy with Adam. This also improves on previously reported top performance in MLWIC2 at 96.8% albeit using partly unpublished (potentially different) partitioning, optimiser, and evaluation protocols. To evaluate domain shifts (e.g. night-time captures, occlusion, motion-blur) towards other datasets we construct a Reduced-Bias Test set from the ENA-Detection dataset where our experimentally optimised long-tail enhanced model achieves leading 52.55% accuracy (up from 51.20% with WCE loss), demonstrating stronger generalisation capabilities under distribution shift. We document the consistent improvements of LTR-enhancing scheduler choices in this NACTI wildlife domain, particularly when in tandem with state-of-the-art LTR losses. We finally discuss qualitative and quantitative shortcomings that LTR methods cannot sufficiently address, including catastrophic breakdown for 'Tail' classes under severe domain shift. For maximum reproducibility we publish all dataset splits, key code, and full network weights.",
        "translated": "与大多数“野外”自然场景数据集类似，北美相机陷阱图像（NACTI）数据集表现出严重的长尾类别不平衡问题，其中最大的“Head”类别单独占据了语料库中370万张图像的超过50%。基于PyTorch Wildlife模型，我们针对NACTI数据集上的物种识别任务，系统性地研究了长尾识别（LTR）方法，涵盖了多种LTR损失函数以及对LTR敏感的正则化技术的实验。我们的最优配置在NACTI测试集划分上实现了99.40%的Top-1准确率，显著优于采用标准交叉熵损失和Adam优化器的95.51%基线性能。该结果也超越了此前在MLWIC2中报告的最高性能96.8%，尽管后者使用了部分未公开（可能不同）的数据划分、优化器及评估协议。为评估域偏移（如夜间拍摄、遮挡、运动模糊）对其他数据集的影响，我们从ENA-Detection数据集中构建了一个低偏差测试集，实验优化的长尾增强模型在此测试集上取得了领先的52.55%准确率（相比使用WCE损失的51.20%有所提升），表明其在分布偏移条件下具有更强的泛化能力。我们记录了在NACTI野生动物领域中，LTR增强调度策略的一致性改进，尤其是在与当前最先进的LTR损失函数联合使用时。最后，我们讨论了LTR方法在定性和定量层面仍无法充分解决的局限性，包括在严重域偏移下对“尾部”类别的灾难性失效。为确保最大可复现性，我们公开了所有数据集划分、关键代码及完整的网络权重。",
        "translated_title": "NACTI野生动物数据集中的长尾物种识别",
        "label": [],
        "label_reason": "论文聚焦野生动物物种识别，属图像分类任务，非像素级图像恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "在长尾分布下优化分类模型，属常规改进，无低层视觉新方法或架构创新。"
    },
    {
        "title": "Group Inertial Poser: Multi-Person Pose and Global Translation from\n  Sparse Inertial Sensors and Ultra-Wideband Ranging",
        "url": "http://arxiv.org/abs/2510.21654v1",
        "pub_date": "2025-10-24",
        "summary": "Tracking human full-body motion using sparse wearable inertial measurement units (IMUs) overcomes the limitations of occlusion and instrumentation of the environment inherent in vision-based approaches. However, purely IMU-based tracking compromises translation estimates and accurate relative positioning between individuals, as inertial cues are inherently self-referential and provide no direct spatial reference for others. In this paper, we present a novel approach for robustly estimating body poses and global translation for multiple individuals by leveraging the distances between sparse wearable sensors - both on each individual and across multiple individuals. Our method Group Inertial Poser estimates these absolute distances between pairs of sensors from ultra-wideband ranging (UWB) and fuses them with inertial observations as input into structured state-space models to integrate temporal motion patterns for precise 3D pose estimation. Our novel two-step optimization further leverages the estimated distances for accurately tracking people's global trajectories through the world. We also introduce GIP-DB, the first IMU+UWB dataset for two-person tracking, which comprises 200 minutes of motion recordings from 14 participants. In our evaluation, Group Inertial Poser outperforms previous state-of-the-art methods in accuracy and robustness across synthetic and real-world data, showing the promise of IMU+UWB-based multi-human motion capture in the wild. Code, models, dataset: https://github.com/eth-siplab/GroupInertialPoser",
        "translated": "利用稀疏可穿戴惯性测量单元（IMUs）进行人体全身运动追踪，能够克服基于视觉方法固有的遮挡和环境布设限制。然而，纯IMU-based追踪在全局平移估计以及个体间精确相对定位方面存在不足，因为惯性信息本质上是自参考的，无法为其他个体提供直接的空间参考。本文提出一种新颖方法，通过利用稀疏可穿戴传感器之间的距离信息——包括每个个体内部以及多个个体之间的传感器距离——来稳健估计多人的躯体姿态和全局平移。我们的方法Group Inertial Poser从超宽带测距（UWB）中估计传感器对之间的绝对距离，并将其与惯性观测融合，作为结构化状态空间模型的输入，以整合时间运动模式，实现精确的3D姿态估计。我们提出的新型两步优化方法进一步利用估计的距离信息，准确追踪个体在世界坐标系中的全局轨迹。此外，我们还发布了GIP-DB，这是首个用于两人追踪的IMU+UWB数据集，包含来自14名参与者的200分钟运动记录。在评估中，Group Inertial Poser在合成数据和真实世界数据上均优于现有最先进方法，在准确性和鲁棒性方面表现出色，展现了基于IMU+UWB的野外多人运动捕捉的巨大潜力。代码、模型、数据集：https://github.com/eth-siplab/GroupInertialPoser",
        "translated_title": "群体惯性姿态估计器：基于稀疏惯性传感器与超宽带测距的多人姿态与全局位移估计",
        "label": [],
        "label_reason": "论文聚焦于多人体姿态与全局位移估计，属于三维运动捕捉，非图像像素级处理。",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出两步优化与结构化状态空间模型，融合IMU与UWB数据，方法新颖但非图像处理领域。"
    },
    {
        "title": "A Dynamic Knowledge Distillation Method Based on the Gompertz Curve",
        "url": "http://arxiv.org/abs/2510.21649v1",
        "pub_date": "2025-10-24",
        "summary": "This paper introduces a novel dynamic knowledge distillation framework, Gompertz-CNN, which integrates the Gompertz growth model into the training process to address the limitations of traditional knowledge distillation. Conventional methods often fail to capture the evolving cognitive capacity of student models, leading to suboptimal knowledge transfer. To overcome this, we propose a stage-aware distillation strategy that dynamically adjusts the weight of distillation loss based on the Gompertz curve, reflecting the student's learning progression: slow initial growth, rapid mid-phase improvement, and late-stage saturation. Our framework incorporates Wasserstein distance to measure feature-level discrepancies and gradient matching to align backward propagation behaviors between teacher and student models. These components are unified under a multi-loss objective, where the Gompertz curve modulates the influence of distillation losses over time. Extensive experiments on CIFAR-10 and CIFAR-100 using various teacher-student architectures (e.g., ResNet50 and MobileNet_v2) demonstrate that Gompertz-CNN consistently outperforms traditional distillation methods, achieving up to 8% and 4% accuracy gains on CIFAR-10 and CIFAR-100, respectively.",
        "translated": "本文提出了一种新颖的动态知识蒸馏框架Gompertz-CNN，将Gompertz生长模型融入训练过程，以克服传统知识蒸馏方法的局限性。传统方法通常无法捕捉学生模型认知能力的动态演变，导致知识迁移效果欠佳。为解决该问题，我们提出一种阶段感知的蒸馏策略，基于Gompertz曲线动态调整蒸馏损失的权重，以反映学生模型的学习进程：初期缓慢增长、中期快速提升、后期趋于饱和。本框架引入Wasserstein距离来度量特征层面的差异，并通过梯度匹配对齐教师模型与学生模型的反向传播行为。这些组件统一于一个多损失目标函数中，其中Gompertz曲线调节蒸馏损失随时间的影响。在CIFAR-10和CIFAR-100数据集上，采用多种教师-学生网络架构（如ResNet50和MobileNet_v2）进行大量实验，结果表明Gompertz-CNN始终优于传统蒸馏方法，在CIFAR-10和CIFAR-100上分别实现了最高8%和4%的准确率提升。",
        "translated_title": "基于Gompertz曲线的动态知识蒸馏方法",
        "label": [],
        "label_reason": "论文聚焦知识蒸馏框架优化，用于分类任务，非图像恢复或增强，不涉及像素级图像质量改善。",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出基于Gompertz曲线的动态蒸馏策略，结合Wasserstein距离和梯度匹配，对蒸馏方法有改进。"
    },
    {
        "title": "DAP-MAE: Domain-Adaptive Point Cloud Masked Autoencoder for Effective\n  Cross-Domain Learning",
        "url": "http://arxiv.org/abs/2510.21635v1",
        "pub_date": "2025-10-24",
        "summary": "Compared to 2D data, the scale of point cloud data in different domains available for training, is quite limited. Researchers have been trying to combine these data of different domains for masked autoencoder (MAE) pre-training to leverage such a data scarcity issue. However, the prior knowledge learned from mixed domains may not align well with the downstream 3D point cloud analysis tasks, leading to degraded performance. To address such an issue, we propose the Domain-Adaptive Point Cloud Masked Autoencoder (DAP-MAE), an MAE pre-training method, to adaptively integrate the knowledge of cross-domain datasets for general point cloud analysis. In DAP-MAE, we design a heterogeneous domain adapter that utilizes an adaptation mode during pre-training, enabling the model to comprehensively learn information from point clouds across different domains, while employing a fusion mode in the fine-tuning to enhance point cloud features. Meanwhile, DAP-MAE incorporates a domain feature generator to guide the adaptation of point cloud features to various downstream tasks. With only one pre-training, DAP-MAE achieves excellent performance across four different point cloud analysis tasks, reaching 95.18% in object classification on ScanObjectNN and 88.45% in facial expression recognition on Bosphorus.",
        "translated": "与二维数据相比，可用于训练的不同领域点云数据的规模相当有限。研究人员尝试将这些跨领域数据结合用于掩码自编码器（MAE）的预训练，以应对数据稀缺问题。然而，从混合领域中学习到的先验知识可能与下游三维点云分析任务不匹配，从而导致性能下降。为解决这一问题，我们提出了一种领域自适应点云掩码自编码器（DAP-MAE），这是一种MAE预训练方法，能够自适应地整合跨领域数据集的知识，以实现通用点云分析。在DAP-MAE中，我们设计了一个异构领域适配器，在预训练阶段采用适配模式，使模型能够全面学习来自不同领域点云的信息；在微调阶段则采用融合模式，以增强点云特征。同时，DAP-MAE引入了一个领域特征生成器，用于引导点云特征向多种下游任务进行适配。仅通过一次预训练，DAP-MAE在四种不同的点云分析任务中均取得了优异性能，在ScanObjectNN上的物体分类准确率达到95.18%，在Bosphorus上的面部表情识别准确率达到88.45%。",
        "translated_title": "DAP-MAE：面向有效跨域学习的域自适应点云掩码自编码器",
        "label": [],
        "label_reason": "论文聚焦3D点云跨域预训练，属于高阶3D分析任务，非图像像素级恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 6,
        "novelty_reason": "提出域自适应机制提升跨域点云特征学习，属领域内常规改进，非颠覆性创新。"
    },
    {
        "title": "Epipolar Geometry Improves Video Generation Models",
        "url": "http://arxiv.org/abs/2510.21615v1",
        "pub_date": "2025-10-24",
        "summary": "Video generation models have progressed tremendously through large latent diffusion transformers trained with rectified flow techniques. Yet these models still struggle with geometric inconsistencies, unstable motion, and visual artifacts that break the illusion of realistic 3D scenes. 3D-consistent video generation could significantly impact numerous downstream applications in generation and reconstruction tasks. We explore how epipolar geometry constraints improve modern video diffusion models. Despite massive training data, these models fail to capture fundamental geometric principles underlying visual content. We align diffusion models using pairwise epipolar geometry constraints via preference-based optimization, directly addressing unstable camera trajectories and geometric artifacts through mathematically principled geometric enforcement. Our approach efficiently enforces geometric principles without requiring end-to-end differentiability. Evaluation demonstrates that classical geometric constraints provide more stable optimization signals than modern learned metrics, which produce noisy targets that compromise alignment quality. Training on static scenes with dynamic cameras ensures high-quality measurements while the model generalizes effectively to diverse dynamic content. By bridging data-driven deep learning with classical geometric computer vision, we present a practical method for generating spatially consistent videos without compromising visual quality.",
        "translated": "视频生成模型通过采用修正流技术训练的大规模潜在扩散变换器取得了显著进展。然而，这些模型在生成过程中仍面临几何不一致性、运动不稳定以及视觉伪影等问题，这些问题破坏了真实3D场景的视觉幻觉。具备3D一致性的视频生成有望在生成与重建任务的众多下游应用中产生重大影响。我们探讨了如何利用极线几何约束提升现代视频扩散模型的性能。尽管这些模型拥有海量训练数据，但它们仍无法捕捉视觉内容背后的基本几何原理。我们通过基于偏好的优化方法，利用成对的极线几何约束对扩散模型进行对齐，直接通过数学上严谨的几何约束解决相机轨迹不稳定和几何伪影问题。我们的方法能够高效地强制执行几何原理，且无需端到端可微性。实验结果表明，经典几何约束提供的优化信号比现代学习到的度量指标更为稳定，后者会产生噪声目标，从而损害对齐质量。在静态场景与动态相机条件下进行训练，既能保证高质量的测量结果，又能使模型有效泛化到多样化的动态内容。通过融合数据驱动的深度学习与经典几何计算机视觉，我们提出了一种实用方法，在不牺牲视觉质量的前提下生成空间一致的视频。",
        "translated_title": "极线几何提升视频生成模型",
        "label": [],
        "label_reason": "论文聚焦视频生成，目标为生成3D一致视频，属于high-level生成任务，非像素级图像恢复。",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "引入经典几何约束优化生成模型，提升几何一致性，方法新颖但非low-level任务创新。"
    },
    {
        "title": "Modest-Align: Data-Efficient Alignment for Vision-Language Models",
        "url": "http://arxiv.org/abs/2510.21606v1",
        "pub_date": "2025-10-24",
        "summary": "Cross-modal alignment aims to map heterogeneous modalities into a shared latent space, as exemplified by models like CLIP, which benefit from large-scale image-text pretraining for strong recognition capabilities. However, when operating in resource-constrained settings with limited or low-quality data, these models often suffer from overconfidence and degraded performance due to the prevalence of ambiguous or weakly correlated image-text pairs. Current contrastive learning approaches, which rely on single positive pairs, further exacerbate this issue by reinforcing overconfidence on uncertain samples. To address these challenges, we propose Modest-Align, a lightweight alignment framework designed for robustness and efficiency. Our approach leverages two complementary strategies -- Random Perturbation, which introduces controlled noise to simulate uncertainty, and Embedding Smoothing, which calibrates similarity distributions in the embedding space. These mechanisms collectively reduce overconfidence and improve performance on noisy or weakly aligned samples. Extensive experiments across multiple benchmark datasets demonstrate that Modest-Align outperforms state-of-the-art methods in retrieval tasks, achieving competitive results with over 100x less training data and 600x less GPU time than CLIP. Our method offers a practical and scalable solution for cross-modal alignment in real-world, low-resource scenarios.",
        "translated": "跨模态对齐旨在将异构模态映射到共享的潜在空间，例如CLIP等模型，通过大规模图像-文本预训练获得了强大的识别能力。然而，在资源受限、数据有限或质量较低的场景下，这些模型常因存在大量模糊或弱相关图像-文本对而表现出过度自信，并导致性能下降。当前基于对比学习的方法依赖单一正样本对，进一步加剧了这一问题，即在不确定性样本上强化了过度自信。为应对这些挑战，我们提出Modest-Align，一种轻量级的对齐框架，旨在提升鲁棒性与效率。我们的方法采用两种互补策略：随机扰动（Random Perturbation），通过引入受控噪声模拟不确定性；以及嵌入平滑（Embedding Smoothing），用于校准嵌入空间中的相似性分布。这两种机制共同作用，有效降低过度自信，并提升在噪声或弱对齐样本上的性能。在多个基准数据集上的大量实验表明，Modest-Align在检索任务中超越了现有最先进方法，在训练数据量减少超过100倍、GPU训练时间减少600倍的情况下，仍能取得具有竞争力的结果。本方法为现实世界中低资源场景下的跨模态对齐提供了实用且可扩展的解决方案。",
        "translated_title": "Modest-Align：面向视觉-语言模型的数据高效对齐方法",
        "label": [],
        "label_reason": "论文聚焦视觉-语言模型对齐，属于高阶视觉理解任务，不涉及图像像素级恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 6,
        "novelty_reason": "提出随机扰动与嵌入平滑机制，提升对齐鲁棒性，属方法改进但非低级视觉创新。"
    },
    {
        "title": "S3OD: Towards Generalizable Salient Object Detection with Synthetic Data",
        "url": "http://arxiv.org/abs/2510.21605v1",
        "pub_date": "2025-10-24",
        "summary": "Salient object detection exemplifies data-bounded tasks where expensive pixel-precise annotations force separate model training for related subtasks like DIS and HR-SOD. We present a method that dramatically improves generalization through large-scale synthetic data generation and ambiguity-aware architecture. We introduce S3OD, a dataset of over 139,000 high-resolution images created through our multi-modal diffusion pipeline that extracts labels from diffusion and DINO-v3 features. The iterative generation framework prioritizes challenging categories based on model performance. We propose a streamlined multi-mask decoder that naturally handles the inherent ambiguity in salient object detection by predicting multiple valid interpretations. Models trained solely on synthetic data achieve 20-50% error reduction in cross-dataset generalization, while fine-tuned versions reach state-of-the-art performance across DIS and HR-SOD benchmarks.",
        "translated": "显著性目标检测是典型的数据受限任务，其中昂贵的像素级标注迫使相关子任务（如DIS和HR-SOD）需分别进行模型训练。我们提出一种方法，通过大规模合成数据生成与模糊感知架构显著提升模型泛化能力。我们引入S3OD，一个包含超过139,000张高分辨率图像的数据集，通过我们的多模态扩散生成管道构建，并从扩散模型与DINO-v3特征中提取标签。该迭代生成框架根据模型性能优先生成具有挑战性的类别。我们提出一种精简的多掩码解码器，能够自然处理显著性目标检测中固有的模糊性，通过预测多个有效解释来实现。仅在合成数据上训练的模型在跨数据集泛化中实现20-50%的误差降低，而微调版本在DIS和HR-SOD基准测试中均达到当前最优性能。",
        "translated_title": "S3OD：基于合成数据实现可泛化的显著目标检测",
        "label": [],
        "label_reason": "论文聚焦显著性目标检测，属于高阶视觉任务，不涉及像素级图像恢复或增强。",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出合成数据生成与多掩码解码器，对SOD任务有显著改进，但非low-level任务创新。"
    },
    {
        "title": "Automated interictal epileptic spike detection from simple and noisy\n  annotations in MEG data",
        "url": "http://arxiv.org/abs/2510.21596v1",
        "pub_date": "2025-10-24",
        "summary": "In drug-resistant epilepsy, presurgical evaluation of epilepsy can be considered. Magnetoencephalography (MEG) has been shown to be an effective exam to inform the localization of the epileptogenic zone through the localization of interictal epileptic spikes. Manual detection of these pathological biomarkers remains a fastidious and error-prone task due to the high dimensionality of MEG recordings, and interrater agreement has been reported to be only moderate. Current automated methods are unsuitable for clinical practice, either requiring extensively annotated data or lacking robustness on non-typical data. In this work, we demonstrate that deep learning models can be used for detecting interictal spikes in MEG recordings, even when only temporal and single-expert annotations are available, which represents real-world clinical practice. We propose two model architectures: a feature-based artificial neural network (ANN) and a convolutional neural network (CNN), trained on a database of 59 patients, and evaluated against a state-of-the-art model to classify short time windows of signal. In addition, we employ an interactive machine learning strategy to iteratively improve our data annotation quality using intermediary model outputs. Both proposed models outperform the state-of-the-art model (F1-scores: CNN=0.46, ANN=0.44) when tested on 10 holdout test patients. The interactive machine learning strategy demonstrates that our models are robust to noisy annotations. Overall, results highlight the robustness of models with simple architectures when analyzing complex and imperfectly annotated data. Our method of interactive machine learning offers great potential for faster data annotation, while our models represent useful and efficient tools for automated interictal spikes detection.",
        "translated": "在耐药性癫痫中，可考虑进行术前癫痫评估。脑磁图（MEG）已被证明是一种有效的检查手段，可通过定位发作间期癫痫棘波来确定致痫区的位置。然而，由于MEG记录具有高维特性，手动检测这些病理生物标志物仍是一项繁琐且容易出错的任务，且已有研究表明不同评估者之间的一致性仅为中等水平。当前的自动化方法尚不适合临床实践，要么需要大量标注数据，要么在非典型数据上缺乏鲁棒性。在本研究中，我们证明了深度学习模型可用于检测MEG记录中的发作间期棘波，即使仅提供时间维度和单专家标注数据——这代表了真实的临床实践场景。我们提出了两种模型架构：一种基于特征的人工神经网络（ANN）和一种卷积神经网络（CNN），在包含59名患者的数据库上进行训练，并与当前最先进的模型进行对比，用于分类短时间窗信号。此外，我们采用了一种交互式机器学习策略，通过中间模型输出迭代提升数据标注质量。在10名保留测试患者上测试时，两种所提模型均优于当前最先进的模型（F1分数：CNN=0.46，ANN=0.44）。交互式机器学习策略表明，我们的模型对噪声标注具有鲁棒性。总体而言，结果突显了在分析复杂且标注不完美的数据时，简单架构模型的鲁棒性。我们的交互式机器学习方法在加快数据标注方面具有巨大潜力，而所提出的模型则为自动化发作间期棘波检测提供了实用且高效的工具。",
        "translated_title": "自动化从简单且含噪声的MEG数据标注中检测癫痫间歇期尖波",
        "label": [],
        "label_reason": "论文聚焦于MEG数据中的癫痫尖波检测，属于脑电生理信号分析，非图像处理任务",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "提出交互式机器学习策略，但方法为常规模型组合，无显著创新"
    },
    {
        "title": "Restore Text First, Enhance Image Later: Two-Stage Scene Text Image\n  Super-Resolution with Glyph Structure Guidance",
        "url": "http://arxiv.org/abs/2510.21590v1",
        "pub_date": "2025-10-24",
        "summary": "Current generative super-resolution methods show strong performance on natural images but distort text, creating a fundamental trade-off between image quality and textual readability. To address this, we introduce \\textbf{TIGER} (\\textbf{T}ext-\\textbf{I}mage \\textbf{G}uided sup\\textbf{E}r-\\textbf{R}esolution), a novel two-stage framework that breaks this trade-off through a \\textit{\"text-first, image-later\"} paradigm. \\textbf{TIGER} explicitly decouples glyph restoration from image enhancement: it first reconstructs precise text structures and then uses them to guide subsequent full-image super-resolution. This glyph-to-image guidance ensures both high fidelity and visual consistency. To support comprehensive training and evaluation, we also contribute the \\textbf{UltraZoom-ST} (UltraZoom-Scene Text), the first scene text dataset with extreme zoom (\\textbf{$\\times$14.29}). Extensive experiments show that \\textbf{TIGER} achieves \\textbf{state-of-the-art} performance, enhancing readability while preserving overall image quality.",
        "translated": "当前的生成式超分辨率方法在自然图像上表现出色，但在处理文本时容易产生失真，从而在图像质量与文本可读性之间形成根本性的权衡。为解决这一问题，我们提出了一种新颖的两阶段框架——**TIGER**（**T**ext-**I**mage **G**uided sup**E**r-**R**esolution），该框架通过“**文本优先、图像后处理**”的范式打破上述权衡。**TIGER** 显式地将字形恢复与图像增强过程解耦：首先重建精确的文本结构，随后利用这些结构引导后续的全图像超分辨率处理。这种从字形到图像的引导机制确保了高保真度与视觉一致性。为支持全面的训练与评估，我们还构建了首个包含极端放大倍率（**×14.29**）的场景文本数据集——**UltraZoom-ST**（UltraZoom-Scene Text）。大量实验表明，**TIGER** 达到了**最先进的性能**，在提升文本可读性的同时，有效保持了整体图像质量。",
        "translated_title": "恢复文字先行，图像增强随后：基于字形结构引导的两阶段场景文本图像超分辨率",
        "label": [
            "超分辨率",
            "图像恢复"
        ],
        "label_reason": "论文聚焦场景文本图像超分辨率，显著提升文字可读性与图像质量，属典型low-level图像恢复任务。",
        "relevance_score": 10,
        "novelty_score": 9,
        "novelty_reason": "提出两阶段‘文字优先’框架，引入字形结构引导机制，对现有SR方法有显著创新。"
    },
    {
        "title": "MATrack: Efficient Multiscale Adaptive Tracker for Real-Time Nighttime\n  UAV Operations",
        "url": "http://arxiv.org/abs/2510.21586v1",
        "pub_date": "2025-10-24",
        "summary": "Nighttime UAV tracking faces significant challenges in real-world robotics operations. Low-light conditions not only limit visual perception capabilities, but cluttered backgrounds and frequent viewpoint changes also cause existing trackers to drift or fail during deployment. To address these difficulties, researchers have proposed solutions based on low-light enhancement and domain adaptation. However, these methods still have notable shortcomings in actual UAV systems: low-light enhancement often introduces visual artifacts, domain adaptation methods are computationally expensive and existing lightweight designs struggle to fully leverage dynamic object information. Based on an in-depth analysis of these key issues, we propose MATrack-a multiscale adaptive system designed specifically for nighttime UAV tracking. MATrack tackles the main technical challenges of nighttime tracking through the collaborative work of three core modules: Multiscale Hierarchy Blende (MHB) enhances feature consistency between static and dynamic templates. Adaptive Key Token Gate accurately identifies object information within complex backgrounds. Nighttime Template Calibrator (NTC) ensures stable tracking performance over long sequences. Extensive experiments show that MATrack achieves a significant performance improvement. On the UAVDark135 benchmark, its precision, normalized precision and AUC surpass state-of-the-art (SOTA) methods by 5.9%, 5.4% and 4.2% respectively, while maintaining a real-time processing speed of 81 FPS. Further tests on a real-world UAV platform validate the system's reliability, demonstrating that MATrack can provide stable and effective nighttime UAV tracking support for critical robotics applications such as nighttime search and rescue and border patrol.",
        "translated": "夜间无人机跟踪在实际机器人作业中面临显著挑战。低光照条件不仅限制了视觉感知能力，杂乱的背景和频繁的视角变化也会导致现有跟踪器在部署过程中出现漂移或失效。为应对这些难题，研究者们提出了基于低光照增强和领域自适应的解决方案。然而，这些方法在实际无人机系统中仍存在明显不足：低光照增强通常会引入视觉伪影，领域自适应方法计算开销较大，而现有的轻量化设计难以充分挖掘动态目标信息。基于对上述关键问题的深入分析，我们提出 MATrack——一种专为夜间无人机跟踪设计的多尺度自适应系统。MATrack 通过三个核心模块的协同工作，有效应对夜间跟踪的主要技术挑战：多尺度层次融合模块（Multiscale Hierarchy Blende, MHB）增强静态与动态模板之间的特征一致性；自适应关键令牌门控（Adaptive Key Token Gate）在复杂背景下精确识别目标信息；夜间模板校准器（Nighttime Template Calibrator, NTC）确保在长序列中保持稳定的跟踪性能。大量实验表明，MATrack 实现了显著的性能提升。在 UAVDark135 基准测试集上，其精度、归一化精度和 AUC 分别比当前最先进的（SOTA）方法提升 5.9%、5.4% 和 4.2%，同时保持 81 FPS 的实时处理速度。在真实无人机平台上的进一步测试验证了系统的可靠性，表明 MATrack 能够为夜间搜救、边境巡逻等关键机器人应用提供稳定且有效的夜间无人机跟踪支持。",
        "translated_title": "MATrack：面向实时夜间无人机操作的高效多尺度自适应跟踪器",
        "label": [
            "低光照增强"
        ],
        "label_reason": "论文虽涉及低光照增强，但核心为跟踪任务，非像素级图像恢复。",
        "relevance_score": 4,
        "novelty_score": 7,
        "novelty_reason": "提出多尺度自适应跟踪框架，模块设计有创新，提升跟踪性能。"
    },
    {
        "title": "Sample By Step, Optimize By Chunk: Chunk-Level GRPO For Text-to-Image\n  Generation",
        "url": "http://arxiv.org/abs/2510.21583v1",
        "pub_date": "2025-10-24",
        "summary": "Group Relative Policy Optimization (GRPO) has shown strong potential for flow-matching-based text-to-image (T2I) generation, but it faces two key limitations: inaccurate advantage attribution, and the neglect of temporal dynamics of generation. In this work, we argue that shifting the optimization paradigm from the step level to the chunk level can effectively alleviate these issues. Building on this idea, we propose Chunk-GRPO, the first chunk-level GRPO-based approach for T2I generation. The insight is to group consecutive steps into coherent 'chunk's that capture the intrinsic temporal dynamics of flow matching, and to optimize policies at the chunk level. In addition, we introduce an optional weighted sampling strategy to further enhance performance. Extensive experiments show that ChunkGRPO achieves superior results in both preference alignment and image quality, highlighting the promise of chunk-level optimization for GRPO-based methods.",
        "translated": "组相对策略优化（GRPO）在基于流匹配的文本到图像（T2I）生成任务中展现出强大潜力，但面临两个关键局限：优势归因不准确，以及生成过程时间动态的忽略。本文认为，将优化范式从单步级别提升至块级别，可有效缓解上述问题。基于此思路，我们提出Chunk-GRPO，这是首个面向T2I生成的块级别GRPO方法。其核心思想是将连续生成步骤分组为具有内在一致性的“块”，以捕捉流匹配过程中的固有时间动态，并在块级别上优化策略。此外，我们引入一种可选的加权采样策略以进一步提升性能。大量实验表明，Chunk-GRPO在偏好对齐和图像质量方面均取得更优结果，凸显了块级别优化在GRPO类方法中的潜力。",
        "translated_title": "逐样本、逐块优化：面向文本到图像生成的块级GRPO方法",
        "label": [],
        "label_reason": "论文聚焦于文本到图像生成，属于high-level任务，不涉及像素级图像恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出chunk-level优化范式，对GRPO方法有显著改进，但应用于图像生成而非图像复原。"
    },
    {
        "title": "Foley Control: Aligning a Frozen Latent Text-to-Audio Model to Video",
        "url": "http://arxiv.org/abs/2510.21581v1",
        "pub_date": "2025-10-24",
        "summary": "Foley Control is a lightweight approach to video-guided Foley that keeps pretrained single-modality models frozen and learns only a small cross-attention bridge between them. We connect V-JEPA2 video embeddings to a frozen Stable Audio Open DiT text-to-audio (T2A) model by inserting compact video cross-attention after the model's existing text cross-attention, so prompts set global semantics while video refines timing and local dynamics. The frozen backbones retain strong marginals (video; audio given text) and the bridge learns the audio-video dependency needed for synchronization -- without retraining the audio prior. To cut memory and stabilize training, we pool video tokens before conditioning. On curated video-audio benchmarks, Foley Control delivers competitive temporal and semantic alignment with far fewer trainable parameters than recent multi-modal systems, while preserving prompt-driven controllability and production-friendly modularity (swap/upgrade encoders or the T2A backbone without end-to-end retraining). Although we focus on Video-to-Foley, the same bridge design can potentially extend to other audio modalities (e.g., speech).",
        "translated": "Foley Control 是一种轻量级的视频引导音效生成方法，其保持预训练的单模态模型参数冻结，仅学习一个小型的跨注意力连接桥接结构。我们将 V-JEPA2 视频嵌入与冻结的 Stable Audio Open DiT 文本到音频（T2A）模型相连接，方法是在模型已有的文本跨注意力模块之后插入紧凑的视频跨注意力模块，从而使得文本提示设定全局语义，而视频则用于精炼时序和局部动态特性。冻结的骨干网络保留了强大的边缘分布（视频；给定文本的音频），而桥接结构则学习音频与视频之间所需的时间同步依赖关系——无需重新训练音频先验。为减少内存占用并稳定训练过程，我们在条件输入前对视频 token 进行池化处理。在精心构建的音视频基准数据集上，Foley Control 在参数量远少于近期多模态系统的情况下，实现了具有竞争力的时间与语义对齐性能，同时保持了基于提示的可控性以及便于实际生产的模块化特性（可替换或升级编码器或 T2A 骨干网络，无需端到端重新训练）。尽管我们主要聚焦于视频到音效（Video-to-Foley）任务，但该桥接结构设计同样可潜在扩展至其他音频模态（例如语音）。",
        "translated_title": "Foley Control：对齐冻结的潜在文本到音频模型以匹配视频",
        "label": [],
        "label_reason": "论文聚焦视频引导的音效生成，属于多模态音频合成，非图像像素级处理任务。",
        "relevance_score": 1,
        "novelty_score": 6,
        "novelty_reason": "提出轻量级跨模态桥接结构，但属于音频生成领域常规改进，非图像处理创新。"
    },
    {
        "title": "Scalable Vision-Language-Action Model Pretraining for Robotic\n  Manipulation with Real-Life Human Activity Videos",
        "url": "http://arxiv.org/abs/2510.21571v1",
        "pub_date": "2025-10-24",
        "summary": "This paper presents a novel approach for pretraining robotic manipulation Vision-Language-Action (VLA) models using a large corpus of unscripted real-life video recordings of human hand activities. Treating human hand as dexterous robot end-effector, we show that \"in-the-wild\" egocentric human videos without any annotations can be transformed into data formats fully aligned with existing robotic V-L-A training data in terms of task granularity and labels. This is achieved by the development of a fully-automated holistic human activity analysis approach for arbitrary human hand videos. This approach can generate atomic-level hand activity segments and their language descriptions, each accompanied with framewise 3D hand motion and camera motion. We process a large volume of egocentric videos and create a hand-VLA training dataset containing 1M episodes and 26M frames. This training data covers a wide range of objects and concepts, dexterous manipulation tasks, and environment variations in real life, vastly exceeding the coverage of existing robot data. We design a dexterous hand VLA model architecture and pretrain the model on this dataset. The model exhibits strong zero-shot capabilities on completely unseen real-world observations. Additionally, fine-tuning it on a small amount of real robot action data significantly improves task success rates and generalization to novel objects in real robotic experiments. We also demonstrate the appealing scaling behavior of the model's task performance with respect to pretraining data scale. We believe this work lays a solid foundation for scalable VLA pretraining, advancing robots toward truly generalizable embodied intelligence.",
        "translated": "本文提出了一种新颖的方法，利用大量未经脚本记录的真实人类手部活动视频对机器人操作视觉-语言-动作（VLA）模型进行预训练。将人类手部视为灵巧的机器人末端执行器，我们表明，无需任何标注的“野外”第一人称人类视频可以被转换为与现有机器人V-L-A训练数据在任务粒度和标签上完全对齐的数据格式。这一目标通过开发一种适用于任意人类手部视频的全自动整体性人类活动分析方法实现。该方法能够生成原子级的手部活动片段及其语言描述，每个片段均附带逐帧的3D手部运动和相机运动信息。我们处理了大量第一人称视频，构建了一个包含100万集、2600万帧的手部VLA训练数据集。该训练数据覆盖了现实生活中广泛存在的物体与概念、灵巧操作任务以及环境变化，其覆盖范围远超现有机器人数据集。我们设计了一种灵巧手部VLA模型架构，并在该数据集上进行预训练。该模型在完全未见过的真实世界观测中展现出强大的零样本能力。此外，在少量真实机器人动作数据上进行微调，可显著提升真实机器人实验中的任务成功率以及对新物体的泛化能力。我们还展示了模型任务性能随预训练数据规模增长而表现出的优异扩展特性。我们认为，本工作为可扩展的VLA预训练奠定了坚实基础，推动机器人向真正的泛化具身智能迈进。",
        "translated_title": "可扩展的视觉-语言-动作模型预训练用于基于真实人类活动视频的机器人操作",
        "label": [],
        "label_reason": "论文聚焦于机器人视觉-语言-动作模型预训练，属于高阶视觉任务，不涉及像素级图像恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出自动化人类活动分析方法并构建大规模数据集，对VLA模型预训练有显著创新。"
    },
    {
        "title": "CausalRec: A CausalBoost Attention Model for Sequential Recommendation",
        "url": "http://arxiv.org/abs/2510.21333v1",
        "pub_date": "2025-10-24",
        "summary": "Recent advances in correlation-based sequential recommendation systems have demonstrated substantial success. Specifically, the attention-based model outperforms other RNN-based and Markov chains-based models by capturing both short- and long-term dependencies more effectively. However, solely focusing on item co-occurrences overlooks the underlying motivations behind user behaviors, leading to spurious correlations and potentially inaccurate recommendations. To address this limitation, we present a novel framework that integrates causal attention for sequential recommendation, CausalRec. It incorporates a causal discovery block and a CausalBooster. The causal discovery block learns the causal graph in user behavior sequences, and we provide a theory to guarantee the identifiability of the learned causal graph. The CausalBooster utilizes the discovered causal graph to refine the attention mechanism, prioritizing behaviors with causal significance. Experimental evaluations on real-world datasets indicate that CausalRec outperforms several state-of-the-art methods, with average improvements of 7.21% in Hit Rate (HR) and 8.65% in Normalized Discounted Cumulative Gain (NDCG). To the best of our knowledge, this is the first model to incorporate causality through the attention mechanism in sequential recommendation, demonstrating the value of causality in generating more accurate and reliable recommendations.",
        "translated": "近年来，基于关联性的序列推荐系统取得了显著进展。特别是，基于注意力机制的模型通过更有效地捕捉用户行为序列中的短期和长期依赖关系，优于其他基于RNN和马尔可夫链的模型。然而，仅关注物料共现关系会忽略用户行为背后的潜在动机，从而导致虚假相关性，并可能产生不准确的推荐结果。为解决这一局限性，我们提出了一种新颖的框架——CausalRec，用于序列推荐中的因果注意力机制。该框架包含一个因果发现模块和一个CausalBooster模块。因果发现模块学习用户行为序列中的因果图，并我们提供了理论保证以确保所学习因果图的可识别性。CausalBooster模块利用所发现的因果图来优化注意力机制，优先考虑具有因果重要性的行为。在真实数据集上的实验评估表明，CausalRec优于多种最先进的方法，平均在命中率（HR）上提升7.21%，在归一化折损累计增益（NDCG）上提升8.65%。据我们所知，这是首个通过注意力机制引入因果性以进行序列推荐的模型，展示了因果性在生成更准确、更可靠推荐中的重要价值。",
        "translated_title": "CausalRec：一种用于序列推荐的因果增强注意力模型",
        "label": [
            "序列推荐",
            "因果推荐",
            "精排"
        ],
        "label_reason": "论文针对序列推荐提出因果注意力机制，显著提升推荐准确性，属于推荐系统核心环节",
        "relevance_score": 9,
        "novelty_score": 9,
        "novelty_reason": "首次将因果发现与注意力机制结合，提出CausalBooster模块，创新性强"
    },
    {
        "title": "Pctx: Tokenizing Personalized Context for Generative Recommendation",
        "url": "http://arxiv.org/abs/2510.21276v1",
        "pub_date": "2025-10-24",
        "summary": "Generative recommendation (GR) models tokenize each action into a few discrete tokens (called semantic IDs) and autoregressively generate the next tokens as predictions, showing advantages such as memory efficiency, scalability, and the potential to unify retrieval and ranking. Despite these benefits, existing tokenization methods are static and non-personalized. They typically derive semantic IDs solely from item features, assuming a universal item similarity that overlooks user-specific perspectives. However, under the autoregressive paradigm, semantic IDs with the same prefixes always receive similar probabilities, so a single fixed mapping implicitly enforces a universal item similarity standard across all users. In practice, the same item may be interpreted differently depending on user intentions and preferences. To address this issue, we propose a personalized context-aware tokenizer that incorporates a user's historical interactions when generating semantic IDs. This design allows the same item to be tokenized into different semantic IDs under different user contexts, enabling GR models to capture multiple interpretive standards and produce more personalized predictions. Experiments on three public datasets demonstrate up to 11.44% improvement in NDCG@10 over non-personalized action tokenization baselines. Our code is available at https://github.com/YoungZ365/Pctx.",
        "translated": "生成式推荐（GR）模型将每个行为编码为若干离散的token（称为语义ID），并以自回归方式生成下一个token作为预测，展现出内存高效、可扩展性强以及有望统一召回与排序等优势。尽管如此，现有token化方法仍为静态且非个性化。它们通常仅基于物料特征生成语义ID，假设存在一种通用的物料相似性标准，而忽略了用户特定视角。然而，在自回归范式下，具有相同前缀的语义ID始终会获得相似的概率，因此单一固定的映射隐式地强制所有用户遵循统一的物料相似性标准。在实际场景中，同一物料可能因用户意图和偏好不同而被赋予不同解读。为解决该问题，我们提出一种个性化上下文感知的token化器，该方法在生成语义ID时融入用户的历史交互信息。该设计允许同一物料在不同用户上下文中被编码为不同的语义ID，从而使得GR模型能够捕捉多种解读标准，生成更具个性化的预测结果。在三个公开数据集上的实验表明，相较于非个性化行为token化基线，我们的方法在NDCG@10指标上最高提升11.44%。我们的代码开源于https://github.com/YoungZ365/Pctx。",
        "translated_title": "Pctx：用于生成式推荐的个性化上下文令牌化",
        "label": [
            "LLM生成式推荐（LLM-based Generative Recommendation）",
            "序列推荐（Sequential Recommendation）",
            "通用推荐技术（General Recommendation Techniques）"
        ],
        "label_reason": "论文聚焦生成式推荐中的个性化上下文建模，直接提升推荐精度，属核心推荐问题。",
        "relevance_score": 10,
        "novelty_score": 9,
        "novelty_reason": "提出个性化上下文感知分词器，打破静态语义ID限制，实现用户特定的动态编码，创新性强。"
    },
    {
        "title": "Bi-Level Optimization for Generative Recommendation: Bridging\n  Tokenization and Generation",
        "url": "http://arxiv.org/abs/2510.21242v1",
        "pub_date": "2025-10-24",
        "summary": "Generative recommendation is emerging as a transformative paradigm by directly generating recommended items, rather than relying on matching. Building such a system typically involves two key components: (1) optimizing the tokenizer to derive suitable item identifiers, and (2) training the recommender based on those identifiers. Existing approaches often treat these components separately--either sequentially or in alternation--overlooking their interdependence. This separation can lead to misalignment: the tokenizer is trained without direct guidance from the recommendation objective, potentially yielding suboptimal identifiers that degrade recommendation performance.   To address this, we propose BLOGER, a Bi-Level Optimization for GEnerative Recommendation framework, which explicitly models the interdependence between the tokenizer and the recommender in a unified optimization process. The lower level trains the recommender using tokenized sequences, while the upper level optimizes the tokenizer based on both the tokenization loss and recommendation loss. We adopt a meta-learning approach to solve this bi-level optimization efficiently, and introduce gradient surgery to mitigate gradient conflicts in the upper-level updates, thereby ensuring that item identifiers are both informative and recommendation-aligned. Extensive experiments on real-world datasets demonstrate that BLOGER consistently outperforms state-of-the-art generative recommendation methods while maintaining practical efficiency with no significant additional computational overhead, effectively bridging the gap between item tokenization and autoregressive generation.",
        "translated": "生成式推荐正作为一种变革性范式崭露头角，其通过直接生成推荐物料，而非依赖匹配机制。构建此类系统通常包含两个关键组件：（1）优化分词器以获得合适的物料标识符；（2）基于这些标识符训练推荐模型。现有方法通常将这两个组件独立处理——无论是顺序进行还是交替训练——忽视了它们之间的相互依赖性。这种分离可能导致对齐偏差：分词器在缺乏推荐目标直接指导的情况下进行训练，可能生成次优的标识符，从而降低推荐性能。\n\n为解决这一问题，我们提出 BLOGER，即面向生成式推荐的双层优化框架，其显式地在统一优化过程中建模分词器与推荐器之间的相互依赖性。底层利用分词后的序列训练推荐器，而上层则基于分词损失和推荐损失共同优化分词器。我们采用元学习方法高效求解该双层优化问题，并引入梯度手术以缓解上层更新中的梯度冲突，从而确保物料标识符既具有信息性又与推荐目标对齐。在多个真实数据集上的大量实验表明，BLOGER 在保持实用效率且无显著额外计算开销的前提下，始终优于当前最先进的生成式推荐方法，有效弥合了物料分词与自回归生成之间的鸿沟。",
        "translated_title": "生成式推荐的双层优化：连接分词与生成",
        "label": [
            "LLM生成式推荐（LLM-based Generative Recommendation）",
            "通用推荐技术（General Recommendation Techniques）"
        ],
        "label_reason": "论文聚焦生成式推荐中tokenization与生成的联合优化，直接解决推荐系统核心问题。",
        "relevance_score": 10,
        "novelty_score": 9,
        "novelty_reason": "提出双层优化框架BLOGER，巧妙整合tokenizer与推荐器训练，显著提升生成式推荐性能。"
    },
    {
        "title": "VOGUE: A Multimodal Dataset for Conversational Recommendation in Fashion",
        "url": "http://arxiv.org/abs/2510.21151v1",
        "pub_date": "2025-10-24",
        "summary": "Multimodal conversational recommendation has emerged as a promising paradigm for delivering personalized experiences through natural dialogue enriched by visual and contextual grounding. Yet, current multimodal conversational recommendation datasets remain limited: existing resources either simulate conversations, omit user history, or fail to collect sufficiently detailed feedback, all of which constrain the types of research and evaluation they support.   To address these gaps, we introduce VOGUE, a novel dataset of 60 humanhuman dialogues in realistic fashion shopping scenarios. Each dialogue is paired with a shared visual catalogue, item metadata, user fashion profiles and histories, and post-conversation ratings from both Seekers and Assistants. This design enables rigorous evaluation of conversational inference, including not only alignment between predicted and ground-truth preferences, but also calibration against full rating distributions and comparison with explicit and implicit user satisfaction signals.   Our initial analyses of VOGUE reveal distinctive dynamics of visually grounded dialogue. For example, recommenders frequently suggest items simultaneously in feature-based groups, which creates distinct conversational phases bridged by Seeker critiques and refinements. Benchmarking multimodal large language models against human recommenders shows that while MLLMs approach human-level alignment in aggregate, they exhibit systematic distribution errors in reproducing human ratings and struggle to generalize preference inference beyond explicitly discussed items. These findings establish VOGUE as both a unique resource for studying multimodal conversational systems and as a challenge dataset beyond the current recommendation capabilities of existing top-tier multimodal foundation models such as GPT-4o-mini, GPT-5-mini, and Gemini-2.5-Flash.",
        "translated": "多模态对话推荐已成为通过融合视觉和上下文信息的自然对话实现个性化体验的有前景范式。然而，当前的多模态对话推荐数据集仍存在局限：现有资源要么模拟对话，要么忽略用户历史，要么未能收集足够详尽的反馈，这些都限制了其支持的研究类型和评估能力。\n\n为弥补这些空白，我们提出VOGUE，一个包含60组真实时尚购物场景中的人与人对话的新数据集。每组对话均配有共享的视觉商品目录、物品元数据、用户时尚画像与历史记录，以及对话结束后由Seeker和Assistant双方提供的评分。该设计支持对对话推理进行严格评估，不仅包括预测偏好与真实偏好之间的一致性，还可与完整的评分分布进行校准，并与显式和隐式用户满意度信号进行对比。\n\n对VOGUE的初步分析揭示了视觉化对话的独特动态。例如，推荐系统常以基于特征的分组方式同时推荐多个物品，从而形成由Seeker的批评与细化所连接的特定对话阶段。将多模态大语言模型与人类推荐者进行基准对比显示，尽管MLLMs在整体对齐度上接近人类水平，但其在复现人类评分分布时存在系统性偏差，并且在超越明确讨论物品的偏好推断方面表现不佳。这些发现确立了VOGUE作为研究多模态对话系统独特资源的地位，并表明其作为对现有顶级多模态基础模型（如GPT-4o-mini、GPT-5-mini和Gemini-2.5-Flash）推荐能力的挑战性数据集。",
        "translated_title": "VOGUE：一个面向时尚领域对话式推荐的多模态数据集",
        "label": [
            "多模态推荐",
            "LLM生成式推荐",
            "对话推荐",
            "推荐系统评估"
        ],
        "label_reason": "论文构建多模态对话推荐数据集，支持LLM生成式推荐评估，涉及推荐系统核心环节",
        "relevance_score": 8,
        "novelty_score": 8,
        "novelty_reason": "提出真实场景对话数据集VOGUE，支持多模态LLM评估，推动生成式推荐研究"
    },
    {
        "title": "From Questions to Queries: An AI-powered Multi-Agent Framework for\n  Spatial Text-to-SQL",
        "url": "http://arxiv.org/abs/2510.21045v1",
        "pub_date": "2025-10-23",
        "summary": "The complexity of Structured Query Language (SQL) and the specialized nature of geospatial functions in tools like PostGIS present significant barriers to non-experts seeking to analyze spatial data. While Large Language Models (LLMs) offer promise for translating natural language into SQL (Text-to-SQL), single-agent approaches often struggle with the semantic and syntactic complexities of spatial queries. To address this, we propose a multi-agent framework designed to accurately translate natural language questions into spatial SQL queries. The framework integrates several innovative components, including a knowledge base with programmatic schema profiling and semantic enrichment, embeddings for context retrieval, and a collaborative multi-agent pipeline as its core. This pipeline comprises specialized agents for entity extraction, metadata retrieval, query logic formulation, SQL generation, and a review agent that performs programmatic and semantic validation of the generated SQL to ensure correctness (self-verification). We evaluate our system using both the non-spatial KaggleDBQA benchmark and a new, comprehensive SpatialQueryQA benchmark that includes diverse geometry types, predicates, and three levels of query complexity. On KaggleDBQA, the system achieved an overall accuracy of 81.2% (221 out of 272 questions) after the review agent's review and corrections. For spatial queries, the system achieved an overall accuracy of 87.7% (79 out of 90 questions), compared with 76.7% without the review agent. Beyond accuracy, results also show that in some instances the system generates queries that are more semantically aligned with user intent than those in the benchmarks. This work makes spatial analysis more accessible, and provides a robust, generalizable foundation for spatial Text-to-SQL systems, advancing the development of autonomous GIS.",
        "translated": "结构化查询语言（SQL）的复杂性以及在PostGIS等工具中地理空间函数的专业性，为非专业人士分析空间数据带来了显著障碍。尽管大语言模型（LLM）在将自然语言转化为SQL（文本到SQL）方面展现出潜力，但单智能体方法在处理空间查询的语义和语法复杂性时往往表现不佳。为此，我们提出了一种多智能体框架，旨在准确地将自然语言问题转化为空间SQL查询。该框架集成了多个创新组件，包括具有程序化模式剖析和语义增强的知识库、用于上下文检索的嵌入，以及作为核心的协作式多智能体流水线。该流水线包含专门用于实体提取、元数据检索、查询逻辑构建、SQL生成的智能体，以及一个审查智能体，该智能体对生成的SQL进行程序化和语义验证以确保其正确性（自验证）。我们使用非空间的KaggleDBQA基准测试和一个新的综合性SpatialQueryQA基准测试对系统进行了评估，后者涵盖了多种几何类型、谓词以及三个层次的查询复杂度。在KaggleDBQA上，系统在审查智能体进行审查和修正后，整体准确率达到81.2%（272个问题中正确回答221个）。对于空间查询，系统整体准确率达到87.7%（90个问题中正确回答79个），而未使用审查智能体时准确率为76.7%。除了准确率外，结果还表明，在某些情况下，系统生成的查询在语义上比基准测试中的查询更符合用户意图。本工作使空间分析更加易于访问，并为空间文本到SQL系统提供了稳健、可泛化的基础，推动了自主地理信息系统（GIS）的发展。",
        "translated_title": "从问题到查询：一种用于空间文本到SQL的AI驱动多智能体框架",
        "label": [],
        "label_reason": "论文聚焦于自然语言到SQL的转换，属于数据库查询生成，与推荐系统无直接关联。",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出多智能体协作框架，结合知识库与验证机制，改进了Text-to-SQL的准确性与鲁棒性。"
    },
    {
        "title": "Communication Platform for Non-verbal Autistic children in Oman using\n  Android mobile",
        "url": "http://arxiv.org/abs/2510.21028v1",
        "pub_date": "2025-10-23",
        "summary": "This paper discusses the issue regarding Non-verbal Autism Spectrum Disorder. It has been observed that this mental disorder is listed in major parts of the world including the US, UK, and India. To mitigate this type of disorder, a wide range of smartphones, computers, and artificial intelligence technologies have been used. This technology has helped the population cope with socialization and communication needs. Many applications have been developed to enhance the communication capabilities of non-verbal autistic children. This thesis project proposes the development of a platform that includes a web panel and an Android mobile application to assist non-verbal autistic children in communication, especially in Oman. Different interventions have been merged to improve the quality of life for people on the autism spectrum. The main problem identified in this case is that fragmented approaches are not suitable for autistic children. The augmented reality framework provides the capability to engage autistic children in creative play and self-reflection through interactive screen-based activities.",
        "translated": "本文讨论了非言语型自闭症谱系障碍（Autism Spectrum Disorder）相关的问题。观察发现，该精神障碍在世界多个地区（包括美国、英国和印度）均有广泛报道。为缓解此类障碍，人们已广泛采用智能手机、计算机及人工智能技术。这些技术帮助患者应对社交与沟通需求。许多应用程序已被开发，以提升非言语型自闭症儿童的沟通能力。本论文项目提出开发一个包含网页管理面板和安卓移动应用程序的平台，旨在协助非言语型自闭症儿童进行沟通，尤其适用于阿曼地区。多种干预措施被整合，以提升自闭症谱系人群的生活质量。本研究识别出的主要问题是，碎片化的干预方法并不适合自闭症儿童。增强现实框架通过基于交互式屏幕的活动，使自闭症儿童能够参与创造性游戏和自我反思，从而提供有效的支持。",
        "translated_title": "基于安卓移动设备的阿曼非语言自闭症儿童沟通平台",
        "label": [],
        "label_reason": "论文聚焦于自闭症儿童沟通辅助平台，与推荐系统无直接关联，属教育科技应用。",
        "relevance_score": 1,
        "novelty_score": 2,
        "novelty_reason": "采用AR框架增强互动，属于应用层面常规设计，无推荐系统领域创新。"
    },
    {
        "title": "Gaussian Mixture Flow Matching with Domain Alignment for Multi-Domain\n  Sequential Recommendation",
        "url": "http://arxiv.org/abs/2510.21021v1",
        "pub_date": "2025-10-23",
        "summary": "Users increasingly interact with content across multiple domains, resulting in sequential behaviors marked by frequent and complex transitions. While Cross-Domain Sequential Recommendation (CDSR) models two-domain interactions, Multi-Domain Sequential Recommendation (MDSR) introduces significantly more domain transitions, compounded by challenges such as domain heterogeneity and imbalance. Existing approaches often overlook the intricacies of domain transitions, tend to overfit to dense domains while underfitting sparse ones, and struggle to scale effectively as the number of domains increases. We propose \\textit{GMFlowRec}, an efficient generative framework for MDSR that models domain-aware transition trajectories via Gaussian Mixture Flow Matching. GMFlowRec integrates: (1) a unified dual-masked Transformer to disentangle domain-invariant and domain-specific intents, (2) a Gaussian Mixture flow field to capture diverse behavioral patterns, and (3) a domain-aligned prior to support frequent and sparse transitions. Extensive experiments on JD and Amazon datasets demonstrate that GMFlowRec achieves state-of-the-art performance with up to 44\\% improvement in NDCG@5, while maintaining high efficiency via a single unified backbone, making it scalable for real-world multi-domain sequential recommendation.",
        "translated": "用户在多个领域内日益频繁地交互内容，导致其序列行为呈现出频繁且复杂的领域转换特征。虽然跨领域序列推荐（CDSR）建模了两个领域的交互，但多领域序列推荐（MDSR）引入了显著更多的领域转换，并伴随领域异质性与不平衡等挑战。现有方法往往忽视领域转换的复杂性，倾向于对密集领域过拟合而对稀疏领域欠拟合，且随着领域数量增加，难以有效扩展。我们提出 \\textit{GMFlowRec}，一种高效的生成式框架用于MDSR，通过高斯混合流匹配（Gaussian Mixture Flow Matching）建模领域感知的转换轨迹。GMFlowRec集成以下三部分：(1) 统一的双掩码Transformer，以解耦领域不变与领域特定的意图；(2) 高斯混合流场，以捕捉多样的行为模式；(3) 领域对齐先验，以支持频繁与稀疏的领域转换。在JD和Amazon数据集上的大量实验表明，GMFlowRec在NDCG@5指标上实现了最高达44%的性能提升，同时通过单一统一的主干结构保持了高效率，使其适用于实际场景中的多领域序列推荐。",
        "translated_title": "高斯混合流匹配与领域对齐的多领域序列推荐",
        "label": [
            "序列推荐",
            "多模态推荐",
            "通用推荐技术"
        ],
        "label_reason": "论文针对多域序列推荐提出生成式框架，建模跨域行为轨迹，直接解决推荐系统核心问题",
        "relevance_score": 9,
        "novelty_score": 8,
        "novelty_reason": "创新性引入高斯混合流匹配建模行为轨迹，结合域对齐先验，提升跨域推荐性能"
    },
    {
        "title": "LimRank: Less is More for Reasoning-Intensive Information Reranking",
        "url": "http://arxiv.org/abs/2510.23544v1",
        "pub_date": "2025-10-27",
        "summary": "Existing approaches typically rely on large-scale fine-tuning to adapt LLMs for information reranking tasks, which is computationally expensive. In this work, we demonstrate that modern LLMs can be effectively adapted using only minimal, high-quality supervision. To enable this, we design LIMRANK-SYNTHESIZER, a reusable and open-source pipeline for generating diverse, challenging, and realistic reranking examples. Using this synthetic data, we fine-tune our reranker model, LIMRANK. We evaluate LIMRANK on two challenging benchmarks, i.e., BRIGHT for reasoning-intensive retrieval and FollowIR for instruction-following retrieval. Our experiments demonstrate that LIMRANK achieves competitive performance, while being trained on less than 5% of the data typically used in prior work. Further ablation studies demonstrate the effectiveness of LIMRANK-SYNTHESIZER and the strong generalization capabilities of LIMRANK across downstream tasks, including scientific literature search and retrieval-augmented generation for knowledge-intensive problem solving.",
        "translated": "现有方法通常依赖大规模微调来适配大语言模型（LLM）以完成信息重排任务，计算成本较高。在本研究中，我们证明现代大语言模型仅需少量高质量监督即可有效适配。为此，我们设计了 LIMRANK-SYNTHESIZER，一个可复用且开源的流水线，用于生成多样化、具有挑战性且贴近真实场景的重排示例。基于该合成数据，我们微调了重排模型 LIMRANK。我们在两个具有挑战性的基准数据集上评估了 LIMRANK，即用于推理密集型检索的 BRIGHT 和用于指令跟随型检索的 FollowIR。实验结果表明，LIMRANK 在仅使用先前工作通常所需数据量不到 5% 的情况下，仍能取得具有竞争力的性能。进一步的消融实验验证了 LIMRANK-SYNTHESIZER 的有效性，以及 LIMRANK 在下游任务中的强大泛化能力，涵盖科学文献检索和知识密集型问题求解中的检索增强生成等场景。",
        "translated_title": "LimRank：少即是多——面向推理密集型信息重排的高效方法",
        "label": [
            "重排（Re-ranking）",
            "LLM生成式推荐（LLM-based Generative Recommendation）"
        ],
        "label_reason": "论文聚焦LLM在信息重排任务中的高效微调，直接应用于推荐系统的重排环节，具备推荐系统相关性。",
        "relevance_score": 8,
        "novelty_score": 7,
        "novelty_reason": "提出轻量级微调策略与合成数据生成框架，提升效率与泛化能力，属方法层面创新。"
    },
    {
        "title": "Accurate and Scalable Multimodal Pathology Retrieval via Attentive\n  Vision-Language Alignment",
        "url": "http://arxiv.org/abs/2510.23224v1",
        "pub_date": "2025-10-27",
        "summary": "The rapid digitization of histopathology slides has opened up new possibilities for computational tools in clinical and research workflows. Among these, content-based slide retrieval stands out, enabling pathologists to identify morphologically and semantically similar cases, thereby supporting precise diagnoses, enhancing consistency across observers, and assisting example-based education. However, effective retrieval of whole slide images (WSIs) remains challenging due to their gigapixel scale and the difficulty of capturing subtle semantic differences amid abundant irrelevant content. To overcome these challenges, we present PathSearch, a retrieval framework that unifies fine-grained attentive mosaic representations with global-wise slide embeddings aligned through vision-language contrastive learning. Trained on a corpus of 6,926 slide-report pairs, PathSearch captures both fine-grained morphological cues and high-level semantic patterns to enable accurate and flexible retrieval. The framework supports two key functionalities: (1) mosaic-based image-to-image retrieval, ensuring accurate and efficient slide research; and (2) multi-modal retrieval, where text queries can directly retrieve relevant slides. PathSearch was rigorously evaluated on four public pathology datasets and three in-house cohorts, covering tasks including anatomical site retrieval, tumor subtyping, tumor vs. non-tumor discrimination, and grading across diverse organs such as breast, lung, kidney, liver, and stomach. External results show that PathSearch outperforms traditional image-to-image retrieval frameworks. A multi-center reader study further demonstrates that PathSearch improves diagnostic accuracy, boosts confidence, and enhances inter-observer agreement among pathologists in real clinical scenarios. These results establish PathSearch as a scalable and generalizable retrieval solution for digital pathology.",
        "translated": "组织病理切片的快速数字化为临床和研究工作流程中的计算工具开辟了新的可能性。其中，基于内容的切片检索尤为突出，使病理学家能够识别形态和语义上相似的病例，从而支持精准诊断、提升观察者间的一致性，并辅助基于实例的教学。然而，由于全切片图像（WSIs）具有数十亿像素的规模，且在大量无关内容中捕捉细微语义差异存在困难，因此有效检索全切片图像仍面临挑战。为克服这些挑战，我们提出PathSearch，一种融合了细粒度注意力马赛克表示与全局切片嵌入的检索框架，通过视觉-语言对比学习对齐二者。PathSearch在包含6,926个切片-报告对的语料库上进行训练，能够同时捕捉细粒度形态特征和高层次语义模式，从而实现准确且灵活的检索。该框架支持两大核心功能：（1）基于马赛克的图像到图像检索，确保切片检索的准确性和效率；（2）多模态检索，允许文本查询直接检索相关切片。PathSearch在四个公开病理数据集和三个内部队列上进行了严格评估，涵盖的任务包括解剖部位检索、肿瘤亚型识别、肿瘤与非肿瘤区分以及在乳腺、肺、肾、肝和胃等不同器官中的分级任务。外部评估结果表明，PathSearch优于传统的图像到图像检索框架。一项多中心读者研究进一步证明，在真实临床场景中，PathSearch能够提高诊断准确性、增强病理学家的信心，并提升观察者间的一致性。这些结果确立了PathSearch作为数字病理学中可扩展且具备泛化能力的检索解决方案的地位。",
        "translated_title": "基于注意力机制的视觉-语言对齐实现准确且可扩展的多模态病理学检索",
        "label": [],
        "label_reason": "论文聚焦医学图像检索，属计算机视觉与医疗信息学，与推荐系统无直接关联。",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出注意力机制与对比学习结合的新框架，方法新颖，但应用于医学图像检索而非推荐。"
    },
    {
        "title": "Leveraging Hierarchical Organization for Medical Multi-document\n  Summarization",
        "url": "http://arxiv.org/abs/2510.23104v1",
        "pub_date": "2025-10-27",
        "summary": "Medical multi-document summarization (MDS) is a complex task that requires effectively managing cross-document relationships. This paper investigates whether incorporating hierarchical structures in the inputs of MDS can improve a model's ability to organize and contextualize information across documents compared to traditional flat summarization methods. We investigate two ways of incorporating hierarchical organization across three large language models (LLMs), and conduct comprehensive evaluations of the resulting summaries using automated metrics, model-based metrics, and domain expert evaluation of preference, understandability, clarity, complexity, relevance, coverage, factuality, and coherence. Our results show that human experts prefer model-generated summaries over human-written summaries. Hierarchical approaches generally preserve factuality, coverage, and coherence of information, while also increasing human preference for summaries. Additionally, we examine whether simulated judgments from GPT-4 align with human judgments, finding higher agreement along more objective evaluation facets. Our findings demonstrate that hierarchical structures can improve the clarity of medical summaries generated by models while maintaining content coverage, providing a practical way to improve human preference for generated summaries.",
        "translated": "医学多文档摘要（MDS）是一项复杂的任务，需要有效管理跨文档之间的关系。本文探讨了在MDS输入中引入层次结构是否能够相比传统平面摘要方法，提升模型在跨文档组织和上下文化信息方面的能力。我们研究了在三种大语言模型（LLMs）中引入层次组织的两种方式，并通过自动化指标、基于模型的指标以及领域专家对偏好、可理解性、清晰度、复杂度、相关性、覆盖率、事实性及连贯性等方面的评估，对生成的摘要进行了全面评测。实验结果表明，人类专家更偏好模型生成的摘要而非人工撰写的摘要。层次化方法通常能保持信息的事实性、覆盖率和连贯性，同时提升人类对摘要的偏好程度。此外，我们还考察了GPT-4模拟判断与人类判断的一致性，发现在更客观的评估维度上一致性更高。我们的研究结果表明，层次结构能够提升模型生成医学摘要的清晰度，同时保持内容覆盖率，为提升生成摘要的人类偏好提供了一种实用途径。",
        "translated_title": "利用层级结构进行医学多文档摘要",
        "label": [],
        "label_reason": "论文聚焦医学多文档摘要，属NLP任务，与推荐系统无直接关联。",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出层次化输入结构提升摘要质量，属NLP领域常规改进，非推荐系统创新。"
    },
    {
        "title": "Think before Recommendation: Autonomous Reasoning-enhanced Recommender",
        "url": "http://arxiv.org/abs/2510.23077v1",
        "pub_date": "2025-10-27",
        "summary": "The core task of recommender systems is to learn user preferences from historical user-item interactions. With the rapid development of large language models (LLMs), recent research has explored leveraging the reasoning capabilities of LLMs to enhance rating prediction tasks. However, existing distillation-based methods suffer from limitations such as the teacher model's insufficient recommendation capability, costly and static supervision, and superficial transfer of reasoning ability. To address these issues, this paper proposes RecZero, a reinforcement learning (RL)-based recommendation paradigm that abandons the traditional multi-model and multi-stage distillation approach. Instead, RecZero trains a single LLM through pure RL to autonomously develop reasoning capabilities for rating prediction. RecZero consists of two key components: (1) \"Think-before-Recommendation\" prompt construction, which employs a structured reasoning template to guide the model in step-wise analysis of user interests, item features, and user-item compatibility; and (2) rule-based reward modeling, which adopts group relative policy optimization (GRPO) to compute rewards for reasoning trajectories and optimize the LLM. Additionally, the paper explores a hybrid paradigm, RecOne, which combines supervised fine-tuning with RL, initializing the model with cold-start reasoning samples and further optimizing it with RL. Experimental results demonstrate that RecZero and RecOne significantly outperform existing baseline methods on multiple benchmark datasets, validating the superiority of the RL paradigm in achieving autonomous reasoning-enhanced recommender systems.",
        "translated": "推荐系统的核心任务是从历史用户-物料交互中学习用户偏好。随着大语言模型（LLM）的快速发展，近期研究探索了利用LLM的推理能力来增强评分预测任务。然而，现有的基于蒸馏的方法存在诸多局限，例如教师模型推荐能力不足、监督成本高且静态、推理能力转移流于表面等。为解决这些问题，本文提出RecZero，一种基于强化学习（RL）的推荐范式，摒弃了传统的多模型、多阶段蒸馏方法。RecZero通过纯强化学习训练单个LLM，使其自主发展用于评分预测的推理能力。RecZero包含两个关键组件：（1）“先思考后推荐”提示构造，采用结构化推理模板，引导模型逐步分析用户兴趣、物料特征以及用户-物料匹配度；（2）基于规则的奖励建模，采用群体相对策略优化（GRPO）计算推理路径的奖励，并优化LLM。此外，本文还探索了一种混合范式RecOne，结合监督微调与强化学习，利用冷启动推理样本初始化模型，并进一步通过强化学习进行优化。实验结果表明，RecZero和RecOne在多个基准数据集上显著优于现有基线方法，验证了强化学习范式在实现具备自主推理能力的推荐系统方面的优越性。",
        "translated_title": "推荐前思考：自主推理增强的推荐系统",
        "label": [
            "LLM生成式推荐（LLM-based Generative Recommendation）",
            "精排（Ranking）",
            "通用推荐技术（General Recommendation Techniques）"
        ],
        "label_reason": "论文提出基于强化学习的LLM自主推理推荐框架，直接提升精排阶段的预测能力，属推荐系统核心环节。",
        "relevance_score": 10,
        "novelty_score": 9,
        "novelty_reason": "首创纯强化学习训练LLM进行推荐推理，脱离传统蒸馏范式，具备显著创新性与技术突破。"
    },
    {
        "title": "Multi-Stage Field Extraction of Financial Documents with OCR and Compact\n  Vision-Language Models",
        "url": "http://arxiv.org/abs/2510.23066v1",
        "pub_date": "2025-10-27",
        "summary": "Financial documents are essential sources of information for regulators, auditors, and financial institutions, particularly for assessing the wealth and compliance of Small and Medium-sized Businesses. However, SMB documents are often difficult to parse. They are rarely born digital and instead are distributed as scanned images that are none machine readable. The scans themselves are low in resolution, affected by skew or rotation, and often contain noisy backgrounds. These documents also tend to be heterogeneous, mixing narratives, tables, figures, and multilingual content within the same report. Such characteristics pose major challenges for automated information extraction, especially when relying on end to end large Vision Language Models, which are computationally expensive, sensitive to noise, and slow when applied to files with hundreds of pages.   We propose a multistage pipeline that leverages traditional image processing models and OCR extraction, together with compact VLMs for structured field extraction of large-scale financial documents. Our approach begins with image pre-processing, including segmentation, orientation detection, and size normalization. Multilingual OCR is then applied to recover page-level text. Upon analyzing the text information, pages are retrieved for coherent sections. Finally, compact VLMs are operated within these narrowed-down scopes to extract structured financial indicators.   Our approach is evaluated using an internal corpus of multi-lingual, scanned financial documents. The results demonstrate that compact VLMs, together with a multistage pipeline, achieves 8.8 times higher field level accuracy relative to directly feeding the whole document into large VLMs, only at 0.7 percent of the GPU cost and 92.6 percent less end-to-end service latency.",
        "translated": "财务文件是监管机构、审计师及金融机构的重要信息来源，尤其在评估中小型企业（SMB）的资产状况与合规性方面具有关键作用。然而，SMB的文件通常难以解析：它们极少以数字形式生成，而是以扫描图像形式分发，且不具备机器可读性。这些扫描件分辨率较低，常存在倾斜或旋转问题，且背景通常含有噪声。此外，这些文档往往具有异构性，同一个报告中混合了叙述文本、表格、图表以及多语言内容。这些特征给自动化信息抽取带来了重大挑战，尤其是在依赖端到端大语言模型（VLM）时，因为此类模型在处理数百页文件时计算成本高昂、对噪声敏感且运行速度缓慢。\n\n我们提出了一种多阶段流水线方法，结合传统图像处理模型与OCR提取技术，并利用紧凑型视觉语言模型（VLM）实现大规模财务文档的结构化字段抽取。该方法首先进行图像预处理，包括分割、方向检测与尺寸归一化。随后，采用多语言OCR技术恢复页面级文本内容。在分析文本信息后，对页面进行检索，以定位语义连贯的章节。最后，在这些缩小的上下文范围内应用紧凑型VLM，提取结构化财务指标。\n\n本方法在内部构建的多语言扫描财务文档语料库上进行了评估。实验结果表明，相较于直接将整份文档输入大VLM，所提出的多阶段流水线与紧凑型VLM相结合的方法，在字段级准确率上提升了8.8倍，同时仅消耗0.7%的GPU计算成本，端到端服务延迟减少了92.6%。",
        "translated_title": "多阶段金融文档字段提取：基于OCR与紧凑型视觉-语言模型的方法",
        "label": [],
        "label_reason": "论文聚焦金融文档信息抽取，涉及OCR与视觉语言模型，与推荐系统无直接关联。",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出多阶段处理流程，结合轻量VLM与OCR，提升效率与精度，属常规改进。"
    },
    {
        "title": "Improving Product Search Relevance with EAR-MP: A Solution for the CIKM\n  2025 AnalytiCup",
        "url": "http://arxiv.org/abs/2510.23018v1",
        "pub_date": "2025-10-27",
        "summary": "Multilingual e-commerce search is challenging due to linguistic diversity and the noise inherent in user-generated queries. This paper documents the solution employed by our team (EAR-MP) for the CIKM 2025 AnalytiCup, which addresses two core tasks: Query-Category (QC) relevance and Query-Item (QI) relevance. Our approach first normalizes the multilingual dataset by translating all text into English, then mitigates noise through extensive data cleaning and normalization. For model training, we build on DeBERTa-v3-large and improve performance with label smoothing, self-distillation, and dropout. In addition, we introduce task-specific upgrades, including hierarchical token injection for QC and a hybrid scoring mechanism for QI. Under constrained compute, our method achieves competitive results, attaining an F1 score of 0.8796 on QC and 0.8744 on QI. These findings underscore the importance of systematic data preprocessing and tailored training strategies for building robust, resource-efficient multilingual relevance systems.",
        "translated": "多语言电商搜索因语言多样性以及用户生成查询中固有的噪声而面临挑战。本文记录了我们团队（EAR-MP）在CIKM 2025 AnalytiCup竞赛中采用的解决方案，该方案针对两个核心任务：查询-类别（QC）相关性和查询-物料（QI）相关性。我们的方法首先通过对所有文本进行翻译，将多语言数据集统一归一化为英文，随后通过大规模的数据清洗与归一化处理降低噪声。在模型训练方面，我们基于DeBERTa-v3-large架构，并通过标签平滑、自蒸馏和Dropout技术提升性能。此外，我们引入了针对任务的优化策略，包括面向QC任务的层次化token注入，以及面向QI任务的混合打分机制。在计算资源受限的条件下，我们的方法取得了具有竞争力的结果，在QC任务上F1得分为0.8796，在QI任务上F1得分为0.8744。这些发现强调了系统性数据预处理与定制化训练策略在构建鲁棒且资源高效的多语言相关性系统中的重要性。",
        "translated_title": "提升产品搜索相关性：EAR-MP 解决方案在 CIKM 2025 AnalytiCup 中的应用",
        "label": [],
        "label_reason": "论文聚焦多语言电商搜索相关性，属于信息检索范畴，与推荐系统无直接关联。",
        "relevance_score": 3,
        "novelty_score": 5,
        "novelty_reason": "采用DeBERTa改进策略，属常规优化，无突破性创新。"
    },
    {
        "title": "Tagging-Augmented Generation: Assisting Language Models in Finding\n  Intricate Knowledge In Long Contexts",
        "url": "http://arxiv.org/abs/2510.22956v1",
        "pub_date": "2025-10-27",
        "summary": "Recent investigations into effective context lengths of modern flagship large language models (LLMs) have revealed major limitations in effective question answering (QA) and reasoning over long and complex contexts for even the largest and most impressive cadre of models. While approaches like retrieval-augmented generation (RAG) and chunk-based re-ranking attempt to mitigate this issue, they are sensitive to chunking, embedding and retrieval strategies and models, and furthermore, rely on extensive pre-processing, knowledge acquisition and indexing steps. In this paper, we propose Tagging-Augmented Generation (TAG), a lightweight data augmentation strategy that boosts LLM performance in long-context scenarios, without degrading and altering the integrity and composition of retrieved documents. We validate our hypothesis by augmenting two challenging and directly relevant question-answering benchmarks -- NoLima and NovelQA -- and show that tagging the context or even just adding tag definitions into QA prompts leads to consistent performance gains over the baseline -- up to 17% for 32K token contexts, and 2.9% in complex reasoning question-answering for multi-hop queries requiring knowledge across a wide span of text. Additional details are available at https://sites.google.com/view/tag-emnlp.",
        "translated": "近期对现代主流大语言模型（LLM）有效上下文长度的研究表明，即使是最先进、规模最大的模型，在处理长且复杂的上下文时，其在问答（QA）和推理任务上的表现仍存在显著局限。尽管诸如检索增强生成（RAG）和基于片段的重排等方法试图缓解这一问题，但它们对片段划分、嵌入和检索策略及模型高度敏感，且依赖大量预处理、知识获取和索引步骤。本文提出一种轻量级数据增强策略——标签增强生成（TAG），用于提升LLM在长上下文场景下的性能，同时不破坏检索文档的完整性和结构组成。我们通过增强两个具有挑战性且直接相关的问答基准数据集——NoLima 和 NovelQA——验证了该假设，结果表明，在问答提示中对上下文进行标签标注，或仅添加标签定义，均能相对于基线模型带来稳定的性能提升：在32K token上下文场景下最高提升达17%，在需要跨越广泛文本跨度的知识多跳推理问答任务中提升2.9%。更多细节可参见 https://sites.google.com/view/tag-emnlp。",
        "translated_title": "标签增强生成：辅助语言模型在长上下文中发现复杂知识",
        "label": [],
        "label_reason": "论文聚焦LLM长上下文问答增强，与推荐系统无直接关联，属通用NLP任务。",
        "relevance_score": 3,
        "novelty_score": 6,
        "novelty_reason": "提出轻量级标签增强策略，提升长文本理解，属常规改进，非核心推荐创新。"
    },
    {
        "title": "GTR-Mamba: Geometry-to-Tangent Routing for Hyperbolic POI Recommendation",
        "url": "http://arxiv.org/abs/2510.22942v1",
        "pub_date": "2025-10-27",
        "summary": "Next Point-of-Interest (POI) recommendation is a critical task in modern Location-Based Social Networks (LBSNs), aiming to model the complex decision-making process of human mobility to provide personalized recommendations for a user's next check-in location. Existing POI recommendation models, predominantly based on Graph Neural Networks and sequential models, have been extensively studied. However, these models face a fundamental limitation: they struggle to simultaneously capture the inherent hierarchical structure of spatial choices and the dynamics and irregular shifts of user-specific temporal contexts. To overcome this limitation, we propose GTR-Mamba, a novel framework for cross-manifold conditioning and routing. GTR-Mamba leverages the distinct advantages of different mathematical spaces for different tasks: it models the static, tree-like preference hierarchies in hyperbolic geometry, while routing the dynamic sequence updates to a novel Mamba layer in the computationally stable and efficient Euclidean tangent space. This process is coordinated by a cross-manifold channel that fuses spatio-temporal information to explicitly steer the State Space Model (SSM), enabling flexible adaptation to contextual changes. Extensive experiments on three real-world datasets demonstrate that GTR-Mamba consistently outperforms state-of-the-art baseline models in next POI recommendation.",
        "translated": "下一点兴趣点（POI）推荐是现代基于位置的社交网络（LBSNs）中的关键任务，旨在建模人类移动行为的复杂决策过程，为用户提供个性化的下一个签到地点推荐。现有的POI推荐模型主要基于图神经网络和序列模型，已被广泛研究。然而，这些模型面临一个根本性局限：难以同时捕捉空间选择的内在层次结构，以及用户特定时间上下文的动态性和不规则变化。为克服这一局限，我们提出GTR-Mamba，一种新型的跨流形条件与路由框架。GTR-Mamba利用不同数学空间在不同任务中的独特优势：在双曲几何空间中建模静态、树状的偏好层次结构，同时将动态序列更新路由至计算稳定且高效的欧几里得切空间中的新型Mamba层。该过程由一个跨流形通道协调，该通道融合时空信息以显式引导状态空间模型（SSM），从而实现对上下文变化的灵活适应。在三个真实数据集上的大量实验表明，GTR-Mamba在下一点POI推荐任务中始终优于当前最先进的基线模型。",
        "translated_title": "GTR-Mamba：用于超球面POI推荐的几何到切空间路由机制",
        "label": [
            "序列推荐",
            "图神经网络推荐",
            "通用推荐技术"
        ],
        "label_reason": "论文聚焦POI推荐，融合超球面几何与Mamba序列建模，解决时空动态建模问题，属于推荐系统核心环节。",
        "relevance_score": 8,
        "novelty_score": 8,
        "novelty_reason": "提出跨流形路由机制，结合超球面与欧氏空间建模，创新性地融合几何与序列建模，提升推荐动态适应性。"
    },
    {
        "title": "MGFRec: Towards Reinforced Reasoning Recommendation with Multiple\n  Groundings and Feedback",
        "url": "http://arxiv.org/abs/2510.22888v1",
        "pub_date": "2025-10-27",
        "summary": "The powerful reasoning and generative capabilities of large language models (LLMs) have inspired researchers to apply them to reasoning-based recommendation tasks, which require in-depth reasoning about user interests and the generation of recommended items. However, previous reasoning-based recommendation methods have typically performed inference within the language space alone, without incorporating the actual item space. This has led to over-interpreting user interests and deviating from real items. Towards this research gap, we propose performing multiple rounds of grounding during inference to help the LLM better understand the actual item space, which could ensure that its reasoning remains aligned with real items. Furthermore, we introduce a user agent that provides feedback during each grounding step, enabling the LLM to better recognize and adapt to user interests. Comprehensive experiments conducted on three Amazon review datasets demonstrate the effectiveness of incorporating multiple groundings and feedback. These findings underscore the critical importance of reasoning within the actual item space, rather than being confined to the language space, for recommendation tasks.",
        "translated": "大语言模型（LLM）强大的推理与生成能力激发了研究者将其应用于基于推理的推荐任务，这类任务要求对用户兴趣进行深度推理，并生成推荐物料。然而，以往的基于推理的推荐方法通常仅在语言空间内执行推理，未融入真实的物料空间，导致对用户兴趣的过度解读，并偏离真实物料。针对这一研究空白，我们提出在推理过程中执行多轮接地操作，以帮助LLM更好地理解真实物料空间，从而确保其推理过程与真实物料保持一致。此外，我们引入了一个用户代理，在每一阶段的接地过程中提供反馈，使LLM能够更好地识别并适应用户兴趣。在三个Amazon评论数据集上进行的综合性实验表明，引入多轮接地与反馈机制具有显著有效性。这些发现凸显了在推荐任务中，推理应基于真实物料空间，而非局限于语言空间，具有关键重要性。",
        "translated_title": "MGFRec：基于多锚点与反馈的强化推理推荐",
        "label": [
            "LLM生成式推荐（LLM-based Generative Recommendation）",
            "序列推荐（Sequential Recommendation）",
            "召回（Recall）"
        ],
        "label_reason": "论文聚焦LLM在推荐中推理与生成，通过多轮接地和用户反馈增强推荐准确性，直接关联推荐系统核心环节。",
        "relevance_score": 9,
        "novelty_score": 8,
        "novelty_reason": "提出多轮接地与用户反馈机制，提升LLM在真实物品空间的推理能力，改进现有生成式推荐方法。"
    },
    {
        "title": "Civic Ground Truth in News Recommenders: A Method for Public Value\n  Scoring",
        "url": "http://arxiv.org/abs/2510.22865v1",
        "pub_date": "2025-10-26",
        "summary": "Research in news recommendation systems (NRS) continues to explore the best ways to integrate normative goals such as editorial objectives and public service values into existing systems. Prior efforts have incorporated expert input or audience feedback to quantify these values, laying the groundwork for more civic-minded recommender systems. This paper contributes to that trajectory, introducing a method for embedding civic values into NRS through large-scale, structured audience evaluations. The proposed civic ground truth approach aims to generate value-based labels through a nationally representative survey that are generalisable across a wider news corpus, using automated metadata enrichment.",
        "translated": "新闻推荐系统（NRS）的研究持续探索将规范性目标（如编辑目标与公共服务价值）融入现有系统中的最佳方式。先前的研究已通过引入专家意见或受众反馈来量化这些价值，为构建更具公共责任感的推荐系统奠定了基础。本文沿着这一研究路径做出贡献，提出一种通过大规模、结构化的受众评估将公共价值嵌入NRS的方法。所提出的公共价值基准方法旨在通过一项具有全国代表性的调查生成基于价值的标签，并利用自动化元数据增强技术，使这些标签能够推广至更广泛的新闻语料库。",
        "translated_title": "新闻推荐系统中的公民事实真相：一种公共价值评分方法",
        "label": [
            "通用推荐技术",
            "推荐系统评估"
        ],
        "label_reason": "论文聚焦新闻推荐中公共价值评估，提出基于大规模受众调查的地面真值构建方法，属推荐系统评估与价值导向技术。",
        "relevance_score": 7,
        "novelty_score": 6,
        "novelty_reason": "方法基于调查数据与自动化元数据增强，属现有评估框架的扩展，无颠覆性创新。"
    },
    {
        "title": "REVISION:Reflective Intent Mining and Online Reasoning Auxiliary for\n  E-commerce Visual Search System Optimization",
        "url": "http://arxiv.org/abs/2510.22739v1",
        "pub_date": "2025-10-26",
        "summary": "In Taobao e-commerce visual search, user behavior analysis reveals a large proportion of no-click requests, suggesting diverse and implicit user intents. These intents are expressed in various forms and are difficult to mine and discover, thereby leading to the limited adaptability and lag in platform strategies. This greatly restricts users' ability to express diverse intents and hinders the scalability of the visual search system. This mismatch between user implicit intent expression and system response defines the User-SearchSys Intent Discrepancy. To alleviate the issue, we propose a novel framework REVISION. This framework integrates offline reasoning mining with online decision-making and execution, enabling adaptive strategies to solve implicit user demands. In the offline stage, we construct a periodic pipeline to mine discrepancies from historical no-click requests. Leveraging large models, we analyze implicit intent factors and infer optimal suggestions by jointly reasoning over query and product metadata. These inferred suggestions serve as actionable insights for refining platform strategies. In the online stage, REVISION-R1-3B, trained on the curated offline data, performs holistic analysis over query images and associated historical products to generate optimization plans and adaptively schedule strategies across the search pipeline. Our framework offers a streamlined paradigm for integrating large models with traditional search systems, enabling end-to-end intelligent optimization across information aggregation and user interaction. Experimental results demonstrate that our approach improves the efficiency of implicit intent mining from large-scale search logs and significantly reduces the no-click rate.",
        "translated": "在淘宝电商视觉搜索中，用户行为分析显示存在大量无点击请求，表明用户意图具有多样性且具有隐性特征。这些意图以多种形式表达，难以挖掘和发现，导致平台策略的适应性受限且响应滞后。这极大地限制了用户表达多样化意图的能力，阻碍了视觉搜索系统的扩展性。用户隐性意图表达与系统响应之间的不匹配定义为“用户-搜索系统意图差异”（User-SearchSys Intent Discrepancy）。为缓解该问题，我们提出一种新颖的框架 REVISION。该框架融合离线推理挖掘与在线决策执行，支持自适应策略以解决隐性用户需求。在离线阶段，我们构建周期性流程，从历史无点击请求中挖掘意图差异。借助大语言模型，我们联合推理查询与商品元数据，分析隐性意图因素并推断最优建议。这些推断出的建议作为可操作的洞察，用于优化平台策略。在在线阶段，基于精心整理的离线数据训练得到的 REVISION-R1-3B，对查询图像及关联历史物料进行整体分析，生成优化方案，并在搜索流程中自适应调度策略。本框架为大语言模型与传统搜索系统的集成提供了简洁范式，支持在信息聚合与用户交互中实现端到端智能优化。实验结果表明，我们的方法提升了从大规模搜索日志中挖掘隐性意图的效率，并显著降低了无点击率。",
        "translated_title": "修订：面向电商视觉搜索系统优化的反思意图挖掘与在线推理辅助机制",
        "label": [
            "LLM生成式推荐（LLM-based Generative Recommendation）",
            "序列推荐（Sequential Recommendation）",
            "召回（Recall）"
        ],
        "label_reason": "论文通过LLM挖掘用户隐式意图，辅助电商视觉搜索优化，涉及召回与生成式推荐环节",
        "relevance_score": 8,
        "novelty_score": 8,
        "novelty_reason": "结合离线意图挖掘与在线推理调度，提出端到端优化框架，具有显著创新性"
    },
    {
        "title": "$\\text{E}^2\\text{Rank}$: Your Text Embedding can Also be an Effective\n  and Efficient Listwise Reranker",
        "url": "http://arxiv.org/abs/2510.22733v1",
        "pub_date": "2025-10-26",
        "summary": "Text embedding models serve as a fundamental component in real-world search applications. By mapping queries and documents into a shared embedding space, they deliver competitive retrieval performance with high efficiency. However, their ranking fidelity remains limited compared to dedicated rerankers, especially recent LLM-based listwise rerankers, which capture fine-grained query-document and document-document interactions. In this paper, we propose a simple yet effective unified framework $\\text{E}^2\\text{Rank}$, means Efficient Embedding-based Ranking (also means Embedding-to-Rank), which extends a single text embedding model to perform both high-quality retrieval and listwise reranking through continued training under a listwise ranking objective, thereby achieving strong effectiveness with remarkable efficiency. By applying cosine similarity between the query and document embeddings as a unified ranking function, the listwise ranking prompt, which is constructed from the original query and its candidate documents, serves as an enhanced query enriched with signals from the top-K documents, akin to pseudo-relevance feedback (PRF) in traditional retrieval models. This design preserves the efficiency and representational quality of the base embedding model while significantly improving its reranking performance. Empirically, $\\textrm{E}^2\\text{Rank}$ achieves state-of-the-art results on the BEIR reranking benchmark and demonstrates competitive performance on the reasoning-intensive BRIGHT benchmark, with very low reranking latency. We also show that the ranking training process improves embedding performance on the MTEB benchmark. Our findings indicate that a single embedding model can effectively unify retrieval and reranking, offering both computational efficiency and competitive ranking accuracy.",
        "translated": "文本嵌入模型是实际搜索应用中的基础组件。通过将查询与文档映射到共享的嵌入空间，它们在保证高效率的同时实现了具有竞争力的召回性能。然而，其排序保真度相较于专用的重排模型仍显不足，尤其是近期基于大语言模型（LLM）的列表式重排模型，后者能够捕捉细粒度的查询-文档和文档-文档交互。本文提出了一种简单而有效的统一框架 $\\text{E}^2\\text{Rank}$，即高效嵌入式排序（也称嵌入到排序），该框架通过在列表式排序目标下对单一文本嵌入模型进行持续训练，使其同时具备高质量的召回与列表式重排能力，从而在显著提升效率的同时实现强大的有效性。通过在查询与文档嵌入之间采用余弦相似度作为统一的排序函数，由原始查询及其候选文档构建的列表式排序提示，相当于一个融合了前K个文档信号的增强查询，类似于传统检索模型中的伪相关反馈（PRF）。该设计在保持基础嵌入模型效率与表征质量的同时，显著提升了其重排性能。实验表明，$\\textrm{E}^2\\text{Rank}$ 在 BEIR 重排基准测试中达到了当前最佳水平，在推理密集型的 BRIGHT 基准测试中也展现出具有竞争力的表现，且重排延迟极低。我们还发现，排序训练过程能够提升嵌入模型在 MTEB 基准测试上的性能。我们的研究结果表明，单一嵌入模型能够有效统一召回与重排，兼具计算效率与具有竞争力的排序精度。",
        "translated_title": "$\\text{E}^2\\text{Rank}$：你的文本嵌入也可以成为一种高效且有效的列表式重排模型",
        "label": [
            "重排（Re-ranking）",
            "通用推荐技术（General Recommendation Techniques）"
        ],
        "label_reason": "论文提出将文本嵌入模型扩展为高效重排器，直接应用于推荐系统重排环节，提升排序质量与效率。",
        "relevance_score": 8,
        "novelty_score": 8,
        "novelty_reason": "通过统一框架将嵌入模型用于重排，结合列表排序目标训练，创新性地提升效率与效果，非传统方法。"
    },
    {
        "title": "ATLAS: Actor-Critic Task-Completion with Look-ahead Action Simulation",
        "url": "http://arxiv.org/abs/2510.22732v1",
        "pub_date": "2025-10-26",
        "summary": "We observe that current state-of-the-art web-agents are unable to effectively adapt to new environments without neural network fine-tuning, without which they produce inefficient execution plans due to a lack of awareness of the structure and dynamics of the new environment. To address this limitation, we introduce ATLAS (Actor-Critic Task-completion with Look-ahead Action Simulation), a memory-augmented agent that is able to make plans grounded in a model of the environment by simulating the consequences of those actions in cognitive space. Our agent starts by building a \"cognitive map\" by performing a lightweight curiosity driven exploration of the environment. The planner proposes candidate actions; the simulator predicts their consequences in cognitive space; a critic analyzes the options to select the best roll-out and update the original plan; and a browser executor performs the chosen action. On the WebArena-Lite Benchmark, we achieve a 63% success rate compared to 53.9% success rate for the previously published state-of-the-art. Unlike previous systems, our modular architecture requires no website-specific LLM fine-tuning. Ablations show sizable drops without the world-model, hierarchical planner, and look-ahead-based replanner confirming their complementary roles within the design of our system",
        "translated": "我们观察到，当前最先进的网页智能体在未进行神经网络微调的情况下，无法有效适应新环境，从而因缺乏对新环境结构与动态特性的认知，产生低效的执行计划。为解决这一局限，我们提出 ATLAS（Actor-Critic Task-completion with Look-ahead Action Simulation），一种具备记忆增强能力的智能体，其能够通过在认知空间中模拟动作后果，基于环境模型制定计划。该智能体首先通过轻量级好奇心驱动的探索构建“认知地图”。规划器提出候选动作；模拟器预测这些动作在认知空间中的后果；评判器分析各选项以选择最优执行路径并更新原始计划；浏览器执行器则执行选定的动作。在 WebArena-Lite 基准测试中，我们实现了 63% 的成功率，而此前发表的最先进方法的成功率为 53.9%。与以往系统不同，我们的模块化架构无需针对特定网站进行大语言模型微调。消融实验表明，在缺少世界模型、分层规划器和基于前瞻的重规划器的情况下，性能显著下降，验证了它们在系统设计中的互补作用。",
        "translated_title": "ATLAS：基于前瞻动作模拟的演员-评论家任务完成方法",
        "label": [],
        "label_reason": "论文聚焦于网页智能体任务执行与环境建模，未涉及推荐系统核心环节。",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出模块化智能体架构与前瞻行动模拟，对任务规划有创新性改进。"
    },
    {
        "title": "Windsock is Dancing: Adaptive Multimodal Retrieval-Augmented Generation",
        "url": "http://arxiv.org/abs/2510.22694v1",
        "pub_date": "2025-10-26",
        "summary": "Multimodal Retrieval-Augmented Generation (MRAG) has emerged as a promising method to generate factual and up-to-date responses of Multimodal Large Language Models (MLLMs) by incorporating non-parametric knowledge from external knowledge bases. However, existing MRAG approaches suffer from static retrieval strategies, inflexible modality selection, and suboptimal utilization of retrieved information, leading to three critical challenges: determining when to retrieve, what modality to incorporate, and how to utilize retrieved information effectively. To address these challenges, we introduce Windsock, a query-dependent module making decisions on retrieval necessity and modality selection, effectively reducing computational overhead and improving response quality. Additionally, we propose Dynamic Noise-Resistance (DANCE) Instruction Tuning, an adaptive training strategy that enhances MLLMs' ability to utilize retrieved information while maintaining robustness against noise. Moreover, we adopt a self-assessment approach leveraging knowledge within MLLMs to convert question-answering datasets to MRAG training datasets. Extensive experiments demonstrate that our proposed method significantly improves the generation quality by 17.07% while reducing 8.95% retrieval times.",
        "translated": "多模态检索增强生成（MRAG）作为一种有前景的方法，通过融合外部知识库中的非参数化知识，提升了多模态大语言模型（MLLMs）生成事实性且时效性强的回答的能力。然而，现有MRAG方法受限于静态的检索策略、缺乏灵活性的模态选择机制，以及对检索信息利用不足，导致三个关键挑战：何时进行检索、应引入何种模态，以及如何有效利用检索到的信息。为应对这些挑战，我们提出Windsock，一种依赖于查询的模块，用于决策是否进行检索及选择何种模态，从而有效降低计算开销并提升响应质量。此外，我们提出动态抗噪（DANCE）指令微调，一种自适应训练策略，在增强MLLMs利用检索信息能力的同时，保持其对噪声的鲁棒性。同时，我们采用一种自评估方法，利用MLLMs内部知识将问答数据集转换为MRAG训练数据集。大量实验表明，所提出的方法在生成质量上显著提升17.07%，同时将检索次数减少8.95%。",
        "translated_title": "风袜在舞动：自适应多模态检索增强生成",
        "label": [
            "LLM生成式推荐",
            "多模态推荐",
            "召回"
        ],
        "label_reason": "论文聚焦多模态检索增强生成，涉及LLM生成式推荐中的召回与模态选择，与推荐系统中信息检索环节相关。",
        "relevance_score": 7,
        "novelty_score": 8,
        "novelty_reason": "提出动态噪声抵抗训练与自评估数据转换，改进检索策略与信息利用，具有显著创新性。"
    },
    {
        "title": "Diversification as Risk Minimization",
        "url": "http://arxiv.org/abs/2510.22681v1",
        "pub_date": "2025-10-26",
        "summary": "Users tend to remember failures of a search session more than its many successes. This observation has led to work on search robustness, where systems are penalized if they perform very poorly on some queries. However, this principle of robustness has been overlooked within a single query. An ambiguous or underspecified query (e.g., ``jaguar'') can have several user intents, where popular intents often dominate the ranking, leaving users with minority intents unsatisfied. Although the diversification literature has long recognized this issue, existing metrics only model the average relevance across intents and provide no robustness guarantees. More surprisingly, we show theoretically and empirically that many well-known diversification algorithms are no more robust than a naive, non-diversified algorithm. To address this critical gap, we propose to frame diversification as a risk-minimization problem. We introduce VRisk, which measures the expected risk faced by the least-served fraction of intents in a query. Optimizing VRisk produces a robust ranking, reducing the likelihood of poor user experiences. We then propose VRisker, a fast greedy re-ranker with provable approximation guarantees. Finally, experiments on NTCIR INTENT-2, TREC Web 2012, and MovieLens show the vulnerability of existing methods. VRisker reduces worst-case intent failures by up to 33% with a minimal 2% drop in average performance.",
        "translated": "用户往往对搜索会话中的失败记忆更为深刻，而非其诸多成功。这一观察促使人们开展搜索鲁棒性研究，即当系统在某些查询上表现极差时，会受到惩罚。然而，这种鲁棒性原则在单个查询内部却未被充分重视。一个模糊或定义不足的查询（如“jaguar”）可能对应多个用户意图，其中主流意图通常主导排序结果，导致少数意图的用户需求得不到满足。尽管多样性相关文献早已意识到这一问题，但现有指标仅建模各意图的平均相关性，无法提供任何鲁棒性保障。更令人惊讶的是，我们从理论和实验两方面证明，许多知名的多样性算法在鲁棒性上并不优于朴素的非多样性算法。为弥补这一关键空白，我们提出将多样性建模为一种风险最小化问题。我们引入VRisk，用于衡量查询中服务最差的意图子集所面临的预期风险。优化VRisk可生成鲁棒排序，降低用户糟糕体验的可能性。随后，我们提出VRisker，一种快速贪心重排算法，具有可证明的近似保证。最后，在NTCIR INTENT-2、TREC Web 2012和MovieLens数据集上的实验表明现有方法存在脆弱性，VRisker在平均性能仅下降2%的情况下，将最坏情况下的意图失败率最高降低33%。",
        "translated_title": "多样化作为风险最小化",
        "label": [
            "重排（Re-ranking）",
            "推荐系统评估（Evaluation Metrics / Offline/Online Testing）"
        ],
        "label_reason": "论文聚焦查询内意图多样性优化，提出风险最小化框架与重排算法，直接应用于推荐系统重排环节。",
        "relevance_score": 8,
        "novelty_score": 8,
        "novelty_reason": "提出VRisk新指标与VRisker重排算法，理论与实验双重验证，显著提升重排鲁棒性，属创新性工作。"
    },
    {
        "title": "Tools are under-documented: Simple Document Expansion Boosts Tool\n  Retrieval",
        "url": "http://arxiv.org/abs/2510.22670v1",
        "pub_date": "2025-10-26",
        "summary": "Large Language Models (LLMs) have recently demonstrated strong capabilities in tool use, yet progress in tool retrieval remains hindered by incomplete and heterogeneous tool documentation. To address this challenge, we introduce Tool-DE, a new benchmark and framework that systematically enriches tool documentation with structured fields to enable more effective tool retrieval, together with two dedicated models, Tool-Embed and Tool-Rank. We design a scalable document expansion pipeline that leverages both open- and closed-source LLMs to generate, validate, and refine enriched tool profiles at low cost, producing large-scale corpora with 50k instances for embedding-based retrievers and 200k for rerankers. On top of this data, we develop two models specifically tailored for tool retrieval: Tool-Embed, a dense retriever, and Tool-Rank, an LLM-based reranker. Extensive experiments on ToolRet and Tool-DE demonstrate that document expansion substantially improves retrieval performance, with Tool-Embed and Tool-Rank achieving new state-of-the-art results on both benchmarks. We further analyze the contribution of individual fields to retrieval effectiveness, as well as the broader impact of document expansion on both training and evaluation. Overall, our findings highlight both the promise and limitations of LLM-driven document expansion, positioning Tool-DE, along with the proposed Tool-Embed and Tool-Rank, as a foundation for future research in tool retrieval.",
        "translated": "大语言模型（LLM）近期在工具使用方面展现出强大的能力，然而工具召回的进展仍受限于工具文档的不完整性和异构性。为应对这一挑战，我们提出Tool-DE，一个全新的基准和框架，该框架系统性地通过结构化字段丰富工具文档，以实现更有效的工具召回，并配套提出两个专用模型：Tool-Embed与Tool-Rank。我们设计了一条可扩展的文档扩展流水线，利用开源和闭源大语言模型以低成本生成、验证并优化增强后的工具档案，构建了大规模语料库，包含50k实例用于基于嵌入的召回模型，以及200k实例用于重排模型。在此数据基础上，我们开发了两个专门针对工具召回设计的模型：Tool-Embed（一种稠密召回模型）和Tool-Rank（一种基于大语言模型的重排模型）。在ToolRet和Tool-DE两个基准上的大量实验表明，文档扩展显著提升了召回性能，Tool-Embed和Tool-Rank在两个基准上均取得了新的最先进结果。我们进一步分析了各个结构化字段对召回效果的贡献，以及文档扩展对训练和评估的整体影响。总体而言，我们的研究揭示了基于大语言模型的文档扩展的潜力与局限性，确立了Tool-DE及所提出的Tool-Embed和Tool-Rank作为未来工具召回研究的重要基础。",
        "translated_title": "工具文档不足：简单的文档扩展提升工具召回",
        "label": [
            "召回",
            "重排",
            "LLM生成式推荐"
        ],
        "label_reason": "论文聚焦工具检索中的召回与重排，使用LLM生成文档扩展，提升检索效果，属于推荐系统中信息检索环节的延伸应用。",
        "relevance_score": 7,
        "novelty_score": 8,
        "novelty_reason": "提出基于LLM的文档扩展框架并构建专用模型，对工具检索有显著改进，创新性较强但非推荐系统核心问题。"
    },
    {
        "title": "ATOM: AdapTive and OptiMized dynamic temporal knowledge graph\n  construction using LLMs",
        "url": "http://arxiv.org/abs/2510.22590v1",
        "pub_date": "2025-10-26",
        "summary": "In today's rapidly expanding data landscape, knowledge extraction from unstructured text is vital for real-time analytics, temporal inference, and dynamic memory frameworks. However, traditional static knowledge graph (KG) construction often overlooks the dynamic and time-sensitive nature of real-world data, limiting adaptability to continuous changes. Moreover, recent zero- or few-shot approaches that avoid domain-specific fine-tuning or reliance on prebuilt ontologies often suffer from instability across multiple runs, as well as incomplete coverage of key facts. To address these challenges, we introduce ATOM (AdapTive and OptiMized), a few-shot and scalable approach that builds and continuously updates Temporal Knowledge Graphs (TKGs) from unstructured texts. ATOM splits input documents into minimal, self-contained \"atomic\" facts, improving extraction exhaustivity and stability. Then, it constructs atomic TKGs from these facts while employing a dual-time modeling that distinguishes when information is observed from when it is valid. The resulting atomic TKGs are subsequently merged in parallel. Empirical evaluations demonstrate that ATOM achieves ~18% higher exhaustivity, ~17% better stability, and over 90% latency reduction compared to baseline methods, demonstrating a strong scalability potential for dynamic TKG construction.",
        "translated": "在当今快速增长的数据环境中，从非结构化文本中提取知识对于实时分析、时序推理和动态记忆框架至关重要。然而，传统的静态知识图谱（KG）构建方法通常忽视了现实世界数据的动态性和时效性，限制了其对持续变化的适应能力。此外，近期提出的零样本或少样本方法，虽避免了领域特定微调或依赖预构建本体，但往往在多次运行中表现出不稳定性，并且对关键事实的覆盖不完整。为应对这些挑战，我们提出 ATOM（AdapTive and OptiMized），一种少样本且可扩展的方法，用于从非结构化文本中构建并持续更新时序知识图谱（TKGs）。ATOM 将输入文档分解为最小、自包含的“原子”事实，从而提升提取的完备性和稳定性。随后，它基于这些原子事实构建原子 TKG，并采用双时间建模机制，区分信息被观测到的时间与信息有效的时间。最终，这些原子 TKG 以并行方式合并。实证评估表明，与基线方法相比，ATOM 的提取完备性提高约 18%，稳定性提升约 17%，延迟降低超过 90%，展现出在动态 TKG 构建中强大的可扩展性潜力。",
        "translated_title": "ATOM：基于大语言模型的自适应与优化动态时序知识图谱构建",
        "label": [
            "多模态推荐",
            "负采样与对比学习"
        ],
        "label_reason": "论文聚焦动态知识图谱构建，与推荐系统间接相关，可应用于多模态推荐中的知识增强",
        "relevance_score": 4,
        "novelty_score": 7,
        "novelty_reason": "提出原子化事实提取与双时间建模，提升稳定性与覆盖度，为知识图谱构建提供新方法"
    },
    {
        "title": "Open Multimodal Retrieval-Augmented Factual Image Generation",
        "url": "http://arxiv.org/abs/2510.22521v1",
        "pub_date": "2025-10-26",
        "summary": "Large Multimodal Models (LMMs) have achieved remarkable progress in generating photorealistic and prompt-aligned images, but they often produce outputs that contradict verifiable knowledge, especially when prompts involve fine-grained attributes or time-sensitive events. Conventional retrieval-augmented approaches attempt to address this issue by introducing external information, yet they are fundamentally incapable of grounding generation in accurate and evolving knowledge due to their reliance on static sources and shallow evidence integration. To bridge this gap, we introduce ORIG, an agentic open multimodal retrieval-augmented framework for Factual Image Generation (FIG), a new task that requires both visual realism and factual grounding. ORIG iteratively retrieves and filters multimodal evidence from the web and incrementally integrates the refined knowledge into enriched prompts to guide generation. To support systematic evaluation, we build FIG-Eval, a benchmark spanning ten categories across perceptual, compositional, and temporal dimensions. Experiments demonstrate that ORIG substantially improves factual consistency and overall image quality over strong baselines, highlighting the potential of open multimodal retrieval for factual image generation.",
        "translated": "大语言多模态模型（LMMs）在生成逼真且与提示对齐的图像方面取得了显著进展，但它们常常生成与可验证知识相矛盾的结果，尤其是在提示涉及细粒度属性或时间敏感事件时。传统的检索增强方法试图通过引入外部信息来解决这一问题，但由于其依赖静态信息源和浅层证据整合，本质上无法将生成过程扎根于准确且不断演化的知识体系。为弥合这一差距，我们提出ORIG，一个面向事实图像生成（FIG）任务的代理式开放多模态检索增强框架。该任务要求图像在具备视觉真实感的同时，还需具备事实性支撑。ORIG通过迭代地从网络中检索并过滤多模态证据，逐步将精炼后的知识整合进增强的提示中，以引导图像生成。为支持系统性评估，我们构建了FIG-Eval基准，涵盖感知、组合和时间三个维度的十个类别。实验表明，ORIG在事实一致性与整体图像质量方面显著优于强基线方法，凸显了开放多模态检索在事实图像生成中的潜力。",
        "translated_title": "开放多模态检索增强事实性图像生成",
        "label": [],
        "label_reason": "论文聚焦于多模态图像生成与事实一致性，与推荐系统无直接关联。",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出开放多模态检索增强框架，改进事实一致性，方法新颖但非推荐领域。"
    },
    {
        "title": "FAIR-RAG: Faithful Adaptive Iterative Refinement for Retrieval-Augmented\n  Generation",
        "url": "http://arxiv.org/abs/2510.22344v1",
        "pub_date": "2025-10-25",
        "summary": "While Retrieval-Augmented Generation (RAG) mitigates hallucination and knowledge staleness in Large Language Models (LLMs), existing frameworks often falter on complex, multi-hop queries that require synthesizing information from disparate sources. Current advanced RAG methods, employing iterative or adaptive strategies, lack a robust mechanism to systematically identify and fill evidence gaps, often propagating noise or failing to gather a comprehensive context. We introduce FAIR-RAG, a novel agentic framework that transforms the standard RAG pipeline into a dynamic, evidence-driven reasoning process. At its core is an Iterative Refinement Cycle governed by a module we term Structured Evidence Assessment (SEA). The SEA acts as an analytical gating mechanism: it deconstructs the initial query into a checklist of required findings and audits the aggregated evidence to identify confirmed facts and, critically, explicit informational gaps. These gaps provide a precise signal to an Adaptive Query Refinement agent, which generates new, targeted sub-queries to retrieve missing information. This cycle repeats until the evidence is verified as sufficient, ensuring a comprehensive context for a final, strictly faithful generation. We conducted experiments on challenging multi-hop QA benchmarks, including HotpotQA, 2WikiMultiHopQA, and MusiQue. In a unified experimental setup, FAIR-RAG significantly outperforms strong baselines. On HotpotQA, it achieves an F1-score of 0.453 -- an absolute improvement of 8.3 points over the strongest iterative baseline -- establishing a new state-of-the-art for this class of methods on these benchmarks. Our work demonstrates that a structured, evidence-driven refinement process with explicit gap analysis is crucial for unlocking reliable and accurate reasoning in advanced RAG systems for complex, knowledge-intensive tasks.",
        "translated": "尽管检索增强生成（RAG）缓解了大语言模型（LLM）中的幻觉和知识陈旧问题，但现有框架在处理需要从多个来源综合信息的复杂多跳查询时往往表现不佳。当前先进的RAG方法，采用迭代或自适应策略，缺乏一种稳健机制来系统性地识别和填补证据空白，常常传播噪声或无法获取全面的上下文。我们提出FAIR-RAG，一种新颖的智能体框架，将标准RAG流程转化为动态、以证据驱动的推理过程。其核心是一个由我们称为结构化证据评估（SEA）模块所控制的迭代优化循环。SEA作为分析性门控机制：它将初始查询解构为所需发现的清单，并审核聚合后的证据，以识别已确认的事实，以及关键的、明确的信息空白。这些空白为自适应查询优化智能体提供精确信号，该智能体生成新的、有针对性的子查询以检索缺失信息。此循环持续进行，直至证据被验证为充分，从而确保最终生成阶段具备全面的上下文，且严格忠实于事实。我们在具有挑战性的多跳问答基准测试集上进行了实验，包括HotpotQA、2WikiMultiHopQA和MusiQue。在统一的实验设置下，FAIR-RAG显著超越了强基线方法。在HotpotQA上，其F1得分达到0.453——比最强的迭代基线方法绝对提升8.3个百分点——在这些基准测试中确立了该类方法的新state-of-the-art。我们的工作表明，具有显式空白分析的结构化、以证据驱动的优化过程，对于在复杂、知识密集型任务中实现先进RAG系统的可靠且准确的推理至关重要。",
        "translated_title": "FAIR-RAG：面向检索增强生成的忠实自适应迭代优化方法",
        "label": [],
        "label_reason": "论文聚焦RAG框架的迭代优化，用于多跳问答，非推荐系统核心环节",
        "relevance_score": 3,
        "novelty_score": 8,
        "novelty_reason": "提出结构化证据评估与自适应查询优化机制，显著提升复杂问答性能"
    },
    {
        "title": "PatenTEB: A Comprehensive Benchmark and Model Family for Patent Text\n  Embedding",
        "url": "http://arxiv.org/abs/2510.22264v1",
        "pub_date": "2025-10-25",
        "summary": "Patent text embeddings enable prior art search, technology landscaping, and patent analysis, yet existing benchmarks inadequately capture patent-specific challenges. We introduce PatenTEB, a comprehensive benchmark comprising 15 tasks across retrieval, classification, paraphrase, and clustering, with 2.06 million examples. PatenTEB employs domain-stratified splits, domain specific hard negative mining, and systematic coverage of asymmetric fragment-to-document matching scenarios absent from general embedding benchmarks. We develop the patembed model family through multi-task training, spanning 67M to 344M parameters with context lengths up to 4096 tokens. External validation shows strong generalization: patembed-base achieves state-of-the-art on MTEB BigPatentClustering.v2 (0.494 V-measure vs. 0.445 previous best), while patembed-large achieves 0.377 NDCG@100 on DAPFAM. Systematic ablations reveal that multi-task training improves external generalization despite minor benchmark costs, and that domain-pretrained initialization provides consistent advantages across task families. All resources will be made available at https://github.com/iliass-y/patenteb. Keywords: patent retrieval, sentence embeddings, multi-task learning, asymmetric retrieval, benchmark evaluation, contrastive learning.",
        "translated": "专利文本嵌入能够支持现有技术检索、技术布局分析以及专利分析，然而现有的基准测试未能充分捕捉专利领域特有的挑战。我们提出 PatenTEB，一个综合性基准测试，包含检索、分类、改写和聚类等15项任务，共涵盖206万例样本。PatenTEB采用领域分层划分、领域特定的难负例挖掘，并系统性覆盖了通用嵌入基准中缺失的非对称片段-文档匹配场景。我们通过多任务训练开发了 patembed 模型系列，参数规模从67M到344M不等，上下文长度最高达4096个token。外部验证表明其具有良好的泛化能力：patembed-base 在 MTEB BigPatentClustering.v2 上达到最新性能（0.494 V-measure，优于此前最佳的0.445），而 patembed-large 在 DAPFAM 上取得0.377的 NDCG@100。系统性消融实验表明，尽管在基准测试中成本略有增加，多任务训练仍能提升外部泛化性能；同时，领域预训练初始化在各类任务中均表现出持续优势。所有资源将公开于 https://github.com/iliass-y/patenteb。关键词：专利检索、句子嵌入、多任务学习、非对称检索、基准评估、对比学习。",
        "translated_title": "PatenTEB：面向专利文本嵌入的综合基准与模型家族",
        "label": [
            "负采样与对比学习",
            "推荐系统评估"
        ],
        "label_reason": "论文聚焦专利文本嵌入与检索，与推荐系统无直接关联，但涉及对比学习与评估方法",
        "relevance_score": 3,
        "novelty_score": 7,
        "novelty_reason": "提出多任务训练与领域特定硬负样本挖掘，方法新颖且有效"
    },
    {
        "title": "Concerto: Joint 2D-3D Self-Supervised Learning Emerges Spatial\n  Representations",
        "url": "http://arxiv.org/abs/2510.23607v1",
        "pub_date": "2025-10-27",
        "summary": "Humans learn abstract concepts through multisensory synergy, and once formed, such representations can often be recalled from a single modality. Inspired by this principle, we introduce Concerto, a minimalist simulation of human concept learning for spatial cognition, combining 3D intra-modal self-distillation with 2D-3D cross-modal joint embedding. Despite its simplicity, Concerto learns more coherent and informative spatial features, as demonstrated by zero-shot visualizations. It outperforms both standalone SOTA 2D and 3D self-supervised models by 14.2% and 4.8%, respectively, as well as their feature concatenation, in linear probing for 3D scene perception. With full fine-tuning, Concerto sets new SOTA results across multiple scene understanding benchmarks (e.g., 80.7% mIoU on ScanNet). We further present a variant of Concerto tailored for video-lifted point cloud spatial understanding, and a translator that linearly projects Concerto representations into CLIP's language space, enabling open-world perception. These results highlight that Concerto emerges spatial representations with superior fine-grained geometric and semantic consistency.",
        "translated": "人类通过多感官协同学习抽象概念，一旦形成，这些表征通常可仅通过单一模态被激活回忆。受此原理启发，我们提出 Concerto，一种用于空间认知的人类概念学习的极简模拟，其结合了3D模态内自蒸馏与2D-3D跨模态联合嵌入。尽管结构简洁，Concerto 仍能学习到更具连贯性和信息量的空间特征，这一点通过零样本可视化得到验证。在3D场景感知的线性探测任务中，Concerto 相较于独立的最先进2D和3D自监督模型分别提升14.2%和4.8%，并优于其特征拼接结果。在全参数微调设置下，Concerto 在多个场景理解基准上均取得新的最先进结果（例如，在ScanNet上达到80.7%的mIoU）。我们进一步提出一种适用于视频提升点云空间理解的Concerto变体，以及一个线性投影器，可将Concerto表征映射至CLIP的语言空间，从而实现开放世界感知。这些结果表明，Concerto 所生成的空间表征具备更优越的细粒度几何与语义一致性。",
        "translated_title": "Concerto：联合2D-3D自监督学习生成空间表征",
        "label": [],
        "label_reason": "论文聚焦3D场景理解与跨模态表征学习，属高阶视觉任务，不涉及像素级图像恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出2D-3D联合自监督框架，新颖性强，但应用于场景理解而非low-level处理。"
    },
    {
        "title": "Track, Inpaint, Resplat: Subject-driven 3D and 4D Generation with\n  Progressive Texture Infilling",
        "url": "http://arxiv.org/abs/2510.23605v1",
        "pub_date": "2025-10-27",
        "summary": "Current 3D/4D generation methods are usually optimized for photorealism, efficiency, and aesthetics. However, they often fail to preserve the semantic identity of the subject across different viewpoints. Adapting generation methods with one or few images of a specific subject (also known as Personalization or Subject-driven generation) allows generating visual content that align with the identity of the subject. However, personalized 3D/4D generation is still largely underexplored. In this work, we introduce TIRE (Track, Inpaint, REsplat), a novel method for subject-driven 3D/4D generation. It takes an initial 3D asset produced by an existing 3D generative model as input and uses video tracking to identify the regions that need to be modified. Then, we adopt a subject-driven 2D inpainting model for progressively infilling the identified regions. Finally, we resplat the modified 2D multi-view observations back to 3D while still maintaining consistency. Extensive experiments demonstrate that our approach significantly improves identity preservation in 3D/4D generation compared to state-of-the-art methods. Our project website is available at https://zsh2000.github.io/track-inpaint-resplat.github.io/.",
        "translated": "当前的3D/4D生成方法通常针对照片真实感、效率和美学效果进行优化。然而，它们往往难以在不同视角下保持主体的语义身份一致性。通过使用特定主体的一张或少数几张图像来适配生成方法（也称为个性化或主体驱动生成），可以生成与主体身份一致的视觉内容。然而，个性化3D/4D生成仍处于被广泛探索的早期阶段。在本文中，我们提出TIRE（Track, Inpaint, REsplat），一种新颖的主体驱动3D/4D生成方法。该方法以现有3D生成模型生成的初始3D资产作为输入，利用视频跟踪技术识别需要修改的区域。随后，我们采用一种主体驱动的2D图像修复模型，逐步对识别出的区域进行填充。最后，将修改后的2D多视角观测结果重新投影回3D空间，同时保持一致性。大量实验表明，与现有最先进方法相比，我们的方法显著提升了3D/4D生成中的身份保持能力。我们的项目网站位于 https://zsh2000.github.io/track-inpaint-resplat.github.io/。",
        "translated_title": "Track, Inpaint, Resplat：基于主体驱动的3D与4D生成及其渐进式纹理填充",
        "label": [],
        "label_reason": "论文聚焦3D/4D生成与主体身份保持，属于high-level生成任务，非像素级图像恢复。",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出渐进式纹理填充流程，结合跟踪与修复，但属生成领域常规改进。"
    },
    {
        "title": "PixelRefer: A Unified Framework for Spatio-Temporal Object Referring\n  with Arbitrary Granularity",
        "url": "http://arxiv.org/abs/2510.23603v1",
        "pub_date": "2025-10-27",
        "summary": "Multimodal large language models (MLLMs) have demonstrated strong general-purpose capabilities in open-world visual comprehension. However, most existing MLLMs primarily focus on holistic, scene-level understanding, often overlooking the need for fine-grained, object-centric reasoning. In this paper, we present PixelRefer, a unified region-level MLLM framework that enables advanced fine-grained understanding over user-specified regions across both images and videos. Motivated by the observation that LLM attention predominantly focuses on object-level tokens, we propose a Scale-Adaptive Object Tokenizer (SAOT) to generate compact and semantically rich object representations from free-form regions. Our analysis reveals that global visual tokens contribute mainly in early LLM layers, inspiring the design of PixelRefer-Lite, an efficient variant that employs an Object-Centric Infusion module to pre-fuse global context into object tokens. This yields a lightweight Object-Only Framework that substantially reduces computational cost while maintaining high semantic fidelity. To facilitate fine-grained instruction tuning, we curate PixelRefer-2.2M, a high-quality object-centric instruction dataset. Extensive experiments across a range of benchmarks validate that PixelRefer achieves leading performance with fewer training samples, while PixelRefer-Lite offers competitive accuracy with notable gains in efficiency.",
        "translated": "多模态大语言模型（MLLMs）在开放世界视觉理解任务中展现出强大的通用能力。然而，现有大多数MLLMs主要关注整体、场景级别的理解，往往忽视了对细粒度、以对象为中心的推理需求。本文提出PixelRefer，一种统一的区域级MLLM框架，能够在用户指定的图像和视频区域上实现先进的细粒度理解。受大语言模型（LLM）注意力机制主要聚焦于对象级token的启发，我们设计了尺度自适应对象编码器（SAOT），用于从任意形状区域生成紧凑且语义丰富的对象表示。我们的分析表明，全局视觉token主要在LLM的早期层发挥作用，由此启发我们设计了PixelRefer-Lite这一高效变体，其采用对象中心注入模块（Object-Centric Infusion module）将全局上下文预先融合到对象token中，从而构建轻量化的纯对象框架，在显著降低计算成本的同时保持高语义保真度。为支持细粒度指令微调，我们构建了PixelRefer-2.2M，一个高质量的对象中心指令数据集。在多个基准数据集上的广泛实验验证了PixelRefer在较少训练样本下即可达到领先性能，而PixelRefer-Lite在保持竞争力精度的同时，实现了显著的效率提升。",
        "translated_title": "PixelRefer: 一种用于时空对象指代的统一框架，支持任意粒度",
        "label": [],
        "label_reason": "论文聚焦于多模态大模型的区域级视觉理解，属于高层语义理解任务，非像素级图像恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 6,
        "novelty_reason": "提出自适应对象标记器和轻量框架，属方法改进，但未涉及低层图像处理核心创新。"
    },
    {
        "title": "PRISM-Bench: A Benchmark of Puzzle-Based Visual Tasks with CoT Error\n  Detection",
        "url": "http://arxiv.org/abs/2510.23594v1",
        "pub_date": "2025-10-27",
        "summary": "We introduce \\textbf{PRISM-Bench}, a benchmark of puzzle-based visual challenges designed to evaluate not only whether models can solve problems, but how their reasoning unfolds. Unlike prior evaluations that measure only final-answer accuracy, PRISM-Bench introduces a diagnostic task: given a visual puzzle and a step-by-step chain-of-thought (CoT) containing exactly one error, models must identify the first incorrect step. This setting enables fine-grained assessment of logical consistency, error detection, and visual reasoning. The puzzles in PRISM-Bench require multi-step symbolic, geometric, and analogical reasoning, resisting shortcuts based on superficial pattern matching. Evaluations across state-of-the-art MLLMs reveal a persistent gap between fluent generation and faithful reasoning: models that produce plausible CoTs often fail to locate simple logical faults. By disentangling answer generation from reasoning verification, PRISM-Bench offers a sharper lens on multimodal reasoning competence and underscores the need for diagnostic evaluation protocols in the development of trustworthy MLLMs.",
        "translated": "我们提出**PRISM-Bench**，一个基于拼图式视觉挑战的基准测试，旨在评估模型不仅能否解决问题，更关注其推理过程如何展开。与以往仅衡量最终答案准确率的评估方式不同，PRISM-Bench 引入了一项诊断任务：给定一个视觉拼图和一条包含恰好一个错误的逐步思维链（CoT），模型需识别出第一个错误步骤。该设置能够对逻辑一致性、错误检测能力以及视觉推理能力进行细粒度评估。PRISM-Bench 中的拼图需要多步骤的符号化、几何化和类比推理，能够有效抵制基于表面模式匹配的捷径策略。在当前最先进的多模态大语言模型（MLLMs）上的评估结果表明，流畅生成与忠实推理之间存在持续的差距：即使模型能够生成看似合理的思维链，往往仍无法识别出简单的逻辑错误。通过将答案生成与推理验证解耦，PRISM-Bench 为多模态推理能力提供了更精准的评估视角，并强调了在可信 MLLMs 开发过程中引入诊断性评估协议的必要性。",
        "translated_title": "PRISM-Bench：一个基于拼图的视觉任务基准，支持思维链错误检测",
        "label": [],
        "label_reason": "论文聚焦多模态大模型的视觉推理与错误检测，属于高阶视觉理解任务，非像素级图像处理。",
        "relevance_score": 1,
        "novelty_score": 6,
        "novelty_reason": "提出基于谜题的推理错误检测基准，方法新颖但未涉及图像恢复或增强技术。"
    },
    {
        "title": "InFlux: A Benchmark for Self-Calibration of Dynamic Intrinsics of Video\n  Cameras",
        "url": "http://arxiv.org/abs/2510.23589v1",
        "pub_date": "2025-10-27",
        "summary": "Accurately tracking camera intrinsics is crucial for achieving 3D understanding from 2D video. However, most 3D algorithms assume that camera intrinsics stay constant throughout a video, which is often not true for many real-world in-the-wild videos. A major obstacle in this field is a lack of dynamic camera intrinsics benchmarks--existing benchmarks typically offer limited diversity in scene content and intrinsics variation, and none provide per-frame intrinsic changes for consecutive video frames. In this paper, we present Intrinsics in Flux (InFlux), a real-world benchmark that provides per-frame ground truth intrinsics annotations for videos with dynamic intrinsics. Compared to prior benchmarks, InFlux captures a wider range of intrinsic variations and scene diversity, featuring 143K+ annotated frames from 386 high-resolution indoor and outdoor videos with dynamic camera intrinsics. To ensure accurate per-frame intrinsics, we build a comprehensive lookup table of calibration experiments and extend the Kalibr toolbox to improve its accuracy and robustness. Using our benchmark, we evaluate existing baseline methods for predicting camera intrinsics and find that most struggle to achieve accurate predictions on videos with dynamic intrinsics. For the dataset, code, videos, and submission, please visit https://influx.cs.princeton.edu/.",
        "translated": "准确追踪相机内参对于从2D视频实现3D理解至关重要。然而，大多数3D算法假设相机内参在整个视频序列中保持恒定，这在许多现实世界中的野外视频中并不成立。该领域的一个主要障碍是缺乏动态相机内参基准数据集——现有基准数据集通常在场景内容和内参变化方面多样性有限，且均未提供连续视频帧的逐帧内参变化标注。本文提出了Intrinsics in Flux（InFlux），一个真实世界的基准数据集，为具有动态内参的视频提供逐帧真实内参标注。与先前基准相比，InFlux涵盖了更广泛的内参变化范围和场景多样性，包含来自386个高分辨率室内和室外视频的143K+个标注帧，这些视频均具有动态相机内参。为确保逐帧内参的准确性，我们构建了一个全面的标定实验查找表，并扩展了Kalibr工具箱以提升其精度和鲁棒性。利用我们的基准数据集，我们评估了现有预测相机内参的基线方法，发现大多数方法在具有动态内参的视频上难以实现准确预测。有关数据集、代码、视频及提交，请访问 https://influx.cs.princeton.edu/。",
        "translated_title": "InFlux：一种用于视频相机动态内部参数自校准的基准数据集",
        "label": [],
        "label_reason": "论文聚焦相机内参动态变化的3D理解，属于高阶视觉任务，非像素级图像恢复或增强。",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出新基准数据集InFlux，对动态内参建模有实用价值，但方法非low-level图像处理创新。"
    },
    {
        "title": "FARMER: Flow AutoRegressive Transformer over Pixels",
        "url": "http://arxiv.org/abs/2510.23588v1",
        "pub_date": "2025-10-27",
        "summary": "Directly modeling the explicit likelihood of the raw data distribution is key topic in the machine learning area, which achieves the scaling successes in Large Language Models by autoregressive modeling. However, continuous AR modeling over visual pixel data suffer from extremely long sequences and high-dimensional spaces. In this paper, we present FARMER, a novel end-to-end generative framework that unifies Normalizing Flows (NF) and Autoregressive (AR) models for tractable likelihood estimation and high-quality image synthesis directly from raw pixels. FARMER employs an invertible autoregressive flow to transform images into latent sequences, whose distribution is modeled implicitly by an autoregressive model. To address the redundancy and complexity in pixel-level modeling, we propose a self-supervised dimension reduction scheme that partitions NF latent channels into informative and redundant groups, enabling more effective and efficient AR modeling. Furthermore, we design a one-step distillation scheme to significantly accelerate inference speed and introduce a resampling-based classifier-free guidance algorithm to boost image generation quality. Extensive experiments demonstrate that FARMER achieves competitive performance compared to existing pixel-based generative models while providing exact likelihoods and scalable training.",
        "translated": "直接建模原始数据分布的显式似然性是机器学习领域的一个关键课题，通过自回归建模已在大型语言模型中取得显著成功。然而，对视觉像素数据进行连续自回归建模会面临序列过长和高维空间带来的挑战。本文提出 FARMER，一种新颖的端到端生成框架，将归一化流（Normalizing Flows, NF）与自回归（Autoregressive, AR）模型相结合，实现对原始像素数据的可计算似然估计和高质量图像合成。FARMER 采用可逆的自回归流将图像转换为潜在序列，其分布由自回归模型隐式建模。为应对像素级建模中的冗余性和复杂性，我们提出一种自监督降维方案，将 NF 的潜在通道划分为信息性与冗余性组，从而实现更高效、更有效的 AR 建模。此外，我们设计了一种单步蒸馏方案以显著加速推理速度，并引入基于重采样的无分类器引导算法以提升图像生成质量。大量实验表明，FARMER 在与现有基于像素的生成模型对比中表现出竞争力，同时提供精确的似然值和可扩展的训练能力。",
        "translated_title": "FARMER：基于像素的流自回归Transformer",
        "label": [],
        "label_reason": "论文聚焦图像生成而非恢复，属high-level任务，不处理像素质量退化问题。",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出NF与AR结合框架，含自监督降维与蒸馏加速，对生成模型有改进。"
    },
    {
        "title": "Lookahead Anchoring: Preserving Character Identity in Audio-Driven Human\n  Animation",
        "url": "http://arxiv.org/abs/2510.23581v1",
        "pub_date": "2025-10-27",
        "summary": "Audio-driven human animation models often suffer from identity drift during temporal autoregressive generation, where characters gradually lose their identity over time. One solution is to generate keyframes as intermediate temporal anchors that prevent degradation, but this requires an additional keyframe generation stage and can restrict natural motion dynamics. To address this, we propose Lookahead Anchoring, which leverages keyframes from future timesteps ahead of the current generation window, rather than within it. This transforms keyframes from fixed boundaries into directional beacons: the model continuously pursues these future anchors while responding to immediate audio cues, maintaining consistent identity through persistent guidance. This also enables self-keyframing, where the reference image serves as the lookahead target, eliminating the need for keyframe generation entirely. We find that the temporal lookahead distance naturally controls the balance between expressivity and consistency: larger distances allow for greater motion freedom, while smaller ones strengthen identity adherence. When applied to three recent human animation models, Lookahead Anchoring achieves superior lip synchronization, identity preservation, and visual quality, demonstrating improved temporal conditioning across several different architectures. Video results are available at the following link: https://lookahead-anchoring.github.io.",
        "translated": "音频驱动的人体动画模型在进行时序自回归生成时，常出现身份漂移问题，即角色随时间推移逐渐丧失其身份特征。一种解决方案是生成关键帧作为中间时序锚点以防止退化，但该方法需要额外的关键帧生成阶段，且可能限制运动的自然动态。为此，我们提出“前瞻锚定”（Lookahead Anchoring）方法，该方法利用当前生成窗口之后未来时间步的关键帧，而非窗口内部的关键帧。这使关键帧从固定的边界转变为具有方向性的引导信标：模型在响应即时音频线索的同时，持续追寻这些未来的锚点，通过持续的引导保持身份一致性。此外，该方法还支持自关键帧机制，即参考图像本身作为前瞻目标，从而完全省去关键帧生成步骤。我们发现，时序前瞻距离自然地控制了表现力与一致性的平衡：较大的距离允许更大的运动自由度，而较小的距离则增强身份保持能力。当应用于三种近期的人体动画模型时，前瞻锚定在唇形同步、身份保持和视觉质量方面均取得更优表现，证明其在多种不同架构中提升了时序条件建模能力。视频结果可访问以下链接：https://lookahead-anchoring.github.io。",
        "translated_title": "前瞻锚定：在音频驱动的人体动画中保持角色身份",
        "label": [],
        "label_reason": "论文聚焦音频驱动的人体动画，属于高阶视觉生成任务，不涉及图像像素级恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出前瞻锚定机制，改进时序条件生成，对动画一致性有显著提升，属方法创新。"
    },
    {
        "title": "UrbanVLA: A Vision-Language-Action Model for Urban Micromobility",
        "url": "http://arxiv.org/abs/2510.23576v1",
        "pub_date": "2025-10-27",
        "summary": "Urban micromobility applications, such as delivery robots, demand reliable navigation across large-scale urban environments while following long-horizon route instructions. This task is particularly challenging due to the dynamic and unstructured nature of real-world city areas, yet most existing navigation methods remain tailored to short-scale and controllable scenarios. Effective urban micromobility requires two complementary levels of navigation skills: low-level capabilities such as point-goal reaching and obstacle avoidance, and high-level capabilities, such as route-visual alignment. To this end, we propose UrbanVLA, a route-conditioned Vision-Language-Action (VLA) framework designed for scalable urban navigation. Our method explicitly aligns noisy route waypoints with visual observations during execution, and subsequently plans trajectories to drive the robot. To enable UrbanVLA to master both levels of navigation, we employ a two-stage training pipeline. The process begins with Supervised Fine-Tuning (SFT) using simulated environments and trajectories parsed from web videos. This is followed by Reinforcement Fine-Tuning (RFT) on a mixture of simulation and real-world data, which enhances the model's safety and adaptability in real-world settings. Experiments demonstrate that UrbanVLA surpasses strong baselines by more than 55% in the SocialNav task on MetaUrban. Furthermore, UrbanVLA achieves reliable real-world navigation, showcasing both scalability to large-scale urban environments and robustness against real-world uncertainties.",
        "translated": "城市微出行应用，如配送机器人，需要在大规模城市环境中可靠地导航，并遵循长时程的路径指令。由于现实城市区域具有动态性和非结构化特点，该任务极具挑战性，而现有大多数导航方法仍局限于小尺度且可控的场景。有效的城市微出行需要两个互补层次的导航能力：低层次能力，如点目标到达和障碍物规避；以及高层次能力，如路径-视觉对齐。为此，我们提出 UrbanVLA，一种以路径为条件的视觉-语言-动作（Vision-Language-Action, VLA）框架，专为可扩展的城市导航而设计。我们的方法在执行过程中显式地将噪声路径路点与视觉观测进行对齐，并随后规划轨迹以驱动机器人。为使 UrbanVLA 掌握上述两个层次的导航能力，我们采用两阶段训练流程。第一阶段使用模拟环境和从网络视频中解析出的轨迹进行监督微调（Supervised Fine-Tuning, SFT）；第二阶段在模拟与真实世界数据混合的数据集上进行强化微调（Reinforcement Fine-Tuning, RFT），从而提升模型在真实环境中的安全性与适应性。实验结果表明，UrbanVLA 在 MetaUrban 平台上的 SocialNav 任务中，相较强基线方法性能提升超过 55%。此外，UrbanVLA 在真实世界导航中表现出色，展现了对大规模城市环境的可扩展性以及对真实世界不确定性的鲁棒性。",
        "translated_title": "UrbanVLA：一种用于城市微出行的视觉-语言-行动模型",
        "label": [],
        "label_reason": "论文聚焦城市微出行导航，属于高阶视觉-语言-动作任务，非像素级图像处理。",
        "relevance_score": 1,
        "novelty_score": 6,
        "novelty_reason": "提出两阶段训练框架，结合模拟与真实数据，对导航任务有改进但非低层图像处理创新。"
    },
    {
        "title": "More Than Generation: Unifying Generation and Depth Estimation via\n  Text-to-Image Diffusion Models",
        "url": "http://arxiv.org/abs/2510.23574v1",
        "pub_date": "2025-10-27",
        "summary": "Generative depth estimation methods leverage the rich visual priors stored in pre-trained text-to-image diffusion models, demonstrating astonishing zero-shot capability. However, parameter updates during training lead to catastrophic degra- dation in the image generation capability of the pre-trained model. We introduce MERGE, a unified model for image generation and depth estimation, starting from a fixed pre-trained text-to-image model. MERGE demonstrates that the pre-trained text-to-image model can do more than image generation, but also expand to depth estimation effortlessly. Specifically, MERGE introduces a play- and-plug framework that enables seamless switching between image generation and depth estimation modes through simple and pluggable converters. Meanwhile, we propose a Group Reuse Mechanism to encourage parameter reuse and im- prove the utilization of the additional learnable parameters. MERGE unleashes the powerful depth estimation capability of the pre-trained text-to-image model while preserving its original image generation ability. Compared to other unified models for image generation and depth estimation, MERGE achieves state-of- the-art performance across multiple depth estimation benchmarks. The code will be made available at https://github.com/H-EmbodVis/MERGE",
        "translated": "生成式深度估计方法利用预训练文本到图像扩散模型中存储的丰富视觉先验，展现出惊人的零样本能力。然而，在训练过程中进行参数更新会导致预训练模型的图像生成能力发生灾难性退化。我们提出MERGE，一种从固定预训练文本到图像模型出发的图像生成与深度估计统一模型。MERGE表明，预训练文本到图像模型不仅能进行图像生成，还可轻松扩展至深度估计任务。具体而言，MERGE引入了一种即插即用框架，通过简单且可插拔的转换器，实现图像生成与深度估计模式之间的无缝切换。同时，我们提出一种组重用机制，以促进参数重用并提升额外可学习参数的利用率。MERGE在保持原始图像生成能力的同时，充分释放了预训练文本到图像模型强大的深度估计能力。与其它图像生成与深度估计统一模型相比，MERGE在多个深度估计基准数据集上均达到当前最优性能。代码将在https://github.com/H-EmbodVis/MERGE公开。",
        "translated_title": "超越生成：通过文本到图像扩散模型统一生成与深度估计",
        "label": [],
        "label_reason": "论文聚焦图像生成与深度估计统一模型，属于高阶视觉任务，非像素级图像恢复。",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出可插拔框架与参数复用机制，对多任务统一模型有显著改进。"
    },
    {
        "title": "RobotArena $\\infty$: Scalable Robot Benchmarking via Real-to-Sim\n  Translation",
        "url": "http://arxiv.org/abs/2510.23571v1",
        "pub_date": "2025-10-27",
        "summary": "The pursuit of robot generalists - instructable agents capable of performing diverse tasks across diverse environments - demands rigorous and scalable evaluation. Yet real-world testing of robot policies remains fundamentally constrained: it is labor-intensive, slow, unsafe at scale, and difficult to reproduce. Existing simulation benchmarks are similarly limited, as they train and test policies within the same synthetic domains and cannot assess models trained from real-world demonstrations or alternative simulation environments. As policies expand in scope and complexity, these barriers only intensify, since defining \"success\" in robotics often hinges on nuanced human judgments of execution quality. In this paper, we introduce a new benchmarking framework that overcomes these challenges by shifting VLA evaluation into large-scale simulated environments augmented with online human feedback. Leveraging advances in vision-language models, 2D-to-3D generative modeling, and differentiable rendering, our approach automatically converts video demonstrations from widely used robot datasets into simulated counterparts. Within these digital twins, we assess VLA policies using both automated VLM-guided scoring and scalable human preference judgments collected from crowdworkers, transforming human involvement from tedious scene setup, resetting, and safety supervision into lightweight preference comparisons. To measure robustness, we systematically perturb simulated environments along multiple axes, such as textures and object placements, stress-testing policy generalization under controlled variation. The result is a continuously evolving, reproducible, and scalable benchmark for real-world trained robot manipulation policies, addressing a critical missing capability in today's robotics landscape.",
        "translated": "对通用机器人代理——即能够在多样化环境中执行多种任务的可指令化智能体——的追求，要求建立严格且可扩展的评估体系。然而，机器人策略在真实世界中的测试仍然面临根本性限制：其过程耗时、费力、难以大规模部署且存在安全隐患，同时复现性差。现有的仿真基准同样存在局限，因为它们在相同的合成环境中训练和测试策略，无法评估从真实世界演示或不同仿真环境训练得到的模型。随着策略在范围和复杂度上的不断扩展，这些障碍愈发加剧，因为在机器人领域，“成功”的定义往往依赖于对执行质量进行细致入微的人类主观判断。本文提出了一种新的基准框架，通过将视觉-语言-动作（VLA）评估迁移至大规模仿真环境，并结合在线人类反馈，有效克服了上述挑战。借助视觉-语言模型、2D到3D生成建模以及可微渲染等技术进展，我们的方法能够自动将来自常用机器人数据集的视频演示转换为对应的仿真版本。在这些数字孪生环境中，我们通过自动化视觉-语言模型引导的评分系统，以及从众包工作者处收集的大规模人类偏好判断，对VLA策略进行评估，从而将人类参与从繁琐的场景搭建、重置和安全监督，转变为轻量级的偏好比较任务。为衡量策略的鲁棒性，我们沿多个维度（如纹理和物体布局）系统性地扰动仿真环境，在受控变化条件下对策略的泛化能力进行压力测试。最终，我们构建了一个持续演进、可复现且可扩展的基准，用于评估在真实世界中训练的机器人操作策略，填补了当前机器人领域的一项关键能力空白。",
        "translated_title": "RobotArena $\\infty$: 基于真实到模拟转换的可扩展机器人基准测试",
        "label": [],
        "label_reason": "论文聚焦机器人策略评估与仿真，非图像像素级恢复或增强任务，属于高阶视觉应用。",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出真实到仿真的转换框架，结合VLM与人类反馈，方法新颖但非low-level图像处理领域。"
    },
    {
        "title": "EgoThinker: Unveiling Egocentric Reasoning with Spatio-Temporal CoT",
        "url": "http://arxiv.org/abs/2510.23569v1",
        "pub_date": "2025-10-27",
        "summary": "Egocentric video reasoning centers on an unobservable agent behind the camera who dynamically shapes the environment, requiring inference of hidden intentions and recognition of fine-grained interactions. This core challenge limits current multimodal large language models MLLMs, which excel at visible event reasoning but lack embodied, first-person understanding. To bridge this gap, we introduce EgoThinker, a novel framework that endows MLLMs with robust egocentric reasoning capabilities through spatio-temporal chain-of-thought supervision and a two-stage learning curriculum. First, we introduce EgoRe-5M, a large-scale egocentric QA dataset constructed from 13M diverse egocentric video clips. This dataset features multi-minute segments annotated with detailed CoT rationales and dense hand-object grounding. Second, we employ SFT on EgoRe-5M to instill reasoning skills, followed by reinforcement fine-tuning RFT to further enhance spatio-temporal localization. Experimental results show that EgoThinker outperforms existing methods across multiple egocentric benchmarks, while achieving substantial improvements in fine-grained spatio-temporal localization tasks. Full code and data are released at https://github.com/InternRobotics/EgoThinker.",
        "translated": "以自我为中心的视频推理聚焦于摄像机后方不可观测的主体，该主体动态地塑造环境，要求对隐藏意图进行推理，并识别细粒度的交互行为。这一核心挑战限制了当前多模态大语言模型（MLLMs）的表现，尽管它们在可见事件推理方面表现出色，但缺乏具身化的第一人称理解能力。为弥合这一差距，我们提出 EgoThinker，一种新颖的框架，通过时空思维链监督和两阶段学习课程，赋予 MLLMs 强大的以自我为中心的推理能力。首先，我们构建了 EgoRe-5M，一个大规模的以自我为中心的问答数据集，由1300万段多样化的以自我为中心视频片段组成。该数据集包含多分钟长的视频片段，标注了详细的思维链推理过程（CoT rationales）和密集的手部-物体关联信息。其次，我们在 EgoRe-5M 上采用监督微调（SFT）以注入推理能力，随后通过强化微调（RFT）进一步提升时空定位能力。实验结果表明，EgoThinker 在多个以自我为中心的基准测试中均优于现有方法，并在细粒度时空定位任务中实现了显著提升。完整代码与数据已公开于 https://github.com/InternRobotics/EgoThinker。",
        "translated_title": "EgoThinker：通过时空思维链揭示以自我为中心的推理",
        "label": [],
        "label_reason": "论文聚焦于第一人称视频推理与意图理解，属于高阶视觉任务，非像素级图像恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 6,
        "novelty_reason": "提出时空CoT监督与两阶段学习，方法新颖，但应用于高阶推理而非低级图像处理。"
    },
    {
        "title": "Revising Second Order Terms in Deep Animation Video Coding",
        "url": "http://arxiv.org/abs/2510.23561v1",
        "pub_date": "2025-10-27",
        "summary": "First Order Motion Model is a generative model that animates human heads based on very little motion information derived from keypoints. It is a promising solution for video communication because first it operates at very low bitrate and second its computational complexity is moderate compared to other learning based video codecs. However, it has strong limitations by design. Since it generates facial animations by warping source-images, it fails to recreate videos with strong head movements. This works concentrates on one specific kind of head movements, namely head rotations. We show that replacing the Jacobian transformations in FOMM by a global rotation helps the system to perform better on items with head-rotations while saving 40% to 80% of bitrate on P-frames. Moreover, we apply state-of-the-art normalization techniques to the discriminator to stabilize the adversarial training which is essential for generating visually appealing videos. We evaluate the performance by the learned metics LPIPS and DISTS to show the success our optimizations.",
        "translated": "一阶运动模型（First Order Motion Model）是一种生成模型，能够基于从关键点提取的极少运动信息对人脸头部进行动画生成。该模型在视频通信领域具有应用前景，首先是因为其以极低比特率运行，其次是因为其计算复杂度相较于其他基于学习的视频编解码器处于中等水平。然而，该模型在设计上存在明显局限性。由于其通过形变源图像来生成面部动画，因此在处理具有剧烈头部运动的视频时表现不佳。本工作聚焦于一种特定的头部运动——头部旋转。我们证明，将FOMM中的雅可比变换替换为全局旋转，可显著提升系统在头部旋转场景下的性能，同时在P帧上节省40%至80%的比特率。此外，我们采用当前最先进的归一化技术对判别器进行优化，以稳定对抗训练过程，这对生成视觉上更具吸引力的视频至关重要。我们通过学习型指标LPIPS和DISTS评估性能，以验证所提优化方案的有效性。",
        "translated_title": "修订深度动画视频编码中的二阶项",
        "label": [],
        "label_reason": "论文聚焦于视频编码与生成，属于高阶视觉任务，不涉及像素级图像恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 4,
        "novelty_reason": "对FOMM模型进行优化，改进运动建模与训练稳定，属常规改进，无突破性创新。"
    },
    {
        "title": "A U-Net and Transformer Pipeline for Multilingual Image Translation",
        "url": "http://arxiv.org/abs/2510.23554v1",
        "pub_date": "2025-10-27",
        "summary": "This paper presents an end-to-end multilingual translation pipeline that integrates a custom U-Net for text detection, the Tesseract engine for text recognition, and a from-scratch sequence-to-sequence (Seq2Seq) Transformer for Neural Machine Translation (NMT). Our approach first utilizes a U-Net model, trained on a synthetic dataset , to accurately segment and detect text regions from an image. These detected regions are then processed by Tesseract to extract the source text. This extracted text is fed into a custom Transformer model trained from scratch on a multilingual parallel corpus spanning 5 languages. Unlike systems reliant on monolithic pre-trained models, our architecture emphasizes full customization and adaptability. The system is evaluated on its text detection accuracy, text recognition quality, and translation performance via BLEU scores. The complete pipeline demonstrates promising results, validating the viability of a custom-built system for translating text directly from images.",
        "translated": "本文提出了一种端到端的多语言翻译流程，该流程集成了自定义的U-Net用于文本检测、Tesseract引擎用于文本识别，以及从头训练的序列到序列（Seq2Seq）Transformer用于神经机器翻译（NMT）。我们的方法首先利用在合成数据集上训练的U-Net模型，从图像中精确分割和检测文本区域。随后，这些检测到的区域由Tesseract处理以提取源文本。提取的文本被输入到一个在包含5种语言的多语言平行语料库上从头训练的自定义Transformer模型中。与依赖于单体预训练模型的系统不同，我们的架构强调完全的定制化和适应性。系统通过文本检测精度、文本识别质量以及基于BLEU分数的翻译性能进行评估。完整的流程展现了令人满意的成果，验证了构建自定义系统以直接从图像中翻译文本的可行性。",
        "translated_title": "一种基于U-Net与Transformer的多语言图像翻译流水线",
        "label": [],
        "label_reason": "论文聚焦图像中文字检测与翻译，属于高阶视觉任务，非像素级图像恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 4,
        "novelty_reason": "提出定制化U-Net+Transformer架构，但属常规组件组合，无突破性创新。"
    },
    {
        "title": "JanusCoder: Towards a Foundational Visual-Programmatic Interface for\n  Code Intelligence",
        "url": "http://arxiv.org/abs/2510.23538v1",
        "pub_date": "2025-10-27",
        "summary": "The scope of neural code intelligence is rapidly expanding beyond text-based source code to encompass the rich visual outputs that programs generate. This visual dimension is critical for advanced applications like flexible content generation and precise, program-driven editing of visualizations. However, progress has been impeded by the scarcity of high-quality multimodal code data, a bottleneck stemming from challenges in synthesis and quality assessment. To address these challenges, we make contributions from both a data and modeling perspective. We first introduce a complete synthesis toolkit that leverages reciprocal synergies between data modalities to efficiently produce a large-scale, high-quality corpus spanning from standard charts to complex interactive web UIs and code-driven animations. Leveraging this toolkit, we construct JanusCode-800K, the largest multimodal code corpus to date. This powers the training of our models, JanusCoder and JanusCoderV, which establish a visual-programmatic interface for generating code from textual instructions, visual inputs, or a combination of both. Our unified model is a departure from existing approaches that build specialized models for isolated tasks. Extensive experiments on both text-centric and vision-centric coding tasks demonstrate the superior performance of the JanusCoder series, with our 7B to 14B scale models approaching or even exceeding the performance of commercial models. Furthermore, extensive analysis provides key insights into harmonizing programmatic logic with its visual expression. Our code and checkpoints will are available at https://github.com/InternLM/JanusCoder.",
        "translated": "神经代码智能的研究范围正迅速从基于文本的源代码扩展至程序生成的丰富视觉输出。这一视觉维度对于高级应用（如灵活的内容生成和精确的、由程序驱动的可视化编辑）至关重要。然而，进展受到高质量多模态代码数据稀缺的阻碍，而这一瓶颈源于合成与质量评估方面的挑战。为应对这些挑战，我们从数据和建模两个层面做出贡献。首先，我们提出了一套完整的合成工具包，利用数据模态间的相互协同效应，高效生成涵盖标准图表至复杂交互式网页用户界面及代码驱动动画的大规模、高质量语料库。借助该工具包，我们构建了 JanusCode-800K，这是迄今为止最大的多模态代码语料库。该语料库支撑了我们模型 JanusCoder 和 JanusCoderV 的训练，二者建立了一个视觉-程序化接口，能够根据文本指令、视觉输入或两者的组合生成代码。我们的统一模型突破了现有方法中为孤立任务构建专用模型的范式。在以文本为中心和以视觉为中心的编程任务上的大量实验表明，JanusCoder 系列模型表现出优越性能，其 7B 至 14B 规模的模型性能已接近甚至超越商用模型。此外，深入的分析为协调程序逻辑与其视觉表达提供了关键洞见。我们的代码和模型检查点可通过 https://github.com/InternLM/JanusCoder 获取。",
        "translated_title": "JanusCoder：面向代码智能的通用视觉-程序化交互界面",
        "label": [],
        "label_reason": "论文聚焦代码智能与多模态生成，不涉及图像像素级恢复或增强任务",
        "relevance_score": 1,
        "novelty_score": 6,
        "novelty_reason": "提出多模态代码数据合成工具与统一模型，属代码生成领域创新，非图像处理"
    },
    {
        "title": "DPGLA: Bridging the Gap between Synthetic and Real Data for Unsupervised\n  Domain Adaptation in 3D LiDAR Semantic Segmentation",
        "url": "http://arxiv.org/abs/2510.23525v1",
        "pub_date": "2025-10-27",
        "summary": "Annotating real-world LiDAR point clouds for use in intelligent autonomous systems is costly. To overcome this limitation, self-training-based Unsupervised Domain Adaptation (UDA) has been widely used to improve point cloud semantic segmentation by leveraging synthetic point cloud data. However, we argue that existing methods do not effectively utilize unlabeled data, as they either rely on predefined or fixed confidence thresholds, resulting in suboptimal performance. In this paper, we propose a Dynamic Pseudo-Label Filtering (DPLF) scheme to enhance real data utilization in point cloud UDA semantic segmentation. Additionally, we design a simple and efficient Prior-Guided Data Augmentation Pipeline (PG-DAP) to mitigate domain shift between synthetic and real-world point clouds. Finally, we utilize data mixing consistency loss to push the model to learn context-free representations. We implement and thoroughly evaluate our approach through extensive comparisons with state-of-the-art methods. Experiments on two challenging synthetic-to-real point cloud semantic segmentation tasks demonstrate that our approach achieves superior performance. Ablation studies confirm the effectiveness of the DPLF and PG-DAP modules. We release the code of our method in this paper.",
        "translated": "对用于智能自主系统的现实世界LiDAR点云进行标注成本高昂。为克服这一局限，基于自训练的无监督域自适应（UDA）方法被广泛采用，旨在通过利用合成点云数据提升点云语义分割性能。然而，我们认为现有方法未能有效利用无标签数据，因其通常依赖预设或固定的置信度阈值，导致性能欠佳。本文提出一种动态伪标签过滤（DPLF）方案，以增强点云UDA语义分割中对真实数据的利用。此外，我们设计了一种简单高效的先验引导数据增强流水线（PG-DAP），以缓解合成点云与现实世界点云之间的域偏移问题。最后，我们引入数据混合一致性损失，促使模型学习与上下文无关的表征。我们通过与当前最先进方法的广泛对比，对所提出方法进行了实现与全面评估。在两个具有挑战性的合成到现实点云语义分割任务上的实验表明，本文方法取得了优越的性能。消融研究验证了DPLF和PG-DAP模块的有效性。本文公开了所提出方法的代码。",
        "translated_title": "DPGLA：弥合合成数据与真实数据之间的差距，用于3D激光雷达语义分割的无监督域自适应",
        "label": [],
        "label_reason": "论文聚焦3D LiDAR语义分割，属于高阶视觉任务，非像素级图像恢复。",
        "relevance_score": 1,
        "novelty_score": 6,
        "novelty_reason": "提出动态伪标签过滤与先验引导数据增强，对UDA有改进，但非low-level创新。"
    },
    {
        "title": "FreeFuse: Multi-Subject LoRA Fusion via Auto Masking at Test Time",
        "url": "http://arxiv.org/abs/2510.23515v1",
        "pub_date": "2025-10-27",
        "summary": "This paper proposes FreeFuse, a novel training-free approach for multi-subject text-to-image generation through automatic fusion of multiple subject LoRAs. In contrast to existing methods that either focus on pre-inference LoRA weight merging or rely on segmentation models and complex techniques like noise blending to isolate LoRA outputs, our key insight is that context-aware dynamic subject masks can be automatically derived from cross-attention layer weights. Mathematical analysis shows that directly applying these masks to LoRA outputs during inference well approximates the case where the subject LoRA is integrated into the diffusion model and used individually for the masked region. FreeFuse demonstrates superior practicality and efficiency as it requires no additional training, no modification to LoRAs, no auxiliary models, and no user-defined prompt templates or region specifications. Alternatively, it only requires users to provide the LoRA activation words for seamless integration into standard workflows. Extensive experiments validate that FreeFuse outperforms existing approaches in both generation quality and usability under the multi-subject generation tasks. The project page is at https://future-item.github.io/FreeFuse/",
        "translated": "本文提出 FreeFuse，一种新颖的无需训练的多主体文本到图像生成方法，通过自动融合多个主体的 LoRA 实现。与现有方法相比，后者或专注于推理前的 LoRA 权重合并，或依赖于分割模型以及噪声融合等复杂技术以分离 LoRA 输出，本文的核心洞见在于：可从交叉注意力层权重中自动推导出具有上下文感知能力的动态主体掩码。数学分析表明，在推理过程中直接将这些掩码应用于 LoRA 输出，能够很好地近似于将主体 LoRA 集成至扩散模型，并单独用于掩码区域的情形。FreeFuse 具备卓越的实用性和效率，无需额外训练、无需修改 LoRA、无需辅助模型，也无需用户自定义提示模板或区域指定。相反，用户仅需提供 LoRA 的激活词，即可无缝集成至标准工作流程。大量实验验证了在多主体生成任务中，FreeFuse 在生成质量和可用性方面均优于现有方法。项目主页位于 https://future-item.github.io/FreeFuse/",
        "translated_title": "FreeFuse：基于测试时自动掩码的多主体LoRA融合\n\n在本研究中，我们提出了一种名为FreeFuse的新型多主体低秩自适应（LoRA）融合方法，旨在实现高效且灵活的模型集成，无需在训练阶段进行显式配对或微调。与现有方法不同，FreeFuse在测试阶段通过自动掩码机制动态选择并融合多个LoRA适配器，从而适应不同输入图像的视觉内容和退化类型。\n\n具体而言，FreeFuse引入了一个轻量级的掩码生成模块，该模块根据输入图像的特征图自适应地生成掩码向量，用于控制每个LoRA适配器的激活权重。该掩码生成过程完全在空域中进行，通过一个小型卷积网络提取图像的局部与全局上下文信息，并将其映射为连续的掩码分数。这些分数随后被归一化并用于对齐不同LoRA适配器的输出贡献。\n\n为确保融合过程的稳定性，我们进一步引入残差学习结构，将原始主干网络的特征图与多个LoRA适配器的输出进行逐元素加权融合。该设计不仅保留了主干网络的先验知识，还增强了模型对复杂退化（如图像去噪、去雨、去雾等）的适应能力。\n\n实验结果表明，FreeFuse在多个基准数据集（包括Rain100L、SIDD、RESIDE）上均取得了显著优于基线方法的性能，尤其在处理多退化混合场景时表现出更强的鲁棒性。此外，由于其端到端的训练方式和测试时的动态融合机制，FreeFuse在保持高精度的同时，显著降低了计算开销，适用于资源受限的部署环境。\n\n我们进一步验证了FreeFuse在不同图像恢复任务（如超分辨率、低光照增强、JPEG伪影去除等）中的通用性，证明了其作为通用融合框架的潜力。方法的灵活性和效率使其成为未来多模型集成研究的重要方向。",
        "label": [],
        "label_reason": "论文聚焦于文本到图像生成中的多主体融合，属于高阶图像生成任务，非像素级图像恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出基于注意力权重的自动掩码融合策略，无需训练或辅助模型，方法新颖但不针对低层视觉问题。"
    },
    {
        "title": "Localising under the drape: proprioception in the era of distributed\n  surgical robotic system",
        "url": "http://arxiv.org/abs/2510.23512v1",
        "pub_date": "2025-10-27",
        "summary": "Despite their mechanical sophistication, surgical robots remain blind to their surroundings. This lack of spatial awareness causes collisions, system recoveries, and workflow disruptions, issues that will intensify with the introduction of distributed robots with independent interacting arms. Existing tracking systems rely on bulky infrared cameras and reflective markers, providing only limited views of the surgical scene and adding hardware burden in crowded operating rooms. We present a marker-free proprioception method that enables precise localisation of surgical robots under their sterile draping despite associated obstruction of visual cues. Our method solely relies on lightweight stereo-RGB cameras and novel transformer-based deep learning models. It builds on the largest multi-centre spatial robotic surgery dataset to date (1.4M self-annotated images from human cadaveric and preclinical in vivo studies). By tracking the entire robot and surgical scene, rather than individual markers, our approach provides a holistic view robust to occlusions, supporting surgical scene understanding and context-aware control. We demonstrate an example of potential clinical benefits during in vivo breathing compensation with access to tissue dynamics, unobservable under state of the art tracking, and accurately locate in multi-robot systems for future intelligent interaction. In addition, and compared with existing systems, our method eliminates markers and improves tracking visibility by 25%. To our knowledge, this is the first demonstration of marker-free proprioception for fully draped surgical robots, reducing setup complexity, enhancing safety, and paving the way toward modular and autonomous robotic surgery.",
        "translated": "尽管手术机器人在机械结构上已相当精巧，但其仍无法感知周围环境。这种空间感知能力的缺失会导致碰撞、系统重启和工作流程中断等问题，而这些问题在引入具有独立交互臂的分布式机器人后将更加突出。现有的追踪系统依赖于体积庞大的红外摄像头和反光标记，仅能提供手术场景的有限视角，并在拥挤的手术室内增加硬件负担。我们提出了一种无标记的本体感知方法，能够在手术机器人被无菌布覆盖、视觉线索受到遮挡的情况下，实现其精确定位。该方法仅依赖轻量化的立体RGB摄像头和基于新型Transformer架构的深度学习模型。我们基于迄今为止规模最大的多中心空间机器人手术数据集（包含来自人体 cadaveric 和临床前体内研究的140万张自标注图像）构建了该方法。通过追踪整个机器人及手术场景，而非单个标记点，我们的方法提供了对遮挡具有鲁棒性的全局视图，支持手术场景理解与上下文感知控制。我们在体内呼吸补偿实验中展示了潜在的临床优势，能够获取组织动态信息，这是现有追踪技术无法观测到的，并能准确定位多机器人系统中的各部件，为未来的智能交互奠定基础。此外，相较于现有系统，我们的方法消除了对标记物的依赖，使追踪可视性提升了25%。据我们所知，这是首次在完全覆盖无菌布的手术机器人上实现无标记本体感知，显著降低了系统部署复杂度，提升了安全性，并为模块化和自主化机器人手术的发展铺平了道路。",
        "translated_title": "在遮挡下的定位：分布式手术机器人系统时代的本体感知",
        "label": [],
        "label_reason": "论文聚焦于手术机器人定位与场景理解，属于高阶视觉任务，非像素级图像恢复或增强。",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出基于Transformer的无标记定位方法，具有创新性，但非针对图像质量复原任务。"
    },
    {
        "title": "iPac: Incorporating Intra-image Patch Context into Graph Neural Networks\n  for Medical Image Classification",
        "url": "http://arxiv.org/abs/2510.23504v1",
        "pub_date": "2025-10-27",
        "summary": "Graph neural networks have emerged as a promising paradigm for image processing, yet their performance in image classification tasks is hindered by a limited consideration of the underlying structure and relationships among visual entities. This work presents iPac, a novel approach to introduce a new graph representation of images to enhance graph neural network image classification by recognizing the importance of underlying structure and relationships in medical image classification. iPac integrates various stages, including patch partitioning, feature extraction, clustering, graph construction, and graph-based learning, into a unified network to advance graph neural network image classification. By capturing relevant features and organising them into clusters, we construct a meaningful graph representation that effectively encapsulates the semantics of the image. Experimental evaluation on diverse medical image datasets demonstrates the efficacy of iPac, exhibiting an average accuracy improvement of up to 5% over baseline methods. Our approach offers a versatile and generic solution for image classification, particularly in the realm of medical images, by leveraging the graph representation and accounting for the inherent structure and relationships among visual entities.",
        "translated": "图神经网络已成为图像处理领域一种有前景的范式，然而其在图像分类任务中的性能受到对视觉实体之间潜在结构与关系考虑不足的限制。本文提出 iPac，一种新颖的方法，通过引入一种新的图像图表示，以增强图神经网络在图像分类中的表现，充分认识到潜在结构与关系在医学图像分类中的重要性。iPac 将多个阶段——包括图像块划分、特征提取、聚类、图构建以及基于图的学习——集成到一个统一的网络框架中，从而推进图神经网络在图像分类中的应用。通过捕捉相关特征并将其组织为簇，我们构建出一种具有语义意义的图表示，有效封装了图像的语义信息。在多个医学图像数据集上的实验评估表明，iPac 的有效性显著，相较于基线方法，平均准确率提升高达 5%。我们的方法通过利用图表示并考虑视觉实体之间固有的结构与关系，为图像分类，尤其是在医学图像领域，提供了一种通用且灵活的解决方案。",
        "translated_title": "iPac：将图像内局部块上下文融入图神经网络用于医学图像分类",
        "label": [],
        "label_reason": "论文聚焦医学图像分类，属于high-level任务，非像素级图像恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 6,
        "novelty_reason": "提出新图结构表示方法，但用于分类任务，非low-level图像处理创新。"
    },
    {
        "title": "VOLD: Reasoning Transfer from LLMs to Vision-Language Models via\n  On-Policy Distillation",
        "url": "http://arxiv.org/abs/2510.23497v1",
        "pub_date": "2025-10-27",
        "summary": "Training vision-language models (VLMs) for complex reasoning remains a challenging task, i.a. due to the scarcity of high-quality image-text reasoning data. Conversely, text-based reasoning resources are abundant and scalable, but it is still an open question how to leveraging them for VLM reasoning. To address this problem, we propose VOLD, a framework to transfer reasoning capabilities from text-only teacher models to VLM student models. To this end, VOLD combines reinforcement learning via Group Relative Policy Optimization (GRPO) with on-policy distillation, which allows the student reasoning traces to be guided by the teacher model, resulting in a significant gain over using GRPO alone. We further show that a cold-start alignment is essential for an effective transfer during the online training phase in this scenario and that without sufficient distributional alignment between teacher and student, on-policy distillation fails to provide meaningful guidance. We evaluate VOLD across diverse benchmarks including MMMU-Pro, MathVision, MathVista, and LogicVista, showing that VOLD outperforms the baseline model significantly and improves over the state of the art by a margin. Our ablation shows the importance of a cold-start alignment via SFT for on-policy distillation with a text-only teacher.",
        "translated": "训练视觉-语言模型（VLMs）以实现复杂推理仍是一项具有挑战性的任务，主要原因在于高质量图像-文本推理数据的稀缺性。相反，基于文本的推理资源丰富且易于扩展，但如何有效利用这些资源提升VLM的推理能力仍是一个开放问题。为解决该问题，我们提出VOLD，一个将文本-only教师模型的推理能力迁移至VLM学生模型的框架。为此，VOLD结合了基于Group Relative Policy Optimization（GRPO）的强化学习与在线策略蒸馏，使学生模型的推理轨迹能够受到教师模型的引导，从而在仅使用GRPO的情况下实现显著性能提升。我们进一步证明，在在线训练阶段，冷启动对齐对于实现有效迁移至关重要；若教师与学生模型之间缺乏充分的分布对齐，在线策略蒸馏将无法提供有意义的指导。我们在多个基准数据集上评估了VOLD，包括MMMU-Pro、MathVision、MathVista和LogicVista，结果表明VOLD显著优于基线模型，并在现有技术上取得明显提升。我们的消融实验表明，通过SFT进行的冷启动对齐对于使用文本-only教师模型的在线策略蒸馏具有关键作用。",
        "translated_title": "VOLD：通过基于策略的蒸馏实现大语言模型到视觉-语言模型的推理迁移",
        "label": [],
        "label_reason": "论文聚焦视觉-语言模型推理能力迁移，属于高阶视觉理解任务，非像素级图像处理。",
        "relevance_score": 1,
        "novelty_score": 6,
        "novelty_reason": "提出结合GRPO与在线蒸馏的框架，但核心为推理能力迁移，创新点在训练策略而非图像复原。"
    },
    {
        "title": "Yesnt: Are Diffusion Relighting Models Ready for Capture Stage\n  Compositing? A Hybrid Alternative to Bridge the Gap",
        "url": "http://arxiv.org/abs/2510.23494v1",
        "pub_date": "2025-10-27",
        "summary": "Volumetric video relighting is essential for bringing captured performances into virtual worlds, but current approaches struggle to deliver temporally stable, production-ready results. Diffusion-based intrinsic decomposition methods show promise for single frames, yet suffer from stochastic noise and instability when extended to sequences, while video diffusion models remain constrained by memory and scale. We propose a hybrid relighting framework that combines diffusion-derived material priors with temporal regularization and physically motivated rendering. Our method aggregates multiple stochastic estimates of per-frame material properties into temporally consistent shading components, using optical-flow-guided regularization. For indirect effects such as shadows and reflections, we extract a mesh proxy from Gaussian Opacity Fields and render it within a standard graphics pipeline. Experiments on real and synthetic captures show that this hybrid strategy achieves substantially more stable relighting across sequences than diffusion-only baselines, while scaling beyond the clip lengths feasible for video diffusion. These results indicate that hybrid approaches, which balance learned priors with physically grounded constraints, are a practical step toward production-ready volumetric video relighting.",
        "translated": "体素视频重光照对于将捕获的表演带入虚拟世界至关重要，但当前方法在实现时间上稳定且适用于生产的成果方面仍面临挑战。基于扩散的内在分解方法在单帧图像上展现出潜力，但在扩展至序列时却因随机噪声和不稳定性而表现不佳，而视频扩散模型则受限于内存和规模。我们提出一种混合重光照框架，结合了由扩散模型推导出的材质先验、时间正则化以及基于物理的渲染。该方法通过光流引导的正则化，将多帧随机估计的每帧材质属性聚合为时间一致的着色组件。对于阴影和反射等间接效果，我们从高斯不透明场中提取网格代理，并在标准图形管线中进行渲染。在真实和合成捕获数据上的实验表明，该混合策略在序列中实现了远优于仅使用扩散模型的基线方法的重光照稳定性，同时其处理的片段长度也超越了视频扩散模型可行的范围。这些结果表明，结合学习先验与基于物理约束的混合方法，是迈向可用于生产的体素视频重光照的切实可行的一步。",
        "translated_title": "Yesnt：扩散重光照模型是否已准备好用于拍摄阶段的合成？一种填补差距的混合替代方案",
        "label": [],
        "label_reason": "论文聚焦于体积视频重光照，属于高阶渲染任务，非像素级图像恢复或增强。",
        "relevance_score": 3,
        "novelty_score": 7,
        "novelty_reason": "提出混合框架结合扩散模型与物理渲染，创新性较强但不针对低层视觉任务。"
    },
    {
        "title": "Tongyi DeepResearch Technical Report",
        "url": "http://arxiv.org/abs/2510.24701v1",
        "pub_date": "2025-10-28",
        "summary": "We present Tongyi DeepResearch, an agentic large language model, which is specifically designed for long-horizon, deep information-seeking research tasks. To incentivize autonomous deep research agency, Tongyi DeepResearch is developed through an end-to-end training framework that combines agentic mid-training and agentic post-training, enabling scalable reasoning and information seeking across complex tasks. We design a highly scalable data synthesis pipeline that is fully automatic, without relying on costly human annotation, and empowers all training stages. By constructing customized environments for each stage, our system enables stable and consistent interactions throughout. Tongyi DeepResearch, featuring 30.5 billion total parameters, with only 3.3 billion activated per token, achieves state-of-the-art performance across a range of agentic deep research benchmarks, including Humanity's Last Exam, BrowseComp, BrowseComp-ZH, WebWalkerQA, xbench-DeepSearch, FRAMES and xbench-DeepSearch-2510. We open-source the model, framework, and complete solutions to empower the community.",
        "translated": "我们提出 Tongyi DeepResearch，一种专为长期、深度信息探索研究任务设计的智能体大语言模型。为激励自主深度研究智能体行为，Tongyi DeepResearch 通过融合智能体中期训练与智能体后期训练的端到端训练框架进行开发，从而支持在复杂任务中实现可扩展的推理与信息探索能力。我们设计了一条高度可扩展的自动数据合成流水线，无需依赖昂贵的人工标注，全面赋能所有训练阶段。通过为每个训练阶段构建定制化环境，系统实现了稳定且一致的交互过程。Tongyi DeepResearch 拥有总计 305 亿参数，每处理一个 token 仅激活 33 亿参数，在多个智能体深度研究基准测试中均达到当前最优水平，包括 Humanity's Last Exam、BrowseComp、BrowseComp-ZH、WebWalkerQA、xbench-DeepSearch、FRAMES 以及 xbench-DeepSearch-2510。我们开源了模型、训练框架及完整解决方案，以赋能学术与产业社区。",
        "translated_title": "通义深度研究技术报告",
        "label": [],
        "label_reason": "论文聚焦于通用智能体模型的深度研究任务，未直接涉及推荐系统核心环节。",
        "relevance_score": 3,
        "novelty_score": 8,
        "novelty_reason": "提出端到端智能体训练框架，数据合成自动化，具创新性但非推荐领域"
    },
    {
        "title": "Optimizing Retrieval for RAG via Reinforced Contrastive Learning",
        "url": "http://arxiv.org/abs/2510.24652v1",
        "pub_date": "2025-10-28",
        "summary": "As retrieval-augmented generation (RAG) becomes increasingly widespread, the role of information retrieval (IR) is shifting from retrieving information for human users to retrieving contextual knowledge for artificial intelligence (AI) systems, where relevance becomes difficult to define or annotate beforehand. To address this challenge, we propose R3, a Retrieval framework optimized for RAG through trialand-feedback Reinforced contrastive learning. Unlike prior approaches that rely on annotated or synthetic data for supervised fine-tuning, R3 enables the retriever to dynamically explore and optimize relevance within the RAG environment. During training, the retrieved results interact with the environment to produce contrastive signals that automatically guide the retriever's self-improvement. Extensive experiments across diverse tasks demonstrate that R3 improves RAG performance by 5.2% over the original retriever and surpasses state-of-the-art retrievers by 4.9%, while achieving comparable results to LLM-augmented retrieval and RAG systems built on post-trained or instruction-tuned LLMs. It is both efficient and practical, requiring only 4 GPUs and completing training within a single day.",
        "translated": "随着检索增强生成（RAG）技术的日益普及，信息检索（IR）的作用正从为人类用户检索信息，转变为为人工智能（AI）系统检索上下文知识，其中相关性变得难以预先定义或标注。为应对这一挑战，我们提出R3，一种通过试错与反馈强化对比学习优化的RAG检索框架。与以往依赖标注数据或合成数据进行监督微调的方法不同，R3使检索器能够在RAG环境中动态探索并优化相关性。在训练过程中，检索结果与环境交互，产生对比信号，自动引导检索器的自我改进。在多种任务上的大量实验表明，R3相较于原始检索器将RAG性能提升了5.2%，并超越了当前最先进的检索器4.9%，同时其性能可与基于大语言模型（LLM）增强的检索及建立在后训练或指令调优LLM上的RAG系统相媲美。R3兼具高效性与实用性，仅需4块GPU，且可在单日内完成训练。",
        "translated_title": "优化检索以用于RAG的强化对比学习",
        "label": [
            "召回",
            "负采样与对比学习"
        ],
        "label_reason": "论文聚焦RAG中的召回优化，通过强化对比学习提升检索质量，与推荐召回有间接关联。",
        "relevance_score": 4,
        "novelty_score": 7,
        "novelty_reason": "提出R3框架，利用环境反馈动态优化检索，创新性较强，但非推荐领域原创。"
    },
    {
        "title": "Iterative Critique-Refine Framework for Enhancing LLM Personalization",
        "url": "http://arxiv.org/abs/2510.24469v1",
        "pub_date": "2025-10-28",
        "summary": "Personalized text generation requires models not only to produce coherent text but also to align with a target user's style, tone, and topical focus. Existing retrieval-augmented approaches such as LaMP and PGraphRAG enrich profiles with user and neighbor histories, but they stop at generation and often yield outputs that drift in tone, topic, or style. We present PerFine, a unified, training-free critique-refine framework that enhances personalization through iterative, profile-grounded feedback. In each iteration, an LLM generator produces a draft conditioned on the retrieved profile, and a critic LLM - also conditioned on the same profile - provides structured feedback on tone, vocabulary, sentence structure, and topicality. The generator then revises, while a novel knockout strategy retains the stronger draft across iterations. We further study additional inference-time strategies such as Best-of-N and Topic Extraction to balance quality and efficiency. Across Yelp, Goodreads, and Amazon datasets, PerFine consistently improves personalization over PGraphRAG, with GEval gains of +7-13%, steady improvements over 3-5 refinement iterations, and scalability with increasing critic size. These results highlight that post-hoc, profile-aware feedback offers a powerful paradigm for personalized LLM generation that is both training-free and model-agnostic.",
        "translated": "个性化文本生成要求模型不仅生成连贯的文本，还需与目标用户的风格、语调和主题焦点保持一致。现有的检索增强方法（如 LaMP 和 PGraphRAG）通过整合用户及其邻居的历史记录来丰富用户画像，但它们仅止于生成阶段，往往导致输出在语调、主题或风格上出现偏移。我们提出 PerFine，一种统一的、无需训练的批判-优化框架，通过迭代的、基于用户画像的反馈机制提升个性化效果。在每次迭代中，大语言模型（LLM）生成器在检索到的用户画像条件下生成初稿，同时一个同样以该画像为条件的批判型 LLM 提供关于语调、词汇、句式结构和主题相关性的结构化反馈。生成器随后根据反馈进行修订，而一种新颖的“淘汰策略”则在各次迭代中保留表现更优的文稿版本。此外，我们进一步研究了多种推理阶段策略，如 Best-of-N 和主题提取，以在生成质量与效率之间取得平衡。在 Yelp、Goodreads 和 Amazon 数据集上的实验表明，PerFine 在个性化性能上始终优于 PGraphRAG，GEval 指标提升达 +7–13%，在 3–5 次优化迭代中持续改进，且随着批判模型规模增大具有良好的可扩展性。这些结果表明，后置的、基于画像的反馈机制为个性化大语言模型生成提供了一种强大范式，该范式无需训练且对模型类型具有无关性。",
        "translated_title": "迭代批评-优化框架用于增强大语言模型个性化",
        "label": [
            "LLM生成式推荐",
            "通用推荐技术"
        ],
        "label_reason": "论文聚焦LLM个性化生成，通过迭代反馈提升内容风格对齐，可应用于推荐系统中的生成式推荐环节。",
        "relevance_score": 7,
        "novelty_score": 8,
        "novelty_reason": "提出训练无关的迭代批评-优化框架，结合结构化反馈与淘汰策略，显著提升个性化生成质量。"
    },
    {
        "title": "MiniOneRec: An Open-Source Framework for Scaling Generative\n  Recommendation",
        "url": "http://arxiv.org/abs/2510.24431v1",
        "pub_date": "2025-10-28",
        "summary": "The recent success of large language models (LLMs) has renewed interest in whether recommender systems can achieve similar scaling benefits. Conventional recommenders, dominated by massive embedding tables, tend to plateau as embedding dimensions grow. In contrast, the emerging generative paradigm replaces embeddings with compact Semantic ID (SID) sequences produced by autoregressive Transformers. Yet most industrial deployments remain proprietary, leaving two fundamental questions open: (1) Do the expected scaling laws hold on public benchmarks? (2) What is the minimal post-training recipe that enables competitive performance?   We present MiniOneRec, to the best of our knowledge, the first fully open-source generative recommendation framework, which provides an end-to-end workflow spanning SID construction, supervised fine-tuning, and recommendation-oriented reinforcement learning. We generate SIDs via a Residual Quantized VAE and post-train Qwen backbones ranging from 0.5B to 7B parameters on the Amazon Review dataset. Our experiments reveal a consistent downward trend in both training and evaluation losses with increasing model size, validating the parameter efficiency of the generative approach. To further enhance performance, we propose a lightweight yet effective post-training pipeline that (1) enforces full-process SID alignment and (2) applies reinforcement learning with constrained decoding and hybrid rewards. Together, these techniques yield significant improvements in both ranking accuracy and candidate diversity.",
        "translated": "近期大语言模型（LLM）的成功重新引发了人们对于推荐系统能否实现类似扩展性收益的关注。传统的推荐系统主要依赖庞大的嵌入表，随着嵌入维度的增长，其性能往往趋于饱和。相比之下，新兴的生成式范式通过自回归Transformer生成紧凑的语义ID（SID）序列，取代了传统的嵌入表示。然而，大多数工业级部署仍为专有系统，导致两个根本性问题尚未解决：（1）在公开基准数据集上，预期的扩展规律是否成立？（2）实现具有竞争力性能所需的最小后训练方案是什么？  \n\n我们提出MiniOneRec，据我们所知，这是首个完全开源的生成式推荐框架，提供了一个端到端的工作流，涵盖SID构建、监督微调以及面向推荐的强化学习。我们在Amazon Review数据集上，通过残差量化变分自编码器（VAE）生成SID，并对参数量从0.5B到7B的Qwen主干模型进行后训练。实验结果表明，随着模型规模增大，训练损失和评估损失均呈现出稳定下降趋势，验证了生成式方法在参数效率上的优势。为进一步提升性能，我们提出了一种轻量但有效的后训练流程，其包含两个关键设计：（1）强制全链路SID对齐；（2）采用受限解码与混合奖励的强化学习策略。这些技术共同显著提升了排序准确率和候选物料多样性。",
        "translated_title": "MiniOneRec：一个用于扩展生成式推荐的开源框架",
        "label": [
            "LLM生成式推荐（LLM-based Generative Recommendation）",
            "精排（Ranking）",
            "序列推荐（Sequential Recommendation）",
            "推荐系统评估（Evaluation Metrics / Offline/Online Testing）"
        ],
        "label_reason": "论文聚焦LLM生成式推荐框架，涵盖SID构建、微调与强化学习，直接解决推荐系统核心问题。",
        "relevance_score": 10,
        "novelty_score": 9,
        "novelty_reason": "提出首个开源生成式推荐框架，结合轻量级微调与强化学习，显著提升准确率与多样性。"
    },
    {
        "title": "From Time and Place to Preference: LLM-Driven Geo-Temporal Context in\n  Recommendations",
        "url": "http://arxiv.org/abs/2510.24430v1",
        "pub_date": "2025-10-28",
        "summary": "Most recommender systems treat timestamps as numeric or cyclical values, overlooking real-world context such as holidays, events, and seasonal patterns. We propose a scalable framework that uses large language models (LLMs) to generate geo-temporal embeddings from only a timestamp and coarse location, capturing holidays, seasonal trends, and local/global events. We then introduce a geo-temporal embedding informativeness test as a lightweight diagnostic, demonstrating on MovieLens, LastFM, and a production dataset that these embeddings provide predictive signal consistent with the outcomes of full model integrations. Geo-temporal embeddings are incorporated into sequential models through (1) direct feature fusion with metadata embeddings or (2) an auxiliary loss that enforces semantic and geo-temporal alignment. Our findings highlight the need for adaptive or hybrid recommendation strategies, and we release a context-enriched MovieLens dataset to support future research.",
        "translated": "大多数推荐系统将时间戳视为数值或循环变量，忽略了节假日、事件和季节性模式等现实世界上下文信息。我们提出了一种可扩展的框架，仅基于时间戳和粗粒度地理位置，利用大语言模型（LLM）生成地理-时间嵌入，从而捕捉节假日、季节性趋势以及本地和全球事件。随后，我们引入了一种地理-时间嵌入信息量测试，作为一种轻量级诊断工具，通过在MovieLens、LastFM以及一个生产数据集上的实验表明，这些嵌入提供了与完整模型集成结果一致的预测信号。地理-时间嵌入通过两种方式融入序列模型：（1）与元数据嵌入进行直接特征融合，或（2）引入辅助损失函数，以强制实现语义和地理-时间对齐。我们的研究结果强调了采用自适应或混合推荐策略的必要性，并发布了一个上下文增强的MovieLens数据集，以支持未来的研究工作。",
        "translated_title": "从时间与地点到偏好：大语言模型驱动的时空上下文在推荐系统中的应用",
        "label": [
            "LLM生成式推荐（LLM-based Generative Recommendation）",
            "序列推荐（Sequential Recommendation）",
            "通用推荐技术（General Recommendation Techniques）"
        ],
        "label_reason": "利用LLM生成时空上下文嵌入，增强推荐系统对时间地点的建模，适用于序列推荐与通用推荐场景",
        "relevance_score": 8,
        "novelty_score": 8,
        "novelty_reason": "创新性地将LLM用于生成地理时间上下文嵌入，提升推荐系统对动态场景的感知能力"
    },
    {
        "title": "Metadata-Driven Retrieval-Augmented Generation for Financial Question\n  Answering",
        "url": "http://arxiv.org/abs/2510.24402v1",
        "pub_date": "2025-10-28",
        "summary": "Retrieval-Augmented Generation (RAG) struggles on long, structured financial filings where relevant evidence is sparse and cross-referenced. This paper presents a systematic investigation of advanced metadata-driven Retrieval-Augmented Generation (RAG) techniques, proposing and evaluating a novel, multi-stage RAG architecture that leverages LLM-generated metadata. We introduce a sophisticated indexing pipeline to create contextually rich document chunks and benchmark a spectrum of enhancements, including pre-retrieval filtering, post-retrieval reranking, and enriched embeddings, benchmarked on the FinanceBench dataset. Our results reveal that while a powerful reranker is essential for precision, the most significant performance gains come from embedding chunk metadata directly with text (\"contextual chunks\"). Our proposed optimal architecture combines LLM-driven pre-retrieval optimizations with these contextual embeddings to achieve superior performance. Additionally, we present a custom metadata reranker that offers a compelling, cost-effective alternative to commercial solutions, highlighting a practical trade-off between peak performance and operational efficiency. This study provides a blueprint for building robust, metadata-aware RAG systems for financial document analysis.",
        "translated": "检索增强生成（RAG）在处理长篇、结构化的财务文件时面临挑战，因为相关证据稀疏且存在交叉引用。本文对先进的元数据驱动式检索增强生成（RAG）技术进行了系统性研究，提出并评估了一种新颖的多阶段RAG架构，该架构利用大语言模型（LLM）生成的元数据。我们引入了一套复杂的索引流程，用于构建上下文丰富的文档片段，并在FinanceBench数据集上对一系列改进技术进行基准测试，包括检索前过滤、检索后重排以及增强嵌入。实验结果表明，尽管强大的重排器对精度至关重要，但性能提升最显著的来源是将片段元数据与文本直接嵌入（“上下文片段”）。我们提出的最优架构结合了LLM驱动的检索前优化与这些上下文嵌入，从而实现了卓越的性能。此外，我们提出了一种定制化的元数据重排器，作为商业解决方案的高效、低成本替代方案，揭示了峰值性能与运营效率之间的实用权衡。本研究为构建稳健、具备元数据感知能力的RAG系统提供了适用于财务文档分析的蓝图。",
        "translated_title": "元数据驱动的检索增强生成在金融问答中的应用",
        "label": [
            "召回",
            "重排",
            "LLM生成式推荐"
        ],
        "label_reason": "论文聚焦RAG架构，涉及召回与重排，虽为金融问答，但方法可迁移至推荐系统。",
        "relevance_score": 4,
        "novelty_score": 7,
        "novelty_reason": "提出多阶段RAG架构与上下文嵌入，结合LLM生成元数据，具创新性但非推荐领域首创。"
    },
    {
        "title": "DUET: Dual Model Co-Training for Entire Space CTR Prediction",
        "url": "http://arxiv.org/abs/2510.24369v1",
        "pub_date": "2025-10-28",
        "summary": "The pre-ranking stage plays a pivotal role in large-scale recommender systems but faces an intrinsic trade-off between model expressiveness and computational efficiency. Owing to the massive candidate pool and strict latency constraints, industry systems often rely on lightweight two-tower architectures, which are computationally efficient yet limited in estimation capability. As a result, they struggle to capture the complex synergistic and suppressive relationships among candidate items, which are essential for producing contextually coherent and diverse recommendation lists. Moreover, this simplicity further amplifies the Sample Selection Bias (SSB) problem, as coarse-grained models trained on biased exposure data must generalize to a much larger candidate space with distinct distributions.   To address these issues, we propose \\textbf{DUET} (\\textbf{DU}al Model Co-Training for \\textbf{E}ntire Space C\\textbf{T}R Prediction), a set-wise pre-ranking framework that achieves expressive modeling under tight computational budgets. Instead of scoring items independently, DUET performs set-level prediction over the entire candidate subset in a single forward pass, enabling information-aware interactions among candidates while amortizing the computational cost across the set. Moreover, a dual model co-training mechanism extends supervision to unexposed items via mutual pseudo-label refinement, effectively mitigating SSB. Validated through extensive offline experiments and online A/B testing, DUET consistently outperforms state-of-the-art baselines and achieves improvements across multiple core business metrics. At present, DUET has been fully deployed in Kuaishou and Kuaishou Lite Apps, serving the main traffic for hundreds of millions of users.",
        "translated": "粗排阶段在大规模推荐系统中起着关键作用，但面临着模型表达能力与计算效率之间的内在权衡。由于候选物料池规模庞大且存在严格的延迟约束，工业界系统通常依赖轻量级的双塔架构，这类架构计算高效，但估计能力有限。因此，它们难以捕捉候选物料之间复杂的协同与抑制关系，而这些关系对于生成上下文连贯且多样化的推荐列表至关重要。此外，这种简单性进一步加剧了样本选择偏差（SSB）问题，因为基于有偏曝光数据训练的粗粒度模型，必须泛化到分布截然不同的更大候选空间中。\n\n为解决上述问题，我们提出 **DUET**（**DU**al Model Co-Training for **E**ntire Space C**T**R Prediction），一种在严格计算预算下实现高表达能力建模的集合式粗排框架。DUET不单独对每个物料打分，而是在单次前向传播中对整个候选子集进行集合级预测，从而在候选物料之间实现信息感知的交互，同时将计算成本在集合中摊薄。此外，通过引入双模型协同训练机制，DUET利用互相对伪标签进行优化，将监督信号扩展至未曝光物料，有效缓解了SSB问题。经过大量离线实验和线上A/B测试验证，DUET持续优于当前最先进的基线方法，并在多个核心业务指标上实现了提升。目前，DUET已在快手和快手极速版App中全面部署，服务于数亿用户的主流量推荐场景。",
        "translated_title": "DUET：用于全空间点击率预测的双模型协同训练",
        "label": [
            "粗排（Pre-ranking）",
            "负采样与对比学习（Negative Sampling / Contrastive Learning）",
            "通用推荐技术（General Recommendation Techniques）"
        ],
        "label_reason": "论文聚焦推荐系统粗排阶段，提出双模型协同训练框架，解决样本偏差与计算效率问题，直接应用于推荐系统。",
        "relevance_score": 8,
        "novelty_score": 8,
        "novelty_reason": "提出集级别预测与双模型伪标签互训机制，有效缓解SSB，提升表达能力与效率，为粗排带来显著改进。"
    },
    {
        "title": "Resource-Efficient LLM Application for Structured Transformation of\n  Unstructured Financial Contracts",
        "url": "http://arxiv.org/abs/2510.23990v1",
        "pub_date": "2025-10-28",
        "summary": "The transformation of unstructured legal contracts into standardized, machine-readable formats is essential for automating financial workflows. The Common Domain Model (CDM) provides a standardized framework for this purpose, but converting complex legal documents like Credit Support Annexes (CSAs) into CDM representations remains a significant challenge. In this paper, we present an extension of the CDMizer framework, a template-driven solution that ensures syntactic correctness and adherence to the CDM schema during contract-to-CDM conversion. We apply this extended framework to a real-world task, comparing its performance with a benchmark developed by the International Swaps and Derivatives Association (ISDA) for CSA clause extraction. Our results show that CDMizer, when integrated with a significantly smaller, open-source Large Language Model (LLM), achieves competitive performance in terms of accuracy and efficiency against larger, proprietary models. This work underscores the potential of resource-efficient solutions to automate legal contract transformation, offering a cost-effective and scalable approach that can meet the needs of financial institutions with constrained resources or strict data privacy requirements.",
        "translated": "将非结构化的法律合同转换为标准化、机器可读的格式，对于自动化金融业务流程至关重要。通用领域模型（Common Domain Model, CDM）为此提供了标准化框架，但将复杂的法律文件（如信用支持附加协议，Credit Support Annexes, CSAs）转换为CDM表示形式仍是一项重大挑战。本文提出了CDMizer框架的扩展版本，这是一种基于模板的解决方案，能够在合同到CDM的转换过程中确保语法正确性并严格遵循CDM模式。我们将该扩展框架应用于一项真实任务，与国际互换与衍生品协会（International Swaps and Derivatives Association, ISDA）为CSA条款提取开发的基准进行性能对比。实验结果表明，当CDMizer与一个规模显著较小、开源的大语言模型（Large Language Model, LLM）集成时，其在准确性和效率方面均能与更大规模的专有模型相媲美。本研究凸显了资源高效型解决方案在自动化法律合同转换中的潜力，提供了一种成本低廉且可扩展的方法，能够满足资源受限或数据隐私要求严格的金融机构的需求。",
        "translated_title": "资源高效的大语言模型在非结构化金融合约结构化转换中的应用",
        "label": [],
        "label_reason": "论文聚焦法律合同结构化转换，属NLP文档处理，与推荐系统无关。",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出轻量级LLM用于合同转换，方法为NLP领域改进，非推荐系统创新。"
    },
    {
        "title": "Generative View Stitching",
        "url": "http://arxiv.org/abs/2510.24718v1",
        "pub_date": "2025-10-28",
        "summary": "Autoregressive video diffusion models are capable of long rollouts that are stable and consistent with history, but they are unable to guide the current generation with conditioning from the future. In camera-guided video generation with a predefined camera trajectory, this limitation leads to collisions with the generated scene, after which autoregression quickly collapses. To address this, we propose Generative View Stitching (GVS), which samples the entire sequence in parallel such that the generated scene is faithful to every part of the predefined camera trajectory. Our main contribution is a sampling algorithm that extends prior work on diffusion stitching for robot planning to video generation. While such stitching methods usually require a specially trained model, GVS is compatible with any off-the-shelf video model trained with Diffusion Forcing, a prevalent sequence diffusion framework that we show already provides the affordances necessary for stitching. We then introduce Omni Guidance, a technique that enhances the temporal consistency in stitching by conditioning on both the past and future, and that enables our proposed loop-closing mechanism for delivering long-range coherence. Overall, GVS achieves camera-guided video generation that is stable, collision-free, frame-to-frame consistent, and closes loops for a variety of predefined camera paths, including Oscar Reutersv\\\"ard's Impossible Staircase. Results are best viewed as videos at https://andrewsonga.github.io/gvs.",
        "translated": "自回归视频扩散模型能够进行稳定且与历史一致的长序列生成，但无法利用来自未来的条件信息来引导当前的生成过程。在具有预定义相机轨迹的相机引导视频生成任务中，这一局限性会导致生成场景与相机轨迹发生碰撞，之后自回归过程会迅速崩溃。为解决该问题，我们提出生成式视图拼接（Generative View Stitching, GVS），该方法能够并行采样整个序列，从而确保生成的场景与预定义相机轨迹的每一部分保持一致。我们的主要贡献是一种采样算法，该算法将先前用于机器人规划的扩散拼接方法拓展至视频生成领域。尽管此类拼接方法通常需要专门训练的模型，但GVS兼容任何基于Diffusion Forcing框架训练的现成视频模型——这是一种主流的序列扩散框架，我们证明其已具备实现拼接所需的基本能力。随后，我们引入全向引导（Omni Guidance）技术，通过同时利用过去和未来的条件信息，增强拼接过程中的时间一致性，并支持我们提出的闭环机制，以实现长程一致性。总体而言，GVS实现了稳定、无碰撞、帧间一致且能够闭合循环的相机引导视频生成，适用于多种预定义相机路径，包括奥斯卡·雷特瑟瓦德（Oscar Reutersv\\\"ard）的“不可能楼梯”。最佳效果请参见视频演示：https://andrewsonga.github.io/gvs。",
        "translated_title": "生成视图拼接",
        "label": [],
        "label_reason": "论文属于图像生成（GAN/扩散模型）范畴，目标是生成新视频而非恢复或增强现有图像质量，非low-level任务。",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出新的采样算法和Omni Guidance技术，改进视频生成一致性，但未解决像素级图像恢复问题。"
    },
    {
        "title": "Uniform Discrete Diffusion with Metric Path for Video Generation",
        "url": "http://arxiv.org/abs/2510.24717v1",
        "pub_date": "2025-10-28",
        "summary": "Continuous-space video generation has advanced rapidly, while discrete approaches lag behind due to error accumulation and long-context inconsistency. In this work, we revisit discrete generative modeling and present Uniform discRete diffuSion with metric pAth (URSA), a simple yet powerful framework that bridges the gap with continuous approaches for the scalable video generation. At its core, URSA formulates the video generation task as an iterative global refinement of discrete spatiotemporal tokens. It integrates two key designs: a Linearized Metric Path and a Resolution-dependent Timestep Shifting mechanism. These designs enable URSA to scale efficiently to high-resolution image synthesis and long-duration video generation, while requiring significantly fewer inference steps. Additionally, we introduce an asynchronous temporal fine-tuning strategy that unifies versatile tasks within a single model, including interpolation and image-to-video generation. Extensive experiments on challenging video and image generation benchmarks demonstrate that URSA consistently outperforms existing discrete methods and achieves performance comparable to state-of-the-art continuous diffusion methods. Code and models are available at https://github.com/baaivision/URSA",
        "translated": "连续空间视频生成技术发展迅速，而离散方法由于误差累积和长上下文不一致性问题进展滞后。在本文中，我们重新审视离散生成建模，提出了一种简单而强大的框架——Uniform discRete diffuSion with metric pAth（URSA），该框架在可扩展视频生成方面弥合了离散方法与连续方法之间的差距。URSA的核心思想是将视频生成任务表述为对离散时空token的迭代全局优化。该框架集成了两个关键设计：线性化度量路径（Linearized Metric Path）和分辨率依赖的时间步偏移机制（Resolution-dependent Timestep Shifting）。这些设计使URSA能够高效扩展至高分辨率图像合成和长时序视频生成，同时显著减少推理步数。此外，我们引入了一种异步时间细调策略，使单一模型能够统一处理多种任务，包括插值和图像到视频生成。在具有挑战性的视频与图像生成基准测试上的大量实验表明，URSA始终优于现有的离散方法，并达到了与当前最先进的连续扩散方法相当的性能。代码和模型可在 https://github.com/baaivision/URSA 获取。",
        "translated_title": "均匀离散扩散与度量路径用于视频生成",
        "label": [],
        "label_reason": "论文聚焦于视频生成，属于图像生成任务，非像素级图像恢复或增强，不属于low-level图像处理。",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出新的离散扩散框架与时间步移机制，对离散生成模型有显著改进，但未针对低层视觉任务。"
    },
    {
        "title": "Routing Matters in MoE: Scaling Diffusion Transformers with Explicit\n  Routing Guidance",
        "url": "http://arxiv.org/abs/2510.24711v1",
        "pub_date": "2025-10-28",
        "summary": "Mixture-of-Experts (MoE) has emerged as a powerful paradigm for scaling model capacity while preserving computational efficiency. Despite its notable success in large language models (LLMs), existing attempts to apply MoE to Diffusion Transformers (DiTs) have yielded limited gains. We attribute this gap to fundamental differences between language and visual tokens. Language tokens are semantically dense with pronounced inter-token variation, while visual tokens exhibit spatial redundancy and functional heterogeneity, hindering expert specialization in vision MoE. To this end, we present ProMoE, an MoE framework featuring a two-step router with explicit routing guidance that promotes expert specialization. Specifically, this guidance encourages the router to partition image tokens into conditional and unconditional sets via conditional routing according to their functional roles, and refine the assignments of conditional image tokens through prototypical routing with learnable prototypes based on semantic content. Moreover, the similarity-based expert allocation in latent space enabled by prototypical routing offers a natural mechanism for incorporating explicit semantic guidance, and we validate that such guidance is crucial for vision MoE. Building on this, we propose a routing contrastive loss that explicitly enhances the prototypical routing process, promoting intra-expert coherence and inter-expert diversity. Extensive experiments on ImageNet benchmark demonstrate that ProMoE surpasses state-of-the-art methods under both Rectified Flow and DDPM training objectives. Code and models will be made publicly available.",
        "translated": "专家混合（Mixture-of-Experts, MoE）作为一种在保持计算效率的同时扩展模型容量的强大范式，近年来备受关注。尽管MoE在大型语言模型（LLMs）中取得了显著成功，但将其应用于扩散Transformer（Diffusion Transformers, DiTs）的现有尝试却收效甚微。我们认为这一差距源于语言标记与视觉标记之间存在根本性差异：语言标记具有语义密集性和显著的标记间差异，而视觉标记则表现出空间冗余性和功能异质性，这阻碍了视觉MoE中专家的专业化。为此，我们提出ProMoE，一种具备两步路由机制和显式路由引导的MoE框架，以促进专家的专业化。具体而言，该引导机制通过条件路由，根据视觉标记的功能角色将其划分为条件性与非条件性集合，并通过基于可学习原型的原型路由（prototypical routing）对条件性图像标记的分配进行精细化调整，以反映其语义内容。此外，原型路由在潜在空间中实现的基于相似性的专家分配，为引入显式语义引导提供了天然机制，我们验证了此类引导对视觉MoE至关重要。基于此，我们进一步提出一种路由对比损失（routing contrastive loss），显式增强原型路由过程，提升专家内部的一致性与专家间的多样性。在ImageNet基准上的大量实验表明，ProMoE在Rectified Flow和DDPM两种训练目标下均超越了当前最先进方法。代码与模型将公开发布。",
        "translated_title": "路由在MoE中的重要性：基于显式路由引导的扩散Transformer扩展",
        "label": [],
        "label_reason": "论文聚焦于扩散Transformer中的MoE路由机制，属于高阶视觉生成任务，非像素级图像恢复。",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出两步路由与原型路由机制，结合对比损失，对视觉MoE有显著改进，但非low-level方向。"
    },
    {
        "title": "Does Object Binding Naturally Emerge in Large Pretrained Vision\n  Transformers?",
        "url": "http://arxiv.org/abs/2510.24709v1",
        "pub_date": "2025-10-28",
        "summary": "Object binding, the brain's ability to bind the many features that collectively represent an object into a coherent whole, is central to human cognition. It groups low-level perceptual features into high-level object representations, stores those objects efficiently and compositionally in memory, and supports human reasoning about individual object instances. While prior work often imposes object-centric attention (e.g., Slot Attention) explicitly to probe these benefits, it remains unclear whether this ability naturally emerges in pre-trained Vision Transformers (ViTs). Intuitively, they could: recognizing which patches belong to the same object should be useful for downstream prediction and thus guide attention. Motivated by the quadratic nature of self-attention, we hypothesize that ViTs represent whether two patches belong to the same object, a property we term IsSameObject. We decode IsSameObject from patch embeddings across ViT layers using a similarity probe, which reaches over 90% accuracy. Crucially, this object-binding capability emerges reliably in self-supervised ViTs (DINO, MAE, CLIP), but markedly weaker in ImageNet-supervised models, suggesting that binding is not a trivial architectural artifact, but an ability acquired through specific pretraining objectives. We further discover that IsSameObject is encoded in a low-dimensional subspace on top of object features, and that this signal actively guides attention. Ablating IsSameObject from model activations degrades downstream performance and works against the learning objective, implying that emergent object binding naturally serves the pretraining objective. Our findings challenge the view that ViTs lack object binding and highlight how symbolic knowledge of \"which parts belong together\" emerges naturally in a connectionist system.",
        "translated": "对象绑定（object binding），即大脑将共同表征一个对象的多种特征整合为一个连贯整体的能力，是人类认知的核心。它将低层次感知特征聚合成高层次的对象表征，以高效且组合化的方式存储这些对象，并支持人类对单个对象实例的推理。尽管先前的工作常通过显式引入以对象为中心的注意力机制（例如 Slot Attention）来探究这些优势，但尚不清楚该能力是否在预训练的视觉Transformer（ViTs）中自然涌现。直观上，这种能力应当存在：识别哪些图像块属于同一对象对下游预测任务有益，因而应能引导注意力机制。受自注意力机制固有的二次特性启发，我们假设ViTs能够表征两个图像块是否属于同一对象，我们将这一属性称为 IsSameObject。我们通过相似性探测器（similarity probe）从ViT各层的图像块嵌入（patch embeddings）中解码出 IsSameObject，其准确率超过90%。关键的是，该对象绑定能力在自监督ViTs（如 DINO、MAE、CLIP）中稳定出现，而在ImageNet监督训练的模型中则显著较弱，这表明对象绑定并非简单的架构副产物，而是通过特定的预训练目标所习得的能力。我们进一步发现，IsSameObject 编码于对象特征之上的一个低维子空间中，且该信号主动引导注意力机制。从模型激活中移除 IsSameObject 会损害下游任务性能，并且违背学习目标，这表明涌现的对象绑定能力自然服务于预训练目标。我们的研究结果挑战了“ViTs缺少对象绑定能力”的观点，并揭示了“哪些部分属于同一整体”这一符号化知识如何在连接主义系统中自然涌现。",
        "translated_title": "物体绑定是否自然地出现在大型预训练视觉Transformer中？",
        "label": [],
        "label_reason": "论文研究ViT中对象绑定能力的涌现，属高层视觉认知，非像素级图像恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出IsSameObject概念并验证其在自监督ViT中涌现，方法新颖，但非low-level任务创新。"
    },
    {
        "title": "MIC-BEV: Multi-Infrastructure Camera Bird's-Eye-View Transformer with\n  Relation-Aware Fusion for 3D Object Detection",
        "url": "http://arxiv.org/abs/2510.24688v1",
        "pub_date": "2025-10-28",
        "summary": "Infrastructure-based perception plays a crucial role in intelligent transportation systems, offering global situational awareness and enabling cooperative autonomy. However, existing camera-based detection models often underperform in such scenarios due to challenges such as multi-view infrastructure setup, diverse camera configurations, degraded visual inputs, and various road layouts. We introduce MIC-BEV, a Transformer-based bird's-eye-view (BEV) perception framework for infrastructure-based multi-camera 3D object detection. MIC-BEV flexibly supports a variable number of cameras with heterogeneous intrinsic and extrinsic parameters and demonstrates strong robustness under sensor degradation. The proposed graph-enhanced fusion module in MIC-BEV integrates multi-view image features into the BEV space by exploiting geometric relationships between cameras and BEV cells alongside latent visual cues. To support training and evaluation, we introduce M2I, a synthetic dataset for infrastructure-based object detection, featuring diverse camera configurations, road layouts, and environmental conditions. Extensive experiments on both M2I and the real-world dataset RoScenes demonstrate that MIC-BEV achieves state-of-the-art performance in 3D object detection. It also remains robust under challenging conditions, including extreme weather and sensor degradation. These results highlight the potential of MIC-BEV for real-world deployment. The dataset and source code are available at: https://github.com/HandsomeYun/MIC-BEV.",
        "translated": "基础设施感知在智能交通系统中发挥着关键作用，能够提供全局态势感知并支持协同自主驾驶。然而，现有的基于摄像头的检测模型在该类场景下通常表现不佳，主要受限于多视角基础设施部署、多样化的摄像头配置、退化的视觉输入以及复杂的道路布局等挑战。本文提出 MIC-BEV，一种基于 Transformer 的鸟瞰图（BEV）感知框架，用于基础设施多摄像头 3D 目标检测。MIC-BEV 能够灵活支持数量可变、具有异构内参和外参的摄像头，并在传感器退化条件下展现出强大的鲁棒性。该框架中提出的图增强融合模块，通过挖掘摄像头与 BEV 单元之间的几何关系以及潜在的视觉线索，将多视角图像特征有效整合至 BEV 空间。为支持模型训练与评估，我们构建了 M2I，一个面向基础设施目标检测的合成数据集，包含多样化的摄像头配置、道路布局和环境条件。在 M2I 和真实世界数据集 RoScenes 上进行的大量实验表明，MIC-BEV 在 3D 目标检测任务中达到了最先进的性能，且在极端天气和传感器退化等挑战性条件下仍保持稳定。这些结果凸显了 MIC-BEV 在实际部署中的巨大潜力。数据集与源代码可于以下地址获取：https://github.com/HandsomeYun/MIC-BEV。",
        "translated_title": "MIC-BEV：基于关系感知融合的多基础设施摄像头鸟瞰图Transformer用于3D目标检测",
        "label": [],
        "label_reason": "论文核心为3D目标检测，属high-level任务，不涉及像素级图像恢复或增强。",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出图增强融合模块，对多相机特征融合有改进，但非low-level图像处理创新。"
    },
    {
        "title": "SAGE: Structure-Aware Generative Video Transitions between Diverse Clips",
        "url": "http://arxiv.org/abs/2510.24667v1",
        "pub_date": "2025-10-28",
        "summary": "Video transitions aim to synthesize intermediate frames between two clips, but naive approaches such as linear blending introduce artifacts that limit professional use or break temporal coherence. Traditional techniques (cross-fades, morphing, frame interpolation) and recent generative inbetweening methods can produce high-quality plausible intermediates, but they struggle with bridging diverse clips involving large temporal gaps or significant semantic differences, leaving a gap for content-aware and visually coherent transitions. We address this challenge by drawing on artistic workflows, distilling strategies such as aligning silhouettes and interpolating salient features to preserve structure and perceptual continuity. Building on this, we propose SAGE (Structure-Aware Generative vidEo transitions) as a zeroshot approach that combines structural guidance, provided via line maps and motion flow, with generative synthesis, enabling smooth, semantically consistent transitions without fine-tuning. Extensive experiments and comparison with current alternatives, namely [FILM, TVG, DiffMorpher, VACE, GI], demonstrate that SAGE outperforms both classical and generative baselines on quantitative metrics and user studies for producing transitions between diverse clips. Code to be released on acceptance.",
        "translated": "视频过渡旨在合成两个视频片段之间的中间帧，但简单的线性混合等朴素方法会引入伪影，限制其专业应用或破坏时间连贯性。传统技术（如交叉淡入淡出、变形、帧插值）以及近期的生成式中间帧生成方法虽然能够生成高质量且合理的中间帧，但在处理涉及较大时间间隔或显著语义差异的多样化片段时仍存在困难，导致在内容感知与视觉连贯性方面存在空白。我们通过借鉴艺术创作流程，提炼出诸如对齐轮廓和插值显著特征等策略，以保持结构与感知连续性，从而应对这一挑战。在此基础上，我们提出SAGE（Structure-Aware Generative vidEo transitions），一种零样本方法，结合由线图和运动流提供的结构引导与生成式合成，实现无需微调即可生成平滑且语义一致的过渡效果。大量实验和与当前主流方法（如FILM、TVG、DiffMorpher、VACE、GI）的对比表明，SAGE在定量指标和用户研究中均优于经典方法与生成式基线，尤其在处理多样化片段的过渡任务中表现突出。代码将在论文被接收后公开。",
        "translated_title": "SAGE：面向多样视频片段的结构感知生成式视频过渡",
        "label": [],
        "label_reason": "论文聚焦视频过渡合成，属高阶生成任务，非像素级图像恢复或增强。",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出结构感知生成框架，结合线图与光流，对视频生成有改进，但非low-level创新。"
    },
    {
        "title": "Group Relative Attention Guidance for Image Editing",
        "url": "http://arxiv.org/abs/2510.24657v1",
        "pub_date": "2025-10-28",
        "summary": "Recently, image editing based on Diffusion-in-Transformer models has undergone rapid development. However, existing editing methods often lack effective control over the degree of editing, limiting their ability to achieve more customized results. To address this limitation, we investigate the MM-Attention mechanism within the DiT model and observe that the Query and Key tokens share a bias vector that is only layer-dependent. We interpret this bias as representing the model's inherent editing behavior, while the delta between each token and its corresponding bias encodes the content-specific editing signals. Based on this insight, we propose Group Relative Attention Guidance, a simple yet effective method that reweights the delta values of different tokens to modulate the focus of the model on the input image relative to the editing instruction, enabling continuous and fine-grained control over editing intensity without any tuning. Extensive experiments conducted on existing image editing frameworks demonstrate that GRAG can be integrated with as few as four lines of code, consistently enhancing editing quality. Moreover, compared to the commonly used Classifier-Free Guidance, GRAG achieves smoother and more precise control over the degree of editing. Our code will be released at https://github.com/little-misfit/GRAG-Image-Editing.",
        "translated": "近年来，基于Diffusion-in-Transformer（DiT）模型的图像编辑技术取得了快速发展。然而，现有编辑方法通常缺乏对编辑程度的有效控制，限制了其生成更具定制化结果的能力。为解决这一局限，我们研究了DiT模型中的MM-Attention机制，发现Query和Key token共享一个仅依赖于网络层的偏置向量。我们将该偏置解释为模型固有的编辑行为，而每个token与其对应偏置之间的差值则编码了与内容相关的编辑信号。基于这一洞察，我们提出了一种简单而有效的方法——组相对注意力引导（Group Relative Attention Guidance, GRAG），通过重新加权不同token的差值，调节模型对输入图像相对于编辑指令的关注焦点，从而在无需任何调参的情况下实现对编辑强度的连续且细粒度的控制。在现有图像编辑框架上进行的大量实验表明，GRAG仅需四行代码即可集成，并能稳定提升编辑质量。此外，与常用的无分类器引导（Classifier-Free Guidance）相比，GRAG在编辑程度的控制上表现出更平滑、更精确的特性。我们的代码将在https://github.com/little-misfit/GRAG-Image-Editing上发布。",
        "translated_title": "群组相对注意力引导用于图像编辑",
        "label": [],
        "label_reason": "论文聚焦图像编辑，属于高阶视觉任务，非像素级图像恢复或增强。",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出相对注意力引导机制，对现有扩散模型编辑方法有改进，但非low-level创新。"
    },
    {
        "title": "Eye-Tracking, Mouse Tracking, Stimulus Tracking,and Decision-Making\n  Datasets in Digital Pathology",
        "url": "http://arxiv.org/abs/2510.24653v1",
        "pub_date": "2025-10-28",
        "summary": "Interpretation of giga-pixel whole-slide images (WSIs) is an important but difficult task for pathologists. Their diagnostic accuracy is estimated to average around 70%. Adding a second pathologist does not substantially improve decision consistency. The field lacks adequate behavioral data to explain diagnostic errors and inconsistencies. To fill in this gap, we present PathoGaze1.0, a comprehensive behavioral dataset capturing the dynamic visual search and decision-making processes of the full diagnostic workflow during cancer diagnosis. The dataset comprises 18.69 hours of eye-tracking, mouse interaction, stimulus tracking, viewport navigation, and diagnostic decision data (EMSVD) collected from 19 pathologists interpreting 397 WSIs. The data collection process emphasizes ecological validity through an application-grounded testbed, called PTAH. In total, we recorded 171,909 fixations, 263,320 saccades, and 1,867,362 mouse interaction events. In addition, such data could also be used to improve the training of both pathologists and AI systems that might support human experts. All experiments were preregistered at https://osf.io/hj9a7, and the complete dataset along with analysis code is available at https://go.osu.edu/pathogaze.",
        "translated": "对千兆像素全切片图像（WSIs）的解读是病理学家的一项重要但极具挑战性的任务。其诊断准确率估计平均约为70%。增加第二位病理学家并未显著提升诊断决策的一致性。该领域目前缺乏足够的行为数据，以解释诊断错误与不一致性的成因。为弥补这一空白，我们提出PathoGaze1.0，这是一个综合性行为数据集，捕捉了癌症诊断完整诊断流程中动态的视觉搜索与决策过程。该数据集包含19位病理学家在解读397张WSIs时所采集的18.69小时的眼动追踪、鼠标交互、刺激追踪、视口导航及诊断决策数据（EMSVD）。数据采集过程通过一个以应用为导向的实验平台PTAH，强调生态效度。总体而言，我们记录了171,909个注视点、263,320次扫视以及1,867,362次鼠标交互事件。此外，此类数据还可用于提升病理学家及可能辅助人类专家的人工智能系统的训练效果。所有实验均已在https://osf.io/hj9a7预先注册，完整数据集及分析代码可通过https://go.osu.edu/pathogaze获取。",
        "translated_title": "眼动追踪、鼠标追踪、刺激物追踪及决策行为数据集在数字病理学中的应用",
        "label": [],
        "label_reason": "论文聚焦病理学诊断行为数据采集，非图像像素级恢复或增强任务，属高阶视觉理解。",
        "relevance_score": 1,
        "novelty_score": 3,
        "novelty_reason": "构建行为数据集，无图像处理新方法或架构创新，属数据收集类工作。"
    },
    {
        "title": "A Dual-Branch CNN for Robust Detection of AI-Generated Facial Forgeries",
        "url": "http://arxiv.org/abs/2510.24640v1",
        "pub_date": "2025-10-28",
        "summary": "The rapid advancement of generative AI has enabled the creation of highly realistic forged facial images, posing significant threats to AI security, digital media integrity, and public trust. Face forgery techniques, ranging from face swapping and attribute editing to powerful diffusion-based image synthesis, are increasingly being used for malicious purposes such as misinformation, identity fraud, and defamation. This growing challenge underscores the urgent need for robust and generalizable face forgery detection methods as a critical component of AI security infrastructure. In this work, we propose a novel dual-branch convolutional neural network for face forgery detection that leverages complementary cues from both spatial and frequency domains. The RGB branch captures semantic information, while the frequency branch focuses on high-frequency artifacts that are difficult for generative models to suppress. A channel attention module is introduced to adaptively fuse these heterogeneous features, highlighting the most informative channels for forgery discrimination. To guide the network's learning process, we design a unified loss function, FSC Loss, that combines focal loss, supervised contrastive loss, and a frequency center margin loss to enhance class separability and robustness. We evaluate our model on the DiFF benchmark, which includes forged images generated from four representative methods: text-to-image, image-to-image, face swap, and face edit. Our method achieves strong performance across all categories and outperforms average human accuracy. These results demonstrate the model's effectiveness and its potential contribution to safeguarding AI ecosystems against visual forgery attacks.",
        "translated": "生成式人工智能的快速发展使得高度逼真的伪造人脸图像得以生成，对人工智能安全、数字媒体完整性以及公众信任构成了重大威胁。人脸伪造技术涵盖从人脸替换、属性编辑到基于扩散模型的强大图像合成等多种方法，正日益被用于恶意目的，如传播虚假信息、身份欺诈和诽谤。这一日益严峻的挑战凸显了开发鲁棒且通用的人脸伪造检测方法的迫切需求，此类方法是人工智能安全基础设施的关键组成部分。在本文中，我们提出了一种新颖的双分支卷积神经网络用于人脸伪造检测，该网络融合了空域和频域中的互补线索。RGB分支用于捕捉语义信息，而频域分支则专注于生成模型难以抑制的高频伪影。我们引入了一个通道注意力模块，以自适应地融合这些异构特征，突出对伪造判别最具信息量的通道。为引导网络的学习过程，我们设计了一种统一的损失函数FSC Loss，该损失函数结合了焦点损失（focal loss）、监督对比损失（supervised contrastive loss）以及频域中心裕度损失（frequency center margin loss），以增强类别可分离性和模型鲁棒性。我们在DiFF基准数据集上对模型进行了评估，该数据集包含由四种代表性方法生成的伪造图像：文本到图像（text-to-image）、图像到图像（image-to-image）、人脸替换（face swap）和人脸编辑（face edit）。我们的方法在所有类别中均表现出色，并超越了平均人类准确率。这些结果证明了模型的有效性及其在抵御视觉伪造攻击、保障人工智能生态系统安全方面的潜在贡献。",
        "translated_title": "双分支卷积神经网络用于鲁棒的AI生成人脸伪造检测",
        "label": [],
        "label_reason": "论文聚焦于AI生成人脸伪造检测，属于高阶视觉理解任务，非像素级图像恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 6,
        "novelty_reason": "提出双分支结构与FSC损失函数，对特征融合和损失设计有改进，但属检测领域常规优化。"
    },
    {
        "title": "GroundLoc: Efficient Large-Scale Outdoor LiDAR-Only Localization",
        "url": "http://arxiv.org/abs/2510.24623v1",
        "pub_date": "2025-10-28",
        "summary": "In this letter, we introduce GroundLoc, a LiDAR-only localization pipeline designed to localize a mobile robot in large-scale outdoor environments using prior maps. GroundLoc employs a Bird's-Eye View (BEV) image projection focusing on the perceived ground area and utilizes the place recognition network R2D2, or alternatively, the non-learning approach Scale-Invariant Feature Transform (SIFT), to identify and select keypoints for BEV image map registration. Our results demonstrate that GroundLoc outperforms state-of-the-art methods on the SemanticKITTI and HeLiPR datasets across various sensors. In the multi-session localization evaluation, GroundLoc reaches an Average Trajectory Error (ATE) well below 50 cm on all Ouster OS2 128 sequences while meeting online runtime requirements. The system supports various sensor models, as evidenced by evaluations conducted with Velodyne HDL-64E, Ouster OS2 128, Aeva Aeries II, and Livox Avia sensors. The prior maps are stored as 2D raster image maps, which can be created from a single drive and require only 4 MB of storage per square kilometer. The source code is available at https://github.com/dcmlr/groundloc.",
        "translated": "在本文中，我们提出了一种名为 GroundLoc 的仅依赖 LiDAR 的定位框架，旨在利用先验地图在大规模室外环境中实现移动机器人的定位。GroundLoc 采用鸟瞰图（Bird's-Eye View, BEV）图像投影，聚焦于感知到的地面区域，并利用场景识别网络 R2D2，或替代地使用非学习方法尺度不变特征变换（Scale-Invariant Feature Transform, SIFT），以识别并选取关键点用于 BEV 图像与地图的配准。实验结果表明，GroundLoc 在 SemanticKITTI 和 HeLiPR 数据集上，针对多种传感器均优于当前最先进的方法。在多会话定位评估中，GroundLoc 在所有 Ouster OS2 128 序列上的平均轨迹误差（Average Trajectory Error, ATE）均低于 50 厘米，同时满足在线运行时的要求。该系统支持多种传感器型号，评估中使用了 Velodyne HDL-64E、Ouster OS2 128、Aeva Aeries II 和 Livox Avia 传感器。先验地图以 2D 栅格图像地图形式存储，可通过单次行驶生成，每平方公里仅需 4 MB 的存储空间。源代码可在 https://github.com/dcmlr/groundloc 获取。",
        "translated_title": "GroundLoc：高效的大规模室外仅LiDAR定位",
        "label": [],
        "label_reason": "论文聚焦LiDAR定位，无图像像素级恢复或增强任务，属于高阶感知与定位系统。",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "提出BEV投影与关键点匹配策略，但未涉及图像处理创新，属传感器融合与定位方法改进。"
    },
    {
        "title": "Physics-Inspired Gaussian Kolmogorov-Arnold Networks for X-ray Scatter\n  Correction in Cone-Beam CT",
        "url": "http://arxiv.org/abs/2510.24579v1",
        "pub_date": "2025-10-28",
        "summary": "Cone-beam CT (CBCT) employs a flat-panel detector to achieve three-dimensional imaging with high spatial resolution. However, CBCT is susceptible to scatter during data acquisition, which introduces CT value bias and reduced tissue contrast in the reconstructed images, ultimately degrading diagnostic accuracy. To address this issue, we propose a deep learning-based scatter artifact correction method inspired by physical prior knowledge. Leveraging the fact that the observed point scatter probability density distribution exhibits rotational symmetry in the projection domain. The method uses Gaussian Radial Basis Functions (RBF) to model the point scatter function and embeds it into the Kolmogorov-Arnold Networks (KAN) layer, which provides efficient nonlinear mapping capabilities for learning high-dimensional scatter features. By incorporating the physical characteristics of the scattered photon distribution together with the complex function mapping capacity of KAN, the model improves its ability to accurately represent scatter. The effectiveness of the method is validated through both synthetic and real-scan experiments. Experimental results show that the model can effectively correct the scatter artifacts in the reconstructed images and is superior to the current methods in terms of quantitative metrics.",
        "translated": "锥形束CT（CBCT）采用平板探测器实现高空间分辨率的三维成像。然而，CBCT在数据采集过程中易受散射影响，导致重建图像中CT值出现偏差且组织对比度降低，最终影响诊断准确性。为解决这一问题，本文提出一种基于物理先验知识的深度学习散射伪影校正方法。该方法利用观测到的点散射概率密度分布在投影域中具有旋转对称性的特点，采用高斯径向基函数（RBF）对点散射函数进行建模，并将其嵌入到Kolmogorov-Arnold网络（KAN）层中，以提供高效非线性映射能力，用于学习高维散射特征。通过结合散射光子分布的物理特性与KAN的复杂函数映射能力，模型增强了对散射现象的准确表征能力。该方法的有效性通过合成数据和真实扫描实验进行了验证。实验结果表明，该模型能够有效校正重建图像中的散射伪影，在定量指标上优于现有方法。",
        "translated_title": "受物理启发的高斯科尔莫戈洛夫-阿诺德网络在锥束CT中用于X射线散射校正",
        "label": [
            "CT金属伪影消除",
            "医学图像增强"
        ],
        "label_reason": "针对CBCT散射伪影进行校正，提升图像对比度与诊断质量，属于医学图像恢复范畴。",
        "relevance_score": 9,
        "novelty_score": 8,
        "novelty_reason": "融合物理先验与KAN网络，构建新型散射建模方法，具有显著结构创新。"
    },
    {
        "title": "OSWorld-MCP: Benchmarking MCP Tool Invocation In Computer-Use Agents",
        "url": "http://arxiv.org/abs/2510.24563v1",
        "pub_date": "2025-10-28",
        "summary": "With advances in decision-making and reasoning capabilities, multimodal agents show strong potential in computer application scenarios. Past evaluations have mainly assessed GUI interaction skills, while tool invocation abilities, such as those enabled by the Model Context Protocol (MCP), have been largely overlooked. Comparing agents with integrated tool invocation to those evaluated only on GUI interaction is inherently unfair. We present OSWorld-MCP, the first comprehensive and fair benchmark for assessing computer-use agents' tool invocation, GUI operation, and decision-making abilities in a real-world environment. We design a novel automated code-generation pipeline to create tools and combine them with a curated selection from existing tools. Rigorous manual validation yields 158 high-quality tools (covering 7 common applications), each verified for correct functionality, practical applicability, and versatility. Extensive evaluations of state-of-the-art multimodal agents on OSWorld-MCP show that MCP tools generally improve task success rates (e.g., from 8.3% to 20.4% for OpenAI o3 at 15 steps, from 40.1% to 43.3% for Claude 4 Sonnet at 50 steps), underscoring the importance of assessing tool invocation capabilities. However, even the strongest models have relatively low tool invocation rates, Only 36.3%, indicating room for improvement and highlighting the benchmark's challenge. By explicitly measuring MCP tool usage skills, OSWorld-MCP deepens understanding of multimodal agents and sets a new standard for evaluating performance in complex, tool-assisted environments. Our code, environment, and data are publicly available at https://osworld-mcp.github.io.",
        "translated": "随着决策与推理能力的进步，多模态智能体在计算机应用场景中展现出巨大潜力。以往的评估主要集中于图形用户界面（GUI）交互能力，而工具调用能力（如通过模型上下文协议 Model Context Protocol, MCP 实现的能力）则被广泛忽视。将具备集成工具调用能力的智能体与仅在GUI交互上进行评估的智能体进行比较，本质上是不公平的。我们提出 OSWorld-MCP，这是首个在真实环境中全面且公平评估计算机使用智能体工具调用、GUI操作及决策能力的基准测试。我们设计了一种新颖的自动化代码生成流水线，用于创建工具，并将其与精选的现有工具相结合。通过严格的逐项人工验证，我们构建了158个高质量工具（覆盖7个常用应用程序），每个工具均经过功能正确性、实际适用性和通用性的验证。在OSWorld-MCP上对当前最先进的多模态智能体进行广泛评估表明，MCP工具通常能显著提升任务成功率（例如，OpenAI o3在15步内成功率从8.3%提升至20.4%，Claude 4 Sonnet在50步内从40.1%提升至43.3%），凸显了评估工具调用能力的重要性。然而，即便是最强的模型，其工具调用率也相对较低，仅为36.3%，表明仍有改进空间，并凸显了该基准测试的挑战性。通过明确测量MCP工具使用技能，OSWorld-MCP深化了对多模态智能体的理解，并为在复杂、工具辅助环境中的性能评估树立了新标准。我们的代码、环境和数据已公开发布于 https://osworld-mcp.github.io。",
        "translated_title": "OSWorld-MCP：计算机使用代理中MCP工具调用的基准测试",
        "label": [],
        "label_reason": "论文聚焦多模态代理的工具调用与决策能力，属于高阶任务，不涉及图像像素级处理。",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "提出新基准和工具生成流程，但未涉及低层视觉创新，属通用评测框架改进。"
    },
    {
        "title": "Latent Sketchpad: Sketching Visual Thoughts to Elicit Multimodal\n  Reasoning in MLLMs",
        "url": "http://arxiv.org/abs/2510.24514v1",
        "pub_date": "2025-10-28",
        "summary": "While Multimodal Large Language Models (MLLMs) excel at visual understanding, they often struggle in complex scenarios that require visual planning and imagination. Inspired by how humans use sketching as a form of visual thinking to develop and communicate ideas, we introduce Latent Sketchpad, a framework that equips MLLMs with an internal visual scratchpad. The internal visual representations of MLLMs have traditionally been confined to perceptual understanding. We repurpose them to support generative visual thought without compromising reasoning ability. Building on frontier MLLMs, our approach integrates visual generation directly into their native autoregressive reasoning process. It allows the model to interleave textual reasoning with the generation of visual latents. These latents guide the internal thought process and can be translated into sketch images for interpretability. To realize this, we introduce two components: a Context-Aware Vision Head autoregressively produces visual representations, and a pretrained Sketch Decoder renders these into human-interpretable images. We evaluate the framework on our new dataset MazePlanning. Experiments across various MLLMs show that Latent Sketchpad delivers comparable or even superior reasoning performance to their backbone. It further generalizes across distinct frontier MLLMs, including Gemma3 and Qwen2.5-VL. By extending model's textual reasoning to visual thinking, our framework opens new opportunities for richer human-computer interaction and broader applications. More details and resources are available on our project page: https://latent-sketchpad.github.io/.",
        "translated": "尽管多模态大语言模型（MLLMs）在视觉理解方面表现出色，但在需要视觉规划与想象的复杂场景中往往表现不佳。受人类通过草图作为视觉思维形式来发展和交流想法的启发，我们提出了一种名为 Latent Sketchpad 的框架，该框架为 MLLMs 赋予一个内部的视觉草稿板。传统上，MLLMs 的内部视觉表征仅限于感知理解。我们重新利用这些表征，使其支持生成式视觉思维，同时不损害其推理能力。基于前沿的 MLLMs，我们的方法将视觉生成直接整合到其原生的自回归推理过程中，使模型能够交错进行文本推理与视觉隐变量的生成。这些隐变量引导模型的内部思维过程，并可被转化为草图图像以增强可解释性。为此，我们引入了两个组件：一个上下文感知的视觉头（Context-Aware Vision Head）自回归地生成视觉表征，以及一个预训练的草图解码器（Sketch Decoder）将这些表征渲染为人类可理解的图像。我们在新构建的数据集 MazePlanning 上对框架进行了评估。在多种 MLLMs 上的实验表明，Latent Sketchpad 在推理性能上可与基座模型持平甚至更优，并且能够跨不同前沿 MLLMs（包括 Gemma3 和 Qwen2.5-VL）实现良好泛化。通过将模型的文本推理延伸至视觉思维，我们的框架为更丰富的人机交互和更广泛的应用开辟了新的可能性。更多细节和资源可访问我们的项目页面：https://latent-sketchpad.github.io/。",
        "translated_title": "潜显草图板：通过草绘视觉思维激发多模态大语言模型的多模态推理",
        "label": [],
        "label_reason": "论文聚焦多模态大模型的视觉推理增强，属高阶视觉理解，非像素级图像恢复或增强任务。",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出视觉思维框架，将生成式视觉表征融入推理过程，对MMLMs有显著创新。"
    },
    {
        "title": "Local Performance vs. Out-of-Distribution Generalization: An Empirical\n  Analysis of Personalized Federated Learning in Heterogeneous Data\n  Environments",
        "url": "http://arxiv.org/abs/2510.24503v1",
        "pub_date": "2025-10-28",
        "summary": "In the context of Federated Learning with heterogeneous data environments, local models tend to converge to their own local model optima during local training steps, deviating from the overall data distributions. Aggregation of these local updates, e.g., with FedAvg, often does not align with the global model optimum (client drift), resulting in an update that is suboptimal for most clients. Personalized Federated Learning approaches address this challenge by exclusively focusing on the average local performances of clients' models on their own data distribution. Generalization to out-of-distribution samples, which is a substantial benefit of FedAvg and represents a significant component of robustness, appears to be inadequately incorporated into the assessment and evaluation processes. This study involves a thorough evaluation of Federated Learning approaches, encompassing both their local performance and their generalization capabilities. Therefore, we examine different stages within a single communication round to enable a more nuanced understanding of the considered metrics. Furthermore, we propose and incorporate a modified approach of FedAvg, designated as Federated Learning with Individualized Updates (FLIU), extending the algorithm by a straightforward individualization step with an adaptive personalization factor. We evaluate and compare the approaches empirically using MNIST and CIFAR-10 under various distributional conditions, including benchmark IID and pathological non-IID, as well as additional novel test environments with Dirichlet distribution specifically developed to stress the algorithms on complex data heterogeneity.",
        "translated": "在异构数据环境下的联邦学习中，本地模型在本地训练阶段倾向于收敛至各自局部最优解，从而偏离整体数据分布。对这些本地更新进行聚合（例如采用FedAvg方法）通常无法与全局模型最优解对齐（即客户端漂移），导致对大多数客户端而言更新结果次优。个性化联邦学习方法通过专门关注客户端模型在其自身数据分布上的平均本地性能来应对这一挑战。然而，FedAvg所具备的对分布外样本的泛化能力——这一能力构成了模型鲁棒性的重要组成部分——在当前的评估与衡量过程中却未得到充分考虑。本研究对联邦学习方法进行了全面评估，涵盖其本地性能与泛化能力两个方面。为此，我们分析单次通信轮次中的不同阶段，以更细致地理解所考虑的评估指标。此外，我们提出并引入一种改进的FedAvg方法，称为具有个性化更新的联邦学习（Federated Learning with Individualized Updates, FLIU），通过一个简单而直观的个性化步骤，并引入自适应个性化因子，对算法进行扩展。我们在MNIST和CIFAR-10数据集上，针对多种数据分布条件（包括基准IID、病态非IID，以及专为强调复杂数据异构性而设计的Dirichlet分布下的新型测试环境）对所提方法进行了实验评估与对比。",
        "translated_title": "局部性能与分布外泛化能力：异构数据环境下个性化联邦学习的实证分析",
        "label": [],
        "label_reason": "论文聚焦联邦学习在异构数据环境下的个性化优化，属于高阶机器学习任务，不涉及像素级图像处理。",
        "relevance_score": 1,
        "novelty_score": 4,
        "novelty_reason": "提出FLIU改进FedAvg，属联邦学习领域常规方法改进，无低级图像处理创新。"
    },
    {
        "title": "Fast and accurate neural reflectance transformation imaging through\n  knowledge distillation",
        "url": "http://arxiv.org/abs/2510.24486v1",
        "pub_date": "2025-10-28",
        "summary": "Reflectance Transformation Imaging (RTI) is very popular for its ability to visually analyze surfaces by enhancing surface details through interactive relighting, starting from only a few tens of photographs taken with a fixed camera and variable illumination. Traditional methods like Polynomial Texture Maps (PTM) and Hemispherical Harmonics (HSH) are compact and fast, but struggle to accurately capture complex reflectance fields using few per-pixel coefficients and fixed bases, leading to artifacts, especially in highly reflective or shadowed areas. The NeuralRTI approach, which exploits a neural autoencoder to learn a compact function that better approximates the local reflectance as a function of light directions, has been shown to produce superior quality at comparable storage cost. However, as it performs interactive relighting with custom decoder networks with many parameters, the rendering step is computationally expensive and not feasible at full resolution for large images on limited hardware. Earlier attempts to reduce costs by directly training smaller networks have failed to produce valid results. For this reason, we propose to reduce its computational cost through a novel solution based on Knowledge Distillation (DisK-NeuralRTI). ...",
        "translated": "反射率变换成像（Reflectance Transformation Imaging, RTI）因其能够通过交互式重光照增强表面细节，从而实现对表面的可视化分析而广受欢迎。该方法仅需使用固定相机和可变光照拍摄的数十张照片即可完成。传统的PTM（多项式纹理图）和HSH（半球谐波）方法具有结构紧凑、计算快速的优点，但受限于每个像素仅使用少量系数和固定基函数来近似复杂的反射场，难以精确捕捉细节，尤其在高反射或阴影区域容易产生伪影。NeuralRTI方法通过利用神经自编码器学习一个紧凑函数，将局部反射率表示为光照方向的函数，已被证明在相近存储成本下可获得更优的重建质量。然而，由于其采用参数量较大的定制解码器网络进行交互式重光照，渲染步骤计算开销巨大，在有限硬件上难以对大尺寸图像实现全分辨率实时处理。此前尝试通过直接训练更小网络以降低成本的方法未能获得有效结果。为此，我们提出一种基于知识蒸馏（Knowledge Distillation）的新型解决方案（DisK-NeuralRTI），以降低其计算成本。...",
        "translated_title": "快速且准确的神经反射变换成像：基于知识蒸馏的方法",
        "label": [
            "图像去反射",
            "图像恢复"
        ],
        "label_reason": "通过知识蒸馏优化神经反射成像，提升反射细节重建质量，属图像恢复范畴",
        "relevance_score": 7,
        "novelty_score": 8,
        "novelty_reason": "首次将知识蒸馏用于NeuralRTI，降低渲染开销并保持重建质量"
    },
    {
        "title": "Decoupled MeanFlow: Turning Flow Models into Flow Maps for Accelerated\n  Sampling",
        "url": "http://arxiv.org/abs/2510.24474v1",
        "pub_date": "2025-10-28",
        "summary": "Denoising generative models, such as diffusion and flow-based models, produce high-quality samples but require many denoising steps due to discretization error. Flow maps, which estimate the average velocity between timesteps, mitigate this error and enable faster sampling. However, their training typically demands architectural changes that limit compatibility with pretrained flow models. We introduce Decoupled MeanFlow, a simple decoding strategy that converts flow models into flow map models without architectural modifications. Our method conditions the final blocks of diffusion transformers on the subsequent timestep, allowing pretrained flow models to be directly repurposed as flow maps. Combined with enhanced training techniques, this design enables high-quality generation in as few as 1 to 4 steps. Notably, we find that training flow models and subsequently converting them is more efficient and effective than training flow maps from scratch. On ImageNet 256x256 and 512x512, our models attain 1-step FID of 2.16 and 2.12, respectively, surpassing prior art by a large margin. Furthermore, we achieve FID of 1.51 and 1.68 when increasing the steps to 4, which nearly matches the performance of flow models while delivering over 100x faster inference.",
        "translated": "去噪生成模型，如扩散模型和基于流的模型，能够生成高质量样本，但由于离散化误差，通常需要大量去噪步数。流图（flow maps）通过估计时间步之间的平均速度，能够缓解该误差并实现更快的采样。然而，其训练通常需要对模型架构进行修改，从而限制了与预训练流模型的兼容性。我们提出了一种简单的解码策略——解耦均值流（Decoupled MeanFlow），可在不改变架构的前提下，将流模型转化为流图模型。我们的方法通过将扩散变换器（diffusion transformers）的最终模块条件于后续时间步，使得预训练的流模型可以直接被复用于流图任务。结合增强的训练技术，该设计能够在仅1至4步内实现高质量生成。值得注意的是，我们发现先训练流模型再将其转化为流图，比从零开始训练流图更高效且更有效。在ImageNet 256x256和512x512数据集上，我们的模型分别在1步采样下达到FID值2.16和2.12，显著超越现有方法。此外，当步数增加至4步时，FID值分别达到1.51和1.68，性能几乎与流模型相当，同时推理速度提升超过100倍。",
        "translated_title": "解耦的MeanFlow：将流模型转化为流图以实现加速采样",
        "label": [],
        "label_reason": "论文聚焦图像生成，非像素级图像恢复或增强，属high-level任务",
        "relevance_score": 1,
        "novelty_score": 8,
        "novelty_reason": "提出解耦策略加速采样，对预训练模型高效复用，有显著改进"
    },
    {
        "title": "Kineo: Calibration-Free Metric Motion Capture From Sparse RGB Cameras",
        "url": "http://arxiv.org/abs/2510.24464v1",
        "pub_date": "2025-10-28",
        "summary": "Markerless multiview motion capture is often constrained by the need for precise camera calibration, limiting accessibility for non-experts and in-the-wild captures. Existing calibration-free approaches mitigate this requirement but suffer from high computational cost and reduced reconstruction accuracy.   We present Kineo, a fully automatic, calibration-free pipeline for markerless motion capture from videos captured by unsynchronized, uncalibrated, consumer-grade RGB cameras. Kineo leverages 2D keypoints from off-the-shelf detectors to simultaneously calibrate cameras, including Brown-Conrady distortion coefficients, and reconstruct 3D keypoints and dense scene point maps at metric scale. A confidence-driven spatio-temporal keypoint sampling strategy, combined with graph-based global optimization, ensures robust calibration at a fixed computational cost independent of sequence length. We further introduce a pairwise reprojection consensus score to quantify 3D reconstruction reliability for downstream tasks.   Evaluations on EgoHumans and Human3.6M demonstrate substantial improvements over prior calibration-free methods. Compared to previous state-of-the-art approaches, Kineo reduces camera translation error by approximately 83-85%, camera angular error by 86-92%, and world mean-per-joint error (W-MPJPE) by 83-91%.   Kineo is also efficient in real-world scenarios, processing multi-view sequences faster than their duration in specific configuration (e.g., 36min to process 1h20min of footage). The full pipeline and evaluation code are openly released to promote reproducibility and practical adoption at https://liris-xr.github.io/kineo/.",
        "translated": "无标记多视角动作捕捉通常受限于精确相机标定的需求，这限制了非专家用户和野外场景下的应用可及性。现有的无标定方法虽缓解了这一需求，但往往计算成本高且重建精度降低。\n\n我们提出 Kineo，一种完全自动、无需标定的无标记动作捕捉流水线，可从未同步、未标定的消费级 RGB 相机所拍摄的视频中实现动作捕捉。Kineo 利用现成检测器提取的 2D 关键点，同时完成相机标定（包括 Brown-Conrady 畸变系数），并以度量尺度重建 3D 关键点和稠密场景点图。一种基于置信度驱动的时空关键点采样策略，结合基于图的全局优化方法，保证了在固定计算成本下、与序列长度无关的鲁棒标定。我们进一步引入了一种成对重投影一致性得分（pairwise reprojection consensus score），用于量化 3D 重建的可靠性，以服务于下游任务。\n\n在 EgoHumans 和 Human3.6M 数据集上的评估表明，Kineo 相较于现有无标定方法取得了显著提升。与之前最先进的方法相比，Kineo 将相机平移误差降低约 83–85%，相机角度误差降低 86–92%，世界坐标系下的关节平均误差（W-MPJPE）降低 83–91%。\n\nKineo 在真实场景中也表现出高效性，在特定配置下处理多视角序列的速度快于视频时长（例如，36 分钟处理 1 小时 20 分钟的视频）。完整的流水线和评估代码已公开发布，以促进可复现性和实际应用，详见 https://liris-xr.github.io/kineo/。",
        "translated_title": "Kineo：基于稀疏RGB相机的无需标定的度量级运动捕捉",
        "label": [],
        "label_reason": "论文聚焦3D运动捕捉，属于高阶视觉任务，不涉及图像像素级恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出新型无标定运动捕捉框架，优化校准与重建流程，具有显著技术改进。"
    },
    {
        "title": "A Critical Study towards the Detection of Parkinsons Disease using ML\n  Technologies",
        "url": "http://arxiv.org/abs/2510.24456v1",
        "pub_date": "2025-10-28",
        "summary": "The proposed solution is Deep Learning Technique that will be able classify three types of tea leaves diseases from which two diseases are caused by the pests and one due to pathogens (infectious organisms) and environmental conditions and also show the area damaged by a disease in leaves. Namely Red Rust, Helopeltis and Red spider mite respectively. In this paper we have evaluated two models namely SSD MobileNet V2 and Faster R-CNN ResNet50 V1 for the object detection. The SSD MobileNet V2 gave precision of 0.209 for IOU range of 0.50:0.95 with recall of 0.02 on IOU 0.50:0.95 and final mAP of 20.9%. While Faster R-CNN ResNet50 V1 has precision of 0.252 on IOU range of 0.50:0.95 and recall of 0.044 on IOU of 0.50:0.95 with a mAP of 25%, which is better than SSD. Also used Mask R-CNN for Object Instance Segmentation where we have implemented our custom method to calculate the damaged diseased portion of leaves. Keywords: Tea Leaf Disease, Deep Learning, Red Rust, Helopeltis and Red Spider Mite, SSD MobileNet V2, Faster R-CNN ResNet50 V1 and Mask RCNN.",
        "translated": "本研究提出了一种深度学习方法，能够对三种茶叶病害进行分类，其中两种由害虫引起，一种由病原体（感染性生物）及环境因素导致，并可显示叶片上病害所造成的受损区域。这三种病害分别为红锈病、Helopeltis病害和红蜘蛛螨病。本文评估了两种目标检测模型，即SSD MobileNet V2和Faster R-CNN ResNet50 V1。SSD MobileNet V2在IOU范围0.50:0.95下达到0.209的精确率，召回率为0.02，最终mAP为20.9%。Faster R-CNN ResNet50 V1在相同IOU范围内精确率为0.252，召回率为0.044，mAP为25%，优于SSD模型。此外，还采用Mask R-CNN进行实例分割，实现了自定义方法以计算叶片上病害受损区域。关键词：茶叶病害、深度学习、红锈病、Helopeltis病害、红蜘蛛螨病、SSD MobileNet V2、Faster R-CNN ResNet50 V1、Mask R-CNN。",
        "translated_title": "一项基于机器学习技术的帕金森病检测关键研究",
        "label": [],
        "label_reason": "论文聚焦于茶树叶病检测与实例分割，属于高阶目标检测与语义理解任务，非像素级图像恢复。",
        "relevance_score": 1,
        "novelty_score": 2,
        "novelty_reason": "方法为常规目标检测模型应用，无显著创新，仅在特定数据集上评估性能。"
    },
    {
        "title": "Rethinking Visual Intelligence: Insights from Video Pretraining",
        "url": "http://arxiv.org/abs/2510.24448v1",
        "pub_date": "2025-10-28",
        "summary": "Large language models (LLMs) have demonstrated that large-scale pretraining enables systems to adapt rapidly to new problems with little supervision in the language domain. This success, however, has not translated as effectively to the visual domain, where models, including LLMs, continue to struggle with compositional understanding, sample efficiency, and general-purpose problem-solving. We investigate Video Diffusion Models (VDMs) as a promising direction for bridging this gap. Pretraining on spatiotemporal data endows these models with strong inductive biases for structure and dynamics, which we hypothesize can support broad task adaptability. To test this, we design a controlled evaluation in which both a pretrained LLM and a pretrained VDM are equipped with lightweight adapters and presented with tasks in their natural modalities. Across benchmarks including ARC-AGI, ConceptARC, visual games, route planning, and cellular automata, VDMs demonstrate higher data efficiency than their language counterparts. Taken together, our results indicate that video pretraining offers inductive biases that support progress toward visual foundation models.",
        "translated": "大型语言模型（LLMs）已证明，大规模预训练能够使系统在语言领域中仅需少量监督即可快速适应新问题。然而，这一成功尚未在视觉领域得到有效迁移，包括LLMs在内的模型在组合理解、样本效率以及通用问题求解方面仍面临挑战。我们探讨视频扩散模型（VDMs）作为弥合这一差距的有前景方向。在时空数据上进行预训练赋予这些模型对结构和动态的强烈归纳偏置，我们假设这可以支持广泛的 task 适应能力。为验证这一假设，我们设计了一项受控评估实验，其中预训练的LLM和预训练的VDM均配备轻量级适配器，并在各自自然模态下执行任务。在ARC-AGI、ConceptARC、视觉游戏、路径规划和细胞自动机等基准任务中，VDMs展现出优于语言模型的数据效率。综合来看，我们的结果表明，视频预训练所提供的归纳偏置有助于推动视觉基础模型的发展。",
        "translated_title": "重思视觉智能：来自视频预训练的洞见",
        "label": [],
        "label_reason": "论文聚焦视频预训练与视觉基础模型，属于高层视觉理解，非像素级图像恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "提出视频扩散模型用于视觉任务适应性，但未涉及low-level图像处理创新。"
    },
    {
        "title": "SPARTA: Evaluating Reasoning Segmentation Robustness through Black-Box\n  Adversarial Paraphrasing in Text Autoencoder Latent Space",
        "url": "http://arxiv.org/abs/2510.24446v1",
        "pub_date": "2025-10-28",
        "summary": "Multimodal large language models (MLLMs) have shown impressive capabilities in vision-language tasks such as reasoning segmentation, where models generate segmentation masks based on textual queries. While prior work has primarily focused on perturbing image inputs, semantically equivalent textual paraphrases-crucial in real-world applications where users express the same intent in varied ways-remain underexplored. To address this gap, we introduce a novel adversarial paraphrasing task: generating grammatically correct paraphrases that preserve the original query meaning while degrading segmentation performance. To evaluate the quality of adversarial paraphrases, we develop a comprehensive automatic evaluation protocol validated with human studies. Furthermore, we introduce SPARTA-a black-box, sentence-level optimization method that operates in the low-dimensional semantic latent space of a text autoencoder, guided by reinforcement learning. SPARTA achieves significantly higher success rates, outperforming prior methods by up to 2x on both the ReasonSeg and LLMSeg-40k datasets. We use SPARTA and competitive baselines to assess the robustness of advanced reasoning segmentation models. We reveal that they remain vulnerable to adversarial paraphrasing-even under strict semantic and grammatical constraints. All code and data will be released publicly upon acceptance.",
        "translated": "多模态大语言模型（MLLMs）在视觉-语言任务（如推理分割）中展现了令人印象深刻的能力，模型能够根据文本查询生成分割掩码。尽管先前的研究主要集中在扰动图像输入方面，但语义等价的文本改写——在现实应用中至关重要，因为用户常以不同方式表达相同意图——却尚未得到充分探索。为弥补这一空白，我们提出一种新颖的对抗性改写任务：生成语法正确的文本改写，使其在保持原始查询语义的同时降低分割性能。为评估对抗性改写的质量，我们开发了一套全面的自动评估协议，并通过人工实验进行了验证。此外，我们提出SPARTA——一种基于文本自编码器低维语义潜在空间的黑盒、句子级优化方法，该方法由强化学习引导。SPARTA在ReasonSeg和LLMSeg-40k数据集上均实现了显著更高的成功 rate，相较于先前方法最高提升达2倍。我们利用SPARTA及竞争性基线方法评估了先进推理分割模型的鲁棒性，结果表明，即使在严格的语义和语法约束下，这些模型仍易受到对抗性改写的影响。所有代码和数据将在论文被接受后公开发布。",
        "translated_title": "SPARTA：通过文本自编码器潜在空间中的黑盒对抗性重述评估推理分割的鲁棒性",
        "label": [],
        "label_reason": "论文聚焦于多模态大模型的文本查询鲁棒性，属于high-level视觉任务，非像素级图像处理。",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出在文本自编码器潜空间中进行黑盒对抗重述，方法新颖，但不涉及图像恢复或增强。"
    },
    {
        "title": "VFXMaster: Unlocking Dynamic Visual Effect Generation via In-Context\n  Learning",
        "url": "http://arxiv.org/abs/2510.25772v1",
        "pub_date": "2025-10-29",
        "summary": "Visual effects (VFX) are crucial to the expressive power of digital media, yet their creation remains a major challenge for generative AI. Prevailing methods often rely on the one-LoRA-per-effect paradigm, which is resource-intensive and fundamentally incapable of generalizing to unseen effects, thus limiting scalability and creation. To address this challenge, we introduce VFXMaster, the first unified, reference-based framework for VFX video generation. It recasts effect generation as an in-context learning task, enabling it to reproduce diverse dynamic effects from a reference video onto target content. In addition, it demonstrates remarkable generalization to unseen effect categories. Specifically, we design an in-context conditioning strategy that prompts the model with a reference example. An in-context attention mask is designed to precisely decouple and inject the essential effect attributes, allowing a single unified model to master the effect imitation without information leakage. In addition, we propose an efficient one-shot effect adaptation mechanism to boost generalization capability on tough unseen effects from a single user-provided video rapidly. Extensive experiments demonstrate that our method effectively imitates various categories of effect information and exhibits outstanding generalization to out-of-domain effects. To foster future research, we will release our code, models, and a comprehensive dataset to the community.",
        "translated": "视觉特效（VFX）对于数字媒体的表现力至关重要，但其生成仍是生成式人工智能面临的主要挑战。现有方法通常依赖于“每种特效一个LoRA”的范式，该方法资源消耗大，且本质上无法泛化到未见过的特效，从而限制了系统的可扩展性和创作能力。为应对这一挑战，我们提出VFXMaster，这是首个统一的、基于参考的视觉特效视频生成框架。该框架将特效生成重构为一种上下文学习任务，使其能够从参考视频中提取并复现多样化的动态特效到目标内容上。此外，该框架展现出对未见过特效类别的显著泛化能力。具体而言，我们设计了一种上下文条件策略，通过参考示例引导模型生成。我们还设计了一种上下文注意力掩码，以精确地解耦并注入关键的特效属性，使单个统一模型能够在无信息泄露的情况下掌握特效模仿能力。此外，我们提出了一种高效的单次特效适配机制，能够快速利用用户提供的单个视频，增强模型对复杂未见特效的泛化能力。大量实验表明，我们的方法能够有效模仿多种类别的特效信息，并在跨领域特效上展现出卓越的泛化性能。为促进后续研究，我们将向社区开放代码、模型以及一个全面的数据集。",
        "translated_title": "VFXMaster：通过上下文学习解锁动态视觉效果生成",
        "label": [],
        "label_reason": "论文聚焦于动态视觉效果生成，属于图像生成类任务，非像素级图像恢复或增强，不属low-level。",
        "relevance_score": 1,
        "novelty_score": 6,
        "novelty_reason": "提出in-context学习框架与注意力掩码，具一定创新，但属生成任务，非图像复原领域核心突破。"
    },
    {
        "title": "FreeArt3D: Training-Free Articulated Object Generation using 3D\n  Diffusion",
        "url": "http://arxiv.org/abs/2510.25765v1",
        "pub_date": "2025-10-29",
        "summary": "Articulated 3D objects are central to many applications in robotics, AR/VR, and animation. Recent approaches to modeling such objects either rely on optimization-based reconstruction pipelines that require dense-view supervision or on feed-forward generative models that produce coarse geometric approximations and often overlook surface texture. In contrast, open-world 3D generation of static objects has achieved remarkable success, especially with the advent of native 3D diffusion models such as Trellis. However, extending these methods to articulated objects by training native 3D diffusion models poses significant challenges. In this work, we present FreeArt3D, a training-free framework for articulated 3D object generation. Instead of training a new model on limited articulated data, FreeArt3D repurposes a pre-trained static 3D diffusion model (e.g., Trellis) as a powerful shape prior. It extends Score Distillation Sampling (SDS) into the 3D-to-4D domain by treating articulation as an additional generative dimension. Given a few images captured in different articulation states, FreeArt3D jointly optimizes the object's geometry, texture, and articulation parameters without requiring task-specific training or access to large-scale articulated datasets. Our method generates high-fidelity geometry and textures, accurately predicts underlying kinematic structures, and generalizes well across diverse object categories. Despite following a per-instance optimization paradigm, FreeArt3D completes in minutes and significantly outperforms prior state-of-the-art approaches in both quality and versatility.",
        "translated": "可动三维物体在机器人、增强现实/虚拟现实（AR/VR）以及动画等众多应用中具有核心地位。当前针对此类物体的建模方法主要分为两类：一类依赖于基于优化的重建流程，这类方法需要密集视角的监督；另一类则采用前馈生成模型，往往只能生成粗糙的几何近似，并且常忽略表面纹理细节。相比之下，静态三维物体的开放世界生成已取得显著成功，尤其是在原生三维扩散模型（如 Trellis）出现后。然而，将这些方法扩展至可动物体，通过训练原生三维扩散模型，仍面临诸多挑战。在本文中，我们提出 FreeArt3D，一种无需训练的可动三维物体生成框架。FreeArt3D 并非在有限的可动数据上训练新模型，而是复用预训练的静态三维扩散模型（例如 Trellis）作为强大的形状先验。它通过将可动性视为额外的生成维度，将 Score Distillation Sampling（SDS）方法扩展至三维到四维（3D-to-4D）领域。给定在不同可动状态下采集的少量图像，FreeArt3D 能够联合优化物体的几何结构、纹理以及可动参数，无需针对特定任务进行训练，也无需访问大规模可动数据集。我们的方法能够生成高保真度的几何结构与纹理，准确预测底层运动学结构，并在多种物体类别上表现出良好的泛化能力。尽管采用逐实例优化范式，FreeArt3D 仍能在数分钟内完成，且在生成质量与适用性方面显著优于现有最先进方法。",
        "translated_title": "FreeArt3D：基于3D扩散模型的无训练关节物体生成",
        "label": [],
        "label_reason": "论文聚焦3D生成与运动建模，属于高阶视觉任务，非像素级图像恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出将3D扩散模型扩展至4D领域，引入新采样框架，对生成方法有显著改进。"
    },
    {
        "title": "Multimodal Spatial Reasoning in the Large Model Era: A Survey and\n  Benchmarks",
        "url": "http://arxiv.org/abs/2510.25760v1",
        "pub_date": "2025-10-29",
        "summary": "Humans possess spatial reasoning abilities that enable them to understand spaces through multimodal observations, such as vision and sound. Large multimodal reasoning models extend these abilities by learning to perceive and reason, showing promising performance across diverse spatial tasks. However, systematic reviews and publicly available benchmarks for these models remain limited. In this survey, we provide a comprehensive review of multimodal spatial reasoning tasks with large models, categorizing recent progress in multimodal large language models (MLLMs) and introducing open benchmarks for evaluation. We begin by outlining general spatial reasoning, focusing on post-training techniques, explainability, and architecture. Beyond classical 2D tasks, we examine spatial relationship reasoning, scene and layout understanding, as well as visual question answering and grounding in 3D space. We also review advances in embodied AI, including vision-language navigation and action models. Additionally, we consider emerging modalities such as audio and egocentric video, which contribute to novel spatial understanding through new sensors. We believe this survey establishes a solid foundation and offers insights into the growing field of multimodal spatial reasoning. Updated information about this survey, codes and implementation of the open benchmarks can be found at https://github.com/zhengxuJosh/Awesome-Spatial-Reasoning.",
        "translated": "人类具备空间推理能力，能够通过视觉、听觉等多模态观测来理解空间环境。大型多模态推理模型通过学习感知与推理能力，进一步拓展了这种能力，在多种空间任务中展现出优异的性能。然而，针对这些模型的系统性综述和公开可用的基准测试仍较为有限。在本文综述中，我们全面回顾了基于大模型的多模态空间推理任务，对近期多模态大语言模型（MLLMs）的进展进行了分类，并介绍了可用于评估的开放基准。我们首先概述了通用空间推理的基本框架，重点探讨了后训练技术、可解释性及模型架构。除传统的二维空间任务外，我们还研究了空间关系推理、场景与布局理解，以及三维空间中的视觉问答与定位任务。此外，我们回顾了具身人工智能（embodied AI）领域的最新进展，包括视觉-语言导航与动作模型。同时，我们也考察了音频、第一人称视频等新兴模态，这些模态通过新型传感器为实现新颖的空间理解提供了可能。我们认为，本综述为多模态空间推理这一快速发展领域奠定了坚实基础，并提供了有价值的见解。关于本综述的最新信息、代码及开放基准的实现，可访问 https://github.com/zhengxuJosh/Awesome-Spatial-Reasoning。",
        "translated_title": "多模态空间推理在大模型时代：综述与基准测试",
        "label": [],
        "label_reason": "论文聚焦多模态空间推理与大模型，属高阶视觉理解任务，不涉及图像像素级恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 3,
        "novelty_reason": "为综述性工作，整理现有方法与基准，无新架构或算法创新。"
    },
    {
        "title": "Hawk: Leveraging Spatial Context for Faster Autoregressive Text-to-Image\n  Generation",
        "url": "http://arxiv.org/abs/2510.25739v1",
        "pub_date": "2025-10-29",
        "summary": "Autoregressive (AR) image generation models are capable of producing high-fidelity images but often suffer from slow inference due to their inherently sequential, token-by-token decoding process. Speculative decoding, which employs a lightweight draft model to approximate the output of a larger AR model, has shown promise in accelerating text generation without compromising quality. However, its application to image generation remains largely underexplored. The challenges stem from a significantly larger sampling space, which complicates the alignment between the draft and target model outputs, coupled with the inadequate use of the two-dimensional spatial structure inherent in images, thereby limiting the modeling of local dependencies. To overcome these challenges, we introduce Hawk, a new approach that harnesses the spatial structure of images to guide the speculative model toward more accurate and efficient predictions. Experimental results on multiple text-to-image benchmarks demonstrate a 1.71x speedup over standard AR models, while preserving both image fidelity and diversity.",
        "translated": "自回归（AR）图像生成模型能够生成高保真图像，但由于其本质上逐 token 顺序解码的过程，通常存在推理速度较慢的问题。推测解码（speculative decoding）通过使用轻量级草稿模型来近似较大 AR 模型的输出，在加速文本生成的同时保持生成质量方面已展现出潜力。然而，该方法在图像生成中的应用仍处于初步探索阶段。其主要挑战源于采样空间显著增大，这使得草稿模型与目标模型输出之间的对齐变得复杂，同时现有方法未能充分利用图像固有的二维空间结构，从而限制了局部依赖关系的建模能力。为应对这些挑战，我们提出 Hawk，一种新的方法，该方法利用图像的空间结构来引导推测模型实现更准确且高效的预测。在多个文本到图像基准数据集上的实验结果表明，与标准 AR 模型相比，Hawk 实现了 1.71 倍的推理速度提升，同时保持了图像的保真度和多样性。",
        "translated_title": "Hawk：利用空间上下文实现更快的自回归文本到图像生成",
        "label": [],
        "label_reason": "论文聚焦文本到图像生成，属于图像生成任务，非像素级图像恢复或增强，不属low-level。",
        "relevance_score": 1,
        "novelty_score": 6,
        "novelty_reason": "提出空间上下文指导的推测解码，提升生成效率，属方法改进但非low-level创新。"
    },
    {
        "title": "Feedback Alignment Meets Low-Rank Manifolds: A Structured Recipe for\n  Local Learning",
        "url": "http://arxiv.org/abs/2510.25594v1",
        "pub_date": "2025-10-29",
        "summary": "Training deep neural networks (DNNs) with backpropagation (BP) achieves state-of-the-art accuracy but requires global error propagation and full parameterization, leading to substantial memory and computational overhead. Direct Feedback Alignment (DFA) enables local, parallelizable updates with lower memory requirements but is limited by unstructured feedback and poor scalability in deeper architectures, specially convolutional neural networks. To address these limitations, we propose a structured local learning framework that operates directly on low-rank manifolds defined by the Singular Value Decomposition (SVD) of weight matrices. Each layer is trained in its decomposed form, with updates applied to the SVD components using a composite loss that integrates cross-entropy, subspace alignment, and orthogonality regularization. Feedback matrices are constructed to match the SVD structure, ensuring consistent alignment between forward and feedback pathways. Our method reduces the number of trainable parameters relative to the original DFA model, without relying on pruning or post hoc compression. Experiments on CIFAR-10, CIFAR-100, and ImageNet show that our method achieves accuracy comparable to that of BP. Ablation studies confirm the importance of each loss term in the low-rank setting. These results establish local learning on low-rank manifolds as a principled and scalable alternative to full-rank gradient-based training.",
        "translated": "训练深度神经网络（DNNs）采用反向传播（BP）方法可达到最先进的准确率，但需要全局误差传播和完整的参数化，导致显著的内存和计算开销。直接反馈对齐（DFA）能够实现局部、可并行化的更新，且内存需求较低，但受限于无结构的反馈信号以及在更深架构（特别是卷积神经网络）中扩展性较差的问题。为克服这些局限，我们提出一种结构化的局部学习框架，该框架直接在由权重矩阵奇异值分解（SVD）定义的低秩流形上运行。每一层均以分解形式进行训练，通过结合交叉熵、子空间对齐和正交性正则化的复合损失函数，对SVD各分量进行更新。反馈矩阵被构建为与SVD结构相匹配，确保前向与反馈路径之间的一致对齐。我们的方法相对于原始DFA模型减少了可训练参数数量，且无需依赖剪枝或后处理压缩。在CIFAR-10、CIFAR-100和ImageNet上的实验表明，我们的方法实现了与BP相当的准确率。消融研究验证了在低秩设置下每个损失项的重要性。这些结果确立了在低秩流形上的局部学习，作为全秩梯度训练的一种原理性且可扩展的替代方案。",
        "translated_title": "反馈对齐与低秩流形：局部学习的结构化方案",
        "label": [],
        "label_reason": "论文聚焦于神经网络训练机制优化，属于模型训练方法研究，不涉及图像像素级恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出基于SVD的低秩结构化反馈对齐，改进DFA训练效率，具有方法创新性。"
    },
    {
        "title": "RegionE: Adaptive Region-Aware Generation for Efficient Image Editing",
        "url": "http://arxiv.org/abs/2510.25590v1",
        "pub_date": "2025-10-29",
        "summary": "Recently, instruction-based image editing (IIE) has received widespread attention. In practice, IIE often modifies only specific regions of an image, while the remaining areas largely remain unchanged. Although these two types of regions differ significantly in generation difficulty and computational redundancy, existing IIE models do not account for this distinction, instead applying a uniform generation process across the entire image. This motivates us to propose RegionE, an adaptive, region-aware generation framework that accelerates IIE tasks without additional training. Specifically, the RegionE framework consists of three main components: 1) Adaptive Region Partition. We observed that the trajectory of unedited regions is straight, allowing for multi-step denoised predictions to be inferred in a single step. Therefore, in the early denoising stages, we partition the image into edited and unedited regions based on the difference between the final estimated result and the reference image. 2) Region-Aware Generation. After distinguishing the regions, we replace multi-step denoising with one-step prediction for unedited areas. For edited regions, the trajectory is curved, requiring local iterative denoising. To improve the efficiency and quality of local iterative generation, we propose the Region-Instruction KV Cache, which reduces computational cost while incorporating global information. 3) Adaptive Velocity Decay Cache. Observing that adjacent timesteps in edited regions exhibit strong velocity similarity, we further propose an adaptive velocity decay cache to accelerate the local denoising process. We applied RegionE to state-of-the-art IIE base models, including Step1X-Edit, FLUX.1 Kontext, and Qwen-Image-Edit. RegionE achieved acceleration factors of 2.57, 2.41, and 2.06. Evaluations by GPT-4o confirmed that semantic and perceptual fidelity were well preserved.",
        "translated": "近年来，基于指令的图像编辑（IIE）受到了广泛关注。在实际应用中，IIE通常仅修改图像的特定区域，而其余区域基本保持不变。尽管这两类区域在生成难度和计算冗余方面存在显著差异，但现有的IIE模型并未考虑这种区别，而是对整幅图像采用统一的生成流程。这一现象促使我们提出RegionE，一种自适应的、具备区域感知能力的生成框架，能够在无需额外训练的前提下加速IIE任务。具体而言，RegionE框架包含三个主要组件：1）自适应区域划分。我们观察到未编辑区域的去噪轨迹呈直线，因此可以在单步内推断出多步去噪预测结果。因此，在早期去噪阶段，我们基于最终估计结果与参考图像之间的差异，将图像划分为编辑区域和未编辑区域。2）区域感知生成。在区分区域后，我们对未编辑区域采用单步预测替代多步去噪；对于编辑区域，其轨迹呈曲线，需进行局部迭代去噪。为提升局部迭代生成的效率与质量，我们提出区域-指令KV缓存（Region-Instruction KV Cache），在降低计算成本的同时融合全局信息。3）自适应速度衰减缓存。我们观察到编辑区域内相邻时间步具有较强的速度相似性，因此进一步提出自适应速度衰减缓存，以加速局部去噪过程。我们将RegionE应用于当前最先进的IIE基础模型，包括Step1X-Edit、FLUX.1 Kontext和Qwen-Image-Edit。RegionE实现了2.57、2.41和2.06的加速倍数。经GPT-4o评估，语义和感知保真度均得到良好保持。",
        "translated_title": "RegionE：面向高效图像编辑的自适应区域感知生成",
        "label": [],
        "label_reason": "论文聚焦图像编辑，属于高阶视觉任务，非像素级图像恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出区域感知生成框架，优化扩散模型推理效率，具一定创新性。"
    },
    {
        "title": "Comparative Study of UNet-based Architectures for Liver Tumor\n  Segmentation in Multi-Phase Contrast-Enhanced Computed Tomography",
        "url": "http://arxiv.org/abs/2510.25522v1",
        "pub_date": "2025-10-29",
        "summary": "Segmentation of liver structures in multi-phase contrast-enhanced computed tomography (CECT) plays a crucial role in computer-aided diagnosis and treatment planning for liver diseases, including tumor detection. In this study, we investigate the performance of UNet-based architectures for liver tumor segmentation, starting from the original UNet and extending to UNet3+ with various backbone networks. We evaluate ResNet, Transformer-based, and State-space (Mamba) backbones, all initialized with pretrained weights. Surprisingly, despite the advances in modern architecture, ResNet-based models consistently outperform Transformer- and Mamba-based alternatives across multiple evaluation metrics. To further improve segmentation quality, we introduce attention mechanisms into the backbone and observe that incorporating the Convolutional Block Attention Module (CBAM) yields the best performance. ResNetUNet3+ with CBAM module not only produced the best overlap metrics with a Dice score of 0.755 and IoU of 0.662, but also achieved the most precise boundary delineation, evidenced by the lowest HD95 distance of 77.911. The model's superiority was further cemented by its leading overall accuracy of 0.925 and specificity of 0.926, showcasing its robust capability in accurately identifying both lesion and healthy tissue. To further enhance interpretability, Grad-CAM visualizations were employed to highlight the region's most influential predictions, providing insights into its decision-making process. These findings demonstrate that classical ResNet architecture, when combined with modern attention modules, remain highly competitive for medical image segmentation tasks, offering a promising direction for liver tumor detection in clinical practice.",
        "translated": "在多期增强计算机断层扫描（CECT）中对肝脏结构进行分割，在肝病（包括肿瘤检测）的计算机辅助诊断与治疗规划中具有重要作用。本研究探讨了基于UNet架构的肝脏肿瘤分割性能，从原始UNet出发，扩展至UNet3+并结合多种骨干网络。我们评估了ResNet、基于Transformer以及状态空间（Mamba）的骨干网络，所有模型均采用预训练权重进行初始化。令人意外的是，尽管现代架构取得显著进展，ResNet基模型在多个评估指标上始终优于Transformer和Mamba基模型。为进一步提升分割质量，我们在骨干网络中引入注意力机制，并观察到集成卷积块注意力模块（CBAM）可获得最佳性能。带有CBAM模块的ResNetUNet3+不仅在重叠度指标上表现最佳（Dice得分为0.755，IoU为0.662），而且实现了最精确的边界勾勒，其HD95距离最低，为77.911。该模型在整体准确率（0.925）和特异性（0.926）方面也位居首位，展现出其在准确识别病灶和健康组织方面的强大鲁棒性。为进一步增强可解释性，我们采用Grad-CAM可视化技术，突出显示对预测最具影响力的区域，从而揭示模型的决策过程。这些结果表明，经典的ResNet架构在结合现代注意力模块后，在医学图像分割任务中仍具有高度竞争力，为临床实践中肝脏肿瘤检测提供了有前景的方向。",
        "translated_title": "基于U-Net架构的多期增强CT肝肿瘤分割对比研究",
        "label": [],
        "label_reason": "论文聚焦肝脏肿瘤分割，属于医学图像分析中的高阶语义理解任务，非像素级图像恢复。",
        "relevance_score": 1,
        "novelty_score": 4,
        "novelty_reason": "对UNet变体进行比较，引入CBAM，属常规改进，无根本性创新。"
    },
    {
        "title": "FaCT: Faithful Concept Traces for Explaining Neural Network Decisions",
        "url": "http://arxiv.org/abs/2510.25512v1",
        "pub_date": "2025-10-29",
        "summary": "Deep networks have shown remarkable performance across a wide range of tasks, yet getting a global concept-level understanding of how they function remains a key challenge. Many post-hoc concept-based approaches have been introduced to understand their workings, yet they are not always faithful to the model. Further, they make restrictive assumptions on the concepts a model learns, such as class-specificity, small spatial extent, or alignment to human expectations. In this work, we put emphasis on the faithfulness of such concept-based explanations and propose a new model with model-inherent mechanistic concept-explanations. Our concepts are shared across classes and, from any layer, their contribution to the logit and their input-visualization can be faithfully traced. We also leverage foundation models to propose a new concept-consistency metric, C$^2$-Score, that can be used to evaluate concept-based methods. We show that, compared to prior work, our concepts are quantitatively more consistent and users find our concepts to be more interpretable, all while retaining competitive ImageNet performance.",
        "translated": "深度网络在广泛的任务中展现了卓越的性能，然而对其工作机理进行全局层面的概念性理解仍是一项关键挑战。为了解释其工作机制，已有大量基于事后概念分析的方法被提出，但这些方法并不总能忠实反映模型的真实行为。此外，它们通常对模型所学习的概念做出严格假设，例如类别特异性、空间范围较小或与人类直觉一致等。在本研究中，我们着重于概念解释的忠实性，提出一种具有内在机制性概念解释能力的新模型。我们所定义的概念在各类别之间共享，且从任意网络层出发，均可准确追溯其对logit的贡献以及输入可视化结果。同时，我们借助基础模型提出一种新的概念一致性度量指标C²-Score，可用于评估各类基于概念的解释方法。实验表明，与现有方法相比，我们的概念在量化上具有更高的一致性，用户也认为其更具可解释性，同时在ImageNet上仍保持了具有竞争力的性能。",
        "translated_title": "FaCT：用于解释神经网络决策的忠实概念轨迹",
        "label": [],
        "label_reason": "论文聚焦模型解释性，属于高阶视觉任务，非像素级图像处理",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出可追溯概念解释机制，引入C²-Score评估指标，具有创新性"
    },
    {
        "title": "SPADE: Sparsity Adaptive Depth Estimator for Zero-Shot, Real-Time,\n  Monocular Depth Estimation in Underwater Environments",
        "url": "http://arxiv.org/abs/2510.25463v1",
        "pub_date": "2025-10-29",
        "summary": "Underwater infrastructure requires frequent inspection and maintenance due to harsh marine conditions. Current reliance on human divers or remotely operated vehicles is limited by perceptual and operational challenges, especially around complex structures or in turbid water. Enhancing the spatial awareness of underwater vehicles is key to reducing piloting risks and enabling greater autonomy. To address these challenges, we present SPADE: SParsity Adaptive Depth Estimator, a monocular depth estimation pipeline that combines pre-trained relative depth estimator with sparse depth priors to produce dense, metric scale depth maps. Our two-stage approach first scales the relative depth map with the sparse depth points, then refines the final metric prediction with our proposed Cascade Conv-Deformable Transformer blocks. Our approach achieves improved accuracy and generalisation over state-of-the-art baselines and runs efficiently at over 15 FPS on embedded hardware, promising to support practical underwater inspection and intervention. This work has been submitted to IEEE Journal of Oceanic Engineering Special Issue of AUV 2026.",
        "translated": "水下基础设施因恶劣的海洋环境需频繁进行巡检与维护。目前依赖人力潜水员或遥控水下航行器（ROV）的方式受限于感知与操作方面的挑战，尤其在复杂结构周围或浑浊水域中表现尤为明显。提升水下航行器的空间感知能力，对于降低操控风险、实现更高程度的自主性至关重要。为应对上述挑战，本文提出 SPADE：SParsity Adaptive Depth Estimator，一种单目深度估计框架，通过融合预训练的相对深度估计器与稀疏深度先验，生成稠密且具备度量尺度的深度图。我们的两阶段方法首先利用稀疏深度点对相对深度图进行尺度校准，随后通过所提出的级联卷积-可变形Transformer模块对最终的度量深度预测进行精细化优化。该方法在准确性和泛化能力上均优于当前主流基线方法，且在嵌入式硬件上可实现超过15 FPS的高效运行，有望支持实际水下巡检与干预任务。本工作已投稿至 IEEE Journal of Oceanic Engineering AUV 2026 特刊。",
        "translated_title": "SPADE：用于水下环境中零样本、实时、单目深度估计的稀疏自适应深度估计算法",
        "label": [],
        "label_reason": "核心为单目深度估计，属于3D感知任务，非像素级图像恢复或增强，不属low-level。",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出级联卷积-可变形Transformer块，结构有改进，但未解决图像质量退化问题。"
    },
    {
        "title": "More than a Moment: Towards Coherent Sequences of Audio Descriptions",
        "url": "http://arxiv.org/abs/2510.25440v1",
        "pub_date": "2025-10-29",
        "summary": "Audio Descriptions (ADs) convey essential on-screen information, allowing visually impaired audiences to follow videos. To be effective, ADs must form a coherent sequence that helps listeners to visualise the unfolding scene, rather than describing isolated moments. However, most automatic methods generate each AD independently, often resulting in repetitive, incoherent descriptions. To address this, we propose a training-free method, CoherentAD, that first generates multiple candidate descriptions for each AD time interval, and then performs auto-regressive selection across the sequence to form a coherent and informative narrative. To evaluate AD sequences holistically, we introduce a sequence-level metric, StoryRecall, which measures how well the predicted ADs convey the ground truth narrative, alongside repetition metrics that capture the redundancy across consecutive AD outputs. Our method produces coherent AD sequences with enhanced narrative understanding, outperforming prior approaches that rely on independent generations.",
        "translated": "音频描述（Audio Descriptions, ADs）能够传递画面中的关键信息，使视障观众得以理解和跟随视频内容。为实现有效传达，音频描述必须构成一个连贯的序列，帮助听者构建场景的动态发展，而非孤立地描述零散时刻。然而，大多数自动方法独立生成每个音频描述片段，往往导致重复且不连贯的描述。为解决这一问题，我们提出一种无需训练的方法 CoherentAD，该方法首先为每个音频描述时间区间生成多个候选描述，然后在序列中进行自回归选择，以形成连贯且信息丰富的叙事。为全面评估音频描述序列，我们引入了一种序列级评价指标 StoryRecall，该指标衡量预测的音频描述在多大程度上传达了真实叙事内容，同时结合重复性指标以捕捉连续音频描述输出中的冗余程度。我们的方法生成的音频描述序列具有更强的连贯性与叙事理解能力，在无需依赖独立生成的先验方法中表现更优。",
        "translated_title": "超越瞬间：迈向连贯的音频描述序列",
        "label": [],
        "label_reason": "论文聚焦音频描述生成与序列连贯性，属于高阶视频理解任务，非像素级图像处理。",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "提出序列级生成与评估方法，但属自然语言生成范畴，无图像恢复创新。"
    },
    {
        "title": "Instance-Level Composed Image Retrieval",
        "url": "http://arxiv.org/abs/2510.25387v1",
        "pub_date": "2025-10-29",
        "summary": "The progress of composed image retrieval (CIR), a popular research direction in image retrieval, where a combined visual and textual query is used, is held back by the absence of high-quality training and evaluation data. We introduce a new evaluation dataset, i-CIR, which, unlike existing datasets, focuses on an instance-level class definition. The goal is to retrieve images that contain the same particular object as the visual query, presented under a variety of modifications defined by textual queries. Its design and curation process keep the dataset compact to facilitate future research, while maintaining its challenge-comparable to retrieval among more than 40M random distractors-through a semi-automated selection of hard negatives.   To overcome the challenge of obtaining clean, diverse, and suitable training data, we leverage pre-trained vision-and-language models (VLMs) in a training-free approach called BASIC. The method separately estimates query-image-to-image and query-text-to-image similarities, performing late fusion to upweight images that satisfy both queries, while down-weighting those that exhibit high similarity with only one of the two. Each individual similarity is further improved by a set of components that are simple and intuitive. BASIC sets a new state of the art on i-CIR but also on existing CIR datasets that follow a semantic-level class definition. Project page: https://vrg.fel.cvut.cz/icir/.",
        "translated": "组合图像检索（CIR）是图像检索领域的一个热门研究方向，其通过联合使用视觉与文本查询进行图像检索。然而，该方向的进展受到高质量训练与评估数据缺失的制约。我们提出一个新评估数据集 i-CIR，与现有数据集不同，它聚焦于实例级类别定义。其目标是检索出与视觉查询包含相同特定目标的图像，这些图像在由文本查询所定义的多种修改下呈现。该数据集的设计与构建过程通过半自动选择难负样本，保持数据集紧凑，同时维持与在超过40M个随机干扰样本中进行检索相当的挑战性。\n\n为克服获取干净、多样且适合训练数据的挑战，我们采用一种无需训练的策略 BASIC，利用预训练视觉-语言模型（VLMs）。该方法分别估计查询-图像到图像、查询-文本到图像的相似性，通过后期融合策略，提升同时满足两个查询的图像权重，而降低仅与其中一个查询高度相似的图像权重。每个单独的相似性度量进一步通过一组简单直观的组件进行优化。BASIC 在 i-CIR 上达到新的最优性能，同时在遵循语义级类别定义的现有 CIR 数据集上也取得了领先表现。项目主页：https://vrg.fel.cvut.cz/icir/。",
        "translated_title": "实例级合成图像检索",
        "label": [],
        "label_reason": "论文聚焦图像检索，属于high-level视觉任务，不涉及像素级图像质量改善。",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "提出BASIC方法改进检索，但为常规融合策略，无突破性创新。"
    },
    {
        "title": "Prompt Estimation from Prototypes for Federated Prompt Tuning of Vision\n  Transformers",
        "url": "http://arxiv.org/abs/2510.25372v1",
        "pub_date": "2025-10-29",
        "summary": "Visual Prompt Tuning (VPT) of pre-trained Vision Transformers (ViTs) has proven highly effective as a parameter-efficient fine-tuning technique for adapting large models to downstream tasks with limited data. Its parameter efficiency makes it particularly suitable for Federated Learning (FL), where both communication and computation budgets are often constrained. However, global prompt tuning struggles to generalize across heterogeneous clients, while personalized tuning overfits to local data and lacks generalization. We propose PEP-FedPT (Prompt Estimation from Prototypes for Federated Prompt Tuning), a unified framework designed to achieve both generalization and personalization in federated prompt tuning of ViTs. Within this framework, we introduce the novel Class-Contextualized Mixed Prompt (CCMP) - based on class-specific prompts maintained alongside a globally shared prompt. For each input, CCMP adaptively combines class-specific prompts using weights derived from global class prototypes and client class priors. This approach enables per-sample prompt personalization without storing client-dependent trainable parameters. The prompts are collaboratively optimized via traditional federated averaging technique on the same. Comprehensive evaluations on CIFAR-100, TinyImageNet, DomainNet, and iNaturalist datasets demonstrate that PEP-FedPT consistently surpasses the state-of-the-art baselines under diverse data heterogeneity scenarios, establishing a strong foundation for efficient and generalizable federated prompt tuning of Vision Transformers.",
        "translated": "视觉提示调优（Visual Prompt Tuning, VPT）在预训练视觉变换器（Vision Transformers, ViTs）上已被证明是一种高效的参数调优技术，能够以极少的数据将大型模型适配到下游任务。其参数效率使其特别适用于联邦学习（Federated Learning, FL），因为在该场景下通信和计算资源通常受到严格限制。然而，全局提示调优在面对异构客户端时泛化能力较差，而个性化提示调优则容易过拟合本地数据，缺乏泛化能力。我们提出PEP-FedPT（基于原型的提示估计用于联邦提示调优），一种统一框架，旨在实现ViTs联邦提示调优中的泛化与个性化双重目标。在此框架中，我们引入了新型的类上下文混合提示（Class-Contextualized Mixed Prompt, CCMP），其基于一个全局共享提示与多个类特定提示共同维护。对于每个输入，CCMP通过从全局类原型和客户端类先验中导出的权重，自适应地组合类特定提示。该方法实现了逐样本的提示个性化，而无需存储依赖于客户端的可训练参数。提示通过传统的联邦平均技术在各客户端上协同优化。在CIFAR-100、TinyImageNet、DomainNet和iNaturalist数据集上的全面评估表明，PEP-FedPT在多种数据异构性场景下均显著优于现有最先进的基线方法，为高效且泛化的视觉变换器联邦提示调优奠定了坚实基础。",
        "translated_title": "基于原型的提示估计用于视觉Transformer的联邦提示调优",
        "label": [],
        "label_reason": "论文聚焦于联邦学习中的视觉提示调优，属于高阶视觉任务，不涉及像素级图像恢复或增强。",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出基于原型的提示估计机制，对联邦学习中提示调优有显著改进，但非低层图像处理创新。"
    },
    {
        "title": "3D CT-Based Coronary Calcium Assessment: A Feature-Driven Machine\n  Learning Framework",
        "url": "http://arxiv.org/abs/2510.25347v1",
        "pub_date": "2025-10-29",
        "summary": "Coronary artery calcium (CAC) scoring plays a crucial role in the early detection and risk stratification of coronary artery disease (CAD). In this study, we focus on non-contrast coronary computed tomography angiography (CCTA) scans, which are commonly used for early calcification detection in clinical settings. To address the challenge of limited annotated data, we propose a radiomics-based pipeline that leverages pseudo-labeling to generate training labels, thereby eliminating the need for expert-defined segmentations. Additionally, we explore the use of pretrained foundation models, specifically CT-FM and RadImageNet, to extract image features, which are then used with traditional classifiers. We compare the performance of these deep learning features with that of radiomics features. Evaluation is conducted on a clinical CCTA dataset comprising 182 patients, where individuals are classified into two groups: zero versus non-zero calcium scores. We further investigate the impact of training on non-contrast datasets versus combined contrast and non-contrast datasets, with testing performed only on non contrast scans. Results show that radiomics-based models significantly outperform CNN-derived embeddings from foundation models (achieving 84% accuracy and p&lt;0.05), despite the unavailability of expert annotations.",
        "translated": "冠状动脉钙化（CAC）评分在冠状动脉疾病（CAD）的早期检测和风险分层中起着关键作用。本研究聚焦于临床上常用于早期钙化检测的无对比剂冠状动脉计算机断层扫描血管成像（CCTA）扫描。为应对标注数据有限的挑战，我们提出了一种基于放射组学的流程，利用伪标签生成训练标签，从而无需依赖专家定义的分割结果。此外，我们探索了预训练基础模型（特别是CT-FM和RadImageNet）在提取图像特征方面的应用，随后将这些特征与传统分类器结合使用。我们比较了这些深度学习特征与放射组学特征的性能表现。评估在包含182名患者的临床CCTA数据集上进行，将个体分为两组：钙化评分为零与非零。我们进一步研究了在无对比剂数据集上训练与在对比剂和无对比剂数据集联合训练的影响，所有测试均仅在无对比剂扫描上进行。结果表明，尽管缺乏专家标注，基于放射组学的模型在准确率上显著优于基础模型提取的CNN特征嵌入（达到84%准确率，p<0.05）。",
        "translated_title": "基于3D CT的冠状动脉钙化评估：一种特征驱动的机器学习框架",
        "label": [],
        "label_reason": "论文聚焦于CT图像的钙化评分分类，属于医学图像分析中的高阶任务，不涉及像素级图像恢复或增强。",
        "relevance_score": 3,
        "novelty_score": 5,
        "novelty_reason": "提出伪标签策略和基础模型特征提取，但方法属于分类任务，未针对图像质量本身进行优化。"
    },
    {
        "title": "Informative Sample Selection Model for Skeleton-based Action Recognition\n  with Limited Training Samples",
        "url": "http://arxiv.org/abs/2510.25345v1",
        "pub_date": "2025-10-29",
        "summary": "Skeleton-based human action recognition aims to classify human skeletal sequences, which are spatiotemporal representations of actions, into predefined categories. To reduce the reliance on costly annotations of skeletal sequences while maintaining competitive recognition accuracy, the task of 3D Action Recognition with Limited Training Samples, also known as semi-supervised 3D Action Recognition, has been proposed. In addition, active learning, which aims to proactively select the most informative unlabeled samples for annotation, has been explored in semi-supervised 3D Action Recognition for training sample selection. Specifically, researchers adopt an encoder-decoder framework to embed skeleton sequences into a latent space, where clustering information, combined with a margin-based selection strategy using a multi-head mechanism, is utilized to identify the most informative sequences in the unlabeled set for annotation. However, the most representative skeleton sequences may not necessarily be the most informative for the action recognizer, as the model may have already acquired similar knowledge from previously seen skeleton samples. To solve it, we reformulate Semi-supervised 3D action recognition via active learning from a novel perspective by casting it as a Markov Decision Process (MDP). Built upon the MDP framework and its training paradigm, we train an informative sample selection model to intelligently guide the selection of skeleton sequences for annotation. To enhance the representational capacity of the factors in the state-action pairs within our method, we project them from Euclidean space to hyperbolic space. Furthermore, we introduce a meta tuning strategy to accelerate the deployment of our method in real-world scenarios. Extensive experiments on three 3D action recognition benchmarks demonstrate the effectiveness of our method.",
        "translated": "基于骨架的人体动作识别旨在将人体骨架序列——即动作的时空表示——分类到预定义的类别中。为降低对骨架序列昂贵标注的依赖，同时保持具有竞争力的识别精度，提出了样本受限的3D动作识别任务，也称为半监督3D动作识别。此外，主动学习作为一种主动选择最具信息量的未标注样本进行标注的策略，已在半监督3D动作识别中用于训练样本选择。具体而言，研究人员采用编码器-解码器框架将骨架序列嵌入到潜在空间中，并结合聚类信息以及基于多头机制的边界选择策略，用于识别未标注集合中最具信息量的序列以进行标注。然而，最具代表性的骨架序列未必对动作识别器最具信息量，因为模型可能已从先前观察到的骨架样本中获取了类似知识。为解决此问题，我们从新颖视角重新定义了基于主动学习的半监督3D动作识别，将其建模为马尔可夫决策过程（MDP）。基于MDP框架及其训练范式，我们训练一个信息样本选择模型，以智能地指导骨架序列的标注选择。为进一步增强方法中状态-动作对内各因素的表示能力，我们将它们从欧氏空间投影到双曲空间。此外，我们引入一种元调优策略，以加速方法在真实场景中的部署。在三个3D动作识别基准数据集上的大量实验验证了本方法的有效性。",
        "translated_title": "基于骨架的动作识别在有限训练样本下的信息样本选择模型",
        "label": [],
        "label_reason": "论文聚焦于基于骨架的动作识别，属于高阶视觉任务，不涉及像素级图像恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 6,
        "novelty_reason": "提出MDP框架与双曲空间投影，但应用于动作识别，非低层图像处理创新。"
    },
    {
        "title": "StreamingCoT: A Dataset for Temporal Dynamics and Multimodal\n  Chain-of-Thought Reasoning in Streaming VideoQA",
        "url": "http://arxiv.org/abs/2510.25332v1",
        "pub_date": "2025-10-29",
        "summary": "The rapid growth of streaming video applications demands multimodal models with enhanced capabilities for temporal dynamics understanding and complex reasoning. However, current Video Question Answering (VideoQA) datasets suffer from two critical limitations: 1) Static annotation mechanisms fail to capture the evolving nature of answers in temporal video streams, and 2) The absence of explicit reasoning process annotations restricts model interpretability and logical deduction capabilities. To address these challenges, We introduce StreamingCoT, the first dataset explicitly designed for temporally evolving reasoning in streaming VideoQA and multimodal Chain-of-Thought (CoT) tasks. Our framework first establishes a dynamic hierarchical annotation architecture that generates per-second dense descriptions and constructs temporally-dependent semantic segments through similarity fusion, paired with question-answer sets constrained by temporal evolution patterns. We further propose an explicit reasoning chain generation paradigm that extracts spatiotemporal objects via keyframe semantic alignment, derives object state transition-based reasoning paths using large language models, and ensures logical coherence through human-verified validation. This dataset establishes a foundation for advancing research in streaming video understanding, complex temporal reasoning, and multimodal inference. Our StreamingCoT and its construction toolkit can be accessed at https://github.com/Fleeting-hyh/StreamingCoT.",
        "translated": "流媒体视频应用的快速发展对具备时序动态理解与复杂推理能力的多模态模型提出了更高要求。然而，当前的视频问答（VideoQA）数据集存在两个关键局限：1）静态标注机制无法捕捉在时序视频流中不断演变的答案特性；2）缺乏显式的推理过程标注，限制了模型的可解释性及逻辑推导能力。为应对这些挑战，我们引入 StreamingCoT——首个专为流媒体视频问答及多模态思维链（Chain-of-Thought, CoT）任务设计的、明确支持时序演化推理的数据集。我们的框架首先构建了一个动态分层标注架构，生成每秒密集描述，并通过相似性融合构建时序依赖的语义片段，同时配对以符合时序演化模式的问答对。我们进一步提出一种显式推理链生成范式：通过关键帧语义对齐提取时空对象，利用大语言模型推导基于对象状态迁移的推理路径，并通过人工验证确保逻辑一致性。该数据集为流媒体视频理解、复杂时序推理及多模态推理研究奠定了基础。我们的 StreamingCoT 数据集及其构建工具包可通过 https://github.com/Fleeting-hyh/StreamingCoT 获取。",
        "translated_title": "StreamingCoT：用于流式视频问答中时间动态与多模态思维链推理的数据集",
        "label": [],
        "label_reason": "论文聚焦视频问答与多模态推理，属于high-level视觉任务，不涉及像素级图像恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "提出动态标注框架与推理链生成，属多模态推理创新，但非图像处理领域核心贡献。"
    },
    {
        "title": "MMEdge: Accelerating On-device Multimodal Inference via Pipelined\n  Sensing and Encoding",
        "url": "http://arxiv.org/abs/2510.25327v1",
        "pub_date": "2025-10-29",
        "summary": "Real-time multimodal inference on resource-constrained edge devices is essential for applications such as autonomous driving, human-computer interaction, and mobile health. However, prior work often overlooks the tight coupling between sensing dynamics and model execution, as well as the complex inter-modality dependencies. In this paper, we propose MMEdge, an new on-device multi-modal inference framework based on pipelined sensing and encoding. Instead of waiting for complete sensor inputs, MMEdge decomposes the entire inference process into a sequence of fine-grained sensing and encoding units, allowing computation to proceed incrementally as data arrive. MMEdge also introduces a lightweight but effective temporal aggregation module that captures rich temporal dynamics across different pipelined units to maintain accuracy performance. Such pipelined design also opens up opportunities for fine-grained cross-modal optimization and early decision-making during inference. To further enhance system performance under resource variability and input data complexity, MMEdge incorporates an adaptive multimodal configuration optimizer that dynamically selects optimal sensing and model configurations for each modality under latency constraints, and a cross-modal speculative skipping mechanism that bypasses future units of slower modalities when early predictions reach sufficient confidence. We evaluate MMEdge using two public multimodal datasets and deploy it on a real-world unmanned aerial vehicle (UAV)-based multimodal testbed. The results show that MMEdge significantly reduces end-to-end latency while maintaining high task accuracy across various system and data dynamics.",
        "translated": "实时多模态推理在资源受限的边缘设备上对于自动驾驶、人机交互和移动医疗等应用至关重要。然而，先前的研究往往忽略了感知动态与模型执行之间的紧密耦合，以及模态间复杂的相互依赖关系。本文提出MMEdge，一种基于流水线感知与编码的新型设备端多模态推理框架。与等待完整传感器输入不同，MMEdge将整个推理过程分解为一系列细粒度的感知与编码单元，使得计算能够随着数据到达而逐步进行。MMEdge还引入了一种轻量但有效的时序聚合模块，用于捕捉不同流水线单元间的丰富时序动态，以维持准确率性能。这种流水线设计还为推理过程中细粒度的跨模态优化和早期决策提供了可能。为进一步提升系统在资源波动和输入数据复杂性下的性能，MMEdge集成了一个自适应多模态配置优化器，该优化器在延迟约束下动态为各模态选择最优的感知与模型配置，并引入跨模态推测跳过机制，当早期预测达到足够置信度时，可跳过较慢模态的后续单元。我们在两个公开的多模态数据集上评估了MMEdge，并将其部署到基于真实无人机（UAV）的多模态测试平台上。实验结果表明，MMEdge在保持各种系统与数据动态下高任务准确率的同时，显著降低了端到端延迟。",
        "translated_title": "MMEdge：通过流水线式感知与编码加速设备端多模态推理",
        "label": [],
        "label_reason": "论文聚焦多模态推理加速与边缘设备部署，未涉及图像像素级恢复或增强任务。",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出流水线感知与编码框架，结合自适应优化和推测跳过机制，属于系统级创新。"
    },
    {
        "title": "Prototype-Driven Adaptation for Few-Shot Object Detection",
        "url": "http://arxiv.org/abs/2510.25318v1",
        "pub_date": "2025-10-29",
        "summary": "Few-shot object detection (FSOD) often suffers from base-class bias and unstable calibration when only a few novel samples are available. We propose Prototype-Driven Alignment (PDA), a lightweight, plug-in metric head for DeFRCN that provides a prototype-based \"second opinion\" complementary to the linear classifier. PDA maintains support-only prototypes in a learnable identity-initialized projection space and optionally applies prototype-conditioned RoI alignment to reduce geometric mismatch. During fine-tuning, prototypes can be adapted via exponential moving average(EMA) updates on labeled foreground RoIs-without introducing class-specific parameters-and are frozen at inference to ensure strict protocol compliance. PDA employs a best-of-K matching scheme to capture intra-class multi-modality and temperature-scaled fusion to combine metric similarities with detector logits. Experiments on VOC FSOD and GFSOD benchmarks show that PDA consistently improves novel-class performance with minimal impact on base classes and negligible computational overhead.",
        "translated": "少样本目标检测（FSOD）在仅有少量新类别样本时，常面临基类偏置和校准不稳定的挑战。我们提出了一种轻量级、即插即用的度量头——原型驱动对齐（Prototype-Driven Alignment, PDA），将其应用于DeFRCN框架中，为线性分类器提供基于原型的“第二意见”作为补充。PDA在可学习的、恒等初始化的投影空间中维护仅由支持样本生成的原型，并可选地采用原型条件下的RoI对齐操作，以减轻几何错位问题。在微调阶段，原型可通过在标注前景RoI上进行指数移动平均（EMA）更新进行自适应调整——无需引入类别特定参数——并在推理阶段冻结，以确保严格遵循协议要求。PDA采用最佳K匹配策略，捕捉类内多模态特性，并通过温度缩放融合机制，将度量相似性与检测器输出的logits相结合。在VOC FSOD和GFSOD基准数据集上的实验表明，PDA在显著提升新类别性能的同时，对基类性能影响极小，且计算开销可忽略不计。",
        "translated_title": "原型驱动的少样本目标检测适应方法",
        "label": [],
        "label_reason": "论文聚焦小样本目标检测，属于high-level任务，不涉及图像像素级恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 6,
        "novelty_reason": "提出原型驱动对齐机制，对检测头进行改进，属检测任务中的结构优化，非low-level创新。"
    },
    {
        "title": "Seeing Clearly and Deeply: An RGBD Imaging Approach with a Bio-inspired\n  Monocentric Design",
        "url": "http://arxiv.org/abs/2510.25314v1",
        "pub_date": "2025-10-29",
        "summary": "Achieving high-fidelity, compact RGBD imaging presents a dual challenge: conventional compact optics struggle with RGB sharpness across the entire depth-of-field, while software-only Monocular Depth Estimation (MDE) is an ill-posed problem reliant on unreliable semantic priors. While deep optics with elements like DOEs can encode depth, they introduce trade-offs in fabrication complexity and chromatic aberrations, compromising simplicity. To address this, we first introduce a novel bio-inspired all-spherical monocentric lens, around which we build the Bionic Monocentric Imaging (BMI) framework, a holistic co-design. This optical design naturally encodes depth into its depth-varying Point Spread Functions (PSFs) without requiring complex diffractive or freeform elements. We establish a rigorous physically-based forward model to generate a synthetic dataset by precisely simulating the optical degradation process. This simulation pipeline is co-designed with a dual-head, multi-scale reconstruction network that employs a shared encoder to jointly recover a high-fidelity All-in-Focus (AiF) image and a precise depth map from a single coded capture. Extensive experiments validate the state-of-the-art performance of the proposed framework. In depth estimation, the method attains an Abs Rel of 0.026 and an RMSE of 0.130, markedly outperforming leading software-only approaches and other deep optics systems. For image restoration, the system achieves an SSIM of 0.960 and a perceptual LPIPS score of 0.082, thereby confirming a superior balance between image fidelity and depth accuracy. This study illustrates that the integration of bio-inspired, fully spherical optics with a joint reconstruction algorithm constitutes an effective strategy for addressing the intrinsic challenges in high-performance compact RGBD imaging. Source code will be publicly available at https://github.com/ZongxiYu-ZJU/BMI.",
        "translated": "实现高保真、紧凑型RGBD成像面临双重挑战：传统紧凑光学系统难以在整个景深范围内同时保证RGB图像的清晰度，而仅依赖软件的单目深度估计（Monocular Depth Estimation, MDE）则是一个病态问题，高度依赖不可靠的语义先验。虽然采用衍射光学元件（DOEs）等结构的深度光学系统能够编码深度信息，但其在制造复杂性和色差方面引入了折衷，损害了系统的简洁性。为应对这一问题，我们首先提出一种新型仿生全球面单心镜头，并在此基础上构建了仿生单心成像（Bionic Monocentric Imaging, BMI）框架，实现光学与算法的协同设计。该光学设计能够自然地通过其随深度变化的点扩散函数（Point Spread Functions, PSFs）编码深度信息，无需复杂的衍射或自由曲面元件。我们建立了一个严谨的基于物理的前向模型，通过精确模拟光学退化过程生成合成数据集。该模拟流程与一个双头、多尺度重构网络协同设计，该网络采用共享编码器，从单次编码捕获中联合恢复高保真的全焦点（All-in-Focus, AiF）图像与精确的深度图。大量实验验证了所提框架的先进性能：在深度估计方面，该方法达到Abs Rel为0.026、RMSE为0.130，显著优于主流仅软件方法及其他深度光学系统；在图像恢复方面，系统获得SSIM为0.960、感知LPIPS得分为0.082，证实了图像保真度与深度精度之间的优越平衡。本研究表明，将仿生全球面光学与联合重构算法相结合，是解决高性能紧凑型RGBD成像内在挑战的有效策略。源代码将公开于https://github.com/ZongxiYu-ZJU/BMI。",
        "translated_title": "看清与深究：一种基于仿生单心结构的RGBD成像方法",
        "label": [
            "图像恢复",
            "图像去模糊",
            "多帧/视频图像恢复"
        ],
        "label_reason": "基于物理模型的光学退化建模与联合重建网络，恢复高保真全聚焦图像，属图像恢复与去模糊范畴",
        "relevance_score": 9,
        "novelty_score": 8,
        "novelty_reason": "提出生物启发式单中心光学设计与联合重建网络，结合物理模型与深度学习，方法新颖且有效"
    },
    {
        "title": "GaTector+: A Unified Head-free Framework for Gaze Object and Gaze\n  Following Prediction",
        "url": "http://arxiv.org/abs/2510.25301v1",
        "pub_date": "2025-10-29",
        "summary": "Gaze object detection and gaze following are fundamental tasks for interpreting human gaze behavior or intent. However, most previous methods usually solve these two tasks separately, and their prediction of gaze objects and gaze following typically depend on head-related prior knowledge during both the training phase and real-world deployment. This dependency necessitates an auxiliary network to extract head location, thus precluding joint optimization across the entire system and constraining the practical applicability. To this end, we propose GaTector+, a unified framework for gaze object detection and gaze following, which eliminates the dependence on the head-related priors during inference. Specifically, GaTector+ uses an expanded specific-general-specific feature extractor that leverages a shared backbone, which extracts general features for gaze following and object detection using the shared backbone while using specific blocks before and after the shared backbone to better consider the specificity of each sub-task. To obtain head-related knowledge without prior information, we first embed a head detection branch to predict the head of each person. Then, before regressing the gaze point, a head-based attention mechanism is proposed to fuse the sense feature and gaze feature with the help of head location. Since the suboptimization of the gaze point heatmap leads to the performance bottleneck, we propose an attention supervision mechanism to accelerate the learning of the gaze heatmap. Finally, we propose a novel evaluation metric, mean Similarity over Candidates (mSoC), for gaze object detection, which is more sensitive to variations between bounding boxes. The experimental results on multiple benchmark datasets demonstrate the effectiveness of our model in both gaze object detection and gaze following tasks.",
        "translated": "注视目标检测与注视跟踪是理解人类注视行为或意图的基础任务。然而，大多数先前的方法通常将这两个任务分别处理，且其在训练阶段及实际部署中对注视目标和注视跟踪的预测往往依赖于与头部相关的先验知识。这种依赖性需要引入辅助网络以提取头部位置，从而阻碍了整个系统端到端的联合优化，并限制了模型的实际适用性。为此，我们提出 GaTector+，一种用于注视目标检测与注视跟踪的统一框架，该框架在推理过程中消除了对头部相关先验的依赖。具体而言，GaTector+ 采用一种扩展的特定-通用-特定特征提取器，该提取器基于共享主干网络，利用共享主干提取注视跟踪与目标检测所需的通用特征，同时在共享主干前后引入特定模块，以更好地考虑各子任务的特异性。为在无先验信息的情况下获取头部相关知识，我们首先嵌入一个头部检测分支，用于预测每个人的头部位置。随后，在回归注视点之前，提出一种基于头部的注意力机制，借助头部位置信息融合感知特征与注视特征。由于注视点热图的子优化问题导致性能瓶颈，我们进一步提出一种注意力监督机制，以加速注视热图的学习过程。最后，我们提出一种新颖的评估指标——候选框平均相似度（mean Similarity over Candidates, mSoC），用于注视目标检测任务，该指标对边界框之间的变化更为敏感。在多个基准数据集上的实验结果表明，我们的模型在注视目标检测与注视跟踪任务中均表现出优越的有效性。",
        "translated_title": "GaTector+: 一种用于注视目标与注视跟踪预测的统一无头框架",
        "label": [],
        "label_reason": "论文聚焦于眼动目标检测与眼动追踪，属于高阶视觉理解任务，不涉及像素级图像恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "提出统一框架与注意力监督机制，但未突破现有范式，创新点集中在任务联合建模而非图像处理。"
    },
    {
        "title": "Diffusion-Driven Progressive Target Manipulation for Source-Free Domain\n  Adaptation",
        "url": "http://arxiv.org/abs/2510.25279v1",
        "pub_date": "2025-10-29",
        "summary": "Source-free domain adaptation (SFDA) is a challenging task that tackles domain shifts using only a pre-trained source model and unlabeled target data. Existing SFDA methods are restricted by the fundamental limitation of source-target domain discrepancy. Non-generation SFDA methods suffer from unreliable pseudo-labels in challenging scenarios with large domain discrepancies, while generation-based SFDA methods are evidently degraded due to enlarged domain discrepancies in creating pseudo-source data. To address this limitation, we propose a novel generation-based framework named Diffusion-Driven Progressive Target Manipulation (DPTM) that leverages unlabeled target data as references to reliably generate and progressively refine a pseudo-target domain for SFDA. Specifically, we divide the target samples into a trust set and a non-trust set based on the reliability of pseudo-labels to sufficiently and reliably exploit their information. For samples from the non-trust set, we develop a manipulation strategy to semantically transform them into the newly assigned categories, while simultaneously maintaining them in the target distribution via a latent diffusion model. Furthermore, we design a progressive refinement mechanism that progressively reduces the domain discrepancy between the pseudo-target domain and the real target domain via iterative refinement. Experimental results demonstrate that DPTM outperforms existing methods by a large margin and achieves state-of-the-art performance on four prevailing SFDA benchmark datasets with different scales. Remarkably, DPTM can significantly enhance the performance by up to 18.6% in scenarios with large source-target gaps.",
        "translated": "无源域自适应（Source-Free Domain Adaptation, SFDA）是一项具有挑战性的任务，旨在仅利用预训练的源模型和未标记的目标数据来应对域偏移问题。现有SFDA方法受限于源域与目标域之间固有的域差异问题。非生成式SFDA方法在域差异较大的困难场景中，由于伪标签不可靠而表现不佳；而基于生成的SFDA方法在生成伪源数据时，因域差异扩大而明显退化。为解决这一局限，我们提出一种新颖的基于生成的框架，名为扩散驱动的渐进式目标域操控（Diffusion-Driven Progressive Target Manipulation, DPTM），该框架利用未标记的目标数据作为参考，可靠地生成并逐步优化伪目标域以用于SFDA。具体而言，我们根据伪标签的可靠性将目标样本划分为可信集和不可信集，从而充分且可靠地利用其信息。对于来自不可信集的样本，我们设计了一种操控策略，通过潜在扩散模型在语义上将其转换至新分配的类别，同时保持其在目标分布中。此外，我们设计了一种渐进式优化机制，通过迭代优化逐步缩小伪目标域与真实目标域之间的域差异。实验结果表明，DPTM在四个主流的SFDA基准数据集（具有不同规模）上显著优于现有方法，达到当前最佳性能。值得注意的是，在源域与目标域差异较大的场景中，DPTM能够将性能提升高达18.6%。",
        "translated_title": "扩散驱动的渐进式目标操控用于无源域自适应",
        "label": [],
        "label_reason": "论文聚焦于无源域自适应，属于高阶视觉任务，不涉及像素级图像恢复或增强。",
        "relevance_score": 2,
        "novelty_score": 8,
        "novelty_reason": "提出扩散驱动的渐进式目标操纵框架，结合扩散模型与伪标签策略，具有显著创新性。"
    },
    {
        "title": "Retrieval-Augmented Search for Large-Scale Map Collections with ColPali",
        "url": "http://arxiv.org/abs/2510.25718v1",
        "pub_date": "2025-10-29",
        "summary": "Multimodal approaches have shown great promise for searching and navigating digital collections held by libraries, archives, and museums. In this paper, we introduce map-RAS: a retrieval-augmented search system for historic maps. In addition to introducing our framework, we detail our publicly-hosted demo for searching 101,233 map images held by the Library of Congress. With our system, users can multimodally query the map collection via ColPali, summarize search results using Llama 3.2, and upload their own collections to perform inter-collection search. We articulate potential use cases for archivists, curators, and end-users, as well as future work with our system in both machine learning and the digital humanities. Our demo can be viewed at: http://www.mapras.com.",
        "translated": "多模态方法在图书馆、档案馆和博物馆所持有的数字藏品的检索与导航方面展现出巨大潜力。本文介绍了 map-RAS：一种面向历史地图的检索增强型搜索系统。除系统框架外，我们还详细说明了我们公开托管的演示系统，该系统支持对国会图书馆所藏的 101,233 张地图图像进行检索。借助本系统，用户可通过 ColPali 实现多模态查询地图藏品，利用 Llama 3.2 对检索结果进行摘要，并上传自有藏品以实现跨藏品集合的检索。我们阐述了该系统在档案管理员、策展人及终端用户中的潜在应用场景，以及在机器学习与数字人文领域中的未来研究方向。演示系统可访问：http://www.mapras.com。",
        "translated_title": "基于ColPali的大规模地图集合检索增强搜索",
        "label": [
            "多模态推荐",
            "召回"
        ],
        "label_reason": "基于多模态检索的系统，用于地图集合搜索，涉及召回环节，但非典型推荐场景",
        "relevance_score": 4,
        "novelty_score": 6,
        "novelty_reason": "结合ColPali与LLM实现多模态检索与摘要，有一定创新但属现有技术组合"
    },
    {
        "title": "MMQ-v2: Align, Denoise, and Amplify: Adaptive Behavior Mining for\n  Semantic IDs Learning in Recommendation",
        "url": "http://arxiv.org/abs/2510.25622v1",
        "pub_date": "2025-10-29",
        "summary": "Industrial recommender systems rely on unique Item Identifiers (ItemIDs). However, this method struggles with scalability and generalization in large, dynamic datasets that have sparse long-tail data.Content-based Semantic IDs (SIDs) address this by sharing knowledge through content quantization. However, by ignoring dynamic behavioral properties, purely content-based SIDs have limited expressive power. Existing methods attempt to incorporate behavioral information but overlook a critical distinction: unlike relatively uniform content features, user-item interactions are highly skewed and diverse, creating a vast information gap in quality and quantity between popular and long-tail items. This oversight leads to two critical limitations: (1) Noise Corruption: Indiscriminate behavior-content alignment allows collaborative noise from long-tail items to corrupt their content representations, leading to the loss of critical multimodal information. (2)Signal Obscurity: The equal-weighting scheme for SIDs fails to reflect the varying importance of different behavioral signals, making it difficult for downstream tasks to distinguish important SIDs from uninformative ones. To tackle these issues, we propose a mixture-of-quantization framework, MMQ-v2, to adaptively Align, Denoise, and Amplify multimodal information from content and behavior modalities for semantic IDs learning. The semantic IDs generated by this framework named ADA-SID. It introduces two innovations: an adaptive behavior-content alignment that is aware of information richness to shield representations from noise, and a dynamic behavioral router to amplify critical signals by applying different weights to SIDs. Extensive experiments on public and large-scale industrial datasets demonstrate ADA-SID's significant superiority in both generative and discriminative recommendation tasks.",
        "translated": "工业推荐系统依赖于唯一的物料标识符（ItemIDs）。然而，该方法在处理具有稀疏长尾数据的大规模动态数据集时，面临可扩展性和泛化能力的挑战。基于内容的语义标识符（SIDs）通过内容量化共享知识，缓解了上述问题。然而，纯粹基于内容的SIDs忽略了动态行为特性，导致其表达能力受限。现有方法尝试融合行为信息，但忽视了一个关键区别：与相对均匀的内容特征不同，用户-物料交互具有高度偏斜性和多样性，在流行物料与长尾物料之间造成了质量与数量上的巨大信息鸿沟。这一忽视导致两个关键局限：（1）噪声污染：无差别地对齐行为与内容，使得长尾物料的协同噪声污染其内容表示，从而导致关键多模态信息的丢失；（2）信号模糊：SIDs的等权重方案无法反映不同行为信号的重要性差异，导致下游任务难以区分重要SIDs与无信息量的SIDs。为解决这些问题，我们提出一种混合量化框架MMQ-v2，用于自适应地对齐、去噪和增强内容与行为模态中的多模态信息，以学习语义标识符。该框架生成的语义标识符称为ADA-SID。它引入两项创新：一种感知信息丰富度的自适应行为-内容对齐机制，用于保护表示免受噪声干扰；以及一种动态行为路由机制，通过为不同SIDs赋予不同权重，增强关键信号。在公开数据集和大规模工业数据集上的大量实验表明，ADA-SID在生成式和判别式推荐任务中均展现出显著优势。",
        "translated_title": "MMQ-v2：对齐、去噪与增强：面向推荐系统语义ID学习的自适应行为挖掘",
        "label": [
            "召回",
            "精排",
            "序列推荐",
            "多模态推荐",
            "负采样与对比学习"
        ],
        "label_reason": "论文聚焦推荐系统中语义ID学习，融合内容与行为模态，解决噪声与信号稀疏问题，直接服务于召回与排序环节。",
        "relevance_score": 9,
        "novelty_score": 8,
        "novelty_reason": "提出自适应对齐与动态路由机制，创新性地处理行为噪声与信号权重，提升多模态表征质量。"
    },
    {
        "title": "FARSIQA: Faithful and Advanced RAG System for Islamic Question Answering",
        "url": "http://arxiv.org/abs/2510.25621v1",
        "pub_date": "2025-10-29",
        "summary": "The advent of Large Language Models (LLMs) has revolutionized Natural Language Processing, yet their application in high-stakes, specialized domains like religious question answering is hindered by challenges like hallucination and unfaithfulness to authoritative sources. This issue is particularly critical for the Persian-speaking Muslim community, where accuracy and trustworthiness are paramount. Existing Retrieval-Augmented Generation (RAG) systems, relying on simplistic single-pass pipelines, fall short on complex, multi-hop queries requiring multi-step reasoning and evidence aggregation. To address this gap, we introduce FARSIQA, a novel, end-to-end system for Faithful Advanced Question Answering in the Persian Islamic domain. FARSIQA is built upon our innovative FAIR-RAG architecture: a Faithful, Adaptive, Iterative Refinement framework for RAG. FAIR-RAG employs a dynamic, self-correcting process: it adaptively decomposes complex queries, assesses evidence sufficiency, and enters an iterative loop to generate sub-queries, progressively filling information gaps. Operating on a curated knowledge base of over one million authoritative Islamic documents, FARSIQA demonstrates superior performance. Rigorous evaluation on the challenging IslamicPCQA benchmark shows state-of-the-art performance: the system achieves a remarkable 97.0% in Negative Rejection - a 40-point improvement over baselines - and a high Answer Correctness score of 74.3%. Our work establishes a new standard for Persian Islamic QA and validates that our iterative, adaptive architecture is crucial for building faithful, reliable AI systems in sensitive domains.",
        "translated": "大语言模型（LLM）的出现彻底改变了自然语言处理领域，但其在高风险、专业性强的领域（如宗教问答）中的应用仍面临幻觉和对权威来源不忠实等挑战。这一问题对波斯语穆斯林社区尤为关键，因为准确性与可信度至关重要。现有的检索增强生成（RAG）系统依赖于简化的单次处理流程，在需要多步推理和证据聚合的复杂多跳查询任务中表现不足。为解决这一空白，我们提出FARSIQA，一种面向波斯语伊斯兰领域的新型端到端可信高级问答系统。FARSIQA基于我们创新的FAIR-RAG架构：一种可信、自适应、迭代优化的RAG框架。FAIR-RAG采用动态自校正流程：它自适应地分解复杂查询，评估证据充分性，并进入迭代循环以生成子查询，逐步填补信息空白。该系统基于一个包含超过一百万份权威伊斯兰文献的精心构建知识库运行，展现出卓越性能。在具有挑战性的IslamicPCQA基准上的严格评估显示，其性能达到当前最优水平：系统在负样本拒绝指标上达到97.0%，较基线模型提升40个百分点，同时答案正确性得分高达74.3%。本工作为波斯语伊斯兰问答领域设立了新的标准，并验证了我们提出的迭代自适应架构在构建敏感领域中可信、可靠AI系统中的关键作用。",
        "translated_title": "FARSIQA：面向伊斯兰问答的忠实且先进的RAG系统",
        "label": [],
        "label_reason": "论文聚焦宗教问答系统，虽用RAG架构，但非推荐系统核心问题，与推荐无直接关联。",
        "relevance_score": 3,
        "novelty_score": 7,
        "novelty_reason": "提出迭代自校正RAG架构，提升事实准确性，在问答领域有显著创新。"
    },
    {
        "title": "Generalized Pseudo-Relevance Feedback",
        "url": "http://arxiv.org/abs/2510.25488v1",
        "pub_date": "2025-10-29",
        "summary": "Query rewriting is a fundamental technique in information retrieval (IR). It typically employs the retrieval result as relevance feedback to refine the query and thereby addresses the vocabulary mismatch between user queries and relevant documents. Traditional pseudo-relevance feedback (PRF) and its vector-based extension (VPRF) improve retrieval performance by leveraging top-retrieved documents as relevance feedback. However, they are constructed based on two major hypotheses: the relevance assumption (top documents are relevant) and the model assumption (rewriting methods need to be designed specifically for particular model architectures). While recent large language models (LLMs)-based generative relevance feedback (GRF) enables model-free query reformulation, it either suffers from severe LLM hallucination or, again, relies on the relevance assumption to guarantee the effectiveness of rewriting quality. To overcome these limitations, we introduce an assumption-relaxed framework: \\textit{Generalized Pseudo Relevance Feedback} (GPRF), which performs model-free, natural language rewriting based on retrieved documents, not only eliminating the model assumption but also reducing dependence on the relevance assumption. Specifically, we design a utility-oriented training pipeline with reinforcement learning to ensure robustness against noisy feedback. Extensive experiments across multiple benchmarks and retrievers demonstrate that GPRF consistently outperforms strong baselines, establishing it as an effective and generalizable framework for query rewriting.",
        "translated": "查询改写是信息检索（IR）中的核心技术。它通常利用检索结果作为相关性反馈，以优化原始查询，从而缓解用户查询与相关文档之间的词汇不匹配问题。传统的伪相关反馈（PRF）及其基于向量的扩展（VPRF）通过利用检索结果中排名靠前的文档作为相关性反馈，提升了检索性能。然而，这些方法基于两个主要假设：相关性假设（排名靠前的文档是相关的）和模型假设（改写方法需针对特定模型架构专门设计）。尽管近期基于大语言模型（LLM）的生成式相关反馈（GRF）实现了无模型依赖的查询重写，但它要么面临严重的LLM幻觉问题，要么仍依赖相关性假设以确保改写质量的有效性。为克服这些局限，我们提出一种假设宽松的框架：\\textit{广义伪相关反馈}（GPRF），该框架基于检索到的文档，实现无模型依赖的自然语言改写，不仅消除了模型假设，还降低了对相关性假设的依赖。具体而言，我们设计了一个以效用为导向的强化学习训练流程，以增强模型对噪声反馈的鲁棒性。在多个基准数据集和检索器上的广泛实验表明，GPRF始终优于强基线方法，确立其作为查询改写领域有效且可泛化的框架地位。",
        "translated_title": "广义伪相关反馈",
        "label": [],
        "label_reason": "论文聚焦信息检索中的查询重写，虽与推荐有交叉，但核心为通用IR技术，非推荐系统特定环节。",
        "relevance_score": 3,
        "novelty_score": 8,
        "novelty_reason": "提出去假设的通用伪相关反馈框架，结合强化学习训练，方法新颖，提升查询重写鲁棒性。"
    },
    {
        "title": "Alibaba International E-commerce Product Search Competition DcuRAGONs\n  Team Technical Report",
        "url": "http://arxiv.org/abs/2510.25428v1",
        "pub_date": "2025-10-29",
        "summary": "This report details our methodology and results developed for the Multilingual E-commerce Search Competition. The problem aims to recognize relevance between user queries versus product items in a multilingual context and improve recommendation performance on e-commerce platforms. Utilizing Large Language Models (LLMs) and their capabilities in other tasks, our data-centric method achieved the highest score compared to other solutions during the competition. Final leaderboard is publised at https://alibaba-international-cikm2025.github.io. The source code for our project is published at https://github.com/nhtlongcs/e-commerce-product-search.",
        "translated": "本报告详细阐述了我们在多语言电子商务搜索竞赛中所采用的方法及取得的结果。该问题旨在识别在多语言环境下用户查询与产品物料之间的相关性，并提升电子商务平台的推荐性能。通过利用大语言模型（LLM）及其在其他任务中的能力，我们提出的以数据为中心的方法在竞赛中相较于其他方案取得了最高得分。最终排行榜发布于 https://alibaba-international-cikm2025.github.io。本项目的源代码已公开于 https://github.com/nhtlongcs/e-commerce-product-search。",
        "translated_title": "阿里巴巴国际电商产品搜索竞赛 DcuRAGONs  \n团队技术报告",
        "label": [
            "召回",
            "LLM生成式推荐",
            "通用推荐技术"
        ],
        "label_reason": "基于LLM的多语言商品检索，属于推荐系统召回环节，方法通用非专为推荐设计",
        "relevance_score": 7,
        "novelty_score": 6,
        "novelty_reason": "数据驱动方法结合LLM，属常规改进，未提出新架构或范式"
    },
    {
        "title": "Towards Automated Quality Assurance of Patent Specifications: A\n  Multi-Dimensional LLM Framework",
        "url": "http://arxiv.org/abs/2510.25402v1",
        "pub_date": "2025-10-29",
        "summary": "Despite the surge in patent applications and emergence of AI drafting tools, systematic evaluation of patent content quality has received limited research attention. To address this gap, We propose to evaluate patents using regulatory compliance, technical coherence, and figure-reference consistency detection modules, and then generate improvement suggestions via an integration module. The framework is validated on a comprehensive dataset comprising 80 human-authored and 80 AI-generated patents from two patent drafting tools. Experimental results show balanced accuracies of 99.74\\%, 82.12\\%, and 91.2\\% respectively across the three detection modules when validated against expert annotations. Additional analysis was conducted to examine defect distributions across patent sections, technical domains, and authoring sources. Section-based analysis indicates that figure-text consistency and technical detail precision require particular attention. Mechanical Engineering and Construction show more claim-specification inconsistencies due to complex technical documentation requirements. AI-generated patents show a significant gap compared to human-authored ones. While human-authored patents primarily contain surface-level errors like typos, AI-generated patents exhibit more structural defects in figure-text alignment and cross-references.",
        "translated": "尽管专利申请数量激增，且AI撰写工具不断涌现，但对专利内容质量的系统性评估仍鲜有研究关注。为弥补这一空白，我们提出采用监管合规性、技术连贯性以及图示-引用一致性检测模块对专利进行评估，并通过集成模块生成改进建议。该框架在包含80份人工撰写和80份由两种专利撰写工具生成的专利的综合数据集上进行了验证。实验结果表明，在与专家标注对比验证时，三个检测模块的平衡准确率分别达到99.74%、82.12%和91.2%。此外，我们进一步分析了缺陷在专利各部分、技术领域及撰写来源中的分布情况。基于章节的分析表明，图示与文本一致性以及技术细节的精确性需要特别关注。机械工程与建筑领域因技术文档要求复杂，表现出更多的权利要求与说明书不一致问题。AI生成的专利相较于人工撰写的专利存在显著差距：人工撰写的专利主要包含拼写错误等表面级错误，而AI生成的专利则在图示与文本对齐及交叉引用方面表现出更多结构性缺陷。",
        "translated_title": "迈向专利说明书自动化质量保障：一个多维度大语言模型框架",
        "label": [],
        "label_reason": "论文聚焦专利质量评估，涉及LLM框架，但与推荐系统无直接关联。",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出多维度LLM评估框架，方法有一定创新性，但应用于专利领域非推荐系统。"
    },
    {
        "title": "Revisiting scalable sequential recommendation with Multi-Embedding\n  Approach and Mixture-of-Experts",
        "url": "http://arxiv.org/abs/2510.25285v1",
        "pub_date": "2025-10-29",
        "summary": "In recommendation systems, how to effectively scale up recommendation models has been an essential research topic. While significant progress has been made in developing advanced and scalable architectures for sequential recommendation(SR) models, there are still challenges due to items' multi-faceted characteristics and dynamic item relevance in the user context. To address these issues, we propose Fuxi-MME, a framework that integrates a multi-embedding strategy with a Mixture-of-Experts (MoE) architecture. Specifically, to efficiently capture diverse item characteristics in a decoupled manner, we decompose the conventional single embedding matrix into several lower-dimensional embedding matrices. Additionally, by substituting relevant parameters in the Fuxi Block with an MoE layer, our model achieves adaptive and specialized transformation of the enriched representations. Empirical results on public datasets show that our proposed framework outperforms several competitive baselines.",
        "translated": "在推荐系统中，如何有效扩展推荐模型一直是一个重要的研究课题。尽管在序列推荐（SR）模型的先进且可扩展架构开发方面已取得显著进展，但由于物料的多维度特性以及用户上下文中物料相关性的动态变化，仍存在诸多挑战。为解决这些问题，我们提出Fuxi-MME框架，该框架将多嵌入策略与专家混合（MoE）架构相结合。具体而言，为了高效地以解耦方式捕捉物料的多样化特征，我们将传统的单一嵌入矩阵分解为多个低维嵌入矩阵。此外，通过将Fuxi Block中相关参数替换为MoE层，我们的模型实现了对丰富表征的自适应和专业化变换。在公开数据集上的实证结果表明，我们提出的框架优于多个具有竞争力的基线方法。",
        "translated_title": "重访基于多嵌入方法与专家混合机制的可扩展序列推荐",
        "label": [
            "序列推荐",
            "通用推荐技术"
        ],
        "label_reason": "论文聚焦序列推荐中的可扩展性，提出多嵌入与MoE结合框架，直接用于推荐精排环节。",
        "relevance_score": 8,
        "novelty_score": 7,
        "novelty_reason": "创新性结合多嵌入与MoE，提升模型表达能力与效率，非完全新范式但有显著改进。"
    },
    {
        "title": "Measuring the Research Output and Performance of the University of\n  Ibadan from 2014 to 2023: A Scientometric Analysis",
        "url": "http://arxiv.org/abs/2510.25283v1",
        "pub_date": "2025-10-29",
        "summary": "This study employs scientometric methods to assess the research output and performance of the University of Ibadan from 2014 to 2023. By analyzing publication trends, citation patterns, and collaboration networks, the research aims to comprehensively evaluate the university's research productivity, impact, and disciplinary focus. This article's endeavors are characterized by innovation, interdisciplinary collaboration, and commitment to excellence, making the University of Ibadan a significant hub for cutting-edge research in Nigeria and beyond. The goal of the current study is to ascertain the influence of the university's research output and publication patterns between 2014 and 2023. The study focuses on the departments at the University of Ibadan that contribute the most, the best journals for publishing, the nations that collaborate, the impact of citations both locally and globally, well-known authors and their total production, and the research output broken down by year. According to the university's ten-year publication data, 7159 papers with an h-index of 75 were published between 2014 and 2023, garnering 218572 citations. Furthermore, the VOSviewer software mapping approach is used to illustrate the stenographical mapping of data through graphs. The findings of this study will contribute to understanding the university's research strengths, weaknesses, and potential areas for improvement. Additionally, the results will inform evidence-based decision-making for enhancing research strategies and policies at the University of Ibadan.",
        "translated": "本研究采用科学计量学方法，评估伊巴丹大学2014年至2023年的研究成果与研究表现。通过分析出版趋势、引用模式及合作网络，旨在全面评估该大学的研究生产力、影响力及其学科重点。本文的研究工作以创新性、跨学科合作以及对卓越的承诺为特征，使伊巴丹大学成为尼日利亚乃至更广泛地区前沿研究的重要中心。本研究的目标是确定该大学2014至2023年间研究成果及出版模式的影响。研究重点包括对伊巴丹大学贡献最大的院系、最佳发表期刊、合作国家、本地与全球范围内的引用影响力、知名作者及其总产出，以及按年份划分的研究成果。根据该大学十年的出版数据，2014至2023年间共发表7159篇论文，h指数为75，累计获得218572次引用。此外，本研究采用VOSviewer软件的映射方法，通过图形展示数据的图谱化映射。本研究的发现将有助于理解该大学的研究优势、劣势及潜在改进领域。同时，研究结果将为伊巴丹大学提升研究战略与政策提供基于证据的决策支持。",
        "translated_title": "伊巴丹大学2014至2023年研究产出与绩效测量：一项科学计量学分析",
        "label": [],
        "label_reason": "论文为科研产出的科学计量分析，与推荐系统无直接关联。",
        "relevance_score": 1,
        "novelty_score": 1,
        "novelty_reason": "方法为传统科学计量分析，无推荐系统相关创新。"
    },
    {
        "title": "TV-Rec: Time-Variant Convolutional Filter for Sequential Recommendation",
        "url": "http://arxiv.org/abs/2510.25259v1",
        "pub_date": "2025-10-29",
        "summary": "Recently, convolutional filters have been increasingly adopted in sequential recommendation for their ability to capture local sequential patterns. However, most of these models complement convolutional filters with self-attention. This is because convolutional filters alone, generally fixed filters, struggle to capture global interactions necessary for accurate recommendation. We propose Time-Variant Convolutional Filters for Sequential Recommendation (TV-Rec), a model inspired by graph signal processing, where time-variant graph filters capture position-dependent temporal variations in user sequences. By replacing both fixed kernels and self-attention with time-variant filters, TV-Rec achieves higher expressive power and better captures complex interaction patterns in user behavior. This design not only eliminates the need for self-attention but also reduces computation while accelerating inference. Extensive experiments on six public benchmarks show that TV-Rec outperforms state-of-the-art baselines by an average of 7.49%.",
        "translated": "近年来，卷积滤波器因其能够捕捉局部序列模式而在序列推荐中被越来越多地采用。然而，大多数此类模型通常将卷积滤波器与自注意力机制相结合。这是由于单独使用卷积滤波器（通常为固定滤波器）难以捕捉推荐所需的关键全局交互。我们提出了一种基于图信号处理思想的时变卷积滤波器序列推荐模型（TV-Rec），其中时变图滤波器能够捕捉用户序列中与位置相关的时序变化。通过同时替代固定卷积核和自注意力机制，TV-Rec实现了更强的表达能力，并更好地捕捉用户行为中的复杂交互模式。该设计不仅消除了对自注意力机制的依赖，同时降低了计算开销并加速了推理过程。在六个公开基准数据集上的大量实验表明，TV-Rec平均性能优于当前最先进的基线模型7.49%。",
        "translated_title": "TV-Rec：面向序列推荐的时间可变卷积滤波器",
        "label": [
            "序列推荐",
            "通用推荐技术"
        ],
        "label_reason": "论文提出时间变异卷积滤波器用于序列推荐，直接优化推荐系统核心环节",
        "relevance_score": 8,
        "novelty_score": 8,
        "novelty_reason": "创新性地引入图信号处理思想，替代固定卷积与自注意力，提升表达能力"
    },
    {
        "title": "GReF: A Unified Generative Framework for Efficient Reranking via Ordered\n  Multi-token Prediction",
        "url": "http://arxiv.org/abs/2510.25220v1",
        "pub_date": "2025-10-29",
        "summary": "In a multi-stage recommendation system, reranking plays a crucial role in modeling intra-list correlations among items. A key challenge lies in exploring optimal sequences within the combinatorial space of permutations. Recent research follows a two-stage (generator-evaluator) paradigm, where a generator produces multiple feasible sequences, and an evaluator selects the best one. In practice, the generator is typically implemented as an autoregressive model. However, these two-stage methods face two main challenges. First, the separation of the generator and evaluator hinders end-to-end training. Second, autoregressive generators suffer from inference efficiency. In this work, we propose a Unified Generative Efficient Reranking Framework (GReF) to address the two primary challenges. Specifically, we introduce Gen-Reranker, an autoregressive generator featuring a bidirectional encoder and a dynamic autoregressive decoder to generate causal reranking sequences. Subsequently, we pre-train Gen-Reranker on the item exposure order for high-quality parameter initialization. To eliminate the need for the evaluator while integrating sequence-level evaluation during training for end-to-end optimization, we propose post-training the model through Rerank-DPO. Moreover, for efficient autoregressive inference, we introduce ordered multi-token prediction (OMTP), which trains Gen-Reranker to simultaneously generate multiple future items while preserving their order, ensuring practical deployment in real-time recommender systems. Extensive offline experiments demonstrate that GReF outperforms state-of-the-art reranking methods while achieving latency that is nearly comparable to non-autoregressive models. Additionally, GReF has also been deployed in a real-world video app Kuaishou with over 300 million daily active users, significantly improving online recommendation quality.",
        "translated": "在多阶段推荐系统中，重排在建模物料列表内关联性方面发挥着关键作用。核心挑战在于探索排列组合空间中的最优序列。近期研究遵循两阶段（生成器-评估器）范式，其中生成器生成多个可行序列，评估器从中选择最优序列。实践中，生成器通常实现为自回归模型。然而，这类两阶段方法面临两个主要挑战：首先，生成器与评估器的分离阻碍了端到端训练；其次，自回归生成器在推理效率上存在瓶颈。本文提出一种统一的生成式高效重排框架（GReF），以应对上述两个核心挑战。具体而言，我们引入Gen-Reranker，一种具备双向编码器和动态自回归解码器的自回归生成器，用于生成因果重排序列。随后，我们在物料曝光顺序上对Gen-Reranker进行预训练，以实现高质量的参数初始化。为消除对评估器的依赖，同时在训练过程中整合序列级评估以实现端到端优化，我们提出通过Rerank-DPO对模型进行后训练。此外，为提升自回归推理效率，我们提出有序多token预测（OMTP），训练Gen-Reranker同时生成多个未来物料并保持其顺序，确保在实时推荐系统中的实际部署可行性。大量离线实验表明，GReF在性能上优于当前最先进的重排方法，同时延迟接近非自回归模型。此外，GReF已成功部署于拥有超过3亿日活跃用户的短视频应用Kuaishou，显著提升了线上推荐质量。",
        "translated_title": "GReF：一种通过有序多token预测实现高效重排的统一生成式框架",
        "label": [
            "重排（Re-ranking）",
            "LLM生成式推荐（LLM-based Generative Recommendation）",
            "序列推荐（Sequential Recommendation）"
        ],
        "label_reason": "论文核心为推荐系统重排阶段的生成式框架，解决序列建模与高效推理问题，紧密关联推荐系统核心环节。",
        "relevance_score": 10,
        "novelty_score": 9,
        "novelty_reason": "提出统一生成框架GReF，结合OMTP与Rerank-DPO，实现端到端训练与高效推理，显著提升生成式重排性能。"
    },
    {
        "title": "Model-Document Protocol for AI Search",
        "url": "http://arxiv.org/abs/2510.25160v1",
        "pub_date": "2025-10-29",
        "summary": "AI search depends on linking large language models (LLMs) with vast external knowledge sources. Yet web pages, PDF files, and other raw documents are not inherently LLM-ready: they are long, noisy, and unstructured. Conventional retrieval methods treat these documents as verbatim text and return raw passages, leaving the burden of fragment assembly and contextual reasoning to the LLM. This gap underscores the need for a new retrieval paradigm that redefines how models interact with documents.   We introduce the Model-Document Protocol (MDP), a general framework that formalizes how raw text is bridged to LLMs through consumable knowledge representations. Rather than treating retrieval as passage fetching, MDP defines multiple pathways that transform unstructured documents into task-specific, LLM-ready inputs. These include agentic reasoning, which curates raw evidence into coherent context; memory grounding, which accumulates reusable notes to enrich reasoning; and structured leveraging, which encodes documents into formal representations such as graphs or key-value caches. All three pathways share the same goal: ensuring that what reaches the LLM is not raw fragments but compact, structured knowledge directly consumable for reasoning.   As an instantiation, we present MDP-Agent, which realizes the protocol through an agentic process: constructing document-level gist memories for global coverage, performing diffusion-based exploration with vertical exploitation to uncover layered dependencies, and applying map-reduce style synthesis to integrate large-scale evidence into compact yet sufficient context. Experiments on information-seeking benchmarks demonstrate that MDP-Agent outperforms baselines, validating both the soundness of the MDP framework and the effectiveness of its agentic instantiation.",
        "translated": "AI搜索依赖于将大语言模型（LLM）与海量外部知识源连接。然而，网页、PDF文件及其他原始文档并非天然适配LLM：它们长度冗长、噪声多、结构松散。传统检索方法将这些文档视为逐字文本，返回原始段落，将碎片拼接与上下文推理的负担留给LLM承担。这一差距凸显出需要一种新的检索范式，重新定义模型与文档的交互方式。\n\n我们提出模型-文档协议（MDP），一种通用框架，形式化了原始文本如何通过可消费的知识表示与LLM衔接。MDP不再将检索视为段落获取，而是定义了多种路径，将非结构化文档转化为面向特定任务、适配LLM的输入。这些路径包括：代理式推理（agentic reasoning），将原始证据整理为连贯上下文；记忆锚定（memory grounding），积累可复用的笔记以增强推理；以及结构化利用（structured leveraging），将文档编码为图或键值缓存等正式表示。这三种路径共享同一目标：确保送达LLM的并非原始碎片，而是紧凑、结构化的知识，可直接用于推理。\n\n作为具体实现，我们提出MDP-Agent，通过代理式流程实现该协议：构建文档级概要记忆以实现全局覆盖，采用基于扩散的探索结合垂直利用以揭示多层次依赖关系，并应用类似Map-Reduce的合成策略，将大规模证据整合为紧凑但充分的上下文。在信息检索基准测试中的实验表明，MDP-Agent优于基线方法，验证了MDP框架的合理性及其代理式实现的有效性。",
        "translated_title": "模型-文档协议用于人工智能搜索",
        "label": [
            "LLM生成式推荐",
            "召回"
        ],
        "label_reason": "论文聚焦LLM与文档交互，提出MDP框架用于生成结构化知识输入，可应用于推荐系统召回环节。",
        "relevance_score": 4,
        "novelty_score": 8,
        "novelty_reason": "提出模型-文档协议新范式，通过代理推理与记忆接地等机制提升LLM输入质量，创新性强。"
    },
    {
        "title": "Continual Low-Rank Adapters for LLM-based Generative Recommender Systems",
        "url": "http://arxiv.org/abs/2510.25093v1",
        "pub_date": "2025-10-29",
        "summary": "While large language models (LLMs) achieve strong performance in recommendation, they face challenges in continual learning as users, items, and user preferences evolve over time. Existing LoRA-based continual methods primarily focus on preserving performance on previous tasks, but this overlooks the unique nature of recommendation: the goal is not to predict past preferences, and outdated preferences can even harm performance when current interests shift significantly. To address this, we propose PESO (Proximally rEgularized Single evolving lOra, a continual adaptation method for LoRA in recommendation. PESO introduces a proximal regularizer that anchors the current adapter to its most recent frozen state, enabling the model to flexibly balance adaptation and preservation, and to better capture recent user behaviors. Theoretically, we show that this proximal design provides data-aware, direction-wise guidance in the LoRA subspace. Empirically, PESO consistently outperforms existing LoRA-based continual learning methods.",
        "translated": "尽管大语言模型（LLM）在推荐系统中取得了优异性能，但在用户、物料及用户偏好随时间演变的持续学习场景下仍面临挑战。现有的基于LoRA的持续学习方法主要关注保持在先前任务上的性能，但忽视了推荐系统的独特性质：其目标并非预测过去的偏好，当用户当前兴趣发生显著变化时，过时的偏好甚至可能损害性能。为此，我们提出PESO（Proximally rEgularized Single evolving lOra），一种面向推荐系统的LoRA持续适配方法。PESO引入了一种近端正则化项，将当前适配器锚定在其最近冻结的状态上，从而使得模型能够灵活平衡适应与保留，更有效地捕捉近期用户行为。理论上，我们证明该近端设计在LoRA子空间中提供了数据感知、方向性的指导。实验表明，PESO在各类指标上始终优于现有的基于LoRA的持续学习方法。",
        "translated_title": "基于大语言模型的生成式推荐系统中的持续低秩适配器",
        "label": [
            "LLM生成式推荐（LLM-based Generative Recommendation）",
            "序列推荐（Sequential Recommendation）",
            "负采样与对比学习（Negative Sampling / Contrastive Learning）"
        ],
        "label_reason": "论文聚焦LLM生成式推荐中的持续学习，提出新方法PESO适应动态用户偏好，与推荐核心紧密相关。",
        "relevance_score": 9,
        "novelty_score": 8,
        "novelty_reason": "提出proximal正则化机制，平衡适应与保留，提升LLM在推荐场景下的持续学习能力，具显著创新性。"
    },
    {
        "title": "Secure Retrieval-Augmented Generation against Poisoning Attacks",
        "url": "http://arxiv.org/abs/2510.25025v1",
        "pub_date": "2025-10-28",
        "summary": "Large language models (LLMs) have transformed natural language processing (NLP), enabling applications from content generation to decision support. Retrieval-Augmented Generation (RAG) improves LLMs by incorporating external knowledge but also introduces security risks, particularly from data poisoning, where the attacker injects poisoned texts into the knowledge database to manipulate system outputs. While various defenses have been proposed, they often struggle against advanced attacks. To address this, we introduce RAGuard, a detection framework designed to identify poisoned texts. RAGuard first expands the retrieval scope to increase the proportion of clean texts, reducing the likelihood of retrieving poisoned content. It then applies chunk-wise perplexity filtering to detect abnormal variations and text similarity filtering to flag highly similar texts. This non-parametric approach enhances RAG security, and experiments on large-scale datasets demonstrate its effectiveness in detecting and mitigating poisoning attacks, including strong adaptive attacks.",
        "translated": "大语言模型（LLM）已彻底改变自然语言处理（NLP）领域，推动了从内容生成到决策支持等各类应用的发展。检索增强生成（RAG）通过引入外部知识提升了LLM的性能，但同时也带来了安全风险，尤其是在数据投毒攻击中，攻击者将中毒文本注入知识库以操纵系统输出。尽管已有多种防御方案被提出，但它们在应对高级攻击时往往效果有限。为此，我们提出RAGuard，一种用于识别中毒文本的检测框架。RAGuard首先扩展检索范围，提高干净文本的占比，从而降低检索到中毒内容的概率。随后，采用分块困惑度过滤来检测异常变化，并结合文本相似性过滤识别高度相似的文本。该非参数化方法有效增强了RAG的安全性，大规模数据集上的实验表明，其在检测和缓解投毒攻击方面具有显著效果，包括应对强自适应攻击。",
        "translated_title": "安全的检索增强生成对抗投毒攻击",
        "label": [],
        "label_reason": "论文聚焦RAG安全防御，属通用NLP领域，与推荐系统无直接关联。",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出非参数化检测框架，结合困惑度与相似度过滤，方法新颖有效。"
    },
    {
        "title": "Seeing Through the MiRAGE: Evaluating Multimodal Retrieval Augmented\n  Generation",
        "url": "http://arxiv.org/abs/2510.24870v1",
        "pub_date": "2025-10-28",
        "summary": "We introduce MiRAGE, an evaluation framework for retrieval-augmented generation (RAG) from multimodal sources. As audiovisual media becomes a prevalent source of information online, it is essential for RAG systems to integrate information from these sources into generation. However, existing evaluations for RAG are text-centric, limiting their applicability to multimodal, reasoning intensive settings because they don't verify information against sources. MiRAGE is a claim-centric approach to multimodal RAG evaluation, consisting of InfoF1, evaluating factuality and information coverage, and CiteF1, measuring citation support and completeness. We show that MiRAGE, when applied by humans, strongly aligns with extrinsic quality judgments. We additionally introduce automatic variants of MiRAGE and three prominent TextRAG metrics -- ACLE, ARGUE, and RAGAS -- demonstrating the limitations of text-centric work and laying the groundwork for automatic evaluation. We release open-source implementations and outline how to assess multimodal RAG.",
        "translated": "我们提出MiRAGE，一个用于多模态源检索增强生成（RAG）的评估框架。随着音视频媒体成为网络上普遍的信息来源，RAG系统有必要将这些来源的信息整合到生成过程中。然而，现有的RAG评估方法主要以文本为中心，限制了其在多模态、推理密集型场景中的适用性，因为它们无法对生成内容与来源信息进行核验。MiRAGE是一种以断言为中心的多模态RAG评估方法，包含InfoF1（用于评估事实性与信息覆盖度）和CiteF1（用于衡量引文支持与完整性）。我们证明，当由人类应用MiRAGE进行评估时，其结果与外部质量判断高度一致。此外，我们还引入了MiRAGE的自动评估变体，以及三种主流的TextRAG评估指标——ACLE、ARGUE和RAGAS，展示了以文本为中心的评估方法的局限性，并为自动评估奠定了基础。我们开源了相关实现，并概述了如何评估多模态RAG系统。",
        "translated_title": "透过MiRAGE看真相：评估多模态检索增强生成",
        "label": [
            "多模态推荐",
            "LLM生成式推荐"
        ],
        "label_reason": "论文聚焦多模态RAG评估，涉及生成式推荐中信息检索与生成的结合，与推荐系统间接相关。",
        "relevance_score": 4,
        "novelty_score": 7,
        "novelty_reason": "提出多模态RAG评估框架MiRAGE，包含新指标，对现有文本中心评估方法有显著改进。"
    },
    {
        "title": "ProfOlaf: Semi-Automated Tool for Systematic Literature Reviews",
        "url": "http://arxiv.org/abs/2510.26750v1",
        "pub_date": "2025-10-30",
        "summary": "Systematic reviews and mapping studies are critical for synthesizing research, identifying gaps, and guiding future work, but they are often labor-intensive and time-consuming. Existing tools provide partial support for specific steps, leaving much of the process manual and error-prone. We present ProfOlaf, a semi-automated tool designed to streamline systematic reviews while maintaining methodological rigor. ProfOlaf supports iterative snowballing for article collection with human-in-the-loop filtering and uses large language models to assist in analyzing articles, extracting key topics, and answering queries about the content of papers. By combining automation with guided manual effort, ProfOlaf enhances the efficiency, quality, and reproducibility of systematic reviews across research fields. A video describing and demonstrating ProfOlaf is available at: https://youtu.be/4noUXfcmxsE",
        "translated": "系统综述与映射研究对于整合研究成果、识别研究空白以及指导未来工作至关重要，但这些过程通常耗时耗力。现有工具仅对特定步骤提供部分支持，导致大部分流程仍需人工操作，易出错。我们提出 ProfOlaf，一种半自动化工具，旨在简化系统综述流程，同时保持方法学严谨性。ProfOlaf 支持带有人工介入过滤的迭代滚雪球式文献收集，并利用大语言模型辅助分析文献、提取关键主题以及回答关于论文内容的查询。通过结合自动化与引导式人工努力，ProfOlaf 提升了跨研究领域系统综述的效率、质量与可复现性。介绍并演示 ProfOlaf 的视频可访问：https://youtu.be/4noUXfcmxsE",
        "translated_title": "ProfOlaf: 用于系统性文献综述的半自动化工具",
        "label": [],
        "label_reason": "论文为文献综述工具，不涉及推荐系统任何环节",
        "relevance_score": 1,
        "novelty_score": 4,
        "novelty_reason": "基于LLM辅助文献分析，属通用NLP应用，无推荐领域创新"
    },
    {
        "title": "AdSum: Two-stream Audio-visual Summarization for Automated Video\n  Advertisement Clipping",
        "url": "http://arxiv.org/abs/2510.26569v1",
        "pub_date": "2025-10-30",
        "summary": "Advertisers commonly need multiple versions of the same advertisement (ad) at varying durations for a single campaign. The traditional approach involves manually selecting and re-editing shots from longer video ads to create shorter versions, which is labor-intensive and time-consuming. In this paper, we introduce a framework for automated video ad clipping using video summarization techniques. We are the first to frame video clipping as a shot selection problem, tailored specifically for advertising. Unlike existing general video summarization methods that primarily focus on visual content, our approach emphasizes the critical role of audio in advertising. To achieve this, we develop a two-stream audio-visual fusion model that predicts the importance of video frames, where importance is defined as the likelihood of a frame being selected in the firm-produced short ad. To address the lack of ad-specific datasets, we present AdSum204, a novel dataset comprising 102 pairs of 30-second and 15-second ads from real advertising campaigns. Extensive experiments demonstrate that our model outperforms state-of-the-art methods across various metrics, including Average Precision, Area Under Curve, Spearman, and Kendall.",
        "translated": "广告商通常需要在单个广告活动中为同一则广告（ad）提供不同时长的多个版本。传统方法需要人工从较长的视频广告中选取并重新编辑镜头，以生成较短版本，这一过程耗时且劳动密集。本文提出一种基于视频摘要技术的自动化视频广告剪辑框架。我们首次将视频剪辑问题形式化为一个面向广告场景的镜头选择问题。与现有通用视频摘要方法主要关注视觉内容不同，我们的方法强调音频在广告中的关键作用。为此，我们构建了一个双流音视频融合模型，用于预测视频帧的重要性，其中重要性定义为某帧被选入广告公司制作的短版广告中的概率。为解决广告专用数据集缺失的问题，我们发布了AdSum204，一个包含102对来自真实广告活动的30秒与15秒广告的新数据集。大量实验表明，我们的模型在平均精度（Average Precision）、曲线下面积（Area Under Curve）、斯皮尔曼相关系数（Spearman）和肯德尔相关系数（Kendall）等多个指标上均优于现有最先进方法。",
        "translated_title": "AdSum：面向自动化视频广告剪辑的双流音视频摘要方法",
        "label": [],
        "label_reason": "论文聚焦广告视频剪辑，属于视频处理与内容生成，与推荐系统无直接关联。",
        "relevance_score": 3,
        "novelty_score": 7,
        "novelty_reason": "提出双流音视频融合模型，创新性地引入音频特征，对广告剪辑任务有显著改进。"
    },
    {
        "title": "WeaveRec: An LLM-Based Cross-Domain Sequential Recommendation Framework\n  with Model Merging",
        "url": "http://arxiv.org/abs/2510.26546v1",
        "pub_date": "2025-10-30",
        "summary": "Cross-Domain Sequential Recommendation (CDSR) seeks to improve user preference modeling by transferring knowledge from multiple domains. Despite the progress made in CDSR, most existing methods rely on overlapping users or items to establish cross-domain correlations-a requirement that rarely holds in real-world settings. The advent of large language models (LLM) and model-merging techniques appears to overcome this limitation by unifying multi-domain data without explicit overlaps. Yet, our empirical study shows that naively training an LLM on combined domains-or simply merging several domain-specific LLMs-often degrades performance relative to a model trained solely on the target domain. To address these challenges, we first experimentally investigate the cause of suboptimal performance in LLM-based cross-domain recommendation and model merging. Building on these insights, we introduce WeaveRec, which cross-trains multiple LoRA modules with source and target domain data in a weaving fashion, and fuses them via model merging. WeaveRec can be extended to multi-source domain scenarios and notably does not introduce additional inference-time cost in terms of latency or memory. Furthermore, we provide a theoretical guarantee that WeaveRec can reduce the upper bound of the expected error in the target domain. Extensive experiments on single-source, multi-source, and cross-platform cross-domain recommendation scenarios validate that WeaveRec effectively mitigates performance degradation and consistently outperforms baseline approaches in real-world recommendation tasks.",
        "translated": "跨域序列推荐（CDSR）旨在通过从多个领域迁移知识来提升用户偏好建模能力。尽管CDSR领域已取得一定进展，但现有大多数方法依赖于用户或物料在不同领域间的重叠来建立跨域关联——这一前提在真实场景中往往难以满足。大型语言模型（LLM）与模型融合技术的出现，似乎通过无需显式重叠即可统一多域数据的方式克服了这一限制。然而，我们的实证研究表明，简单地在合并后的多域数据上训练LLM，或直接合并多个领域专用LLM，通常会导致性能劣于仅在目标域上训练的模型。为应对这些挑战，我们首先实验性地探究了基于LLM的跨域推荐及模型融合中性能欠佳的原因。基于这些洞察，我们提出WeaveRec，该方法以“交织”方式利用源域与目标域数据交叉训练多个LoRA模块，并通过模型融合进行整合。WeaveRec可扩展至多源域场景，且在推理阶段不引入额外的延迟或内存开销。此外，我们提供了理论保证，表明WeaveRec能够降低目标域预期误差的上界。在单源、多源及跨平台跨域推荐场景下的大量实验验证表明，WeaveRec有效缓解了性能退化问题，并在真实推荐任务中始终优于基线方法。",
        "translated_title": "WeaveRec：一种基于大语言模型的跨域序列推荐框架及其模型融合方法",
        "label": [
            "序列推荐",
            "跨域/联邦推荐",
            "LLM生成式推荐",
            "通用推荐技术"
        ],
        "label_reason": "论文聚焦跨域序列推荐，结合LLM与模型合并技术，直接解决推荐系统核心问题",
        "relevance_score": 9,
        "novelty_score": 8,
        "novelty_reason": "提出WeaveRec框架，创新性地结合LoRA与模型合并，提升跨域推荐性能"
    },
    {
        "title": "Inside CORE-KG: Evaluating Structured Prompting and Coreference\n  Resolution for Knowledge Graphs",
        "url": "http://arxiv.org/abs/2510.26512v1",
        "pub_date": "2025-10-30",
        "summary": "Human smuggling networks are increasingly adaptive and difficult to analyze. Legal case documents offer critical insights but are often unstructured, lexically dense, and filled with ambiguous or shifting references, which pose significant challenges for automated knowledge graph (KG) construction. While recent LLM-based approaches improve over static templates, they still generate noisy, fragmented graphs with duplicate nodes due to the absence of guided extraction and coreference resolution. The recently proposed CORE-KG framework addresses these limitations by integrating a type-aware coreference module and domain-guided structured prompts, significantly reducing node duplication and legal noise. In this work, we present a systematic ablation study of CORE-KG to quantify the individual contributions of its two key components. Our results show that removing coreference resolution results in a 28.32% increase in node duplication and a 4.32% increase in noisy nodes, while removing structured prompts leads to a 4.34% increase in node duplication and a 73.33% increase in noisy nodes. These findings offer empirical insights for designing robust LLM-based pipelines for extracting structured representations from complex legal texts.",
        "translated": "人口走私网络日益具有适应性，分析难度不断加大。法律案件文档虽提供了关键信息，但通常结构松散、词汇密集，且包含大量模糊或指代变化的表达，给自动化知识图谱（KG）构建带来显著挑战。尽管近期基于大语言模型（LLM）的方法相较于静态模板有所改进，但由于缺乏引导式抽取和共指消解，仍会产生带有噪声、碎片化且存在重复节点的知识图谱。最近提出的 CORE-KG 框架通过集成类型感知的共指消解模块和领域引导的结构化提示，显著减少了节点重复和法律噪声。在本研究中，我们对 CORE-KG 进行了系统的消融实验，以量化其两个核心组件的独立贡献。实验结果表明，移除共指消解会导致节点重复率增加 28.32%，噪声节点增加 4.32%；而移除结构化提示则导致节点重复率增加 4.34%，噪声节点增加 73.33%。这些发现为设计稳健的基于大语言模型的复杂法律文本结构化表示抽取流水线提供了实证依据。",
        "translated_title": "在CORE-KG中：评估知识图谱的结构化提示与共指消解",
        "label": [],
        "label_reason": "论文聚焦法律文本知识图谱构建，与推荐系统无直接关联。",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出结构化提示与共指消解模块，对LLM抽取有改进，但非推荐领域创新。"
    },
    {
        "title": "LINK-KG: LLM-Driven Coreference-Resolved Knowledge Graphs for Human\n  Smuggling Networks",
        "url": "http://arxiv.org/abs/2510.26486v1",
        "pub_date": "2025-10-30",
        "summary": "Human smuggling networks are complex and constantly evolving, making them difficult to analyze comprehensively. Legal case documents offer rich factual and procedural insights into these networks but are often long, unstructured, and filled with ambiguous or shifting references, posing significant challenges for automated knowledge graph (KG) construction. Existing methods either overlook coreference resolution or fail to scale beyond short text spans, leading to fragmented graphs and inconsistent entity linking. We propose LINK-KG, a modular framework that integrates a three-stage, LLM-guided coreference resolution pipeline with downstream KG extraction. At the core of our approach is a type-specific Prompt Cache, which consistently tracks and resolves references across document chunks, enabling clean and disambiguated narratives for structured knowledge graph construction from both short and long legal texts. LINK-KG reduces average node duplication by 45.21% and noisy nodes by 32.22% compared to baseline methods, resulting in cleaner and more coherent graph structures. These improvements establish LINK-KG as a strong foundation for analyzing complex criminal networks.",
        "translated": "人口走私网络复杂且持续演变，难以进行全面分析。法律案件文件虽提供了关于这些网络的丰富事实与程序性信息，但通常篇幅较长、结构松散，且包含大量模糊或指代变化的表达，给自动化知识图谱（KG）构建带来了显著挑战。现有方法要么忽略共指消解，要么无法扩展至长文本片段，导致生成的知识图谱碎片化且实体链接不一致。我们提出LINK-KG，一种模块化框架，将三阶段、大语言模型（LLM）引导的共指消解流水线与下游知识图谱抽取相结合。本方法的核心是类型特定的Prompt缓存，它能够持续追踪并消解文档片段中的指代关系，从而为短文本和长法律文本的结构化知识图谱构建提供清晰、无歧义的叙述。与基线方法相比，LINK-KG将平均节点重复率降低45.21%，噪声节点减少32.22%，生成更干净、更连贯的图结构。这些改进使LINK-KG成为分析复杂犯罪网络的坚实基础。",
        "translated_title": "LINK-KG：基于大语言模型的共指消解知识图谱在人口走私网络中的应用",
        "label": [],
        "label_reason": "论文聚焦于法律文本中的知识图谱构建与指代消解，与推荐系统无直接关联。",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出LLM引导的三阶段指代消解框架，结合类型特定提示缓存，具有创新性，但非推荐领域。"
    },
    {
        "title": "Vectorized Context-Aware Embeddings for GAT-Based Collaborative\n  Filtering",
        "url": "http://arxiv.org/abs/2510.26461v1",
        "pub_date": "2025-10-30",
        "summary": "Recommender systems often struggle with data sparsity and cold-start scenarios, limiting their ability to provide accurate suggestions for new or infrequent users. This paper presents a Graph Attention Network (GAT) based Collaborative Filtering (CF) framework enhanced with Large Language Model (LLM) driven context aware embeddings. Specifically, we generate concise textual user profiles and unify item metadata (titles, genres, overviews) into rich textual embeddings, injecting these as initial node features in a bipartite user item graph. To further optimize ranking performance, we introduce a hybrid loss function that combines Bayesian Personalized Ranking (BPR) with a cosine similarity term and robust negative sampling, ensuring explicit negative feedback is distinguished from unobserved data. Experiments on the MovieLens 100k and 1M datasets show consistent improvements over state-of-the-art baselines in Precision, NDCG, and MAP while demonstrating robustness for users with limited interaction history. Ablation studies confirm the critical role of LLM-augmented embeddings and the cosine similarity term in capturing nuanced semantic relationships. Our approach effectively mitigates sparsity and cold-start limitations by integrating LLM-derived contextual understanding into graph-based architectures. Future directions include balancing recommendation accuracy with coverage and diversity, and introducing fairness-aware constraints and interpretability features to enhance system performance further.",
        "translated": "推荐系统常面临数据稀疏性和冷启动场景的挑战，限制了其为新用户或交互较少用户提供准确建议的能力。本文提出一种基于图注意力网络（GAT）的协同过滤（CF）框架，并引入由大语言模型（LLM）驱动的上下文感知嵌入。具体而言，我们生成简洁的文本化用户画像，并将物料元数据（如标题、类型、简介）统一整合为丰富的文本嵌入，将其作为二分图用户-物料图中的初始节点特征注入。为进一步优化排序性能，我们引入一种混合损失函数，结合贝叶斯个性化排序（BPR）与余弦相似度项，并采用鲁棒的负采样策略，确保显式负反馈与未观测数据能够被有效区分。在MovieLens 100k和1M数据集上的实验表明，与当前主流基线相比，我们的方法在精确率（Precision）、NDCG和MAP等指标上均取得稳定提升，同时对交互历史较少的用户表现出良好的鲁棒性。消融实验验证了LLM增强嵌入和余弦相似度项在捕捉细微语义关系中的关键作用。通过将LLM衍生的上下文理解融入图结构架构，我们的方法有效缓解了数据稀疏性和冷启动问题。未来研究方向包括在推荐准确率与覆盖率、多样性之间取得平衡，并引入公平性约束与可解释性特征，以进一步提升系统性能。",
        "translated_title": "向量化上下文感知嵌入用于基于GAT的协同过滤",
        "label": [
            "图神经网络推荐（GNN for Recommendation）",
            "序列推荐（Sequential Recommendation）",
            "LLM生成式推荐（LLM-based Generative Recommendation）",
            "负采样与对比学习（Negative Sampling / Contrastive Learning）"
        ],
        "label_reason": "基于GAT的协同过滤框架结合LLM生成上下文感知嵌入，解决稀疏性与冷启动，直接用于推荐精排环节。",
        "relevance_score": 9,
        "novelty_score": 8,
        "novelty_reason": "创新性融合LLM与GAT，引入混合损失函数优化排序，显著提升推荐性能与鲁棒性。"
    },
    {
        "title": "Barlow Twins for Sequential Recommendation",
        "url": "http://arxiv.org/abs/2510.26407v1",
        "pub_date": "2025-10-30",
        "summary": "Sequential recommendation models must navigate sparse interaction data popularity bias and conflicting objectives like accuracy versus diversity While recent contrastive selfsupervised learning SSL methods offer improved accuracy they come with tradeoffs large batch requirements reliance on handcrafted augmentations and negative sampling that can reinforce popularity bias In this paper we introduce BT-SR a novel noncontrastive SSL framework that integrates the Barlow Twins redundancyreduction principle into a Transformerbased nextitem recommender BTSR learns embeddings that align users with similar shortterm behaviors while preserving longterm distinctionswithout requiring negative sampling or artificial perturbations This structuresensitive alignment allows BT-SR to more effectively recognize emerging user intent and mitigate the influence of noisy historical context Our experiments on five public benchmarks demonstrate that BTSR consistently improves nextitem prediction accuracy and significantly enhances longtail item coverage and recommendation calibration Crucially we show that a single hyperparameter can control the accuracydiversity tradeoff enabling practitioners to adapt recommendations to specific application needs",
        "translated": "序列推荐模型必须应对稀疏交互数据、流行度偏差以及准确性与多样性之间的冲突目标。尽管近期的对比自监督学习（SSL）方法在提升准确性方面取得进展，但其存在若干权衡：需要较大的批次规模、依赖于手工设计的数据增强策略，以及负采样机制可能加剧流行度偏差。本文提出 BT-SR，一种新颖的非对比式 SSL 框架，将 Barlow Twins 的冗余消除原则集成到基于 Transformer 的下一项推荐模型中。BT-SR 学习用户嵌入，使其在相似短期行为上对齐，同时保留长期行为的区分性，且无需负采样或人工扰动。这种结构敏感的对齐机制使 BT-SR 能更有效地识别用户新兴意图，并减轻噪声历史上下文的影响。在五个公开基准数据集上的实验表明，BT-SR 一致提升了下一项预测的准确性，显著增强了长尾物料的覆盖率和推荐校准性能。尤为重要的是，我们证明单个超参数即可控制准确性与多样性的权衡，使实践者能够根据具体应用场景灵活调整推荐策略。",
        "translated_title": "Barlow Twins 用于序列推荐",
        "label": [
            "序列推荐",
            "负采样与对比学习",
            "通用推荐技术"
        ],
        "label_reason": "论文提出非对比式自监督框架用于序列推荐，解决负采样偏差问题，直接关联推荐系统核心环节。",
        "relevance_score": 8,
        "novelty_score": 8,
        "novelty_reason": "创新性引入Barlow Twins机制替代传统对比学习，无需负采样，有效缓解流行度偏差，提升长尾覆盖。"
    },
    {
        "title": "GraphCompliance: Aligning Policy and Context Graphs for LLM-Based\n  Regulatory Compliance",
        "url": "http://arxiv.org/abs/2510.26309v1",
        "pub_date": "2025-10-30",
        "summary": "Compliance at web scale poses practical challenges: each request may require a regulatory assessment. Regulatory texts (e.g., the General Data Protection Regulation, GDPR) are cross-referential and normative, while runtime contexts are expressed in unstructured natural language. This setting motivates us to align semantic information in unstructured text with the structured, normative elements of regulations. To this end, we introduce GraphCompliance, a framework that represents regulatory texts as a Policy Graph and runtime contexts as a Context Graph, and aligns them. In this formulation, the policy graph encodes normative structure and cross-references, whereas the context graph formalizes events as subject-action-object (SAO) and entity-relation triples. This alignment anchors the reasoning of a judge large language model (LLM) in structured information and helps reduce the burden of regulatory interpretation and event parsing, enabling a focus on the core reasoning step. In experiments on 300 GDPR-derived real-world scenarios spanning five evaluation tasks, GraphCompliance yields 4.1-7.2 percentage points (pp) higher micro-F1 than LLM-only and RAG baselines, with fewer under- and over-predictions, resulting in higher recall and lower false positive rates. Ablation studies indicate contributions from each graph component, suggesting that structured representations and a judge LLM are complementary for normative reasoning.",
        "translated": "在大规模网络场景下，合规性面临实际挑战：每个请求可能都需要进行法规评估。法规文本（例如《通用数据保护条例》，GDPR）具有交叉引用和规范性特征，而运行时上下文则以非结构化的自然语言表达。这一设定促使我们对非结构化文本中的语义信息与法规的结构化、规范性要素进行对齐。为此，我们提出 GraphCompliance 框架，该框架将法规文本表示为政策图（Policy Graph），将运行时上下文表示为上下文图（Context Graph），并实现二者对齐。在此建模中，政策图编码了规范性结构和交叉引用，而上下文图则将事件形式化为“主体-动作-客体”（SAO）三元组和“实体-关系”三元组。这种对齐方式将判别大语言模型（LLM）的推理锚定于结构化信息，有助于减轻法规解释和事件解析的负担，使模型能够专注于核心推理步骤。在涵盖五个评估任务的300个源自GDPR的真实场景实验中，GraphCompliance 相较于仅使用LLM和RAG的基线方法，微F1得分高出4.1至7.2个百分点（pp），且误判和过判情况更少，从而实现了更高的召回率和更低的误报率。消融实验表明，各图组件均对性能提升有贡献，说明结构化表示与判别LLM在规范性推理中具有互补性。",
        "translated_title": "GraphCompliance：对齐策略图与上下文图以实现基于大语言模型的合规性监管",
        "label": [],
        "label_reason": "论文聚焦法律合规判断，虽用LLM和图结构，但非推荐系统核心问题，与推荐无直接关联。",
        "relevance_score": 3,
        "novelty_score": 7,
        "novelty_reason": "提出政策图与上下文图对齐新框架，结合LLM提升合规判断准确率，方法新颖但非推荐领域。"
    },
    {
        "title": "DiSE: A diffusion probabilistic model for automatic structure\n  elucidation of organic compounds",
        "url": "http://arxiv.org/abs/2510.26231v1",
        "pub_date": "2025-10-30",
        "summary": "Automatic structure elucidation is essential for self-driving laboratories as it enables the system to achieve truly autonomous. This capability closes the experimental feedback loop, ensuring that machine learning models receive reliable structure information for real-time decision-making and optimization. Herein, we present DiSE, an end-to-end diffusion-based generative model that integrates multiple spectroscopic modalities, including MS, 13C and 1H chemical shifts, HSQC, and COSY, to achieve automated yet accurate structure elucidation of organic compounds. By learning inherent correlations among spectra through data-driven approaches, DiSE achieves superior accuracy, strong generalization across chemically diverse datasets, and robustness to experimental data despite being trained on calculated spectra. DiSE thus represents a significant advance toward fully automated structure elucidation, with broad potential in natural product research, drug discovery, and self-driving laboratories.",
        "translated": "自动结构解析对于自动驾驶实验室至关重要，因为它使系统能够实现真正的自主性。该能力闭合了实验反馈回路，确保机器学习模型能够获得可靠的结构信息，以支持实时决策与优化。本文提出DiSE，一种端到端的基于扩散的生成模型，该模型整合了多种光谱模态，包括MS、13C和1H化学位移、HSQC以及COSY，以实现有机化合物的自动化且准确的结构解析。通过数据驱动的方法学习光谱之间的内在关联，DiSE在准确性、跨化学多样性数据集的泛化能力以及对实验数据的鲁棒性方面均表现出色，尽管其训练数据为计算得到的光谱。因此，DiSE代表了迈向完全自动化结构解析的重要进展，在天然产物研究、药物发现以及自动驾驶实验室等领域具有广阔的应用潜力。",
        "translated_title": "DiSE：一种用于有机化合物自动结构阐明的扩散概率模型",
        "label": [],
        "label_reason": "论文聚焦有机化合物结构解析，属于化学信息学，与推荐系统无直接关联。",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出基于扩散模型的多模态生成方法，对结构解析有创新，但非推荐领域。"
    },
    {
        "title": "ReaKase-8B: Legal Case Retrieval via Knowledge and Reasoning\n  Representations with LLMs",
        "url": "http://arxiv.org/abs/2510.26178v1",
        "pub_date": "2025-10-30",
        "summary": "Legal case retrieval (LCR) is a cornerstone of real-world legal decision making, as it enables practitioners to identify precedents for a given query case. Existing approaches mainly rely on traditional lexical models and pretrained language models to encode the texts of legal cases. Yet there are rich information in the relations among different legal entities as well as the crucial reasoning process that uncovers how legal facts and legal issues can lead to judicial decisions. Such relational reasoning process reflects the distinctive characteristics of each case that can distinguish one from another, mirroring the real-world judicial process. Naturally, incorporating such information into the precise case embedding could further enhance the accuracy of case retrieval. In this paper, a novel ReaKase-8B framework is proposed to leverage extracted legal facts, legal issues, legal relation triplets and legal reasoning for effective legal case retrieval. ReaKase-8B designs an in-context legal case representation learning paradigm with a fine-tuned large language model. Extensive experiments on two benchmark datasets from COLIEE 2022 and COLIEE 2023 demonstrate that our knowledge and reasoning augmented embeddings substantially improve retrieval performance over baseline models, highlighting the potential of integrating legal reasoning into legal case retrieval systems. The code has been released on https://github.com/yanran-tang/ReaKase-8B.",
        "translated": "法律案例检索（Legal Case Retrieval, LCR）是现实世界法律决策的核心环节，它使从业者能够为给定的查询案例识别出相关判例。现有方法主要依赖传统词法模型和预训练语言模型对法律案例文本进行编码。然而，不同法律实体之间的关系蕴含着丰富的信息，同时，揭示法律事实与法律问题如何导向司法判决的关键推理过程同样至关重要。这种关系推理过程反映了每个案例的独特特征，能够有效区分不同案例，体现了真实的司法推理流程。因此，将此类信息融入精确的案例嵌入表示中，有望进一步提升案例检索的准确性。本文提出了一种新颖的 ReaKase-8B 框架，旨在利用提取出的法律事实、法律问题、法律关系三元组以及法律推理过程，实现高效的法律案例检索。ReaKase-8B 设计了一种基于微调大语言模型的上下文法律案例表示学习范式。在 COLIEE 2022 和 COLIEE 2023 两个基准数据集上的大量实验表明，我们的知识与推理增强型嵌入显著优于基线模型，验证了将法律推理融入法律案例检索系统的潜力。代码已发布于 https://github.com/yanran-tang/ReaKase-8B。",
        "translated_title": "ReaKase-8B：基于大语言模型的知识与推理表示的法律案例检索",
        "label": [
            "召回",
            "LLM生成式推荐",
            "负采样与对比学习"
        ],
        "label_reason": "论文聚焦法律案例检索，属信息检索范畴，与推荐系统间接相关，核心为召回环节。",
        "relevance_score": 4,
        "novelty_score": 7,
        "novelty_reason": "提出基于LLM的上下文法律案例表示学习范式，结合知识与推理增强嵌入，方法新颖且有效。"
    },
    {
        "title": "OneTrans: Unified Feature Interaction and Sequence Modeling with One\n  Transformer in Industrial Recommender",
        "url": "http://arxiv.org/abs/2510.26104v1",
        "pub_date": "2025-10-30",
        "summary": "In recommendation systems, scaling up feature-interaction modules (e.g., Wukong, RankMixer) or user-behavior sequence modules (e.g., LONGER) has achieved notable success. However, these efforts typically proceed on separate tracks, which not only hinders bidirectional information exchange but also prevents unified optimization and scaling. In this paper, we propose OneTrans, a unified Transformer backbone that simultaneously performs user-behavior sequence modeling and feature interaction. OneTrans employs a unified tokenizer to convert both sequential and non-sequential attributes into a single token sequence. The stacked OneTrans blocks share parameters across similar sequential tokens while assigning token-specific parameters to non-sequential tokens. Through causal attention and cross-request KV caching, OneTrans enables precomputation and caching of intermediate representations, significantly reducing computational costs during both training and inference. Experimental results on industrial-scale datasets demonstrate that OneTrans scales efficiently with increasing parameters, consistently outperforms strong baselines, and yields a 5.68% lift in per-user GMV in online A/B tests.",
        "translated": "在推荐系统中，扩展特征交互模块（如 Wukong、RankMixer）或用户行为序列模块（如 LONGER）已取得显著成效。然而，这些努力通常在独立的路径上进行，不仅阻碍了双向信息交换，也妨碍了统一优化与扩展。本文提出 OneTrans，一种统一的 Transformer 主干网络，能够同时完成用户行为序列建模与特征交互。OneTrans 采用统一的分词器，将序列化与非序列化属性均转换为单一的 token 序列。堆叠的 OneTrans 模块在相似的序列化 token 间共享参数，同时为非序列化 token 分配特定的参数。通过因果注意力机制与跨请求 KV 缓存，OneTrans 实现了中间表示的预计算与缓存，显著降低了训练与推理阶段的计算成本。在工业级数据集上的实验结果表明，OneTrans 能够随着参数量增加高效扩展，持续优于强基线模型，并在在线 A/B 测试中实现每用户 GMV 提升 5.68%。",
        "translated_title": "OneTrans：工业推荐系统中基于单一Transformer的统一特征交互与序列建模",
        "label": [
            "精排（Ranking）",
            "序列推荐（Sequential Recommendation）",
            "通用推荐技术（General Recommendation Techniques）"
        ],
        "label_reason": "论文提出统一Transformer架构，融合序列建模与特征交互，直接用于推荐系统精排环节，显著提升性能。",
        "relevance_score": 9,
        "novelty_score": 8,
        "novelty_reason": "创新性地统一序列与非序列特征处理，共享参数并支持KV缓存，提升效率与可扩展性，属结构化改进。"
    },
    {
        "title": "ORBIT - Open Recommendation Benchmark for Reproducible Research with\n  Hidden Tests",
        "url": "http://arxiv.org/abs/2510.26095v1",
        "pub_date": "2025-10-30",
        "summary": "Recommender systems are among the most impactful AI applications, interacting with billions of users every day, guiding them to relevant products, services, or information tailored to their preferences. However, the research and development of recommender systems are hindered by existing datasets that fail to capture realistic user behaviors and inconsistent evaluation settings that lead to ambiguous conclusions. This paper introduces the Open Recommendation Benchmark for Reproducible Research with HIdden Tests (ORBIT), a unified benchmark for consistent and realistic evaluation of recommendation models. ORBIT offers a standardized evaluation framework of public datasets with reproducible splits and transparent settings for its public leaderboard. Additionally, ORBIT introduces a new webpage recommendation task, ClueWeb-Reco, featuring web browsing sequences from 87 million public, high-quality webpages. ClueWeb-Reco is a synthetic dataset derived from real, user-consented, and privacy-guaranteed browsing data. It aligns with modern recommendation scenarios and is reserved as the hidden test part of our leaderboard to challenge recommendation models' generalization ability. ORBIT measures 12 representative recommendation models on its public benchmark and introduces a prompted LLM baseline on the ClueWeb-Reco hidden test. Our benchmark results reflect general improvements of recommender systems on the public datasets, with variable individual performances. The results on the hidden test reveal the limitations of existing approaches in large-scale webpage recommendation and highlight the potential for improvements with LLM integrations. ORBIT benchmark, leaderboard, and codebase are available at https://www.open-reco-bench.ai.",
        "translated": "推荐系统是影响力最大的人工智能应用之一，每天与数十亿用户交互，引导他们发现符合其偏好的相关产品、服务或信息。然而，推荐系统的研究与开发受到现有数据集的制约，这些数据集无法准确捕捉真实用户行为，且评估设置不一致，导致结论模糊。本文提出了面向可复现研究的隐藏测试开放推荐基准（Open Recommendation Benchmark for Reproducible Research with HIdden Tests, ORBIT），这是一个用于一致且真实评估推荐模型的统一基准。ORBIT 提供了包含可复现划分和透明设置的公开数据集标准化评估框架，并为公开排行榜提供支持。此外，ORBIT 引入了一个新的网页推荐任务 ClueWeb-Reco，该任务基于 8700 万份公开、高质量网页的浏览序列。ClueWeb-Reco 是一个从真实、用户同意且隐私保障的浏览数据中衍生出的合成数据集，契合现代推荐场景，并作为排行榜的隐藏测试部分，用于挑战推荐模型的泛化能力。ORBIT 在其公开基准上评估了 12 个代表性推荐模型，并在 ClueWeb-Reco 隐藏测试集上引入了一个提示式大语言模型（LLM）基线。我们的基准测试结果反映出推荐系统在公开数据集上的整体性能提升，但各模型个体表现存在差异。隐藏测试集的结果揭示了现有方法在大规模网页推荐场景中的局限性，并凸显了通过集成大语言模型实现性能提升的潜力。ORBIT 基准、排行榜及代码库可通过 https://www.open-reco-bench.ai 获取。",
        "translated_title": "ORBIT —— 带隐藏测试集的可复现研究开放推荐系统基准",
        "label": [
            "推荐系统评估",
            "通用推荐技术"
        ],
        "label_reason": "论文构建推荐系统基准ORBIT，含公开与隐藏测试集，用于评估推荐模型，属推荐系统评估与通用技术范畴。",
        "relevance_score": 8,
        "novelty_score": 7,
        "novelty_reason": "提出标准化评估框架与隐藏测试集，提升可复现性，但未提出新算法或模型架构。"
    },
    {
        "title": "The Quest for Reliable Metrics of Responsible AI",
        "url": "http://arxiv.org/abs/2510.26007v1",
        "pub_date": "2025-10-29",
        "summary": "The development of Artificial Intelligence (AI), including AI in Science (AIS), should be done following the principles of responsible AI. Progress in responsible AI is often quantified through evaluation metrics, yet there has been less work on assessing the robustness and reliability of the metrics themselves. We reflect on prior work that examines the robustness of fairness metrics for recommender systems as a type of AI application and summarise their key takeaways into a set of non-exhaustive guidelines for developing reliable metrics of responsible AI. Our guidelines apply to a broad spectrum of AI applications, including AIS.",
        "translated": "人工智能（AI）的发展，包括科学领域的人工智能（AIS），应遵循负责任AI的原则。负责任AI的进展通常通过评估指标进行量化，然而针对这些指标本身稳健性与可靠性的评估工作却相对较少。我们回顾了先前研究中关于推荐系统（一种AI应用）公平性指标稳健性的相关工作，并将其核心结论总结为一套非穷尽的指导原则，用于开发负责任AI的可靠评估指标。这些指导原则适用于广泛的AI应用场景，包括AIS。",
        "translated_title": "负责任人工智能可靠指标的探索",
        "label": [
            "推荐系统公平性/可解释性",
            "推荐系统评估"
        ],
        "label_reason": "论文聚焦推荐系统公平性评估指标的可靠性，与推荐系统评估和公平性相关",
        "relevance_score": 4,
        "novelty_score": 5,
        "novelty_reason": "提出评估指标可靠性的指南，非全新方法，属总结性改进"
    },
    {
        "title": "Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with\n  the MME-CoF Benchmark",
        "url": "http://arxiv.org/abs/2510.26802v1",
        "pub_date": "2025-10-30",
        "summary": "Recent video generation models can produce high-fidelity, temporally coherent videos, indicating that they may encode substantial world knowledge. Beyond realistic synthesis, they also exhibit emerging behaviors indicative of visual perception, modeling, and manipulation. Yet, an important question still remains: Are video models ready to serve as zero-shot reasoners in challenging visual reasoning scenarios? In this work, we conduct an empirical study to comprehensively investigate this question, focusing on the leading and popular Veo-3. We evaluate its reasoning behavior across 12 dimensions, including spatial, geometric, physical, temporal, and embodied logic, systematically characterizing both its strengths and failure modes. To standardize this study, we curate the evaluation data into MME-CoF, a compact benchmark that enables in-depth and thorough assessment of Chain-of-Frame (CoF) reasoning. Our findings reveal that while current video models demonstrate promising reasoning patterns on short-horizon spatial coherence, fine-grained grounding, and locally consistent dynamics, they remain limited in long-horizon causal reasoning, strict geometric constraints, and abstract logic. Overall, they are not yet reliable as standalone zero-shot reasoners, but exhibit encouraging signs as complementary visual engines alongside dedicated reasoning models. Project page: https://video-cof.github.io",
        "translated": "近期的视频生成模型能够生成高保真、时间连贯的视频，表明它们可能编码了丰富的世界知识。除了逼真的合成能力外，这些模型还展现出一些新兴行为，体现出视觉感知、建模与操控的能力。然而，一个重要问题仍然存在：视频模型是否已准备好在具有挑战性的视觉推理场景中充当零样本推理器？在本研究中，我们开展了一项实证研究，全面探讨这一问题，重点聚焦于当前主流且广受欢迎的 Veo-3 模型。我们从12个维度评估其推理行为，包括空间、几何、物理、时间以及具身逻辑，系统性地刻画了其优势与失败模式。为规范本研究，我们构建了 MME-CoF，一个紧凑型基准数据集，用于深入且全面地评估帧序列推理（Chain-of-Frame, CoF）能力。我们的研究结果表明，尽管当前视频模型在短时域空间一致性、细粒度 grounding 以及局部一致动态方面展现出有前景的推理模式，但在长时域因果推理、严格几何约束和抽象逻辑推理方面仍存在明显局限。总体而言，它们尚不能作为独立的零样本推理器可靠使用，但作为专用推理模型的辅助视觉引擎，已展现出令人鼓舞的潜力。项目主页：https://video-cof.github.io",
        "translated_title": "视频模型是否已具备零样本推理能力？基于MME-CoF基准的实证研究",
        "label": [],
        "label_reason": "论文聚焦视频模型在零样本推理能力评估，属于高阶视觉理解任务，非像素级图像恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "提出MME-CoF基准评估视频模型推理能力，属评估方法创新，非图像处理核心算法创新。"
    },
    {
        "title": "OmniX: From Unified Panoramic Generation and Perception to\n  Graphics-Ready 3D Scenes",
        "url": "http://arxiv.org/abs/2510.26800v1",
        "pub_date": "2025-10-30",
        "summary": "There are two prevalent ways to constructing 3D scenes: procedural generation and 2D lifting. Among them, panorama-based 2D lifting has emerged as a promising technique, leveraging powerful 2D generative priors to produce immersive, realistic, and diverse 3D environments. In this work, we advance this technique to generate graphics-ready 3D scenes suitable for physically based rendering (PBR), relighting, and simulation. Our key insight is to repurpose 2D generative models for panoramic perception of geometry, textures, and PBR materials. Unlike existing 2D lifting approaches that emphasize appearance generation and ignore the perception of intrinsic properties, we present OmniX, a versatile and unified framework. Based on a lightweight and efficient cross-modal adapter structure, OmniX reuses 2D generative priors for a broad range of panoramic vision tasks, including panoramic perception, generation, and completion. Furthermore, we construct a large-scale synthetic panorama dataset containing high-quality multimodal panoramas from diverse indoor and outdoor scenes. Extensive experiments demonstrate the effectiveness of our model in panoramic visual perception and graphics-ready 3D scene generation, opening new possibilities for immersive and physically realistic virtual world generation.",
        "translated": "构建三维场景主要有两种主流方法：程序化生成与二维提升（2D lifting）。其中，基于全景图的二维提升技术近年来展现出巨大潜力，通过利用强大的二维生成先验，能够生成沉浸式、逼真且多样化的三维环境。在本研究中，我们进一步发展该技术，以生成适用于基于物理的渲染（PBR）、重光照（relighting）和模拟的图形就绪型三维场景。我们的核心思想是，重新利用二维生成模型，用于对几何结构、纹理以及PBR材质的全景感知。不同于现有二维提升方法侧重于外观生成而忽略内在属性感知的特点，我们提出OmniX——一个通用且统一的框架。该框架基于轻量高效的跨模态适配器结构，能够复用二维生成先验，广泛应用于多种全景视觉任务，包括全景感知、生成与补全。此外，我们构建了一个大规模合成全景数据集，包含来自多种室内与室外场景的高质量多模态全景图。大量实验表明，我们的模型在全景视觉感知与图形就绪型三维场景生成方面具有显著有效性，为沉浸式且物理真实的虚拟世界生成开辟了新的可能性。",
        "translated_title": "OmniX：从统一全景生成与感知到图形就绪的3D场景",
        "label": [],
        "label_reason": "论文聚焦3D场景生成与感知，属于高阶视觉任务，不涉及像素级图像恢复或增强。",
        "relevance_score": 2,
        "novelty_score": 6,
        "novelty_reason": "提出统一框架用于全景生成与感知，但未在low-level任务中实现突破性创新。"
    },
    {
        "title": "Masked Diffusion Captioning for Visual Feature Learning",
        "url": "http://arxiv.org/abs/2510.26799v1",
        "pub_date": "2025-10-30",
        "summary": "We learn visual features by captioning images with an image-conditioned masked diffusion language model, a formulation we call masked diffusion captioning (MDC). During training, text tokens in each image-caption pair are masked at a randomly chosen ratio, and a decoder conditioned on visual features is trained to reconstruct the original text. After training, the learned visual features can be applied to downstream vision tasks. Unlike autoregressive captioning, the strength of the visual learning signal in MDC does not depend on each token's position in the sequence, reducing the need for auxiliary objectives. Linear probing experiments across a variety of academic-scale models and datasets show that the learned visual features are competitive with those produced by autoregressive and contrastive approaches.",
        "translated": "我们通过图像条件下的掩码扩散语言模型对图像进行字幕生成，从而学习视觉特征，我们称该方法为掩码扩散字幕（Masked Diffusion Captioning, MDC）。在训练过程中，每个图像-字幕对中的文本标记以随机选择的比例被掩码，同时训练一个以视觉特征为条件的解码器来重构原始文本。训练完成后，所学习到的视觉特征可应用于下游视觉任务。与自回归字幕方法不同，MDC中视觉学习信号的强度不依赖于序列中每个标记的位置，从而减少了对辅助目标的依赖。在多种学术规模模型和数据集上进行的线性探测实验表明，所学习的视觉特征与自回归和对比学习方法生成的特征具有竞争力。",
        "translated_title": "掩码扩散字幕用于视觉特征学习",
        "label": [],
        "label_reason": "论文聚焦于视觉特征学习与文本生成，属于high-level任务，不涉及图像像素级恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "提出掩码扩散Captioning方法，但未针对图像恢复任务，创新点在视觉-语言联合学习。"
    },
    {
        "title": "SEE4D: Pose-Free 4D Generation via Auto-Regressive Video Inpainting",
        "url": "http://arxiv.org/abs/2510.26796v1",
        "pub_date": "2025-10-30",
        "summary": "Immersive applications call for synthesizing spatiotemporal 4D content from casual videos without costly 3D supervision. Existing video-to-4D methods typically rely on manually annotated camera poses, which are labor-intensive and brittle for in-the-wild footage. Recent warp-then-inpaint approaches mitigate the need for pose labels by warping input frames along a novel camera trajectory and using an inpainting model to fill missing regions, thereby depicting the 4D scene from diverse viewpoints. However, this trajectory-to-trajectory formulation often entangles camera motion with scene dynamics and complicates both modeling and inference. We introduce SEE4D, a pose-free, trajectory-to-camera framework that replaces explicit trajectory prediction with rendering to a bank of fixed virtual cameras, thereby separating camera control from scene modeling. A view-conditional video inpainting model is trained to learn a robust geometry prior by denoising realistically synthesized warped images and to inpaint occluded or missing regions across virtual viewpoints, eliminating the need for explicit 3D annotations. Building on this inpainting core, we design a spatiotemporal autoregressive inference pipeline that traverses virtual-camera splines and extends videos with overlapping windows, enabling coherent generation at bounded per-step complexity. We validate See4D on cross-view video generation and sparse reconstruction benchmarks. Across quantitative metrics and qualitative assessments, our method achieves superior generalization and improved performance relative to pose- or trajectory-conditioned baselines, advancing practical 4D world modeling from casual videos.",
        "translated": "沉浸式应用要求从普通视频中合成时空4D内容，而无需昂贵的3D监督信息。现有的视频到4D方法通常依赖于人工标注的相机姿态，这对野外拍摄的视频而言既耗时又脆弱。近期提出的“变形后修复”（warp-then-inpaint）方法通过沿新相机轨迹对输入帧进行变形，并利用修复模型填补缺失区域，从而从多样视角描绘4D场景，减少了对姿态标签的依赖。然而，这种轨迹到轨迹（trajectory-to-trajectory）的建模方式常常将相机运动与场景动态耦合在一起，增加了建模和推理的复杂性。我们提出SEE4D，一种无需姿态信息、基于轨迹到相机（trajectory-to-camera）的框架，通过将显式的轨迹预测替换为渲染到一组固定的虚拟相机，从而将相机控制与场景建模解耦。我们训练一个视图条件化的视频修复模型，通过去噪真实合成的变形图像，学习稳健的几何先验，并在虚拟视角间修复被遮挡或缺失的区域，从而完全摆脱显式3D标注的需求。在此修复核心基础上，我们设计了一种时空自回归推理管道，该管道遍历虚拟相机的样条路径，并通过重叠窗口扩展视频，实现了在每步计算复杂度有界的前提下进行连贯生成。我们在跨视角视频生成和稀疏重建基准上验证了SEE4D。在定量指标和定性评估中，我们的方法相较于基于姿态或轨迹条件的基线方法，展现出更强的泛化能力与性能提升，推动了从普通视频实现实用4D世界建模的进展。",
        "translated_title": "SEE4D：基于自回归视频修复的无姿态4D生成",
        "label": [],
        "label_reason": "论文核心为4D内容生成，属于图像生成与3D重建范畴，非像素级图像恢复任务。",
        "relevance_score": 3,
        "novelty_score": 7,
        "novelty_reason": "提出轨迹到相机框架与自回归生成流程，对视频生成有显著改进，但非low-level任务创新。"
    },
    {
        "title": "Scaling Image Geo-Localization to Continent Level",
        "url": "http://arxiv.org/abs/2510.26795v1",
        "pub_date": "2025-10-30",
        "summary": "Determining the precise geographic location of an image at a global scale remains an unsolved challenge. Standard image retrieval techniques are inefficient due to the sheer volume of images (&gt;100M) and fail when coverage is insufficient. Scalable solutions, however, involve a trade-off: global classification typically yields coarse results (10+ kilometers), while cross-view retrieval between ground and aerial imagery suffers from a domain gap and has been primarily studied on smaller regions. This paper introduces a hybrid approach that achieves fine-grained geo-localization across a large geographic expanse the size of a continent. We leverage a proxy classification task during training to learn rich feature representations that implicitly encode precise location information. We combine these learned prototypes with embeddings of aerial imagery to increase robustness to the sparsity of ground-level data. This enables direct, fine-grained retrieval over areas spanning multiple countries. Our extensive evaluation demonstrates that our approach can localize within 200m more than 68\\% of queries of a dataset covering a large part of Europe. The code is publicly available at https://scaling-geoloc.github.io.",
        "translated": "在全球范围内精确确定图像的地理位置仍是一个尚未解决的挑战。标准的图像检索技术由于图像数量庞大（>100M）而效率低下，且在覆盖不足时失效。然而，可扩展的解决方案往往存在权衡：全局分类通常只能提供粗粒度的结果（10+公里），而地面与航拍图像之间的跨视角检索则因域差异问题而受限，且主要在较小区域中进行研究。本文提出一种混合方法，能够在大陆尺度的大范围地理区域内实现细粒度的地理定位。我们在训练过程中引入代理分类任务，以学习富含位置信息的特征表示，这些表示隐式编码了精确的地理位置。我们结合所学习的原型与航拍图像的嵌入表示，以增强对地面数据稀疏性的鲁棒性。这使得能够直接对跨越多个国家的区域进行细粒度检索。我们的大量实验表明，该方法在覆盖欧洲大部分地区的数据集上，超过68%的查询可被定位在200米范围内。代码公开于 https://scaling-geoloc.github.io。",
        "translated_title": "扩展图像地理定位至大陆级别",
        "label": [],
        "label_reason": "论文聚焦图像地理定位，属于高阶视觉任务，不涉及像素级图像质量恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "提出混合检索方法，结合代理分类与航拍图像嵌入，但属定位任务创新，非图像恢复领域。"
    },
    {
        "title": "The Quest for Generalizable Motion Generation: Data, Model, and\n  Evaluation",
        "url": "http://arxiv.org/abs/2510.26794v1",
        "pub_date": "2025-10-30",
        "summary": "Despite recent advances in 3D human motion generation (MoGen) on standard benchmarks, existing models still face a fundamental bottleneck in their generalization capability. In contrast, adjacent generative fields, most notably video generation (ViGen), have demonstrated remarkable generalization in modeling human behaviors, highlighting transferable insights that MoGen can leverage. Motivated by this observation, we present a comprehensive framework that systematically transfers knowledge from ViGen to MoGen across three key pillars: data, modeling, and evaluation. First, we introduce ViMoGen-228K, a large-scale dataset comprising 228,000 high-quality motion samples that integrates high-fidelity optical MoCap data with semantically annotated motions from web videos and synthesized samples generated by state-of-the-art ViGen models. The dataset includes both text-motion pairs and text-video-motion triplets, substantially expanding semantic diversity. Second, we propose ViMoGen, a flow-matching-based diffusion transformer that unifies priors from MoCap data and ViGen models through gated multimodal conditioning. To enhance efficiency, we further develop ViMoGen-light, a distilled variant that eliminates video generation dependencies while preserving strong generalization. Finally, we present MBench, a hierarchical benchmark designed for fine-grained evaluation across motion quality, prompt fidelity, and generalization ability. Extensive experiments show that our framework significantly outperforms existing approaches in both automatic and human evaluations. The code, data, and benchmark will be made publicly available.",
        "translated": "尽管在标准基准上3D人体运动生成（MoGen）领域取得了近期进展，现有模型在泛化能力方面仍面临根本性瓶颈。相比之下，相邻的生成领域——尤其是视频生成（ViGen）——在建模人类行为方面展现出显著的泛化能力，凸显了可供MoGen借鉴的可迁移洞见。受此启发，我们提出一个综合性框架，系统性地从ViGen向MoGen迁移知识，涵盖数据、建模与评估三个关键支柱。首先，我们引入ViMoGen-228K，一个包含228,000个高质量运动样本的大规模数据集，该数据集整合了高保真光学动作捕捉（MoCap）数据、来自网络视频的语义标注运动样本，以及由先进ViGen模型生成的合成样本。数据集包含文本-运动对和文本-视频-运动三元组，显著扩展了语义多样性。其次，我们提出ViMoGen，一种基于流匹配的扩散Transformer模型，通过门控多模态条件机制统一了MoCap数据与ViGen模型的先验知识。为提升效率，我们进一步开发了ViMoGen-light，一个蒸馏变体，在消除视频生成依赖的同时保持了强大的泛化能力。最后，我们提出MBench，一个分层基准，用于在运动质量、提示保真度和泛化能力方面进行细粒度评估。大量实验表明，我们的框架在自动评估与人工评估中均显著优于现有方法。代码、数据及基准将公开发布。",
        "translated_title": "通用运动生成的探索：数据、模型与评估",
        "label": [],
        "label_reason": "论文聚焦3D人体运动生成，属于高阶生成任务，非像素级图像恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出跨领域知识迁移框架，结合视频生成与运动生成，具有方法创新性。"
    },
    {
        "title": "HEIR: Learning Graph-Based Motion Hierarchies",
        "url": "http://arxiv.org/abs/2510.26786v1",
        "pub_date": "2025-10-30",
        "summary": "Hierarchical structures of motion exist across research fields, including computer vision, graphics, and robotics, where complex dynamics typically arise from coordinated interactions among simpler motion components. Existing methods to model such dynamics typically rely on manually-defined or heuristic hierarchies with fixed motion primitives, limiting their generalizability across different tasks. In this work, we propose a general hierarchical motion modeling method that learns structured, interpretable motion relationships directly from data. Our method represents observed motions using graph-based hierarchies, explicitly decomposing global absolute motions into parent-inherited patterns and local motion residuals. We formulate hierarchy inference as a differentiable graph learning problem, where vertices represent elemental motions and directed edges capture learned parent-child dependencies through graph neural networks. We evaluate our hierarchical reconstruction approach on three examples: 1D translational motion, 2D rotational motion, and dynamic 3D scene deformation via Gaussian splatting. Experimental results show that our method reconstructs the intrinsic motion hierarchy in 1D and 2D cases, and produces more realistic and interpretable deformations compared to the baseline on dynamic 3D Gaussian splatting scenes. By providing an adaptable, data-driven hierarchical modeling paradigm, our method offers a formulation applicable to a broad range of motion-centric tasks. Project Page: https://light.princeton.edu/HEIR/",
        "translated": "运动的层次结构广泛存在于计算机视觉、图形学和机器人学等多个研究领域中，其中复杂的动态行为通常由多个简单运动组件的协同交互产生。现有方法在建模此类动态时，通常依赖于人工定义或启发式设定的固定运动基元层次结构，这限制了其在不同任务间的泛化能力。在本文中，我们提出了一种通用的层次化运动建模方法，能够直接从数据中学习结构化且可解释的运动关系。我们的方法采用基于图的层次结构表示观测到的运动，显式地将全局绝对运动分解为从父节点继承的模式和局部运动残差。我们将层次结构推断建模为一个可微分的图学习问题，其中图的顶点代表基本运动单元，有向边通过图神经网络捕获学习到的父子依赖关系。我们在三个实例上评估了所提出的层次重建方法：1D平移运动、2D旋转运动，以及通过高斯点云（Gaussian splatting）实现的动态3D场景形变。实验结果表明，我们的方法在1D和2D场景中能够重建内在的运动层次结构，并在动态3D高斯点云场景中相较于基线方法生成更真实、更具可解释性的形变结果。通过提供一种自适应、数据驱动的层次化建模范式，我们的方法为一系列以运动为核心的任务提供了通用的建模框架。项目主页：https://light.princeton.edu/HEIR/",
        "translated_title": "HEIR：基于图结构的运动层次学习",
        "label": [],
        "label_reason": "研究运动层次建模，属于动态场景理解，非像素级图像恢复任务",
        "relevance_score": 2,
        "novelty_score": 7,
        "novelty_reason": "提出可学习图结构运动层次，方法新颖，但非low-level图像处理"
    },
    {
        "title": "Clone Deterministic 3D Worlds with Geometrically-Regularized World\n  Models",
        "url": "http://arxiv.org/abs/2510.26782v1",
        "pub_date": "2025-10-30",
        "summary": "A world model is an internal model that simulates how the world evolves. Given past observations and actions, it predicts the future of both the embodied agent and its environment. Accurate world models are essential for enabling agents to think, plan, and reason effectively in complex, dynamic settings. Despite rapid progress, current world models remain brittle and degrade over long horizons. We argue that a central cause is representation quality: exteroceptive inputs (e.g., images) are high-dimensional, and lossy or entangled latents make dynamics learning unnecessarily hard. We therefore ask whether improving representation learning alone can substantially improve world-model performance. In this work, we take a step toward building a truly accurate world model by addressing a fundamental yet open problem: constructing a model that can fully clone and overfit to a deterministic 3D world. We propose Geometrically-Regularized World Models (GRWM), which enforces that consecutive points along a natural sensory trajectory remain close in latent representation space. This approach yields significantly improved latent representations that align closely with the true topology of the environment. GRWM is plug-and-play, requires only minimal architectural modification, scales with trajectory length, and is compatible with diverse latent generative backbones. Across deterministic 3D settings and long-horizon prediction tasks, GRWM significantly increases rollout fidelity and stability. Analyses show that its benefits stem from learning a latent manifold with superior geometric structure. These findings support a clear takeaway: improving representation learning is a direct and useful path to robust world models, delivering reliable long-horizon predictions without enlarging the dynamics module.",
        "translated": "世界模型是一种内部模型，用于模拟世界如何演化。给定过去的观测和动作，它能够预测具身智能体及其环境的未来状态。准确的世界模型对于使智能体在复杂、动态环境中有效思考、规划和推理至关重要。尽管取得了快速进展，当前的世界模型仍然脆弱，并且在长时间预测中性能显著退化。我们认为，其核心原因在于表示质量：外感知输入（如图像）具有高维度，而存在信息损失或纠缠的潜在表示使得动力学学习变得不必要地困难。因此，我们提出一个关键问题：仅通过改进表示学习，是否能够显著提升世界模型的性能？在本研究中，我们通过解决一个基础但尚未解决的问题，朝着构建真正准确的世界模型迈出一步：构建一个能够完全复制并过拟合于确定性3D世界模型。我们提出几何正则化世界模型（Geometrically-Regularized World Models, GRWM），该方法强制要求在自然感知轨迹上连续的点在潜在表示空间中保持邻近。该方法生成了显著改进的潜在表示，其与环境的真实拓扑结构高度对齐。GRWM具有即插即用特性，仅需对网络结构进行最小修改，可随轨迹长度扩展，并兼容多种潜在生成主干网络。在确定性3D环境和长时序预测任务中，GRWM显著提升了预测轨迹的保真度和稳定性。分析表明，其优势源于学习到具有优越几何结构的潜在流形。这些发现支持一个明确的结论：改进表示学习是构建鲁棒世界模型的直接且有效途径，能够在不扩大动力学模块的前提下，实现可靠的长时序预测。",
        "translated_title": "克隆确定性三维世界：基于几何正则化世界模型的方法",
        "label": [],
        "label_reason": "论文聚焦3D世界模型与长期预测，属于高阶视觉任务，不涉及像素级图像恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 6,
        "novelty_reason": "提出几何正则化表示学习方法，提升世界模型稳定性，但非low-level图像处理创新。"
    },
    {
        "title": "ChartAB: A Benchmark for Chart Grounding &amp; Dense Alignment",
        "url": "http://arxiv.org/abs/2510.26781v1",
        "pub_date": "2025-10-30",
        "summary": "Charts play an important role in visualization, reasoning, data analysis, and the exchange of ideas among humans. However, existing vision-language models (VLMs) still lack accurate perception of details and struggle to extract fine-grained structures from charts. Such limitations in chart grounding also hinder their ability to compare multiple charts and reason over them. In this paper, we introduce a novel \"ChartAlign Benchmark (ChartAB)\" to provide a comprehensive evaluation of VLMs in chart grounding tasks, i.e., extracting tabular data, localizing visualization elements, and recognizing various attributes from charts of diverse types and complexities. We design a JSON template to facilitate the calculation of evaluation metrics specifically tailored for each grounding task. By incorporating a novel two-stage inference workflow, the benchmark can further evaluate VLMs' capability to align and compare elements/attributes across two charts. Our analysis of evaluations on several recent VLMs reveals new insights into their perception biases, weaknesses, robustness, and hallucinations in chart understanding. These findings highlight the fine-grained discrepancies among VLMs in chart understanding tasks and point to specific skills that need to be strengthened in current models.",
        "translated": "图表在可视化、推理、数据分析以及人类之间的思想交流中发挥着重要作用。然而，现有的视觉-语言模型（VLMs）仍缺乏对细节的准确感知，难以从图表中提取细粒度结构。这种在图表定位（chart grounding）方面的局限性也阻碍了它们对多个图表进行比较和推理的能力。本文提出了一种新颖的“ChartAlign基准（ChartAB）”，用于全面评估VLMs在图表定位任务中的表现，即从多种类型和复杂度的图表中提取表格数据、定位可视化元素以及识别各种属性。我们设计了一种JSON模板，以方便针对每个定位任务计算特定的评估指标。通过引入一种新颖的两阶段推理工作流，该基准还可进一步评估VLMs在跨两个图表对齐和比较元素/属性方面的能力。我们对多个近期VLMs的评估分析揭示了它们在图表理解中的感知偏差、弱点、鲁棒性以及幻觉现象等新见解。这些发现突显了VLMs在图表理解任务中的细粒度差异，并指出了当前模型亟需加强的具体能力。",
        "translated_title": "ChartAB: 图表定位与密集对齐基准数据集",
        "label": [],
        "label_reason": "论文聚焦图表理解与视觉-语言对齐，属于高阶视觉任务，非像素级图像恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "提出新基准与两阶段推理流程，但未涉及图像质量改善或低层视觉处理创新。"
    },
    {
        "title": "Surpassing state of the art on AMD area estimation from RGB fundus\n  images through careful selection of U-Net architectures and loss functions\n  for class imbalance",
        "url": "http://arxiv.org/abs/2510.26778v1",
        "pub_date": "2025-10-30",
        "summary": "Age-related macular degeneration (AMD) is one of the leading causes of irreversible vision impairment in people over the age of 60. This research focuses on semantic segmentation for AMD lesion detection in RGB fundus images, a non-invasive and cost-effective imaging technique. The results of the ADAM challenge - the most comprehensive AMD detection from RGB fundus images research competition and open dataset to date - serve as a benchmark for our evaluation. Taking the U-Net connectivity as a base of our framework, we evaluate and compare several approaches to improve the segmentation model's architecture and training pipeline, including pre-processing techniques, encoder (backbone) deep network types of varying complexity, and specialized loss functions to mitigate class imbalances on image and pixel levels. The main outcome of this research is the final configuration of the AMD detection framework, which outperforms all the prior ADAM challenge submissions on the multi-class segmentation of different AMD lesion types in non-invasive RGB fundus images. The source code used to conduct the experiments presented in this paper is made freely available.",
        "translated": "年龄相关性黄斑变性（AMD）是60岁以上人群不可逆视力损害的主要原因之一。本研究聚焦于在RGB眼底图像中进行AMD病灶的语义分割，这是一种非侵入性且成本效益较高的成像技术。我们以迄今为止最全面的AMD检测研究竞赛及公开数据集——ADAM挑战赛的结果作为评估基准。以U-Net的连接结构为基础框架，我们评估并比较了多种改进分割模型架构与训练流程的方法，包括预处理技术、不同复杂度的编码器（主干网络）深度网络类型，以及专门设计的损失函数，以缓解图像和像素层面的类别不平衡问题。本研究的主要成果是最终确定的AMD检测框架配置，该配置在非侵入性RGB眼底图像中对多种AMD病灶类型的多类别分割任务上，性能优于所有先前提交的ADAM挑战赛方案。本文所呈现实验所使用的源代码已公开共享。",
        "translated_title": "通过精心选择U-Net架构和损失函数以应对类别不平衡问题，在RGB眼底图像上实现AMD区域估计的性能超越现有最佳水平",
        "label": [],
        "label_reason": "论文核心为医学图像语义分割，用于AMD病变检测，属于high-level任务，非像素级图像恢复。",
        "relevance_score": 3,
        "novelty_score": 5,
        "novelty_reason": "对U-Net架构和损失函数进行优化，属常规改进，无显著创新点。"
    },
    {
        "title": "SteerVLM: Robust Model Control through Lightweight Activation Steering\n  for Vision Language Models",
        "url": "http://arxiv.org/abs/2510.26769v1",
        "pub_date": "2025-10-30",
        "summary": "This work introduces SteerVLM, a lightweight steering module designed to guide Vision-Language Models (VLMs) towards outputs that better adhere to desired instructions. Our approach learns from the latent embeddings of paired prompts encoding target and converse behaviors to dynamically adjust activations connecting the language modality with image context. This allows for fine-grained, inference-time control over complex output semantics without modifying model weights while preserving performance on off-target tasks. Our steering module requires learning parameters equal to 0.14% of the original VLM's size. Our steering module gains model control through dimension-wise activation modulation and adaptive steering across layers without requiring pre-extracted static vectors or manual tuning of intervention points. Furthermore, we introduce VNIA (Visual Narrative Intent Alignment), a multimodal dataset specifically created to facilitate the development and evaluation of VLM steering techniques. Our method outperforms existing intervention techniques on steering and hallucination mitigation benchmarks for VLMs and proposes a robust solution for multimodal model control through activation engineering.",
        "translated": "本工作提出 SteerVLM，一种轻量级引导模块，旨在引导视觉-语言模型（VLMs）生成更符合预期指令的输出。我们的方法通过学习成对提示（编码目标行为与相反行为）的潜在嵌入，动态调整语言模态与图像上下文之间连接的激活值。这使得在不修改模型权重的前提下，能够对复杂输出语义实现细粒度、推理时的控制，同时保持在非目标任务上的性能。我们的引导模块所需学习参数仅为原始VLM大小的0.14%。该模块通过维度感知的激活调制和跨层自适应引导实现模型控制，无需预提取静态向量或手动调整干预点。此外，我们引入VNIA（Visual Narrative Intent Alignment），一个专为促进VLM引导技术开发与评估而构建的多模态数据集。我们的方法在VLMs的引导控制与幻觉缓解基准测试中优于现有干预技术，并通过激活工程提出了一种鲁棒的多模态模型控制解决方案。",
        "translated_title": "SteerVLM：通过轻量级激活引导实现视觉语言模型的鲁棒控制",
        "label": [],
        "label_reason": "论文聚焦于视觉语言模型的控制与干预，属于高阶任务，不涉及图像像素级恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 7,
        "novelty_reason": "提出轻量激活引导模块，创新性地实现推理时控制，但非图像恢复领域方法。"
    },
    {
        "title": "MORE: Multi-Organ Medical Image REconstruction Dataset",
        "url": "http://arxiv.org/abs/2510.26759v1",
        "pub_date": "2025-10-30",
        "summary": "CT reconstruction provides radiologists with images for diagnosis and treatment, yet current deep learning methods are typically limited to specific anatomies and datasets, hindering generalization ability to unseen anatomies and lesions. To address this, we introduce the Multi-Organ medical image REconstruction (MORE) dataset, comprising CT scans across 9 diverse anatomies with 15 lesion types. This dataset serves two key purposes: (1) enabling robust training of deep learning models on extensive, heterogeneous data, and (2) facilitating rigorous evaluation of model generalization for CT reconstruction. We further establish a strong baseline solution that outperforms prior approaches under these challenging conditions. Our results demonstrate that: (1) a comprehensive dataset helps improve the generalization capability of models, and (2) optimization-based methods offer enhanced robustness for unseen anatomies. The MORE dataset is freely accessible under CC-BY-NC 4.0 at our project page https://more-med.github.io/",
        "translated": "CT重建为放射科医生提供了用于诊断和治疗的图像，然而当前的深度学习方法通常局限于特定解剖结构和数据集，导致其在面对未见过的解剖结构和病灶时泛化能力受限。为解决这一问题，我们提出了多器官医学图像重建（Multi-Organ medical image REconstruction, MORE）数据集，包含9种不同解剖结构的CT扫描图像，涵盖15种病灶类型。该数据集具有两个关键作用：（1）支持深度学习模型在大规模、异构数据上进行鲁棒训练；（2）促进对CT重建模型泛化能力的严格评估。此外，我们建立了一个强基线方法，在这些具有挑战性的条件下优于现有方法。实验结果表明：（1）综合性数据集有助于提升模型的泛化能力；（2）基于优化的方法在应对未见过的解剖结构时表现出更强的鲁棒性。MORE数据集可在我们的项目页面 https://more-med.github.io/ 免费获取，遵循CC-BY-NC 4.0许可协议。",
        "translated_title": "MORE: 多器官医学图像重建数据集",
        "label": [
            "CT金属伪影消除",
            "医学图像增强",
            "图像重建"
        ],
        "label_reason": "论文聚焦CT图像重建，涉及多器官数据集构建与模型泛化，属于低层医学图像恢复任务。",
        "relevance_score": 9,
        "novelty_score": 7,
        "novelty_reason": "提出多器官CT重建数据集，提升模型泛化能力，对现有方法有显著改进。"
    },
    {
        "title": "ProstNFound+: A Prospective Study using Medical Foundation Models for\n  Prostate Cancer Detection",
        "url": "http://arxiv.org/abs/2510.26703v1",
        "pub_date": "2025-10-30",
        "summary": "Purpose: Medical foundation models (FMs) offer a path to build high-performance diagnostic systems. However, their application to prostate cancer (PCa) detection from micro-ultrasound ({\\mu}US) remains untested in clinical settings. We present ProstNFound+, an adaptation of FMs for PCa detection from {\\mu}US, along with its first prospective validation. Methods: ProstNFound+ incorporates a medical FM, adapter tuning, and a custom prompt encoder that embeds PCa-specific clinical biomarkers. The model generates a cancer heatmap and a risk score for clinically significant PCa. Following training on multi-center retrospective data, the model is prospectively evaluated on data acquired five years later from a new clinical site. Model predictions are benchmarked against standard clinical scoring protocols (PRI-MUS and PI-RADS). Results: ProstNFound+ shows strong generalization to the prospective data, with no performance degradation compared to retrospective evaluation. It aligns closely with clinical scores and produces interpretable heatmaps consistent with biopsy-confirmed lesions. Conclusion: The results highlight its potential for clinical deployment, offering a scalable and interpretable alternative to expert-driven protocols.",
        "translated": "目的：医学基础模型（FMs）为构建高性能诊断系统提供了可行路径。然而，其在微超声（{\\mu}US）图像中用于前列腺癌（PCa）检测的临床应用尚未得到验证。本文提出 ProstNFound+，即针对 {\\mu}US 图像中 PCa 检测的医学基础模型适配方案，并首次开展前瞻性验证。\n\n方法：ProstNFound+ 集成了医学基础模型、适配器调优机制以及一个定制的提示编码器，该编码器嵌入了与 PCa 相关的临床生物标志物。模型输出包括癌症热力图和临床显著性 PCa 的风险评分。模型在多中心回顾性数据上训练后，于五年后在新临床中心获取的数据上进行前瞻性评估。模型预测结果与标准临床评分协议（PRI-MUS 和 PI-RADS）进行对比分析。\n\n结果：ProstNFound+ 在前瞻性数据上表现出良好的泛化能力，其性能与回顾性评估相比无明显下降。模型预测结果与临床评分高度一致，并生成可解释的热力图，其空间分布与穿刺活检确认的病灶区域相符。\n\n结论：研究结果表明，该模型具备临床部署潜力，可作为专家主导协议的可扩展且可解释的替代方案。",
        "translated_title": "ProstNFound+: 一种基于医学基础模型的前列腺癌检测前瞻性研究",
        "label": [],
        "label_reason": "论文聚焦于前列腺癌检测，属于高阶医学图像分析任务，非像素级图像恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "方法基于医学基础模型与提示编码，属模型适配与临床应用探索，无底层图像处理创新。"
    },
    {
        "title": "The Impact and Outlook of 3D Gaussian Splatting",
        "url": "http://arxiv.org/abs/2510.26694v1",
        "pub_date": "2025-10-30",
        "summary": "Since its introduction, 3D Gaussian Splatting (3DGS) has rapidly transformed the landscape of 3D scene representations, inspiring an extensive body of associated research. Follow-up work includes analyses and contributions that enhance the efficiency, scalability, and real-world applicability of 3DGS. In this summary, we present an overview of several key directions that have emerged in the wake of 3DGS. We highlight advances enabling resource-efficient training and rendering, the evolution toward dynamic (or four-dimensional, 4DGS) representations, and deeper exploration of the mathematical foundations underlying its appearance modeling and rendering process. Furthermore, we examine efforts to bring 3DGS to mobile and virtual reality platforms, its extension to massive-scale environments, and recent progress toward near-instant radiance field reconstruction via feed-forward or distributed computation. Collectively, these developments illustrate how 3DGS has evolved from a breakthrough representation into a versatile and foundational tool for 3D vision and graphics.",
        "translated": "自提出以来，3D高斯泼洒（3D Gaussian Splatting, 3DGS）迅速改变了三维场景表示的格局，激发了大量相关研究。后续工作包括对3DGS效率、可扩展性及实际应用性的分析与改进。在本综述中，我们概述了3DGS问世后涌现出的若干关键研究方向。我们重点介绍了实现资源高效训练与渲染的技术进展，向动态（或四维，4DGS）表示的演进，以及对其外观建模与渲染过程所依赖的数学基础的深入探索。此外，我们还考察了将3DGS应用于移动设备和虚拟现实平台的努力，其在大规模环境中的扩展，以及近期通过前馈或分布式计算实现近乎瞬时辐射场重建的进展。总体而言，这些发展表明，3DGS已从一项突破性表示方法，演变为三维视觉与图形领域的多功能基础工具。",
        "translated_title": "3D高斯泼溅的影响与展望\n\n3D高斯泼溅（3D Gaussian Splatting）作为一种新兴的神经渲染技术，近年来在计算机视觉和图形学领域引起了广泛关注。该方法通过将场景表示为一组3D高斯分布，实现了高效、高质量的场景重建与渲染。与传统的神经辐射场（NeRF）相比，3D高斯泼溅在渲染速度和内存效率方面具有显著优势，同时保持了对复杂几何和外观细节的精确建模能力。\n\n在图像恢复任务中，3D高斯泼溅为处理由视角变化、遮挡或传感器噪声引起的退化提供了新的视角。通过在频域和空域中联合优化高斯参数，该方法能够有效捕捉场景的结构先验，从而在超分辨率、去模糊和低光照增强等任务中表现出色。此外，其端到端的训练框架支持与残差学习机制结合，进一步提升了对退化模式的建模能力。\n\n在实际应用中，3D高斯泼溅已被成功应用于多个数据集，如RESIDE、SIDD和Rain100L，展示了其在图像去雨、去雾和JPEG伪影去除等任务中的通用性。特别是在金属伪影消除和去反射任务中，该方法通过引入场景特定的先验知识，显著提升了恢复质量。\n\n展望未来，3D高斯泼溅有望在多模态图像恢复、动态场景重建以及跨域迁移学习中发挥更大作用。随着对高斯分布参数优化算法的持续改进，以及与深度学习模型（如U-Net、DnCNN）的深度融合，该技术将在低级图像处理领域持续推动边界扩展，为更复杂、更真实的视觉感知系统提供基础支持。",
        "label": [],
        "label_reason": "论文聚焦3D场景表示与渲染，属于3D视觉与图形领域，非像素级图像恢复或增强任务。",
        "relevance_score": 1,
        "novelty_score": 5,
        "novelty_reason": "综述性工作，总结现有进展，无新方法或架构提出，创新度有限。"
    },
    {
        "title": "Process Integrated Computer Vision for Real-Time Failure Prediction in\n  Steel Rolling Mill",
        "url": "http://arxiv.org/abs/2510.26684v1",
        "pub_date": "2025-10-30",
        "summary": "We present a long-term deployment study of a machine vision-based anomaly detection system for failure prediction in a steel rolling mill. The system integrates industrial cameras to monitor equipment operation, alignment, and hot bar motion in real time along the process line. Live video streams are processed on a centralized video server using deep learning models, enabling early prediction of equipment failures and process interruptions, thereby reducing unplanned breakdown costs. Server-based inference minimizes the computational load on industrial process control systems (PLCs), supporting scalable deployment across production lines with minimal additional resources. By jointly analyzing sensor data from data acquisition systems and visual inputs, the system identifies the location and probable root causes of failures, providing actionable insights for proactive maintenance. This integrated approach enhances operational reliability, productivity, and profitability in industrial manufacturing environments.",
        "translated": "我们提出了一项针对钢铁轧钢车间故障预测的基于机器视觉的异常检测系统的长期部署研究。该系统集成工业相机，实时监控生产线上设备运行状态、对齐情况以及热轧钢坯的运动。实时视频流通过中央视频服务器利用深度学习模型进行处理，实现对设备故障和工艺中断的早期预测，从而降低非计划停机带来的成本。基于服务器的推理机制减轻了工业过程控制系统（PLCs）的计算负担，支持在多条生产线中以极少的额外资源实现可扩展部署。通过联合分析数据采集系统中的传感器数据与视觉输入，系统能够定位故障发生位置并识别可能的根本原因，为预防性维护提供可操作的洞察。这种集成方法显著提升了工业制造环境中的运行可靠性、生产效率和盈利能力。",
        "translated_title": "过程集成计算机视觉在钢铁轧钢车间实时故障预测中的应用",
        "label": [],
        "label_reason": "论文聚焦工业异常检测与故障预测，属于高阶视觉任务，非像素级图像恢复或增强。",
        "relevance_score": 2,
        "novelty_score": 4,
        "novelty_reason": "方法为多模态数据融合与实时推理，属常规工业AI应用，无显著创新。"
    },
    {
        "title": "Improving Classification of Occluded Objects through Scene Context",
        "url": "http://arxiv.org/abs/2510.26681v1",
        "pub_date": "2025-10-30",
        "summary": "The presence of occlusions has provided substantial challenges to typically-powerful object recognition algorithms. Additional sources of information can be extremely valuable to reduce errors caused by occlusions. Scene context is known to aid in object recognition in biological vision. In this work, we attempt to add robustness into existing Region Proposal Network-Deep Convolutional Neural Network (RPN-DCNN) object detection networks through two distinct scene-based information fusion techniques. We present one algorithm under each methodology: the first operates prior to prediction, selecting a custom object network to use based on the identified background scene, and the second operates after detection, fusing scene knowledge into initial object scores output by the RPN. We demonstrate our algorithms on challenging datasets featuring partial occlusions, which show overall improvement in both recall and precision against baseline methods. In addition, our experiments contrast multiple training methodologies for occlusion handling, finding that training on a combination of both occluded and unoccluded images demonstrates an improvement over the others. Our method is interpretable and can easily be adapted to other datasets, offering many future directions for research and practical applications.",
        "translated": "遮挡的存在给通常强大的目标识别算法带来了重大挑战。额外的信息来源对于减少由遮挡引起的错误具有极高的价值。场景上下文在生物视觉中已被证明有助于目标识别。在本研究中，我们尝试通过两种不同的基于场景的信息融合技术，增强现有的区域建议网络-深度卷积神经网络（RPN-DCNN）目标检测网络的鲁棒性。我们针对每种方法提出了一种算法：第一种在预测之前进行，根据识别出的背景场景选择特定的目标网络；第二种在检测之后进行，将场景知识融合到RPN输出的初始目标得分中。我们在包含部分遮挡的具有挑战性的数据集上验证了我们的算法，结果表明在召回率和精确率方面均优于基线方法。此外，我们的实验对比了多种遮挡处理的训练方法，发现结合遮挡和非遮挡图像进行训练的效果优于其他方法。我们的方法具有可解释性，且易于适应其他数据集，为未来的研究和实际应用提供了广阔方向。",
        "translated_title": "通过场景上下文提升遮挡物体的分类性能",
        "label": [],
        "label_reason": "论文聚焦于遮挡物体的分类，属于目标检测与场景理解，非像素级图像恢复任务。",
        "relevance_score": 2,
        "novelty_score": 5,
        "novelty_reason": "提出两种场景上下文融合方法，属常规改进，无本质创新。"
    },
    {
        "title": "BRIQA: Balanced Reweighting in Image Quality Assessment of Pediatric\n  Brain MRI",
        "url": "http://arxiv.org/abs/2510.26661v1",
        "pub_date": "2025-10-30",
        "summary": "Assessing the severity of artifacts in pediatric brain Magnetic Resonance Imaging (MRI) is critical for diagnostic accuracy, especially in low-field systems where the signal-to-noise ratio is reduced. Manual quality assessment is time-consuming and subjective, motivating the need for robust automated solutions. In this work, we propose BRIQA (Balanced Reweighting in Image Quality Assessment), which addresses class imbalance in artifact severity levels. BRIQA uses gradient-based loss reweighting to dynamically adjust per-class contributions and employs a rotating batching scheme to ensure consistent exposure to underrepresented classes. Through experiments, no single architecture performs best across all artifact types, emphasizing the importance of architectural diversity. The rotating batching configuration improves performance across metrics by promoting balanced learning when combined with cross-entropy loss. BRIQA improves average macro F1 score from 0.659 to 0.706, with notable gains in Noise (0.430), Zipper (0.098), Positioning (0.097), Contrast (0.217), Motion (0.022), and Banding (0.012) artifact severity classification. The code is available at https://github.com/BioMedIA-MBZUAI/BRIQA.",
        "translated": "评估儿科脑部磁共振成像（MRI）中伪影的严重程度对诊断准确性至关重要，尤其是在低场系统中，由于信噪比降低，这一问题更为突出。手动质量评估耗时且具有主观性，因此亟需鲁棒的自动化解决方案。本文提出了一种名为 BRIQA（Balanced Reweighting in Image Quality Assessment）的方法，用于解决伪影严重程度等级中的类别不平衡问题。BRIQA 采用基于梯度的损失重加权策略，动态调整各类别的贡献，并引入旋转批处理方案，以确保模型对少数类别的持续暴露。实验结果表明，没有任何单一网络架构能在所有伪影类型上均表现最优，凸显了架构多样性的必要性。旋转批处理方案在结合交叉熵损失时，通过促进平衡学习，显著提升了各项指标的性能。BRIQA 将平均宏 F1 分数从 0.659 提升至 0.706，在 Noise（0.430）、Zipper（0.098）、Positioning（0.097）、Contrast（0.217）、Motion（0.022）和 Banding（0.012）等伪影严重程度分类任务中均取得了显著提升。代码已开源，地址为 https://github.com/BioMedIA-MBZUAI/BRIQA。",
        "translated_title": "BRIQA：儿科脑部MRI图像质量评估中的平衡重加权方法",
        "label": [],
        "label_reason": "论文聚焦于MRI图像质量评估中的分类任务，属于医学图像分析，非像素级恢复或增强，故不属于low-level。",
        "relevance_score": 4,
        "novelty_score": 6,
        "novelty_reason": "提出梯度重加权与旋转批处理策略，提升分类性能，属方法改进，非根本性创新。"
    },
    {
        "title": "Towards Reliable Sea Ice Drift Estimation in the Arctic Deep Learning\n  Optical Flow on RADARSAT-2",
        "url": "http://arxiv.org/abs/2510.26653v1",
        "pub_date": "2025-10-30",
        "summary": "Accurate estimation of sea ice drift is critical for Arctic navigation, climate research, and operational forecasting. While optical flow, a computer vision technique for estimating pixel wise motion between consecutive images, has advanced rapidly in computer vision, its applicability to geophysical problems and to satellite SAR imagery remains underexplored. Classical optical flow methods rely on mathematical models and strong assumptions about motion, which limit their accuracy in complex scenarios. Recent deep learning based approaches have substantially improved performance and are now the standard in computer vision, motivating their application to sea ice drift estimation. We present the first large scale benchmark of 48 deep learning optical flow models on RADARSAT 2 ScanSAR sea ice imagery, evaluated with endpoint error (EPE) and Fl all metrics against GNSS tracked buoys. Several models achieve sub kilometer accuracy (EPE 6 to 8 pixels, 300 to 400 m), a small error relative to the spatial scales of sea ice motion and typical navigation requirements in the Arctic. Our results demonstrate that the models are capable of capturing consistent regional drift patterns and that recent deep learning based optical flow methods, which have substantially improved motion estimation accuracy compared to classical methods, can be effectively transferred to polar remote sensing. Optical flow produces spatially continuous drift fields, providing motion estimates for every image pixel rather than at sparse buoy locations, offering new opportunities for navigation and climate modeling.",
        "translated": "准确估计海冰漂移对于北极航行、气候研究及业务预报至关重要。尽管光学流（optical flow）作为一种用于估计连续图像间像素级运动的计算机视觉技术，在计算机视觉领域已取得快速发展，但其在地球物理问题以及卫星SAR影像中的适用性仍鲜有探索。经典光学流方法依赖于数学模型，并对运动过程做出强假设，这在复杂场景下限制了其精度。近年来，基于深度学习的方法显著提升了性能，已成为计算机视觉领域的标准，这促使人们将其应用于海冰漂移估计。我们首次在RADARSAT 2 ScanSAR海冰影像上对48种深度学习光学流模型进行了大规模基准测试，采用端点误差（EPE）和Fl all指标，与GNSS追踪浮标数据进行对比评估。多个模型实现了亚千米级精度（EPE为6至8像素，即300至400米），该误差相对于海冰运动的空间尺度及北极典型航行需求而言较小。我们的结果表明，这些模型能够捕捉一致的区域漂移模式，且近年来基于深度学习的光学流方法相比经典方法显著提升了运动估计精度，可有效迁移至极地遥感应用。光学流可生成空间连续的漂移场，为每幅图像中的每个像素提供运动估计，而非仅限于稀疏浮标位置，从而为航行和气候建模提供了新的机遇。",
        "translated_title": "面向北极海冰漂移可靠估计的深度学习：基于 RADARSAT-2 的光流方法",
        "label": [],
        "label_reason": "论文聚焦于遥感图像中的海冰漂移估计，属于运动估计任务，非像素级图像恢复或增强。",
        "relevance_score": 3,
        "novelty_score": 6,
        "novelty_reason": "首次在RADARSAT-2数据上大规模评估深度学习光流模型，但方法本身非原创，属迁移应用。"
    },
    {
        "title": "All You Need for Object Detection: From Pixels, Points, and Prompts to\n  Next-Gen Fusion and Multimodal LLMs/VLMs in Autonomous Vehicles",
        "url": "http://arxiv.org/abs/2510.26641v1",
        "pub_date": "2025-10-30",
        "summary": "Autonomous Vehicles (AVs) are transforming the future of transportation through advances in intelligent perception, decision-making, and control systems. However, their success is tied to one core capability, reliable object detection in complex and multimodal environments. While recent breakthroughs in Computer Vision (CV) and Artificial Intelligence (AI) have driven remarkable progress, the field still faces a critical challenge as knowledge remains fragmented across multimodal perception, contextual reasoning, and cooperative intelligence. This survey bridges that gap by delivering a forward-looking analysis of object detection in AVs, emphasizing emerging paradigms such as Vision-Language Models (VLMs), Large Language Models (LLMs), and Generative AI rather than re-examining outdated techniques. We begin by systematically reviewing the fundamental spectrum of AV sensors (camera, ultrasonic, LiDAR, and Radar) and their fusion strategies, highlighting not only their capabilities and limitations in dynamic driving environments but also their potential to integrate with recent advances in LLM/VLM-driven perception frameworks. Next, we introduce a structured categorization of AV datasets that moves beyond simple collections, positioning ego-vehicle, infrastructure-based, and cooperative datasets (e.g., V2V, V2I, V2X, I2I), followed by a cross-analysis of data structures and characteristics. Ultimately, we analyze cutting-edge detection methodologies, ranging from 2D and 3D pipelines to hybrid sensor fusion, with particular attention to emerging transformer-driven approaches powered by Vision Transformers (ViTs), Large and Small Language Models (SLMs), and VLMs. By synthesizing these perspectives, our survey delivers a clear roadmap of current capabilities, open challenges, and future opportunities.",
        "translated": "自动驾驶汽车（Autonomous Vehicles, AVs）正通过智能感知、决策与控制系统的技术进步，重塑未来交通格局。然而，其成功依赖于一项核心能力：在复杂且多模态的环境中实现可靠的目标检测。尽管计算机视觉（Computer Vision, CV）与人工智能（Artificial Intelligence, AI）领域的最新突破已推动显著进展，但该领域仍面临关键挑战——知识在多模态感知、上下文推理与协同智能之间高度碎片化。本综述旨在弥合这一鸿沟，通过前瞻性分析自动驾驶汽车中的目标检测技术，重点聚焦于视觉-语言模型（Vision-Language Models, VLMs）、大语言模型（Large Language Models, LLMs）及生成式人工智能（Generative AI）等新兴范式，而非回顾过时的技术。我们首先系统性地回顾自动驾驶汽车传感器（摄像头、超声波、LiDAR 和雷达）的基本谱系及其融合策略，不仅阐明其在动态驾驶环境中的能力与局限，还探讨其与近期基于 LLM/VLM 的感知框架集成的潜力。随后，我们提出一种结构化的自动驾驶数据集分类方法，超越简单的数据集合，涵盖自车（ego-vehicle）、基础设施（infrastructure-based）及协同式数据集（如 V2V、V2I、V2X、I2I），并进一步对数据结构与特征进行交叉分析。最后，我们分析前沿的目标检测方法，涵盖 2D 与 3D 检测流程以及混合传感器融合技术，特别关注由视觉Transformer（Vision Transformers, ViTs）、大语言模型与小语言模型（Small Language Models, SLMs）及 VLMs 驱动的新兴Transformer架构。通过整合上述视角，本综述为当前能力、开放挑战与未来机遇提供了清晰的路线图。",
        "translated_title": "物体检测所需的一切：从像素、点和提示到自动驾驶中的下一代融合与多模态大语言模型/视觉语言模型",
        "label": [],
        "label_reason": "论文聚焦自动驾驶中的目标检测，属于high-level视觉任务，不涉及像素级图像恢复或增强。",
        "relevance_score": 1,
        "novelty_score": 3,
        "novelty_reason": "综述性工作，整合现有方法，无新架构或理论创新，创新度较低。"
    },
    {
        "title": "SAMRI: Segment Anything Model for MRI",
        "url": "http://arxiv.org/abs/2510.26635v1",
        "pub_date": "2025-10-30",
        "summary": "Accurate magnetic resonance imaging (MRI) segmentation is crucial for clinical decision-making, but remains labor-intensive when performed manually. Convolutional neural network (CNN)-based methods can be accurate and efficient, but often generalize poorly to MRI's variable contrast, intensity inhomogeneity, and protocols. Although the transformer-based Segment Anything Model (SAM) has demonstrated remarkable generalizability in natural images, existing adaptations often treat MRI as another imaging modality, overlooking these modality-specific challenges. We present SAMRI, an MRI-specialized SAM trained and validated on 1.1 million labeled MR slices spanning whole-body organs and pathologies. We demonstrate that SAM can be effectively adapted to MRI by simply fine-tuning its mask decoder using a two-stage strategy, reducing training time by 94% and trainable parameters by 96% versus full-model retraining. Across diverse MRI segmentation tasks, SAMRI achieves a mean Dice of 0.87, delivering state-of-the-art accuracy across anatomical regions and robust generalization on unseen structures, particularly small and clinically important structures.",
        "translated": "准确的磁共振成像（MRI）分割对于临床决策至关重要，但手动操作仍耗时费力。基于卷积神经网络（CNN）的方法虽能实现高精度和高效率，但在面对MRI中常见的对比度变化、强度不均匀性及不同扫描协议时，往往泛化能力较差。尽管基于Transformer的Segment Anything Model（SAM）在自然图像中表现出卓越的泛化能力，但现有适配方法通常将MRI视为另一种成像模态，忽视了其特有的挑战。本文提出SAMRI，一种专为MRI设计的SAM，其在包含全身器官和病理的110万张标注MR切片上进行训练与验证。我们证明，仅通过采用两阶段策略微调其掩码解码器，即可有效将SAM适配至MRI任务，相比全模型重新训练，训练时间减少94%，可训练参数减少96%。在多种MRI分割任务中，SAMRI实现了平均Dice系数0.87，在解剖区域上达到当前最优精度，并在未见过的结构（尤其是小而具有临床重要性的结构）上展现出强大的泛化能力。",
        "translated_title": "SAMRI：用于磁共振成像的分割一切模型",
        "label": [],
        "label_reason": "论文聚焦MRI分割，属于高阶语义理解任务，非像素级图像恢复或增强。",
        "relevance_score": 3,
        "novelty_score": 6,
        "novelty_reason": "提出针对MRI的SAM微调策略，提升泛化性，但方法基于现有架构，创新有限。"
    }
]